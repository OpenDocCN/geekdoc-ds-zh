["```r\npossible_values <- [c](https://rdrr.io/r/base/c.html)(1,0)\nBernoulli <- [sample](https://rdrr.io/r/base/sample.html)(possible_values,\n size=10000,\n replace=TRUE,\n prob=[c](https://rdrr.io/r/base/c.html)(0.3, 0.7))\n[prop.table](https://rdrr.io/r/base/proportions.html)([table](https://rdrr.io/r/base/table.html)(Bernoulli))\n```", "```r\n## Bernoulli\n##      0      1 \n## 0.7034 0.2966\n```", "```r\nh <- [hist](https://rdrr.io/r/graphics/hist.html)(Bernoulli,plot=FALSE)\nh$density = h$counts/[sum](https://rdrr.io/r/base/sum.html)(h$counts)*100\n[plot](https://rdrr.io/r/graphics/plot.default.html)(h,freq=FALSE, axes=FALSE)\n[axis](https://rdrr.io/r/graphics/axis.html)(1, at = [c](https://rdrr.io/r/base/c.html)(0, 1), labels = [c](https://rdrr.io/r/base/c.html)(\"Blue\", \"Red\"))\n[axis](https://rdrr.io/r/graphics/axis.html)(2, at = [c](https://rdrr.io/r/base/c.html)(0, 10, 20, 30, 40, 50, 60, 70))\n```", "```r\n# Draw a standard normal distribution:\nz = [seq](https://rdrr.io/r/base/seq.html)(-4, 4, length.out=1001)\nx = [rnorm](https://rdrr.io/r/stats/Normal.html)(z)\n[plot](https://rdrr.io/r/graphics/plot.default.html)( x=z, y=[dnorm](https://rdrr.io/r/stats/Normal.html)(z), bty='n', type='l', main=\"Standard normal distribution\", ylab=\"Probability density\", xlab=\"z\", xlim=[c](https://rdrr.io/r/base/c.html)(-3,3))\n[axis](https://rdrr.io/r/graphics/axis.html)(1, at = [seq](https://rdrr.io/r/base/seq.html)(-4, 4, by = 1))\n # annotate the density function with the 5% probability mass tails\n[polygon](https://rdrr.io/r/graphics/polygon.html)(x=[c](https://rdrr.io/r/base/c.html)(z[z<=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.025)], [qnorm](https://rdrr.io/r/stats/Normal.html)(0.025), [min](https://rdrr.io/r/base/Extremes.html)(z)), y=[c](https://rdrr.io/r/base/c.html)([dnorm](https://rdrr.io/r/stats/Normal.html)(z[z<=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.025)]), 0, 0), col=[grey](https://rdrr.io/r/grDevices/gray.html)(0.8))\n[polygon](https://rdrr.io/r/graphics/polygon.html)(x=[c](https://rdrr.io/r/base/c.html)(z[z>=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.975)], [max](https://rdrr.io/r/base/Extremes.html)(z), [qnorm](https://rdrr.io/r/stats/Normal.html)(0.975)), y=[c](https://rdrr.io/r/base/c.html)([dnorm](https://rdrr.io/r/stats/Normal.html)(z[z>=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.975)]), 0, 0), col=[grey](https://rdrr.io/r/grDevices/gray.html)(0.8))\n```", "```r\n[set.seed](https://rdrr.io/r/base/Random.html)(123)\ndata <- [rnorm](https://rdrr.io/r/stats/Normal.html)(100000, 4.2, 1)\n[hist](https://rdrr.io/r/graphics/hist.html)(data, freq = FALSE, col = \"gray\", xlab = \"Data Values\", main = \"Means of government satisfaction\")\n[curve](https://rdrr.io/r/graphics/curve.html)([dnorm](https://rdrr.io/r/stats/Normal.html)(x, mean = [mean](https://rdrr.io/r/base/mean.html)(data), sd = [sd](https://rdrr.io/r/stats/sd.html)(data)), col = \"black\", lwd = 2, add = TRUE)\n```", "```r\ndata_ess <- read_dta(\"data/ESS9_DE.dta\", encoding = \"latin1\") \n [hist](https://rdrr.io/r/graphics/hist.html)(data_ess$stfgov, breaks = \"FD\")\n```", "```r\n[summary](https://rdrr.io/r/base/summary.html)(data_ess$stfgov)\n```", "```r\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00    3.00    4.00    4.28    6.00   10.00      66\n```", "```r\n# We store the mean, the standard deviation and the number of observations as objects, because this way we can refer to them later on\nn <- 2292\nxbar <- [mean](https://rdrr.io/r/base/mean.html)(data_ess$stfgov, na.rm = TRUE)\ns <- [sd](https://rdrr.io/r/stats/sd.html)(data_ess$stfgov, na.rm = TRUE)\n # Set confidence level with 1-alpha (alpha = our willingness to be wrong in repeated samples)\nconf.level <- 0.95\n # Calculating the critical z-value for a two-sided test \nz <- [qnorm](https://rdrr.io/r/stats/Normal.html)((1 + conf.level) / 2)\n # Calculate the confidence interval\nlower.ci <- xbar - z * s / [sqrt](https://rdrr.io/r/base/MathFun.html)(n)\nupper.ci <- xbar + z * s / [sqrt](https://rdrr.io/r/base/MathFun.html)(n)\n # Print confidence intervals\n[cat](https://rdrr.io/r/base/cat.html)(\"The\", conf.level*100,\"% confidence interval for the population mean is (\",[round](https://rdrr.io/r/base/Round.html)(lower.ci, 2), \",\", [round](https://rdrr.io/r/base/Round.html)(upper.ci, 2),\").\\n\")\n```", "```r\n## The 95 % confidence interval for the population mean is ( 4.19 , 4.37 ).\n```", "```r\n[t.test](https://rdrr.io/r/stats/t.test.html)(data_ess$stfgov, conf.level = 0.95)\n```", "```r\n## \n##  One Sample t-test\n## \n## data:  data_ess$stfgov\n## t = 92.783, df = 2291, p-value < 2.2e-16\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  4.189216 4.370120\n## sample estimates:\n## mean of x \n##  4.279668\n```", "```r\n<p><br/></p> <details><summary> Solution: </summary><br/><blockquote> <p><font size=\"-1\"> <i/></font></p> <p><strong>Interpretation:</strong> Likely values for the population mean are within the limits of 4.2 and 4.4.</p> </blockquote> </details><hr/> <p><br/></p> <p>There is also the possibility to display the confidence interval as a plot:</p> <div class=\"sourceCode\" id=\"cb97\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">df</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/data.frame.html\">data.frame</a></span><span class=\"op\">(</span><span class=\"va\">xbar</span>, <span class=\"va\">lower.ci</span>, <span class=\"va\">upper.ci</span><span class=\"op\">)</span></span> <span/> <span><span class=\"fu\">ggplot</span><span class=\"op\">(</span><span class=\"va\">df</span>, <span class=\"fu\">aes</span><span class=\"op\">(</span>x <span class=\"op\">=</span> <span class=\"fl\">1</span>, y <span class=\"op\">=</span> <span class=\"va\">xbar</span><span class=\"op\">)</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">theme</span><span class=\"op\">(</span>axis.text.y <span class=\"op\">=</span> <span class=\"fu\">element_blank</span><span class=\"op\">(</span><span class=\"op\">)</span>,</span> <span> axis.ticks.x <span class=\"op\">=</span> <span class=\"fu\">element_blank</span><span class=\"op\">(</span><span class=\"op\">)</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">geom_point</span><span class=\"op\">(</span>size <span class=\"op\">=</span> <span class=\"fl\">3</span>, shape <span class=\"op\">=</span> <span class=\"fl\">21</span>, fill <span class=\"op\">=</span> <span class=\"st\">\"white\"</span>, colour <span class=\"op\">=</span> <span class=\"st\">\"black\"</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">geom_errorbar</span><span class=\"op\">(</span><span class=\"fu\">aes</span><span class=\"op\">(</span>ymin <span class=\"op\">=</span> <span class=\"va\">lower.ci</span>, ymax <span class=\"op\">=</span> <span class=\"va\">upper.ci</span><span class=\"op\">)</span>, width <span class=\"op\">=</span> <span class=\"fl\">0.2</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">coord_flip</span><span class=\"op\">(</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">labs</span><span class=\"op\">(</span>x <span class=\"op\">=</span> <span class=\"st\">\"Value\"</span>, y <span class=\"op\">=</span> <span class=\"st\">\"Mean with 95% CI\"</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">scale_x_continuous</span><span class=\"op\">(</span>breaks <span class=\"op\">=</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/seq.html\">seq</a></span><span class=\"op\">(</span><span class=\"fl\">0</span>, <span class=\"fl\">10</span>, by <span class=\"op\">=</span> <span class=\"fl\">1</span><span class=\"op\">)</span>, limits <span class=\"op\">=</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/c.html\">c</a></span><span class=\"op\">(</span><span class=\"fl\">1</span>, <span class=\"fl\">1</span><span class=\"op\">)</span><span class=\"op\">)</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/9521033d831eee314a1722faaaf52c8e.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/03-inference_files/figure-html/unnamed-chunk-8-1.png\"/></div> <p><br/></p> </div> <div id=\"proportion\" class=\"section level4\" number=\"4.4.2.2\"> <h4> <span class=\"header-section-number\">4.4.2.2</span> Proportion </h4> <p>Besides the mean, we can calculate a confidence interval for a proportion value of a sample. The formula is slightly different because the calculation of the standard error is different. It is the square root of the variance divided by the number of observations. The variance is obtained by multiplying the probability of an event <span class=\"math inline\">\\(p\\)</span> times the counter probability <span class=\"math inline\">\\(1-p\\)</span>:</p> <p><span class=\"math inline\">\\(95\\% CIs=[p - 1.96 \\times \\sqrt{\\frac{p\\times(1-p)}{n}}, p + 1.96 \\times \\sqrt{\\frac{p\\times(1-p)}{n}}]\\)</span>.</p> <p>Let’s try this by using a dichotomized version of the government satisfaction variable with an initial 11-point rating scale. In this example, values of 5 or less are assigned to 0 (“little or not satisfied”) and values above 5 are assigned to 1 (“satisfied”). We can now focus on the proportion of those respondents who are satisfied or rather satisfied with the government (variable value = 1). Note that with a 0/1 coding, the mean of a binary variable corresponds to the proportion of observations with a variable value of 1.</p> <div class=\"sourceCode\" id=\"cb98\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Recode of variable</span></span> <span><span class=\"va\">data_ess</span> <span class=\"op\"><-</span> <span class=\"va\">data_ess</span> <span class=\"op\">%>%</span> </span> <span> <span class=\"fu\">mutate</span><span class=\"op\">(</span>stfgov_di <span class=\"op\">=</span></span> <span> <span class=\"fu\">case_when</span><span class=\"op\">(</span><span class=\"va\">stfgov</span> <span class=\"op\"><=</span> <span class=\"fl\">5</span> <span class=\"op\">~</span> <span class=\"fl\">0</span>,</span> <span> <span class=\"va\">stfgov</span> <span class=\"op\">></span> <span class=\"fl\">5</span> <span class=\"op\">~</span> <span class=\"fl\">1</span><span class=\"op\">)</span><span class=\"op\">)</span></span> <span/> <span><span class=\"co\"># Proportion via table command</span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/proportions.html\">prop.table</a></span><span class=\"op\">(</span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/table.html\">table</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov_di</span><span class=\"op\">)</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## 0 1 ## 0.693281 0.306719</code></pre> <div class=\"sourceCode\" id=\"cb100\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Correspondence with the mean of the summary command</span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/summary.html\">summary</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov_di</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's ## 0.0000 0.0000 0.0000 0.3067 1.0000 1.0000 66</code></pre> <div class=\"sourceCode\" id=\"cb102\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Set confidence level with 1-alpha</span></span> <span><span class=\"va\">conf.level</span> <span class=\"op\"><-</span> <span class=\"fl\">0.95</span></span> <span/> <span><span class=\"co\"># Calculating the critical z-value for a two-sided test</span></span> <span><span class=\"va\">z</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/Normal.html\">qnorm</a></span><span class=\"op\">(</span><span class=\"op\">(</span><span class=\"fl\">1</span> <span class=\"op\">+</span> <span class=\"va\">conf.level</span><span class=\"op\">)</span> <span class=\"op\">/</span> <span class=\"fl\">2</span><span class=\"op\">)</span></span> <span/> <span><span class=\"co\"># Calculation of the confidence interval</span></span> <span><span class=\"va\">lower.ci</span> <span class=\"op\"><-</span> <span class=\"fl\">0.3067</span> <span class=\"op\">-</span> <span class=\"va\">z</span> <span class=\"op\">*</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/MathFun.html\">sqrt</a></span><span class=\"op\">(</span><span class=\"op\">(</span><span class=\"fl\">0.3067</span><span class=\"op\">*</span><span class=\"op\">(</span><span class=\"fl\">1</span><span class=\"op\">-</span><span class=\"fl\">0.3067</span><span class=\"op\">)</span><span class=\"op\">)</span><span class=\"op\">/</span><span class=\"va\">n</span><span class=\"op\">)</span></span> <span><span class=\"va\">upper.ci</span> <span class=\"op\"><-</span> <span class=\"fl\">0.3067</span> <span class=\"op\">+</span> <span class=\"va\">z</span> <span class=\"op\">*</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/MathFun.html\">sqrt</a></span><span class=\"op\">(</span><span class=\"op\">(</span><span class=\"fl\">0.3067</span><span class=\"op\">*</span><span class=\"op\">(</span><span class=\"fl\">1</span><span class=\"op\">-</span><span class=\"fl\">0.3067</span><span class=\"op\">)</span><span class=\"op\">)</span><span class=\"op\">/</span><span class=\"va\">n</span><span class=\"op\">)</span></span> <span/> <span><span class=\"co\"># Print the confidence intervals</span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/cat.html\">cat</a></span><span class=\"op\">(</span><span class=\"st\">\"The\"</span>, <span class=\"va\">conf.level</span><span class=\"op\">*</span><span class=\"fl\">100</span>, <span class=\"st\">\"% confidence interval for the proportion value is (\"</span>, <span class=\"fu\"><a href=\"https://rdrr.io/r/base/Round.html\">round</a></span><span class=\"op\">(</span><span class=\"va\">lower.ci</span>, <span class=\"fl\">2</span><span class=\"op\">)</span>, <span class=\"st\">\",\"</span>, <span class=\"fu\"><a href=\"https://rdrr.io/r/base/Round.html\">round</a></span><span class=\"op\">(</span><span class=\"va\">upper.ci</span>, <span class=\"fl\">2</span><span class=\"op\">)</span>, <span class=\"st\">\").\\n\"</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## The 95 % confidence interval for the proportion value is ( 0.29 , 0.33 ).</code></pre> <div class=\"sourceCode\" id=\"cb104\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Test using t.test</span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/stats/t.test.html\">t.test</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov_di</span>, conf.level <span class=\"op\">=</span> <span class=\"fl\">0.95</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## One Sample t-test ## ## data: data_ess$stfgov_di ## t = 31.837, df = 2291, p-value < 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.2878265 0.3256115 ## sample estimates: ## mean of x ## 0.306719</code></pre> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> Interpret the confidence interval.</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><br/><blockquote> <p><font size=\"-1\"> <i><strong>Interpretation:</strong> Likely values for the population proportion are within the limits of 0.29 and 0.33.</i></font></p> </blockquote> </details><hr/> <p><br/></p> </div> <div id=\"correlation-coefficient\" class=\"section level4\" number=\"4.4.2.3\"> <h4> <span class=\"header-section-number\">4.4.2.3</span> Correlation coefficient </h4> <p>Next, we look at how a confidence interval for a correlation is computed. For this, we use the following formula: <span class=\"math inline\">\\(95\\% CIs=r\\pm 1.96 \\times SE\\)</span>, where the standard error SE is calculated as follows: <span class=\"math inline\">\\(SE_r=\\sqrt {\\frac{(1-r^2)}{n-2}}\\)</span>. To illustrate that, we let the software compute the confidence interval of the correlation between household income and satisfaction with the government. <code>hinctnta</code> measures respondents’ net household income (after deductions) across 10 quantiles (“deciles” 1-10) - the reason for this measurement is that this way income becomes adjusted to a country’s income distribution and is thus comparable across countries. Regarding the correlation, we assume that - in line with economic voting theory - people with a higher income are more satisfied with the government than people with a lower income.</p> <div class=\"sourceCode\" id=\"cb106\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">cor</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/cor.test.html\">cor.test</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">hinctnta</span>, <span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov</span><span class=\"op\">)</span></span> <span><span class=\"va\">cor</span></span></code></pre></div> <pre><code>## ## Pearson's product-moment correlation ## ## data: data_ess$hinctnta and data_ess$stfgov ## t = 1.805, df = 2047, p-value = 0.07122 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.003445968 0.083023781 ## sample estimates: ## cor ## 0.03986354</code></pre> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> Interpret the confidence interval.</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><br/><blockquote> <p><font size=\"-1\"> <i><strong>Interpretation:</strong> Likely values for correlation in the population are within the limits of -0.003 and 0.08.</i></font></p> </blockquote> </details><hr/> <p><br/></p> <p>The graphical representation of the confidence interval of a correlation is two-dimensional. The width of the confidence interval can vary along the values of the x-axis. The value from the previous test is, so to say, the average confidence interval.</p> <div class=\"sourceCode\" id=\"cb108\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">ciplot</span> <span class=\"op\"><-</span> <span class=\"fu\">ggplot</span><span class=\"op\">(</span>data<span class=\"op\">=</span><span class=\"va\">data_ess</span>, <span class=\"fu\">aes</span><span class=\"op\">(</span>x <span class=\"op\">=</span> <span class=\"va\">hinctnta</span> , y <span class=\"op\">=</span> <span class=\"va\">stfgov</span><span class=\"op\">)</span><span class=\"op\">)</span> <span class=\"op\">+</span> </span> <span> <span class=\"fu\">geom_smooth</span><span class=\"op\">(</span>method <span class=\"op\">=</span> <span class=\"va\">lm</span>, se <span class=\"op\">=</span> <span class=\"cn\">TRUE</span>, level <span class=\"op\">=</span> <span class=\"fl\">0.95</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">xlab</span><span class=\"op\">(</span><span class=\"st\">\"Income (Deciles)\"</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">ylab</span><span class=\"op\">(</span><span class=\"st\">\"Satisfaction with the government\"</span><span class=\"op\">)</span></span> <span><span class=\"va\">ciplot</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/d86f767f96c71877b2d04915a8ab5e50.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/03-inference_files/figure-html/unnamed-chunk-12-1.png\"/></div> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> The shown figure displays 95% confidence intervals. Would 90% confidence intervals be narrower or wider?</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><br/> The 90% confidence intervals would be narrower. Going back to the probability-world interpretation: If we only want to capture the true population parameter 90 percent of the time (instead of 95), this means that we could narrow down the range of values in which the parameter will fall. In contrast, if we decrease our readiness to err to 1% (i.e., a 99% confidence interval), the range of values to fulfill this assumption would increase - the confidence interval would get wider. <br/></details><hr/> <p><br/></p> <p><br/></p> </div> </div> </div> <div id=\"null-hypothesis-testing\" class=\"section level2\" number=\"4.5\"> <h2> <span class=\"header-section-number\">4.5</span> Null Hypothesis Testing </h2> <div id=\"basic-idea-and-procedure\" class=\"section level3\" number=\"4.5.1\"> <h3> <span class=\"header-section-number\">4.5.1</span> Basic idea and procedure </h3> <p>Hypothesis testing allows us to test how likely it is that a sample estimate corresponds to another value, more specifically the sampling distribution of another value. We typically use for this purpose the distribution that is obtained under the null hypothesis- that is, for example, the assumption that the difference in means between two groups or the correlation between two variables is zero in the population. The distribution of a test statistic under the assumption that the null hypothesis is true is sometimes labeled “null distribution.” We know from the central limit theorem that such a sampling distribution would be normal given repeated sampling and a high enough sample size (>30), which implies that we can use a normal distribution for our statistical tests.</p> <p>The goal usually is to test the sample estimate we obtained against the null distribution. If the estimate is so “extreme” that it becomes very unlikely to obtain such a result although the null hypothesis is true, then this would provide evidence against the null hypothesis and in favor of our alternative hypothesis (e.g., the correlation between two variables is positive in the population). Hence, the procedure of Null Hypothesis Testing is a form of <em>proof by contradiction</em>. We assume the opposite of what we want to show and then collect as much evidence as possible against that assumption.</p> <p>To begin with, we first formulate the so-called <strong>null hypothesis <span class=\"math inline\">\\(H_0\\)</span></strong> (e.g., there is <em>no</em> or zero correlation in the population). Next, we formulate an <strong>alternative hypothesis <span class=\"math inline\">\\(H_A\\)</span></strong> (e.g., the correlation in the population is positive or negative) and then want to show that the correlation we found is strong enough so that it becomes very unlikely that <span class=\"math inline\">\\(H_0\\)</span> is true. This means that we are trying to disprove <span class=\"math inline\">\\(H_0\\)</span>, which in turn is evidence for <span class=\"math inline\">\\(H_A\\)</span>. Note that we do not assign probabilities for <span class=\"math inline\">\\(H_A\\)</span> to be true but probabilities to observe a given estimate given that <span class=\"math inline\">\\(H_0\\)</span> is true (i.e., <span class=\"math inline\">\\(P\\)</span>(correlation | <span class=\"math inline\">\\(H_0\\)</span>).</p> <p>The following steps illustrate how to do Null Hypothesis Testing:</p> <ul> <li> <strong>Step 1:</strong> Formulate the null hypothesis and alternative hypothesis.</li> <li> <strong>Step 2:</strong> Determine the probability of error (<span class=\"math inline\">\\(\\alpha\\)</span>, i.e., our willingness to err in repeated sampling, the convention is typically 5% or less).</li> <li> <strong>Step 3:</strong> Compute the value of the test statistic for your estimate (=estimator/standard error).</li> <li> <strong>Step 4a:</strong> Determine a critical value c (e.g., from an online table)</li> <li> <strong>Step 5a:</strong> If the value of the test statistic falls within the rejection range (i.e., exceeds the critical value), reject <span class=\"math inline\">\\(H_0\\)</span>.</li> </ul> <p>Alternatively:</p> <ul> <li><p><strong>Step 4b:</strong> Calculate p-value (use statistical software).</p></li> <li><p><strong>Step 5b:</strong> If p-value < probability of error, then reject <span class=\"math inline\">\\(H_0\\)</span>.</p></li> <li><p><strong>Step 6:</strong> Interpret the result of the hypothesis test in substantive terms.</p></li> </ul> <p><br/></p> </div> <div id=\"hypothesis-test-of-a-difference-in-means\" class=\"section level3\" number=\"4.5.2\"> <h3> <span class=\"header-section-number\">4.5.2</span> Hypothesis test of a difference in means </h3> <p>Differences in means usually refer to a comparison of the mean of a variable between two groups. To illustrate that, we use the example of income and government satisfaction again. We leave the variable government satisfaction <code>stfgov</code> in its original metric form so that we can calculate the mean. However, we dichotomize the variable income <code>hinctnta</code> into two groups: low earners and high earners.</p> <div class=\"sourceCode\" id=\"cb109\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">data_ess</span> <span class=\"op\"><-</span> <span class=\"va\">data_ess</span> <span class=\"op\">%>%</span> </span> <span> <span class=\"fu\">mutate</span><span class=\"op\">(</span>hinctnta_di <span class=\"op\">=</span></span> <span> <span class=\"fu\">case_when</span><span class=\"op\">(</span><span class=\"va\">hinctnta</span> <span class=\"op\"><=</span> <span class=\"fl\">6</span> <span class=\"op\">~</span> <span class=\"fl\">0</span>,</span> <span> <span class=\"va\">hinctnta</span> <span class=\"op\">></span> <span class=\"fl\">6</span> <span class=\"op\">~</span> <span class=\"fl\">1</span><span class=\"op\">)</span><span class=\"op\">)</span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/summary.html\">summary</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">hinctnta_di</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's ## 0.0000 0.0000 0.0000 0.4794 1.0000 1.0000 270</code></pre> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> Formulate the null and alternative hypotheses if we assume that persons with a high-income are more satisfied with the government.</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"> H_0: H_A:\n```", "```r\n# Draw a standard normal distribution:\nz = [seq](https://rdrr.io/r/base/seq.html)(-4, 4, length.out=1001)\nx = [rnorm](https://rdrr.io/r/stats/Normal.html)(z)\n # Null distribution of two-sided test\n[plot](https://rdrr.io/r/graphics/plot.default.html)( x=z, y=[dnorm](https://rdrr.io/r/stats/Normal.html)(z), bty='n', type='l', main=\"Null distribution of two-sided test, error probability 0.05\", ylab=\"Probability density\", xlab=\"z\", xlim=[c](https://rdrr.io/r/base/c.html)(-3,3))\n[axis](https://rdrr.io/r/graphics/axis.html)(1, at = [seq](https://rdrr.io/r/base/seq.html)(-4, 4, by = 1))\n[polygon](https://rdrr.io/r/graphics/polygon.html)(x=[c](https://rdrr.io/r/base/c.html)(z[z<=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.025)], [qnorm](https://rdrr.io/r/stats/Normal.html)(0.025), [min](https://rdrr.io/r/base/Extremes.html)(z)), y=[c](https://rdrr.io/r/base/c.html)([dnorm](https://rdrr.io/r/stats/Normal.html)(z[z<=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.025)]), 0, 0), col=\"maroon\")\n[polygon](https://rdrr.io/r/graphics/polygon.html)(x=[c](https://rdrr.io/r/base/c.html)(z[z>=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.975)], [max](https://rdrr.io/r/base/Extremes.html)(z), [qnorm](https://rdrr.io/r/stats/Normal.html)(0.975)), y=[c](https://rdrr.io/r/base/c.html)([dnorm](https://rdrr.io/r/stats/Normal.html)(z[z>=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.975)]), 0, 0), col=\"maroon\")\n```", "```r\n# Null distribution of one-sided test (H_A>0)\n[plot](https://rdrr.io/r/graphics/plot.default.html)( x=z, y=[dnorm](https://rdrr.io/r/stats/Normal.html)(z), bty='n', type='l', main=\"Null distribution of one-sided test (H_A>0), error probability 0.05\", ylab=\"Probability density\", xlab=\"z\", xlim=[c](https://rdrr.io/r/base/c.html)(-3,3))\n[axis](https://rdrr.io/r/graphics/axis.html)(1, at = [seq](https://rdrr.io/r/base/seq.html)(-4, 4, by = 1))\n[polygon](https://rdrr.io/r/graphics/polygon.html)(x=[c](https://rdrr.io/r/base/c.html)(z[z>=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.95)], [max](https://rdrr.io/r/base/Extremes.html)(z), [qnorm](https://rdrr.io/r/stats/Normal.html)(0.95)), y=[c](https://rdrr.io/r/base/c.html)([dnorm](https://rdrr.io/r/stats/Normal.html)(z[z>=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.95)]), 0, 0), col=\"maroon\")\n```", "```r\n# Null distribution of one-sided test (H_A<0)\n[plot](https://rdrr.io/r/graphics/plot.default.html)( x=z, y=[dnorm](https://rdrr.io/r/stats/Normal.html)(z), bty='n', type='l', main=\"Null distribution of one-sided test (H_A<0), error probability 0.05\", ylab=\"Probability density\", xlab=\"z\", xlim=[c](https://rdrr.io/r/base/c.html)(-3,3))\n[axis](https://rdrr.io/r/graphics/axis.html)(1, at = [seq](https://rdrr.io/r/base/seq.html)(-4, 4, by = 1))\n[polygon](https://rdrr.io/r/graphics/polygon.html)(x=[c](https://rdrr.io/r/base/c.html)(z[z<=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.05)], [qnorm](https://rdrr.io/r/stats/Normal.html)(0.05), [min](https://rdrr.io/r/base/Extremes.html)(z)), y=[c](https://rdrr.io/r/base/c.html)([dnorm](https://rdrr.io/r/stats/Normal.html)(z[z<=[qnorm](https://rdrr.io/r/stats/Normal.html)(0.05)]), 0, 0), col=\"maroon\")\n```", "```r\n<p><br/></p> <details><summary> Solution: </summary><p><br/> Assuming a positive mean difference or correlation, the rejection area would be located only on the right side of the distribution. Since we still use an error probability of 5%, this area would be larger than in the two-sided case. Consequently, the critical value c thus moves a bit in the direction of the center of the distribution. For the z-distribution, this value is +1.645 for a positive alternative hypothesis (and -1.645 for a negative hypothesis).</p> <p>If we change the probability of error to 1% and test two-sided, we have a rejection range that is smaller compared to the 5% case. Our estimate and the corresponding z-statistic need to be larger to reject the null hypothesis compared to the 5%-case. In the 1% case, the critical values are -2.326 and +2.326.</p> <p>Critical values at which one can reject the null hypothesis can be obtained from here: <a href=\"https://www.criticalvaluecalculator.com/\" target=\"„_blank“\">https://www.criticalvaluecalculator.com/</a></p> <br/></details><hr/> <p><br/></p> <div id=\"general-remarks-on-p-values\" class=\"section level4\" number=\"4.5.2.1\"> <h4> <span class=\"header-section-number\">4.5.2.1</span> General remarks on P-values </h4> <p>P-values <strong>indicate the probability of finding an estimate as extreme (or more extreme) as we got from our sample (e.g., a difference in means), even though the null hypothesis is true in the population.</strong> In formal terms, that is the probability of the estimate given the null hypothesis <span class=\"math inline\">\\(P\\)</span>(estimate | <span class=\"math inline\">\\(H_0\\)</span>).</p> <p>Some features of p-values:</p> <ul> <li><p>The smaller the p-value, the more “statistically significant” the result (or the more reason we have to reject the null hypothesis, which, in turn, provides evidence in favor of the alternative hypothesis).</p></li> <li><p>Contingent upon the probability of error, if p is smaller than <span class=\"math inline\">\\(\\alpha\\)</span>, then the result is statistically significant (usually < 0.05 or 5%).</p></li> <li><p>If the test statistic associated with the estimate is less than or greater than the critical z- or t-values, respectively, then p < 0.05.</p></li> <li> <p>Correspondence with confidence intervals:</p> <ul> <li><p>If the confidence interval includes the null hypothesis value (usually 0), this is consistent with a high p-value, indicating weak evidence against the null hypothesis.</p></li> <li><p>If the confidence interval does not include 0, it is consistent with a low p-value, suggesting strong evidence against the null hypothesis.</p></li> <li><p>For example: A mean difference of 0.5 with a 95% confidence interval of [0.35; 0.65] is statistically significant at the <span class=\"math inline\">\\(\\alpha\\)</span>=0.05 level (reject <span class=\"math inline\">\\(H_0\\)</span>). However, a mean difference of -0.1 with a 95% confidence interval of [-0.25; 0.05] is statistically <strong>not</strong> significant at the <span class=\"math inline\">\\(\\alpha\\)</span>=0.05 level (confidence intervals include 0, fail to reject <span class=\"math inline\">\\(H_0\\)</span>).</p></li> </ul> </li> <li><p>To calculate the exact p-value, we rely on tables or statistical software.</p></li> <li><p>A p-value says <strong>nothing</strong> about the probability that <span class=\"math inline\">\\(H_0\\)</span> or <span class=\"math inline\">\\(H_A\\)</span> is true; remember <span class=\"math inline\">\\(P\\)</span>(estimate | <span class=\"math inline\">\\(H_0\\)</span>)!</p></li> <li><p>When we formulate one-sided hypotheses but use a two-sided test (which is the norm in social science research), we are especially careful and do not want to make a 1st kind error (<span class=\"math inline\">\\(H_0\\)</span> is true, but we reject it); we can divide the p-value by 2 and end up with the p-value we would have gotten if we tested one-sided.</p></li> </ul> </div> <div id=\"test-implementation\" class=\"section level4\" number=\"4.5.2.2\"> <h4> <span class=\"header-section-number\">4.5.2.2</span> Test implementation </h4> <p>To actually test the difference in the means of satisfaction with the government between high and low earners, the formula for this is given as <span class=\"math inline\">\\(z =\\frac{(\\bar x_1 - \\bar x_2)}{standard error}\\)</span> (note that the standard error of the mean difference is not simply the difference of two standard errors).</p> <p>We now calculate the mean difference and conduct a corresponding hypothesis test:</p> <div class=\"sourceCode\" id=\"cb114\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/mean.html\">mean</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov</span><span class=\"op\">[</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">hinctnta_di</span><span class=\"op\">==</span><span class=\"fl\">1</span><span class=\"op\">]</span>, na.rm <span class=\"op\">=</span> <span class=\"cn\">TRUE</span><span class=\"op\">)</span><span class=\"op\">-</span></span> <span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/mean.html\">mean</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov</span><span class=\"op\">[</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">hinctnta_di</span><span class=\"op\">==</span><span class=\"fl\">0</span><span class=\"op\">]</span>, na.rm <span class=\"op\">=</span> <span class=\"cn\">TRUE</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## [1] 0.1543566</code></pre> <div class=\"sourceCode\" id=\"cb116\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"fu\"><a href=\"https://rdrr.io/r/stats/t.test.html\">t.test</a></span><span class=\"op\">(</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov</span><span class=\"op\">[</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">hinctnta_di</span><span class=\"op\">==</span><span class=\"fl\">1</span><span class=\"op\">]</span>, <span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">stfgov</span><span class=\"op\">[</span><span class=\"va\">data_ess</span><span class=\"op\">$</span><span class=\"va\">hinctnta_di</span><span class=\"op\">==</span><span class=\"fl\">0</span><span class=\"op\">]</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## Welch Two Sample t-test ## ## data: data_ess$stfgov[data_ess$hinctnta_di == 1] and data_ess$stfgov[data_ess$hinctnta_di == 0] ## t = 1.5882, df = 2046.8, p-value = 0.1124 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.03624397 0.34495717 ## sample estimates: ## mean of x mean of y ## 4.354545 4.200189</code></pre> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> Interpret the result of the statistical test in detail with reference to the specific p-value.</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><p><br/> The probability of obtaining such a result from the sample, although <span class=\"math inline\">\\(H_0\\)</span> is true in the population, is 0.11 or 11%. Since this exceeds the threshold of our willingness to err of 5%, we cannot reject the null hypothesis (and thus find no evidence in favor of <span class=\"math inline\">\\(H_A\\)</span>).</p> <p>Even if we take into account that we formulated a directed hypothesis and rely on a one-sided test (we can thus divide the p-value by 2), the p-value still exceeds our assumed probability of error (p-value of 0.055 > <span class=\"math inline\">\\(\\alpha\\)</span> of 0.05). Hence, even with a one-sided test, we are not able to reject the null hypothesis.</p> <br/></details><hr/> <p><br/></p> </div> </div> <div id=\"hypothesis-test-of-a-correlation\" class=\"section level3\" number=\"4.5.3\"> <h3> <span class=\"header-section-number\">4.5.3</span> Hypothesis test of a correlation </h3> <p>Instead of a difference in means, we now focus on hypothesis tests of correlations. The procedure is analogous to the one before. We first formulate the null and alternative hypotheses, specify the probability of error (5% in this case), calculate the test statistic (<span class=\"math inline\">\\(z=\\frac{r}{SE}\\)</span>), calculate the p-value, and interpret the result.</p> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> Assume that higher income is either positively or negatively related to satisfaction with government. Formulate the null and alternative hypotheses (undirected).</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"> H_0: H_A:\n```", "```r\ncor <- [cor.test](https://rdrr.io/r/stats/cor.test.html)(data_ess$hinctnta, data_ess$stfgov)\ncor\n```", "```r\n## \n##  Pearson's product-moment correlation\n## \n## data:  data_ess$hinctnta and data_ess$stfgov\n## t = 1.805, df = 2047, p-value = 0.07122\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.003445968  0.083023781\n## sample estimates:\n##        cor \n## 0.03986354\n```", "```r\n<p><br/></p> <details><summary> Solution: </summary><p><br/> The probability of obtaining such a result from the sample, although <span class=\"math inline\">\\(H_0\\)</span> is true in the population, is 0.07 or 7%. Since this exceeds the threshold of our willingness to err of 5%, the result is not statistically significant and we cannot reject the null hypothesis (and thus find no evidence in favor of <span class=\"math inline\">\\(H_A\\)</span>).</p> <br/></details><hr/> <p><br/></p> </div> </div> <div id=\"outlook-1\" class=\"section level2\" number=\"4.6\"> <h2> <span class=\"header-section-number\">4.6</span> Outlook </h2> <p>This case study reviewed the basics of probability theory, some relevant statistical laws, confidence intervals, and hypothesis testing. In the examples, we have mainly used the z-distribution, which is the way to go if we deal with large or reasonably large samples (>30 is already sufficient). For small samples and other purposes, we use other distributions for statistical testing that are briefly introduced here.</p> <p><br/></p> <div id=\"t-distribution\" class=\"section level3\" number=\"4.6.1\"> <h3> <span class=\"header-section-number\">4.6.1</span> t-distribution </h3> <p>The t-distribution corresponds to the z-distribution for larger samples. For small samples, the distribution is flatter and therefore has wider margins. The critical t-values (above or below we can reject the null hypothesis) are therefore larger on the right and lower on the left side. This makes it more difficult to reject the null hypothesis. In other words: We need more extreme results in small samples to reject the null hypothesis. This accounts for the fact that some outliers have greater weight in small samples. This helps to avoid the so-called alpha error (or type I error), which refers to erroneously inferring statistically significant results although the null hypothesis is true in the population. The alpha error is usually seen as a greater threat to scientific progress than failing to reject the null hypothesis although we should have done so (beta error or type II error).</p> <p>Df refers to degrees of freedom, which is usually the number of cases - 1 (for estimation).</p> <div class=\"float\" id=\"id\"> <img src=\"../Images/c287a1a35548f8f82bf97e42fb7dfcd5.png\" class=\"class\" style=\"width:50.0%;height:50.0%\" alt=\"t-distribution\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/t.png\"/><div class=\"figcaption\">t-distribution</div> </div> <p>Source: <a href=\"https://www.geo.fu-berlin.de/en/v/soga-py/Basics-of-statistics/Continous-Random-Variables/Students-t-Distribution/index.html\" target=\"„_blank“\">https://www.geo.fu-berlin.de/en/v/soga-py/Basics-of-statistics/Continous-Random-Variables/Students-t-Distribution/index.html</a></p> <p><br/></p> </div> <div id=\"chi-square-distribution\" class=\"section level3\" number=\"4.6.2\"> <h3> <span class=\"header-section-number\">4.6.2</span> Chi-square distribution </h3> <p>The chi-square distribution is used, for example, for testing variances. One field of application is in cross tabulations. Chi-square values are never negative, so only one-sided tests are suitable.</p> <p>Df refers to degrees of freedom, which in the case of crosstabs would be the table size, for example.</p> <div class=\"float\" id=\"id\"> <img src=\"../Images/f7ed5980e4a4bc02da3af38a835ec9cc.png\" class=\"class\" style=\"width:50.0%;height:50.0%\" alt=\"Chi-Square-Distribution\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/chi.png\"/><div class=\"figcaption\">Chi-Square-Distribution</div> </div> <p>Source: <a href=\"https://www.geo.fu-berlin.de/en/v/soga-py/Basics-of-statistics/Continous-Random-Variables/Chi-Square-Distribution/index.html\" target=\"„_blank“\">https://www.geo.fu-berlin.de/en/v/soga-py/Basics-of-statistics/Continous-Random-Variables/Chi-Square-Distribution/index.html</a></p> <p><br/></p> </div> <div id=\"f-distribution\" class=\"section level3\" number=\"4.6.3\"> <h3> <span class=\"header-section-number\">4.6.3</span> F-Distribution </h3> <p>The F-distribution is basically the ratio of two chi-squared distributed random variables (e.g., two variances). The F-distribution is also always positive and the functional form depends on the degrees of freedom of the two variables under consideration. F-tests are used, for example, in comparing the model fit of regression models.</p> <p><img src=\"../Images/2de8e3c2e232988c4e1262fb8cdbbea5.png\" id=\"id\" class=\"class\" style=\"width:50.0%;height:50.0%\" alt=\"F-Distribution 1\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/f1.png\"/><img src=\"../Images/8597d82a7302d8203525fc4557db3545.png\" id=\"id\" class=\"class\" style=\"width:50.0%;height:50.0%\" alt=\"F-Distribution 2\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/f2.png\"/></p> <p>Source: <a href=\"https://www.geo.fu-berlin.de/en/v/soga-py/Basics-of-statistics/Continous-Random-Variables/F-Distribution/index.html\" target=\"„_blank“\">https://www.geo.fu-berlin.de/en/v/soga-py/Basics-of-statistics/Continous-Random-Variables/F-Distribution/index.html</a></p> <p><br/><br/></p> </div> <div id=\"statistical-versus-substantive-significance\" class=\"section level3\" number=\"4.6.4\"> <h3> <span class=\"header-section-number\">4.6.4</span> Statistical versus substantive significance </h3> <p>The procedures discussed here relate to the concept of hypothesis testing and statistical significance. They are essentially concerned with quantifying uncertainty and inferring from that whether or not one can assume that a result from a random sample can be generalized to the population the sample has been drawn from. Although these are important concepts, statistical significance is different from scientific or substantive significance, which refers to the degree to which a result is relevant against the background of other research findings or alternative explanatory factors. Thus, a statistically significant result may not be substantial or, conversely, a substantial result may not be statistically significant. Statistical significance, for example, is highly dependent on the number of observations that are incorporated in calculating the standard error. A measure of the substance of, for example, a regression coefficient can be obtained by using standardized effect sizes such as beta or Cohen’s D.</p> <p>In general, we are looking for empirical results that are both statistically and substantively significant.</p> <p><br/></p> <p><br/><br/></p> </div> </div> </div> <div class=\"chapter-nav\"> <div class=\"prev\"><a href=\"bivariate-statistics-case-study-united-states-presidential-election.html\"><span class=\"header-section-number\">3</span> Bivariate Statistics – Case Study United States Presidential Election</a></div> <div class=\"next\"><a href=\"regression-analysis---case-study-attitudes-toward-justice.html\"><span class=\"header-section-number\">5</span> Regression Analysis - Case Study Attitudes Toward Justice</a></div> </div> </body> </html>\n```"]