["```py\nuv init shelter_usage\ncd shelter_usage\n```", "```py\nuv run hello.py\n```", "```py\nuv add numpy\n```", "```py\nimport numpy as np\n\ndef main():\n np.random.seed(853)\n\n mu, sigma = 0, 1\n sample_sizes = [10, 100, 1000, 10000]\n differences = []\n\n for size in sample_sizes:\n sample = np.random.normal(mu, sigma, size)\n sample_mean = np.mean(sample)\n diff = abs(mu - sample_mean)\n differences.append(diff)\n print(f\"Sample size: {size}\")\n print(f\"  Difference between sample and population mean: {round(diff, 3)}\")\n\nif __name__ == \"__main__\":\n main()\n```", "```py\nuv add polars\n```", "```py\n#### Preamble ####\n# Purpose: Simulates a dataset of daily shelter usage\n# Author: Rohan Alexander\n# Date: 12 November 2024\n# Contact: rohan.alexander@utoronto.ca\n# License: MIT\n# Pre-requisites:\n# - Add `polars`: uv add polars\n# - Add `numpy`: uv add numpy\n# - Add `datetime`: uv add datetime\n\n#### Workspace setup ####\nimport polars as pl\nimport numpy as np\nfrom datetime import date\n\nrng = np.random.default_rng(seed=853)\n\n#### Simulate data ####\n# Simulate 10 shelters and some set capacity\nshelters_df = pl.DataFrame(\n {\n \"Shelters\": [f\"Shelter {i}\" for i in range(1, 11)],\n \"Capacity\": rng.integers(low=10, high=100, size=10),\n }\n)\n\n# Create data frame of dates\ndates = pl.date_range(\n start=date(2024, 1, 1), end=date(2024, 12, 31), interval=\"1d\", eager=True\n).alias(\"Dates\")\n\n# Convert dates into a data frame\ndates_df = pl.DataFrame(dates)\n\n# Combine dates and shelters\ndata = dates_df.join(shelters_df, how=\"cross\")\n\n# Add usage as a Poisson draw\npoisson_draw = rng.poisson(lam=data[\"Capacity\"])\nusage = np.minimum(poisson_draw, data[\"Capacity\"])\n\ndata = data.with_columns([pl.Series(\"Usage\", usage)])\n\ndata.write_parquet(\"simulated_data.parquet\")\n```", "```py\nuv add pydantic\n```", "```py\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\nfrom datetime import date\n\n# Define the Pydantic model\nclass ShelterData(BaseModel):\n Dates: date  # Validates date format (e.g., 'YYYY-MM-DD')\n Shelters: str  # Must be a string\n Capacity: int = Field(..., ge=0)  # Must be a non-negative integer\n Usage: int = Field(..., ge=0)  # Must be non-negative\n\n # Add a field validator for usage to ensure it does not exceed capacity\n @field_validator(\"Usage\")\n def check_usage_not_exceed_capacity(cls, usage, info):\n capacity = info.data.get(\"Capacity\")\n if capacity is not None and usage > capacity:\n raise ValueError(f\"Usage ({usage}) exceeds capacity ({capacity}).\")\n return usage\n```", "```py\nimport polars as pl\n\ndf = pl.read_parquet(\"simulated_data.parquet\")\n\n# Convert Polars DataFrame to a list of dictionaries for validation\ndata_dicts = df.to_dicts()\n\n# Validate the dataset in batches\nvalidated_data = []\nerrors = []\n\n# Batch validation\nfor i, row in enumerate(data_dicts):\n try:\n validated_row = ShelterData(**row)  # Validate each row\n validated_data.append(validated_row)\n except ValidationError as e:\n errors.append((i, e))\n\n# Convert validated data back to a Polars DataFrame\nvalidated_df = pl.DataFrame([row.dict() for row in validated_data])\n\n# Display results\nprint(\"Validated Rows:\")\nprint(validated_df)\n\nif errors:\n print(\"\\nErrors:\")\n for i, error in errors:\n print(f\"Row {i}: {error}\")\n```", "```py\nimport polars as pl\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\nfrom datetime import date\n\n# Define the Pydantic model\nclass ShelterData(BaseModel):\n Dates: date  # Validates date format (e.g., 'YYYY-MM-DD')\n Shelters: str  # Must be a string\n Capacity: int = Field(..., ge=0)  # Must be a non-negative integer\n Usage: int = Field(..., ge=0)  # Must be non-negative\n\n # Add a field validator for Usage to ensure it does not exceed Capacity\n @field_validator(\"Usage\")\n def check_usage_not_exceed_capacity(cls, usage, info):\n capacity = info.data.get(\"Capacity\")\n if capacity is not None and usage > capacity:\n raise ValueError(f\"Usage ({usage}) cannot exceed Capacity ({capacity}).\")\n return usage\n\n# Define the dataset\ndf = [\n {\"Dates\": \"2024-01-01\", \"Shelters\": \"Shelter 1\", \"Capacity\": 23, \"Usage\": 22},\n {\"Dates\": \"rohan\", \"Shelters\": \"Shelter 2\", \"Capacity\": 62, \"Usage\": 62},\n {\"Dates\": \"2024-01-01\", \"Shelters\": \"Shelter 3\", \"Capacity\": 93, \"Usage\": 88},\n # Add invalid row for testing\n {\"Dates\": \"2024-01-01\", \"Shelters\": \"Shelter 4\", \"Capacity\": 50, \"Usage\": 55},\n]\n\n# Validate the dataset in batches\nvalidated_data = []\nerrors = []\n\n# Batch validation\nfor i, row in enumerate(df):\n try:\n validated_row = ShelterData(**row)  # Validate each row\n validated_data.append(validated_row)\n except ValidationError as e:\n errors.append((i, e))\n\n# Convert validated data back to a Polars DataFrame\nvalidated_df = pl.DataFrame([row.dict() for row in validated_data])\n\n# Display results\nprint(\"Validated Rows:\")\nprint(validated_df)\n\nif errors:\n print(\"\\nErrors:\")\n for i, error in errors:\n print(f\"Row {i}: {error}\")\n```", "```py\nErrors:\nRow 1: 1 validation error for ShelterData\nDates\n  Input should be a valid date or datetime, input is too short [type=date_from_datetime_parsing, input_value='rohan', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/date_from_datetime_parsing\nRow 3: 1 validation error for ShelterData\nUsage\n  Value error, Usage (55) cannot exceed Capacity (50). [type=value_error, input_value=55, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error\n```", "```py\nimport polars as pl\n\n# URL of the CSV file\nurl = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/21c83b32-d5a8-4106-a54f-010dbe49f6f2/resource/ffd20867-6e3c-4074-8427-d63810edf231/download/Daily%20shelter%20overnight%20occupancy.csv\"\n\n# Read the CSV file into a Polars DataFrame\ndf = pl.read_csv(url)\n\n# Save the raw data\ndf.write_parquet(\"shelter_usage.parquet\")\n```", "```py\nimport polars as pl\n\ndf = pl.read_parquet(\"shelter_usage.parquet\")\n\n# Select specific columns\nselected_columns = [\"OCCUPANCY_DATE\", \"SHELTER_ID\", \"OCCUPIED_BEDS\", \"CAPACITY_ACTUAL_BED\"]\n\nselected_df = df.select(selected_columns)\n\n# Filter to only rows that have data\nfiltered_df = selected_df.filter(df[\"OCCUPIED_BEDS\"].is_not_null())\n\nprint(filtered_df.head())\n\nrenamed_df = filtered_df.rename({\"OCCUPANCY_DATE\": \"date\",\n \"SHELTER_ID\": \"Shelters\",\n \"CAPACITY_ACTUAL_BED\": \"Capacity\",\n \"OCCUPIED_BEDS\": \"Usage\"\n })\n\nprint(renamed_df.head())\n\nrenamed_df.write_parquet(\"cleaned_shelter_usage.parquet\")\n```", "```py\nimport polars as pl\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\nfrom datetime import date\n\n# Define the Pydantic model\nclass ShelterData(BaseModel):\n Dates: date  # Validates date format (e.g., 'YYYY-MM-DD')\n Shelters: str  # Must be a string\n Capacity: int = Field(..., ge=0)  # Must be a non-negative integer\n Usage: int = Field(..., ge=0)  # Must be non-negative\n\n # Add a field validator for Usage to ensure it does not exceed Capacity\n @field_validator(\"Usage\")\n def check_usage_not_exceed_capacity(cls, usage, info):\n capacity = info.data.get(\"Capacity\")\n if capacity is not None and usage > capacity:\n raise ValueError(f\"Usage ({usage}) cannot exceed Capacity ({capacity}).\")\n return usage\n\ndf = pl.read_parquet(\"cleaned_shelter_usage.parquet\")\n\n# Convert Polars DataFrame to a list of dictionaries for validation\ndata_dicts = df.to_dicts()\n\n# Validate the dataset in batches\nvalidated_data = []\nerrors = []\n\n# Batch validation\nfor i, row in enumerate(data_dicts):\n try:\n validated_row = ShelterData(**row)  # Validate each row\n validated_data.append(validated_row)\n except ValidationError as e:\n errors.append((i, e))\n\n# Convert validated data back to a Polars DataFrame\nvalidated_df = pl.DataFrame([row.dict() for row in validated_data])\n\n# Display results\nprint(\"Validated Rows:\")\nprint(validated_df)\n\nif errors:\n print(\"\\nErrors:\")\n for i, error in errors:\n print(f\"Row {i}: {error}\")\n```", "```py\nimport polars as pl\n\ndf = pl.read_parquet(\"cleaned_shelter_usage.parquet\")\n\n# Convert the date column to datetime and rename it for clarity\ndf = df.with_columns(pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\"))\n\n# Group by \"Dates\" and calculate total \"Capacity\" and \"Usage\"\naggregated_df = (\n df.group_by(\"date\")\n .agg([\n pl.col(\"Capacity\").sum().alias(\"Total_Capacity\"),\n pl.col(\"Usage\").sum().alias(\"Total_Usage\")\n ])\n .sort(\"date\")  # Sort the results by date\n)\n\n# Display the aggregated DataFrame\nprint(aggregated_df)\n```", "```py\nimport polars as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.dates as mdates\n\n# Read the Polars DataFrame from a Parquet file\ndf = pl.read_parquet(\"analysis_data.parquet\")\n\n# Ensure the 'date' column is of datetime type in Polars\ndf = df.with_columns([\n pl.col('date').cast(pl.Date)\n])\n\n# Select the relevant columns and reshape the DataFrame\ndf_melted = df.select([\"date\", \"Total_Capacity\", \"Total_Usage\"]).melt(\n id_vars=\"date\",\n variable_name=\"Metric\",\n value_name=\"Value\"\n)\n\n# Convert Polars DataFrame to a Pandas DataFrame for Seaborn\ndf_melted_pd = df_melted.to_pandas()\n\n# Ensure 'date' column is datetime in Pandas\ndf_melted_pd['date'] = pd.to_datetime(df_melted_pd['date'])\n\n# Set the plotting style\nsns.set_theme(style=\"whitegrid\")\n\n# Create the plot\nplt.figure(figsize=(12, 6))\nsns.lineplot(\n data=df_melted_pd,\n x=\"date\",\n y=\"Value\",\n hue=\"Metric\",\n linewidth=2.5\n)\n\n# Format the x-axis to show dates nicely\nplt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# Add labels and title\nplt.xlabel(\"Date\")\nplt.ylabel(\"Values\")\nplt.title(\"Total Capacity and Usage Over Time\")\n\n# Adjust layout to prevent clipping of tick-labels\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n```"]