- en: 4.3\. Approximating subspaces and the SVD#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.3\. 近似子空间和奇异值分解#
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/03_svd/roch-mmids-svd-svd.html](https://mmids-textbook.github.io/chap04_svd/03_svd/roch-mmids-svd-svd.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap04_svd/03_svd/roch-mmids-svd-svd.html](https://mmids-textbook.github.io/chap04_svd/03_svd/roch-mmids-svd-svd.html)
- en: In this section, we introduce the singular value decomposition (SVD). We motivate
    it via the problem of finding a best approximating subspace to a collection of
    data points – although it has applications far beyond.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了奇异值分解（SVD）。我们通过寻找数据点集合的最佳近似子空间的问题来激发其动机——尽管它的应用远不止于此。
- en: 4.3.1\. An objective, an algorithm, and a guarantee[#](#an-objective-an-algorithm-and-a-guarantee
    "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3.1\. 一个目标，一个算法和一个保证[#](#an-objective-an-algorithm-and-a-guarantee "链接到本标题")
- en: Let \(\boldsymbol{\alpha}_1,\dots,\boldsymbol{\alpha}_n\) be a collection of
    \(n\) data points in \(\mathbb{R}^m\). A natural way to extract low-dimensional
    structure in this dataset is to find a low-dimensional linear subspace \(\mathcal{Z}\)
    of \(\mathbb{R}^m\) such that the \(\boldsymbol{\alpha}_i\)’s are “close to it.”
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\boldsymbol{\alpha}_1,\dots,\boldsymbol{\alpha}_n\) 是 \(\mathbb{R}^m\) 中的
    \(n\) 个数据点集合。提取该数据集低维结构的一种自然方法是找到一个 \(\mathbb{R}^m\) 的低维线性子空间 \(\mathcal{Z}\)，使得
    \(\boldsymbol{\alpha}_i\) 的点“接近”它。
- en: '![The closest line to some data points (with help from ChatGPT; code converted
    from (Source))](../Images/bb6c0e303b9f07ef152154d91bcdbf18.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![与某些数据点最接近的线（借助 ChatGPT；代码从（来源）转换而来）](../Images/bb6c0e303b9f07ef152154d91bcdbf18.png)'
- en: '**Mathematical formulation of the problem** Again the squared Euclidean norm
    turns out to be computationally convenient. So we look for a linear subspace \(\mathcal{Z}\)
    that minimizes'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题的数学表述** 再次，平方欧几里得范数在计算上很方便。因此，我们寻找一个线性子空间 \(\mathcal{Z}\)，它最小化'
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
- en: over all linear subspaces of \(\mathbb{R}^m\) of dimension \(k\). To solve this
    problem, which we refer to as the best approximating subspace problem\(\idx{best
    approximating subspace problem}\xdi\), we make a series of observations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 \(\mathbb{R}^m\) 的所有维度为 \(k\) 的线性子空间上。为了解决这个问题，我们称之为最佳近似子空间问题\(\idx{best approximating
    subspace problem}\xdi\)，我们进行了一系列观察。
- en: '**KNOWLEDGE CHECK:** Consider the data points \(\boldsymbol{\alpha}_1 = (-1,1)\)
    and \(\boldsymbol{\alpha}_2 = (1,-1)\). For \(k=1\), what is the solution of the
    best approximating subspace?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 考虑数据点 \(\boldsymbol{\alpha}_1 = (-1,1)\) 和 \(\boldsymbol{\alpha}_2
    = (1,-1)\)。对于 \(k=1\)，最佳近似子空间的解是什么？'
- en: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
- en: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
- en: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x + 1\}\)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x + 1\}\)'
- en: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x - 1\}\)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x - 1\}\)'
- en: e) None of the above
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: e) 以上皆非
- en: \(\checkmark\)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: The first observation gives a related, useful characterization of the optimal
    solution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个观察结果给出了一个与最优解相关的有用特征。
- en: '**LEMMA** **(Best Subspace as Maximimization)** \(\idx{best subspace as maximimization
    lemma}\xdi\) Let \(\boldsymbol{\alpha}_i\), \(i =1\ldots,n\), be vectors in \(\mathbb{R}^m\).
    A linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that minimizes'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（最佳子空间作为最大化）** \(\idx{best subspace as maximization lemma}\xdi\) 设
    \(\boldsymbol{\alpha}_i\), \(i =1\ldots,n\)，是 \(\mathbb{R}^m\) 中的向量。一个 \(\mathbb{R}^m\)
    的线性子空间 \(\mathcal{Z}\)，它最小化'
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
- en: over all linear subspaces of dimension at most \(k\) also maximizes
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有维度不超过 \(k\) 的线性子空间中也是最大化的
- en: \[ \sum_{i=1}^n \|\mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2 \]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2 \]
- en: over the same linear subspaces. And vice versa. \(\flat\)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的线性子空间上。反之亦然。 \(\flat\)
- en: '*Proof idea:* This is a straightforward application of the triangle inequality.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 这是对三角不等式的一个直接应用。'
- en: '*Proof:* By *Pythagoras’ Theorem*,'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过**毕达哥拉斯定理**，'
- en: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    + \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2 = \|\boldsymbol{\alpha}_i\|^2
    \]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    + \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2 = \|\boldsymbol{\alpha}_i\|^2
    \]
- en: since, by the *Orthogonal Projection Theorem*, \(\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\)
    is orthogonal to \(\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\).
    Rearranging,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为，根据 *正交投影定理*，\(\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\) 与 \(\boldsymbol{\alpha}_i
    - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\) 正交。重新排列，
- en: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2.
    \]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2.
    \]
- en: The result follows from the fact that the first term on the right-hand side
    does not depend on the choice of \(\mathcal{Z}\). More specifically, optimizing
    over linear subspaces \(\mathcal{Z}\) of dimension \(k\),
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 结果源于右侧第一个项不依赖于 \(\mathcal{Z}\) 的选择。更具体地说，在维度为 \(k\) 的线性子空间 \(\mathcal{Z}\) 上进行优化，
- en: \[\begin{align*} \min_{\mathcal{Z}} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \min_{\mathcal{Z}} \sum_{i=1}^n \left\{\|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 + \min_{\mathcal{Z}} \left\{- \sum_{i=1}^n\|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \max_{\mathcal{Z}} \sum_{i=1}^n
    \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2. \end{align*}\]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \min_{\mathcal{Z}} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \min_{\mathcal{Z}} \sum_{i=1}^n \left\{\|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 + \min_{\mathcal{Z}} \left\{- \sum_{i=1}^n\|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \max_{\mathcal{Z}} \sum_{i=1}^n
    \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2. \end{align*}\]
- en: \(\square\)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: How do we specify a \(k\)-dimensional linear subspace? Through a basis of it,
    or – even better – an orthonormal basis. In the latter case, we also have an explicit
    formula for the orthogonal projection. And the dimension of the linear subspace
    is captured by the number of elements in the basis, by the *Dimension Theorem*.
    In other words, the best approximating subspace can be obtained by solving the
    problem
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何指定一个 \(k\) 维线性子空间？通过它的基，或者——甚至更好——一个正交基。在后一种情况下，我们也有一个正交投影的显式公式。线性子空间的维度由基的元素数量通过
    *维度定理* 来捕捉。换句话说，最佳逼近子空间可以通过求解以下问题获得
- en: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{i=1}^n \left\|\sum_{j=1}^k
    \langle \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 \]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{i=1}^n \left\|\sum_{j=1}^k
    \langle \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 \]
- en: over all orthonormal lists \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) of length \(k\).
    Our next observation rewrites the problem in matrix form.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有长度为 \(k\) 的正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) 上。我们的下一个观察将问题重新写成矩阵形式。
- en: '**LEMMA** **(Best Subpace in Matrix Form)** \(\idx{best subpace in matrix form
    lemma}\xdi\) Consider the matrix \(A \in \mathbb{R}^{n \times m}\) with rows \(\boldsymbol{\alpha}_i^T\).
    A solution to the best approximating subspace problem is obtained by solving'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(矩阵形式下的最佳子空间)** \(\idx{best subpace in matrix form lemma}\xdi\) 考虑矩阵
    \(A \in \mathbb{R}^{n \times m}\) 的行 \(\boldsymbol{\alpha}_i^T\)。通过求解以下问题获得最佳逼近子空间问题的解'
- en: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \]
- en: over all orthonormal lists \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) of length \(k\).
    \(\flat\)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有长度为 \(k\) 的正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) 上。
- en: '*Proof idea:* We start with the one-dimensional case. A one-dimensional space
    \(\mathcal{Z}\) is determined by a unit vector \(\mathbf{w}_1\). The projection
    \(\boldsymbol{\alpha}_i\) onto the span of \(\mathbf{w}_1\) is given by the inner
    product formula \(\langle \boldsymbol{\alpha}_i, \mathbf{w}_1 \rangle \,\mathbf{w}_1\).
    So'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们从一维情况开始。一维空间 \(\mathcal{Z}\) 由一个单位向量 \(\mathbf{w}_1\) 确定。\(\boldsymbol{\alpha}_i\)
    投影到 \(\mathbf{w}_1\) 的张成空间由内积公式 \(\langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle \,\mathbf{w}_1\) 给出。因此'
- en: \[\begin{align*} \sum_{i=1}^n \|\langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle \,\mathbf{w}_1 \|^2 &= \sum_{i=1}^n \langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle ^2\\ &= \sum_{i=1}^n (\boldsymbol{\alpha}_i^T \mathbf{w}_1)^2\\ &= \|A
    \mathbf{w}_1\|^2 \end{align*}\]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \|\langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle \,\mathbf{w}_1 \|^2 &= \sum_{i=1}^n \langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle ^2\\ &= \sum_{i=1}^n (\boldsymbol{\alpha}_i^T \mathbf{w}_1)^2\\ &= \|A
    \mathbf{w}_1\|^2 \end{align*}\]
- en: where, again, \(A\) is the matrix with rows \(\boldsymbol{\alpha}_i ^T\), \(i=1,\ldots,
    n\). Hence the solution to the one-dimensional problem is
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，再次强调，\(A\) 是具有行 \(\boldsymbol{\alpha}_i ^T\) 的矩阵，\(i=1,\ldots, n\)。因此，一维问题的解是
- en: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
- en: 'Here \(\arg\max\) means that \(\mathbf{v}_1\) is a vector \(\mathbf{w}_1\)
    that achieves the maximum. Note that there could be more than one such \(\mathbf{w}_1\),
    so the right-hand side is a set containing all such solutions. By the *Extreme
    Value Theorem* (since the set \(\{\mathbf{w}_1 : \|\mathbf{w}_1\| = 1\}\) is closed
    and bounded, and since furthermore the function \(\|A \mathbf{w}_1\|^2\) is continuous
    in \(\mathbf{w}_1\)), there is at least one solution.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '这里 \(\arg\max\) 意味着 \(\mathbf{v}_1\) 是一个向量 \(\mathbf{w}_1\)，它实现了最大值。请注意，可能存在多个这样的
    \(\mathbf{w}_1\)，因此右侧是一个包含所有这些解的集合。根据*极值定理*（因为集合 \(\{\mathbf{w}_1 : \|\mathbf{w}_1\|
    = 1\}\) 是闭集且有界，并且此外函数 \(\|A \mathbf{w}_1\|^2\) 在 \(\mathbf{w}_1\) 上是连续的），至少存在一个解。'
- en: '*Proof:* For general \(k\), we are looking for an orthonormal list \(\mathbf{w}_1,\ldots,\mathbf{w}_k\)
    of length \(k\) that maximizes'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*：对于一般的 \(k\)，我们正在寻找一个长度为 \(k\) 的正交列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\)，它最大化'
- en: \[\begin{align*} \sum_{i=1}^n \left\|\sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 &= \sum_{i=1}^n \sum_{j=1}^k \langle
    \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle ^2\\ &= \sum_{j=1}^k \left(\sum_{i=1}^n
    (\boldsymbol{\alpha}_i^T \mathbf{w}_j)^2\right)\\ &= \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \end{align*}\]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \left\|\sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 &= \sum_{i=1}^n \sum_{j=1}^k \langle
    \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle ^2\\ &= \sum_{j=1}^k \left(\sum_{i=1}^n
    (\boldsymbol{\alpha}_i^T \mathbf{w}_j)^2\right)\\ &= \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \end{align*}\]
- en: where \(\mathcal{Z}\) is the subspace spanned by \(\mathbf{w}_1,\ldots,\mathbf{w}_k\).
    On the second line, we used the *Properties of Orthonormal Lists*. That proves
    the claim. \(\square\)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathcal{Z}\) 是由 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) 张成的子空间。在第二行，我们使用了*正交列表的性质*。这证明了我们的主张。
    \(\square\)
- en: We show next that a simple algorithm solves this problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将展示一个简单的算法可以解决这个问题。
- en: '**A greedy algorithm** \(\idx{greedy algorithm}\xdi\) Remarkably, the problem
    admits a greedy solution. Before discussing this solution, we take a small detour
    and give a classical example. Indeed, [greedy approaches](https://en.wikipedia.org/wiki/Greedy_algorithm)
    are a standard algorithmic tool for optimization problems. This is how Wikipedia
    describes them:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪心算法** \(\idx{greedy algorithm}\xdi\) 非常明显，这个问题接受一个贪心解。在讨论这个解之前，我们先稍微偏离一下，给出一个经典例子。实际上，[贪心方法](https://en.wikipedia.org/wiki/Greedy_algorithm)是优化问题的一个标准算法工具。维基百科这样描述它们：'
- en: A greedy algorithm is any algorithm that follows the problem-solving heuristic
    of making the locally optimal choice at each stage. In many problems, a greedy
    strategy does not produce an optimal solution, but a greedy heuristic can yield
    locally optimal solutions that approximate a globally optimal solution in a reasonable
    amount of time.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 贪心算法是指任何在每个阶段都遵循局部最优解问题求解启发式的算法。在许多问题中，贪心策略不会产生最优解，但贪心启发式可以产生局部最优解，这些解在合理的时间内近似全局最优解。
- en: '**Figure:** A thief in an antique shop (*Credit:* Made with Gemini; and here
    is your reminder that AI generation of images still has a long way to go…)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**图**：古董店中的小偷（*来源：Gemini制作；这里也提醒您，AI生成图像还有很长的路要走…）'
- en: '![A thief in an antique shop](../Images/523187bb9f3dd58102ed1bcdc7aa179f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![古董店中的小偷](../Images/523187bb9f3dd58102ed1bcdc7aa179f.png)'
- en: \(\bowtie\)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**EXAMPLE:** Suppose you are thief and you broke into an antique shop at night.
    (*Legal disclaimer:* The greedy algorithm should be applied to legitimate resource
    allocation problems only.) You cannot steal every item in the store. You have
    estimated that you can carry 10 lbs worth of merchandise, and still run fast enough
    to get away. Suppose that there are 4 items of interest with the following weights
    and values'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：假设你是一名小偷，在夜间闯入了一家古董店。（*法律声明：* 贪心算法仅应用于合法的资源分配问题。）你不能偷走店里所有的物品。你估计你可以携带价值10磅的货物，并且还能跑得足够快以逃脱。假设有4件感兴趣的物品，其重量和价值如下'
- en: '| Item | Weight (lbs) | Value ($) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 物品 | 重量（磅） | 价值（美元） |'
- en: '| --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 8 | 1600 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8 | 1600 |'
- en: '| 2 | 6 | 1100 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 6 | 1100 |'
- en: '| 3 | 4 | 700 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 700 |'
- en: '| 4 | 1 | 100 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 100 |'
- en: There is exactly one of each item. Which items do you take? The siren is blaring,
    and you cannot try every combination. A quick scheme is to first pick the item
    of greatest value, i.e., Item 1\. Now your bag has 8 lbs of merchandise in it.
    Then you consider the remaining items and choose whichever has highest value among
    those that still fit, i.e., those that are 2 lbs or lighter. That leaves only
    one choice, Item 4\. Then you go – with a total value of 1700.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每种物品恰好只有一个。你将选择哪些物品？警报声正在响起，你无法尝试每一种组合。一个快速的方法是首先选择价值最高的物品，即物品1。现在你的包里已经有8磅的商品了。然后，考虑剩余的物品，并选择其中价值最高的一个，即那些重量为2磅或更轻的物品。这仅剩下一种选择，即物品4。然后你继续选择——总价值为1700。
- en: 'This is called a greedy or myopic strategy, because you chose the first item
    to maximize your profit without worrying about the constraints it imposes on future
    choice. Indeed, in this case, there is a better combination: you could have picked
    Items 2 and 3 with a total value of 1800.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为贪婪或短视策略，因为你选择了第一个物品以最大化你的利润，而不考虑它对未来选择的约束。实际上，在这种情况下，有一个更好的组合：你可以选择物品2和3，总价值为1800。
- en: Other greedy schemes are possible here. A slightly more clever approach is to
    choose items of high *value per unit weight*, rather than considering value alone.
    But, that would not make a difference in this particular example (Try it!). \(\lhd\)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可能有其他的贪婪方案。一个稍微聪明一点的方法是选择单位重量价值高的物品，而不仅仅是考虑价值。但在这种特定例子中，这并不会产生影响（试试看！）\(\lhd\)
- en: Going back to the approximating subspace problem, we derive a greedy solution
    for it. Recall that we are looking for a solution to
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 回到逼近子空间问题，我们为它推导出一个贪婪解。回想一下，我们正在寻找一个解决方案来
- en: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2
    + \cdots + \|A \mathbf{w}_k\|^2 \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2
    + \cdots + \|A \mathbf{w}_k\|^2 \]
- en: over all orthonormal lists \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) of length \(k\).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有长度为\(k\)的正交归一列表\(\mathbf{w}_1,\ldots,\mathbf{w}_k\)中。
- en: In a greedy approach, we first solve for \(\mathbf{w}_1\) by itself, without
    worrying about constraints it will impose on the next steps. That is, we compute
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在贪婪方法中，我们首先单独求解\(\mathbf{w}_1\)，而不考虑它将对下一步施加的约束。也就是说，我们计算
- en: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
- en: As indicated before, by the *Extreme Value Theorem*, such a \(\mathbf{v}_1\)
    exists, but may not be unique (in which case we pick an arbitrary one). Then,
    fixing \(\mathbf{w}_1 = \mathbf{v}_1\), we consider all unit vectors \(\mathbf{w}_2\)
    orthogonal to \(\mathbf{v}_1\) and maximize the contribution of \(\mathbf{w}_2\)
    to the objective function. That is, we solve
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，根据*极值定理*，这样的\(\mathbf{v}_1\)存在，但可能不是唯一的（在这种情况下，我们可以选择任意一个）。然后，固定\(\mathbf{w}_1
    = \mathbf{v}_1\)，我们考虑所有与\(\mathbf{v}_1\)正交的单位向量\(\mathbf{w}_2\)，并最大化\(\mathbf{w}_2\)对目标函数的贡献。也就是说，我们求解
- en: \[ \mathbf{v}_2\in \arg\max \{\|A \mathbf{w}_2\|^2 :\|\mathbf{w}_2\| = 1,\ \langle
    \mathbf{w}_2, \mathbf{v}_1 \rangle = 0\}. \]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_2\in \arg\max \{\|A \mathbf{w}_2\|^2 :\|\mathbf{w}_2\| = 1,\ \langle
    \mathbf{w}_2, \mathbf{v}_1 \rangle = 0\}. \]
- en: Again, such a \(\mathbf{v}_2\) exists by the *Extreme Value Theorem*. Then proceeding
    by induction, for each \(i = 3, \ldots, k\), we compute
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，根据*极值定理*，这样的\(\mathbf{v}_2\)存在。然后通过归纳法，对于每个\(i = 3, \ldots, k\)，我们计算
- en: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \
    \langle \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}. \]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \
    \langle \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}. \]
- en: A different way to write the constraint is
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种写约束的方法是
- en: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \mathbf{w}_i
    \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathrm{v}_{i-1})^\perp\}. \]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \mathbf{w}_i
    \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathrm{v}_{i-1})^\perp\}. \]
- en: While it is clear that, after \(k\) steps, this procedure constructs an orthonormal
    set of size \(k\), it is far from obvious that it maximizes \(\sum_{j=1}^k \|A
    \mathbf{v}_j\|^2\) over all such sets. Remarkably it does. The claim – which requires
    a proof – is that the best \(k\)-dimensional approximating subspace is obtained
    by finding the best \(1\)-dimensional subspace, then the best \(1\)-dimensional
    subspace orthogonal to the first one, and so on. This follows from the next theorem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然，经过 \(k\) 步后，这个程序构建了一个大小为 \(k\) 的正交归一集，但它远非显而易见的是它最大化了 \(\sum_{j=1}^k \|A
    \mathbf{v}_j\|^2\) 在所有这样的集合上。令人惊讶的是它确实做到了。这个断言——需要证明——是，最佳的 \(k\) 维逼近子空间是通过找到最佳的
    \(1\) 维子空间，然后是第一个子空间正交的 \(1\) 维子空间，以此类推来获得的。这可以从下一个定理中得出。
- en: '**THEOREM** **(Greedy Finds Best Subspace)** \(\idx{greedy finds best subspace
    theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with rows \(\boldsymbol{\alpha}_i^T\),
    \(i=1,\ldots,n\). For any \(k \leq m\), let \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    be a greedy sequence as constructed above. Then \(\mathcal{Z}^* = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\)
    is a solution to the minimization problem'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（贪婪寻找最佳子空间）** \(\idx{greedy finds best subspace theorem}\xdi\) 设 \(A
    \in \mathbb{R}^{n \times m}\) 是一个矩阵，其行是 \(\boldsymbol{\alpha}_i^T\)，\(i=1,\ldots,n\)。对于任何
    \(k \leq m\)，设 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 是如上所述构建的贪婪序列。那么 \(\mathcal{Z}^*
    = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) 是最小化问题的解'
- en: \[ \min \left\{ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\
    :\ \text{$\mathcal{Z}$ is a linear subspace of dimension $k$} \right\}. \]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min \left\{ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\
    :\ \text{$\mathcal{Z}$ 是一个维度为 $k$ 的线性子空间} \right\}. \]
- en: \(\sharp\)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Beyond the potential computational advantage of solving several lower-dimensional
    problems rather one larger-dimensional one, a greedy sequence has a more subtle
    property that is powerful. It allows us to solve the problem for all choices \(k\)
    of target dimension *simultaneously*. To explain, note that the largest \(k\)
    value, i.e. \(k=m\), leads to a trivial problem. Indeed, the data points \(\boldsymbol{\alpha}_i\),
    \(i=1,\ldots,n\), already lie in an \(m\)-dimensional linear subspace, \(\mathbb{R}^m\)
    itself. So we can take \(\mathcal{Z} = \mathbb{R}^m\), and we have an objective
    value of
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决多个低维问题而不是一个高维问题的潜在计算优势之外，贪婪序列还有一个更微妙但强大的特性。它允许我们同时解决所有目标维度 \(k\) 的选择的问题。为了解释这一点，请注意，最大的
    \(k\) 值，即 \(k=m\)，导致了一个平凡的问题。确实，数据点 \(\boldsymbol{\alpha}_i\)，\(i=1,\ldots,n\)，已经位于一个
    \(m\) 维线性子空间中，即 \(\mathbb{R}^m\) 本身。因此，我们可以取 \(\mathcal{Z} = \mathbb{R}^m\)，并且我们有一个目标值
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \boldsymbol{\alpha}_i\|^2 = 0, \]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \boldsymbol{\alpha}_i\|^2 = 0, \]
- en: which clearly cannot be improved. So any orthonormal basis of \(\mathbb{R}^m\)
    will do. Say \(\mathbf{e}_1,\ldots,\mathbf{e}_m\).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然无法改进。所以任何 \(\mathbb{R}^m\) 的正交基都将适用。比如说 \(\mathbf{e}_1,\ldots,\mathbf{e}_m\)。
- en: On the other hand, a greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_m\) has
    a very special property. For any \(k \leq m\), the *truncation* \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    solves the approximating subspace problem in \(k\) dimensions. That follows immediately
    from the *Greedy Finds Best Subspace Theorem*. The basis \(\mathbf{e}_1,\ldots,\mathbf{e}_m\)
    (or any old basis of \(\mathbb{R}^m\) for that matter) does *not* have this property.
    The idea of truncation is very useful and plays an important role in many data
    science applications; we will come back to it later in this section and the next
    one.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，贪婪序列 \(\mathbf{v}_1,\ldots,\mathbf{v}_m\) 有一个非常特殊的性质。对于任何 \(k \leq m\)，截断
    \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 解决了 \(k\) 维逼近子空间问题。这直接来自 *Greedy Finds Best
    Subspace Theorem*。基 \(\mathbf{e}_1,\ldots,\mathbf{e}_m\)（或者任何旧的 \(\mathbb{R}^m\)
    的基）没有这个性质。截断的想法非常有用，在许多数据科学应用中起着重要作用；我们将在本节和下一节中稍后回到它。
- en: We sketch the proof of a weaker claim via the *Spectral Theorem*, an approach
    which reveals additional structure in the solution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 *谱定理* 简要地描述了一个较弱断言的证明，这种方法揭示了解决方案中的额外结构。
- en: We re-write the objective function as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将目标函数重新写为
- en: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T A^T A \mathbf{w}_j
    \]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T A^T A \mathbf{w}_j
    \]
- en: and we observe that \(A^T A \in \mathbb{R}^{m \times m}\) is a square, symmetric
    matrix (Why?). It is also positive, semidefinite (Why?). Hence, by the *Spectral
    Theorem*, the matrix \(A^T A\) has \(m\) orthonormal eigenvectors \(\mathbf{q}_1,
    \ldots, \mathbf{q}_m \in \mathbb{R}^m\) with corresponding real eigenvalues \(\lambda_1
    \geq \lambda_2 \geq \cdots \geq \lambda_m \geq 0\). This ordering of the eigenvalues
    will play a critical role. Moreover
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到 \(A^T A \in \mathbb{R}^{m \times m}\) 是一个方阵，对称矩阵（为什么？）。它也是正定半定的（为什么？）。因此，根据**谱定理**，矩阵
    \(A^T A\) 有 \(m\) 个正交归一特征向量 \(\mathbf{q}_1, \ldots, \mathbf{q}_m \in \mathbb{R}^m\)，对应于实特征值
    \(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_m \geq 0\)。这种特征值的排序将发挥关键作用。此外
- en: \[ A^T A = \sum_{i=1}^m \lambda_i \mathbf{q}_i \mathbf{q}_i^T. \]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A = \sum_{i=1}^m \lambda_i \mathbf{q}_i \mathbf{q}_i^T. \]
- en: Plugging this in the objective we get
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将其代入目标函数中，我们得到
- en: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T \left(\sum_{i=1}^m
    \lambda_i \mathbf{q}_i \mathbf{q}_i^T\right) \mathbf{w}_j = \sum_{j=1}^k \sum_{i=1}^m
    \lambda_i (\mathbf{w}_j^T\mathbf{q}_i)^2. \]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T \left(\sum_{i=1}^m
    \lambda_i \mathbf{q}_i \mathbf{q}_i^T\right) \mathbf{w}_j = \sum_{j=1}^k \sum_{i=1}^m
    \lambda_i (\mathbf{w}_j^T\mathbf{q}_i)^2. \]
- en: Here is the claim. While a greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    is not in general unique, one can always choose \(\mathbf{v}_i = \mathbf{q}_i\)
    for all \(i\). Moreover that particular choice indeed solves the \(k\)-dimensional
    best approximating subspace problem. We restrict ourselves to the case \(k = 2\).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们要证明的。虽然贪婪序列 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 在一般情况下不是唯一的，但我们可以总是选择
    \(\mathbf{v}_i = \mathbf{q}_i\) 对于所有的 \(i\)。此外，这种特定的选择确实解决了 \(k\) 维最佳逼近子空间问题。我们限制自己考虑
    \(k = 2\) 的情况。
- en: '***Eigenvectors form a greedy sequence:*** Recall that \(\mathbf{v}_1\) maximizes
    \(\|A \mathbf{w}_1\|\) over all unit vectors \(\mathbf{w}_1\). Now note that,
    expanding over the eigenvectors (which form an orthonormal basis), we have'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '***特征向量形成贪婪序列：*** 回想一下，\(\mathbf{v}_1\) 在所有单位向量 \(\mathbf{w}_1\) 中最大化 \(\|A
    \mathbf{w}_1\|\)。现在注意，在特征向量（它们形成一个正交基）上展开，我们有'
- en: \[ \|A \mathbf{w}_1\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}_1\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    \]
- en: and
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
- en: Writing \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\), this boils down to maximizing
    \(\sum_{i=1}^m \lambda_i x_i\) subject to the constraints \(\sum_{i=1}^m x_i =
    1\) and \(x_i \geq 0\) for all \(i\). But, under the constraints and the assumption
    on the ordering of the eigenvalues,
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 写作 \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\)，这归结为在约束 \(\sum_{i=1}^m x_i = 1\)
    和 \(x_i \geq 0\) 对于所有 \(i\) 的条件下，最大化 \(\sum_{i=1}^m \lambda_i x_i\)。但是，在约束和特征值排序的假设下，
- en: \[ \sum_{i=1}^m \lambda_i x_i \leq \lambda_1 \sum_{i=1}^m x_i = \lambda_1. \]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^m \lambda_i x_i \leq \lambda_1 \sum_{i=1}^m x_i = \lambda_1. \]
- en: Formally, we have shown that \(\|A \mathbf{w}_1\|^2 \leq \lambda_1\), for any
    unit vector \(\mathbf{w}_1\). Now, note that this upper bound is actually achieved
    by taking \(\mathbf{v}_1 = \mathbf{w}_1 = \mathbf{q}_1\), which corresponds to
    \(\mathbf{x} = (x_1,\ldots,x_m) = \mathbf{e}_1\).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们已经证明了对于任何单位向量 \(\mathbf{w}_1\)，\(\|A \mathbf{w}_1\|^2 \leq \lambda_1\)。现在，注意这个上界实际上是通过取
    \(\mathbf{v}_1 = \mathbf{w}_1 = \mathbf{q}_1\) 来实现的，这对应于 \(\mathbf{x} = (x_1,\ldots,x_m)
    = \mathbf{e}_1\)。
- en: Given that choice, the vector \(\mathbf{v}_2\) maximizes \(\|A \mathbf{w}_2\|\)
    over all unit vectors \(\mathbf{w}_2\) such that further \(\mathbf{w}_2^T\mathbf{v}_1
    = \mathbf{w}_2^T\mathbf{q}_1 = 0\), where this time
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定选择的情况下，向量 \(\mathbf{v}_2\) 在所有单位向量 \(\mathbf{w}_2\) 中最大化 \(\|A \mathbf{w}_2\|\)，且满足
    \(\mathbf{w}_2^T\mathbf{v}_1 = \mathbf{w}_2^T\mathbf{q}_1 = 0\)，这次
- en: \[ \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2
    = \sum_{i=2}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2
    = \sum_{i=2}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
- en: and
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = \sum_{i=2}^m
    (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = \sum_{i=2}^m
    (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
- en: In both equations above, we used the orthogonality constraint. This reduces
    to the previous problem without the term depending on \(\mathbf{q}_1\). The solution
    is otherwise the same, i.e., the optimal objective is \(\lambda_2\) and is achieved
    by taking \(\mathbf{v}_2 = \mathbf{w}_2 = \mathbf{q}_2\).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两个方程中，我们使用了正交约束。这简化为没有依赖于 \(\mathbf{q}_1\) 的项的先前问题。否则，解是相同的，即最优目标函数是 \(\lambda_2\)，通过取
    \(\mathbf{v}_2 = \mathbf{w}_2 = \mathbf{q}_2\) 来实现。
- en: '***Eigenvectors solve the approximating subspace problem:*** The approximating
    subspace problem for \(k = 2\) involves maximizing'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '***特征向量解决了逼近子空间问题：*** 当 \(k = 2\) 时，逼近子空间问题涉及最大化'
- en: \[ \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    + \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    + \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
- en: over orthonormal lists \(\mathbf{w}_1, \mathbf{w}_2\). In particular, we require
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在正交归一列表 \(\mathbf{w}_1, \mathbf{w}_2\) 上。特别是，我们要求
- en: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
- en: and
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
- en: Moreover, for each \(i\), by definition of the orthogonal projection on the
    subspace \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1, \mathbf{w}_2)\) and the *Properties
    of Orhtonormal Lists*
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于每个 \(i\)，根据子空间 \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1, \mathbf{w}_2)\)
    上的正交投影的定义和 *正交归一列表的性质*
- en: \[ (\mathbf{w}_1^T\mathbf{q}_i)^2 + (\mathbf{w}_2^T\mathbf{q}_i)^2 = \|\mathrm{proj}_{\mathcal{W}}
    \mathbf{q}_i\|^2 \leq \|\mathbf{q}_i\|^2 = 1. \]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{w}_1^T\mathbf{q}_i)^2 + (\mathbf{w}_2^T\mathbf{q}_i)^2 = \|\mathrm{proj}_{\mathcal{W}}
    \mathbf{q}_i\|^2 \leq \|\mathbf{q}_i\|^2 = 1. \]
- en: (Prove the inequality!) Write \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\) and \(y_i
    = (\mathbf{w}_2^T\mathbf{q}_i)^2\). The objective function can be written as \(\sum_{i=1}^m
    \lambda_i (x_i + y_i)\) and the constraints we have derived are \(\sum_{i=1}^m
    x_i = \sum_{i=1}^m y_i = 1\) and \(x_i + y_i \leq 1\) for all \(i\). Also clearly
    \(x_i, y_i \geq 0\) for all \(i\). So
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: （证明不等式！）设 \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\) 和 \(y_i = (\mathbf{w}_2^T\mathbf{q}_i)^2\)。目标函数可以写成
    \(\sum_{i=1}^m \lambda_i (x_i + y_i)\)，而我们推导出的约束条件是 \(\sum_{i=1}^m x_i = \sum_{i=1}^m
    y_i = 1\) 和 \(x_i + y_i \leq 1\) 对于所有 \(i\) 都成立。显然，对于所有 \(i\)，有 \(x_i, y_i \geq
    0\)。因此
- en: \[\begin{align*} \sum_{i=1}^m \lambda_i (x_i + y_i) &= \lambda_1 (x_1 + y_1)
    + \lambda_2 (x_2 + y_2) + \sum_{i=3}^m \lambda_i (x_i + y_i)\\ &\leq \lambda_1
    (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \sum_{i=3}^m (x_i + y_i)\\ &=
    \lambda_1 (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \left([1 - x_1 - x_2]
    + [1 - y_1 - y_2]\right)\\ &= (\lambda_1 - \lambda_2) (x_1 + y_1) + (\lambda_2
    - \lambda_2) (x_2 + y_2) + 2 \lambda_2\\ &\leq \lambda_1 - \lambda_2 + 2 \lambda_2\\
    &= \lambda_1 + \lambda_2. \end{align*}\]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^m \lambda_i (x_i + y_i) &= \lambda_1 (x_1 + y_1)
    + \lambda_2 (x_2 + y_2) + \sum_{i=3}^m \lambda_i (x_i + y_i)\\ &\leq \lambda_1
    (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \sum_{i=3}^m (x_i + y_i)\\ &=
    \lambda_1 (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \left([1 - x_1 - x_2]
    + [1 - y_1 - y_2]\right)\\ &= (\lambda_1 - \lambda_2) (x_1 + y_1) + (\lambda_2
    - \lambda_2) (x_2 + y_2) + 2 \lambda_2\\ &\leq \lambda_1 - \lambda_2 + 2 \lambda_2\\
    &= \lambda_1 + \lambda_2. \end{align*}\]
- en: Formally, we have shown that \(\|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2 \leq
    \lambda_1 + \lambda_2\) for any orthonormal list \(\mathbf{w}_1, \mathbf{w}_2\).
    That upper bound is achieved by taking \(\mathbf{w}_1 = \mathbf{q}_1\) and \(\mathbf{w}_2
    = \mathbf{q}_2\), proving the claim.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们已经证明了对于任何正交归一列表 \(\mathbf{w}_1, \mathbf{w}_2\)，有 \(\|A \mathbf{w}_1\|^2
    + \|A \mathbf{w}_2\|^2 \leq \lambda_1 + \lambda_2\)。这个上界可以通过取 \(\mathbf{w}_1 =
    \mathbf{q}_1\) 和 \(\mathbf{w}_2 = \mathbf{q}_2\) 来达到，从而证明了该命题。
- en: '**KNOWLEDGE CHECK:** Proceed by induction to show that the claim holds for
    any \(k\). \(\checkmark\)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查：** 通过归纳法证明该命题对于任何 \(k\) 都成立。 \(\checkmark\)'
- en: Note that we have not entirely solved the best approximating subspace problem
    from a computational point of view, as we have not given an explicit procedure
    to construct a solution to the lower-dimensional subproblems, i.e., construct
    the eigenvectors. We have only shown that the solutions exist and have the right
    properties. We will take care of computational issues later in this chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从计算的角度来看，我们并没有完全解决最佳逼近子空间问题，因为我们没有给出一个明确的构造解的步骤来构建低维子问题的解，即构造特征向量。我们只证明了这些解存在并且具有正确的性质。我们将在本章的后面部分处理计算问题。
- en: 4.3.2\. From approximating subspaces to the SVD[#](#from-approximating-subspaces-to-the-svd
    "Link to this heading")
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3.2\. 从逼近子空间到奇异值分解（SVD）[#](#from-approximating-subspaces-to-the-svd "链接到本标题")
- en: While solving the approximating subspace problem in the previous section, we
    derived the building blocks of a matrix factorization that has found many applications,
    the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition).
    In this section, we define the SVD formally. We describe a simple method to compute
    it in the next section, where we also return to the application to dimensionality
    reduction.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节解决逼近子空间问题时，我们推导出了矩阵分解的构建块，这种分解在许多应用中得到了应用，即 [奇异值分解 (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition)。在本节中，我们正式定义SVD。在下一节中，我们将描述计算它的简单方法，同时也会回到降维的应用。
- en: '**Definition and existence of the SVD** We now come to our main definition.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解的定义和存在性** 我们现在来到我们的主要定义。'
- en: '**DEFINITION** **(Singular Value Decomposition)** \(\idx{singular value decomposition}\xdi\)
    Let \(A \in \mathbb{R}^{n\times m}\) be a matrix. A singular value decomposition
    (SVD) of \(A\) is a matrix factorization'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(奇异值分解)** \(\idx{singular value decomposition}\xdi\) 设 \(A \in \mathbb{R}^{n\times
    m}\) 是一个矩阵。\(A\) 的奇异值分解是一个矩阵分解'
- en: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: where the columns of \(U \in \mathbb{R}^{n \times r}\) and those of \(V \in
    \mathbb{R}^{m \times r}\) are orthonormal, and \(\Sigma \in \mathbb{R}^{r \times
    r}\) is a diagonal matrix. Here the \(\mathbf{u}_j\)s are the columns of \(U\)
    and are referred to as left singular vectors\(\idx{singular vector}\xdi\). Similarly
    the \(\mathbf{v}_j\)s are the columns of \(V\) and are referred to as right singular
    vectors. The \(\sigma_j\)s, which are positive and in non-increasing order, i.e.,
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(U \in \mathbb{R}^{n \times r}\) 的列和 \(V \in \mathbb{R}^{m \times r}\) 的列是正交归一的，且
    \(\Sigma \in \mathbb{R}^{r \times r}\) 是一个对角矩阵。这里 \(\mathbf{u}_j\) 是 \(U\) 的列，被称为左奇异向量\(\idx{singular
    vector}\xdi\)。同样地，\(\mathbf{v}_j\) 是 \(V\) 的列，被称为右奇异向量。\(\sigma_j\) 是正数，且按非递增顺序排列，即，
- en: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0, \]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0, \]
- en: are the diagonal elements of \(\Sigma\) and are referred to as singular values\(\idx{singular
    value}\xdi\). \(\natural\)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 是 \(\Sigma\) 的对角元素，被称为奇异值\(\idx{singular value}\xdi\)。 \(\natural\)
- en: To see where the equality \(U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) comes from, we break it up into two steps.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到等式 \(U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)
    的来源，我们将其分解为两个步骤。
- en: First note that the matrix product \(U \Sigma\) has columns \(\sigma_1 \mathbf{u}_1,\ldots,\sigma_r
    \mathbf{u}_r\).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先注意矩阵乘积 \(U \Sigma\) 的列是 \(\sigma_1 \mathbf{u}_1,\ldots,\sigma_r \mathbf{u}_r\).
- en: The rows of \(V^T\) are the columns of \(V\) as row vectors.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(V^T\) 的行是 \(V\) 的列，作为行向量。
- en: In terms of outer products, the matrix product \(U \Sigma V^T = (U \Sigma) V^T\)
    is the sum of the outer products of the columns of \(U \Sigma\) and of the rows
    of \(V^T\) (i.e., the columns of \(V\) as row vectors).
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从外积的角度来看，矩阵乘积 \(U \Sigma V^T = (U \Sigma) V^T\) 是 \(U \Sigma\) 的列和外积以及 \(V^T\)
    的行（即 \(V\) 的列作为行向量）的外积之和。
- en: That proves the equality.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这就证明了等式。
- en: '**KNOWLEDGE CHECK:** Let \(A \in \mathbb{R}^{n\times m}\) be a matrix with
    SVD'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 设 \(A \in \mathbb{R}^{n\times m}\) 是一个具有奇异值分解的矩阵'
- en: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T. \]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T. \]
- en: Which statement is true in general?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个陈述在一般情况下是正确的？
- en: a) \(\mathrm{col}(A) = \mathrm{col}(V)\)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathrm{col}(A) = \mathrm{col}(V)\)
- en: b) \(\mathrm{col}(A) = \mathrm{col}(V^T)\)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathrm{col}(A) = \mathrm{col}(V^T)\)
- en: c) \(\mathrm{col}(A) = \mathrm{col}(U)\)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathrm{col}(A) = \mathrm{col}(U)\)
- en: d) \(\mathrm{col}(A) = \mathrm{col}(U^T)\)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathrm{col}(A) = \mathrm{col}(U^T)\)
- en: e) \(\mathrm{col}(A) = \mathrm{col}(\Sigma)\)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: e) \(\mathrm{col}(A) = \mathrm{col}(\Sigma)\)
- en: \(\checkmark\)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: Remarkably, any matrix has an SVD.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，任何矩阵都有一个SVD。
- en: '**THEOREM** **(Existence of an SVD)** \(\idx{existence of an SVD}\xdi\) Any
    matrix \(A \in \mathbb{R}^{n\times m}\) has a singular value decomposition. \(\sharp\)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(SVD的存在性)** \(\idx{existence of an SVD}\xdi\) 任何矩阵 \(A \in \mathbb{R}^{n\times
    m}\) 都有一个奇异值分解。 \(\sharp\)'
- en: We give a proof via the *Spectral Theorem*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 *谱定理* 给出证明。
- en: '*The construction:* Let \(A \in \mathbb{R}^{n \times m}\) and recall that \(A^T
    A\) is symmetric and positive semidefinite. Hence the latter has a spectral decomposition'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*构造方法:* 设 \(A \in \mathbb{R}^{n \times m}\) 并回忆 \(A^T A\) 是对称和正半定的。因此，后者有一个谱分解'
- en: \[ A^T A = Q \Lambda Q^T. \]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A = Q \Lambda Q^T. \]
- en: Order the eigenvalues in non-increasing order \(\lambda_1 \geq \cdots \geq \lambda_m
    \geq 0\). Assume that the eigenvalues \(\lambda_1,\ldots,\lambda_r\) are nonzero
    while \(\lambda_{r+1} = \cdots = \lambda_m = 0\). Let \(\mathbf{q}_1,\ldots,\mathbf{q}_n\)
    be corresponding eigenvectors. Let \(Q_1 \in \mathbb{R}^{m \times r}\) be the
    matrix whose columns are \(\mathbf{q}_1,\ldots,\mathbf{q}_r\) and \(\Lambda_1
    \in \mathbb{R}^{r \times r}\) be the diagonal matrix with \(\lambda_1,\ldots,\lambda_r\)
    on its diagonal. Similarly, let \(Q_2 \in \mathbb{R}^{m \times (m-r)}\) be the
    matrix whose columns are \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\) and \(\Lambda_2
    = \mathbf{0} \in \mathbb{R}^{(m-r) \times (m-r)}\).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征值按非递减顺序排列 \(\lambda_1 \geq \cdots \geq \lambda_m \geq 0\)。假设特征值 \(\lambda_1,\ldots,\lambda_r\)
    是非零的，而 \(\lambda_{r+1} = \cdots = \lambda_m = 0\)。设 \(\mathbf{q}_1,\ldots,\mathbf{q}_n\)
    是相应的特征向量。设 \(Q_1 \in \mathbb{R}^{m \times r}\) 是列向量分别为 \(\mathbf{q}_1,\ldots,\mathbf{q}_r\)
    的矩阵，且 \(\Lambda_1 \in \mathbb{R}^{r \times r}\) 是对角矩阵，其对角线上的元素为 \(\lambda_1,\ldots,\lambda_r\)。类似地，设
    \(Q_2 \in \mathbb{R}^{m \times (m-r)}\) 是列向量分别为 \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\)
    的矩阵，且 \(\Lambda_2 = \mathbf{0} \in \mathbb{R}^{(m-r) \times (m-r)}\).
- en: The matrix \(A^T A\), which is comprised of all inner products of the data points,
    is known as a Gram matrix.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由所有数据点的内积组成的矩阵 \(A^T A\) 被称为 Gram 矩阵。
- en: We are now ready for our main claim. For a diagonal matrix \(D\) with nonnegative
    diagonal entries, we let \(D^{1/2}\) denote the diagonal matrix obtained by taking
    the square root of each diagonal entry. Similarly, when \(D\) has positive diagonal
    entries, we define \(D^{-1/2}\) as the diagonal matrix whose diagonal entries
    are the reciprocals of the square roots of the corresponding diagonal entries
    of \(D\).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备提出我们的主要论断。对于一个对角矩阵 \(D\)，其对角线上的元素非负，我们让 \(D^{1/2}\) 表示通过对每个对角线元素开平方得到的对角矩阵。类似地，当
    \(D\) 的对角线元素为正时，我们定义 \(D^{-1/2}\) 为对角线元素为 \(D\) 的对应对角线元素的平方根的倒数的对角矩阵。
- en: '**THEOREM** **(SVD via Spectral Decomposition)** \(\idx{SVD via spectral decomposition}\xdi\)
    Let \(A \in \mathbb{R}^{n \times m}\) and let \(Q_1, \Lambda_1\) be as above.
    Define'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(通过谱分解的奇异值分解)** \(\idx{SVD via spectral decomposition}\xdi\) 设 \(A
    \in \mathbb{R}^{n \times m}\) 且 \(Q_1, \Lambda_1\) 如上所述。定义'
- en: \[ U = A Q_1 \Lambda_1^{-1/2} \quad \text{and} \quad \Sigma = \Lambda_1^{1/2}
    \quad \text{and} \quad V = Q_1. \]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U = A Q_1 \Lambda_1^{-1/2} \quad \text{和} \quad \Sigma = \Lambda_1^{1/2}
    \quad \text{和} \quad V = Q_1. \]
- en: Then \(A = U \Sigma V^T\) is a singular value decomposition of \(A\). \(\sharp\)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 \(A = U \Sigma V^T\) 是 \(A\) 的奇异值分解。 \(\sharp\)
- en: '*Proof idea:* Check by hand that all properties of the SVD are satisfied by
    the construction above.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 通过手工检查上述构造满足 SVD 的所有性质。'
- en: '*Proof:* By construction, the columns of \(V = Q_1\) are orthonormal. The matrix
    \(\Sigma = \Lambda_1^{1/2}\) is diagonal and, because \(A^T A\) is positive semidefinite,
    the eigenvalues are non-negative. So it remains to prove two things: that the
    columns of \(U\) are orthonormal and, finally, that \(A = U \Sigma V^T\).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过构造，\(V = Q_1\) 的列是正交归一的。矩阵 \(\Sigma = \Lambda_1^{1/2}\) 是对角矩阵，并且由于
    \(A^T A\) 是正半定矩阵，其特征值是非负的。因此，剩下要证明两件事：\(U\) 的列是正交归一的，最后，\(A = U \Sigma V^T\).'
- en: '**KNOWLEDGE CHECK:** Prove that'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 证明'
- en: a) \(A^T A Q_1 = Q_1 \Lambda_1\) and \(A^T A Q_2 = Q_2 \Lambda_2 = \mathbf{0}\),
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(A^T A Q_1 = Q_1 \Lambda_1\) 和 \(A^T A Q_2 = Q_2 \Lambda_2 = \mathbf{0}\),
- en: b) \(Q_1 Q_1^T + Q_2 Q_2^T = I_{m \times m}\).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(Q_1 Q_1^T + Q_2 Q_2^T = I_{m \times m}\).
- en: \(\checkmark\)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: '**LEMMA** **(Step 1)** The columns of \(U\) are orthonormal. \(\flat\)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(步骤 1)** \(U\) 的列是正交归一的。\(\flat\)'
- en: '*Proof:* By direct computation,'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过直接计算，'
- en: \[ U^T U = (A Q_1 \Lambda_1^{-1/2})^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2}
    Q_1^T A^T A Q_1 \Lambda_1^{-1/2}. \]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U^T U = (A Q_1 \Lambda_1^{-1/2})^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2}
    Q_1^T A^T A Q_1 \Lambda_1^{-1/2}. \]
- en: Because the columns of \(Q_1\) are eigenvectors of \(A^T A\), we have that \(A^T
    A Q_1 = Q_1 \Lambda_1\). Further those eigenvectors are orthonormal so that \(Q_1^T
    Q_1 = I_{r \times r}\). Plugging above and simplifying gives
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(Q_1\) 的列是 \(A^T A\) 的特征向量，所以我们有 \(A^T A Q_1 = Q_1 \Lambda_1\)。进一步，这些特征向量是正交归一的，因此
    \(Q_1^T Q_1 = I_{r \times r}\)。将上述内容代入并简化给出
- en: \[ \Lambda_1^{-1/2} Q_1^T A^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} Q_1^T
    Q_1 \Lambda_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} I_{r \times r} \Lambda_1 \Lambda_1^{-1/2}
    = I_{r \times r}, \]
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \Lambda_1^{-1/2} Q_1^T A^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} Q_1^T
    Q_1 \Lambda_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} I_{r \times r} \Lambda_1 \Lambda_1^{-1/2}
    = I_{r \times r}, \]
- en: as claimed. \(\square\)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。 \(\square\)
- en: '**LEMMA** **(Step 2)** It holds that \(A = U \Sigma V^T\). \(\flat\)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(步骤 2)** 成立 \(A = U \Sigma V^T\). \(\flat\)'
- en: '*Proof:* By direct computation, we have'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过直接计算，我们有'
- en: \[ U \Sigma V^T = A Q_1 \Lambda_1^{-1/2} \Lambda_1^{1/2} Q_1^T = A Q_1 Q_1^T.
    \]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U \Sigma V^T = A Q_1 \Lambda_1^{-1/2} \Lambda_1^{1/2} Q_1^T = A Q_1 Q_1^T.
    \]
- en: The matrix \(Q_1 Q_1^T\) is an orthogonal projection on the subspace spanned
    by the vectors \(\mathbf{q}_1,\ldots,\mathbf{q}_r\). Similarly, the matrix \(Q_2
    Q_2^T\) is an orthogonal projection on the orthogonal complement (spanned by \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\)).
    Hence \(Q_1 Q_1^T + Q_2 Q_2^T = I_{m \times m}\). Replacing above we get
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(Q_1 Q_1^T\) 是由向量 \(\mathbf{q}_1,\ldots,\mathbf{q}_r\) 张成的子空间上的正交投影。同样，矩阵
    \(Q_2 Q_2^T\) 是由 \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\) 张成的正交补（子空间）上的正交投影。因此
    \(Q_1 Q_1^T + Q_2 Q_2^T = I_{m \times m}\)。替换上述内容我们得到
- en: \[ U \Sigma V^T = A (I_{n \times n} - Q_2 Q_2^T) = A - A Q_2 Q_2^T. \]
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U \Sigma V^T = A (I_{n \times n} - Q_2 Q_2^T) = A - A Q_2 Q_2^T. \]
- en: Now note that for any \(\mathbf{q}_i\), \(i=r+1,\ldots,m\), we have \(A^T A
    \mathbf{q}_i = \mathbf{0}\), so that \(\mathbf{q}_i^T A^T A \mathbf{q}_i = \|A
    \mathbf{q}_i\|^2 = 0\). That implies that \(A \mathbf{q}_i = \mathbf{0}\) and
    further \(A Q_2 = \mathbf{0}\). Substituting above concludes the proof. \(\square\)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意到对于任何 \(\mathbf{q}_i\)，\(i=r+1,\ldots,m\)，我们有 \(A^T A \mathbf{q}_i = \mathbf{0}\)，因此
    \(\mathbf{q}_i^T A^T A \mathbf{q}_i = \|A \mathbf{q}_i\|^2 = 0\)。这表明 \(A \mathbf{q}_i
    = \mathbf{0}\) 并且进一步 \(A Q_2 = \mathbf{0}\)。代入上述内容完成证明。\(\square\)
- en: That concludes the proof of the theorem. \(\square\)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了定理的证明。\(\square\)
- en: We record the following important consequence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记录以下重要后果。
- en: '**LEMMA** **(SVD and Rank)** \(\idx{SVD and rank lemma}\xdi\) Let \(A \in \mathbb{R}^{n\times
    m}\) have singular value decomposition \(A = U \Sigma V^T\) with \(U \in \mathbb{R}^{n
    \times r}\) and \(V \in \mathbb{R}^{m \times r}\). Then the columns of \(U\) form
    an orthonormal basis of \(\mathrm{col}(A)\) and the columns of \(V\) form an orthonormal
    basis of \(\mathrm{row}(A)\). In particular, the rank of \(A\) is \(r\). \(\flat\)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(奇异值分解与秩)** \(\idx{SVD and rank lemma}\xdi\) 设 \(A \in \mathbb{R}^{n\times
    m}\) 有奇异值分解 \(A = U \Sigma V^T\)，其中 \(U \in \mathbb{R}^{n \times r}\) 和 \(V \in
    \mathbb{R}^{m \times r}\)。那么 \(U\) 的列构成 \(\mathrm{col}(A)\) 的一个正交基，\(V\) 的列构成
    \(\mathrm{row}(A)\) 的一个正交基。特别是，\(A\) 的秩是 \(r\)。\(\flat\)'
- en: '*Proof idea:* We use the SVD to show that the span of the columns of \(U\)
    is \(\mathrm{col}(A)\), and similarly for \(V\).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明思路：** 我们使用奇异值分解来证明 \(U\) 的列的张成是 \(\mathrm{col}(A)\)，对 \(V\) 同样适用。'
- en: '*Proof:* We first prove that any column of \(A\) can be written as a linear
    combination of the columns of \(U\). Indeed, this follows immediately from the
    SVD by noting that for any canonical basis vector \(\mathbf{e}_i \in \mathbb{R}^m\)
    (which produces column \(i\) of \(A\) with \(A \mathbf{e}_i\))'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：** 我们首先证明 \(A\) 的任何一列都可以写成 \(U\) 的列的线性组合。实际上，这直接从奇异值分解得出，注意到对于任何标准基向量
    \(\mathbf{e}_i \in \mathbb{R}^m\)（它产生 \(A\) 的第 \(i\) 列，即 \(A \mathbf{e}_i\))'
- en: \[\begin{align*} A \mathbf{e}_i =\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{e}_i
    = \sum_{j=1}^r (\sigma_j \mathbf{v}_j^T \mathbf{e}_i) \,\mathbf{u}_j. \end{align*}\]
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{e}_i =\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{e}_i
    = \sum_{j=1}^r (\sigma_j \mathbf{v}_j^T \mathbf{e}_i) \,\mathbf{u}_j. \end{align*}\]
- en: Vice versa, any column of \(U\) can be written as a linear combination of the
    columns of \(A\). To see this, we use the orthonormality of the \(\mathbf{v}_j\)’s
    and the positivity of the singular values to obtain
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，\(U\) 的任何一列都可以写成 \(A\) 的列的线性组合。为了看到这一点，我们使用 \(\mathbf{v}_j\) 的正交性和奇异值的正定性来获得
- en: \[\begin{align*} A (\sigma_i^{-1} \mathbf{v}_i) = \sigma_i^{-1}\left(\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{v}_i = \sigma_i^{-1} \sum_{j=1}^r
    (\sigma_j \mathbf{v}_j^T \mathbf{v}_i) \,\mathbf{u}_j = \sigma_i^{-1} (\sigma_i
    \mathbf{v}_i^T \mathbf{v}_i) \,\mathbf{u}_i = \mathbf{u}_i. \end{align*}\]
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A (\sigma_i^{-1} \mathbf{v}_i) = \sigma_i^{-1}\left(\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{v}_i = \sigma_i^{-1} \sum_{j=1}^r
    (\sigma_j \mathbf{v}_j^T \mathbf{v}_i) \,\mathbf{u}_j = \sigma_i^{-1} (\sigma_i
    \mathbf{v}_i^T \mathbf{v}_i) \,\mathbf{u}_i = \mathbf{u}_i. \end{align*}\]
- en: That is, \(\mathrm{col}(U) = \mathrm{col}(A)\). We have already shown that the
    columns of \(U\) are orthonormal. Since their span is \(\mathrm{col}(A)\), they
    form an orthonormal basis of it. Applying the same argument to \(A^T\) gives the
    claim for \(V\) (try it!). \(\square\)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 即，\(\mathrm{col}(U) = \mathrm{col}(A)\)。我们已经证明了 \(U\) 的列是正交的。由于它们的张成是 \(\mathrm{col}(A)\)，它们构成了它的一个正交基。将相同的论点应用于
    \(A^T\) 得到 \(V\) 的结论（试试看！）。\(\square\)
- en: '**EXAMPLE:** Let'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 让'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: We compute its SVD. In this case it can be done (or guessed) using what we know
    about the SVD. Note first that \(A\) is not invertible. Indeed, its rows are a
    multiple of one another. In particular, they are not linearly independent. In
    fact, that tells us that the rank of \(A\) is \(1\), the dimension of its row
    space. In the rank one case, computing the SVD boils down to writing the matrix
    \(A\) in outer product form
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算其奇异值分解（SVD）。在这种情况下，我们可以利用我们对SVD的了解来完成（或猜测）它。首先要注意的是，\(A\)是不可逆的。确实，它的行是彼此的倍数。特别是，它们不是线性无关的。事实上，这告诉我们\(A\)的秩是\(1\)，即其行空间的维度。在秩为1的情况下，计算SVD归结为将矩阵\(A\)写成外积形式
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
- en: where we require that \(\sigma_1 > 0\) and that \(\mathbf{u}_1, \mathbf{v}_1\)
    are of unit norm.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们要求\(\sigma_1 > 0\)，并且\(\mathbf{u}_1, \mathbf{v}_1\)是单位范数。
- en: Recall that an outer product has columns that are all multiples of the same
    vector. Here because the second column of \(A\) is \(\mathbf{0}\), it must be
    that the second component of \(\mathbf{v}_1\) is \(0\). To be of unit norm, its
    first component must be \(1\) or \(-1\). (The choice here does not matter because
    multiplying all left and right singular vectors by \(-1\) produces another SVD.)
    We choose \(1\), i.e., we let
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，外积的列都是同一个向量的倍数。在这里，因为\(A\)的第二列是\(\mathbf{0}\)，所以\(\mathbf{v}_1\)的第二分量必须是\(0\)。为了是单位范数，其第一分量必须是\(1\)或\(-1\)。（这里的选项并不重要，因为将所有左奇异向量和右奇异向量乘以\(-1\)会产生另一个SVD。）我们选择\(1\)，即我们让
- en: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
- en: That vector is indeed an orthonormal basis of the row space of \(A\). Then we
    need
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 那个向量确实是\(A\)的行空间的一个正交归一基。然后我们需要
- en: \[\begin{split} \sigma_1 \mathbf{u}_1 = \begin{pmatrix} 1\\ -1 \end{pmatrix}.
    \end{split}\]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \sigma_1 \mathbf{u}_1 = \begin{pmatrix} 1\\ -1 \end{pmatrix}.
    \end{split}\]
- en: For \(\mathbf{u}_1\) to be of unit norm, we must have
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使\(\mathbf{u}_1\)是单位范数，我们必须有
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}
    \quad \text{and} \quad \sigma_1 = \sqrt{2}. \end{split}\]
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}
    \quad \text{和} \quad \sigma_1 = \sqrt{2}. \end{split}\]
- en: Observe that \(\mathbf{u}_1\) is indeed an orthonormal basis of the column space
    of \(A\). \(\lhd\)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到\(\mathbf{u}_1\)确实是\(A\)的列空间的一个正交归一基。\(\lhd\)
- en: One might hope that the SVD of a symmetric matrix generates identical left and
    right singular vectors. However that is not the case.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能希望对称矩阵的SVD生成相同的左奇异向量和右奇异向量。然而，情况并非如此。
- en: '**EXAMPLE:** An SVD of \(A = (-1)\) is \(A = (1)\,(1)\,(-1)\). That is, \(\mathbf{u}_1
    = (1)\) and \(\mathbf{v} = (-1)\). \(\lhd\)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**例证：** \(A = (-1)\)的SVD是\(A = (1)\,(1)\,(-1)\)。也就是说，\(\mathbf{u}_1 = (1)\)和\(\mathbf{v}
    = (-1)\)。\(\lhd\)'
- en: We collect in the next lemma some relationships between the singular vectors
    and singular values that will be used repeatedly. It also further clarifies the
    connection between the SVD of \(A\) and the spectral decomposition of \(A^T A\)
    and \(A A^T\).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的引理中，我们收集了一些关于奇异向量和奇异值之间关系的一些关系，这些关系将被反复使用。它还进一步阐明了\(A\)的SVD与\(A^T A\)和\(A
    A^T\)的谱分解之间的联系。
- en: '**LEMMA** **(SVD Relations)** \(\idx{SVD relations}\xdi\) Let \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be an SVD of \(A \in \mathbb{R}^{n \times
    m}\) with \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\). Then, for
    \(i=1,\ldots,r\),'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(SVD关系)** \(\idx{SVD relations}\xdi\) 设\(A = \sum_{j=1}^r \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\)是\(A \in \mathbb{R}^{n \times m}\)的SVD，其中\(\sigma_1
    \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\)。那么，对于\(i=1,\ldots,r\)，'
- en: \[ A \mathbf{v}_i = \sigma_i \mathbf{u}_i, \qquad A^T \mathbf{u}_i = \sigma_i
    \mathbf{v}_i, \qquad \|A \mathbf{v}_i\| = \sigma_i, \qquad \|A^T \mathbf{u}_i\|
    = \sigma_i. \]
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{v}_i = \sigma_i \mathbf{u}_i, \qquad A^T \mathbf{u}_i = \sigma_i
    \mathbf{v}_i, \qquad \|A \mathbf{v}_i\| = \sigma_i, \qquad \|A^T \mathbf{u}_i\|
    = \sigma_i. \]
- en: A fortiori
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当然
- en: \[ A^T A \mathbf{v}_i = \sigma_i^2 \mathbf{v}_i, \qquad A A^T \mathbf{u}_i =
    \sigma_i^2 \mathbf{u}_i. \]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{v}_i = \sigma_i^2 \mathbf{v}_i, \qquad A A^T \mathbf{u}_i =
    \sigma_i^2 \mathbf{u}_i. \]
- en: and, for \(j \neq i\),
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 和，对于\(j \neq i\)，
- en: \[ \langle A \mathbf{v}_i, A \mathbf{v}_j \rangle = 0, \qquad \langle A^T \mathbf{u}_i,
    A^T \mathbf{u}_j \rangle = 0. \]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle A \mathbf{v}_i, A \mathbf{v}_j \rangle = 0, \qquad \langle A^T \mathbf{u}_i,
    A^T \mathbf{u}_j \rangle = 0. \]
- en: \(\flat\)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: We previously established the existence of an SVD via the spectral decomposition
    of \(A^T A\). The previous lemma shows that in fact, in any SVD, the \(\mathbf{v}_i\)s
    are orthonormal eigenvectors of \(A^T A\). They do not form an orthonormal basis
    of the full space \(\mathbb{R}^m\) however, as the rank \(r\) can be strictly
    smaller than \(m\). But observe that any vector \(\mathbf{w}\) orthogonal to \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\)
    is such that
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前通过 \(A^T A\) 的谱分解建立了奇异值分解的存在性。前面的引理表明，实际上，在任何奇异值分解中，\(\mathbf{v}_i\) 是 \(A^T
    A\) 的正交特征向量。然而，它们并不构成整个空间 \(\mathbb{R}^m\) 的正交基，因为秩 \(r\) 可以严格小于 \(m\)。但请注意，任何与
    \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\) 正交的向量 \(\mathbf{w}\) 都满足
- en: \[ A\mathbf{w} = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{w}
    = \mathbf{0} \]
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A\mathbf{w} = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{w}
    = \mathbf{0} \]
- en: and, a fortiori,
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，更不用说
- en: \[ A^T A \mathbf{w} = \mathbf{0}. \]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{w} = \mathbf{0}. \]
- en: So \(\mathbf{w}\) is in fact an eigenvector of \(A^T A\) with eigenvalue \(0\).
    Let \(\mathbf{v}_{r+1}, \ldots, \mathbf{v}_m\) be any orthonormal basis of \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)^\perp\).
    Then \(\mathbf{v}_1,\ldots,\mathbf{v}_m\) is an orthonormal basis of eigenvectors
    of \(A^T A\).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(\mathbf{w}\) 实际上是 \(A^T A\) 的特征向量，特征值为 \(0\)。令 \(\mathbf{v}_{r+1}, \ldots,
    \mathbf{v}_m\) 为 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)^\perp\) 的任意正交基。那么，\(\mathbf{v}_1,\ldots,\mathbf{v}_m\)
    是 \(A^T A\) 的特征向量的正交基。
- en: The lemma also shows that the \(\mathbf{u}_i\)s are orthonormal eigenvectors
    of \(A A^T\)!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该引理还表明，\(\mathbf{u}_i\) 是 \(A A^T\) 的正交特征向量！
- en: '**Full vs. compact SVD** What we have introduced above is in fact referred
    to as a [compact SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Compact_SVD).
    In contrast, in a [full SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\(\idx{full
    SVD}\xdi\), the matrices \(U\) and \(V\) are square and orthogonal, and the matrix
    \(\Sigma\) is diagonal, but may not be square and may have zeros on the diagonal.
    In particular, in that case, the columns of \(U \in \mathbb{R}^{n \times n}\)
    form an orthonormal basis of \(\mathbb{R}^n\) and the columns of \(V \in \mathbb{R}^{m
    \times m}\) form an orthonormal basis of \(\mathbb{R}^m\).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整奇异值分解与紧凑奇异值分解** 我们上面介绍的是所谓的紧凑奇异值分解。相比之下，在完整奇异值分解（[full SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Compact_SVD)）中，矩阵
    \(U\) 和 \(V\) 是方阵且正交，矩阵 \(\Sigma\) 是对角矩阵，但不一定是方阵，并且对角线上可能有零。特别是，在这种情况下，\(U \in
    \mathbb{R}^{n \times n}\) 的列构成 \(\mathbb{R}^n\) 的一个正交基，而 \(V \in \mathbb{R}^{m
    \times m}\) 的列构成 \(\mathbb{R}^m\) 的一个正交基。'
- en: '**Figure:** SVD in full form ([Source](https://commons.wikimedia.org/wiki/File:Singular_value_decomposition_visualisation.svg))'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：** 完整形式的奇异值分解 ([来源](https://commons.wikimedia.org/wiki/File:Singular_value_decomposition_visualisation.svg))'
- en: '![SVD](../Images/cf2dd761c707eee39f5210e1337dc37d.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![SVD](../Images/cf2dd761c707eee39f5210e1337dc37d.png)'
- en: \(\bowtie\)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: Let \(A = U_1 \Sigma_1 V_1^T\) be a compact SVD. Complete the columns of \(U_1\)
    into an orthonormal basis of \(\mathbb{R}^n\) and let \(U_2\) be the matrix whose
    columns are the additional basis vectors. Similary, complete the columns of \(V_1\)
    into an orthonormal basis of \(\mathbb{R}^m\) and let \(V_2\) be the matrix whose
    columns are the additional basis vectors. Then a full SVD is given by
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(A = U_1 \Sigma_1 V_1^T\) 为一个紧凑的奇异值分解。将 \(U_1\) 的列扩展为 \(\mathbb{R}^n\) 的一个正交基，并令
    \(U_2\) 为其列是额外基向量的矩阵。类似地，将 \(V_1\) 的列扩展为 \(\mathbb{R}^m\) 的一个正交基，并令 \(V_2\) 为其列是额外基向量的矩阵。然后，一个完整的奇异值分解由以下给出
- en: \[\begin{split} U = \begin{pmatrix} U_1 & U_2 \end{pmatrix} \quad V = \begin{pmatrix}
    V_1 & V_2 \end{pmatrix} \quad \Sigma = \begin{pmatrix} \Sigma_1 & \mathbf{0}_{r
    \times (m-r)}\\ \mathbf{0}_{(n-r)\times r} & \mathbf{0}_{(n-r)\times (m-r)} \end{pmatrix}.
    \end{split}\]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} U = \begin{pmatrix} U_1 & U_2 \end{pmatrix} \quad V = \begin{pmatrix}
    V_1 & V_2 \end{pmatrix} \quad \Sigma = \begin{pmatrix} \Sigma_1 & \mathbf{0}_{r
    \times (m-r)}\\ \mathbf{0}_{(n-r)\times r} & \mathbf{0}_{(n-r)\times (m-r)} \end{pmatrix}.
    \end{split}\]
- en: By the *SVD and Rank Lemma*, the columns of \(U_1\) form an orthonormal basis
    of \(\mathrm{col}(A)\). Because \(\mathrm{col}(A)^\perp = \mathrm{null}(A^T)\),
    the columns of \(U_2\) form an orthonormal basis of \(\mathrm{null}(A^T)\). Similarly,
    the columns of \(V_1\) form an orthonormal basis of \(\mathrm{col}(A^T)\). Because
    \(\mathrm{col}(A^T)^\perp = \mathrm{null}(A)\), the columns of \(V_2\) form an
    orthonormal basis of \(\mathrm{null}(A)\). Hence, a full SVD provides an orthonormal
    basis for all four fundamental subspaces of \(A\).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SVD和秩引理，\(U_1\) 的列构成 \(\mathrm{col}(A)\) 的正交基。因为 \(\mathrm{col}(A)^\perp =
    \mathrm{null}(A^T)\)，所以 \(U_2\) 的列构成 \(\mathrm{null}(A^T)\) 的正交基。类似地，\(V_1\) 的列构成
    \(\mathrm{col}(A^T)\) 的正交基。因为 \(\mathrm{col}(A^T)^\perp = \mathrm{null}(A)\)，所以
    \(V_2\) 的列构成 \(\mathrm{null}(A)\) 的正交基。因此，完整的SVD为 \(A\) 的所有四个基本子空间提供了正交基。
- en: Vice versa, given a full SVD \(A = U \Sigma V^T\), the compact SVD can be obtained
    by keeping only the square submatrix of \(\Sigma\) with stricly positive diagonal
    entries, together with the corresponding columns of \(U\) and \(V\).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，给定一个完整的SVD \(A = U \Sigma V^T\)，可以通过仅保留 \(\Sigma\) 的严格正对角线元素的平方子矩阵，以及相应的
    \(U\) 和 \(V\) 的列来获得紧凑的SVD。
- en: '**Figure:** Different variants of the SVD ([Source](https://commons.wikimedia.org/wiki/File:Reduced_Singular_Value_Decompositions.svg))'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：SVD的不同变体 ([来源](https://commons.wikimedia.org/wiki/File:Reduced_Singular_Value_Decompositions.svg**))'
- en: '![ReducedSVD](../Images/2bc162cb67041b1a46fc5b7ca370d66c.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![ReducedSVD](../Images/2bc162cb67041b1a46fc5b7ca370d66c.png)'
- en: \(\bowtie\)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**EXAMPLE:** **(continued)** Let again'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(继续**) 再次'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: We previously computed its compact SVD
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经计算了它的紧凑SVD
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
- en: where
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: where
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{和}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
- en: We now compute a full SVD. For this, we need to complete the bases. We can choose
    (why?)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在计算一个完整的SVD。为此，我们需要完成基。我们可以选择（为什么？）
- en: \[\begin{split} \mathbf{u}_2 = \begin{pmatrix} 1/\sqrt{2}\\ 1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_2 = \begin{pmatrix} 0\\ 1 \end{pmatrix}, \quad \text{and}
    \quad \sigma_2 = 0. \end{split}\]
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_2 = \begin{pmatrix} 1/\sqrt{2}\\ 1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_2 = \begin{pmatrix} 0\\ 1 \end{pmatrix}, \quad \text{和}
    \quad \sigma_2 = 0. \end{split}\]
- en: Then, a full SVD is given by
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，完整的SVD可以表示为
- en: \[\begin{split} U = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2}\\ -1/\sqrt{2} &
    1/\sqrt{2} \end{pmatrix}, \quad \quad V = \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix},
    \quad \text{and} \quad \Sigma = \begin{pmatrix} \sqrt{2} & 0\\ 0 & 0 \end{pmatrix}.
    \end{split}\]
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} U = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2}\\ -1/\sqrt{2} &
    1/\sqrt{2} \end{pmatrix}, \quad \quad V = \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix},
    \quad \text{和} \quad \Sigma = \begin{pmatrix} \sqrt{2} & 0\\ 0 & 0 \end{pmatrix}.
    \end{split}\]
- en: Indeed, \(A = U \Sigma V^T\) (check it!). \(\lhd\)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，\(A = U \Sigma V^T\) (检查它!) \(\lhd\)
- en: 'The full SVD also has a natural geometric interpretation. To quote [Sol, p.
    133]:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的SVD也有一个自然的几何解释。引用 [Sol, p. 133]：
- en: The SVD provides a complete geometric characterization of the action of \(A\).
    Since \(U\) and \(V\) are orthogonal, they have no effect on lengths and angles;
    as a diagonal matrix, \(\Sigma\) scales individual coordinate axes. Since the
    SVD always exists, all matrices \(A \in \mathbb{R}^{n \times m}\) are a composition
    of an isometry, a scale in each coordinate, and a second isometry.
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SVD提供了 \(A\) 作用的完整几何特征。由于 \(U\) 和 \(V\) 是正交的，它们对长度和角度没有影响；作为对角矩阵，\(\Sigma\)
    放缩各个坐标轴。由于SVD总是存在的，所有 \(A \in \mathbb{R}^{n \times m}\) 的矩阵都是等距变换、每个坐标的缩放和第二个等距变换的合成。
- en: '**Figure:** Geometric interpretation of the SVD ([Source](https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg))'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：SVD的几何解释 ([来源](https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg**))'
- en: '![Geometric meaning](../Images/ea0108f7b4d77cc40c274d7c98d6d646.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![几何意义](../Images/ea0108f7b4d77cc40c274d7c98d6d646.png)'
- en: \(\bowtie\)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**Coming full circle: solving the approximating subspace problem via the SVD**
    Think of the rows \(\boldsymbol{\alpha}_i^T\) of a matrix \(A \in \mathbb{R}^{n
    \times m}\) as a collection of \(n\) data points in \(\mathbb{R}^m\). Let'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整闭环：通过SVD解决逼近子空间问题** 将矩阵 \(A \in \mathbb{R}^{n \times m}\) 的行 \(\boldsymbol{\alpha}_i^T\)
    视为 \(\mathbb{R}^m\) 中的 \(n\) 个数据点的集合。令'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: be a (compact) SVD of \(A\). Fix \(k \leq \mathrm{rk}(A)\). We are looking for
    a linear subspace \(\mathcal{Z}\) that minimizes
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 是 \(A\) 的一个（紧致）奇异值分解。固定 \(k \leq \mathrm{rk}(A)\)。我们正在寻找一个线性子空间 \(\mathcal{Z}\)，使其
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
- en: over all linear subspaces of \(\mathbb{R}^m\) of dimension at most \(k\). By
    the observations above, a solution is given by
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 \(\mathbb{R}^m\) 的所有线性子空间中，其维度最多为 \(k\)。根据上述观察，一个解由以下给出
- en: \[ \mathcal{Z} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k). \]
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{Z} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k). \]
- en: By the proofs of the *Best Subspace as Maximization* and *Best Subspace in Matrix
    Form* lemmas, the objective value achieved is
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**最佳子空间最大化**和**矩阵形式中的最佳子空间**引理的证明，所达到的目标值
- en: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \|A\mathbf{v}_j\|^2\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \sigma_j^2. \end{align*}\]
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \|A\mathbf{v}_j\|^2\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \sigma_j^2. \end{align*}\]
- en: So the singular value \(\sigma_j\) associated to the right singular vector \(\mathbf{v}_j\)
    captures its contribution to the fit of the approximating subspace. The larger
    the singular value, the larger the contribution.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与右奇异向量 \(\mathbf{v}_j\) 相关的奇异值 \(\sigma_j\) 捕获了其对逼近子空间的贡献。奇异值越大，贡献越大。
- en: To obtain a low-dimensional embedding of our original datasets, we compute \(\mathbf{z}_i
    := \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\) for each \(i\) as follows
    (in row form)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得原始数据集的低维嵌入，我们计算每个 \(i\) 的 \(\mathbf{z}_i := \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\)
    如下（以行形式）
- en: \[\begin{align*} \mathbf{z}_i^T &= \sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{v}_j\rangle \,\mathbf{v}_j^T\\ &= \sum_{j=1}^k \boldsymbol{\alpha}_i^T
    \mathbf{v}_j \mathbf{v}_j^T\\ &= A_{i,\cdot} V_{(k)} V_{(k)}^T, \end{align*}\]
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_i^T &= \sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{v}_j\rangle \,\mathbf{v}_j^T\\ &= \sum_{j=1}^k \boldsymbol{\alpha}_i^T
    \mathbf{v}_j \mathbf{v}_j^T\\ &= A_{i,\cdot} V_{(k)} V_{(k)}^T, \end{align*}\]
- en: where \(V_{(k)}\) is the matrix with the first \(k\) columns of \(V\). Let \(Z\)
    be the matrix with rows \(\mathbf{z}_i^T\). Then we have
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(V_{(k)}\) 是包含 \(V\) 的前 \(k\) 列的矩阵。设 \(Z\) 为包含行 \(\mathbf{z}_i^T\) 的矩阵。那么我们有
- en: \[ Z = A V_{(k)} V_{(k)}^T = U_{(k)} \Sigma_{(k)} V_{(k)}^T = \sum_{j=1}^k \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T, \]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z = A V_{(k)} V_{(k)}^T = U_{(k)} \Sigma_{(k)} V_{(k)}^T = \sum_{j=1}^k \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T, \]
- en: where \(U_{(k)}\) is the matrix with the first \(k\) columns of \(U\), and \(\Sigma_{(k)}\)
    is the matrix with the first \(k\) rows and columns of \(\Sigma\). Indeed, recall
    that \(A \mathbf{v}_j = \sigma_j \mathbf{u}_j\), or in matrix form \(A V_{(k)}
    = U_{(k)} \Sigma_{(k)}\). The rightmost expression for \(Z\) reveals that it is
    in fact a truncated SVD. We can interpret the rows of \(U_{(k)} \Sigma_{(k)}\)
    as the coefficients of each data point in the basis \(\mathbf{v}_1,\ldots,\mathbf{v}_k\).
    Those coefficients provide the desired low-dimensional representation.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(U_{(k)}\) 是包含 \(U\) 的前 \(k\) 列的矩阵，而 \(\Sigma_{(k)}\) 是包含 \(\Sigma\) 的前
    \(k\) 行和列的矩阵。实际上，回想一下 \(A \mathbf{v}_j = \sigma_j \mathbf{u}_j\)，或者用矩阵形式 \(A V_{(k)}
    = U_{(k)} \Sigma_{(k)}\)。\(Z\) 的最右表达式表明它实际上是一个截断的奇异值分解。我们可以将 \(U_{(k)} \Sigma_{(k)}\)
    的行解释为每个数据点在基 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 中的系数。这些系数提供了所需的低维表示。
- en: We can re-write the objective function in a more compact matrix form by using
    the Frobenius norm as follows
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用Frobenius范数将目标函数重写为更紧凑的矩阵形式，如下所示
- en: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{z}_i\|^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i^T
    - \mathbf{z}_i^T\|^2 = \|A - Z\|_F^2. \end{align*}\]
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{z}_i\|^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i^T
    - \mathbf{z}_i^T\|^2 = \|A - Z\|_F^2. \end{align*}\]
- en: We note that the matrix \(Z\) has rank smaller or equal than \(k\). Indeed,
    all of its rows lie in the optimal subspace \(\mathcal{Z}\), which has dimension
    \(k\) by construction. We will see later that \(Z\) is the best approximation
    to \(A\) among all rank-\(k\) matrices under the Frobenius norm, that is,
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到矩阵 \(Z\) 的秩小于或等于 \(k\)。实际上，它的所有行都位于由构造得到的最优子空间 \(\mathcal{Z}\) 中，该子空间的维度为
    \(k\)。我们将在后面看到，\(Z\) 是在Frobenius范数下所有秩为 \(k\) 的矩阵中最佳逼近 \(A\) 的矩阵，
- en: \[ \|A - Z\|_F \leq \|A - B\|_F \]
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - Z\|_F \leq \|A - B\|_F \]
- en: for any matrix \(B\) of rank at most \(k\).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何秩最多为 \(k\) 的矩阵 \(B\)。
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由克劳德、双子星和ChatGPT协助)*'
- en: '**1** Let \(\boldsymbol{\alpha}_1, \dots, \boldsymbol{\alpha}_n\) be data points
    in \(\mathbb{R}^m\). What is the objective of the best approximating subspace
    problem?'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 设 \(\boldsymbol{\alpha}_1, \dots, \boldsymbol{\alpha}_n\) 是 \(\mathbb{R}^m\)
    中的数据点。最佳逼近子空间问题的目标是什么？'
- en: a) To find a linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that minimizes
    the sum of the distances between the \(\boldsymbol{\alpha}_i\)’s and \(\mathcal{Z}\).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: a) 找到一个线性子空间 \(\mathcal{Z}\) 属于 \(\mathbb{R}^m\)，使得 \(\boldsymbol{\alpha}_i\)
    与 \(\mathcal{Z}\) 之间的距离之和最小。
- en: b) To find a linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that minimizes
    the sum of the squared distances between the \(\boldsymbol{\alpha}_i\)’s and their
    orthogonal projections onto \(\mathcal{Z}\).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: b) 找到一个线性子空间 \(\mathcal{Z}\) 属于 \(\mathbb{R}^m\)，使得 \(\boldsymbol{\alpha}_i\)
    与其在 \(\mathcal{Z}\) 上的正交投影之间的平方距离之和最小。
- en: c) To find a linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that maximizes
    the sum of the squared norms of the orthogonal projections of the \(\boldsymbol{\alpha}_i\)’s
    onto \(\mathcal{Z}\).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: c) 找到一个线性子空间 \(\mathcal{Z}\) 属于 \(\mathbb{R}^m\)，使得 \(\boldsymbol{\alpha}_i\)
    在 \(\mathcal{Z}\) 上的正交投影的平方范数之和最大。
- en: d) Both b and c.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: d) b 和 c 都正确。
- en: '**2** Consider the data points \(\boldsymbol{\alpha}_1 = (-2,2)\) and \(\boldsymbol{\alpha}_2
    = (3,-3)\). For \(k=1\), what is the solution of the best approximating subspace
    problem?'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 考虑数据点 \(\boldsymbol{\alpha}_1 = (-2,2)\) 和 \(\boldsymbol{\alpha}_2 =
    (3,-3)\)。对于 \(k=1\)，最佳逼近子空间问题的解是什么？'
- en: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
- en: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
- en: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x+1\}\)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x+1\}\)'
- en: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x-1\}\)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x-1\}\)'
- en: '**3** Which of the following is true about the SVD of a matrix \(A\)?'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 以下关于矩阵 \(A\) 的奇异值分解的哪个说法是正确的？'
- en: a) The SVD of \(A\) is unique.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(A\) 的奇异值分解是唯一的。
- en: b) The right singular vectors of \(A\) are the eigenvectors of \(A^TA\).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(A\) 的右奇异向量是 \(A^TA\) 的特征向量。
- en: c) The left singular vectors of \(A\) are the eigenvectors of \(AA^T\).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(A\) 的左奇异向量是 \(AA^T\) 的特征向量。
- en: d) Both b and c.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: d) b 和 c 都正确。
- en: '**4** Let \(A = U \Sigma V^T\) be an SVD of \(A\). Which of the following is
    true?'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 设 \(A = U \Sigma V^T\) 是 \(A\) 的奇异值分解。以下哪个说法是正确的？'
- en: a) \(A \mathbf{v}_i = \sigma_i \mathbf{u}_i\) for all \(i\).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对于所有 \(i\)，\(A \mathbf{v}_i = \sigma_i \mathbf{u}_i\)。
- en: b) \(A^T \mathbf{u}_i = \sigma_i \mathbf{v}_i\) for all \(i\).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: b) 对于所有 \(i\)，\(A^T \mathbf{u}_i = \sigma_i \mathbf{v}_i\)。
- en: c) \(\|A\mathbf{v}_i\| = \sigma_i\) for all \(i\).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\|A\mathbf{v}_i\| = \sigma_i\) 对于所有 \(i\)。
- en: d) All of the above.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有上述说法都正确。
- en: '**5** The columns of \(U\) in the compact SVD form an orthonormal basis for:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在紧凑的奇异值分解形式中，\(U\) 的列构成以下空间的正交基：'
- en: a) \(\mathrm{col}(A)\)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathrm{col}(A)\)
- en: b) \(\mathrm{row}(A)\)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathrm{row}(A)\)
- en: c) \(\mathrm{null}(A)\)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathrm{null}(A)\)
- en: d) \(\mathrm{null}(A^T)\)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathrm{null}(A^T)\)
- en: 'Answer for 1: d. Justification: The text defines the best approximating subspace
    problem as minimizing the sum of squared distances between the data points and
    their projections onto the subspace, and it also states a lemma that this problem
    is equivalent to maximizing the sum of squared norms of the projections.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：d. 理由：文本将最佳逼近子空间问题定义为最小化数据点与其在子空间上的投影之间的平方距离之和，并且还提到一个引理，即该问题等价于最大化投影的平方范数之和。
- en: 'Answer for 2: b. Justification: By symmetry, the best approximating line must
    pass through the origin and bisect the angle between the two points. This is the
    line \(y=-x\).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：b. 理由：由于对称性，最佳逼近线必须通过原点并平分两点之间的角度。这就是线 \(y=-x\)。
- en: 'Answer for 3: c and d. Justification: The SVD is not unique in general. The
    other two statements are true and are mentioned in the text.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：c 和 d. 理由：奇异值分解在一般情况下不是唯一的。其他两个陈述是正确的，并在文本中提到。
- en: 'Answer for 4: d. Justification: This is a lemma stated in the text.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：d. 理由：这是文本中陈述的一个引理。
- en: 'Answer for 5: a. Justification: The text states in the SVD and Rank Lemma:
    “the columns of \(U\) form an orthonormal basis of \(\mathrm{col}(A)\)”.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：a. 理由：文本在 SVD 和秩引理中声明：“\(U\) 的列构成了 \(A\) 的列空间的正交基”。
- en: 4.3.1\. An objective, an algorithm, and a guarantee[#](#an-objective-an-algorithm-and-a-guarantee
    "Link to this heading")
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3.1\. 一个目标，一个算法和一个保证[#](#an-objective-an-algorithm-and-a-guarantee "链接到这个标题")
- en: Let \(\boldsymbol{\alpha}_1,\dots,\boldsymbol{\alpha}_n\) be a collection of
    \(n\) data points in \(\mathbb{R}^m\). A natural way to extract low-dimensional
    structure in this dataset is to find a low-dimensional linear subspace \(\mathcal{Z}\)
    of \(\mathbb{R}^m\) such that the \(\boldsymbol{\alpha}_i\)’s are “close to it.”
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\boldsymbol{\alpha}_1,\dots,\boldsymbol{\alpha}_n\) 是 \(\mathbb{R}^m\) 中的
    \(n\) 个数据点。从这些数据集中提取低维结构的一种自然方式是找到一个 \(\mathbb{R}^m\) 的低维线性子空间 \(\mathcal{Z}\)，使得
    \(\boldsymbol{\alpha}_i\) 是“接近”它的。
- en: '![The closest line to some data points (with help from ChatGPT; code converted
    from (Source))](../Images/bb6c0e303b9f07ef152154d91bcdbf18.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![一些数据点的最近线（借助 ChatGPT；代码从（来源）转换而来）](../Images/bb6c0e303b9f07ef152154d91bcdbf18.png)'
- en: '**Mathematical formulation of the problem** Again the squared Euclidean norm
    turns out to be computationally convenient. So we look for a linear subspace \(\mathcal{Z}\)
    that minimizes'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题的数学表述** 再次，平方欧几里得范数在计算上很方便。因此，我们寻找一个线性子空间 \(\mathcal{Z}\)，它使'
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
- en: over all linear subspaces of \(\mathbb{R}^m\) of dimension \(k\). To solve this
    problem, which we refer to as the best approximating subspace problem\(\idx{best
    approximating subspace problem}\xdi\), we make a series of observations.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在 \(\mathbb{R}^m\) 的所有维度为 \(k\) 的线性子空间上。为了解决这个问题，我们称之为最佳逼近子空间问题\(\idx{best approximating
    subspace problem}\xdi\)，我们进行了一系列观察。
- en: '**KNOWLEDGE CHECK:** Consider the data points \(\boldsymbol{\alpha}_1 = (-1,1)\)
    and \(\boldsymbol{\alpha}_2 = (1,-1)\). For \(k=1\), what is the solution of the
    best approximating subspace?'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查：** 考虑数据点 \(\boldsymbol{\alpha}_1 = (-1,1)\) 和 \(\boldsymbol{\alpha}_2
    = (1,-1)\)。对于 \(k=1\)，最佳逼近子空间的解是什么？'
- en: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
- en: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
- en: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x + 1\}\)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x + 1\}\)'
- en: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x - 1\}\)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x - 1\}\)'
- en: e) None of the above
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: e) 以上皆非
- en: \(\checkmark\)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: The first observation gives a related, useful characterization of the optimal
    solution.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个观察结果给出了一个与最优解相关的、有用的特征描述。
- en: '**LEMMA** **(Best Subspace as Maximimization)** \(\idx{best subspace as maximimization
    lemma}\xdi\) Let \(\boldsymbol{\alpha}_i\), \(i =1\ldots,n\), be vectors in \(\mathbb{R}^m\).
    A linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that minimizes'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（最佳子空间最大化）** \(\idx{best subspace as maximization lemma}\xdi\) 设 \(\boldsymbol{\alpha}_i\)，\(i
    =1\ldots,n\)，是 \(\mathbb{R}^m\) 中的向量。一个 \(\mathbb{R}^m\) 的线性子空间 \(\mathcal{Z}\)，它使'
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
- en: over all linear subspaces of dimension at most \(k\) also maximizes
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在维度最多为 \(k\) 的所有线性子空间上也是最大化的
- en: \[ \sum_{i=1}^n \|\mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2 \]
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2 \]
- en: over the same linear subspaces. And vice versa. \(\flat\)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的线性子空间上。反之亦然。\(\flat\)
- en: '*Proof idea:* This is a straightforward application of the triangle inequality.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明思路**：这是对三角不等式的一个直接应用。'
- en: '*Proof:* By *Pythagoras’ Theorem*,'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：根据**毕达哥拉斯定理**，'
- en: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    + \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2 = \|\boldsymbol{\alpha}_i\|^2
    \]
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    + \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2 = \|\boldsymbol{\alpha}_i\|^2
    \]
- en: since, by the *Orthogonal Projection Theorem*, \(\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\)
    is orthogonal to \(\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\).
    Rearranging,
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 因为，根据**正交投影定理**，\(\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\)与\(\boldsymbol{\alpha}_i
    - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\)正交。重新排列，
- en: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2.
    \]
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2.
    \]
- en: The result follows from the fact that the first term on the right-hand side
    does not depend on the choice of \(\mathcal{Z}\). More specifically, optimizing
    over linear subspaces \(\mathcal{Z}\) of dimension \(k\),
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 结果来自于右侧第一个项不依赖于 \(\mathcal{Z}\) 的选择。更具体地说，优化线性子空间 \(\mathcal{Z}\) 的维度 \(k\)，
- en: \[\begin{align*} \min_{\mathcal{Z}} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \min_{\mathcal{Z}} \sum_{i=1}^n \left\{\|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 + \min_{\mathcal{Z}} \left\{- \sum_{i=1}^n\|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \max_{\mathcal{Z}} \sum_{i=1}^n
    \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2. \end{align*}\]
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \min_{\mathcal{Z}} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \min_{\mathcal{Z}} \sum_{i=1}^n \left\{\|\boldsymbol{\alpha}_i\|^2 - \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 + \min_{\mathcal{Z}} \left\{- \sum_{i=1}^n\|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\right\}\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \max_{\mathcal{Z}} \sum_{i=1}^n
    \|\mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2. \end{align*}\]
- en: \(\square\)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: How do we specify a \(k\)-dimensional linear subspace? Through a basis of it,
    or – even better – an orthonormal basis. In the latter case, we also have an explicit
    formula for the orthogonal projection. And the dimension of the linear subspace
    is captured by the number of elements in the basis, by the *Dimension Theorem*.
    In other words, the best approximating subspace can be obtained by solving the
    problem
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何指定一个 \(k\) 维线性子空间？通过它的基，或者——甚至更好——一个正交基。在后一种情况下，我们也有一个正交投影的显式公式。线性子空间的维度由基中的元素数量捕获，由
    *维度定理*。换句话说，最佳逼近子空间可以通过解决以下问题获得
- en: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{i=1}^n \left\|\sum_{j=1}^k
    \langle \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 \]
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{i=1}^n \left\|\sum_{j=1}^k
    \langle \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 \]
- en: over all orthonormal lists \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) of length \(k\).
    Our next observation rewrites the problem in matrix form.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有长度为 \(k\) 的正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) 上。我们的下一个观察将问题重新写成矩阵形式。
- en: '**LEMMA** **(Best Subpace in Matrix Form)** \(\idx{best subpace in matrix form
    lemma}\xdi\) Consider the matrix \(A \in \mathbb{R}^{n \times m}\) with rows \(\boldsymbol{\alpha}_i^T\).
    A solution to the best approximating subspace problem is obtained by solving'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（矩阵形式下的最佳子空间）** \(\idx{best subpace in matrix form lemma}\xdi\) 考虑矩阵
    \(A \in \mathbb{R}^{n \times m}\) 的行 \(\boldsymbol{\alpha}_i^T\)。通过解决'
- en: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \]
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \]
- en: over all orthonormal lists \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) of length \(k\).
    \(\flat\)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有长度为 \(k\) 的正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) 上。\(\flat\)
- en: '*Proof idea:* We start with the one-dimensional case. A one-dimensional space
    \(\mathcal{Z}\) is determined by a unit vector \(\mathbf{w}_1\). The projection
    \(\boldsymbol{\alpha}_i\) onto the span of \(\mathbf{w}_1\) is given by the inner
    product formula \(\langle \boldsymbol{\alpha}_i, \mathbf{w}_1 \rangle \,\mathbf{w}_1\).
    So'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们从一维情况开始。一维空间 \(\mathcal{Z}\) 由一个单位向量 \(\mathbf{w}_1\) 确定。\(\boldsymbol{\alpha}_i\)
    投影到 \(\mathbf{w}_1\) 张成的空间由内积公式 \(\langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle \,\mathbf{w}_1\) 给出。因此'
- en: \[\begin{align*} \sum_{i=1}^n \|\langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle \,\mathbf{w}_1 \|^2 &= \sum_{i=1}^n \langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle ^2\\ &= \sum_{i=1}^n (\boldsymbol{\alpha}_i^T \mathbf{w}_1)^2\\ &= \|A
    \mathbf{w}_1\|^2 \end{align*}\]
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \|\langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle \,\mathbf{w}_1 \|^2 &= \sum_{i=1}^n \langle \boldsymbol{\alpha}_i, \mathbf{w}_1
    \rangle ^2\\ &= \sum_{i=1}^n (\boldsymbol{\alpha}_i^T \mathbf{w}_1)^2\\ &= \|A
    \mathbf{w}_1\|^2 \end{align*}\]
- en: where, again, \(A\) is the matrix with rows \(\boldsymbol{\alpha}_i ^T\), \(i=1,\ldots,
    n\). Hence the solution to the one-dimensional problem is
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，再次，\(A\) 是由行 \(\boldsymbol{\alpha}_i ^T\) 组成的矩阵，\(i=1,\ldots, n\)。因此，一维问题的解是
- en: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
- en: 'Here \(\arg\max\) means that \(\mathbf{v}_1\) is a vector \(\mathbf{w}_1\)
    that achieves the maximum. Note that there could be more than one such \(\mathbf{w}_1\),
    so the right-hand side is a set containing all such solutions. By the *Extreme
    Value Theorem* (since the set \(\{\mathbf{w}_1 : \|\mathbf{w}_1\| = 1\}\) is closed
    and bounded, and since furthermore the function \(\|A \mathbf{w}_1\|^2\) is continuous
    in \(\mathbf{w}_1\)), there is at least one solution.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '这里 \(\arg\max\) 表示 \(\mathbf{v}_1\) 是一个向量 \(\mathbf{w}_1\)，它实现了最大值。请注意，可能存在多个这样的
    \(\mathbf{w}_1\)，因此右侧是一个包含所有这些解的集合。根据*极值定理*（因为集合 \(\{\mathbf{w}_1 : \|\mathbf{w}_1\|
    = 1\}\) 是闭集且有界，并且由于函数 \(\|A \mathbf{w}_1\|^2\) 在 \(\mathbf{w}_1\) 上是连续的），至少存在一个解。'
- en: '*Proof:* For general \(k\), we are looking for an orthonormal list \(\mathbf{w}_1,\ldots,\mathbf{w}_k\)
    of length \(k\) that maximizes'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 对于一般的 \(k\)，我们正在寻找一个长度为 \(k\) 的正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\)，它使最大化'
- en: \[\begin{align*} \sum_{i=1}^n \left\|\sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 &= \sum_{i=1}^n \sum_{j=1}^k \langle
    \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle ^2\\ &= \sum_{j=1}^k \left(\sum_{i=1}^n
    (\boldsymbol{\alpha}_i^T \mathbf{w}_j)^2\right)\\ &= \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \end{align*}\]
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \left\|\sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{w}_j \rangle \,\mathbf{w}_j\right\|^2 &= \sum_{i=1}^n \sum_{j=1}^k \langle
    \boldsymbol{\alpha}_i, \mathbf{w}_j \rangle ^2\\ &= \sum_{j=1}^k \left(\sum_{i=1}^n
    (\boldsymbol{\alpha}_i^T \mathbf{w}_j)^2\right)\\ &= \sum_{j=1}^k \|A \mathbf{w}_j\|^2
    \end{align*}\]
- en: where \(\mathcal{Z}\) is the subspace spanned by \(\mathbf{w}_1,\ldots,\mathbf{w}_k\).
    On the second line, we used the *Properties of Orthonormal Lists*. That proves
    the claim. \(\square\)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathcal{Z}\) 是由 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) 张成的子空间。在第二行，我们使用了*正交归一列表的性质*。这证明了该命题。\(\square\)
- en: We show next that a simple algorithm solves this problem.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将展示一个简单的算法可以解决这个问题。
- en: '**A greedy algorithm** \(\idx{greedy algorithm}\xdi\) Remarkably, the problem
    admits a greedy solution. Before discussing this solution, we take a small detour
    and give a classical example. Indeed, [greedy approaches](https://en.wikipedia.org/wiki/Greedy_algorithm)
    are a standard algorithmic tool for optimization problems. This is how Wikipedia
    describes them:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪算法** \(\idx{greedy algorithm}\xdi\) 值得注意的是，这个问题接受一个贪婪解。在讨论这个解之前，我们先稍微偏离一下，给出一个经典例子。确实，[贪婪方法](https://en.wikipedia.org/wiki/Greedy_algorithm)是优化问题的一个标准算法工具。维基百科这样描述它们：'
- en: A greedy algorithm is any algorithm that follows the problem-solving heuristic
    of making the locally optimal choice at each stage. In many problems, a greedy
    strategy does not produce an optimal solution, but a greedy heuristic can yield
    locally optimal solutions that approximate a globally optimal solution in a reasonable
    amount of time.
  id: totrans-320
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 贪婪算法是任何遵循在每个阶段做出局部最优选择的解决问题的启发式算法。在许多问题中，贪婪策略不会产生最优解，但贪婪启发式可以在合理的时间内产生局部最优解，这些解可以近似全局最优解。
- en: '**Figure:** A thief in an antique shop (*Credit:* Made with Gemini; and here
    is your reminder that AI generation of images still has a long way to go…)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** 古董店中的小偷（*来源：Gemini 制作；这里也是你提醒，AI 生成图像还有很长的路要走…）'
- en: '![A thief in an antique shop](../Images/523187bb9f3dd58102ed1bcdc7aa179f.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![古董店中的小偷](../Images/523187bb9f3dd58102ed1bcdc7aa179f.png)'
- en: \(\bowtie\)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**EXAMPLE:** Suppose you are thief and you broke into an antique shop at night.
    (*Legal disclaimer:* The greedy algorithm should be applied to legitimate resource
    allocation problems only.) You cannot steal every item in the store. You have
    estimated that you can carry 10 lbs worth of merchandise, and still run fast enough
    to get away. Suppose that there are 4 items of interest with the following weights
    and values'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 假设你是一名小偷，在夜间闯入了一家古董店。（*法律免责声明：* 贪婪算法仅应用于合法的资源分配问题。）你不能偷走店里所有的物品。你估计你可以携带价值
    10 磅的货物，并且还能跑得足够快以逃脱。假设有 4 件感兴趣的物品，其重量和价值如下'
- en: '| Item | Weight (lbs) | Value ($) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 项目 | 重量（磅） | 价值（美元） |'
- en: '| --- | --- | --- |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 8 | 1600 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8 | 1600 |'
- en: '| 2 | 6 | 1100 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 6 | 1100 |'
- en: '| 3 | 4 | 700 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 700 |'
- en: '| 4 | 1 | 100 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 100 |'
- en: There is exactly one of each item. Which items do you take? The siren is blaring,
    and you cannot try every combination. A quick scheme is to first pick the item
    of greatest value, i.e., Item 1\. Now your bag has 8 lbs of merchandise in it.
    Then you consider the remaining items and choose whichever has highest value among
    those that still fit, i.e., those that are 2 lbs or lighter. That leaves only
    one choice, Item 4\. Then you go – with a total value of 1700.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 每种物品恰好只有一个。你会选择哪些？警报声此起彼伏，你无法尝试每一种组合。一个快速的方法是首先选择价值最高的物品，即物品1。现在你的包里已经有8磅的商品了。然后，考虑剩余的物品，并选择其中价值最高的那些，即那些重量为2磅或更轻的物品。这仅剩下一种选择，即物品4。然后你继续前进——总价值为1700。
- en: 'This is called a greedy or myopic strategy, because you chose the first item
    to maximize your profit without worrying about the constraints it imposes on future
    choice. Indeed, in this case, there is a better combination: you could have picked
    Items 2 and 3 with a total value of 1800.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为贪婪或近视策略，因为你选择了第一个物品以最大化你的利润，而不考虑它对未来选择的约束。实际上，在这种情况下，有一个更好的组合：你可以选择物品2和3，总价值为1800。
- en: Other greedy schemes are possible here. A slightly more clever approach is to
    choose items of high *value per unit weight*, rather than considering value alone.
    But, that would not make a difference in this particular example (Try it!). \(\lhd\)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，还有其他的贪婪方案是可能的。一个稍微聪明一点的方法是选择单位重量价值高的物品，而不仅仅是考虑价值。但是，在这个特定的例子中，这不会产生影响（试试看！）\(\lhd\)
- en: Going back to the approximating subspace problem, we derive a greedy solution
    for it. Recall that we are looking for a solution to
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 回到逼近子空间问题，我们为其推导出一个贪婪解。回想一下，我们正在寻找一个解来
- en: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2
    + \cdots + \|A \mathbf{w}_k\|^2 \]
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{w}_1,\ldots,\mathbf{w}_k} \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2
    + \cdots + \|A \mathbf{w}_k\|^2 \]
- en: over all orthonormal lists \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) of length \(k\).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有长度为 \(k\) 的正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\) 上。
- en: In a greedy approach, we first solve for \(\mathbf{w}_1\) by itself, without
    worrying about constraints it will impose on the next steps. That is, we compute
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在贪婪方法中，我们首先单独求解 \(\mathbf{w}_1\)，而不考虑它将对下一步施加的约束。也就是说，我们计算
- en: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}_1\|^2:\|\mathbf{w}_1\| = 1\}. \]
- en: As indicated before, by the *Extreme Value Theorem*, such a \(\mathbf{v}_1\)
    exists, but may not be unique (in which case we pick an arbitrary one). Then,
    fixing \(\mathbf{w}_1 = \mathbf{v}_1\), we consider all unit vectors \(\mathbf{w}_2\)
    orthogonal to \(\mathbf{v}_1\) and maximize the contribution of \(\mathbf{w}_2\)
    to the objective function. That is, we solve
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，根据*极值定理*，这样的 \(\mathbf{v}_1\) 存在，但可能不是唯一的（在这种情况下，我们选择任意一个）。然后，固定 \(\mathbf{w}_1
    = \mathbf{v}_1\)，我们考虑所有与 \(\mathbf{v}_1\) 正交的单位向量 \(\mathbf{w}_2\)，并最大化 \(\mathbf{w}_2\)
    对目标函数的贡献。也就是说，我们解决
- en: \[ \mathbf{v}_2\in \arg\max \{\|A \mathbf{w}_2\|^2 :\|\mathbf{w}_2\| = 1,\ \langle
    \mathbf{w}_2, \mathbf{v}_1 \rangle = 0\}. \]
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_2\in \arg\max \{\|A \mathbf{w}_2\|^2 :\|\mathbf{w}_2\| = 1,\ \langle
    \mathbf{w}_2, \mathbf{v}_1 \rangle = 0\}. \]
- en: Again, such a \(\mathbf{v}_2\) exists by the *Extreme Value Theorem*. Then proceeding
    by induction, for each \(i = 3, \ldots, k\), we compute
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，根据*极值定理*，这样的 \(\mathbf{v}_2\) 存在。然后通过归纳法，对于每个 \(i = 3, \ldots, k\)，我们计算
- en: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \
    \langle \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}. \]
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \
    \langle \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}. \]
- en: A different way to write the constraint is
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表达约束的方法是
- en: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \mathbf{w}_i
    \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathrm{v}_{i-1})^\perp\}. \]
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_i\in \arg\max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \mathbf{w}_i
    \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathrm{v}_{i-1})^\perp\}. \]
- en: While it is clear that, after \(k\) steps, this procedure constructs an orthonormal
    set of size \(k\), it is far from obvious that it maximizes \(\sum_{j=1}^k \|A
    \mathbf{v}_j\|^2\) over all such sets. Remarkably it does. The claim – which requires
    a proof – is that the best \(k\)-dimensional approximating subspace is obtained
    by finding the best \(1\)-dimensional subspace, then the best \(1\)-dimensional
    subspace orthogonal to the first one, and so on. This follows from the next theorem.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很明显，经过 \(k\) 步后，这个程序构建了一个大小为 \(k\) 的正交集，但它远非显而易见的是它最大化了 \(\sum_{j=1}^k \|A
    \mathbf{v}_j\|^2\) 在所有这样的集合上。令人惊讶的是它确实做到了。这个断言——需要证明——是，通过找到最佳 \(1\) 维子空间，然后找到第一个子空间正交的最佳
    \(1\) 维子空间，以此类推，可以得到最佳的 \(k\) 维逼近子空间。这来自于下一个定理。
- en: '**THEOREM** **(Greedy Finds Best Subspace)** \(\idx{greedy finds best subspace
    theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with rows \(\boldsymbol{\alpha}_i^T\),
    \(i=1,\ldots,n\). For any \(k \leq m\), let \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    be a greedy sequence as constructed above. Then \(\mathcal{Z}^* = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\)
    is a solution to the minimization problem'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(贪婪寻找最佳子空间)** \(\idx{greedy finds best subspace theorem}\xdi\) 设 \(A
    \in \mathbb{R}^{n \times m}\) 是一个矩阵，其行是 \(\boldsymbol{\alpha}_i^T\)，\(i=1,\ldots,n\)。对于任何
    \(k \leq m\)，设 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 是如上所述构建的贪婪序列。那么 \(\mathcal{Z}^*
    = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) 是最小化问题的解'
- en: \[ \min \left\{ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\
    :\ \text{$\mathcal{Z}$ is a linear subspace of dimension $k$} \right\}. \]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min \left\{ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2\
    :\ \text{$\mathcal{Z}$ 是一个维度为 $k$ 的线性子空间} \right\}. \]
- en: \(\sharp\)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Beyond the potential computational advantage of solving several lower-dimensional
    problems rather one larger-dimensional one, a greedy sequence has a more subtle
    property that is powerful. It allows us to solve the problem for all choices \(k\)
    of target dimension *simultaneously*. To explain, note that the largest \(k\)
    value, i.e. \(k=m\), leads to a trivial problem. Indeed, the data points \(\boldsymbol{\alpha}_i\),
    \(i=1,\ldots,n\), already lie in an \(m\)-dimensional linear subspace, \(\mathbb{R}^m\)
    itself. So we can take \(\mathcal{Z} = \mathbb{R}^m\), and we have an objective
    value of
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决多个低维问题而不是一个高维问题的潜在计算优势之外，贪婪序列还有一个更微妙但强大的性质。它允许我们同时解决所有目标维度 \(k\) 的选择问题。为了解释，请注意，最大的
    \(k\) 值，即 \(k=m\)，导致了一个平凡问题。确实，数据点 \(\boldsymbol{\alpha}_i\)，\(i=1,\ldots,n\)，已经位于一个
    \(m\) 维线性子空间中，即 \(\mathbb{R}^m\) 本身。因此，我们可以取 \(\mathcal{Z} = \mathbb{R}^m\)，并且我们有一个目标值
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \boldsymbol{\alpha}_i\|^2 = 0, \]
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_{\mathcal{Z}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \boldsymbol{\alpha}_i\|^2 = 0, \]
- en: which clearly cannot be improved. So any orthonormal basis of \(\mathbb{R}^m\)
    will do. Say \(\mathbf{e}_1,\ldots,\mathbf{e}_m\).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然无法改进。因此，任何 \(\mathbb{R}^m\) 的正交基都将适用。比如说 \(\mathbf{e}_1,\ldots,\mathbf{e}_m\)。
- en: On the other hand, a greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_m\) has
    a very special property. For any \(k \leq m\), the *truncation* \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    solves the approximating subspace problem in \(k\) dimensions. That follows immediately
    from the *Greedy Finds Best Subspace Theorem*. The basis \(\mathbf{e}_1,\ldots,\mathbf{e}_m\)
    (or any old basis of \(\mathbb{R}^m\) for that matter) does *not* have this property.
    The idea of truncation is very useful and plays an important role in many data
    science applications; we will come back to it later in this section and the next
    one.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一个贪婪序列 \(\mathbf{v}_1,\ldots,\mathbf{v}_m\) 具有一个非常特殊的性质。对于任何 \(k \leq m\)，截断序列
    \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 在 \(k\) 维度上解决了逼近子空间问题。这直接来自于 *贪婪寻找最佳子空间定理*。基
    \(\mathbf{e}_1,\ldots,\mathbf{e}_m\)（或者任何旧的 \(\mathbb{R}^m\) 的基）并不具备这个性质。截断的想法非常有用，在许多数据科学应用中扮演着重要的角色；我们将在本节和下一节中稍后回到它。
- en: We sketch the proof of a weaker claim via the *Spectral Theorem*, an approach
    which reveals additional structure in the solution.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 *谱定理* 简要描述了一个较弱断言的证明，这种方法揭示了解决方案中的额外结构。
- en: We re-write the objective function as
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将目标函数重新写为
- en: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T A^T A \mathbf{w}_j
    \]
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T A^T A \mathbf{w}_j
    \]
- en: and we observe that \(A^T A \in \mathbb{R}^{m \times m}\) is a square, symmetric
    matrix (Why?). It is also positive, semidefinite (Why?). Hence, by the *Spectral
    Theorem*, the matrix \(A^T A\) has \(m\) orthonormal eigenvectors \(\mathbf{q}_1,
    \ldots, \mathbf{q}_m \in \mathbb{R}^m\) with corresponding real eigenvalues \(\lambda_1
    \geq \lambda_2 \geq \cdots \geq \lambda_m \geq 0\). This ordering of the eigenvalues
    will play a critical role. Moreover
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们观察到 \(A^T A \in \mathbb{R}^{m \times m}\) 是一个平方、对称矩阵（为什么？）。它也是正定半定的（为什么？）。因此，根据**谱定理**，矩阵
    \(A^T A\) 有 \(m\) 个正交归一特征向量 \(\mathbf{q}_1, \ldots, \mathbf{q}_m \in \mathbb{R}^m\)，对应于实特征值
    \(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_m \geq 0\)。这种特征值的排序将发挥关键作用。此外
- en: \[ A^T A = \sum_{i=1}^m \lambda_i \mathbf{q}_i \mathbf{q}_i^T. \]
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A = \sum_{i=1}^m \lambda_i \mathbf{q}_i \mathbf{q}_i^T. \]
- en: Plugging this in the objective we get
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 将此代入目标函数中，我们得到
- en: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T \left(\sum_{i=1}^m
    \lambda_i \mathbf{q}_i \mathbf{q}_i^T\right) \mathbf{w}_j = \sum_{j=1}^k \sum_{i=1}^m
    \lambda_i (\mathbf{w}_j^T\mathbf{q}_i)^2. \]
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{j=1}^k \mathbf{w}_j^T \left(\sum_{i=1}^m
    \lambda_i \mathbf{q}_i \mathbf{q}_i^T\right) \mathbf{w}_j = \sum_{j=1}^k \sum_{i=1}^m
    \lambda_i (\mathbf{w}_j^T\mathbf{q}_i)^2. \]
- en: Here is the claim. While a greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    is not in general unique, one can always choose \(\mathbf{v}_i = \mathbf{q}_i\)
    for all \(i\). Moreover that particular choice indeed solves the \(k\)-dimensional
    best approximating subspace problem. We restrict ourselves to the case \(k = 2\).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们要提出的主张。虽然贪婪序列 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 在一般情况下不是唯一的，但人们总是可以选择对于所有
    \(i\)，\(\mathbf{v}_i = \mathbf{q}_i\)。此外，这种特定的选择确实解决了 \(k\) 维最佳逼近子空间问题。我们限制自己考虑
    \(k = 2\) 的情况。
- en: '***Eigenvectors form a greedy sequence:*** Recall that \(\mathbf{v}_1\) maximizes
    \(\|A \mathbf{w}_1\|\) over all unit vectors \(\mathbf{w}_1\). Now note that,
    expanding over the eigenvectors (which form an orthonormal basis), we have'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '***特征向量形成贪婪序列：*** 回想一下，\(\mathbf{v}_1\) 在所有单位向量 \(\mathbf{w}_1\) 中最大化 \(\|A
    \mathbf{w}_1\|\)。现在请注意，展开到特征向量（它们形成一个正交基），我们有'
- en: \[ \|A \mathbf{w}_1\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    \]
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}_1\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    \]
- en: and
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
- en: Writing \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\), this boils down to maximizing
    \(\sum_{i=1}^m \lambda_i x_i\) subject to the constraints \(\sum_{i=1}^m x_i =
    1\) and \(x_i \geq 0\) for all \(i\). But, under the constraints and the assumption
    on the ordering of the eigenvalues,
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 写作 \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\)，这归结为在约束 \(\sum_{i=1}^m x_i = 1\)
    和 \(x_i \geq 0\) 对于所有 \(i\) 的条件下，最大化 \(\sum_{i=1}^m \lambda_i x_i\)。但是，在约束和关于特征值排序的假设下，
- en: \[ \sum_{i=1}^m \lambda_i x_i \leq \lambda_1 \sum_{i=1}^m x_i = \lambda_1. \]
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^m \lambda_i x_i \leq \lambda_1 \sum_{i=1}^m x_i = \lambda_1. \]
- en: Formally, we have shown that \(\|A \mathbf{w}_1\|^2 \leq \lambda_1\), for any
    unit vector \(\mathbf{w}_1\). Now, note that this upper bound is actually achieved
    by taking \(\mathbf{v}_1 = \mathbf{w}_1 = \mathbf{q}_1\), which corresponds to
    \(\mathbf{x} = (x_1,\ldots,x_m) = \mathbf{e}_1\).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们已经证明了对于任何单位向量 \(\mathbf{w}_1\)，\(\|A \mathbf{w}_1\|^2 \leq \lambda_1\)。现在，请注意，这个上界实际上是通过取
    \(\mathbf{v}_1 = \mathbf{w}_1 = \mathbf{q}_1\) 来实现的，这对应于 \(\mathbf{x} = (x_1,\ldots,x_m)
    = \mathbf{e}_1\)。
- en: Given that choice, the vector \(\mathbf{v}_2\) maximizes \(\|A \mathbf{w}_2\|\)
    over all unit vectors \(\mathbf{w}_2\) such that further \(\mathbf{w}_2^T\mathbf{v}_1
    = \mathbf{w}_2^T\mathbf{q}_1 = 0\), where this time
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定选择的情况下，向量 \(\mathbf{v}_2\) 在所有满足进一步 \(\mathbf{w}_2^T\mathbf{v}_1 = \mathbf{w}_2^T\mathbf{q}_1
    = 0\) 的单位向量 \(\mathbf{w}_2\) 中最大化 \(\|A \mathbf{w}_2\|\)，其中这次
- en: \[ \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2
    = \sum_{i=2}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2
    = \sum_{i=2}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
- en: and
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = \sum_{i=2}^m
    (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = \sum_{i=2}^m
    (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
- en: In both equations above, we used the orthogonality constraint. This reduces
    to the previous problem without the term depending on \(\mathbf{q}_1\). The solution
    is otherwise the same, i.e., the optimal objective is \(\lambda_2\) and is achieved
    by taking \(\mathbf{v}_2 = \mathbf{w}_2 = \mathbf{q}_2\).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两个方程中，我们使用了正交约束。这简化为没有依赖于 \(\mathbf{q}_1\) 的项的先前问题。解决方案否则相同，即最优目标函数是 \(\lambda_2\)，通过取
    \(\mathbf{v}_2 = \mathbf{w}_2 = \mathbf{q}_2\) 来实现。
- en: '***Eigenvectors solve the approximating subspace problem:*** The approximating
    subspace problem for \(k = 2\) involves maximizing'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征向量解决了逼近子空间问题：** 当 \(k = 2\) 时，逼近子空间问题涉及最大化'
- en: \[ \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    + \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2 = \sum_{i=1}^m \lambda_i (\mathbf{w}_1^T\mathbf{q}_i)^2
    + \sum_{i=1}^m \lambda_i (\mathbf{w}_2^T\mathbf{q}_i)^2 \]
- en: over orthonormal lists \(\mathbf{w}_1, \mathbf{w}_2\). In particular, we require
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在正交归一列表 \(\mathbf{w}_1, \mathbf{w}_2\) 上。特别是，我们需要
- en: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_1\|^2 = \sum_{i=1}^m (\mathbf{w}_1^T\mathbf{q}_i)^2 = 1. \]
- en: and
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{w}_2\|^2 = \sum_{i=1}^m (\mathbf{w}_2^T\mathbf{q}_i)^2 = 1. \]
- en: Moreover, for each \(i\), by definition of the orthogonal projection on the
    subspace \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1, \mathbf{w}_2)\) and the *Properties
    of Orhtonormal Lists*
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于每个 \(i\)，根据子空间 \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1, \mathbf{w}_2)\)
    上的正交投影和**正交归一列表的性质**
- en: \[ (\mathbf{w}_1^T\mathbf{q}_i)^2 + (\mathbf{w}_2^T\mathbf{q}_i)^2 = \|\mathrm{proj}_{\mathcal{W}}
    \mathbf{q}_i\|^2 \leq \|\mathbf{q}_i\|^2 = 1. \]
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{w}_1^T\mathbf{q}_i)^2 + (\mathbf{w}_2^T\mathbf{q}_i)^2 = \|\mathrm{proj}_{\mathcal{W}}
    \mathbf{q}_i\|^2 \leq \|\mathbf{q}_i\|^2 = 1. \]
- en: (Prove the inequality!) Write \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\) and \(y_i
    = (\mathbf{w}_2^T\mathbf{q}_i)^2\). The objective function can be written as \(\sum_{i=1}^m
    \lambda_i (x_i + y_i)\) and the constraints we have derived are \(\sum_{i=1}^m
    x_i = \sum_{i=1}^m y_i = 1\) and \(x_i + y_i \leq 1\) for all \(i\). Also clearly
    \(x_i, y_i \geq 0\) for all \(i\). So
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: （证明不等式！）将 \(x_i = (\mathbf{w}_1^T\mathbf{q}_i)^2\) 和 \(y_i = (\mathbf{w}_2^T\mathbf{q}_i)^2\)。目标函数可以写成
    \(\sum_{i=1}^m \lambda_i (x_i + y_i)\)，我们推导出的约束条件是 \(\sum_{i=1}^m x_i = \sum_{i=1}^m
    y_i = 1\) 和 \(x_i + y_i \leq 1\) 对所有 \(i\) 成立。显然，\(x_i, y_i \geq 0\) 对所有 \(i\)
    成立。因此
- en: \[\begin{align*} \sum_{i=1}^m \lambda_i (x_i + y_i) &= \lambda_1 (x_1 + y_1)
    + \lambda_2 (x_2 + y_2) + \sum_{i=3}^m \lambda_i (x_i + y_i)\\ &\leq \lambda_1
    (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \sum_{i=3}^m (x_i + y_i)\\ &=
    \lambda_1 (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \left([1 - x_1 - x_2]
    + [1 - y_1 - y_2]\right)\\ &= (\lambda_1 - \lambda_2) (x_1 + y_1) + (\lambda_2
    - \lambda_2) (x_2 + y_2) + 2 \lambda_2\\ &\leq \lambda_1 - \lambda_2 + 2 \lambda_2\\
    &= \lambda_1 + \lambda_2. \end{align*}\]
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^m \lambda_i (x_i + y_i) &= \lambda_1 (x_1 + y_1)
    + \lambda_2 (x_2 + y_2) + \sum_{i=3}^m \lambda_i (x_i + y_i)\\ &\leq \lambda_1
    (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \sum_{i=3}^m (x_i + y_i)\\ &=
    \lambda_1 (x_1 + y_1) + \lambda_2 (x_2 + y_2) + \lambda_2 \left([1 - x_1 - x_2]
    + [1 - y_1 - y_2]\right)\\ &= (\lambda_1 - \lambda_2) (x_1 + y_1) + (\lambda_2
    - \lambda_2) (x_2 + y_2) + 2 \lambda_2\\ &\leq \lambda_1 - \lambda_2 + 2 \lambda_2\\
    &= \lambda_1 + \lambda_2. \end{align*}\]
- en: Formally, we have shown that \(\|A \mathbf{w}_1\|^2 + \|A \mathbf{w}_2\|^2 \leq
    \lambda_1 + \lambda_2\) for any orthonormal list \(\mathbf{w}_1, \mathbf{w}_2\).
    That upper bound is achieved by taking \(\mathbf{w}_1 = \mathbf{q}_1\) and \(\mathbf{w}_2
    = \mathbf{q}_2\), proving the claim.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们已经证明了对于任何正交归一列表 \(\mathbf{w}_1, \mathbf{w}_2\)，\(\|A \mathbf{w}_1\|^2
    + \|A \mathbf{w}_2\|^2 \leq \lambda_1 + \lambda_2\)。这个上界通过取 \(\mathbf{w}_1 = \mathbf{q}_1\)
    和 \(\mathbf{w}_2 = \mathbf{q}_2\) 来实现，从而证明了该命题。
- en: '**KNOWLEDGE CHECK:** Proceed by induction to show that the claim holds for
    any \(k\). \(\checkmark\)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查：** 通过归纳法来证明该命题对任何 \(k\) 都成立。 \(\checkmark\)'
- en: Note that we have not entirely solved the best approximating subspace problem
    from a computational point of view, as we have not given an explicit procedure
    to construct a solution to the lower-dimensional subproblems, i.e., construct
    the eigenvectors. We have only shown that the solutions exist and have the right
    properties. We will take care of computational issues later in this chapter.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从计算的角度来看，我们并没有完全解决最佳逼近子空间问题，因为我们没有给出一个明确的构造解的步骤来构建低维子问题的解，即构造特征向量。我们只证明了这些解存在并且具有正确的性质。我们将在本章的后面部分处理计算问题。
- en: 4.3.2\. From approximating subspaces to the SVD[#](#from-approximating-subspaces-to-the-svd
    "Link to this heading")
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3.2\. 从逼近子空间到奇异值分解[#](#from-approximating-subspaces-to-the-svd "链接到本标题")
- en: While solving the approximating subspace problem in the previous section, we
    derived the building blocks of a matrix factorization that has found many applications,
    the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition).
    In this section, we define the SVD formally. We describe a simple method to compute
    it in the next section, where we also return to the application to dimensionality
    reduction.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节解决逼近子空间问题时，我们推导出了矩阵分解的构建块，这种分解已经找到了许多应用，即 [奇异值分解 (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition)。在本节中，我们正式定义
    SVD。我们将在下一节描述计算它的简单方法，同时我们也将回到降维的应用。
- en: '**Definition and existence of the SVD** We now come to our main definition.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义和奇异值分解的存在性** 我们现在来到我们的主要定义。'
- en: '**DEFINITION** **(Singular Value Decomposition)** \(\idx{singular value decomposition}\xdi\)
    Let \(A \in \mathbb{R}^{n\times m}\) be a matrix. A singular value decomposition
    (SVD) of \(A\) is a matrix factorization'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(奇异值分解)** \(\idx{奇异值分解}\xdi\) 设 \(A \in \mathbb{R}^{n\times m}\) 是一个矩阵。\(A\)
    的奇异值分解是一个矩阵分解'
- en: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: where the columns of \(U \in \mathbb{R}^{n \times r}\) and those of \(V \in
    \mathbb{R}^{m \times r}\) are orthonormal, and \(\Sigma \in \mathbb{R}^{r \times
    r}\) is a diagonal matrix. Here the \(\mathbf{u}_j\)s are the columns of \(U\)
    and are referred to as left singular vectors\(\idx{singular vector}\xdi\). Similarly
    the \(\mathbf{v}_j\)s are the columns of \(V\) and are referred to as right singular
    vectors. The \(\sigma_j\)s, which are positive and in non-increasing order, i.e.,
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(U \in \mathbb{R}^{n \times r}\) 的列和 \(V \in \mathbb{R}^{m \times r}\) 的列是正交归一的，且
    \(\Sigma \in \mathbb{R}^{r \times r}\) 是一个对角矩阵。这里 \(\mathbf{u}_j\) 是 \(U\) 的列，被称为左奇异向量\(\idx{奇异向量}\xdi\)。同样地，\(\mathbf{v}_j\)
    是 \(V\) 的列，被称为右奇异向量。\(\sigma_j\) 是正数，且按非递增顺序排列，即，
- en: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0, \]
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0, \]
- en: are the diagonal elements of \(\Sigma\) and are referred to as singular values\(\idx{singular
    value}\xdi\). \(\natural\)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 是 \(\Sigma\) 的对角元素，被称为奇异值\(\idx{奇异值}\xdi\)。 \(\natural\)
- en: To see where the equality \(U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) comes from, we break it up into two steps.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到等式 \(U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) 的来源，我们将其分解为两个步骤。
- en: First note that the matrix product \(U \Sigma\) has columns \(\sigma_1 \mathbf{u}_1,\ldots,\sigma_r
    \mathbf{u}_r\).
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先注意矩阵乘积 \(U \Sigma\) 的列是 \(\sigma_1 \mathbf{u}_1,\ldots,\sigma_r \mathbf{u}_r\)。
- en: The rows of \(V^T\) are the columns of \(V\) as row vectors.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(V^T\) 的行是 \(V\) 作为行向量的列。
- en: In terms of outer products, the matrix product \(U \Sigma V^T = (U \Sigma) V^T\)
    is the sum of the outer products of the columns of \(U \Sigma\) and of the rows
    of \(V^T\) (i.e., the columns of \(V\) as row vectors).
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从外积的角度来看，矩阵乘积 \(U \Sigma V^T = (U \Sigma) V^T\) 是 \(U \Sigma\) 的列和外积以及 \(V^T\)
    的行（即 \(V\) 的列向量）的外积之和。
- en: That proves the equality.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这就证明了等式。
- en: '**KNOWLEDGE CHECK:** Let \(A \in \mathbb{R}^{n\times m}\) be a matrix with
    SVD'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 设 \(A \in \mathbb{R}^{n\times m}\) 是一个具有奇异值分解的矩阵'
- en: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T. \]
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T. \]
- en: Which statement is true in general?
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个陈述在一般情况下是正确的？
- en: a) \(\mathrm{col}(A) = \mathrm{col}(V)\)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathrm{col}(A) = \mathrm{col}(V)\)
- en: b) \(\mathrm{col}(A) = \mathrm{col}(V^T)\)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathrm{col}(A) = \mathrm{col}(V^T)\)
- en: c) \(\mathrm{col}(A) = \mathrm{col}(U)\)
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathrm{col}(A) = \mathrm{col}(U)\)
- en: d) \(\mathrm{col}(A) = \mathrm{col}(U^T)\)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathrm{col}(A) = \mathrm{col}(U^T)\)
- en: e) \(\mathrm{col}(A) = \mathrm{col}(\Sigma)\)
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: e) \(\mathrm{col}(A) = \mathrm{col}(\Sigma)\)
- en: \(\checkmark\)
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: Remarkably, any matrix has an SVD.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，任何矩阵都有一个奇异值分解。
- en: '**THEOREM** **(Existence of an SVD)** \(\idx{existence of an SVD}\xdi\) Any
    matrix \(A \in \mathbb{R}^{n\times m}\) has a singular value decomposition. \(\sharp\)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(奇异值分解的存在性)** \(\idx{存在性}\xdi\) 任何矩阵 \(A \in \mathbb{R}^{n\times m}\)
    都有一个奇异值分解。 \(\sharp\)'
- en: We give a proof via the *Spectral Theorem*.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 *谱定理* 给出证明。
- en: '*The construction:* Let \(A \in \mathbb{R}^{n \times m}\) and recall that \(A^T
    A\) is symmetric and positive semidefinite. Hence the latter has a spectral decomposition'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '*构造方法:* 设 \(A \in \mathbb{R}^{n \times m}\) 并回忆 \(A^T A\) 是对称和正半定的。因此后者有一个谱分解'
- en: \[ A^T A = Q \Lambda Q^T. \]
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A = Q \Lambda Q^T. \]
- en: Order the eigenvalues in non-increasing order \(\lambda_1 \geq \cdots \geq \lambda_m
    \geq 0\). Assume that the eigenvalues \(\lambda_1,\ldots,\lambda_r\) are nonzero
    while \(\lambda_{r+1} = \cdots = \lambda_m = 0\). Let \(\mathbf{q}_1,\ldots,\mathbf{q}_n\)
    be corresponding eigenvectors. Let \(Q_1 \in \mathbb{R}^{m \times r}\) be the
    matrix whose columns are \(\mathbf{q}_1,\ldots,\mathbf{q}_r\) and \(\Lambda_1
    \in \mathbb{R}^{r \times r}\) be the diagonal matrix with \(\lambda_1,\ldots,\lambda_r\)
    on its diagonal. Similarly, let \(Q_2 \in \mathbb{R}^{m \times (m-r)}\) be the
    matrix whose columns are \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\) and \(\Lambda_2
    = \mathbf{0} \in \mathbb{R}^{(m-r) \times (m-r)}\).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征值按非递减顺序排列 \(\lambda_1 \geq \cdots \geq \lambda_m \geq 0\)。假设特征值 \(\lambda_1,\ldots,\lambda_r\)
    是非零的，而 \(\lambda_{r+1} = \cdots = \lambda_m = 0\)。令 \(\mathbf{q}_1,\ldots,\mathbf{q}_n\)
    为相应的特征向量。令 \(Q_1 \in \mathbb{R}^{m \times r}\) 为列向量为 \(\mathbf{q}_1,\ldots,\mathbf{q}_r\)
    的矩阵，且 \(\Lambda_1 \in \mathbb{R}^{r \times r}\) 为对角矩阵，其对角线元素为 \(\lambda_1,\ldots,\lambda_r\)。类似地，令
    \(Q_2 \in \mathbb{R}^{m \times (m-r)}\) 为列向量为 \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\)
    的矩阵，且 \(\Lambda_2 = \mathbf{0} \in \mathbb{R}^{(m-r) \times (m-r)}\)。
- en: The matrix \(A^T A\), which is comprised of all inner products of the data points,
    is known as a Gram matrix.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(A^T A\)，它由所有数据点的内积组成，被称为Gram矩阵。
- en: We are now ready for our main claim. For a diagonal matrix \(D\) with nonnegative
    diagonal entries, we let \(D^{1/2}\) denote the diagonal matrix obtained by taking
    the square root of each diagonal entry. Similarly, when \(D\) has positive diagonal
    entries, we define \(D^{-1/2}\) as the diagonal matrix whose diagonal entries
    are the reciprocals of the square roots of the corresponding diagonal entries
    of \(D\).
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以准备我们的主要论断了。对于一个对角矩阵 \(D\)，其对角线元素非负，我们让 \(D^{1/2}\) 表示通过对角线每个元素开平方得到的对角矩阵。类似地，当
    \(D\) 的对角线元素为正时，我们定义 \(D^{-1/2}\) 为对角线元素是 \(D\) 对应对角线元素平方根的倒数的对角矩阵。
- en: '**THEOREM** **(SVD via Spectral Decomposition)** \(\idx{SVD via spectral decomposition}\xdi\)
    Let \(A \in \mathbb{R}^{n \times m}\) and let \(Q_1, \Lambda_1\) be as above.
    Define'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(通过谱分解的SVD)** \(\idx{SVD via spectral decomposition}\xdi\) 设 \(A \in
    \mathbb{R}^{n \times m}\) 且 \(Q_1, \Lambda_1\) 如上所述。定义'
- en: \[ U = A Q_1 \Lambda_1^{-1/2} \quad \text{and} \quad \Sigma = \Lambda_1^{1/2}
    \quad \text{and} \quad V = Q_1. \]
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U = A Q_1 \Lambda_1^{-1/2} \quad \text{和} \quad \Sigma = \Lambda_1^{1/2}
    \quad \text{和} \quad V = Q_1. \]
- en: Then \(A = U \Sigma V^T\) is a singular value decomposition of \(A\). \(\sharp\)
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 \(A = U \Sigma V^T\) 是 \(A\) 的奇异值分解。 \(\sharp\)
- en: '*Proof idea:* Check by hand that all properties of the SVD are satisfied by
    the construction above.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 通过手工检查，验证上述构造满足SVD的所有性质。'
- en: '*Proof:* By construction, the columns of \(V = Q_1\) are orthonormal. The matrix
    \(\Sigma = \Lambda_1^{1/2}\) is diagonal and, because \(A^T A\) is positive semidefinite,
    the eigenvalues are non-negative. So it remains to prove two things: that the
    columns of \(U\) are orthonormal and, finally, that \(A = U \Sigma V^T\).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过构造，\(V = Q_1\) 的列向量是正交归一的。矩阵 \(\Sigma = \Lambda_1^{1/2}\) 是对角矩阵，并且因为
    \(A^T A\) 是正半定矩阵，所以特征值是非负的。因此，需要证明两件事：\(U\) 的列向量是正交归一的，最后，\(A = U \Sigma V^T\)。'
- en: '**KNOWLEDGE CHECK:** Prove that'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 证明以下内容'
- en: a) \(A^T A Q_1 = Q_1 \Lambda_1\) and \(A^T A Q_2 = Q_2 \Lambda_2 = \mathbf{0}\),
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(A^T A Q_1 = Q_1 \Lambda_1\) 和 \(A^T A Q_2 = Q_2 \Lambda_2 = \mathbf{0}\)，
- en: b) \(Q_1 Q_1^T + Q_2 Q_2^T = I_{m \times m}\).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(Q_1 Q_1^T + Q_2 Q_2^T = I_{m \times m}\)。
- en: \(\checkmark\)
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: '**LEMMA** **(Step 1)** The columns of \(U\) are orthonormal. \(\flat\)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(步骤1)** \(U\) 的列向量是正交归一的。 \(\flat\)'
- en: '*Proof:* By direct computation,'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过直接计算，'
- en: \[ U^T U = (A Q_1 \Lambda_1^{-1/2})^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2}
    Q_1^T A^T A Q_1 \Lambda_1^{-1/2}. \]
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U^T U = (A Q_1 \Lambda_1^{-1/2})^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2}
    Q_1^T A^T A Q_1 \Lambda_1^{-1/2}. \]
- en: Because the columns of \(Q_1\) are eigenvectors of \(A^T A\), we have that \(A^T
    A Q_1 = Q_1 \Lambda_1\). Further those eigenvectors are orthonormal so that \(Q_1^T
    Q_1 = I_{r \times r}\). Plugging above and simplifying gives
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(Q_1\) 的列向量是 \(A^T A\) 的特征向量，所以我们有 \(A^T A Q_1 = Q_1 \Lambda_1\)。进一步，这些特征向量是正交归一的，因此
    \(Q_1^T Q_1 = I_{r \times r}\)。将上述结果代入并简化得到
- en: \[ \Lambda_1^{-1/2} Q_1^T A^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} Q_1^T
    Q_1 \Lambda_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} I_{r \times r} \Lambda_1 \Lambda_1^{-1/2}
    = I_{r \times r}, \]
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \Lambda_1^{-1/2} Q_1^T A^T A Q_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} Q_1^T
    Q_1 \Lambda_1 \Lambda_1^{-1/2} = \Lambda_1^{-1/2} I_{r \times r} \Lambda_1 \Lambda_1^{-1/2}
    = I_{r \times r}, \]
- en: as claimed. \(\square\)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。 \(\square\)
- en: '**LEMMA** **(Step 2)** It holds that \(A = U \Sigma V^T\). \(\flat\)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(步骤2)** 成立 \(A = U \Sigma V^T\)。 \(\flat\)'
- en: '*Proof:* By direct computation, we have'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过直接计算，我们得到'
- en: \[ U \Sigma V^T = A Q_1 \Lambda_1^{-1/2} \Lambda_1^{1/2} Q_1^T = A Q_1 Q_1^T.
    \]
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U \Sigma V^T = A Q_1 \Lambda_1^{-1/2} \Lambda_1^{1/2} Q_1^T = A Q_1 Q_1^T.
    \]
- en: The matrix \(Q_1 Q_1^T\) is an orthogonal projection on the subspace spanned
    by the vectors \(\mathbf{q}_1,\ldots,\mathbf{q}_r\). Similarly, the matrix \(Q_2
    Q_2^T\) is an orthogonal projection on the orthogonal complement (spanned by \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\)).
    Hence \(Q_1 Q_1^T + Q_2 Q_2^T = I_{m \times m}\). Replacing above we get
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(Q_1 Q_1^T\) 是由向量 \(\mathbf{q}_1,\ldots,\mathbf{q}_r\) 张成的子空间上的正交投影。同样，矩阵
    \(Q_2 Q_2^T\) 是由 \(\mathbf{q}_{r+1},\ldots,\mathbf{q}_m\) 张成的正交补空间的正交投影。因此 \(Q_1
    Q_1^T + Q_2 Q_2^T = I_{m \times m}\)。替换上述公式我们得到
- en: \[ U \Sigma V^T = A (I_{n \times n} - Q_2 Q_2^T) = A - A Q_2 Q_2^T. \]
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U \Sigma V^T = A (I_{n \times n} - Q_2 Q_2^T) = A - A Q_2 Q_2^T. \]
- en: Now note that for any \(\mathbf{q}_i\), \(i=r+1,\ldots,m\), we have \(A^T A
    \mathbf{q}_i = \mathbf{0}\), so that \(\mathbf{q}_i^T A^T A \mathbf{q}_i = \|A
    \mathbf{q}_i\|^2 = 0\). That implies that \(A \mathbf{q}_i = \mathbf{0}\) and
    further \(A Q_2 = \mathbf{0}\). Substituting above concludes the proof. \(\square\)
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意到对于任何 \(\mathbf{q}_i\)，\(i=r+1,\ldots,m\)，我们有 \(A^T A \mathbf{q}_i = \mathbf{0}\)，因此
    \(\mathbf{q}_i^T A^T A \mathbf{q}_i = \|A \mathbf{q}_i\|^2 = 0\)。这意味着 \(A \mathbf{q}_i
    = \mathbf{0}\) 并且进一步 \(A Q_2 = \mathbf{0}\)。替换上述公式完成证明。\(\square\)
- en: That concludes the proof of the theorem. \(\square\)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了定理的证明。\(\square\)
- en: We record the following important consequence.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记录以下重要结论。
- en: '**LEMMA** **(SVD and Rank)** \(\idx{SVD and rank lemma}\xdi\) Let \(A \in \mathbb{R}^{n\times
    m}\) have singular value decomposition \(A = U \Sigma V^T\) with \(U \in \mathbb{R}^{n
    \times r}\) and \(V \in \mathbb{R}^{m \times r}\). Then the columns of \(U\) form
    an orthonormal basis of \(\mathrm{col}(A)\) and the columns of \(V\) form an orthonormal
    basis of \(\mathrm{row}(A)\). In particular, the rank of \(A\) is \(r\). \(\flat\)'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(SVD 和秩)** \(\idx{SVD and rank lemma}\xdi\) 设 \(A \in \mathbb{R}^{n\times
    m}\) 有奇异值分解 \(A = U \Sigma V^T\)，其中 \(U \in \mathbb{R}^{n \times r}\) 和 \(V \in
    \mathbb{R}^{m \times r}\)。那么 \(U\) 的列构成 \(\mathrm{col}(A)\) 的正交基，\(V\) 的列构成 \(\mathrm{row}(A)\)
    的正交基。特别是，\(A\) 的秩是 \(r\)。\(\flat\)'
- en: '*Proof idea:* We use the SVD to show that the span of the columns of \(U\)
    is \(\mathrm{col}(A)\), and similarly for \(V\).'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* 我们使用奇异值分解（SVD）来证明 \(U\) 的列的张成是 \(\mathrm{col}(A)\)，对 \(V\) 同样如此。'
- en: '*Proof:* We first prove that any column of \(A\) can be written as a linear
    combination of the columns of \(U\). Indeed, this follows immediately from the
    SVD by noting that for any canonical basis vector \(\mathbf{e}_i \in \mathbb{R}^m\)
    (which produces column \(i\) of \(A\) with \(A \mathbf{e}_i\))'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 我们首先证明 \(A\) 的任意一列可以表示为 \(U\) 的列的线性组合。实际上，这直接从奇异值分解（SVD）得出，注意到对于任何标准基向量
    \(\mathbf{e}_i \in \mathbb{R}^m\)（它产生 \(A\) 的第 \(i\) 列，即 \(A \mathbf{e}_i\))'
- en: \[\begin{align*} A \mathbf{e}_i =\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{e}_i
    = \sum_{j=1}^r (\sigma_j \mathbf{v}_j^T \mathbf{e}_i) \,\mathbf{u}_j. \end{align*}\]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{e}_i =\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{e}_i
    = \sum_{j=1}^r (\sigma_j \mathbf{v}_j^T \mathbf{e}_i) \,\mathbf{u}_j. \end{align*}\]
- en: Vice versa, any column of \(U\) can be written as a linear combination of the
    columns of \(A\). To see this, we use the orthonormality of the \(\mathbf{v}_j\)’s
    and the positivity of the singular values to obtain
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，\(U\) 的任意一列也可以表示为 \(A\) 的列的线性组合。为了看到这一点，我们使用 \(\mathbf{v}_j\) 的正交性和奇异值的正性来获得
- en: \[\begin{align*} A (\sigma_i^{-1} \mathbf{v}_i) = \sigma_i^{-1}\left(\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{v}_i = \sigma_i^{-1} \sum_{j=1}^r
    (\sigma_j \mathbf{v}_j^T \mathbf{v}_i) \,\mathbf{u}_j = \sigma_i^{-1} (\sigma_i
    \mathbf{v}_i^T \mathbf{v}_i) \,\mathbf{u}_i = \mathbf{u}_i. \end{align*}\]
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A (\sigma_i^{-1} \mathbf{v}_i) = \sigma_i^{-1}\left(\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{v}_i = \sigma_i^{-1} \sum_{j=1}^r
    (\sigma_j \mathbf{v}_j^T \mathbf{v}_i) \,\mathbf{u}_j = \sigma_i^{-1} (\sigma_i
    \mathbf{v}_i^T \mathbf{v}_i) \,\mathbf{u}_i = \mathbf{u}_i. \end{align*}\]
- en: That is, \(\mathrm{col}(U) = \mathrm{col}(A)\). We have already shown that the
    columns of \(U\) are orthonormal. Since their span is \(\mathrm{col}(A)\), they
    form an orthonormal basis of it. Applying the same argument to \(A^T\) gives the
    claim for \(V\) (try it!). \(\square\)
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 即，\(\mathrm{col}(U) = \mathrm{col}(A)\)。我们已经证明 \(U\) 的列是正交归一的。由于它们的张成是 \(\mathrm{col}(A)\)，它们构成了它的正交基。将相同的论点应用于
    \(A^T\) 得到 \(V\) 的结论（试试看！）。\(\square\)
- en: '**EXAMPLE:** Let'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '**例：** 设'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: We compute its SVD. In this case it can be done (or guessed) using what we know
    about the SVD. Note first that \(A\) is not invertible. Indeed, its rows are a
    multiple of one another. In particular, they are not linearly independent. In
    fact, that tells us that the rank of \(A\) is \(1\), the dimension of its row
    space. In the rank one case, computing the SVD boils down to writing the matrix
    \(A\) in outer product form
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算它的奇异值分解。在这种情况下，可以使用我们关于奇异值分解的知识来完成（或猜测）。首先，注意到 \(A\) 不是可逆的。确实，它的行是彼此的倍数。特别是，它们不是线性独立的。事实上，这告诉我们
    \(A\) 的秩是 \(1\)，它是其行空间的维度。在秩为 \(1\) 的情况下，计算奇异值分解归结为将矩阵 \(A\) 写成外积形式
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
- en: where we require that \(\sigma_1 > 0\) and that \(\mathbf{u}_1, \mathbf{v}_1\)
    are of unit norm.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们要求 \(\sigma_1 > 0\)，并且 \(\mathbf{u}_1, \mathbf{v}_1\) 是单位范数。
- en: Recall that an outer product has columns that are all multiples of the same
    vector. Here because the second column of \(A\) is \(\mathbf{0}\), it must be
    that the second component of \(\mathbf{v}_1\) is \(0\). To be of unit norm, its
    first component must be \(1\) or \(-1\). (The choice here does not matter because
    multiplying all left and right singular vectors by \(-1\) produces another SVD.)
    We choose \(1\), i.e., we let
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，外积的列都是同一个向量的倍数。在这里，因为 \(A\) 的第二列是 \(\mathbf{0}\)，所以 \(\mathbf{v}_1\) 的第二分量必须是
    \(0\)。为了成为单位范数，其第一分量必须是 \(1\) 或 \(-1\)。（这里的选取并不重要，因为将所有左奇异向量和右奇异向量乘以 \(-1\) 会产生另一个奇异值分解。）我们选择
    \(1\)，即我们让
- en: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
- en: That vector is indeed an orthonormal basis of the row space of \(A\). Then we
    need
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 那个向量确实是 \(A\) 的行空间的正交归一基。然后我们需要
- en: \[\begin{split} \sigma_1 \mathbf{u}_1 = \begin{pmatrix} 1\\ -1 \end{pmatrix}.
    \end{split}\]
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \sigma_1 \mathbf{u}_1 = \begin{pmatrix} 1\\ -1 \end{pmatrix}.
    \end{split}\]
- en: For \(\mathbf{u}_1\) to be of unit norm, we must have
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(\mathbf{u}_1\) 要成为单位范数，我们必须有
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}
    \quad \text{and} \quad \sigma_1 = \sqrt{2}. \end{split}\]
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}
    \quad \text{和} \quad \sigma_1 = \sqrt{2}. \end{split}\]
- en: Observe that \(\mathbf{u}_1\) is indeed an orthonormal basis of the column space
    of \(A\). \(\lhd\)
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mathbf{u}_1\) 确实是 \(A\) 的列空间的正交归一基。 \(\lhd\)
- en: One might hope that the SVD of a symmetric matrix generates identical left and
    right singular vectors. However that is not the case.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能希望对称矩阵的奇异值分解产生相同的左奇异向量和右奇异向量。然而情况并非如此。
- en: '**EXAMPLE:** An SVD of \(A = (-1)\) is \(A = (1)\,(1)\,(-1)\). That is, \(\mathbf{u}_1
    = (1)\) and \(\mathbf{v} = (-1)\). \(\lhd\)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** \(A = (-1)\) 的奇异值分解是 \(A = (1)\,(1)\,(-1)\)。也就是说，\(\mathbf{u}_1 = (1)\)
    和 \(\mathbf{v} = (-1)\)。 \(\lhd\)'
- en: We collect in the next lemma some relationships between the singular vectors
    and singular values that will be used repeatedly. It also further clarifies the
    connection between the SVD of \(A\) and the spectral decomposition of \(A^T A\)
    and \(A A^T\).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个引理中，我们收集了一些关于奇异向量和奇异值之间的一些关系，这些关系将被反复使用。它还进一步阐明了 \(A\) 的奇异值分解与 \(A^T A\)
    和 \(A A^T\) 的谱分解之间的联系。
- en: '**LEMMA** **(SVD Relations)** \(\idx{SVD relations}\xdi\) Let \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be an SVD of \(A \in \mathbb{R}^{n \times
    m}\) with \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\). Then, for
    \(i=1,\ldots,r\),'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（奇异值分解关系）** \(\idx{SVD relations}\xdi\) 设 \(A = \sum_{j=1}^r \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) 是 \(A \in \mathbb{R}^{n \times m}\) 的奇异值分解，其中 \(\sigma_1
    \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\)。那么，对于 \(i=1,\ldots,r\)，'
- en: \[ A \mathbf{v}_i = \sigma_i \mathbf{u}_i, \qquad A^T \mathbf{u}_i = \sigma_i
    \mathbf{v}_i, \qquad \|A \mathbf{v}_i\| = \sigma_i, \qquad \|A^T \mathbf{u}_i\|
    = \sigma_i. \]
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{v}_i = \sigma_i \mathbf{u}_i, \qquad A^T \mathbf{u}_i = \sigma_i
    \mathbf{v}_i, \qquad \|A \mathbf{v}_i\| = \sigma_i, \qquad \|A^T \mathbf{u}_i\|
    = \sigma_i. \]
- en: A fortiori
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: A fortiori
- en: \[ A^T A \mathbf{v}_i = \sigma_i^2 \mathbf{v}_i, \qquad A A^T \mathbf{u}_i =
    \sigma_i^2 \mathbf{u}_i. \]
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{v}_i = \sigma_i^2 \mathbf{v}_i, \qquad A A^T \mathbf{u}_i =
    \sigma_i^2 \mathbf{u}_i. \]
- en: and, for \(j \neq i\),
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(j \neq i\)，
- en: \[ \langle A \mathbf{v}_i, A \mathbf{v}_j \rangle = 0, \qquad \langle A^T \mathbf{u}_i,
    A^T \mathbf{u}_j \rangle = 0. \]
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle A \mathbf{v}_i, A \mathbf{v}_j \rangle = 0, \qquad \langle A^T \mathbf{u}_i,
    A^T \mathbf{u}_j \rangle = 0. \]
- en: \(\flat\)
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: We previously established the existence of an SVD via the spectral decomposition
    of \(A^T A\). The previous lemma shows that in fact, in any SVD, the \(\mathbf{v}_i\)s
    are orthonormal eigenvectors of \(A^T A\). They do not form an orthonormal basis
    of the full space \(\mathbb{R}^m\) however, as the rank \(r\) can be strictly
    smaller than \(m\). But observe that any vector \(\mathbf{w}\) orthogonal to \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\)
    is such that
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前通过 \(A^T A\) 的谱分解证明了奇异值分解的存在性。前述引理表明，实际上，在任何奇异值分解中，\(\mathbf{v}_i\) 是 \(A^T
    A\) 的正交特征向量。然而，它们并不构成 \(\mathbb{R}^m\) 的正交基，因为秩 \(r\) 可以严格小于 \(m\)。但请注意，任何与 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\)
    正交的向量 \(\mathbf{w}\) 都满足
- en: \[ A\mathbf{w} = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{w}
    = \mathbf{0} \]
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A\mathbf{w} = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{w}
    = \mathbf{0} \]
- en: and, a fortiori,
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，更不用说
- en: \[ A^T A \mathbf{w} = \mathbf{0}. \]
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{w} = \mathbf{0}. \]
- en: So \(\mathbf{w}\) is in fact an eigenvector of \(A^T A\) with eigenvalue \(0\).
    Let \(\mathbf{v}_{r+1}, \ldots, \mathbf{v}_m\) be any orthonormal basis of \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)^\perp\).
    Then \(\mathbf{v}_1,\ldots,\mathbf{v}_m\) is an orthonormal basis of eigenvectors
    of \(A^T A\).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 \(\mathbf{w}\) 实际上是 \(A^T A\) 的一个特征向量，其特征值为 \(0\)。设 \(\mathbf{v}_{r+1}, \ldots,
    \mathbf{v}_m\) 是 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)^\perp\) 的任意正交基。那么
    \(\mathbf{v}_1,\ldots,\mathbf{v}_m\) 是 \(A^T A\) 的特征向量的正交基。
- en: The lemma also shows that the \(\mathbf{u}_i\)s are orthonormal eigenvectors
    of \(A A^T\)!
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 该引理还表明，\(\mathbf{u}_i\) 是 \(A A^T\) 的正交特征向量！
- en: '**Full vs. compact SVD** What we have introduced above is in fact referred
    to as a [compact SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Compact_SVD).
    In contrast, in a [full SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\(\idx{full
    SVD}\xdi\), the matrices \(U\) and \(V\) are square and orthogonal, and the matrix
    \(\Sigma\) is diagonal, but may not be square and may have zeros on the diagonal.
    In particular, in that case, the columns of \(U \in \mathbb{R}^{n \times n}\)
    form an orthonormal basis of \(\mathbb{R}^n\) and the columns of \(V \in \mathbb{R}^{m
    \times m}\) form an orthonormal basis of \(\mathbb{R}^m\).'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '**完全SVD与紧凑SVD** 我们上面介绍的是所谓的 [紧凑SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Compact_SVD)。相比之下，在
    [完全SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\(\idx{full
    SVD}\xdi\) 中，矩阵 \(U\) 和 \(V\) 是方阵且正交，矩阵 \(\Sigma\) 是对角矩阵，但不一定是方阵，并且对角线上可能有零。特别是，在这种情况下，\(U
    \in \mathbb{R}^{n \times n}\) 的列向量构成 \(\mathbb{R}^n\) 的正交基，而 \(V \in \mathbb{R}^{m
    \times m}\) 的列向量构成 \(\mathbb{R}^m\) 的正交基。'
- en: '**Figure:** SVD in full form ([Source](https://commons.wikimedia.org/wiki/File:Singular_value_decomposition_visualisation.svg))'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：完全形式的SVD** ([来源](https://commons.wikimedia.org/wiki/File:Singular_value_decomposition_visualisation.svg))'
- en: '![SVD](../Images/cf2dd761c707eee39f5210e1337dc37d.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![SVD](../Images/cf2dd761c707eee39f5210e1337dc37d.png)'
- en: \(\bowtie\)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: Let \(A = U_1 \Sigma_1 V_1^T\) be a compact SVD. Complete the columns of \(U_1\)
    into an orthonormal basis of \(\mathbb{R}^n\) and let \(U_2\) be the matrix whose
    columns are the additional basis vectors. Similary, complete the columns of \(V_1\)
    into an orthonormal basis of \(\mathbb{R}^m\) and let \(V_2\) be the matrix whose
    columns are the additional basis vectors. Then a full SVD is given by
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(A = U_1 \Sigma_1 V_1^T\) 为紧凑SVD。将 \(U_1\) 的列向量补全为 \(\mathbb{R}^n\) 的正交基，并令
    \(U_2\) 为列向量是额外基向量的矩阵。类似地，将 \(V_1\) 的列向量补全为 \(\mathbb{R}^m\) 的正交基，并令 \(V_2\) 为列向量是额外基向量的矩阵。那么完全SVD由以下给出
- en: \[\begin{split} U = \begin{pmatrix} U_1 & U_2 \end{pmatrix} \quad V = \begin{pmatrix}
    V_1 & V_2 \end{pmatrix} \quad \Sigma = \begin{pmatrix} \Sigma_1 & \mathbf{0}_{r
    \times (m-r)}\\ \mathbf{0}_{(n-r)\times r} & \mathbf{0}_{(n-r)\times (m-r)} \end{pmatrix}.
    \end{split}\]
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} U = \begin{pmatrix} U_1 & U_2 \end{pmatrix} \quad V = \begin{pmatrix}
    V_1 & V_2 \end{pmatrix} \quad \Sigma = \begin{pmatrix} \Sigma_1 & \mathbf{0}_{r
    \times (m-r)}\\ \mathbf{0}_{(n-r)\times r} & \mathbf{0}_{(n-r)\times (m-r)} \end{pmatrix}.
    \end{split}\]
- en: By the *SVD and Rank Lemma*, the columns of \(U_1\) form an orthonormal basis
    of \(\mathrm{col}(A)\). Because \(\mathrm{col}(A)^\perp = \mathrm{null}(A^T)\),
    the columns of \(U_2\) form an orthonormal basis of \(\mathrm{null}(A^T)\). Similarly,
    the columns of \(V_1\) form an orthonormal basis of \(\mathrm{col}(A^T)\). Because
    \(\mathrm{col}(A^T)^\perp = \mathrm{null}(A)\), the columns of \(V_2\) form an
    orthonormal basis of \(\mathrm{null}(A)\). Hence, a full SVD provides an orthonormal
    basis for all four fundamental subspaces of \(A\).
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SVD和秩引理，\(U_1\)的列构成了\(\mathrm{col}(A)\)的一个正交基。因为\(\mathrm{col}(A)^\perp =
    \mathrm{null}(A^T)\)，\(U_2\)的列构成了\(\mathrm{null}(A^T)\)的一个正交基。类似地，\(V_1\)的列构成了\(\mathrm{col}(A^T)\)的一个正交基。因为\(\mathrm{col}(A^T)^\perp
    = \mathrm{null}(A)\)，\(V_2\)的列构成了\(\mathrm{null}(A)\)的一个正交基。因此，完整的SVD为\(A\)的所有四个基本子空间提供了一个正交基。
- en: Vice versa, given a full SVD \(A = U \Sigma V^T\), the compact SVD can be obtained
    by keeping only the square submatrix of \(\Sigma\) with stricly positive diagonal
    entries, together with the corresponding columns of \(U\) and \(V\).
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，给定一个完整的SVD \(A = U \Sigma V^T\)，可以通过只保留\(\Sigma\)的严格正对角线元素的平方子矩阵，以及相应的\(U\)和\(V\)的列来获得紧凑SVD。
- en: '**Figure:** Different variants of the SVD ([Source](https://commons.wikimedia.org/wiki/File:Reduced_Singular_Value_Decompositions.svg))'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：SVD的不同变体([来源](https://commons.wikimedia.org/wiki/File:Reduced_Singular_Value_Decompositions.svg))**'
- en: '![ReducedSVD](../Images/2bc162cb67041b1a46fc5b7ca370d66c.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![ReducedSVD](../Images/2bc162cb67041b1a46fc5b7ca370d66c.png)'
- en: \(\bowtie\)
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**EXAMPLE:** **(continued)** Let again'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(继续)** 再次'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: We previously computed its compact SVD
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前计算了它的紧凑SVD
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
- en: where
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{和}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
- en: We now compute a full SVD. For this, we need to complete the bases. We can choose
    (why?)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在计算一个完整的SVD。为此，我们需要完成基的构建。我们可以选择（为什么？）
- en: \[\begin{split} \mathbf{u}_2 = \begin{pmatrix} 1/\sqrt{2}\\ 1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_2 = \begin{pmatrix} 0\\ 1 \end{pmatrix}, \quad \text{and}
    \quad \sigma_2 = 0. \end{split}\]
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_2 = \begin{pmatrix} 1/\sqrt{2}\\ 1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_2 = \begin{pmatrix} 0\\ 1 \end{pmatrix}, \quad \text{和}
    \quad \sigma_2 = 0. \end{split}\]
- en: Then, a full SVD is given by
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，完整的SVD由以下给出
- en: \[\begin{split} U = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2}\\ -1/\sqrt{2} &
    1/\sqrt{2} \end{pmatrix}, \quad \quad V = \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix},
    \quad \text{and} \quad \Sigma = \begin{pmatrix} \sqrt{2} & 0\\ 0 & 0 \end{pmatrix}.
    \end{split}\]
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} U = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2}\\ -1/\sqrt{2} &
    1/\sqrt{2} \end{pmatrix}, \quad \quad V = \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix},
    \quad \text{和} \quad \Sigma = \begin{pmatrix} \sqrt{2} & 0\\ 0 & 0 \end{pmatrix}.
    \end{split}\]
- en: Indeed, \(A = U \Sigma V^T\) (check it!). \(\lhd\)
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，\(A = U \Sigma V^T\)（检查它！）\(\lhd\)
- en: 'The full SVD also has a natural geometric interpretation. To quote [Sol, p.
    133]:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的SVD也有一个自然的几何解释。引用[索尔，第133页]：
- en: The SVD provides a complete geometric characterization of the action of \(A\).
    Since \(U\) and \(V\) are orthogonal, they have no effect on lengths and angles;
    as a diagonal matrix, \(\Sigma\) scales individual coordinate axes. Since the
    SVD always exists, all matrices \(A \in \mathbb{R}^{n \times m}\) are a composition
    of an isometry, a scale in each coordinate, and a second isometry.
  id: totrans-497
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SVD提供了对\(A\)作用的完整几何描述。由于\(U\)和\(V\)是正交的，它们对长度和角度没有影响；作为对角矩阵，\(\Sigma\)缩放各个坐标轴。由于SVD总是存在的，所有矩阵\(A
    \in \mathbb{R}^{n \times m}\)都是等距变换、每个坐标的缩放和第二个等距变换的组合。
- en: '**Figure:** Geometric interpretation of the SVD ([Source](https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg))'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：SVD的几何解释([来源](https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg))**'
- en: '![Geometric meaning](../Images/ea0108f7b4d77cc40c274d7c98d6d646.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![几何意义](../Images/ea0108f7b4d77cc40c274d7c98d6d646.png)'
- en: \(\bowtie\)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**Coming full circle: solving the approximating subspace problem via the SVD**
    Think of the rows \(\boldsymbol{\alpha}_i^T\) of a matrix \(A \in \mathbb{R}^{n
    \times m}\) as a collection of \(n\) data points in \(\mathbb{R}^m\). Let'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '**回到起点：通过SVD解决逼近子空间问题** 将矩阵\(A \in \mathbb{R}^{n \times m}\)的行\(\boldsymbol{\alpha}_i^T\)视为\(\mathbb{R}^m\)中的\(n\)个数据点的集合。让'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: be a (compact) SVD of \(A\). Fix \(k \leq \mathrm{rk}(A)\). We are looking for
    a linear subspace \(\mathcal{Z}\) that minimizes
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 是 \(A\) 的一个（紧凑的）奇异值分解。固定 \(k \leq \mathrm{rk}(A)\)。我们正在寻找一个线性子空间 \(\mathcal{Z}\)，它使以下最小化
- en: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    \]
- en: over all linear subspaces of \(\mathbb{R}^m\) of dimension at most \(k\). By
    the observations above, a solution is given by
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在 \(\mathbb{R}^m\) 的所有线性子空间中，其维度最多为 \(k\)。根据上述观察，一个解由以下给出
- en: \[ \mathcal{Z} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k). \]
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{Z} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k). \]
- en: By the proofs of the *Best Subspace as Maximization* and *Best Subspace in Matrix
    Form* lemmas, the objective value achieved is
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最佳子空间最大化定理和最佳子空间矩阵形式引理的证明，所达到的目标值是
- en: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \|A\mathbf{v}_j\|^2\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \sigma_j^2. \end{align*}\]
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \|A\mathbf{v}_j\|^2\\
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 - \sum_{j=1}^k \sigma_j^2. \end{align*}\]
- en: So the singular value \(\sigma_j\) associated to the right singular vector \(\mathbf{v}_j\)
    captures its contribution to the fit of the approximating subspace. The larger
    the singular value, the larger the contribution.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与右奇异向量 \(\mathbf{v}_j\) 相关的奇异值 \(\sigma_j\) 捕获了它对逼近子空间的拟合的贡献。奇异值越大，贡献就越大。
- en: To obtain a low-dimensional embedding of our original datasets, we compute \(\mathbf{z}_i
    := \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\) for each \(i\) as follows
    (in row form)
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得原始数据集的低维嵌入，我们计算每个 \(i\) 的 \(\mathbf{z}_i := \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\)
    如下（以行形式）
- en: \[\begin{align*} \mathbf{z}_i^T &= \sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{v}_j\rangle \,\mathbf{v}_j^T\\ &= \sum_{j=1}^k \boldsymbol{\alpha}_i^T
    \mathbf{v}_j \mathbf{v}_j^T\\ &= A_{i,\cdot} V_{(k)} V_{(k)}^T, \end{align*}\]
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_i^T &= \sum_{j=1}^k \langle \boldsymbol{\alpha}_i,
    \mathbf{v}_j\rangle \,\mathbf{v}_j^T\\ &= \sum_{j=1}^k \boldsymbol{\alpha}_i^T
    \mathbf{v}_j \mathbf{v}_j^T\\ &= A_{i,\cdot} V_{(k)} V_{(k)}^T, \end{align*}\]
- en: where \(V_{(k)}\) is the matrix with the first \(k\) columns of \(V\). Let \(Z\)
    be the matrix with rows \(\mathbf{z}_i^T\). Then we have
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(V_{(k)}\) 是由 \(V\) 的前 \(k\) 列组成的矩阵。设 \(Z\) 为具有行 \(\mathbf{z}_i^T\) 的矩阵。然后我们有
- en: \[ Z = A V_{(k)} V_{(k)}^T = U_{(k)} \Sigma_{(k)} V_{(k)}^T = \sum_{j=1}^k \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T, \]
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z = A V_{(k)} V_{(k)}^T = U_{(k)} \Sigma_{(k)} V_{(k)}^T = \sum_{j=1}^k \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T, \]
- en: where \(U_{(k)}\) is the matrix with the first \(k\) columns of \(U\), and \(\Sigma_{(k)}\)
    is the matrix with the first \(k\) rows and columns of \(\Sigma\). Indeed, recall
    that \(A \mathbf{v}_j = \sigma_j \mathbf{u}_j\), or in matrix form \(A V_{(k)}
    = U_{(k)} \Sigma_{(k)}\). The rightmost expression for \(Z\) reveals that it is
    in fact a truncated SVD. We can interpret the rows of \(U_{(k)} \Sigma_{(k)}\)
    as the coefficients of each data point in the basis \(\mathbf{v}_1,\ldots,\mathbf{v}_k\).
    Those coefficients provide the desired low-dimensional representation.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(U_{(k)}\) 是由 \(U\) 的前 \(k\) 列组成的矩阵，而 \(\Sigma_{(k)}\) 是由 \(\Sigma\) 的前
    \(k\) 行和列组成的矩阵。实际上，回想一下 \(A \mathbf{v}_j = \sigma_j \mathbf{u}_j\)，或者用矩阵形式表示为
    \(A V_{(k)} = U_{(k)} \Sigma_{(k)}\)。\(Z\) 的最右侧表达式表明它实际上是一个截断的奇异值分解。我们可以将 \(U_{(k)}
    \Sigma_{(k)}\) 的行解释为每个数据点在基 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 中的系数。这些系数提供了所需的低维表示。
- en: We can re-write the objective function in a more compact matrix form by using
    the Frobenius norm as follows
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用Frobenius范数将目标函数重写为更紧凑的矩阵形式，如下所示
- en: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{z}_i\|^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i^T
    - \mathbf{z}_i^T\|^2 = \|A - Z\|_F^2. \end{align*}\]
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2
    &= \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{z}_i\|^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i^T
    - \mathbf{z}_i^T\|^2 = \|A - Z\|_F^2. \end{align*}\]
- en: We note that the matrix \(Z\) has rank smaller or equal than \(k\). Indeed,
    all of its rows lie in the optimal subspace \(\mathcal{Z}\), which has dimension
    \(k\) by construction. We will see later that \(Z\) is the best approximation
    to \(A\) among all rank-\(k\) matrices under the Frobenius norm, that is,
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到矩阵 \(Z\) 的秩小于或等于 \(k\)。实际上，它的所有行都位于由构造得到的最优子空间 \(\mathcal{Z}\) 中，该子空间的维度为
    \(k\)。我们将在后面看到，\(Z\) 是在Frobenius范数下所有秩为 \(k\) 的矩阵中最佳逼近 \(A\) 的矩阵，
- en: \[ \|A - Z\|_F \leq \|A - B\|_F \]
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - Z\|_F \leq \|A - B\|_F \]
- en: for any matrix \(B\) of rank at most \(k\).
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何秩最多为 \(k\) 的矩阵 \(B\)。
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(在Claude、Gemini和ChatGPT的帮助下)*'
- en: '**1** Let \(\boldsymbol{\alpha}_1, \dots, \boldsymbol{\alpha}_n\) be data points
    in \(\mathbb{R}^m\). What is the objective of the best approximating subspace
    problem?'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 设 \(\boldsymbol{\alpha}_1, \dots, \boldsymbol{\alpha}_n\) 是 \(\mathbb{R}^m\)
    中的数据点。最佳逼近子空间问题的目标是什么？'
- en: a) To find a linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that minimizes
    the sum of the distances between the \(\boldsymbol{\alpha}_i\)’s and \(\mathcal{Z}\).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: a) 找到一个线性子空间 \(\mathcal{Z}\) 属于 \(\mathbb{R}^m\)，使得 \(\boldsymbol{\alpha}_i\)
    与 \(\mathcal{Z}\) 之间的距离最小化。
- en: b) To find a linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that minimizes
    the sum of the squared distances between the \(\boldsymbol{\alpha}_i\)’s and their
    orthogonal projections onto \(\mathcal{Z}\).
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: b) 找到一个线性子空间 \(\mathcal{Z}\) 属于 \(\mathbb{R}^m\)，使得 \(\boldsymbol{\alpha}_i\)
    与其投影到 \(\mathcal{Z}\) 上的距离平方和最小化。
- en: c) To find a linear subspace \(\mathcal{Z}\) of \(\mathbb{R}^m\) that maximizes
    the sum of the squared norms of the orthogonal projections of the \(\boldsymbol{\alpha}_i\)’s
    onto \(\mathcal{Z}\).
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: c) 找到一个线性子空间 \(\mathcal{Z}\) 属于 \(\mathbb{R}^m\)，使得 \(\boldsymbol{\alpha}_i\)
    投影到 \(\mathcal{Z}\) 上的正交投影的平方范数之和最大化。
- en: d) Both b and c.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: d) b 和 c。
- en: '**2** Consider the data points \(\boldsymbol{\alpha}_1 = (-2,2)\) and \(\boldsymbol{\alpha}_2
    = (3,-3)\). For \(k=1\), what is the solution of the best approximating subspace
    problem?'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 考虑数据点 \(\boldsymbol{\alpha}_1 = (-2,2)\) 和 \(\boldsymbol{\alpha}_2 =
    (3,-3)\)。对于 \(k=1\)，最佳逼近子空间问题的解是什么？'
- en: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x\}\)'
- en: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = -x\}\)'
- en: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x+1\}\)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x+1\}\)'
- en: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x-1\}\)'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\mathcal{Z} = \{(x,y) \in \mathbb{R}^2 : y = x-1\}\)'
- en: '**3** Which of the following is true about the SVD of a matrix \(A\)?'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 关于矩阵 \(A\) 的奇异值分解（SVD），以下哪个说法是正确的？'
- en: a) The SVD of \(A\) is unique.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(A\) 的SVD是唯一的。
- en: b) The right singular vectors of \(A\) are the eigenvectors of \(A^TA\).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(A\) 的右奇异向量是 \(A^TA\) 的特征向量。
- en: c) The left singular vectors of \(A\) are the eigenvectors of \(AA^T\).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(A\) 的左奇异向量是 \(AA^T\) 的特征向量。
- en: d) Both b and c.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: d) b 和 c。
- en: '**4** Let \(A = U \Sigma V^T\) be an SVD of \(A\). Which of the following is
    true?'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 设 \(A = U \Sigma V^T\) 是 \(A\) 的一个SVD。以下哪个说法是正确的？'
- en: a) \(A \mathbf{v}_i = \sigma_i \mathbf{u}_i\) for all \(i\).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对于所有 \(i\)，\(A \mathbf{v}_i = \sigma_i \mathbf{u}_i\)。
- en: b) \(A^T \mathbf{u}_i = \sigma_i \mathbf{v}_i\) for all \(i\).
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: b) 对于所有 \(i\)，\(A^T \mathbf{u}_i = \sigma_i \mathbf{v}_i\)。
- en: c) \(\|A\mathbf{v}_i\| = \sigma_i\) for all \(i\).
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: c) 对于所有 \(i\)，\(\|A\mathbf{v}_i\| = \sigma_i\)。
- en: d) All of the above.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有上述说法。
- en: '**5** The columns of \(U\) in the compact SVD form an orthonormal basis for:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 紧凑SVD形式中 \(U\) 的列构成以下哪个的正交基：'
- en: a) \(\mathrm{col}(A)\)
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathrm{col}(A)\)
- en: b) \(\mathrm{row}(A)\)
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathrm{row}(A)\)
- en: c) \(\mathrm{null}(A)\)
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathrm{null}(A)\)
- en: d) \(\mathrm{null}(A^T)\)
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathrm{null}(A^T)\)
- en: 'Answer for 1: d. Justification: The text defines the best approximating subspace
    problem as minimizing the sum of squared distances between the data points and
    their projections onto the subspace, and it also states a lemma that this problem
    is equivalent to maximizing the sum of squared norms of the projections.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 答案1：d. 理由：文本将最佳逼近子空间问题定义为最小化数据点与其子空间投影之间的距离平方和，并且还提到一个引理，即该问题等价于最大化投影的平方范数之和。
- en: 'Answer for 2: b. Justification: By symmetry, the best approximating line must
    pass through the origin and bisect the angle between the two points. This is the
    line \(y=-x\).'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 答案2：b. 理由：由于对称性，最佳逼近线必须通过原点并平分两点之间的角度。这就是线 \(y=-x\)。
- en: 'Answer for 3: c and d. Justification: The SVD is not unique in general. The
    other two statements are true and are mentioned in the text.'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 答案3：c 和 d. 理由：SVD在一般情况下不是唯一的。其他两个说法是正确的，并在文本中提到。
- en: 'Answer for 4: d. Justification: This is a lemma stated in the text.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：d. 证明：这是文本中陈述的一个引理。
- en: 'Answer for 5: a. Justification: The text states in the SVD and Rank Lemma:
    “the columns of \(U\) form an orthonormal basis of \(\mathrm{col}(A)\)”.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：a. 证明：文本在SVD和秩引理中提到：“\(U\)的列构成\(\mathrm{col}(A)\)的正交归一基”。
