- en: '**Appendix: Advanced Atomics**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**附录：高级原子操作**'
- en: '[Chapter 9](ch09.html#ch09) covered some of the ways in which we can use atomic
    operations to enable hundreds of threads to safely make concurrent modifications
    to shared data. In this appendix, we’ll look at an advanced method for using atomics
    to implement locking data structures. On its surface, this topic does not seem
    much more complicated than anything else we’ve examined. And in reality, this
    is accurate. You’ve learned a lot of complex topics through this book, and locking
    data structures are no more challenging than these. So, why is this material hiding
    in the appendix? We don’t want to reveal any spoilers, so if you’re intrigued,
    read on, and we’ll discuss this through the course of the appendix.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](ch09.html#ch09)讨论了我们可以如何利用原子操作使数百个线程安全地对共享数据进行并发修改。在本附录中，我们将探讨一种使用原子操作实现锁数据结构的高级方法。从表面上看，这个话题似乎并不比我们研究过的其他内容更复杂。实际上，这也是准确的。通过本书，你已经学习了许多复杂的话题，而锁数据结构并不比这些更具挑战性。那么，为什么这些内容会藏在附录里呢？我们不想剧透，如果你感兴趣，可以继续阅读，我们将在附录中讨论这个问题。'
- en: '**A.1 Dot Product Revisited**'
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**A.1 点积重访**'
- en: 'In [Chapter 5](ch05.html#ch05), we looked at the implementation of a vector
    dot product using CUDA C. This algorithm was one of a large family of algorithms
    known as *reductions*. If you recall, the algorithm computed the dot product of
    two input vectors by doing the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#ch05)中，我们讨论了使用CUDA C实现向量点积的算法。这个算法是众多*归约*算法之一。如果你还记得，该算法通过以下步骤计算两个输入向量的点积：
- en: 1\. Each thread in each block multiplies two corresponding elements of the input
    vectors and stores the products in shared memory.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 每个块中的每个线程将输入向量中的两个对应元素相乘，并将乘积存储在共享内存中。
- en: 2\. Although a block has more than one product, a thread adds two of the products
    and stores the result back to shared memory. Each step results in half as many
    values as it started with (this is where the term *reduction* comes from)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 尽管一个块中有多个乘积，但每个线程会将其中的两个乘积相加，并将结果存储回共享内存。每一步都会使值的数量减少一半（这就是*归约*一词的由来）。
- en: 3\. When every block has a final sum, each one writes its value to global memory
    and exits.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 当每个块都有一个最终的和时，每个块将其值写入全局内存并退出。
- en: 4\. If the kernel ran with `N` parallel blocks, the CPU sums these remaining
    `N` values to generate the final dot product.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 如果内核以`N`个并行块运行，CPU会将这`N`个值相加，以生成最终的点积。
- en: This high-level look at the dot product algorithm is intended to be review,
    so if it’s been a while or you’ve had a couple glasses of Chardonnay, it may be
    worth the time to review [Chapter 5](ch05.html#ch05). If you feel comfortable
    enough with the dot product code to continue, draw your attention to step 4 in
    the algorithm. Although it doesn’t involve copying much data to the host or performing
    many calculations on the CPU, moving the computation back to the CPU to finish
    is indeed as awkward as it sounds.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对点积算法的这一高层次回顾旨在帮助复习，因此，如果你已经有一段时间没有接触，或者你刚喝了几杯霞多丽，复习一下[第5章](ch05.html#ch05)可能会很有帮助。如果你对点积代码已经足够熟悉，可以继续，注意算法中的第4步。尽管这一步并不涉及大量数据复制到主机或在CPU上进行复杂计算，但将计算移回CPU完成确实像听起来那样尴尬。
- en: But it’s more than an issue of an awkward step to the algorithm or the inelegance
    of the solution. Consider a scenario where a dot product computation is just one
    step in a long sequence of operations. If you want to perform *every* operation
    on the GPU because your CPU is busy with other tasks or computations, you’re out
    of luck. As it stands, you’ll be forced to stop computing on the GPU, copy intermediate
    results back to the host, finish the computation with the CPU, and finally upload
    that result back to the GPU and resume computing with your next kernel.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不仅仅是算法中的一个尴尬步骤，或者解决方案的不优雅。设想一种情况，点积计算只是一个长长操作序列中的一步。如果你希望在GPU上执行*每一个*操作，因为你的CPU正忙于其他任务或计算，你就会陷入困境。照目前的情况，你将不得不停止在GPU上的计算，将中间结果复制回主机，利用CPU完成计算，然后将结果上传回GPU，并继续使用下一个内核进行计算。
- en: Since this is an appendix on atomics and we have gone to such lengths to explain
    what a pain our original dot product algorithm is, you should see where we’re
    heading. We intend to fix our dot product using atomics so the entire computation
    can stay on the GPU, leaving your CPU free to perform other tasks. Ideally, instead
    of exiting the kernel in step 3 and returning to the CPU in step 4, we want each
    block to add its final result to a total in global memory. If each value were
    added atomically, we would not have to worry about potential collisions or indeterminate
    results. Since we have already used an `atomicAdd()` operation in the histogram
    operation, this seems like an obvious choice.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是关于原子操作的附录，而且我们已经详细解释了原始点积算法的痛苦，你应该能看出我们要做什么。我们打算使用原子操作修复点积，这样整个计算就可以留在GPU上，释放CPU去执行其他任务。理想情况下，我们希望每个块将其最终结果加到全局内存中的总和中，而不是在步骤3退出内核，步骤4返回到CPU。如果每个值都采用原子操作相加，我们就不必担心潜在的碰撞或不确定的结果。由于我们已经在直方图操作中使用了`atomicAdd()`操作，因此这似乎是一个显而易见的选择。
- en: Unfortunately, prior to compute capability 2.0, `atomicAdd()`operated only on
    integers. Although this might be fine if you plan to compute dot products of vectors
    with integer components, it is significantly more common to use floating-point
    components. However, the majority of NVIDIA hardware does not support atomic arithmetic
    on floating-point numbers! But there’s a reasonable explanation for this, so don’t
    throw your GPU in the garbage just yet.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在计算能力2.0之前，`atomicAdd()`仅支持整数。如果你计划计算具有整数分量的向量的点积，这可能没问题，但更常见的是使用浮点分量。然而，大多数NVIDIA硬件并不支持浮点数的原子运算！不过，对于这个问题是有合理解释的，所以别急着把你的GPU丢掉。
- en: Atomic operations on a value in memory guarantee only that each thread’s read-modify-write
    sequence will complete without other threads reading or writing the target value
    while in process. There is no stipulation about the order in which the threads
    will perform their operations, so in the case of three threads performing addition,
    sometimes the hardware will perform `(A+B)+C` and sometimes it will compute `A+(B+C)`.
    This is acceptable for integers because integer math is associative, so `(A+B)+C
    = A+(B+C)`. Floating-point arithmetic is *not* associative because of the rounding
    of intermediate results, so `(A+B)+C` often does not equal `A+(B+C)`. As a result,
    atomic arithmetic on floating-point values is of dubious utility because it gives
    rise to nondeterministic results in a highly multithreaded environment such as
    on the GPU. There are many applications where it is simply unacceptable to get
    two different results from two runs of an application, so the support of floating-point
    atomic arithmetic was not a priority for earlier hardware.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对内存中值的原子操作仅保证每个线程的读-修改-写序列在操作过程中不会被其他线程读取或写入目标值。没有规定线程执行操作的顺序，因此在三个线程执行加法的情况下，有时硬件会执行`(A+B)+C`，有时会计算`A+(B+C)`。对于整数来说，这是可以接受的，因为整数运算是可结合的，所以`(A+B)+C
    = A+(B+C)`。浮点运算*不是*可结合的，因为中间结果的四舍五入，所以`(A+B)+C`通常不等于`A+(B+C)`。因此，浮点值的原子运算在高度多线程的环境中（如GPU）可能会产生非确定性结果，因此其效用值得怀疑。有许多应用场景中，两个不同运行的结果是不被接受的，因此浮点原子运算的支持在早期硬件中并非优先考虑。
- en: However, if we are willing to tolerate some nondeterminism in the results, we
    can still accomplish the reduction entirely on the GPU. But we’ll first need to
    develop a way to work around the lack of atomic floating-point arithmetic. The
    solution will still use atomic operations, but not for the arithmetic itself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们愿意容忍结果中的一些非确定性，我们仍然可以完全在GPU上完成归约操作。但我们首先需要开发一种绕过浮点原子运算缺失的方法。这个解决方案仍然会使用原子操作，但不是针对算术本身。
- en: '**A.1.1 Atomic Locks**'
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**A.1.1 原子锁**'
- en: The `atomicAdd()` function we used to build GPU histograms performed a read-modify-write
    operation without interruption from other threads. At a low level, you can imagine
    the hardware locking the target memory location while this operation is underway,
    and while locked, no other threads can read or write the value at the location.
    If we had a way of emulating this lock in our CUDA C kernels, we could perform
    arbitrary operations on an associated memory location or data structure. The locking
    mechanism itself will operate exactly like a typical CPU *mutex*. If you are unfamiliar
    with mutual exclusion (*mutex*), don’t fret. It’s not any more complicated than
    the things you’ve already learned.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于构建GPU直方图的`atomicAdd()`函数执行了一个读-修改-写操作，并且没有受到其他线程的干扰。在底层，你可以想象硬件在此操作进行时锁定目标内存位置，并且在锁定期间，其他线程无法读写该位置的值。如果我们能够在我们的CUDA
    C内核中模拟这种锁定，我们就能对相关的内存位置或数据结构执行任意操作。锁定机制本身将完全像一个典型的CPU *mutex*。如果你不熟悉互斥量（*mutex*），不用担心，它并没有比你已经学过的东西更复杂。
- en: The basic idea is that we allocate a small piece of memory to be used as a *mutex*.
    The mutex will act like something of a traffic signal that governs access to some
    resource. The resource could be a data structure, a buffer, or simply a memory
    location we want to modify atomically. When a thread reads a 0 from the mutex,
    it interprets this value as a “green light” indicating that no other thread is
    using the memory. Therefore, the thread is free to lock the memory and make whatever
    changes it desires, free of interference from other threads. To lock the memory
    location in question, the thread writes a 1 to the mutex. This 1 will act as a
    “red light” for potentially competing threads. The competing threads must then
    wait until the owner has written a 0 to the mutex before they can attempt to modify
    the locked memory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的思路是，我们分配一小块内存用作*mutex*。这个mutex将像一个交通信号灯，控制对某些资源的访问。这个资源可以是数据结构、缓冲区，或者仅仅是我们希望原子地修改的内存位置。当线程从mutex读取到0时，它会将这个值解释为“绿灯”，表示没有其他线程在使用该内存。因此，线程可以自由地锁定内存并进行任何它想做的修改，不会受到其他线程的干扰。为了锁定相关的内存位置，线程会将1写入mutex。这个1将作为“红灯”信号，阻止可能竞争的线程。竞争线程必须等待直到所有者将0写入mutex，才能尝试修改已锁定的内存。
- en: 'A simple code sequence to accomplish this locking process might look like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此锁定过程的一个简单代码序列可能如下所示：
- en: '![image](graphics/p0252-01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0252-01.jpg)'
- en: 'Unfortunately, there’s a problem with this code. Fortunately, it’s a familiar
    problem: What happens if another thread writes a 1 to the mutex after our thread
    has read the value to be zero? That is, both threads check the value at `mutex`
    and see that it’s zero. They then both write a 1 to this location to signify to
    other threads that the structure is locked and unavailable for modification. After
    doing so, both threads think they own the associated memory or data structure
    and begin making unsafe modifications. Catastrophe ensues!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这段代码存在问题。幸运的是，这是一个熟悉的问题：如果在我们的线程读取到值为零之后，另一个线程向互斥量写入1，会发生什么呢？也就是说，两个线程都检查`mutex`的值，并看到它是零。然后它们都将1写入该位置，以表示其他线程该结构已被锁定且无法修改。这样做后，两个线程都认为它们拥有与该内存或数据结构相关的所有权，并开始进行不安全的修改。结果导致灾难！
- en: 'The operation we want to complete is fairly simple: We need to compare the
    value at `mutex` to 0 and store a 1 at that location if and only if the `mutex`
    was 0\. To accomplish this correctly, this entire operation needs to be performed
    atomically so we know that no other thread can interfere while our thread examines
    and updates the value at `mutex`. In CUDA C, this operation can be performed with
    the function `atomicCAS()`, an atomic compare-and-swap. The function `atomicCAS()`
    takes a pointer to memory, a value with which to compare the value at that location,
    and a value to store in that location if the comparison is successful. Using this
    operation, we can implement a GPU lock function as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要完成的操作其实很简单：我们需要将`mutex`的值与0进行比较，并且只有在`mutex`为0时才会在该位置存储1。为了正确完成这一操作，整个过程需要以原子方式执行，这样我们就能确保在我们的线程检查并更新`mutex`的值时，不会有其他线程进行干扰。在CUDA
    C中，这个操作可以通过`atomicCAS()`函数来完成，它是一个原子比较与交换操作。`atomicCAS()`函数接受一个指向内存的指针，一个与该位置的值进行比较的值，以及一个如果比较成功时存储在该位置的值。使用此操作，我们可以实现一个GPU锁定功能，如下所示：
- en: '![image](graphics/p0253-01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0253-01.jpg)'
- en: 'The call to `atomicCAS()` returns the value that it found at the address `mutex`.
    As a result, the `while()` loop will continue to run until `atomicCAS()` sees
    a 0 at `mutex`. When it sees a 0, the comparison is successful, and the thread
    writes a 1 to `mutex`. Essentially, the thread will spin in the `while()` loop
    until it has successfully locked the data structure. We’ll use this locking mechanism
    to implement our GPU hash table. But first, we dress the code up in a structure
    so it will be cleaner to use in the dot product application:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`atomicCAS()`返回它在地址`mutex`处找到的值。因此，`while()`循环将继续运行，直到`atomicCAS()`在`mutex`处看到0。当它看到0时，比较成功，线程会将1写入`mutex`。本质上，线程将在`while()`循环中旋转，直到成功锁定数据结构。我们将使用这种锁定机制来实现我们的GPU哈希表。但首先，我们将代码包装成一个结构体，以便在点积应用中使用时更加清晰：
- en: '![image](graphics/p0253-02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0253-02.jpg)'
- en: Notice that we restore the value of `mutex` with `atomicExch( mutex, 0 )`. The
    function `atomicExch()` reads the value that is located at `mutex`, exchanges
    it with the second argument (a 0 in this case), and returns the original value
    it read. Why would we use an atomic function for this rather than the more obvious
    method to reset the value at `mutex`?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`atomicExch(mutex, 0)`恢复`mutex`的值。`atomicExch()`函数读取位于`mutex`处的值，将其与第二个参数（在这种情况下为0）交换，并返回它读取的原始值。为什么我们要使用原子操作而不是更明显的方式来重置`mutex`的值呢？
- en: '*mutex = 0;'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*mutex = 0;'
- en: If you’re expecting some subtle, hidden reason why this method fails, we hate
    to disappoint you, but this would work as well. So, why not use this more obvious
    method? Atomic transactions and generic global memory operations follow different
    paths through the GPU. Using both atomics and standard global memory operations
    could therefore lead to an `unlock()` seeming out of sync with a subsequent attempt
    to `lock()` the mutex. The behavior would still be functionally correct, but to
    ensure consistently intuitive behavior from the application’s perspective, it’s
    best to use the same pathway for all accesses to the mutex. Because we’re required
    to use an atomic to lock the resource, we have chosen to also use an atomic to
    unlock the resource.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你期待这个方法失败的某种微妙、隐藏的原因，我们很遗憾地告诉你，实际上这个方法也可以正常工作。那么，为什么不使用这种更明显的方法呢？原子事务和常规全局内存操作在GPU中会走不同的路径。因此，同时使用原子操作和标准全局内存操作可能导致`unlock()`与后续尝试`lock()`互斥锁的操作不同步。尽管行为在功能上仍然是正确的，但为了确保应用程序的一致性和直观性，最好对所有互斥锁的访问使用相同的路径。由于我们需要使用原子操作来锁定资源，因此我们也选择使用原子操作来解锁资源。
- en: '**A.1.2 Dot Product Redux: Atomic Locks**'
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**A.1.2 点积重构：原子锁**'
- en: 'The only piece of our earlier dot product example that we endeavor to change
    is the final CPU-based portion of the reduction. In the previous section, we described
    how we implement a mutex on the GPU. The `Lock` structure that implements this
    mutex is located in `lock.h` and included at the beginning of our improved dot
    product example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们努力改变的唯一部分是之前点积示例中的最终基于CPU的归约部分。在上一节中，我们描述了如何在GPU上实现互斥锁。实现该互斥锁的`Lock`结构体位于`lock.h`中，并在我们改进的点积示例开头包含：
- en: '![image](graphics/p0254-01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0254-01.jpg)'
- en: 'With two exceptions, the beginning of our dot product kernel is identical to
    the kernel we used in [Chapter 5](ch05.html#ch05). Both exceptions involve the
    kernel’s signature:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了两个例外，我们的点积内核的开始部分与我们在[第5章](ch05.html#ch05)中使用的内核完全相同。这两个例外都涉及内核的签名：
- en: '![image](graphics/p0254-02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0254-02.jpg)'
- en: 'In our updated dot product, we pass a `Lock` to the kernel in addition to input
    vectors and the output buffer. The `Lock` will govern access to the output buffer
    during the final accumulation step. The other change is not *noticeable* from
    the signature but involves the signature. Previously, the `float *c` argument
    was a buffer for `N` floats where each of the `N` blocks could store its partial
    result. This buffer was copied back to the CPU to compute the final sum. Now,
    the argument `c` no longer points to a temporary buffer but to a single floating-point
    value that will store the dot product of the vectors in `a` and `b`. But even
    with these changes, the kernel starts out exactly as it did in [Chapter 5](ch05.html#ch05):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新后的点积中，我们除了输入向量和输出缓冲区，还将一个`Lock`传递给内核。`Lock`将在最终累加步骤中控制对输出缓冲区的访问。另一个变化在函数签名上并不*明显*，但实际上与签名有关。之前，`float
    *c`参数是一个包含`N`个浮点数的缓冲区，每个`N`个线程块可以存储其部分结果。这个缓冲区被复制回CPU以计算最终和。现在，`c`参数不再指向临时缓冲区，而是指向一个单一的浮点值，该值将存储向量`a`和`b`的点积。即便有这些变化，内核仍然和[第5章](ch05.html#ch05)中的实现完全相同：
- en: '![image](graphics/p0255-01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0255-01.jpg)'
- en: At this point in execution, the 256 threads in each block have summed their
    256 pairwise products and computed a single value that’s sitting in `cache[0]`.
    Each thread block now needs to add its final value to the value at `c`. To do
    this safely, we’ll use the lock to govern access to this memory location, so each
    thread needs to acquire the lock before updating the value *`c`. After adding
    the block’s partial sum to the value at `c`, it unlocks the mutex so other threads
    can accumulate their values. After adding its value to the final result, the block
    has nothing remaining to compute and can return from the kernel.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行的这个阶段，每个线程块中的256个线程已经求和它们的256个成对乘积，并计算出一个存储在`cache[0]`中的单一值。现在，每个线程块需要将它的最终值加到`c`上的值。为了安全地完成这个操作，我们将使用锁来控制对该内存位置的访问，因此每个线程在更新`c`值之前需要先获取锁。在将线程块的部分和添加到`c`上的值后，它会解锁互斥体，以便其他线程可以继续累加它们的值。在将自己的值添加到最终结果后，该线程块就没有剩余的计算，能够从内核返回。
- en: '![image](graphics/p0256-01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0256-01.jpg)'
- en: 'The `main()` routine is very similar to our original implementation, though
    it does have a couple differences. First, we no longer need to allocate a buffer
    for partial results as we did in [Chapter 5](ch05.html#ch05). We now allocate
    space for only a single floating-point result:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数与我们原始实现非常相似，尽管有一些不同之处。首先，我们不再需要像在[第5章](ch05.html#ch05)中那样为部分结果分配缓冲区。现在我们只为一个单一的浮点结果分配空间：'
- en: '![image](graphics/p0256-02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0256-02.jpg)'
- en: 'As we did in [Chapter 5](ch05.html#ch05), we initialize our input arrays and
    copy them to the GPU. But you’ll notice an additional copy in this example: We’re
    also copying a zero to `dev_c`, the location that we intend to use to accumulate
    our final dot product. Since each block wants to read this value, add its partial
    sum, and store the result back, we need the initial value to be zero in order
    to get the correct result.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第5章](ch05.html#ch05)中所做的那样，我们初始化输入数组并将它们复制到GPU。但你会注意到在这个例子中有一个额外的复制操作：我们还将零复制到`dev_c`，即我们打算用来累加最终点积的内存位置。由于每个线程块都需要读取这个值、添加它的部分和，并将结果存回，我们需要确保初始值为零，以便获得正确的结果。
- en: '![image](graphics/p0257-01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0257-01.jpg)'
- en: All that remains is declaring our `Lock`, invoking the kernel, and copying the
    result back to the CPU.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是声明我们的`Lock`，调用内核，并将结果复制回CPU。
- en: '![image](graphics/p0257-02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0257-02.jpg)'
- en: 'In [Chapter 5](ch05.html#ch05), this is when we would do a final `for()` loop
    to add the partial sums. Since this is done on the GPU using atomic locks, we
    can skip right to the answer-checking and cleanup code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#ch05)中，我们会进行最终的`for()`循环来添加部分和。由于这是在GPU上通过原子锁来实现的，因此我们可以直接跳到答案检查和清理代码：
- en: '![image](graphics/p0258-01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0258-01.jpg)'
- en: Because there is no way to precisely predict the order in which each block will
    add its partial sum to the final total, it is very likely (almost certain) that
    the final result will be summed in a different order than the CPU will sum it.
    Because of the nonassociativity of floating-point addition, it’s therefore quite
    probable that the final result will be slightly different between the GPU and
    CPU. There is not much that can be done about this without adding a nontrivial
    chunk of code to ensure that the blocks acquire the lock in a deterministic order
    that matches the summation order on the CPU. If you feel extraordinarily motivated,
    give this a try. Otherwise, we’ll move on to see how these atomic locks can be
    used to implement a multithreaded data structure.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无法精确预测每个块将如何按顺序将其部分和添加到最终总和，因此最终结果很可能（几乎可以肯定）将以与 CPU 累加顺序不同的顺序被加总。由于浮点加法的不结合性，因此最终结果在
    GPU 和 CPU 之间很可能会略有不同。除非添加一段非平凡的代码，以确保各个块按确定性顺序获取锁，这个顺序与 CPU 上的累加顺序匹配，否则对此几乎无法进行修正。如果你特别有动力，可以尝试一下这个方法。否则，我们将继续了解如何使用这些原子锁来实现多线程数据结构。
- en: '**A.2 Implementing a Hash Table**'
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**A.2 实现哈希表**'
- en: 'The hash table is one of the most important and commonly used data structures
    in computer science, playing an important role in a wide variety of applications.
    For readers not already familiar with hash tables, we’ll provide a quick primer
    here. The study of data structures warrants more in-depth study than we intend
    to provide, but in the interest of making forward progress, we will keep this
    brief. If you already feel comfortable with the concepts behind hash tables, you
    should skip to the hash table implementation in [Section A.2.2](app.html#ch13lev2sec2):
    A CPU Hash Table.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希表是计算机科学中最重要和最常用的数据结构之一，在各种应用中发挥着重要作用。对于那些还不熟悉哈希表的读者，我们将在这里提供一个简短的入门介绍。数据结构的研究比我们计划提供的更为深入，但为了推动进展，我们将简要介绍。如果你已经对哈希表背后的概念感到熟悉，可以跳到[第
    A.2.2 节](app.html#ch13lev2sec2)：一个 CPU 哈希表的实现。
- en: '**A.2.1 Hash Table Overview**'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**A.2.1 哈希表概述**'
- en: A hash table is essentially a structure that is designed to store pairs of *keys*
    and *values*. For example, you could think of a dictionary as a hash table. Every
    word in the dictionary is a *key*, and each word has a definition associated with
    it. The definition is the *value* associated with the word, and thus every word
    and definition in the dictionary form a key/value pair. For this data structure
    to be useful, though, it is important that we minimize the time it takes to find
    a particular value if we’re given a key. In general, this should be a constant
    amount of time. That is, the time to look up a value given a key should be the
    same, regardless of how many key/ value pairs are in the hash table.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希表本质上是一个用于存储*键*和*值*对的结构。例如，你可以将字典视为一个哈希表。字典中的每个单词都是一个*键*，每个单词都有一个相关的定义。这个定义是与单词相关联的*值*，因此字典中的每个单词和定义形成一个键/值对。为了使这个数据结构有用，重要的是要最小化在给定一个键的情况下查找特定值所需的时间。通常，这应该是一个常量时间。也就是说，给定一个键，查找一个值所需的时间应该是相同的，无论哈希表中有多少个键/值对。
- en: At an abstract level, our hash table will place values in “buckets” based on
    the value’s corresponding key. The method by which we map keys to buckets is often
    called the *hash function*. A good hash function will map the set of possible
    keys uniformly across all the buckets because this will help satisfy our requirement
    that it take constant time to find any value, regardless of the number of values
    we’ve added to the hash table.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从抽象层次来看，我们的哈希表会根据值对应的键将值放入“桶”中。我们将键映射到桶中的方法通常称为*哈希函数*。一个好的哈希函数会将可能的键集合均匀地分布到所有桶中，因为这将有助于满足我们的要求：无论我们向哈希表中添加了多少值，查找任何值都应该是常量时间。
- en: For example, consider our dictionary hash table. One obvious hash function would
    involve using 26 buckets, one for each letter of the alphabet. This simple hash
    function might simply look at the first letter of the key and put the value in
    one of the 26 buckets based on this letter. [Figure A.1](app.html#appfiga1) shows
    how this hash function would assign few sample words.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，考虑我们的字典哈希表。一个明显的哈希函数是使用 26 个桶，每个字母对应一个桶。这个简单的哈希函数可能只会查看键的第一个字母，并根据该字母将值放入
    26 个桶中的一个。[图 A.1](app.html#appfiga1)展示了这个哈希函数如何分配一些示例单词。
- en: '***Figure A.1*** Hashing of words into buckets'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 A.1*** 单词哈希到桶中的示意图'
- en: '![image](graphics/appendix_a_figure_7-5-1_u.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/appendix_a_figure_7-5-1_u.jpg)'
- en: Given what we know about the distribution of words in the English language,
    this hash function leaves much to be desired because it will not map words uniformly
    across the 26 buckets. Some of the buckets will contain very few key/value pairs,
    and some of the buckets will contain a large number of pairs. Accordingly, it
    will take much longer to find the value associated with a word that begins with
    a common letter such as S than it would take to find the value associated with
    a word that begins with the letter X. Since we are looking for hash functions
    that will give us constant-time retrieval of any value, this consequence is fairly
    undesirable. An immense amount of research has gone into the study of hash functions,
    but even a brief survey of these techniques is beyond the scope of this book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对英语中单词分布的了解，这个哈希函数有很大的改进空间，因为它无法在26个桶之间均匀地映射单词。一些桶将包含非常少的键/值对，而一些桶将包含大量的键/值对。因此，找到与以常见字母（例如S）开头的单词相关的值将比找到与以字母X开头的单词相关的值花费更多的时间。由于我们正在寻找能实现常数时间检索任何值的哈希函数，这种结果是相当不可取的。关于哈希函数的研究已经进行了大量工作，但即使是对这些技术的简要概述也超出了本书的范围。
- en: The last component of our hash table data structure involves the buckets. If
    we had a perfect hash function, every key would map to a different bucket. In
    this case, we can simply store the key/value pairs in an array where each entry
    in the array is what we’ve been calling a *bucket*. However, even with an excellent
    hash function, in most situations we will have to deal with *collisions*. A collision
    occurs when more than one key maps to a bucket, such as when we add both the words
    *avocado* and *aardvark* to our dictionary hash table. The simplest way to store
    all of the values that map to a given bucket is simply to maintain a list of values
    in the bucket. When we encounter a collision, such as adding *aardvark* to a dictionary
    that already contains *avocado*, we put the value associated with *aardvark* at
    the end of the list we’re maintaining in the “A” bucket, as shown in [Figure A.2](app.html#appfiga2).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的哈希表数据结构的最后一个组成部分涉及到桶。如果我们有一个完美的哈希函数，每个键都会映射到一个不同的桶。在这种情况下，我们可以简单地将键/值对存储在一个数组中，数组中的每个条目就是我们所称之为的*桶*。然而，即使有一个优秀的哈希函数，在大多数情况下我们仍然需要处理*冲突*。当多个键映射到同一个桶时，就会发生冲突，例如当我们将单词
    *avocado* 和 *aardvark* 都添加到字典哈希表中时。存储映射到某个桶的所有值的最简单方法是简单地在桶中维护一个值的列表。当我们遇到冲突时，例如将
    *aardvark* 添加到已经包含 *avocado* 的字典中，我们将与 *aardvark* 相关的值放到我们在“A”桶中维护的列表的末尾，如[图 A.2](app.html#appfiga2)所示。
- en: 'After adding the word *avocado* in [Figure A.2](app.html#appfiga2), the first
    bucket has a single key/ value pair in its list. Later in this imaginary application
    we add the word *aardvark*, a word that collides with *avocado* because they both
    start with the letter *A*. You will notice in [Figure A.3](app.html#appfiga3)
    that it simply gets placed at the end of the list in the first bucket:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 A.2](app.html#appfiga2)中添加单词 *avocado* 后，第一个桶的列表中有一个键/值对。稍后在这个假想的应用程序中，我们添加了单词
    *aardvark*，这个单词与 *avocado* 发生冲突，因为它们都以字母 *A* 开头。你会注意到在[图 A.3](app.html#appfiga3)中，它被简单地放置在第一个桶列表的末尾：
- en: '***Figure A.2*** Inserting the word *avocado* into the hash table'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 A.2*** 将单词 *avocado* 插入哈希表'
- en: '![image](graphics/appendix_a_figure_7-5-2_u.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/appendix_a_figure_7-5-2_u.jpg)'
- en: '***Figure A.3*** Resolving the conflict when adding the word *aardvark*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 A.3*** 解决添加单词 *aardvark* 时的冲突'
- en: '![image](graphics/appendix_a_figure_7-5-3_u.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/appendix_a_figure_7-5-3_u.jpg)'
- en: Armed with some background on the notions of a *hash function* and *collision
    resolution*, we’re ready to take a look at implementing our own hash table.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了*哈希函数*和*冲突解决*的概念后，我们已经准备好开始实现自己的哈希表。
- en: '**A.2.2 A CPU Hash Table**'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**A.2.2 一个 CPU 哈希表**'
- en: 'As described in the previous section, our hash table will consist of essentially
    two parts: a hash function and a data structure of buckets. Our buckets will be
    implemented exactly as before: We will allocate an array of length `N`, and each
    entry in the array holds a list of key/value pairs. Before concerning ourselves
    with a hash function, we will take a look at the data structures involved:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一节所述，我们的哈希表将基本由两部分组成：哈希函数和一个桶的数据结构。我们的桶将完全按照之前的方式实现：我们将分配一个长度为`N`的数组，每个数组条目存储一个键/值对的列表。在关注哈希函数之前，我们先来看一下涉及的数据显示结构：
- en: '![image](graphics/p0261-01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0261-01.jpg)'
- en: As described in the introductory section, the structure `Entry` holds both a
    key and a value. In our application, we will use unsigned integer keys to store
    our key/value pairs. The value associated with this key can be any data, so we
    have declared `value` as a `void*` to indicate this. Our application will primarily
    be concerned with creating the hash table data structure, so we won’t actually
    store anything in the `value` field. We have included it in the structure for
    completeness, in case you want to use this code in your own applications. The
    last piece of data in our hash table `Entry` is a pointer to the next `Entry`.
    After collisions, we’ll have multiple entries in the same bucket, and we have
    decided to store these entries as a list. So, each entry will point to the next
    entry in the bucket, thereby forming a list of entries that have hashed to the
    same location in the table. The last entry will have a `NULL` `next` pointer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如介绍部分所述，`Entry` 结构体同时保存一个键和值。在我们的应用中，我们将使用无符号整数键来存储我们的键值对。与此键相关联的值可以是任何数据，因此我们将
    `value` 声明为 `void*` 来表示这一点。我们的应用程序主要关注的是创建哈希表数据结构，因此我们实际上不会在 `value` 字段中存储任何内容。我们将其包含在结构体中是为了完整性，以防你在自己的应用中使用这段代码。我们哈希表
    `Entry` 结构中的最后一项数据是指向下一个 `Entry` 的指针。在发生冲突后，我们会在同一个桶中有多个条目，我们决定将这些条目存储为一个列表。因此，每个条目将指向桶中下一个条目，从而形成一个条目列表，这些条目在表中哈希到了相同的位置。最后一个条目的
    `next` 指针将为 `NULL`。
- en: At its heart, the `Table` structure itself is an array of “buckets.” This bucket
    array is just an array of length `count`, where each bucket in `entries` is just
    a pointer to an `Entry`. To avoid incurring the complication and performance hit
    of allocating memory every time we want to add an `Entry` to the table, the table
    will maintain a large array of available entries in `pool`. The field `firstFree`
    points to the next available `Entry` for use, so when we need to add an entry
    to the table, we can simply use the `Entry` pointed to by `firstFree` and increment
    that pointer. Note that this will also simplify our cleanup code because we can
    free all of these entries with a single call to `free()`. If we had allocated
    every entry as we went, we would have to walk through the table and free every
    entry one by one.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`Table` 结构的核心是一个“桶”的数组。这个桶数组只是一个长度为 `count` 的数组，其中 `entries` 中的每个桶只是指向一个 `Entry`
    的指针。为了避免每次向表中添加 `Entry` 时都需要分配内存带来的复杂性和性能损失，表将保持一个大量的可用条目数组 `pool`。字段 `firstFree`
    指向下一个可用的 `Entry`，因此当我们需要向表中添加条目时，我们可以直接使用 `firstFree` 指向的 `Entry` 并递增该指针。注意，这还将简化我们的清理代码，因为我们可以通过一次调用
    `free()` 来释放所有这些条目。如果我们在每次添加条目时都进行内存分配，那么我们就必须遍历整个表，逐个释放每个条目。'
- en: 'After understanding the data structures involved, let’s take a look at some
    of the other support code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了涉及的数据结构后，让我们来看一下其他一些支持代码：
- en: '![image](graphics/p0262-01.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0262-01.jpg)'
- en: Table initialization consists primarily of allocating memory and clearing memory
    for the bucket array `entries`. We also allocate storage for a pool of entries
    and initialize the `firstFree` pointer to be the first entry in the pool array.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表的初始化主要包括为桶数组 `entries` 分配内存并清理内存。我们还为条目池分配存储空间，并将 `firstFree` 指针初始化为池数组中的第一个条目。
- en: 'At the end of the application, we’ll want to free the memory we’ve allocated,
    so our cleanup routine frees the bucket array and the pool of free entries:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序结束时，我们需要释放我们分配的内存，因此我们的清理程序会释放桶数组和空闲条目的池：
- en: '![image](graphics/p0263-01.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0263-01.jpg)'
- en: 'In our introduction, we spoke quite a bit about the hash function. Specifically,
    we discussed how a good hash function can make the difference between an excellent
    hash table implementation and poor one. In this example, we’re using unsigned
    integers as our keys, and we need to map these to the indices of our bucket array.
    The simplest way to do this would be to select the bucket with an index equal
    to the key. That is, we could store the entry `e` in `table.entries[e.key]`. However,
    we have no way of guaranteeing that every key will be less than the length of
    the array of buckets. Fortunately, this problem can be solved relatively painlessly:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0263-02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: If the hash function is so important, how can we get away with such a simple
    one? Ideally, we want the keys to map uniformly across all the buckets in our
    table, and all we’re doing here is taking the key modulo the array length. In
    reality, hash functions may not normally be this simple, but because this is just
    an example program, we will be randomly generating our keys. If we assume that
    the random number generator generates values roughly uniformly, this hash function
    should map these keys uniformly across all of the buckets of the hash table. In
    your own hash table implementation, you may require a more complicated hash function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Having seen the hash table structures and the hash function, we’re ready to
    look at the process of adding a key/value pair to the table. The process involves
    three basic steps:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Compute the hash function on the input key to determine the new entry’s
    bucket.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Take a preallocated `Entry` from the pool and initialize its `key` and `value`
    fields.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Insert the entry at the front of the proper bucket’s list.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: We translate these steps to code in a fairly straightforward way.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0264-01.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: 'If you have never seen linked lists (or it’s been a while), step 3 may be tricky
    to understand at first. The existing list has its first node stored at `table.entries[hashValue]`.
    With this in mind, we can insert a new node at the head of the list in two steps:
    First, we set our new entry’s `next` pointer to point to the first node in the
    existing list. Then, we store the new entry in the bucket array so *it* becomes
    the first node of the new list.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Since it’s a good idea to have some idea whether the code you’ve written works,
    we’ve implemented a routine to perform a sanity check on a hash table. The check
    involves first walking through the table and examining every node. We compute
    the hash function on the node’s key and confirm that the node is stored in the
    correct bucket. After checking every node, we verify that the number of nodes
    *actually* in the table is indeed equal to the number of elements we *intended*
    to add to the table. If these numbers don’t agree, then either we’ve added a node
    accidentally to multiple buckets or we haven’t inserted it correctly.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0265-01.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: 'With all the infrastructure code out of the way, we can look at `main()`. As
    with many of this book’s examples, a lot of the heavy lifting has been done in
    helper functions, so we hope that `main()` will be relatively easy to follow:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0266-01.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: As you can see, we start by allocating a big chunk of random numbers. These
    randomly generated unsigned integers will be the keys we insert into our hash
    table. After generating the numbers, we read the system time in order to measure
    the performance of our implementation. We initialize the hash table and then insert
    each random key into the table using a `for()` loop. After adding all the keys,
    we read the system time again to compute the elapsed time to initialize and add
    the keys. Finally, we verify the hash table with our sanity check routine and
    free the buffers we’ve allocated.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: You probably noticed that we are using `NULL` as the value for every key/value
    pair. In a typical application, you would likely store some useful data with the
    key, but because we are primarily concerned with the hash table implementation
    itself, we’re storing a meaningless value with each key.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '**A.2.3 Multithreaded Hash Table**'
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are some assumptions built into our CPU hash table that will no longer
    be valid when we move to the GPU. First, we have assumed that only one node can
    be added to the table at a time in order to make the addition of a node simpler.
    If more than one thread were trying to add a node to the table at once, we could
    end up with problems similar to the multithreaded addition problems in the example
    from [Chapter 9](ch09.html#ch09).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s revisit our “avocado and aardvark” example and imagine that
    threads A and B are trying to add these entries to the table. Thread A computes
    a hash function on *avocado*, and thread B computes the function on *aardvark*.
    They both decide their keys belong in the same bucket. To add the new entry to
    the list, thread A and B start by setting their new entry’s `next` pointer to
    the first node of the existing list as in [Figure A.4](app.html#appfiga4).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Then, both threads try to replace the entry in the bucket array with their new
    entry. However, the thread that finishes second is the only thread that has its
    update preserved because it overwrites the work of the previous thread. So consider
    the scenario where thread A replaces the entry *altitude* with its entry for *avocado*.
    Immediately after finishing, thread B replaces what it believes to be the entry
    for *altitude* with its entry for *aardvark*. Unfortunately, it’s replacing *avocado*
    instead of *altitude*, resulting in the situation illustrated in [Figure A.5](app.html#appfiga5).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '***Figure A.4*** Multiple threads attempting to add a node to the same bucket'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/appendix_a_figure_7-5-4_u.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: '***Figure A.5*** The hash table after an unsuccessful concurrent modification
    by two threads'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/appendix_a_figure_7-5-5_u.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'Thread A’s entry is tragically “floating” outside of the hash table. Fortunately,
    our sanity check routine would catch this and alert us to the presence of a problem
    because it would count fewer nodes than we expected. But we still need to answer
    this question: How do we build a hash table on the GPU?! The key observation here
    involves the fact that only one thread can safely make modifications to a bucket
    at a time. This is similar to our dot product example where only one thread at
    a time could safely add its value to the final result. If each bucket had an atomic
    lock associated with it, we could ensure that only a single thread was making
    changes to a given bucket at a time.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**A.2.4 A GPU Hash Table**'
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Armed with a method to ensure safe multithreaded access to the hash table,
    we can proceed with a GPU implementation of the hash table application we wrote
    in [Section A.2.2](app.html#ch13lev2sec2): A CPU Hash Table. We’ll need to include
    `lock.h`, the implementation of our GPU `Lock` structure from [Section A.1.1](app.html#ch13lev1sec1)
    Atomic Locks, and we’ll need to declare the hash function as a `__device_` function.
    Aside from these changes, the fundamental data structures and hash function are
    identical to the CPU implementation.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0269-01.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Initializing and freeing the hash table consists of the same steps as we performed
    on the CPU, but as with previous examples, we use CUDA runtime functions to accomplish
    this. We use `cudaMalloc()` to allocate a bucket array and a pool of entries,
    and we use `cudaMemset()` to set the bucket array entries to zero. To free the
    memory upon application completion, we use `cudaFree()`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0269-02.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'We used a routine to check our hash table for correctness in the CPU implementation.
    We need a similar routine for the GPU version, so we have two options. We could
    write a GPU-based version of `verify_table()`, or we could use the same code we
    used in the CPU version and add a function that copies a hash table from the GPU
    to the CPU. Although either option gets us what we need, the second option seems
    superior for two reasons: First, it involves reusing our CPU version of `verify_table()`.
    As with code reuse in general, this saves time and ensures that future changes
    to the code would need to be made in only one place for both versions of the hash
    table. Second, implementing a copy function will uncover an interesting problem,
    the solution to which may be very useful to you in the future.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'As promised, `verify_table()` is identical to the CPU implementation and is
    reprinted here for your convenience:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0270-01.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Since we chose to reuse our CPU implementation of `verify_table()`, we need
    a function to copy the table from GPU memory to host memory. There are three steps
    to this function, two relatively obvious steps and a third, trickier step. The
    first two steps involve allocating host memory for the hash table data and performing
    a copy of the GPU data structures into this memory with `cudaMemcpy()`. We have
    done this many times previously, so this should come as no surprise.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0271-01.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: The tricky portion of this routine involves the fact that some of the data we
    have copied are pointers. We cannot simply copy these pointers to the host because
    they are addresses on the GPU; they will no longer be valid pointers on the host.
    However, the relative offsets of the pointers *will* still be valid. Every GPU
    pointer to an `Entry` points somewhere within the `table.pool[]` array, but for
    the hash table to be usable on the host, we need them to point to the same `Entry`
    in the `hostTable.pool[]` array.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a GPU pointer X, we therefore need to add the pointer’s offset from `table.pool`
    to `hostTable.pool` to get a valid host pointer. That is, the new pointer should
    be computed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: (X - table.pool) + hostTable.pool
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform this update for every `Entry` pointer we’ve copied from the GPU:
    the `Entry` pointers in `hostTable.entries` and the `next` pointer of every `Entry`
    in the table’s pool of entries:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0272-01.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Having seen the data structures, hash function, initialization, cleanup, and
    verification code, the most important piece remaining is the one that actually
    involves CUDA C atomics. As arguments, the `add_to_table()` kernel will take an
    array of keys and values to be added to the hash table. Its next argument is the
    hash table itself, and the final argument is an array of locks that will be used
    to lock each of the table’s buckets. Since our input is two arrays that our threads
    will need to index, we also need our all-too-common index linearization:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0272-02.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Our threads walk through the input arrays exactly like they did in the dot product
    example. For each key in the `keys[]` array, the thread will compute the hash
    function in order to determine which bucket the key/value pair belongs in. After
    determining the target bucket, the thread locks the bucket, adds its key/value
    pair, and unlocks the bucket.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0273-01.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: There is something remarkably peculiar about this bit of code, however. The
    `for()` loop and subsequent `if()` statement seem decidedly unnecessary. In [Chapter
    6](ch06.html#ch06), we introduced the concept of a *warp*. If you’ve forgotten,
    a warp is a collection of 32 threads that execute together in lockstep. Although
    the nuances of how this gets implemented in the GPU are beyond the scope of this
    book, only one thread in the warp can acquire the lock at a time, and we will
    suffer many a headache if we let all 32 threads in the warp contend for the lock
    simultaneously. In this situation, we’ve found that it’s best to do some of the
    work in software and simply walk through each thread in the warp, giving each
    a chance to acquire the data structure’s lock, do its work, and subsequently release
    the lock.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow of `main()` should appear identical to the CPU implementation. We
    start by allocating a large chunk of random data for our hash table keys. Then
    we create start and stop CUDA events and record the start event for our performance
    measurements. We proceed to allocate GPU memory for our array of random keys,
    copy the array up to the device, and initialize our hash table:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0274-01.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: 'The last step of preparation to build our hash table involves preparing locks
    for the hash table’s buckets. We allocate one lock for each bucket in the hash
    table. Conceivably we could save a lot of memory by using only one lock for the
    whole table. But doing so would utterly destroy performance because every thread
    would have to compete for the table lock whenever a group of threads tries to
    simultaneously add entries to the table. So we declare an array of locks, one
    for every bucket in the array. We then allocate a GPU array for the locks and
    copy them up to the device:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0275-01.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'The rest of `main()` is similar to the CPU version: We add all of our keys
    to the hash table, stop the performance timer, verify the correctness of the hash
    table, and clean up after ourselves:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/p0275-02.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: '**A.2.5 Hash Table Performance**'
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using an Intel Core 2 Duo, the CPU hash table example in [Section A.2.2](app.html#ch13lev2sec2):
    A CPU Hash Table takes 360ms to build a hash table from 100MB of data. The code
    was built with the option `-O3` to ensure maximally optimized CPU code. The multithreaded
    GPU hash table in [Section A.2.4](app.html#ch13lev2sec4): A GPU Hash Table takes
    375ms to complete the same task. Differing by less than 5 percent, these are roughly
    comparable execution times, which raises an excellent question: Why would such
    a massively parallel machine such as a GPU get beaten by a single-threaded CPU
    version of the same application? Frankly, this is because GPUs were not designed
    to excel at multithreaded access to complex data structures such as a hash table.
    For this reason, there are very few performance motivations to build a data structure
    such as a hash table on the GPU. So if *all* your application needs to do is build
    a hash table or similar data structure, you would likely be better off doing this
    on your CPU.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, you will sometimes find yourself in a situation where a
    long computation pipeline involves one or two stages that the GPU does not enjoy
    a performance advantage over comparable CPU implementations. In these situations,
    you have three (somewhat obvious) options:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: • Perform every step of the pipeline on the GPU
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: • Perform every step of the pipeline on the CPU
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: • Perform some pipeline steps on the GPU and some on the CPU
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The last option sounds like the best of both worlds; however, it implies that
    you will need to synchronize your CPU and GPU at any point in your application
    where you want to move computation from the GPU to CPU or back. This synchronization
    and subsequent data transfer between host and GPU can often kill any performance
    advantage you might have derived from employing a hybrid approach in the first
    place.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In such a situation, it may be worth your time to perform every phase of computation
    on the GPU, even if the GPU is not ideally suited for some steps of the algorithm.
    In this vein, the GPU hash table can potentially prevent a CPU/GPU synchronization
    point, minimize data transfer between the host and GPU and free the CPU to perform
    other computations. In such a scenario, it’s possible that the overall performance
    of a GPU implementation would exceed a CPU/GPU hybrid approach, despite the GPU
    being no faster than the CPU on certain steps (or potentially even getting trounced
    by the CPU in some cases).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**A.3 Appendix Review**'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw how to use atomic compare-and-swap operations to implement a GPU mutex.
    Using a lock built with this mutex, we saw how to improve our original dot product
    application to run entirely on the GPU. We carried this idea further by implementing
    a multithreaded hash table that used an array of locks to prevent unsafe simultaneous
    modifications by multiple threads. In fact, the mutex we developed could be used
    for any manner of parallel data structures, and we hope that you’ll find it useful
    in your own experimentation and application development. Of course, the performance
    of applications that use the GPU to implement mutex-based data structures needs
    careful study. Our GPU hash table gets beaten by a single-threaded CPU version
    of the same code, so it will make sense to use the GPU for this type of application
    only in certain situations. There is no blanket rule that can be used to determine
    whether a GPU-only, CPU-only, or hybrid approach will work best, but knowing how
    to use atomics will allow you to make that decision on a case-by-case basis.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
