["```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  cudaGetDeviceCount(&count);\n  cudaGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  hipGetDeviceCount(&count);\n  hipGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main()  {\n  Kokkos::initialize();\n\n  int  count  =  Kokkos::Cuda().concurrency();\n  int  device  =\n  Kokkos::Cuda().impl_internal_space_instance()->impl_internal_space_id();\n\n  std::cout  <<  \"Hello! I'm GPU \"  <<  device  <<  \" out of \"  <<  count\n  <<  \" GPUs in total.\"  <<  std::endl;\n\n  Kokkos::finalize();\n\n  return  0;\n} \n```", "```\n#include  <CL/opencl.h>\n#include  <stdio.h>\nint  main(void)  {\n  cl_uint  count;\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  &count);\n\n  char  deviceName[1024];\n  clGetDeviceInfo(device,  CL_DEVICE_NAME,  sizeof(deviceName),  deviceName,  NULL);\n\n  printf(\"Hello! I'm GPU %s out of %d GPUs in total.\\n\",  deviceName,  count);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  auto  gpu_devices  =  sycl::device::get_devices(sycl::info::device_type::gpu);\n  auto  count  =  gpu_devices.size();\n  std::cout  <<  \"Hello! I'm using a SYCL device by \"\n  <<  gpu_devices[0].get_info<sycl::info::device::vendor>()\n  <<  \", the first of \"  <<  count  <<  \" devices.\"  <<  std::endl;\n  return  0;\n} \n```", "```\n> cudaSetDevice(deviceNumber);  // For CUDA\n> hipSetDevice(deviceNumber);  // For HIP \n> ```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n  int  i;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  cudaMalloc((void  **)&Ad,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Bd,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  cudaMemcpy(Ad,  Ah,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n  cudaMemcpy(Bd,  Bh,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  cudaMemcpy(Ch,  Cd,  sizeof(float)  *  N,  cudaMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ad);\n  cudaFree(Bd);\n  cudaFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  hipMalloc((void  **)&Ad,  N  *  sizeof(float));\n  hipMalloc((void  **)&Bd,  N  *  sizeof(float));\n  hipMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  hipMemcpy(Ad,  Ah,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n  hipMemcpy(Bd,  Bh,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ad, Bd, Cd, N); // or\n  vector_add<<<blocks,  threads,  0,  0>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  hipMemcpy(Ch,  Cd,  sizeof(float)  *  N,  hipMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  hipFree(Ad);\n  hipFree(Bd);\n  hipFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n// We're using C API here; examples with C++ API can be found in the \"Portable\n// kernel models\" chapter\n#include  <CL/cl.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n#define N 10000\n\nstatic  const  char  *programSource  =\n  \"__kernel void vector_add(__global const float* A, __global const float* \"\n  \"B, __global float* C, int N) {\\n\"\n  \"    int tid = get_global_id(0);\\n\"\n  \"    if (tid < N) {\\n\"\n  \"        C[tid] = A[tid] + B[tid];\\n\"\n  \"    }\\n\"\n  \"}\\n\";\n\nint  main()  {\n  // Initialize data and calculate reference values on CPU\n  float  Ah[N],  Bh[N],  Ch[N],  Cref[N];\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3f;\n  Bh[i]  =  cos(i)  *  1.1f;\n  Ch[i]  =  12.f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Use the default device\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Build the kernel from string\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &programSource,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"vector_add\",  NULL);\n\n  // Allocate the arrays on GPU\n  cl_mem  d_A  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_B  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_C  =\n  clCreateBuffer(context,  CL_MEM_WRITE_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n\n  clEnqueueWriteBuffer(queue,  d_A,  CL_TRUE,  0,  N  *  sizeof(float),  Ah,  0,  NULL,\n  NULL);\n  clEnqueueWriteBuffer(queue,  d_B,  CL_TRUE,  0,  N  *  sizeof(float),  Bh,  0,  NULL,\n  NULL);\n\n  // Set arguments and launch the kernel\n  clSetKernelArg(kernel,  0,  sizeof(cl_mem),  &d_A);\n  clSetKernelArg(kernel,  1,  sizeof(cl_mem),  &d_B);\n  clSetKernelArg(kernel,  2,  sizeof(cl_mem),  &d_C);\n  cl_int  N_as_cl_int  =  N;\n  clSetKernelArg(kernel,  3,  sizeof(cl_int),  &N_as_cl_int);\n  size_t  globalSize  =  N;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Copy the results back\n  clEnqueueReadBuffer(queue,  d_C,  CL_TRUE,  0,  N  *  sizeof(float),  Ch,  0,  NULL,\n  NULL);\n\n  // Print reference and result values\n  printf(\"Reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"Result   : %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  printf(\"Total error: %f\\n\",  error);\n  printf(\"Reference:   %f at (42)\\n\",  Cref[42]);\n  printf(\"Result   :   %f at (42)\\n\",  Ch[42]);\n\n  clReleaseMemObject(d_A);\n  clReleaseMemObject(d_B);\n  clReleaseMemObject(d_C);\n  clReleaseKernel(kernel);\n  clReleaseProgram(program);\n  clReleaseCommandQueue(queue);\n  clReleaseContext(context);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Ah(N);\n  std::vector<float>  Bh(N);\n  std::vector<float>  Ch(N);\n  std::vector<float>  Cref(N);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  std::sin(i)  *  2.3f;\n  Bh[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  float  *Ad  =  sycl::malloc_device<float>(N,  q);\n  float  *Bd  =  sycl::malloc_device<float>(N,  q);\n  float  *Cd  =  sycl::malloc_device<float>(N,  q);\n\n  q.copy<float>(Ah.data(),  Ad,  N);\n  q.copy<float>(Bh.data(),  Bd,  N);\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  Cd[tid]  =  Ad[tid]  +  Bd[tid];\n  });\n  });\n\n  // Copy results back to CPU\n  sycl::event  eventCCopy  =  q.copy<float>(Cd,  Ch.data(),  N);\n  // Wait for the copy to finish\n  eventCCopy.wait();\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  Ch[0]  <<  \" \"  <<  Ch[1]  <<  \" \"  <<  Ch[2]  <<  \" \"\n  <<  Ch[3]  <<  \" ... \"  <<  Ch[N  -  2]  <<  \" \"  <<  Ch[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  Ch[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the GPU memory\n  sycl::free(Ad,  q);\n  sycl::free(Bd,  q);\n  sycl::free(Cd,  q);\n\n  return  0;\n} \n```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  int  i;\n\n  // Allocate the arrays using Unified Memory\n  cudaMallocManaged(&Ah,  N  *  sizeof(float));\n  cudaMallocManaged(&Bh,  N  *  sizeof(float));\n  cudaMallocManaged(&Ch,  N  *  sizeof(float));\n  cudaMallocManaged(&Cref,  N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // define grid dimensions\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  cudaDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ah);\n  cudaFree(Bh);\n  cudaFree(Ch);\n  cudaFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  // Allocate the arrays using Unified Memory\n  hipMallocManaged((void  **)&Ah,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Bh,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Ch,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Cref,  N  *  sizeof(float));\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n  // All data at this point is on CPU\n\n  // Define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ah, Bh, Ch, N); // or\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  hipDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on the CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the Unified Memory arrays\n  hipFree(Ah);\n  hipFree(Bh);\n  hipFree(Ch);\n  hipFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Cref(N);\n\n  // Allocate the shared arrays\n  float  *A  =  sycl::malloc_shared<float>(N,  q);\n  float  *B  =  sycl::malloc_shared<float>(N,  q);\n  float  *C  =  sycl::malloc_shared<float>(N,  q);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  A[i]  =  std::sin(i)  *  2.3f;\n  B[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  A[i]  +  B[i];\n  }\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  C[tid]  =  A[tid]  +  B[tid];\n  });\n  }).wait();  // Wait for the kernel to finish\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  C[0]  <<  \" \"  <<  C[1]  <<  \" \"  <<  C[2]  <<  \" \"\n  <<  C[3]  <<  \" ... \"  <<  C[N  -  2]  <<  \" \"  <<  C[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  C[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  C[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the shared memory\n  sycl::free(A,  q);\n  sycl::free(B,  q);\n  sycl::free(C,  q);\n\n  return  0;\n} \n```", "```\n#include  <cstdlib>\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  cudaMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  cudaMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  cudaMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  cudaMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  cudaEvent_t  start_kernel_event;\n  cudaEventCreate(&start_kernel_event);\n  cudaEvent_t  end_kernel_event;\n  cudaEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(end_kernel_event,  0);\n  cudaEventSynchronize(end_kernel_event);\n\n  cudaDeviceSynchronize();\n  float  time_kernel;\n  cudaEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  cudaMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  cudaMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n\n#include  <cstdlib>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  hipMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  hipMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  hipMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  hipMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  hipEvent_t  start_kernel_event;\n  hipEventCreate(&start_kernel_event);\n  hipEvent_t  end_kernel_event;\n  hipEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(end_kernel_event,  0);\n  hipEventSynchronize(end_kernel_event);\n\n  hipDeviceSynchronize();\n  float  time_kernel;\n  hipEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  hipMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  hipMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n// Instead of defining kernel lambda at the place of submission,\n// we can define it here:\nauto  copyKernel(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  index  =  y_index  *  width  +  x_index;\n  out[index]  =  in[index];\n  };\n}\n\nint  main()  {\n  std::vector<float>  matrix_in(width  *  height);\n  std::vector<float>  matrix_out(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  // Create queue on the default device with profiling enabled\n  sycl::queue  queue{{sycl::property::queue::in_order(),\n  sycl::property::queue::enable_profiling()}};\n\n  float  *d_in  =  sycl::malloc_device<float>(width  *  height,  queue);\n  float  *d_out  =  sycl::malloc_device<float>(width  *  height,  queue);\n\n  queue.copy<float>(matrix_in.data(),  d_in,  width  *  height);\n  queue.wait();\n\n  printf(\"Setup complete. Launching kernel\\n\");\n  sycl::range<2>  global_size{height,  width},  local_size{tile_dim,  tile_dim};\n  sycl::nd_range<2>  kernel_range{global_size,  local_size};\n\n  // Create events\n  printf(\"Warm up the GPU!\\n\");\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  }\n\n  // Unlike in CUDA or HIP, for SYCL we have to store all events\n  std::vector<sycl::event>  kernel_events;\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  sycl::event  kernel_event  =  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  kernel_events.push_back(kernel_event);\n  }\n\n  queue.wait();\n\n  auto  first_kernel_started  =\n  kernel_events.front()\n  .get_profiling_info<sycl::info::event_profiling::command_start>();\n  auto  last_kernel_ended  =\n  kernel_events.back()\n  .get_profiling_info<sycl::info::event_profiling::command_end>();\n  double  total_kernel_time_ns  =\n  static_cast<double>(last_kernel_ended  -  first_kernel_started);\n  double  time_kernels  =  total_kernel_time_ns  /  1e6;  // convert ns to ms\n  double  bandwidth  =  2.0  *  10000  *\n  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernels  *  1024  *  1024  *  1024);\n\n  printf(\"Kernel execution complete\\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6lf ms - copy\\n Bandwidth %.6lf GB/s\\n\",  time_kernels  /  10,\n  bandwidth);\n\n  sycl::free(d_in,  queue);\n  sycl::free(d_out,  queue);\n  return  0;\n} \n```", "```\n__global__  void  transpose_naive_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n\n  out[out_index]  =  in[in_index];\n} \n```", "```\nauto  transposeKernelNaive(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n  out[out_index]  =  in[in_index];\n  };\n} \n```", "```\n> __global__  void  transpose_SM_kernel(float  *in,  float  *out,  int  width,\n>   int  height)  {\n>   __shared__  float  tile[tile_dim][tile_dim];\n> \n>   int  x_tile_index  =  blockIdx.x  *  tile_dim;\n>   int  y_tile_index  =  blockIdx.y  *  tile_dim;\n> \n>   int  in_index  =\n>   (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n>   int  out_index  =\n>   (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n> \n>   tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n> \n>   __syncthreads();\n> \n>   out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n> } \n> ```", "```\n> auto  transposeKernelSM(sycl::handler  &cgh,  const  float  *in,  float  *out,\n>   int  width,  int  height)  {\n>   sycl::local_accessor<float,  1>  tile{{tile_dim  *  tile_dim},  cgh};\n>   return  [=](sycl::nd_item<2>  item)  {\n>   int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n>   int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n>   int  x_local_index  =  item.get_local_id(1);\n>   int  y_local_index  =  item.get_local_id(0);\n>   int  in_index  =\n>   (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n>   int  out_index  =\n>   (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n> \n>   tile[y_local_index  *  tile_dim  +  x_local_index]  =  in[in_index];\n>   item.barrier();\n>   out[out_index]  =  tile[x_local_index  *  tile_dim  +  y_local_index];\n>   };\n> } \n> ```", "```\n__global__  void  transpose_SM_nobc_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  __shared__  float  tile[tile_dim][tile_dim  +  1];\n\n  int  x_tile_index  =  blockIdx.x  *  tile_dim;\n  int  y_tile_index  =  blockIdx.y  *  tile_dim;\n\n  int  in_index  =\n  (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n  int  out_index  =\n  (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n\n  tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n\n  __syncthreads();\n\n  out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n} \n```", "```\nauto  transposeKernelSMNoBC(sycl::handler  &cgh,  const  float  *in,  float  *out,\n  int  width,  int  height)  {\n  sycl::local_accessor<float,  1>  tile{{tile_dim  *  (tile_dim  +  1)},  cgh};\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n  int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n  int  x_local_index  =  item.get_local_id(1);\n  int  y_local_index  =  item.get_local_id(0);\n  int  in_index  =\n  (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n  int  out_index  =\n  (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n\n  tile[y_local_index  *  (tile_dim  +  1)  +  x_local_index]  =  in[in_index];\n  item.barrier();\n  out[out_index]  =  tile[x_local_index  *  (tile_dim  +  1)  +  y_local_index];\n  };\n} \n```", "```\n#define tpb 512 // size in this case has to be known at compile time\n// this kernel has to be launched with at least N/2 threads\n__global__  void  reduction_one(double  x,  double  *sum,  int  N){\n  int  ibl=blockIdx.y+blockIdx.x*gridDim.y;\n  int  ind=threadIdx.x+blockDim.x*ibl;\n\n  __shared__  double  shtmp[2*tpb];\n  shtmp[threadIdx.x]=0;  // for sums we initiate with 0, for other operations should be different\n  if(ind<N/2)\n  {\n  shtmp[threadIdx.x]=x[ind];\n  }\n  if(ind+N/2<N)\n  {\n  shtmp[threadIdx.x+tpb]=x[ind+N/2];\n  }\n  __syncthreads();\n  for(int  s=tpb;s>0;s>>=1){\n  if(threadIdx.x<s){\n  shtmp[threadIdx.x]+=shtmp[threadIdx.x+s];}\n  __syncthreads();\n  }\n  if(threadIdx.x==0)\n  {\n  sum[ibl]=shtmp[0];  // each block saves its partial result to an array\n  // atomicAdd(&sum[0], shene[0]); // alternatively could aggregate everything together at index 0\\. Only use when there not many partial sums left\n  }\n} \n```", "```\n// SYCL has built-in sycl::reduction primitive, the use of which is demonstrated\n// in the \"Portable kernel models\" chapter. Here is how the reduction can be\n// implemented manually:\nauto  redutionKernel(sycl::handler  &cgh,  double  *x,  double  *sum,  int  N)  {\n  sycl::local_accessor<double,  1>  shtmp{{2  *  tpb},  cgh};\n  return  [=](sycl::nd_item<1>  item)  {\n  int  ibl  =  item.get_group(0);\n  int  ind  =  item.get_global_id(0);\n  int  tid  =  item.get_local_id(0);\n  shtmp[item.get_local_id(0)]  =  0;\n  if  (ind  <  N  /  2)  {\n  shtmp[tid]  =  x[ind];\n  }  else  {\n  shtmp[tid]  =  0.0;\n  }\n  if  (ind  +  N  /  2  <  N)  {\n  shtmp[tid  +  tpb]  =  x[ind  +  N  /  2];\n  }  else  {\n  shtmp[tid  +  tpb]  =  0.0;\n  }\n\n  for  (int  s  =  tpb;  s  >  0;  s  >>=  1)  {\n  if  (tid  <  s)  {\n  shtmp[tid]  +=  shtmp[tid  +  s];\n  }\n  item.barrier();\n  }\n  if  (tid  ==  0)  {\n  if  constexpr  (useHostReduction)  {\n  sum[ibl]  =  shtmp[0];  // each block saves its partial result to an array\n  }  else  {\n  // Alternatively, we could agregate everything together at index 0.\n  // Only useful when there not many partial sums left and when the device\n  // supports atomic operations on FP64/double operands.\n  sycl::atomic_ref<double,  sycl::memory_order::relaxed,\n  sycl::memory_scope::device,\n  sycl::access::address_space::global_space>\n  ref(sum[0]);\n  ref.fetch_add(shtmp[0]);\n  }\n  }\n  };\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  stream_size;\n  cudaEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  cudaMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  cudaMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  cudaMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  cudaMemcpyDeviceToHost,  stream[i]);\n\n  cudaEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  (N/stream_size);\n  hipEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  hipMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  hipMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  hipMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  hipMemcpyDeviceToHost,  stream[i]);\n\n  hipEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n}\n... \n```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  cudaGetDeviceCount(&count);\n  cudaGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  hipGetDeviceCount(&count);\n  hipGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main()  {\n  Kokkos::initialize();\n\n  int  count  =  Kokkos::Cuda().concurrency();\n  int  device  =\n  Kokkos::Cuda().impl_internal_space_instance()->impl_internal_space_id();\n\n  std::cout  <<  \"Hello! I'm GPU \"  <<  device  <<  \" out of \"  <<  count\n  <<  \" GPUs in total.\"  <<  std::endl;\n\n  Kokkos::finalize();\n\n  return  0;\n} \n```", "```\n#include  <CL/opencl.h>\n#include  <stdio.h>\nint  main(void)  {\n  cl_uint  count;\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  &count);\n\n  char  deviceName[1024];\n  clGetDeviceInfo(device,  CL_DEVICE_NAME,  sizeof(deviceName),  deviceName,  NULL);\n\n  printf(\"Hello! I'm GPU %s out of %d GPUs in total.\\n\",  deviceName,  count);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  auto  gpu_devices  =  sycl::device::get_devices(sycl::info::device_type::gpu);\n  auto  count  =  gpu_devices.size();\n  std::cout  <<  \"Hello! I'm using a SYCL device by \"\n  <<  gpu_devices[0].get_info<sycl::info::device::vendor>()\n  <<  \", the first of \"  <<  count  <<  \" devices.\"  <<  std::endl;\n  return  0;\n} \n```", "```\n> cudaSetDevice(deviceNumber);  // For CUDA\n> hipSetDevice(deviceNumber);  // For HIP \n> ```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n  int  i;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  cudaMalloc((void  **)&Ad,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Bd,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  cudaMemcpy(Ad,  Ah,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n  cudaMemcpy(Bd,  Bh,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  cudaMemcpy(Ch,  Cd,  sizeof(float)  *  N,  cudaMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ad);\n  cudaFree(Bd);\n  cudaFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  hipMalloc((void  **)&Ad,  N  *  sizeof(float));\n  hipMalloc((void  **)&Bd,  N  *  sizeof(float));\n  hipMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  hipMemcpy(Ad,  Ah,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n  hipMemcpy(Bd,  Bh,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ad, Bd, Cd, N); // or\n  vector_add<<<blocks,  threads,  0,  0>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  hipMemcpy(Ch,  Cd,  sizeof(float)  *  N,  hipMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  hipFree(Ad);\n  hipFree(Bd);\n  hipFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n// We're using C API here; examples with C++ API can be found in the \"Portable\n// kernel models\" chapter\n#include  <CL/cl.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n#define N 10000\n\nstatic  const  char  *programSource  =\n  \"__kernel void vector_add(__global const float* A, __global const float* \"\n  \"B, __global float* C, int N) {\\n\"\n  \"    int tid = get_global_id(0);\\n\"\n  \"    if (tid < N) {\\n\"\n  \"        C[tid] = A[tid] + B[tid];\\n\"\n  \"    }\\n\"\n  \"}\\n\";\n\nint  main()  {\n  // Initialize data and calculate reference values on CPU\n  float  Ah[N],  Bh[N],  Ch[N],  Cref[N];\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3f;\n  Bh[i]  =  cos(i)  *  1.1f;\n  Ch[i]  =  12.f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Use the default device\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Build the kernel from string\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &programSource,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"vector_add\",  NULL);\n\n  // Allocate the arrays on GPU\n  cl_mem  d_A  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_B  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_C  =\n  clCreateBuffer(context,  CL_MEM_WRITE_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n\n  clEnqueueWriteBuffer(queue,  d_A,  CL_TRUE,  0,  N  *  sizeof(float),  Ah,  0,  NULL,\n  NULL);\n  clEnqueueWriteBuffer(queue,  d_B,  CL_TRUE,  0,  N  *  sizeof(float),  Bh,  0,  NULL,\n  NULL);\n\n  // Set arguments and launch the kernel\n  clSetKernelArg(kernel,  0,  sizeof(cl_mem),  &d_A);\n  clSetKernelArg(kernel,  1,  sizeof(cl_mem),  &d_B);\n  clSetKernelArg(kernel,  2,  sizeof(cl_mem),  &d_C);\n  cl_int  N_as_cl_int  =  N;\n  clSetKernelArg(kernel,  3,  sizeof(cl_int),  &N_as_cl_int);\n  size_t  globalSize  =  N;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Copy the results back\n  clEnqueueReadBuffer(queue,  d_C,  CL_TRUE,  0,  N  *  sizeof(float),  Ch,  0,  NULL,\n  NULL);\n\n  // Print reference and result values\n  printf(\"Reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"Result   : %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  printf(\"Total error: %f\\n\",  error);\n  printf(\"Reference:   %f at (42)\\n\",  Cref[42]);\n  printf(\"Result   :   %f at (42)\\n\",  Ch[42]);\n\n  clReleaseMemObject(d_A);\n  clReleaseMemObject(d_B);\n  clReleaseMemObject(d_C);\n  clReleaseKernel(kernel);\n  clReleaseProgram(program);\n  clReleaseCommandQueue(queue);\n  clReleaseContext(context);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Ah(N);\n  std::vector<float>  Bh(N);\n  std::vector<float>  Ch(N);\n  std::vector<float>  Cref(N);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  std::sin(i)  *  2.3f;\n  Bh[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  float  *Ad  =  sycl::malloc_device<float>(N,  q);\n  float  *Bd  =  sycl::malloc_device<float>(N,  q);\n  float  *Cd  =  sycl::malloc_device<float>(N,  q);\n\n  q.copy<float>(Ah.data(),  Ad,  N);\n  q.copy<float>(Bh.data(),  Bd,  N);\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  Cd[tid]  =  Ad[tid]  +  Bd[tid];\n  });\n  });\n\n  // Copy results back to CPU\n  sycl::event  eventCCopy  =  q.copy<float>(Cd,  Ch.data(),  N);\n  // Wait for the copy to finish\n  eventCCopy.wait();\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  Ch[0]  <<  \" \"  <<  Ch[1]  <<  \" \"  <<  Ch[2]  <<  \" \"\n  <<  Ch[3]  <<  \" ... \"  <<  Ch[N  -  2]  <<  \" \"  <<  Ch[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  Ch[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the GPU memory\n  sycl::free(Ad,  q);\n  sycl::free(Bd,  q);\n  sycl::free(Cd,  q);\n\n  return  0;\n} \n```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  int  i;\n\n  // Allocate the arrays using Unified Memory\n  cudaMallocManaged(&Ah,  N  *  sizeof(float));\n  cudaMallocManaged(&Bh,  N  *  sizeof(float));\n  cudaMallocManaged(&Ch,  N  *  sizeof(float));\n  cudaMallocManaged(&Cref,  N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // define grid dimensions\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  cudaDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ah);\n  cudaFree(Bh);\n  cudaFree(Ch);\n  cudaFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  // Allocate the arrays using Unified Memory\n  hipMallocManaged((void  **)&Ah,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Bh,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Ch,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Cref,  N  *  sizeof(float));\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n  // All data at this point is on CPU\n\n  // Define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ah, Bh, Ch, N); // or\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  hipDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on the CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the Unified Memory arrays\n  hipFree(Ah);\n  hipFree(Bh);\n  hipFree(Ch);\n  hipFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Cref(N);\n\n  // Allocate the shared arrays\n  float  *A  =  sycl::malloc_shared<float>(N,  q);\n  float  *B  =  sycl::malloc_shared<float>(N,  q);\n  float  *C  =  sycl::malloc_shared<float>(N,  q);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  A[i]  =  std::sin(i)  *  2.3f;\n  B[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  A[i]  +  B[i];\n  }\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  C[tid]  =  A[tid]  +  B[tid];\n  });\n  }).wait();  // Wait for the kernel to finish\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  C[0]  <<  \" \"  <<  C[1]  <<  \" \"  <<  C[2]  <<  \" \"\n  <<  C[3]  <<  \" ... \"  <<  C[N  -  2]  <<  \" \"  <<  C[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  C[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  C[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the shared memory\n  sycl::free(A,  q);\n  sycl::free(B,  q);\n  sycl::free(C,  q);\n\n  return  0;\n} \n```", "```\n#include  <cstdlib>\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  cudaMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  cudaMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  cudaMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  cudaMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  cudaEvent_t  start_kernel_event;\n  cudaEventCreate(&start_kernel_event);\n  cudaEvent_t  end_kernel_event;\n  cudaEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(end_kernel_event,  0);\n  cudaEventSynchronize(end_kernel_event);\n\n  cudaDeviceSynchronize();\n  float  time_kernel;\n  cudaEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  cudaMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  cudaMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n\n#include  <cstdlib>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  hipMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  hipMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  hipMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  hipMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  hipEvent_t  start_kernel_event;\n  hipEventCreate(&start_kernel_event);\n  hipEvent_t  end_kernel_event;\n  hipEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(end_kernel_event,  0);\n  hipEventSynchronize(end_kernel_event);\n\n  hipDeviceSynchronize();\n  float  time_kernel;\n  hipEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  hipMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  hipMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n// Instead of defining kernel lambda at the place of submission,\n// we can define it here:\nauto  copyKernel(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  index  =  y_index  *  width  +  x_index;\n  out[index]  =  in[index];\n  };\n}\n\nint  main()  {\n  std::vector<float>  matrix_in(width  *  height);\n  std::vector<float>  matrix_out(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  // Create queue on the default device with profiling enabled\n  sycl::queue  queue{{sycl::property::queue::in_order(),\n  sycl::property::queue::enable_profiling()}};\n\n  float  *d_in  =  sycl::malloc_device<float>(width  *  height,  queue);\n  float  *d_out  =  sycl::malloc_device<float>(width  *  height,  queue);\n\n  queue.copy<float>(matrix_in.data(),  d_in,  width  *  height);\n  queue.wait();\n\n  printf(\"Setup complete. Launching kernel\\n\");\n  sycl::range<2>  global_size{height,  width},  local_size{tile_dim,  tile_dim};\n  sycl::nd_range<2>  kernel_range{global_size,  local_size};\n\n  // Create events\n  printf(\"Warm up the GPU!\\n\");\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  }\n\n  // Unlike in CUDA or HIP, for SYCL we have to store all events\n  std::vector<sycl::event>  kernel_events;\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  sycl::event  kernel_event  =  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  kernel_events.push_back(kernel_event);\n  }\n\n  queue.wait();\n\n  auto  first_kernel_started  =\n  kernel_events.front()\n  .get_profiling_info<sycl::info::event_profiling::command_start>();\n  auto  last_kernel_ended  =\n  kernel_events.back()\n  .get_profiling_info<sycl::info::event_profiling::command_end>();\n  double  total_kernel_time_ns  =\n  static_cast<double>(last_kernel_ended  -  first_kernel_started);\n  double  time_kernels  =  total_kernel_time_ns  /  1e6;  // convert ns to ms\n  double  bandwidth  =  2.0  *  10000  *\n  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernels  *  1024  *  1024  *  1024);\n\n  printf(\"Kernel execution complete\\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6lf ms - copy\\n Bandwidth %.6lf GB/s\\n\",  time_kernels  /  10,\n  bandwidth);\n\n  sycl::free(d_in,  queue);\n  sycl::free(d_out,  queue);\n  return  0;\n} \n```", "```\n__global__  void  transpose_naive_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n\n  out[out_index]  =  in[in_index];\n} \n```", "```\nauto  transposeKernelNaive(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n  out[out_index]  =  in[in_index];\n  };\n} \n```", "```\n> __global__  void  transpose_SM_kernel(float  *in,  float  *out,  int  width,\n>   int  height)  {\n>   __shared__  float  tile[tile_dim][tile_dim];\n> \n>   int  x_tile_index  =  blockIdx.x  *  tile_dim;\n>   int  y_tile_index  =  blockIdx.y  *  tile_dim;\n> \n>   int  in_index  =\n>   (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n>   int  out_index  =\n>   (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n> \n>   tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n> \n>   __syncthreads();\n> \n>   out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n> } \n> ```", "```\n> auto  transposeKernelSM(sycl::handler  &cgh,  const  float  *in,  float  *out,\n>   int  width,  int  height)  {\n>   sycl::local_accessor<float,  1>  tile{{tile_dim  *  tile_dim},  cgh};\n>   return  [=](sycl::nd_item<2>  item)  {\n>   int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n>   int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n>   int  x_local_index  =  item.get_local_id(1);\n>   int  y_local_index  =  item.get_local_id(0);\n>   int  in_index  =\n>   (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n>   int  out_index  =\n>   (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n> \n>   tile[y_local_index  *  tile_dim  +  x_local_index]  =  in[in_index];\n>   item.barrier();\n>   out[out_index]  =  tile[x_local_index  *  tile_dim  +  y_local_index];\n>   };\n> } \n> ```", "```\n__global__  void  transpose_SM_nobc_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  __shared__  float  tile[tile_dim][tile_dim  +  1];\n\n  int  x_tile_index  =  blockIdx.x  *  tile_dim;\n  int  y_tile_index  =  blockIdx.y  *  tile_dim;\n\n  int  in_index  =\n  (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n  int  out_index  =\n  (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n\n  tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n\n  __syncthreads();\n\n  out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n} \n```", "```\nauto  transposeKernelSMNoBC(sycl::handler  &cgh,  const  float  *in,  float  *out,\n  int  width,  int  height)  {\n  sycl::local_accessor<float,  1>  tile{{tile_dim  *  (tile_dim  +  1)},  cgh};\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n  int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n  int  x_local_index  =  item.get_local_id(1);\n  int  y_local_index  =  item.get_local_id(0);\n  int  in_index  =\n  (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n  int  out_index  =\n  (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n\n  tile[y_local_index  *  (tile_dim  +  1)  +  x_local_index]  =  in[in_index];\n  item.barrier();\n  out[out_index]  =  tile[x_local_index  *  (tile_dim  +  1)  +  y_local_index];\n  };\n} \n```", "```\n#define tpb 512 // size in this case has to be known at compile time\n// this kernel has to be launched with at least N/2 threads\n__global__  void  reduction_one(double  x,  double  *sum,  int  N){\n  int  ibl=blockIdx.y+blockIdx.x*gridDim.y;\n  int  ind=threadIdx.x+blockDim.x*ibl;\n\n  __shared__  double  shtmp[2*tpb];\n  shtmp[threadIdx.x]=0;  // for sums we initiate with 0, for other operations should be different\n  if(ind<N/2)\n  {\n  shtmp[threadIdx.x]=x[ind];\n  }\n  if(ind+N/2<N)\n  {\n  shtmp[threadIdx.x+tpb]=x[ind+N/2];\n  }\n  __syncthreads();\n  for(int  s=tpb;s>0;s>>=1){\n  if(threadIdx.x<s){\n  shtmp[threadIdx.x]+=shtmp[threadIdx.x+s];}\n  __syncthreads();\n  }\n  if(threadIdx.x==0)\n  {\n  sum[ibl]=shtmp[0];  // each block saves its partial result to an array\n  // atomicAdd(&sum[0], shene[0]); // alternatively could aggregate everything together at index 0\\. Only use when there not many partial sums left\n  }\n} \n```", "```\n// SYCL has built-in sycl::reduction primitive, the use of which is demonstrated\n// in the \"Portable kernel models\" chapter. Here is how the reduction can be\n// implemented manually:\nauto  redutionKernel(sycl::handler  &cgh,  double  *x,  double  *sum,  int  N)  {\n  sycl::local_accessor<double,  1>  shtmp{{2  *  tpb},  cgh};\n  return  [=](sycl::nd_item<1>  item)  {\n  int  ibl  =  item.get_group(0);\n  int  ind  =  item.get_global_id(0);\n  int  tid  =  item.get_local_id(0);\n  shtmp[item.get_local_id(0)]  =  0;\n  if  (ind  <  N  /  2)  {\n  shtmp[tid]  =  x[ind];\n  }  else  {\n  shtmp[tid]  =  0.0;\n  }\n  if  (ind  +  N  /  2  <  N)  {\n  shtmp[tid  +  tpb]  =  x[ind  +  N  /  2];\n  }  else  {\n  shtmp[tid  +  tpb]  =  0.0;\n  }\n\n  for  (int  s  =  tpb;  s  >  0;  s  >>=  1)  {\n  if  (tid  <  s)  {\n  shtmp[tid]  +=  shtmp[tid  +  s];\n  }\n  item.barrier();\n  }\n  if  (tid  ==  0)  {\n  if  constexpr  (useHostReduction)  {\n  sum[ibl]  =  shtmp[0];  // each block saves its partial result to an array\n  }  else  {\n  // Alternatively, we could agregate everything together at index 0.\n  // Only useful when there not many partial sums left and when the device\n  // supports atomic operations on FP64/double operands.\n  sycl::atomic_ref<double,  sycl::memory_order::relaxed,\n  sycl::memory_scope::device,\n  sycl::access::address_space::global_space>\n  ref(sum[0]);\n  ref.fetch_add(shtmp[0]);\n  }\n  }\n  };\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  stream_size;\n  cudaEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  cudaMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  cudaMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  cudaMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  cudaMemcpyDeviceToHost,  stream[i]);\n\n  cudaEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  (N/stream_size);\n  hipEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  hipMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  hipMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  hipMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  hipMemcpyDeviceToHost,  stream[i]);\n\n  hipEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n}\n... \n```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  cudaGetDeviceCount(&count);\n  cudaGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  hipGetDeviceCount(&count);\n  hipGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main()  {\n  Kokkos::initialize();\n\n  int  count  =  Kokkos::Cuda().concurrency();\n  int  device  =\n  Kokkos::Cuda().impl_internal_space_instance()->impl_internal_space_id();\n\n  std::cout  <<  \"Hello! I'm GPU \"  <<  device  <<  \" out of \"  <<  count\n  <<  \" GPUs in total.\"  <<  std::endl;\n\n  Kokkos::finalize();\n\n  return  0;\n} \n```", "```\n#include  <CL/opencl.h>\n#include  <stdio.h>\nint  main(void)  {\n  cl_uint  count;\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  &count);\n\n  char  deviceName[1024];\n  clGetDeviceInfo(device,  CL_DEVICE_NAME,  sizeof(deviceName),  deviceName,  NULL);\n\n  printf(\"Hello! I'm GPU %s out of %d GPUs in total.\\n\",  deviceName,  count);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  auto  gpu_devices  =  sycl::device::get_devices(sycl::info::device_type::gpu);\n  auto  count  =  gpu_devices.size();\n  std::cout  <<  \"Hello! I'm using a SYCL device by \"\n  <<  gpu_devices[0].get_info<sycl::info::device::vendor>()\n  <<  \", the first of \"  <<  count  <<  \" devices.\"  <<  std::endl;\n  return  0;\n} \n```", "```\n> cudaSetDevice(deviceNumber);  // For CUDA\n> hipSetDevice(deviceNumber);  // For HIP \n> ```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n  int  i;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  cudaMalloc((void  **)&Ad,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Bd,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  cudaMemcpy(Ad,  Ah,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n  cudaMemcpy(Bd,  Bh,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  cudaMemcpy(Ch,  Cd,  sizeof(float)  *  N,  cudaMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ad);\n  cudaFree(Bd);\n  cudaFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  hipMalloc((void  **)&Ad,  N  *  sizeof(float));\n  hipMalloc((void  **)&Bd,  N  *  sizeof(float));\n  hipMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  hipMemcpy(Ad,  Ah,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n  hipMemcpy(Bd,  Bh,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ad, Bd, Cd, N); // or\n  vector_add<<<blocks,  threads,  0,  0>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  hipMemcpy(Ch,  Cd,  sizeof(float)  *  N,  hipMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  hipFree(Ad);\n  hipFree(Bd);\n  hipFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n// We're using C API here; examples with C++ API can be found in the \"Portable\n// kernel models\" chapter\n#include  <CL/cl.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n#define N 10000\n\nstatic  const  char  *programSource  =\n  \"__kernel void vector_add(__global const float* A, __global const float* \"\n  \"B, __global float* C, int N) {\\n\"\n  \"    int tid = get_global_id(0);\\n\"\n  \"    if (tid < N) {\\n\"\n  \"        C[tid] = A[tid] + B[tid];\\n\"\n  \"    }\\n\"\n  \"}\\n\";\n\nint  main()  {\n  // Initialize data and calculate reference values on CPU\n  float  Ah[N],  Bh[N],  Ch[N],  Cref[N];\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3f;\n  Bh[i]  =  cos(i)  *  1.1f;\n  Ch[i]  =  12.f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Use the default device\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Build the kernel from string\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &programSource,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"vector_add\",  NULL);\n\n  // Allocate the arrays on GPU\n  cl_mem  d_A  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_B  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_C  =\n  clCreateBuffer(context,  CL_MEM_WRITE_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n\n  clEnqueueWriteBuffer(queue,  d_A,  CL_TRUE,  0,  N  *  sizeof(float),  Ah,  0,  NULL,\n  NULL);\n  clEnqueueWriteBuffer(queue,  d_B,  CL_TRUE,  0,  N  *  sizeof(float),  Bh,  0,  NULL,\n  NULL);\n\n  // Set arguments and launch the kernel\n  clSetKernelArg(kernel,  0,  sizeof(cl_mem),  &d_A);\n  clSetKernelArg(kernel,  1,  sizeof(cl_mem),  &d_B);\n  clSetKernelArg(kernel,  2,  sizeof(cl_mem),  &d_C);\n  cl_int  N_as_cl_int  =  N;\n  clSetKernelArg(kernel,  3,  sizeof(cl_int),  &N_as_cl_int);\n  size_t  globalSize  =  N;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Copy the results back\n  clEnqueueReadBuffer(queue,  d_C,  CL_TRUE,  0,  N  *  sizeof(float),  Ch,  0,  NULL,\n  NULL);\n\n  // Print reference and result values\n  printf(\"Reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"Result   : %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  printf(\"Total error: %f\\n\",  error);\n  printf(\"Reference:   %f at (42)\\n\",  Cref[42]);\n  printf(\"Result   :   %f at (42)\\n\",  Ch[42]);\n\n  clReleaseMemObject(d_A);\n  clReleaseMemObject(d_B);\n  clReleaseMemObject(d_C);\n  clReleaseKernel(kernel);\n  clReleaseProgram(program);\n  clReleaseCommandQueue(queue);\n  clReleaseContext(context);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Ah(N);\n  std::vector<float>  Bh(N);\n  std::vector<float>  Ch(N);\n  std::vector<float>  Cref(N);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  std::sin(i)  *  2.3f;\n  Bh[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  float  *Ad  =  sycl::malloc_device<float>(N,  q);\n  float  *Bd  =  sycl::malloc_device<float>(N,  q);\n  float  *Cd  =  sycl::malloc_device<float>(N,  q);\n\n  q.copy<float>(Ah.data(),  Ad,  N);\n  q.copy<float>(Bh.data(),  Bd,  N);\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  Cd[tid]  =  Ad[tid]  +  Bd[tid];\n  });\n  });\n\n  // Copy results back to CPU\n  sycl::event  eventCCopy  =  q.copy<float>(Cd,  Ch.data(),  N);\n  // Wait for the copy to finish\n  eventCCopy.wait();\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  Ch[0]  <<  \" \"  <<  Ch[1]  <<  \" \"  <<  Ch[2]  <<  \" \"\n  <<  Ch[3]  <<  \" ... \"  <<  Ch[N  -  2]  <<  \" \"  <<  Ch[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  Ch[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the GPU memory\n  sycl::free(Ad,  q);\n  sycl::free(Bd,  q);\n  sycl::free(Cd,  q);\n\n  return  0;\n} \n```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  int  i;\n\n  // Allocate the arrays using Unified Memory\n  cudaMallocManaged(&Ah,  N  *  sizeof(float));\n  cudaMallocManaged(&Bh,  N  *  sizeof(float));\n  cudaMallocManaged(&Ch,  N  *  sizeof(float));\n  cudaMallocManaged(&Cref,  N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // define grid dimensions\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  cudaDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ah);\n  cudaFree(Bh);\n  cudaFree(Ch);\n  cudaFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  // Allocate the arrays using Unified Memory\n  hipMallocManaged((void  **)&Ah,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Bh,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Ch,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Cref,  N  *  sizeof(float));\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n  // All data at this point is on CPU\n\n  // Define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ah, Bh, Ch, N); // or\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  hipDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on the CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the Unified Memory arrays\n  hipFree(Ah);\n  hipFree(Bh);\n  hipFree(Ch);\n  hipFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Cref(N);\n\n  // Allocate the shared arrays\n  float  *A  =  sycl::malloc_shared<float>(N,  q);\n  float  *B  =  sycl::malloc_shared<float>(N,  q);\n  float  *C  =  sycl::malloc_shared<float>(N,  q);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  A[i]  =  std::sin(i)  *  2.3f;\n  B[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  A[i]  +  B[i];\n  }\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  C[tid]  =  A[tid]  +  B[tid];\n  });\n  }).wait();  // Wait for the kernel to finish\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  C[0]  <<  \" \"  <<  C[1]  <<  \" \"  <<  C[2]  <<  \" \"\n  <<  C[3]  <<  \" ... \"  <<  C[N  -  2]  <<  \" \"  <<  C[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  C[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  C[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the shared memory\n  sycl::free(A,  q);\n  sycl::free(B,  q);\n  sycl::free(C,  q);\n\n  return  0;\n} \n```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  cudaGetDeviceCount(&count);\n  cudaGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <stdio.h>\n\nint  main(void)  {\n  int  count,  device;\n\n  hipGetDeviceCount(&count);\n  hipGetDevice(&device);\n\n  printf(\"Hello! I'm GPU %d out of %d GPUs in total.\\n\",  device,  count);\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main()  {\n  Kokkos::initialize();\n\n  int  count  =  Kokkos::Cuda().concurrency();\n  int  device  =\n  Kokkos::Cuda().impl_internal_space_instance()->impl_internal_space_id();\n\n  std::cout  <<  \"Hello! I'm GPU \"  <<  device  <<  \" out of \"  <<  count\n  <<  \" GPUs in total.\"  <<  std::endl;\n\n  Kokkos::finalize();\n\n  return  0;\n} \n```", "```\n#include  <CL/opencl.h>\n#include  <stdio.h>\nint  main(void)  {\n  cl_uint  count;\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  &count);\n\n  char  deviceName[1024];\n  clGetDeviceInfo(device,  CL_DEVICE_NAME,  sizeof(deviceName),  deviceName,  NULL);\n\n  printf(\"Hello! I'm GPU %s out of %d GPUs in total.\\n\",  deviceName,  count);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  auto  gpu_devices  =  sycl::device::get_devices(sycl::info::device_type::gpu);\n  auto  count  =  gpu_devices.size();\n  std::cout  <<  \"Hello! I'm using a SYCL device by \"\n  <<  gpu_devices[0].get_info<sycl::info::device::vendor>()\n  <<  \", the first of \"  <<  count  <<  \" devices.\"  <<  std::endl;\n  return  0;\n} \n```", "```\n> cudaSetDevice(deviceNumber);  // For CUDA\n> hipSetDevice(deviceNumber);  // For HIP \n> ```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n  int  i;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  cudaMalloc((void  **)&Ad,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Bd,  N  *  sizeof(float));\n  cudaMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  cudaMemcpy(Ad,  Ah,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n  cudaMemcpy(Bd,  Bh,  sizeof(float)  *  N,  cudaMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  cudaMemcpy(Ch,  Cd,  sizeof(float)  *  N,  cudaMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ad);\n  cudaFree(Bd);\n  cudaFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  float  *Ad,  *Bd,  *Cd;\n\n  // Allocate the arrays on CPU\n  Ah  =  (float  *)malloc(N  *  sizeof(float));\n  Bh  =  (float  *)malloc(N  *  sizeof(float));\n  Ch  =  (float  *)malloc(N  *  sizeof(float));\n  Cref  =  (float  *)malloc(N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  hipMalloc((void  **)&Ad,  N  *  sizeof(float));\n  hipMalloc((void  **)&Bd,  N  *  sizeof(float));\n  hipMalloc((void  **)&Cd,  N  *  sizeof(float));\n\n  // Transfer the data from CPU to GPU\n  hipMemcpy(Ad,  Ah,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n  hipMemcpy(Bd,  Bh,  sizeof(float)  *  N,  hipMemcpyHostToDevice);\n\n  // define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ad, Bd, Cd, N); // or\n  vector_add<<<blocks,  threads,  0,  0>>>(Ad,  Bd,  Cd,  N);\n\n  // copy results back to CPU\n  hipMemcpy(Ch,  Cd,  sizeof(float)  *  N,  hipMemcpyDeviceToHost);\n\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  hipFree(Ad);\n  hipFree(Bd);\n  hipFree(Cd);\n\n  // Free the CPU arrays\n  free(Ah);\n  free(Bh);\n  free(Ch);\n  free(Cref);\n\n  return  0;\n} \n```", "```\n// We're using C API here; examples with C++ API can be found in the \"Portable\n// kernel models\" chapter\n#include  <CL/cl.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <stdlib.h>\n\n#define N 10000\n\nstatic  const  char  *programSource  =\n  \"__kernel void vector_add(__global const float* A, __global const float* \"\n  \"B, __global float* C, int N) {\\n\"\n  \"    int tid = get_global_id(0);\\n\"\n  \"    if (tid < N) {\\n\"\n  \"        C[tid] = A[tid] + B[tid];\\n\"\n  \"    }\\n\"\n  \"}\\n\";\n\nint  main()  {\n  // Initialize data and calculate reference values on CPU\n  float  Ah[N],  Bh[N],  Ch[N],  Cref[N];\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3f;\n  Bh[i]  =  cos(i)  *  1.1f;\n  Ch[i]  =  12.f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Use the default device\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Build the kernel from string\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &programSource,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"vector_add\",  NULL);\n\n  // Allocate the arrays on GPU\n  cl_mem  d_A  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_B  =\n  clCreateBuffer(context,  CL_MEM_READ_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n  cl_mem  d_C  =\n  clCreateBuffer(context,  CL_MEM_WRITE_ONLY,  N  *  sizeof(float),  NULL,  NULL);\n\n  clEnqueueWriteBuffer(queue,  d_A,  CL_TRUE,  0,  N  *  sizeof(float),  Ah,  0,  NULL,\n  NULL);\n  clEnqueueWriteBuffer(queue,  d_B,  CL_TRUE,  0,  N  *  sizeof(float),  Bh,  0,  NULL,\n  NULL);\n\n  // Set arguments and launch the kernel\n  clSetKernelArg(kernel,  0,  sizeof(cl_mem),  &d_A);\n  clSetKernelArg(kernel,  1,  sizeof(cl_mem),  &d_B);\n  clSetKernelArg(kernel,  2,  sizeof(cl_mem),  &d_C);\n  cl_int  N_as_cl_int  =  N;\n  clSetKernelArg(kernel,  3,  sizeof(cl_int),  &N_as_cl_int);\n  size_t  globalSize  =  N;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Copy the results back\n  clEnqueueReadBuffer(queue,  d_C,  CL_TRUE,  0,  N  *  sizeof(float),  Ch,  0,  NULL,\n  NULL);\n\n  // Print reference and result values\n  printf(\"Reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"Result   : %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  printf(\"Total error: %f\\n\",  error);\n  printf(\"Reference:   %f at (42)\\n\",  Cref[42]);\n  printf(\"Result   :   %f at (42)\\n\",  Ch[42]);\n\n  clReleaseMemObject(d_A);\n  clReleaseMemObject(d_B);\n  clReleaseMemObject(d_C);\n  clReleaseKernel(kernel);\n  clReleaseProgram(program);\n  clReleaseCommandQueue(queue);\n  clReleaseContext(context);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Ah(N);\n  std::vector<float>  Bh(N);\n  std::vector<float>  Ch(N);\n  std::vector<float>  Cref(N);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  std::sin(i)  *  2.3f;\n  Bh[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // Allocate the arrays on GPU\n  float  *Ad  =  sycl::malloc_device<float>(N,  q);\n  float  *Bd  =  sycl::malloc_device<float>(N,  q);\n  float  *Cd  =  sycl::malloc_device<float>(N,  q);\n\n  q.copy<float>(Ah.data(),  Ad,  N);\n  q.copy<float>(Bh.data(),  Bd,  N);\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  Cd[tid]  =  Ad[tid]  +  Bd[tid];\n  });\n  });\n\n  // Copy results back to CPU\n  sycl::event  eventCCopy  =  q.copy<float>(Cd,  Ch.data(),  N);\n  // Wait for the copy to finish\n  eventCCopy.wait();\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  Ch[0]  <<  \" \"  <<  Ch[1]  <<  \" \"  <<  Ch[2]  <<  \" \"\n  <<  Ch[3]  <<  \" ... \"  <<  Ch[N  -  2]  <<  \" \"  <<  Ch[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  Ch[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the GPU memory\n  sycl::free(Ad,  q);\n  sycl::free(Bd,  q);\n  sycl::free(Cd,  q);\n\n  return  0;\n} \n```", "```\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  int  i;\n\n  // Allocate the arrays using Unified Memory\n  cudaMallocManaged(&Ah,  N  *  sizeof(float));\n  cudaMallocManaged(&Bh,  N  *  sizeof(float));\n  cudaMallocManaged(&Ch,  N  *  sizeof(float));\n  cudaMallocManaged(&Cref,  N  *  sizeof(float));\n\n  // initialise data and calculate reference values on CPU\n  for  (i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n\n  // define grid dimensions\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  cudaDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the GPU arrays\n  cudaFree(Ah);\n  cudaFree(Bh);\n  cudaFree(Ch);\n  cudaFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n\n__global__  void  vector_add(float  *A,  float  *B,  float  *C,  int  n)  {\n  int  tid  =  threadIdx.x  +  blockIdx.x  *  blockDim.x;\n  if  (tid  <  n)  {\n  C[tid]  =  A[tid]  +  B[tid];\n  }\n}\n\nint  main(void)  {\n  const  int  N  =  10000;\n  float  *Ah,  *Bh,  *Ch,  *Cref;\n  // Allocate the arrays using Unified Memory\n  hipMallocManaged((void  **)&Ah,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Bh,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Ch,  N  *  sizeof(float));\n  hipMallocManaged((void  **)&Cref,  N  *  sizeof(float));\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  Ah[i]  =  sin(i)  *  2.3;\n  Bh[i]  =  cos(i)  *  1.1;\n  Cref[i]  =  Ah[i]  +  Bh[i];\n  }\n  // All data at this point is on CPU\n\n  // Define grid dimensions + launch the device kernel\n  dim3  blocks,  threads;\n  threads  =  dim3(256,  1,  1);\n  blocks  =  dim3((N  +  256  -  1)  /  256,  1,  1);\n\n  // Launch Kernel\n  //  use\n  // hipLaunchKernelGGL(vector_add, blocks, threads, 0, 0, Ah, Bh, Ch, N); // or\n  vector_add<<<blocks,  threads>>>(Ah,  Bh,  Ch,  N);\n  hipDeviceSynchronize();  // Wait for the kernel to complete\n\n  // At this point we want to access the data on the CPU\n  printf(\"reference: %f %f %f %f ... %f %f\\n\",  Cref[0],  Cref[1],  Cref[2],\n  Cref[3],  Cref[N  -  2],  Cref[N  -  1]);\n  printf(\"   result: %f %f %f %f ... %f %f\\n\",  Ch[0],  Ch[1],  Ch[2],  Ch[3],\n  Ch[N  -  2],  Ch[N  -  1]);\n\n  // Confirm that results are correct\n  float  error  =  0.0;\n  float  tolerance  =  1e-6;\n  float  diff;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  diff  =  fabs(Cref[i]  -  Ch[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n  printf(\"total error: %f\\n\",  error);\n  printf(\"  reference: %f at (42)\\n\",  Cref[42]);\n  printf(\"     result: %f at (42)\\n\",  Ch[42]);\n\n  // Free the Unified Memory arrays\n  hipFree(Ah);\n  hipFree(Bh);\n  hipFree(Ch);\n  hipFree(Cref);\n\n  return  0;\n} \n```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  const  int  N  =  10000;\n  // The queue will be executed on the best device in the system\n  // We use in-order queue for simplicity\n  sycl::queue  q{{sycl::property::queue::in_order()}};\n\n  std::vector<float>  Cref(N);\n\n  // Allocate the shared arrays\n  float  *A  =  sycl::malloc_shared<float>(N,  q);\n  float  *B  =  sycl::malloc_shared<float>(N,  q);\n  float  *C  =  sycl::malloc_shared<float>(N,  q);\n\n  // Initialize data and calculate reference values on CPU\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  A[i]  =  std::sin(i)  *  2.3f;\n  B[i]  =  std::cos(i)  *  1.1f;\n  Cref[i]  =  A[i]  +  B[i];\n  }\n\n  // Define grid dimensions\n  // We can specify the block size explicitly, but we don't have to\n  sycl::range<1>  global_size(N);\n  q.submit([&](sycl::handler  &h)  {\n  h.parallel_for<class  VectorAdd>(global_size,  [=](sycl::id<1>  threadId)  {\n  int  tid  =  threadId.get(0);\n  C[tid]  =  A[tid]  +  B[tid];\n  });\n  }).wait();  // Wait for the kernel to finish\n\n  // Print reference and result values\n  std::cout  <<  \"Reference: \"  <<  Cref[0]  <<  \" \"  <<  Cref[1]  <<  \" \"  <<  Cref[2]\n  <<  \" \"  <<  Cref[3]  <<  \" ... \"  <<  Cref[N  -  2]  <<  \" \"  <<  Cref[N  -  1]\n  <<  std::endl;\n  std::cout  <<  \"Result   : \"  <<  C[0]  <<  \" \"  <<  C[1]  <<  \" \"  <<  C[2]  <<  \" \"\n  <<  C[3]  <<  \" ... \"  <<  C[N  -  2]  <<  \" \"  <<  C[N  -  1]  <<  std::endl;\n\n  // Compare results and calculate the total error\n  float  error  =  0.0f;\n  float  tolerance  =  1e-6f;\n  for  (int  i  =  0;  i  <  N;  i++)  {\n  float  diff  =  std::abs(Cref[i]  -  C[i]);\n  if  (diff  >  tolerance)  {\n  error  +=  diff;\n  }\n  }\n\n  std::cout  <<  \"Total error: \"  <<  error  <<  std::endl;\n  std::cout  <<  \"Reference:   \"  <<  Cref[42]  <<  \" at (42)\"  <<  std::endl;\n  std::cout  <<  \"Result   :   \"  <<  C[42]  <<  \" at (42)\"  <<  std::endl;\n\n  // Free the shared memory\n  sycl::free(A,  q);\n  sycl::free(B,  q);\n  sycl::free(C,  q);\n\n  return  0;\n} \n```", "```\n#include  <cstdlib>\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  cudaMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  cudaMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  cudaMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  cudaMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  cudaEvent_t  start_kernel_event;\n  cudaEventCreate(&start_kernel_event);\n  cudaEvent_t  end_kernel_event;\n  cudaEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(end_kernel_event,  0);\n  cudaEventSynchronize(end_kernel_event);\n\n  cudaDeviceSynchronize();\n  float  time_kernel;\n  cudaEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  cudaMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  cudaMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n\n#include  <cstdlib>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  hipMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  hipMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  hipMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  hipMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  hipEvent_t  start_kernel_event;\n  hipEventCreate(&start_kernel_event);\n  hipEvent_t  end_kernel_event;\n  hipEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(end_kernel_event,  0);\n  hipEventSynchronize(end_kernel_event);\n\n  hipDeviceSynchronize();\n  float  time_kernel;\n  hipEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  hipMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  hipMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n// Instead of defining kernel lambda at the place of submission,\n// we can define it here:\nauto  copyKernel(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  index  =  y_index  *  width  +  x_index;\n  out[index]  =  in[index];\n  };\n}\n\nint  main()  {\n  std::vector<float>  matrix_in(width  *  height);\n  std::vector<float>  matrix_out(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  // Create queue on the default device with profiling enabled\n  sycl::queue  queue{{sycl::property::queue::in_order(),\n  sycl::property::queue::enable_profiling()}};\n\n  float  *d_in  =  sycl::malloc_device<float>(width  *  height,  queue);\n  float  *d_out  =  sycl::malloc_device<float>(width  *  height,  queue);\n\n  queue.copy<float>(matrix_in.data(),  d_in,  width  *  height);\n  queue.wait();\n\n  printf(\"Setup complete. Launching kernel\\n\");\n  sycl::range<2>  global_size{height,  width},  local_size{tile_dim,  tile_dim};\n  sycl::nd_range<2>  kernel_range{global_size,  local_size};\n\n  // Create events\n  printf(\"Warm up the GPU!\\n\");\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  }\n\n  // Unlike in CUDA or HIP, for SYCL we have to store all events\n  std::vector<sycl::event>  kernel_events;\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  sycl::event  kernel_event  =  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  kernel_events.push_back(kernel_event);\n  }\n\n  queue.wait();\n\n  auto  first_kernel_started  =\n  kernel_events.front()\n  .get_profiling_info<sycl::info::event_profiling::command_start>();\n  auto  last_kernel_ended  =\n  kernel_events.back()\n  .get_profiling_info<sycl::info::event_profiling::command_end>();\n  double  total_kernel_time_ns  =\n  static_cast<double>(last_kernel_ended  -  first_kernel_started);\n  double  time_kernels  =  total_kernel_time_ns  /  1e6;  // convert ns to ms\n  double  bandwidth  =  2.0  *  10000  *\n  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernels  *  1024  *  1024  *  1024);\n\n  printf(\"Kernel execution complete\\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6lf ms - copy\\n Bandwidth %.6lf GB/s\\n\",  time_kernels  /  10,\n  bandwidth);\n\n  sycl::free(d_in,  queue);\n  sycl::free(d_out,  queue);\n  return  0;\n} \n```", "```\n__global__  void  transpose_naive_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n\n  out[out_index]  =  in[in_index];\n} \n```", "```\nauto  transposeKernelNaive(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n  out[out_index]  =  in[in_index];\n  };\n} \n```", "```\n> __global__  void  transpose_SM_kernel(float  *in,  float  *out,  int  width,\n>   int  height)  {\n>   __shared__  float  tile[tile_dim][tile_dim];\n> \n>   int  x_tile_index  =  blockIdx.x  *  tile_dim;\n>   int  y_tile_index  =  blockIdx.y  *  tile_dim;\n> \n>   int  in_index  =\n>   (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n>   int  out_index  =\n>   (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n> \n>   tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n> \n>   __syncthreads();\n> \n>   out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n> } \n> ```", "```\n> auto  transposeKernelSM(sycl::handler  &cgh,  const  float  *in,  float  *out,\n>   int  width,  int  height)  {\n>   sycl::local_accessor<float,  1>  tile{{tile_dim  *  tile_dim},  cgh};\n>   return  [=](sycl::nd_item<2>  item)  {\n>   int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n>   int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n>   int  x_local_index  =  item.get_local_id(1);\n>   int  y_local_index  =  item.get_local_id(0);\n>   int  in_index  =\n>   (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n>   int  out_index  =\n>   (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n> \n>   tile[y_local_index  *  tile_dim  +  x_local_index]  =  in[in_index];\n>   item.barrier();\n>   out[out_index]  =  tile[x_local_index  *  tile_dim  +  y_local_index];\n>   };\n> } \n> ```", "```\n__global__  void  transpose_SM_nobc_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  __shared__  float  tile[tile_dim][tile_dim  +  1];\n\n  int  x_tile_index  =  blockIdx.x  *  tile_dim;\n  int  y_tile_index  =  blockIdx.y  *  tile_dim;\n\n  int  in_index  =\n  (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n  int  out_index  =\n  (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n\n  tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n\n  __syncthreads();\n\n  out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n} \n```", "```\nauto  transposeKernelSMNoBC(sycl::handler  &cgh,  const  float  *in,  float  *out,\n  int  width,  int  height)  {\n  sycl::local_accessor<float,  1>  tile{{tile_dim  *  (tile_dim  +  1)},  cgh};\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n  int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n  int  x_local_index  =  item.get_local_id(1);\n  int  y_local_index  =  item.get_local_id(0);\n  int  in_index  =\n  (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n  int  out_index  =\n  (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n\n  tile[y_local_index  *  (tile_dim  +  1)  +  x_local_index]  =  in[in_index];\n  item.barrier();\n  out[out_index]  =  tile[x_local_index  *  (tile_dim  +  1)  +  y_local_index];\n  };\n} \n```", "```\n#define tpb 512 // size in this case has to be known at compile time\n// this kernel has to be launched with at least N/2 threads\n__global__  void  reduction_one(double  x,  double  *sum,  int  N){\n  int  ibl=blockIdx.y+blockIdx.x*gridDim.y;\n  int  ind=threadIdx.x+blockDim.x*ibl;\n\n  __shared__  double  shtmp[2*tpb];\n  shtmp[threadIdx.x]=0;  // for sums we initiate with 0, for other operations should be different\n  if(ind<N/2)\n  {\n  shtmp[threadIdx.x]=x[ind];\n  }\n  if(ind+N/2<N)\n  {\n  shtmp[threadIdx.x+tpb]=x[ind+N/2];\n  }\n  __syncthreads();\n  for(int  s=tpb;s>0;s>>=1){\n  if(threadIdx.x<s){\n  shtmp[threadIdx.x]+=shtmp[threadIdx.x+s];}\n  __syncthreads();\n  }\n  if(threadIdx.x==0)\n  {\n  sum[ibl]=shtmp[0];  // each block saves its partial result to an array\n  // atomicAdd(&sum[0], shene[0]); // alternatively could aggregate everything together at index 0\\. Only use when there not many partial sums left\n  }\n} \n```", "```\n// SYCL has built-in sycl::reduction primitive, the use of which is demonstrated\n// in the \"Portable kernel models\" chapter. Here is how the reduction can be\n// implemented manually:\nauto  redutionKernel(sycl::handler  &cgh,  double  *x,  double  *sum,  int  N)  {\n  sycl::local_accessor<double,  1>  shtmp{{2  *  tpb},  cgh};\n  return  [=](sycl::nd_item<1>  item)  {\n  int  ibl  =  item.get_group(0);\n  int  ind  =  item.get_global_id(0);\n  int  tid  =  item.get_local_id(0);\n  shtmp[item.get_local_id(0)]  =  0;\n  if  (ind  <  N  /  2)  {\n  shtmp[tid]  =  x[ind];\n  }  else  {\n  shtmp[tid]  =  0.0;\n  }\n  if  (ind  +  N  /  2  <  N)  {\n  shtmp[tid  +  tpb]  =  x[ind  +  N  /  2];\n  }  else  {\n  shtmp[tid  +  tpb]  =  0.0;\n  }\n\n  for  (int  s  =  tpb;  s  >  0;  s  >>=  1)  {\n  if  (tid  <  s)  {\n  shtmp[tid]  +=  shtmp[tid  +  s];\n  }\n  item.barrier();\n  }\n  if  (tid  ==  0)  {\n  if  constexpr  (useHostReduction)  {\n  sum[ibl]  =  shtmp[0];  // each block saves its partial result to an array\n  }  else  {\n  // Alternatively, we could agregate everything together at index 0.\n  // Only useful when there not many partial sums left and when the device\n  // supports atomic operations on FP64/double operands.\n  sycl::atomic_ref<double,  sycl::memory_order::relaxed,\n  sycl::memory_scope::device,\n  sycl::access::address_space::global_space>\n  ref(sum[0]);\n  ref.fetch_add(shtmp[0]);\n  }\n  }\n  };\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  stream_size;\n  cudaEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  cudaMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  cudaMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  cudaMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  cudaMemcpyDeviceToHost,  stream[i]);\n\n  cudaEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  (N/stream_size);\n  hipEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  hipMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  hipMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  hipMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  hipMemcpyDeviceToHost,  stream[i]);\n\n  hipEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n}\n... \n```", "```\n#include  <cstdlib>\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n#include  <math.h>\n#include  <stdio.h>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  cudaMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  cudaMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  cudaMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  cudaMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  cudaEvent_t  start_kernel_event;\n  cudaEventCreate(&start_kernel_event);\n  cudaEvent_t  end_kernel_event;\n  cudaEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  cudaEventRecord(end_kernel_event,  0);\n  cudaEventSynchronize(end_kernel_event);\n\n  cudaDeviceSynchronize();\n  float  time_kernel;\n  cudaEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  cudaMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  cudaMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <hip/hip_runtime.h>\n\n#include  <cstdlib>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n__global__  void  copy_kernel(float  *in,  float  *out,  int  width,  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  index  =  y_index  *  width  +  x_index;\n\n  out[index]  =  in[index];\n}\n\nint  main()  {\n  std::vector<float>  matrix_in;\n  std::vector<float>  matrix_out;\n\n  matrix_in.resize(width  *  height);\n  matrix_out.resize(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  float  *d_in,  *d_out;\n\n  hipMalloc((void  **)&d_in,  width  *  height  *  sizeof(float));\n  hipMalloc((void  **)&d_out,  width  *  height  *  sizeof(float));\n\n  hipMemcpy(d_in,  matrix_in.data(),  width  *  height  *  sizeof(float),\n  hipMemcpyHostToDevice);\n\n  printf(\"Setup complete. Launching kernel \\n\");\n  int  block_x  =  width  /  tile_dim;\n  int  block_y  =  height  /  tile_dim;\n\n  // Create events\n  hipEvent_t  start_kernel_event;\n  hipEventCreate(&start_kernel_event);\n  hipEvent_t  end_kernel_event;\n  hipEventCreate(&end_kernel_event);\n\n  printf(\"Warm up the gpu!\\n\");\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(start_kernel_event,  0);\n\n  for  (int  i  =  1;  i  <=  10;  i++)  {\n  copy_kernel<<<dim3(block_x,  block_y),  dim3(tile_dim,  tile_dim)>>>(\n  d_in,  d_out,  width,  height);\n  }\n\n  hipEventRecord(end_kernel_event,  0);\n  hipEventSynchronize(end_kernel_event);\n\n  hipDeviceSynchronize();\n  float  time_kernel;\n  hipEventElapsedTime(&time_kernel,  start_kernel_event,  end_kernel_event);\n\n  printf(\"Kernel execution complete \\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6f ms - copy \\n Bandwidth %.6f GB/s\\n\",  time_kernel  /  10,\n  2.0  *  10000  *  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernel  *  1024  *  1024  *  1024));\n\n  hipMemcpy(matrix_out.data(),  d_out,  width  *  height  *  sizeof(float),\n  hipMemcpyDeviceToHost);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nconst  static  int  width  =  4096;\nconst  static  int  height  =  4096;\nconst  static  int  tile_dim  =  16;\n\n// Instead of defining kernel lambda at the place of submission,\n// we can define it here:\nauto  copyKernel(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  index  =  y_index  *  width  +  x_index;\n  out[index]  =  in[index];\n  };\n}\n\nint  main()  {\n  std::vector<float>  matrix_in(width  *  height);\n  std::vector<float>  matrix_out(width  *  height);\n\n  for  (int  i  =  0;  i  <  width  *  height;  i++)  {\n  matrix_in[i]  =  (float)rand()  /  (float)RAND_MAX;\n  }\n\n  // Create queue on the default device with profiling enabled\n  sycl::queue  queue{{sycl::property::queue::in_order(),\n  sycl::property::queue::enable_profiling()}};\n\n  float  *d_in  =  sycl::malloc_device<float>(width  *  height,  queue);\n  float  *d_out  =  sycl::malloc_device<float>(width  *  height,  queue);\n\n  queue.copy<float>(matrix_in.data(),  d_in,  width  *  height);\n  queue.wait();\n\n  printf(\"Setup complete. Launching kernel\\n\");\n  sycl::range<2>  global_size{height,  width},  local_size{tile_dim,  tile_dim};\n  sycl::nd_range<2>  kernel_range{global_size,  local_size};\n\n  // Create events\n  printf(\"Warm up the GPU!\\n\");\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  }\n\n  // Unlike in CUDA or HIP, for SYCL we have to store all events\n  std::vector<sycl::event>  kernel_events;\n  for  (int  i  =  0;  i  <  10;  i++)  {\n  sycl::event  kernel_event  =  queue.submit([&](sycl::handler  &cgh)  {\n  cgh.parallel_for(kernel_range,  copyKernel(d_in,  d_out,  width,  height));\n  });\n  kernel_events.push_back(kernel_event);\n  }\n\n  queue.wait();\n\n  auto  first_kernel_started  =\n  kernel_events.front()\n  .get_profiling_info<sycl::info::event_profiling::command_start>();\n  auto  last_kernel_ended  =\n  kernel_events.back()\n  .get_profiling_info<sycl::info::event_profiling::command_end>();\n  double  total_kernel_time_ns  =\n  static_cast<double>(last_kernel_ended  -  first_kernel_started);\n  double  time_kernels  =  total_kernel_time_ns  /  1e6;  // convert ns to ms\n  double  bandwidth  =  2.0  *  10000  *\n  (((double)(width)  *  (double)height)  *  sizeof(float))  /\n  (time_kernels  *  1024  *  1024  *  1024);\n\n  printf(\"Kernel execution complete\\n\");\n  printf(\"Event timings:\\n\");\n  printf(\"  %.6lf ms - copy\\n Bandwidth %.6lf GB/s\\n\",  time_kernels  /  10,\n  bandwidth);\n\n  sycl::free(d_in,  queue);\n  sycl::free(d_out,  queue);\n  return  0;\n} \n```", "```\n__global__  void  transpose_naive_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  int  x_index  =  blockIdx.x  *  tile_dim  +  threadIdx.x;\n  int  y_index  =  blockIdx.y  *  tile_dim  +  threadIdx.y;\n\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n\n  out[out_index]  =  in[in_index];\n} \n```", "```\nauto  transposeKernelNaive(const  float  *in,  float  *out,  int  width,  int  height)  {\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_index  =  item.get_global_id(1);\n  int  y_index  =  item.get_global_id(0);\n  int  in_index  =  y_index  *  width  +  x_index;\n  int  out_index  =  x_index  *  height  +  y_index;\n  out[out_index]  =  in[in_index];\n  };\n} \n```", "```\n> __global__  void  transpose_SM_kernel(float  *in,  float  *out,  int  width,\n>   int  height)  {\n>   __shared__  float  tile[tile_dim][tile_dim];\n> \n>   int  x_tile_index  =  blockIdx.x  *  tile_dim;\n>   int  y_tile_index  =  blockIdx.y  *  tile_dim;\n> \n>   int  in_index  =\n>   (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n>   int  out_index  =\n>   (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n> \n>   tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n> \n>   __syncthreads();\n> \n>   out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n> } \n> ```", "```\n> auto  transposeKernelSM(sycl::handler  &cgh,  const  float  *in,  float  *out,\n>   int  width,  int  height)  {\n>   sycl::local_accessor<float,  1>  tile{{tile_dim  *  tile_dim},  cgh};\n>   return  [=](sycl::nd_item<2>  item)  {\n>   int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n>   int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n>   int  x_local_index  =  item.get_local_id(1);\n>   int  y_local_index  =  item.get_local_id(0);\n>   int  in_index  =\n>   (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n>   int  out_index  =\n>   (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n> \n>   tile[y_local_index  *  tile_dim  +  x_local_index]  =  in[in_index];\n>   item.barrier();\n>   out[out_index]  =  tile[x_local_index  *  tile_dim  +  y_local_index];\n>   };\n> } \n> ```", "```\n__global__  void  transpose_SM_nobc_kernel(float  *in,  float  *out,  int  width,\n  int  height)  {\n  __shared__  float  tile[tile_dim][tile_dim  +  1];\n\n  int  x_tile_index  =  blockIdx.x  *  tile_dim;\n  int  y_tile_index  =  blockIdx.y  *  tile_dim;\n\n  int  in_index  =\n  (y_tile_index  +  threadIdx.y)  *  width  +  (x_tile_index  +  threadIdx.x);\n  int  out_index  =\n  (x_tile_index  +  threadIdx.y)  *  height  +  (y_tile_index  +  threadIdx.x);\n\n  tile[threadIdx.y][threadIdx.x]  =  in[in_index];\n\n  __syncthreads();\n\n  out[out_index]  =  tile[threadIdx.x][threadIdx.y];\n} \n```", "```\nauto  transposeKernelSMNoBC(sycl::handler  &cgh,  const  float  *in,  float  *out,\n  int  width,  int  height)  {\n  sycl::local_accessor<float,  1>  tile{{tile_dim  *  (tile_dim  +  1)},  cgh};\n  return  [=](sycl::nd_item<2>  item)  {\n  int  x_tile_index  =  item.get_group(1)  *  tile_dim;\n  int  y_tile_index  =  item.get_group(0)  *  tile_dim;\n  int  x_local_index  =  item.get_local_id(1);\n  int  y_local_index  =  item.get_local_id(0);\n  int  in_index  =\n  (y_tile_index  +  y_local_index)  *  width  +  (x_tile_index  +  x_local_index);\n  int  out_index  =\n  (x_tile_index  +  y_local_index)  *  width  +  (y_tile_index  +  x_local_index);\n\n  tile[y_local_index  *  (tile_dim  +  1)  +  x_local_index]  =  in[in_index];\n  item.barrier();\n  out[out_index]  =  tile[x_local_index  *  (tile_dim  +  1)  +  y_local_index];\n  };\n} \n```", "```\n#define tpb 512 // size in this case has to be known at compile time\n// this kernel has to be launched with at least N/2 threads\n__global__  void  reduction_one(double  x,  double  *sum,  int  N){\n  int  ibl=blockIdx.y+blockIdx.x*gridDim.y;\n  int  ind=threadIdx.x+blockDim.x*ibl;\n\n  __shared__  double  shtmp[2*tpb];\n  shtmp[threadIdx.x]=0;  // for sums we initiate with 0, for other operations should be different\n  if(ind<N/2)\n  {\n  shtmp[threadIdx.x]=x[ind];\n  }\n  if(ind+N/2<N)\n  {\n  shtmp[threadIdx.x+tpb]=x[ind+N/2];\n  }\n  __syncthreads();\n  for(int  s=tpb;s>0;s>>=1){\n  if(threadIdx.x<s){\n  shtmp[threadIdx.x]+=shtmp[threadIdx.x+s];}\n  __syncthreads();\n  }\n  if(threadIdx.x==0)\n  {\n  sum[ibl]=shtmp[0];  // each block saves its partial result to an array\n  // atomicAdd(&sum[0], shene[0]); // alternatively could aggregate everything together at index 0\\. Only use when there not many partial sums left\n  }\n} \n```", "```\n// SYCL has built-in sycl::reduction primitive, the use of which is demonstrated\n// in the \"Portable kernel models\" chapter. Here is how the reduction can be\n// implemented manually:\nauto  redutionKernel(sycl::handler  &cgh,  double  *x,  double  *sum,  int  N)  {\n  sycl::local_accessor<double,  1>  shtmp{{2  *  tpb},  cgh};\n  return  [=](sycl::nd_item<1>  item)  {\n  int  ibl  =  item.get_group(0);\n  int  ind  =  item.get_global_id(0);\n  int  tid  =  item.get_local_id(0);\n  shtmp[item.get_local_id(0)]  =  0;\n  if  (ind  <  N  /  2)  {\n  shtmp[tid]  =  x[ind];\n  }  else  {\n  shtmp[tid]  =  0.0;\n  }\n  if  (ind  +  N  /  2  <  N)  {\n  shtmp[tid  +  tpb]  =  x[ind  +  N  /  2];\n  }  else  {\n  shtmp[tid  +  tpb]  =  0.0;\n  }\n\n  for  (int  s  =  tpb;  s  >  0;  s  >>=  1)  {\n  if  (tid  <  s)  {\n  shtmp[tid]  +=  shtmp[tid  +  s];\n  }\n  item.barrier();\n  }\n  if  (tid  ==  0)  {\n  if  constexpr  (useHostReduction)  {\n  sum[ibl]  =  shtmp[0];  // each block saves its partial result to an array\n  }  else  {\n  // Alternatively, we could agregate everything together at index 0.\n  // Only useful when there not many partial sums left and when the device\n  // supports atomic operations on FP64/double operands.\n  sycl::atomic_ref<double,  sycl::memory_order::relaxed,\n  sycl::memory_scope::device,\n  sycl::access::address_space::global_space>\n  ref(sum[0]);\n  ref.fetch_add(shtmp[0]);\n  }\n  }\n  };\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  stream_size;\n  cudaEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  cudaMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  cudaMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  cudaMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  cudaMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  cudaMemcpyDeviceToHost,  stream[i]);\n\n  cudaEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n} \n```", "```\n// Distribute kernel for 'n_streams' streams, and record each stream's timing\nfor  (int  i  =  0;  i  <  n_streams;  ++i)  {\n  int  offset  =  i  *  (N/stream_size);\n  hipEventRecord(start_event[i],  stream[i]);  // stamp the moment when the kernel is submitted on stream i\n\n  hipMemcpyAsync(  &Ad[offset],  &Ah[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  hipMemcpyAsync(  &Bd[offset],  &Bh[offset],  N/n_streams*sizeof(float),  hipMemcpyHostToDevice,  stream[i]);\n  vector_add<<<gridsize  /  n_streams,  blocksize,  0,  stream[i]>>>(&Ad[offset],  &Bd[offset],  &Cd[offset],  N/n_streams);  //each call processes N/n_streams elements\n  hipMemcpyAsync(  &Ch[offset],  &Cd[offset],  N/n_streams*sizeof(float),  hipMemcpyDeviceToHost,  stream[i]);\n\n  hipEventRecord(stop_event[i],  stream[i]);  // stamp the moment when the kernel on stream i finished\n}\n... \n```"]