<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>4.5. Application: principal components analysis#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>4.5. Application: principal components analysis#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html">https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html</a></blockquote>

<p>We discuss an application to principal components analysis and revisit our genetic dataset from ealier in the chapter.</p>
<section id="dimensionality-reduction-via-principal-components-analysis-pca">
<h2><span class="section-number">4.5.1. </span>Dimensionality reduction via principal components analysis (PCA)<a class="headerlink" href="#dimensionality-reduction-via-principal-components-analysis-pca" title="Link to this heading">#</a></h2>
<p>Principal components analysis (PCA)<span class="math notranslate nohighlight">\(\idx{principal components analysis}\xdi\)</span> is a commonly used dimensionality reduction approach that is closely related to what we described in the previous sections. We formalize the connection.</p>
<p><em>The data matrix:</em> In PCA we are given <span class="math notranslate nohighlight">\(n\)</span> data points <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_n \in \mathbb{R}^p\)</span> with <span class="math notranslate nohighlight">\(p\)</span> features (i.e., coordinates). We denote the components of <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> as <span class="math notranslate nohighlight">\((x_{i1},\ldots,x_{ip})\)</span>. As usual, we stack them up into a matrix <span class="math notranslate nohighlight">\(X\)</span> whose <span class="math notranslate nohighlight">\(i\)</span>-th row is <span class="math notranslate nohighlight">\(\mathbf{x}_i^T\)</span>.</p>
<p>The first step of PCA is to center the data, i.e., we assume that<span class="math notranslate nohighlight">\(\idx{mean centering}\xdi\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p
\]</div>
<p>Put differently, the empirical mean of each column is <span class="math notranslate nohighlight">\(0\)</span>. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations">Wikipedia</a> (and this will become clearer below):</p>
<blockquote>
<div><p>Mean subtraction (a.k.a. “mean centering”) is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.</p>
</div></blockquote>
<p>An optional step is to divide each column by the square root of its <a class="reference external" href="https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance">sample variance</a>, i.e., assume that</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p.
\]</div>
<p>As we mentioned in a previous chapter, this is particularly important when the features are measured in different units to ensure that their variability can be meaningfully compared.</p>
<p><em>The first principal component:</em> The first principal component is the linear combination of the features</p>
<div class="math notranslate nohighlight">
\[
t_{i1}
= \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}
\]</div>
<p>with largest sample variance. For this to make sense, we need to constrain the <span class="math notranslate nohighlight">\(\phi_{j1}\)</span>s. Specifically, we require</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^p \phi_{j1}^2 = 1.
\]</div>
<p>The <span class="math notranslate nohighlight">\(\phi_{j1}\)</span>s are referred to as the <em>loadings</em> and the <span class="math notranslate nohighlight">\(t_{i1}\)</span>s are referred to as the <em>scores</em>.</p>
<p>Formally, we seek to solve</p>
<div class="math notranslate nohighlight">
\[
\max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\ :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\},
\]</div>
<p>where we used the fact that the <span class="math notranslate nohighlight">\(t_{i1}\)</span>s are centered</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{1}{n} \sum_{i=1}^n t_{i1}
&amp;= \frac{1}{n} \sum_{i=1}^n [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\
&amp;= \phi_{11} \frac{1}{n} \sum_{i=1}^n  x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n  x_{ip}\\
&amp;= 0,
\end{align*}\]</div>
<p>to compute their sample variance as the mean of their square</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 
= \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{t}_1 = (t_{11},\ldots,t_{n1})\)</span>. Then for all <span class="math notranslate nohighlight">\(i\)</span></p>
<div class="math notranslate nohighlight">
\[
t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1,
\]</div>
<p>or in vector form</p>
<div class="math notranslate nohighlight">
\[
\mathbf{t}_1 = X \boldsymbol{\phi}_1.
\]</div>
<p>Also</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 
= \frac{1}{n-1} \|\mathbf{t}_1\|^2
= \frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2.
\]</div>
<p>Rewriting the maximization problem above in vector form,</p>
<div class="math notranslate nohighlight">
\[
\max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\},
\]</div>
<p>we see that we have already encountered this problem (up to the factor of <span class="math notranslate nohighlight">\(1/(n-1)\)</span> which does not affect the solution). The solution is to take <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span> to be the top right singular vector of <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n-1}}X\)</span> (or simply <span class="math notranslate nohighlight">\(X\)</span>). As we know this is equivalent to computing the top eigenvector of the matrix <span class="math notranslate nohighlight">\(\frac{1}{n-1} X^T X\)</span>, which is the sample covariance matrix of the data (accounting for the fact that the data is already centered).</p>
<p><em>The second principal component:</em> The second principal component is the linear combination of the features</p>
<div class="math notranslate nohighlight">
\[
t_{i2}
= \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip}
\]</div>
<p>with largest sample variance that is also uncorrelated with the first principal component, in the sense that</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0.
\]</div>
<p>The next lemma shows how to deal with this condition. Again, we also require</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^p \phi_{j2}^2 = 1.
\]</div>
<p>As before, let <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{t}_2 = (t_{12},\ldots,t_{n2})\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Uncorrelated Principal Components)</strong> Assume <span class="math notranslate nohighlight">\(X \neq \mathbf{0}\)</span>. Let <span class="math notranslate nohighlight">\(t_{i1}\)</span>, <span class="math notranslate nohighlight">\(t_{i2}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_2\)</span> be as above (where, in particular, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span> is a top right singular vector of <span class="math notranslate nohighlight">\(X\)</span>). Then</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0
\]</div>
<p>holds if and only if</p>
<div class="math notranslate nohighlight">
\[
\langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The condition</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0
\]</div>
<p>is equivalent to</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0,
\]</div>
<p>where we dropped the <span class="math notranslate nohighlight">\(1/(n-1)\)</span> factor as it does not play any role. Using that <span class="math notranslate nohighlight">\(\mathbf{t}_1 = X \boldsymbol{\phi}_1\)</span>, and similarly, <span class="math notranslate nohighlight">\(\mathbf{t}_2 = X \boldsymbol{\phi}_2\)</span>, this is in turn equivalent to</p>
<div class="math notranslate nohighlight">
\[
\langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0.
\]</div>
<p>Because <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span> can be chosen as a top right singular vector in an SVD of <span class="math notranslate nohighlight">\(X\)</span>, it follows from the <em>SVD Relations</em> that
<span class="math notranslate nohighlight">\(X^T X \boldsymbol{\phi}_1 = \sigma_1^2 \boldsymbol{\phi}_1\)</span>, where <span class="math notranslate nohighlight">\(\sigma_1\)</span> is the singular value associated to <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span>. Since <span class="math notranslate nohighlight">\(X \neq 0\)</span>, <span class="math notranslate nohighlight">\(\sigma_1 &gt; 0\)</span>. Plugging this in the inner product on the left hand side above, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
&amp;= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\
&amp;= (X \boldsymbol{\phi}_2)^T (X \boldsymbol{\phi}_1)\\
&amp;= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
&amp;= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\
&amp;= \langle \boldsymbol{\phi}_2, \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\
&amp;= \sigma_1^2 \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle.
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(\sigma_1 \neq 0\)</span>, this is <span class="math notranslate nohighlight">\(0\)</span> if and only if <span class="math notranslate nohighlight">\(\langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>As a result, we can write the maximization problem for the second principal component in matrix form as</p>
<div class="math notranslate nohighlight">
\[
\max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2 = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}.
\]</div>
<p>Again, we see that we have encountered this problem before. The solution is to take <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_2\)</span> to be a second right singular vector in an SVD of <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n-1}}X\)</span> (or simply <span class="math notranslate nohighlight">\(X\)</span>). Again, this is equivalent to computing the second eigenvector of the sample covariance matrix <span class="math notranslate nohighlight">\(\frac{1}{n-1} X^T X\)</span>.</p>
<p><em>Further principal components:</em> We can proceed in a similar fashion and define further principal components.</p>
<p>To quote <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations">Wikipedia</a>:</p>
<blockquote>
<div><p>PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information […] PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest “variance” […]</p>
</div></blockquote>
<p>Formally, let</p>
<div class="math notranslate nohighlight">
\[
X = U \Sigma V^T
\]</div>
<p>be the SVD of the data matrix <span class="math notranslate nohighlight">\(X\)</span>. The principal component transformation, truncated at the <span class="math notranslate nohighlight">\(\ell\)</span>-th component, is</p>
<div class="math notranslate nohighlight">
\[
T = X V_{(\ell)}, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the matrix whose columns are the vectors <span class="math notranslate nohighlight">\(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\)</span>. Recall that <span class="math notranslate nohighlight">\(V_{(\ell)}\)</span> is the matrix made of the first <span class="math notranslate nohighlight">\(k\)</span> columns of <span class="math notranslate nohighlight">\(V\)</span>.</p>
<p>Then, using the orthonormality of the right singular vectors,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
T 
= U \Sigma V^T V_{(\ell)}
= U \Sigma \begin{bmatrix} I_{\ell \times \ell}\\
\mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix}
= U \begin{bmatrix}\Sigma_{(\ell)}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix}
= U_{(\ell)} \Sigma_{(\ell)}.
\end{split}\]</div>
<p>Put differently, the vector <span class="math notranslate nohighlight">\(\mathbf{t}_i\)</span> is the left singular vector <span class="math notranslate nohighlight">\(\mathbf{u}_i\)</span> scaled by the corresponding singular value <span class="math notranslate nohighlight">\(\sigma_i\)</span>.</p>
<p>Having established a formal connection between PCA and SVD, we implement PCA using the SVD algorithm <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.svd</span></code></a>. We perform mean centering (now is the time to read that quote about the importance of mean centering again), but not the optional standardization. We use the fact that, in NumPy, subtracting a matrix by a vector whose dimension matches the number of columns performs row-wise subtraction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">mean</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">l</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">[:</span><span class="n">l</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We apply it to the Gaussian Mixture Model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Plotting the result, we see that PCA does succeed in finding the main direction of variation. Note tha gap in the middle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png" src="../Images/0ad434c45a764ec31a02581fa80c1fd3.png" data-original-src="https://mmids-textbook.github.io/_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png"/>
</div>
</div>
<p>Note however that the first two principal components in fact “capture more noise” than what can be seen in the orginal first two coordinates, a form of overfitting.</p>
<p><strong>TRY IT!</strong> Compute the first two right singular vectors <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> of <span class="math notranslate nohighlight">\(X\)</span> after mean centering. Do they align well with the first and second standard basis vectors <span class="math notranslate nohighlight">\(\mathbf{e}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{e}_2\)</span>? Why or why not? (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb">Open in Colab</a>)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>We return to our motivating example. We apply PCA to our genetic dataset.</p>
<p><strong>Figure:</strong> Viruses (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Viruses" src="../Images/03664e03f12993cb6edd6189d59b79e1.png" data-original-src="https://mmids-textbook.github.io/_images/3D_visualization_of_virus-small.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We load the dataset again. Recall that it contains <span class="math notranslate nohighlight">\(1642\)</span> strains and lives in a <span class="math notranslate nohighlight">\(317\)</span>-dimensional space.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'h3n2-snp.csv'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our goal is to find a “good” low-dimensional representation of the data. We work with ten dimensions using PCA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">))]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">n_dims</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">pca</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">n_dims</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the first two principal components, and see what appears to be some potential structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png" src="../Images/f60d8d2f8537bfd4a76822ba9228e316.png" data-original-src="https://mmids-textbook.github.io/_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png"/>
</div>
</div>
<p>There seems to be some reasonably well-defined clusters in this projection. We use <span class="math notranslate nohighlight">\(k\)</span>-means to identiy clusters. We take advantage of the implementation in scikit-learn, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"><code class="docutils literal notranslate"><span class="pre">sklearn.cluster.KMeans</span></code></a>. By default, it finds <span class="math notranslate nohighlight">\(8\)</span> clusters. The clusters can be extracted from the attribute <code class="docutils literal notranslate"><span class="pre">labels_</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'k-means++'</span><span class="p">,</span> 
                <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="n">assign</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>
</pre></div>
</div>
</div>
</div>
<p>To further reveal the structure, we look at our the clusters spread out over the years. That information is in a separate file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data_oth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'h3n2-other.csv'</span><span class="p">)</span>
<span class="n">data_oth</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>strain</th>
      <th>length</th>
      <th>country</th>
      <th>year</th>
      <th>lon</th>
      <th>lat</th>
      <th>date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AB434107</td>
      <td>1701</td>
      <td>Japan</td>
      <td>2002</td>
      <td>137.215474</td>
      <td>35.584176</td>
      <td>2002/02/25</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AB434108</td>
      <td>1701</td>
      <td>Japan</td>
      <td>2002</td>
      <td>137.215474</td>
      <td>35.584176</td>
      <td>2002/03/01</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CY000113</td>
      <td>1762</td>
      <td>USA</td>
      <td>2002</td>
      <td>-73.940000</td>
      <td>40.670000</td>
      <td>2002/01/29</td>
    </tr>
    <tr>
      <th>3</th>
      <td>CY000209</td>
      <td>1760</td>
      <td>USA</td>
      <td>2002</td>
      <td>-73.940000</td>
      <td>40.670000</td>
      <td>2002/01/17</td>
    </tr>
    <tr>
      <th>4</th>
      <td>CY000217</td>
      <td>1760</td>
      <td>USA</td>
      <td>2002</td>
      <td>-73.940000</td>
      <td>40.670000</td>
      <td>2002/02/26</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">year</span> <span class="o">=</span> <span class="n">data_oth</span><span class="p">[</span><span class="s1">'year'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>For each cluster, we plot how many of its data points come from a specific year. Each cluster has a different color.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
    <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">year</span><span class="p">[</span><span class="n">assign</span> <span class="o">==</span> <span class="n">i</span><span class="p">],</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">unique</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">2001</span><span class="p">,</span> <span class="mi">2007</span><span class="p">),</span> <span class="n">xticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2002</span><span class="p">,</span> <span class="mi">2007</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png" src="../Images/1decff267b5cfeee392565099c5a1864.png" data-original-src="https://mmids-textbook.github.io/_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png"/>
</div>
</div>
<p>Remarkably, we see that each cluster comes mostly from one year or two consecutive ones. In other words, the clustering in this low-dimensional projection captures some true underlying structure that is not explicitly in the genetic data on which it is computed.</p>
<p>Going back to the first two principal components, we color the points on the scatterplot by year. (We use <a class="reference external" href="https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements"><code class="docutils literal notranslate"><span class="pre">legend_elements()</span></code></a> for automatic legend creation.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="n">c</span><span class="o">=</span><span class="n">year</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">year</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="o">*</span><span class="n">scatter</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png" src="../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png" data-original-src="https://mmids-textbook.github.io/_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png"/>
</div>
</div>
<p>To some extent, one can “see” the virus evolving from year to year. The <span class="math notranslate nohighlight">\(x\)</span>-axis in particular seems to correlate strongly with the year, in the sense that samples from later years tend to be towards one side of the plot.</p>
<p>To further quantify this observation, we use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html"><code class="docutils literal notranslate"><span class="pre">numpy.corrcoef</span></code></a> to compute the correlation coefficients between the year and the first <span class="math notranslate nohighlight">\(10\)</span> principal components.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_dims</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_dims</span><span class="p">):</span>
    <span class="n">corr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">T</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">year</span><span class="p">)))[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-0.7905001  -0.42806325  0.0870437  -0.16839491  0.05757342 -0.06046913
 -0.07920042  0.01436618 -0.02544749  0.04314641]
</pre></div>
</div>
</div>
</div>
<p>Indeed, we see that the first three or four principal components correlate well with the year.</p>
<p>Using <a class="reference external" href="https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8">related techniques</a>, one can also identify which mutations distinguish different epidemics (i.e., years).</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot about the difference between principal components analysis (PCA) and linear discriminant analysis (LDA). <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> What is the goal of principal components analysis (PCA)?</p>
<p>a) To find clusters in the data.</p>
<p>b) To find a low-dimensional representation of the data that captures the maximum variance.</p>
<p>c) To find the mean of each feature in the data.</p>
<p>d) To find the correlation between features in the data.</p>
<p><strong>2</strong> Formally, the first principal component is the linear combination of features <span class="math notranslate nohighlight">\(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\)</span> that solves which optimization problem?</p>
<p>a) <span class="math notranslate nohighlight">\(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 \leq 1\right\}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 \leq 1\right\}\)</span></p>
<p><strong>3</strong> What is the relationship between the loadings in PCA and the singular vectors of the data matrix?</p>
<p>a) The loadings are the left singular vectors.</p>
<p>b) The loadings are the right singular vectors.</p>
<p>c) The loadings are the singular values.</p>
<p>d) There is no direct relationship between loadings and singular vectors.</p>
<p><strong>4</strong> What is the dimensionality of the matrix <span class="math notranslate nohighlight">\(T\)</span> in the principal component transformation <span class="math notranslate nohighlight">\(T = XV^{(l)}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(n \times p\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(n \times l\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(l \times p\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(p \times l\)</span></p>
<p><strong>5</strong> What is the purpose of centering the data in PCA?</p>
<p>a) To make the calculations easier.</p>
<p>b) To ensure the first principal component describes the direction of maximum variance.</p>
<p>c) To normalize the data.</p>
<p>d) To remove outliers.</p>
<p>Answer for 1: b. Justification: The text states that “Principal components analysis (PCA) is a commonly used dimensionality reduction approach” and that “The first principal component is the linear combination of the features … with largest sample variance.”</p>
<p>Answer for 2: a. Justification: The text states that “Formally, we seek to solve <span class="math notranslate nohighlight">\(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)</span>.”</p>
<p>Answer for 3: b. Justification: The text explains that the solution to the PCA optimization problem is to take the loadings to be the top right singular vector of the data matrix.</p>
<p>Answer for 4: b. Justification: The matrix <span class="math notranslate nohighlight">\(T\)</span> contains the scores of the data points on the first <span class="math notranslate nohighlight">\(l\)</span> principal components. Since there are <span class="math notranslate nohighlight">\(n\)</span> data points and <span class="math notranslate nohighlight">\(l\)</span> principal components, the dimensionality of <span class="math notranslate nohighlight">\(T\)</span> is <span class="math notranslate nohighlight">\(n \times l\)</span>.</p>
<p>Answer for 5: b. Justification: The text mentions that “Mean subtraction (a.k.a. ‘mean centering’) is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance.”</p>
</section>
&#13;

<h2><span class="section-number">4.5.1. </span>Dimensionality reduction via principal components analysis (PCA)<a class="headerlink" href="#dimensionality-reduction-via-principal-components-analysis-pca" title="Link to this heading">#</a></h2>
<p>Principal components analysis (PCA)<span class="math notranslate nohighlight">\(\idx{principal components analysis}\xdi\)</span> is a commonly used dimensionality reduction approach that is closely related to what we described in the previous sections. We formalize the connection.</p>
<p><em>The data matrix:</em> In PCA we are given <span class="math notranslate nohighlight">\(n\)</span> data points <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_n \in \mathbb{R}^p\)</span> with <span class="math notranslate nohighlight">\(p\)</span> features (i.e., coordinates). We denote the components of <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> as <span class="math notranslate nohighlight">\((x_{i1},\ldots,x_{ip})\)</span>. As usual, we stack them up into a matrix <span class="math notranslate nohighlight">\(X\)</span> whose <span class="math notranslate nohighlight">\(i\)</span>-th row is <span class="math notranslate nohighlight">\(\mathbf{x}_i^T\)</span>.</p>
<p>The first step of PCA is to center the data, i.e., we assume that<span class="math notranslate nohighlight">\(\idx{mean centering}\xdi\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p
\]</div>
<p>Put differently, the empirical mean of each column is <span class="math notranslate nohighlight">\(0\)</span>. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations">Wikipedia</a> (and this will become clearer below):</p>
<blockquote>
<div><p>Mean subtraction (a.k.a. “mean centering”) is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.</p>
</div></blockquote>
<p>An optional step is to divide each column by the square root of its <a class="reference external" href="https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance">sample variance</a>, i.e., assume that</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p.
\]</div>
<p>As we mentioned in a previous chapter, this is particularly important when the features are measured in different units to ensure that their variability can be meaningfully compared.</p>
<p><em>The first principal component:</em> The first principal component is the linear combination of the features</p>
<div class="math notranslate nohighlight">
\[
t_{i1}
= \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}
\]</div>
<p>with largest sample variance. For this to make sense, we need to constrain the <span class="math notranslate nohighlight">\(\phi_{j1}\)</span>s. Specifically, we require</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^p \phi_{j1}^2 = 1.
\]</div>
<p>The <span class="math notranslate nohighlight">\(\phi_{j1}\)</span>s are referred to as the <em>loadings</em> and the <span class="math notranslate nohighlight">\(t_{i1}\)</span>s are referred to as the <em>scores</em>.</p>
<p>Formally, we seek to solve</p>
<div class="math notranslate nohighlight">
\[
\max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\ :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\},
\]</div>
<p>where we used the fact that the <span class="math notranslate nohighlight">\(t_{i1}\)</span>s are centered</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{1}{n} \sum_{i=1}^n t_{i1}
&amp;= \frac{1}{n} \sum_{i=1}^n [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\
&amp;= \phi_{11} \frac{1}{n} \sum_{i=1}^n  x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n  x_{ip}\\
&amp;= 0,
\end{align*}\]</div>
<p>to compute their sample variance as the mean of their square</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 
= \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{t}_1 = (t_{11},\ldots,t_{n1})\)</span>. Then for all <span class="math notranslate nohighlight">\(i\)</span></p>
<div class="math notranslate nohighlight">
\[
t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1,
\]</div>
<p>or in vector form</p>
<div class="math notranslate nohighlight">
\[
\mathbf{t}_1 = X \boldsymbol{\phi}_1.
\]</div>
<p>Also</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 
= \frac{1}{n-1} \|\mathbf{t}_1\|^2
= \frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2.
\]</div>
<p>Rewriting the maximization problem above in vector form,</p>
<div class="math notranslate nohighlight">
\[
\max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\},
\]</div>
<p>we see that we have already encountered this problem (up to the factor of <span class="math notranslate nohighlight">\(1/(n-1)\)</span> which does not affect the solution). The solution is to take <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span> to be the top right singular vector of <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n-1}}X\)</span> (or simply <span class="math notranslate nohighlight">\(X\)</span>). As we know this is equivalent to computing the top eigenvector of the matrix <span class="math notranslate nohighlight">\(\frac{1}{n-1} X^T X\)</span>, which is the sample covariance matrix of the data (accounting for the fact that the data is already centered).</p>
<p><em>The second principal component:</em> The second principal component is the linear combination of the features</p>
<div class="math notranslate nohighlight">
\[
t_{i2}
= \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip}
\]</div>
<p>with largest sample variance that is also uncorrelated with the first principal component, in the sense that</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0.
\]</div>
<p>The next lemma shows how to deal with this condition. Again, we also require</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^p \phi_{j2}^2 = 1.
\]</div>
<p>As before, let <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{t}_2 = (t_{12},\ldots,t_{n2})\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Uncorrelated Principal Components)</strong> Assume <span class="math notranslate nohighlight">\(X \neq \mathbf{0}\)</span>. Let <span class="math notranslate nohighlight">\(t_{i1}\)</span>, <span class="math notranslate nohighlight">\(t_{i2}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_2\)</span> be as above (where, in particular, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span> is a top right singular vector of <span class="math notranslate nohighlight">\(X\)</span>). Then</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0
\]</div>
<p>holds if and only if</p>
<div class="math notranslate nohighlight">
\[
\langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The condition</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0
\]</div>
<p>is equivalent to</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0,
\]</div>
<p>where we dropped the <span class="math notranslate nohighlight">\(1/(n-1)\)</span> factor as it does not play any role. Using that <span class="math notranslate nohighlight">\(\mathbf{t}_1 = X \boldsymbol{\phi}_1\)</span>, and similarly, <span class="math notranslate nohighlight">\(\mathbf{t}_2 = X \boldsymbol{\phi}_2\)</span>, this is in turn equivalent to</p>
<div class="math notranslate nohighlight">
\[
\langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0.
\]</div>
<p>Because <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span> can be chosen as a top right singular vector in an SVD of <span class="math notranslate nohighlight">\(X\)</span>, it follows from the <em>SVD Relations</em> that
<span class="math notranslate nohighlight">\(X^T X \boldsymbol{\phi}_1 = \sigma_1^2 \boldsymbol{\phi}_1\)</span>, where <span class="math notranslate nohighlight">\(\sigma_1\)</span> is the singular value associated to <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span>. Since <span class="math notranslate nohighlight">\(X \neq 0\)</span>, <span class="math notranslate nohighlight">\(\sigma_1 &gt; 0\)</span>. Plugging this in the inner product on the left hand side above, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
&amp;= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\
&amp;= (X \boldsymbol{\phi}_2)^T (X \boldsymbol{\phi}_1)\\
&amp;= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
&amp;= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\
&amp;= \langle \boldsymbol{\phi}_2, \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\
&amp;= \sigma_1^2 \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle.
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(\sigma_1 \neq 0\)</span>, this is <span class="math notranslate nohighlight">\(0\)</span> if and only if <span class="math notranslate nohighlight">\(\langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>As a result, we can write the maximization problem for the second principal component in matrix form as</p>
<div class="math notranslate nohighlight">
\[
\max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2 = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}.
\]</div>
<p>Again, we see that we have encountered this problem before. The solution is to take <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_2\)</span> to be a second right singular vector in an SVD of <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n-1}}X\)</span> (or simply <span class="math notranslate nohighlight">\(X\)</span>). Again, this is equivalent to computing the second eigenvector of the sample covariance matrix <span class="math notranslate nohighlight">\(\frac{1}{n-1} X^T X\)</span>.</p>
<p><em>Further principal components:</em> We can proceed in a similar fashion and define further principal components.</p>
<p>To quote <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations">Wikipedia</a>:</p>
<blockquote>
<div><p>PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information […] PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest “variance” […]</p>
</div></blockquote>
<p>Formally, let</p>
<div class="math notranslate nohighlight">
\[
X = U \Sigma V^T
\]</div>
<p>be the SVD of the data matrix <span class="math notranslate nohighlight">\(X\)</span>. The principal component transformation, truncated at the <span class="math notranslate nohighlight">\(\ell\)</span>-th component, is</p>
<div class="math notranslate nohighlight">
\[
T = X V_{(\ell)}, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the matrix whose columns are the vectors <span class="math notranslate nohighlight">\(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\)</span>. Recall that <span class="math notranslate nohighlight">\(V_{(\ell)}\)</span> is the matrix made of the first <span class="math notranslate nohighlight">\(k\)</span> columns of <span class="math notranslate nohighlight">\(V\)</span>.</p>
<p>Then, using the orthonormality of the right singular vectors,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
T 
= U \Sigma V^T V_{(\ell)}
= U \Sigma \begin{bmatrix} I_{\ell \times \ell}\\
\mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix}
= U \begin{bmatrix}\Sigma_{(\ell)}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix}
= U_{(\ell)} \Sigma_{(\ell)}.
\end{split}\]</div>
<p>Put differently, the vector <span class="math notranslate nohighlight">\(\mathbf{t}_i\)</span> is the left singular vector <span class="math notranslate nohighlight">\(\mathbf{u}_i\)</span> scaled by the corresponding singular value <span class="math notranslate nohighlight">\(\sigma_i\)</span>.</p>
<p>Having established a formal connection between PCA and SVD, we implement PCA using the SVD algorithm <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.svd</span></code></a>. We perform mean centering (now is the time to read that quote about the importance of mean centering again), but not the optional standardization. We use the fact that, in NumPy, subtracting a matrix by a vector whose dimension matches the number of columns performs row-wise subtraction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">mean</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">l</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">[:</span><span class="n">l</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We apply it to the Gaussian Mixture Model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Plotting the result, we see that PCA does succeed in finding the main direction of variation. Note tha gap in the middle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png" src="../Images/0ad434c45a764ec31a02581fa80c1fd3.png" data-original-src="https://mmids-textbook.github.io/_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png"/>
</div>
</div>
<p>Note however that the first two principal components in fact “capture more noise” than what can be seen in the orginal first two coordinates, a form of overfitting.</p>
<p><strong>TRY IT!</strong> Compute the first two right singular vectors <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> of <span class="math notranslate nohighlight">\(X\)</span> after mean centering. Do they align well with the first and second standard basis vectors <span class="math notranslate nohighlight">\(\mathbf{e}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{e}_2\)</span>? Why or why not? (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb">Open in Colab</a>)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>We return to our motivating example. We apply PCA to our genetic dataset.</p>
<p><strong>Figure:</strong> Viruses (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Viruses" src="../Images/03664e03f12993cb6edd6189d59b79e1.png" data-original-src="https://mmids-textbook.github.io/_images/3D_visualization_of_virus-small.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We load the dataset again. Recall that it contains <span class="math notranslate nohighlight">\(1642\)</span> strains and lives in a <span class="math notranslate nohighlight">\(317\)</span>-dimensional space.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'h3n2-snp.csv'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our goal is to find a “good” low-dimensional representation of the data. We work with ten dimensions using PCA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">))]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">n_dims</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">pca</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">n_dims</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the first two principal components, and see what appears to be some potential structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png" src="../Images/f60d8d2f8537bfd4a76822ba9228e316.png" data-original-src="https://mmids-textbook.github.io/_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png"/>
</div>
</div>
<p>There seems to be some reasonably well-defined clusters in this projection. We use <span class="math notranslate nohighlight">\(k\)</span>-means to identiy clusters. We take advantage of the implementation in scikit-learn, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"><code class="docutils literal notranslate"><span class="pre">sklearn.cluster.KMeans</span></code></a>. By default, it finds <span class="math notranslate nohighlight">\(8\)</span> clusters. The clusters can be extracted from the attribute <code class="docutils literal notranslate"><span class="pre">labels_</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'k-means++'</span><span class="p">,</span> 
                <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="n">assign</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>
</pre></div>
</div>
</div>
</div>
<p>To further reveal the structure, we look at our the clusters spread out over the years. That information is in a separate file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data_oth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'h3n2-other.csv'</span><span class="p">)</span>
<span class="n">data_oth</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>strain</th>
      <th>length</th>
      <th>country</th>
      <th>year</th>
      <th>lon</th>
      <th>lat</th>
      <th>date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AB434107</td>
      <td>1701</td>
      <td>Japan</td>
      <td>2002</td>
      <td>137.215474</td>
      <td>35.584176</td>
      <td>2002/02/25</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AB434108</td>
      <td>1701</td>
      <td>Japan</td>
      <td>2002</td>
      <td>137.215474</td>
      <td>35.584176</td>
      <td>2002/03/01</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CY000113</td>
      <td>1762</td>
      <td>USA</td>
      <td>2002</td>
      <td>-73.940000</td>
      <td>40.670000</td>
      <td>2002/01/29</td>
    </tr>
    <tr>
      <th>3</th>
      <td>CY000209</td>
      <td>1760</td>
      <td>USA</td>
      <td>2002</td>
      <td>-73.940000</td>
      <td>40.670000</td>
      <td>2002/01/17</td>
    </tr>
    <tr>
      <th>4</th>
      <td>CY000217</td>
      <td>1760</td>
      <td>USA</td>
      <td>2002</td>
      <td>-73.940000</td>
      <td>40.670000</td>
      <td>2002/02/26</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">year</span> <span class="o">=</span> <span class="n">data_oth</span><span class="p">[</span><span class="s1">'year'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>For each cluster, we plot how many of its data points come from a specific year. Each cluster has a different color.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
    <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">year</span><span class="p">[</span><span class="n">assign</span> <span class="o">==</span> <span class="n">i</span><span class="p">],</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">unique</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">2001</span><span class="p">,</span> <span class="mi">2007</span><span class="p">),</span> <span class="n">xticks</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2002</span><span class="p">,</span> <span class="mi">2007</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png" src="../Images/1decff267b5cfeee392565099c5a1864.png" data-original-src="https://mmids-textbook.github.io/_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png"/>
</div>
</div>
<p>Remarkably, we see that each cluster comes mostly from one year or two consecutive ones. In other words, the clustering in this low-dimensional projection captures some true underlying structure that is not explicitly in the genetic data on which it is computed.</p>
<p>Going back to the first two principal components, we color the points on the scatterplot by year. (We use <a class="reference external" href="https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements"><code class="docutils literal notranslate"><span class="pre">legend_elements()</span></code></a> for automatic legend creation.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="n">c</span><span class="o">=</span><span class="n">year</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">year</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="o">*</span><span class="n">scatter</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png" src="../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png" data-original-src="https://mmids-textbook.github.io/_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png"/>
</div>
</div>
<p>To some extent, one can “see” the virus evolving from year to year. The <span class="math notranslate nohighlight">\(x\)</span>-axis in particular seems to correlate strongly with the year, in the sense that samples from later years tend to be towards one side of the plot.</p>
<p>To further quantify this observation, we use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html"><code class="docutils literal notranslate"><span class="pre">numpy.corrcoef</span></code></a> to compute the correlation coefficients between the year and the first <span class="math notranslate nohighlight">\(10\)</span> principal components.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_dims</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_dims</span><span class="p">):</span>
    <span class="n">corr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">T</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">year</span><span class="p">)))[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-0.7905001  -0.42806325  0.0870437  -0.16839491  0.05757342 -0.06046913
 -0.07920042  0.01436618 -0.02544749  0.04314641]
</pre></div>
</div>
</div>
</div>
<p>Indeed, we see that the first three or four principal components correlate well with the year.</p>
<p>Using <a class="reference external" href="https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8">related techniques</a>, one can also identify which mutations distinguish different epidemics (i.e., years).</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot about the difference between principal components analysis (PCA) and linear discriminant analysis (LDA). <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> What is the goal of principal components analysis (PCA)?</p>
<p>a) To find clusters in the data.</p>
<p>b) To find a low-dimensional representation of the data that captures the maximum variance.</p>
<p>c) To find the mean of each feature in the data.</p>
<p>d) To find the correlation between features in the data.</p>
<p><strong>2</strong> Formally, the first principal component is the linear combination of features <span class="math notranslate nohighlight">\(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\)</span> that solves which optimization problem?</p>
<p>a) <span class="math notranslate nohighlight">\(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 \leq 1\right\}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 \leq 1\right\}\)</span></p>
<p><strong>3</strong> What is the relationship between the loadings in PCA and the singular vectors of the data matrix?</p>
<p>a) The loadings are the left singular vectors.</p>
<p>b) The loadings are the right singular vectors.</p>
<p>c) The loadings are the singular values.</p>
<p>d) There is no direct relationship between loadings and singular vectors.</p>
<p><strong>4</strong> What is the dimensionality of the matrix <span class="math notranslate nohighlight">\(T\)</span> in the principal component transformation <span class="math notranslate nohighlight">\(T = XV^{(l)}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(n \times p\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(n \times l\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(l \times p\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(p \times l\)</span></p>
<p><strong>5</strong> What is the purpose of centering the data in PCA?</p>
<p>a) To make the calculations easier.</p>
<p>b) To ensure the first principal component describes the direction of maximum variance.</p>
<p>c) To normalize the data.</p>
<p>d) To remove outliers.</p>
<p>Answer for 1: b. Justification: The text states that “Principal components analysis (PCA) is a commonly used dimensionality reduction approach” and that “The first principal component is the linear combination of the features … with largest sample variance.”</p>
<p>Answer for 2: a. Justification: The text states that “Formally, we seek to solve <span class="math notranslate nohighlight">\(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)</span>.”</p>
<p>Answer for 3: b. Justification: The text explains that the solution to the PCA optimization problem is to take the loadings to be the top right singular vector of the data matrix.</p>
<p>Answer for 4: b. Justification: The matrix <span class="math notranslate nohighlight">\(T\)</span> contains the scores of the data points on the first <span class="math notranslate nohighlight">\(l\)</span> principal components. Since there are <span class="math notranslate nohighlight">\(n\)</span> data points and <span class="math notranslate nohighlight">\(l\)</span> principal components, the dimensionality of <span class="math notranslate nohighlight">\(T\)</span> is <span class="math notranslate nohighlight">\(n \times l\)</span>.</p>
<p>Answer for 5: b. Justification: The text mentions that “Mean subtraction (a.k.a. ‘mean centering’) is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance.”</p>
    
</body>
</html>