- en: '2.5\. Application: regression analysis#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap02_ls/05_regression/roch-mmids-ls-regression.html](https://mmids-textbook.github.io/chap02_ls/05_regression/roch-mmids-ls-regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We return to our motivating example, the regression problem, and apply the least
    squares approach.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1\. Linear regression[#](#linear-regression "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear regression** \(\idx{linear regression}\xdi\) We seek an affine function
    to fit input data points \(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\), where \(\mathbf{x}_i
    = (x_{i,1}, \ldots, x_{i,d}) \in \mathbb{R}^d\) and \(y_i \in \mathbb{R}\) for
    all \(i\). The common approach involves finding coefficients \(\beta_j\)’s that
    minimize the criterion'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j x_{i,j}\right\}\right)^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: This is indeed a linear least squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![A regression line (with help from ChatGPT; code converted from (Source))](../Images/1f7e102fbf9a1f59441b2ed46f3349a3.png)'
  prefs: []
  type: TYPE_IMG
- en: In matrix form, let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix},
    \quad\quad A = \begin{pmatrix} 1 & \mathbf{x}_1^T \\ 1 & \mathbf{x}_2^T \\ \vdots
    & \vdots \\ 1 & \mathbf{x}_n^T \end{pmatrix} \quad\text{and}\quad \boldsymbol{\beta}
    = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_d \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then the problem is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\boldsymbol{\beta} \in \mathbb{R}^{d+1}} \|\mathbf{y} - A \boldsymbol{\beta}\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We assume that the columns of \(A\) are linearly independent, which is often
    the case with real data (unless there is an algebraic relationship between some
    columns). The normal equations are then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A \boldsymbol{\beta} = A^T \mathbf{y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\boldsymbol{\hat\beta} = (\hat{\beta}_0,\ldots,\hat{\beta}_d)\) be the
    unique solution of the system. It gives the vector of coefficients in our fitted
    model. We refer to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{y}_i = \beta_0 + \sum_{j=1}^d \beta_j x_{i,j}, \quad i = 1,\ldots,n
    \]
  prefs: []
  type: TYPE_NORMAL
- en: as the fitted values and to
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_i = y_i - \hat{y}_i, \quad i = 1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: as the residuals\(\idx{residuals}\xdi\). In vector form, we obtain \(\hat{\mathbf{y}}
    = (\hat{y}_1,\ldots,\hat{y}_n)\) and \(\mathbf{r} = (r_1,\ldots,r_n)\) as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\mathbf{y}} = A \boldsymbol{\hat\beta} \quad \text{and} \quad \mathbf{r}
    = \mathbf{y} - \hat{\mathbf{y}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The residual sum of squares (RSS)\(\idx{residual sum of squares}\xdi\) is given
    by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n r_i^2 = \sum_{i=1}^n \left(y_i - \left\{\hat{\beta}_0 + \sum_{j=1}^d
    \hat{\beta}_j x_{i,j}\right\}\right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: or, in vector form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{r}\|^2 = \|\mathbf{y} - \hat{\mathbf{y}}\|^2 = \|\mathbf{y} - A
    \boldsymbol{\hat\beta}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We test our least-squares method on simulated data. This
    has the advantage that we know the truth.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the truth is a linear function of one variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/b7534b4829e85e0392d88cd63283feab5152d865592acc624d093ad239193eea.png](../Images/2eef44a4747d0f6416d01155aadc0534.png)'
  prefs: []
  type: TYPE_IMG
- en: A perfect straight line is little too easy. So let’s add some noise. That is,
    to each \(y_i\) we add an independent random variable \(\varepsilon_i\) with a
    standard Normal distribution (mean \(0\), variance \(1\)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/b5adb381b022a250bc499f92a2c124a2d277fd2ff3dba90ad87b99b3a8ee4a0a.png](../Images/0ea0617d403c0459cb176423ab08728a.png)'
  prefs: []
  type: TYPE_IMG
- en: We form the matrix \(A\) and use our least-squares code to solve for \(\boldsymbol{\hat\beta}\).
    The function `ls_by_qr`, which we implemented previously, is in [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py),
    which is available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/f3588b24516021d16ce7c2c1f232ecd770e8ed04ad3c889ad37d7d7e7e1f797d.png](../Images/d879007a695e84bcc491f8d33559f076.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2\. Polynomial regression (and overfitting)[#](#polynomial-regression-and-overfitting
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Beyond linearity** \(\idx{polynomial regression}\xdi\) The linear assumption
    is not as restrictive as it may first appear. The same approach can be extended
    straightforwardly to fit polynomials or more complicated combination of functions.
    For instance, suppose \(d=1\). To fit a second degree polynomial to the data \(\{(x_i,
    y_i)\}_{i=1}^n\), we add a column to the \(A\) matrix with the squares of the
    \(x_i\)’s. That is, we let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & x_1 & x_1^2 \\ 1 & x_2 & x_2^2 \\ \vdots
    & \vdots & \vdots \\ 1 & x_n & x_n^2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then, we are indeed fitting a degree-two polynomial as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A \boldsymbol{\beta})_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: The solution otherwise remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: This idea of adding columns can also be used to model interactions between predictors.
    Suppose \(d=2\). Then we can consider the following \(A\) matrix, where the last
    column combines both predictors into their product,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & x_{11} & x_{12} & x_{11} x_{12} \\ 1
    & x_{21} & x_{22} & x_{21} x_{22} \\ \vdots & \vdots & \vdots & \vdots\\ 1 & x_{n1}
    & x_{n2} & x_{n1} x_{n2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Suppose the truth is in fact a degree-two polynomial
    of one variable with Gaussian noise.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/eadda9bff35f63929d01806e5b558ed3350db0b9cbc7efd1b446b52ddff4ae34.png](../Images/9fde58544d9266d798debbcd18cf995a.png)'
  prefs: []
  type: TYPE_IMG
- en: We form the matrix \(A\) and use our least-squares code to solve for \(\boldsymbol{\hat\beta}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]</details> ![../../_images/bbf5609a182e64df13441c79b85c74d9a49dbfbef9716ba956e2f13e70dea1d2.png](../Images/c79327531705ad13d8cb4943a270ee4e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting in polynomial regression** In adding more parameters, one must
    worry about [overfitting](https://en.wikipedia.org/wiki/Overfitting#cite_note-1)\(\idx{overfitting}\xdi\).
    To quote Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: In statistics, overfitting is “the production of an analysis that corresponds
    too closely or exactly to a particular set of data, and may therefore fail to
    fit additional data or predict future observations reliably”.[[1](https://en.wikipedia.org/wiki/Overfitting#cite_note-1)]
    An overfitted model is a statistical model that contains more parameters than
    can be justified by the data.[[2](https://en.wikipedia.org/wiki/Overfitting#cite_note-CDS-2)]
    The essence of overfitting is to have unknowingly extracted some of the residual
    variation (i.e. the noise) as if that variation represented underlying model structure.[[3](https://en.wikipedia.org/wiki/Overfitting#cite_note-BA2002-3)]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We return to the `Advertising` dataset from the [[ISLP]](https://www.statlearning.com/)
    textbook. We load the dataset again.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Pie chart (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting sales](../Images/a2936245bd692ef24ec32aa2c9836872.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We will focus for now on the TV budget. We form the matrix \(A\) and use our
    least-squares code to solve for \(\boldsymbol{\beta}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]</details> ![../../_images/8f726a7109f0d9b61448ef49928a680ed907765437c981ee6e04aeb275693a52.png](../Images/bcffea330b988d225e11b92c7e85f38a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: A degree-two polynomial might be a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]</details> ![../../_images/bae314a4c6b951c24b56035f36157378a8272971c2861c45649e81183d6eb5ce.png](../Images/6df89699e41e6a5486cb045e7360d476.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The fit looks slightly better than the linear one. This is not entirely surprising
    though given that the linear model is a subset of the quadratic one. But, as we
    mentioned earlier, when adding more parameters we must now worry about overfitting
    the data. To illustrate, let’s see what happens with a degree-\(20\) polynomial
    fit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/93ca62931e6172f1807be38a582a4ad5d64a7cd5553092a78c14a6ecc358c896.png](../Images/8a6079d567365e38718954b97e0d5d59.png)'
  prefs: []
  type: TYPE_IMG
- en: The outcome now seems to vary wildly, seemingly driven by the randomness of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN:** Ask your favorite AI chatbot about using cross-validation
    to choose a suitable degree. Ask for code and apply it to this dataset. ([Open
    In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In linear regression, the goal is to find coefficients \(\beta_j\)’s
    that minimize which of the following criteria?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij})\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij})^2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sum_{i=1}^n (y_i - \{\beta_0 + \sum_{j=1}^d \beta_j x_{ij}\}^2)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sum_{i=1}^n |y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij}|\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** The normal equations for linear regression are:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(A^T A \boldsymbol{\beta} = A^T \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A A^T \boldsymbol{\beta} = A \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(A^T A \boldsymbol{\beta} = A \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(A A^T \boldsymbol{\beta} = A^T \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** In the numerical example with a degree-20 polynomial fit, the fitted
    curve:'
  prefs: []
  type: TYPE_NORMAL
- en: a) Fits the data perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: b) Fails to capture the overall trend in the data.
  prefs: []
  type: TYPE_NORMAL
- en: c) Captures the noise in the data as if it were the underlying structure.
  prefs: []
  type: TYPE_NORMAL
- en: d) Is a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the primary advantage of using simulated data to test the least
    squares method?'
  prefs: []
  type: TYPE_NORMAL
- en: a) Simulated data eliminates the need for real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: b) Simulated data provides a perfect fit without noise.
  prefs: []
  type: TYPE_NORMAL
- en: c) Simulated data allows us to know the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: d) Simulated data reduces computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following best describes overfitting?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The model fits the training data well but generalizes poorly to new data.
  prefs: []
  type: TYPE_NORMAL
- en: b) The model fits both the training data and new data well.
  prefs: []
  type: TYPE_NORMAL
- en: c) The model fits the training data poorly but generalizes well to new data.
  prefs: []
  type: TYPE_NORMAL
- en: d) The model ignores random noise in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that in linear regression,
    we seek to find coefficients \(\beta_j\)’s that minimize the criterion \(\sum_{i=1}^n
    (y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij})^2\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states, “The normal equations are
    then \(A^T A \boldsymbol{\beta} = A^T \mathbf{y}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states that “The essence of overfitting
    is to have unknowingly extracted some of the residual variation (i.e., the noise)
    as if that variation represented underlying model structure.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text notes, “This has the advantage that
    we know the truth.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: a. Justification: The text quotes Wikipedia: “An overfitted model
    is a statistical model that contains more parameters than can be justified by
    the data.”'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1\. Linear regression[#](#linear-regression "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear regression** \(\idx{linear regression}\xdi\) We seek an affine function
    to fit input data points \(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\), where \(\mathbf{x}_i
    = (x_{i,1}, \ldots, x_{i,d}) \in \mathbb{R}^d\) and \(y_i \in \mathbb{R}\) for
    all \(i\). The common approach involves finding coefficients \(\beta_j\)’s that
    minimize the criterion'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j x_{i,j}\right\}\right)^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: This is indeed a linear least squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![A regression line (with help from ChatGPT; code converted from (Source))](../Images/1f7e102fbf9a1f59441b2ed46f3349a3.png)'
  prefs: []
  type: TYPE_IMG
- en: In matrix form, let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix},
    \quad\quad A = \begin{pmatrix} 1 & \mathbf{x}_1^T \\ 1 & \mathbf{x}_2^T \\ \vdots
    & \vdots \\ 1 & \mathbf{x}_n^T \end{pmatrix} \quad\text{and}\quad \boldsymbol{\beta}
    = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_d \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then the problem is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\boldsymbol{\beta} \in \mathbb{R}^{d+1}} \|\mathbf{y} - A \boldsymbol{\beta}\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We assume that the columns of \(A\) are linearly independent, which is often
    the case with real data (unless there is an algebraic relationship between some
    columns). The normal equations are then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A \boldsymbol{\beta} = A^T \mathbf{y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\boldsymbol{\hat\beta} = (\hat{\beta}_0,\ldots,\hat{\beta}_d)\) be the
    unique solution of the system. It gives the vector of coefficients in our fitted
    model. We refer to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{y}_i = \beta_0 + \sum_{j=1}^d \beta_j x_{i,j}, \quad i = 1,\ldots,n
    \]
  prefs: []
  type: TYPE_NORMAL
- en: as the fitted values and to
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_i = y_i - \hat{y}_i, \quad i = 1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: as the residuals\(\idx{residuals}\xdi\). In vector form, we obtain \(\hat{\mathbf{y}}
    = (\hat{y}_1,\ldots,\hat{y}_n)\) and \(\mathbf{r} = (r_1,\ldots,r_n)\) as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\mathbf{y}} = A \boldsymbol{\hat\beta} \quad \text{and} \quad \mathbf{r}
    = \mathbf{y} - \hat{\mathbf{y}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The residual sum of squares (RSS)\(\idx{residual sum of squares}\xdi\) is given
    by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n r_i^2 = \sum_{i=1}^n \left(y_i - \left\{\hat{\beta}_0 + \sum_{j=1}^d
    \hat{\beta}_j x_{i,j}\right\}\right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: or, in vector form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{r}\|^2 = \|\mathbf{y} - \hat{\mathbf{y}}\|^2 = \|\mathbf{y} - A
    \boldsymbol{\hat\beta}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We test our least-squares method on simulated data. This
    has the advantage that we know the truth.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the truth is a linear function of one variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/b7534b4829e85e0392d88cd63283feab5152d865592acc624d093ad239193eea.png](../Images/2eef44a4747d0f6416d01155aadc0534.png)'
  prefs: []
  type: TYPE_IMG
- en: A perfect straight line is little too easy. So let’s add some noise. That is,
    to each \(y_i\) we add an independent random variable \(\varepsilon_i\) with a
    standard Normal distribution (mean \(0\), variance \(1\)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/b5adb381b022a250bc499f92a2c124a2d277fd2ff3dba90ad87b99b3a8ee4a0a.png](../Images/0ea0617d403c0459cb176423ab08728a.png)'
  prefs: []
  type: TYPE_IMG
- en: We form the matrix \(A\) and use our least-squares code to solve for \(\boldsymbol{\hat\beta}\).
    The function `ls_by_qr`, which we implemented previously, is in [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py),
    which is available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/f3588b24516021d16ce7c2c1f232ecd770e8ed04ad3c889ad37d7d7e7e1f797d.png](../Images/d879007a695e84bcc491f8d33559f076.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2\. Polynomial regression (and overfitting)[#](#polynomial-regression-and-overfitting
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Beyond linearity** \(\idx{polynomial regression}\xdi\) The linear assumption
    is not as restrictive as it may first appear. The same approach can be extended
    straightforwardly to fit polynomials or more complicated combination of functions.
    For instance, suppose \(d=1\). To fit a second degree polynomial to the data \(\{(x_i,
    y_i)\}_{i=1}^n\), we add a column to the \(A\) matrix with the squares of the
    \(x_i\)’s. That is, we let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & x_1 & x_1^2 \\ 1 & x_2 & x_2^2 \\ \vdots
    & \vdots & \vdots \\ 1 & x_n & x_n^2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then, we are indeed fitting a degree-two polynomial as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A \boldsymbol{\beta})_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: The solution otherwise remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: This idea of adding columns can also be used to model interactions between predictors.
    Suppose \(d=2\). Then we can consider the following \(A\) matrix, where the last
    column combines both predictors into their product,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & x_{11} & x_{12} & x_{11} x_{12} \\ 1
    & x_{21} & x_{22} & x_{21} x_{22} \\ \vdots & \vdots & \vdots & \vdots\\ 1 & x_{n1}
    & x_{n2} & x_{n1} x_{n2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Suppose the truth is in fact a degree-two polynomial
    of one variable with Gaussian noise.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/eadda9bff35f63929d01806e5b558ed3350db0b9cbc7efd1b446b52ddff4ae34.png](../Images/9fde58544d9266d798debbcd18cf995a.png)'
  prefs: []
  type: TYPE_IMG
- en: We form the matrix \(A\) and use our least-squares code to solve for \(\boldsymbol{\hat\beta}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]</details> ![../../_images/bbf5609a182e64df13441c79b85c74d9a49dbfbef9716ba956e2f13e70dea1d2.png](../Images/c79327531705ad13d8cb4943a270ee4e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting in polynomial regression** In adding more parameters, one must
    worry about [overfitting](https://en.wikipedia.org/wiki/Overfitting#cite_note-1)\(\idx{overfitting}\xdi\).
    To quote Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: In statistics, overfitting is “the production of an analysis that corresponds
    too closely or exactly to a particular set of data, and may therefore fail to
    fit additional data or predict future observations reliably”.[[1](https://en.wikipedia.org/wiki/Overfitting#cite_note-1)]
    An overfitted model is a statistical model that contains more parameters than
    can be justified by the data.[[2](https://en.wikipedia.org/wiki/Overfitting#cite_note-CDS-2)]
    The essence of overfitting is to have unknowingly extracted some of the residual
    variation (i.e. the noise) as if that variation represented underlying model structure.[[3](https://en.wikipedia.org/wiki/Overfitting#cite_note-BA2002-3)]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We return to the `Advertising` dataset from the [[ISLP]](https://www.statlearning.com/)
    textbook. We load the dataset again.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Pie chart (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting sales](../Images/a2936245bd692ef24ec32aa2c9836872.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We will focus for now on the TV budget. We form the matrix \(A\) and use our
    least-squares code to solve for \(\boldsymbol{\beta}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]</details> ![../../_images/8f726a7109f0d9b61448ef49928a680ed907765437c981ee6e04aeb275693a52.png](../Images/bcffea330b988d225e11b92c7e85f38a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: A degree-two polynomial might be a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]</details> ![../../_images/bae314a4c6b951c24b56035f36157378a8272971c2861c45649e81183d6eb5ce.png](../Images/6df89699e41e6a5486cb045e7360d476.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The fit looks slightly better than the linear one. This is not entirely surprising
    though given that the linear model is a subset of the quadratic one. But, as we
    mentioned earlier, when adding more parameters we must now worry about overfitting
    the data. To illustrate, let’s see what happens with a degree-\(20\) polynomial
    fit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/93ca62931e6172f1807be38a582a4ad5d64a7cd5553092a78c14a6ecc358c896.png](../Images/8a6079d567365e38718954b97e0d5d59.png)'
  prefs: []
  type: TYPE_IMG
- en: The outcome now seems to vary wildly, seemingly driven by the randomness of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN:** Ask your favorite AI chatbot about using cross-validation
    to choose a suitable degree. Ask for code and apply it to this dataset. ([Open
    In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In linear regression, the goal is to find coefficients \(\beta_j\)’s
    that minimize which of the following criteria?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij})\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij})^2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sum_{i=1}^n (y_i - \{\beta_0 + \sum_{j=1}^d \beta_j x_{ij}\}^2)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sum_{i=1}^n |y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij}|\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** The normal equations for linear regression are:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(A^T A \boldsymbol{\beta} = A^T \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A A^T \boldsymbol{\beta} = A \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(A^T A \boldsymbol{\beta} = A \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(A A^T \boldsymbol{\beta} = A^T \mathbf{y}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** In the numerical example with a degree-20 polynomial fit, the fitted
    curve:'
  prefs: []
  type: TYPE_NORMAL
- en: a) Fits the data perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: b) Fails to capture the overall trend in the data.
  prefs: []
  type: TYPE_NORMAL
- en: c) Captures the noise in the data as if it were the underlying structure.
  prefs: []
  type: TYPE_NORMAL
- en: d) Is a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the primary advantage of using simulated data to test the least
    squares method?'
  prefs: []
  type: TYPE_NORMAL
- en: a) Simulated data eliminates the need for real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: b) Simulated data provides a perfect fit without noise.
  prefs: []
  type: TYPE_NORMAL
- en: c) Simulated data allows us to know the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: d) Simulated data reduces computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following best describes overfitting?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The model fits the training data well but generalizes poorly to new data.
  prefs: []
  type: TYPE_NORMAL
- en: b) The model fits both the training data and new data well.
  prefs: []
  type: TYPE_NORMAL
- en: c) The model fits the training data poorly but generalizes well to new data.
  prefs: []
  type: TYPE_NORMAL
- en: d) The model ignores random noise in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that in linear regression,
    we seek to find coefficients \(\beta_j\)’s that minimize the criterion \(\sum_{i=1}^n
    (y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij})^2\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states, “The normal equations are
    then \(A^T A \boldsymbol{\beta} = A^T \mathbf{y}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states that “The essence of overfitting
    is to have unknowingly extracted some of the residual variation (i.e., the noise)
    as if that variation represented underlying model structure.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text notes, “This has the advantage that
    we know the truth.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: a. Justification: The text quotes Wikipedia: “An overfitted model
    is a statistical model that contains more parameters than can be justified by
    the data.”'
  prefs: []
  type: TYPE_NORMAL
