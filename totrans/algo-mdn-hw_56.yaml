- en: Memory Sharing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://en.algorithmica.org/hpc/cpu-cache/sharing/](https://en.algorithmica.org/hpc/cpu-cache/sharing/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Starting at some level of the hierarchy, the cache becomes *shared* between
    different cores. This reduces the total die area and lets you add more cores on
    a single chip but also poses some “noisy neighbor” problems as it limits the effective
    cache size and bandwidth available to a single execution thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'On most CPUs, only the last layer of cache is shared, and not always in a uniform
    manner. On my machine, there are 8 physical cores, and the size of the L3 cache
    is 8M, but it is split into two halves: two groups of 4 cores have access to their
    own 4M region of the L3 cache, and not all of it.'
  prefs: []
  type: TYPE_NORMAL
- en: There are even more complex topologies, where accessing certain regions of memory
    takes non-constant time, different for each core (which is [sometimes unintended](https://randomascii.wordpress.com/2022/01/12/5-5-mm-in-1-25-nanoseconds/)).
    Such architectural feature is called *non-uniform memory access* (NUMA), and it
    is the case for multi-socket systems that have several separate CPU chips installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux, the topology of the memory system can be retrieved with `lstopo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8766de46d9659d36ee4bbd02aff328df.png)'
  prefs: []
  type: TYPE_IMG
- en: Cache hierarchy of my Ryzen 7 4700U generated by lstopo
  prefs: []
  type: TYPE_NORMAL
- en: 'This has some important implications for parallel algorithms: the performance
    of multi-threaded memory accesses depends on which cores are running which execution
    threads. To demonstrate this, we will run the [bandwidth benchmarks](../bandwidth)
    in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/sharing/#cpu-affinity)CPU
    Affinity'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of modifying the source code to run on multiple threads, we can simply
    run multiple identical processes with [GNU parallel](https://www.gnu.org/software/parallel/).
    To control which cores are executing them, we set their *processor affinity* with
    `taskset`. This combined command runs 4 processes that can run on the first 4
    cores of the CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what we get when we change the number of processes running simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5955ad541212d1735faf1d1d119eeb2c.png)'
  prefs: []
  type: TYPE_IMG
- en: You can now see that the performance decreases with more processes when the
    array exceeds the L2 cache (which is private to each core), as the cores start
    competing for the shared L3 cached and the RAM.
  prefs: []
  type: TYPE_NORMAL
- en: We specifically set all processes to run on the first 4 cores because they have
    a unified L3 cache. If some of the processes were to be scheduled on the other
    half of the cores, there would be less contention for the L3 cache. The operating
    system doesn’t [monitor](/hpc/profiling/events) such activities — what a process
    does is its own private business — so by default, it assigns threads to cores
    arbitrarily during execution, without caring about cache affinity and only taking
    into account the core load.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run another benchmark, but now with pinning the processes to different
    4-core groups that don’t share L3 cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It performs better — as if there were twice as much L3 cache and RAM bandwidth
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c47d34b5f30ae6c11e552c7706a649c6.png)'
  prefs: []
  type: TYPE_IMG
- en: These issues are especially tricky when benchmarking and are a huge source of
    noise when timing parallel applications.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/sharing/#saturating-bandwidth)Saturating
    Bandwidth'
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking at the RAM section of the first graph, it may seem that with more
    cores, the per-process throughput goes ½, ⅓, ¼, and so on, and the total bandwidth
    remains constant. But this isn’t quite true: the contention hurts, but a single
    CPU core usually can’t saturate all of the RAM bandwidth.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we plot it more carefully, we see that the total bandwidth actually increases
    with the number of cores — although not proportionally, and eventually approaches
    its theoretical maximum of ~42.4 GB/s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3194f4406301fe218f4bfa6334f95291.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we still specify processor affinity: the $k$-threaded run uses the
    first $k$ cores. This is why we have such a huge performance increase when switching
    from 4 cores to 5: you can have more RAM bandwidth if the requests go through
    separate L3 caches.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, to achieve maximum bandwidth, you should always split the threads
    of an application symmetrically. [← Cache Lines](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)[Memory-Level
    Parallelism →](https://en.algorithmica.org/hpc/cpu-cache/mlp/)
  prefs: []
  type: TYPE_NORMAL
