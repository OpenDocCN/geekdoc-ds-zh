- en: Memory Sharing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存共享
- en: 原文：[https://en.algorithmica.org/hpc/cpu-cache/sharing/](https://en.algorithmica.org/hpc/cpu-cache/sharing/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/cpu-cache/sharing/](https://en.algorithmica.org/hpc/cpu-cache/sharing/)
- en: Starting at some level of the hierarchy, the cache becomes *shared* between
    different cores. This reduces the total die area and lets you add more cores on
    a single chip but also poses some “noisy neighbor” problems as it limits the effective
    cache size and bandwidth available to a single execution thread.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从层次结构的某个级别开始，缓存在不同核心之间变得*共享*。这减少了总的芯片面积，并允许你在单个芯片上添加更多核心，但同时也带来了一些“嘈杂邻居”问题，因为它限制了单个执行线程可用的有效缓存大小和带宽。
- en: 'On most CPUs, only the last layer of cache is shared, and not always in a uniform
    manner. On my machine, there are 8 physical cores, and the size of the L3 cache
    is 8M, but it is split into two halves: two groups of 4 cores have access to their
    own 4M region of the L3 cache, and not all of it.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数CPU上，只有最后一层缓存是共享的，并且不一定是以统一的方式。在我的机器上，有8个物理核心，L3缓存的容量是8M，但它被分成两半：两组4个核心可以访问它们自己的4M
    L3缓存区域，而不是全部。
- en: There are even more complex topologies, where accessing certain regions of memory
    takes non-constant time, different for each core (which is [sometimes unintended](https://randomascii.wordpress.com/2022/01/12/5-5-mm-in-1-25-nanoseconds/)).
    Such architectural feature is called *non-uniform memory access* (NUMA), and it
    is the case for multi-socket systems that have several separate CPU chips installed.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些更复杂的拓扑结构，访问内存的某些区域需要非恒定的时间，每个核心不同（这有时是不故意的[有时是不故意的](https://randomascii.wordpress.com/2022/01/12/5-5-mm-in-1-25-nanoseconds/)）。这种架构特性被称为*非一致性内存访问*（NUMA），对于安装了多个独立CPU芯片的多插槽系统来说，这种情况是存在的。
- en: 'On Linux, the topology of the memory system can be retrieved with `lstopo`:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，可以使用`lstopo`检索内存系统的拓扑结构：
- en: '![](../Images/8766de46d9659d36ee4bbd02aff328df.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8766de46d9659d36ee4bbd02aff328df.png)'
- en: Cache hierarchy of my Ryzen 7 4700U generated by lstopo
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由lstopo生成的Ryzen 7 4700U的缓存层次结构
- en: 'This has some important implications for parallel algorithms: the performance
    of multi-threaded memory accesses depends on which cores are running which execution
    threads. To demonstrate this, we will run the [bandwidth benchmarks](../bandwidth)
    in parallel.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这对并行算法有一些重要的含义：多线程内存访问的性能取决于哪些核心正在运行哪些执行线程。为了演示这一点，我们将并行运行[带宽基准测试](../bandwidth)。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/sharing/#cpu-affinity)CPU
    Affinity'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/sharing/#cpu-affinity)CPU亲和力'
- en: 'Instead of modifying the source code to run on multiple threads, we can simply
    run multiple identical processes with [GNU parallel](https://www.gnu.org/software/parallel/).
    To control which cores are executing them, we set their *processor affinity* with
    `taskset`. This combined command runs 4 processes that can run on the first 4
    cores of the CPU:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必修改源代码以在多个线程上运行，我们可以简单地运行多个相同的进程，使用[GNU parallel](https://www.gnu.org/software/parallel/)。为了控制哪些核心正在执行它们，我们使用`taskset`设置它们的*处理器亲和力*。这个组合命令运行4个进程，这些进程可以在CPU的第一个4个核心上运行：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is what we get when we change the number of processes running simultaneously:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们改变同时运行的过程数量时，我们会得到以下结果：
- en: '![](../Images/5955ad541212d1735faf1d1d119eeb2c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5955ad541212d1735faf1d1d119eeb2c.png)'
- en: You can now see that the performance decreases with more processes when the
    array exceeds the L2 cache (which is private to each core), as the cores start
    competing for the shared L3 cached and the RAM.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以看到，当数组超过L2缓存（这是每个核心的私有缓存）时，随着核心开始竞争共享的L3缓存和RAM，性能会下降。
- en: We specifically set all processes to run on the first 4 cores because they have
    a unified L3 cache. If some of the processes were to be scheduled on the other
    half of the cores, there would be less contention for the L3 cache. The operating
    system doesn’t [monitor](/hpc/profiling/events) such activities — what a process
    does is its own private business — so by default, it assigns threads to cores
    arbitrarily during execution, without caring about cache affinity and only taking
    into account the core load.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别将所有进程设置为在第一个4个核心上运行，因为它们有一个统一的L3缓存。如果一些进程被调度到核心的另一半，对L3缓存的竞争将会减少。操作系统不会[监控](/hpc/profiling/events)此类活动——进程做什么是其自己的私事——因此默认情况下，它在执行期间任意地将线程分配给核心，而不关心缓存亲和力，只考虑核心负载。
- en: 'Let’s run another benchmark, but now with pinning the processes to different
    4-core groups that don’t share L3 cache:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再运行另一个基准测试，但现在将进程固定到不同的4核心组，这些组不共享L3缓存：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It performs better — as if there were twice as much L3 cache and RAM bandwidth
    available:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它的表现更好——就好像有双倍的 L3 缓存和 RAM 带宽可用：
- en: '![](../Images/c47d34b5f30ae6c11e552c7706a649c6.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c47d34b5f30ae6c11e552c7706a649c6.png)'
- en: These issues are especially tricky when benchmarking and are a huge source of
    noise when timing parallel applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题在基准测试时尤其棘手，并且在并行应用程序的计时中是巨大的噪声来源。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/sharing/#saturating-bandwidth)Saturating
    Bandwidth'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/sharing/#saturating-bandwidth)
    满足带宽'
- en: 'When looking at the RAM section of the first graph, it may seem that with more
    cores, the per-process throughput goes ½, ⅓, ¼, and so on, and the total bandwidth
    remains constant. But this isn’t quite true: the contention hurts, but a single
    CPU core usually can’t saturate all of the RAM bandwidth.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看第一个图的 RAM 部分，可能会觉得随着核心数的增加，每个进程的吞吐量会减半、减到三分之一、减到四分之一等等，而总带宽保持不变。但这并不完全正确：竞争会损害性能，但单个
    CPU 核心通常无法饱和所有的 RAM 带宽。
- en: 'If we plot it more carefully, we see that the total bandwidth actually increases
    with the number of cores — although not proportionally, and eventually approaches
    its theoretical maximum of ~42.4 GB/s:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更仔细地绘制，我们会看到总带宽实际上随着核心数的增加而增加——尽管不是成比例的，并且最终接近其理论最大值约 ~42.4 GB/s：
- en: '![](../Images/3194f4406301fe218f4bfa6334f95291.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3194f4406301fe218f4bfa6334f95291.png)'
- en: 'Note that we still specify processor affinity: the $k$-threaded run uses the
    first $k$ cores. This is why we have such a huge performance increase when switching
    from 4 cores to 5: you can have more RAM bandwidth if the requests go through
    separate L3 caches.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们仍然指定处理器亲和性：$k$ 线程运行使用前 $k$ 个核心。这就是为什么从 4 个核心切换到 5 个核心时性能提升如此巨大：如果请求通过独立的
    L3 缓存，你可以拥有更多的 RAM 带宽。
- en: In general, to achieve maximum bandwidth, you should always split the threads
    of an application symmetrically. [← Cache Lines](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)[Memory-Level
    Parallelism →](https://en.algorithmica.org/hpc/cpu-cache/mlp/)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了实现最大带宽，你应该始终对称地分割应用程序的线程。[← 缓存行](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)[内存级并行处理
    →](https://en.algorithmica.org/hpc/cpu-cache/mlp/)
