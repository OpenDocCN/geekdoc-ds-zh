- en: Chapter 6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章
- en: Memory Handling with CUDA
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA进行内存管理
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In the conventional CPU model we have what is called a linear or flat memory
    model. This is where any single CPU core can access any memory location without
    restriction. In practice, for CPU hardware, you typically see a level one (L1),
    level two (L2), and level three (L3) cache. Those people who have optimized CPU
    code or come from a high-performance computing (HPC) background will be all too
    familiar with this. For most programmers, however, it’s something they can easily
    abstract away.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的CPU模型中，我们拥有所谓的线性或平面内存模型。这意味着任何单个CPU核心都可以不受限制地访问任何内存位置。在实践中，对于CPU硬件，通常会看到一级（L1）、二级（L2）和三级（L3）缓存。那些优化过CPU代码或来自高性能计算（HPC）背景的人会非常熟悉这一点。然而，对于大多数程序员来说，这是他们可以轻松抽象掉的内容。
- en: Abstraction has been a trend in modern programming language, where the programmer
    is further and further removed from the underlying hardware. While this can lead
    to higher levels of productivity, as problems can be specified at a very high
    level, it relies hugely on clever compilers to implement these abstractions into
    a level understood by the hardware. While this is great in theory, the reality
    can be somewhat less than the marketing dream. I’m sure in the decades to come
    we’ll see huge improvements in compilers and languages such that they will take
    advantage of parallel hardware automatically. However, until this point, and certainly
    until we get there, the need to understand how the hardware functions will be
    key to extracting the best performance from any platform.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象化已成为现代编程语言的趋势，程序员与底层硬件的距离越来越远。尽管这可以提高生产力，因为问题可以在非常高的层次上指定，但它极度依赖于聪明的编译器，将这些抽象实现为硬件能够理解的层次。尽管理论上很美好，但现实可能稍逊于市场营销所宣扬的理想。我相信，在未来几十年中，我们将看到编译器和语言的巨大改进，使它们能够自动利用并行硬件。然而，在这一点到来之前，尤其是直到我们到达那一步，理解硬件如何运作将是从任何平台中提取最佳性能的关键。
- en: For real performance on a CPU-based system, you need to understand how the cache
    works. We’ll look at this on the CPU side and then look at the similarities with
    the GPU. The idea of a cache is that most programs execute in a serial fashion,
    with various looping constructs, in terms of their execution flow. If the program
    calls a function, the chances are the program will call it again soon. If the
    program accesses a particular memory location, the chances are most programs will
    access that same location again within a short time period. This is the principle
    of *temporal locality*, that it is highly likely that you will reuse data and
    reexecute the same code having used/executed it once already.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现CPU系统的真正性能，您需要理解缓存的工作原理。我们将在CPU方面进行探讨，然后看看与GPU的相似之处。缓存的概念是，大多数程序以串行方式执行，具有各种循环结构，就执行流程而言。如果程序调用了一个函数，那么程序很可能会很快再次调用它。如果程序访问了特定的内存位置，那么大多数程序也很可能在短时间内再次访问该位置。这就是*时间局部性*的原则，即您很可能会重用数据，并在已经使用/执行过后重新执行相同的代码。
- en: Fetching data from DRAM, the main memory of a computer system is very slow.
    DRAM has historically always been very slow compared to processor clock speeds.
    As processor clock speeds have increased, DRAM speeds have fallen further and
    further behind.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从DRAM（计算机系统的主内存）中获取数据非常缓慢。与处理器时钟速度相比，DRAM历来一直非常慢。随着处理器时钟速度的提高，DRAM的速度则越来越滞后。
- en: DDR-3 DRAM in today’s processors runs up to 1.6 Ghz as standard, although this
    can be pushed to up to 2.6 Ghz with certain high speed modules and the correct
    processor. However, each of the CPU cores is typically running at around 3 GHz.
    Without a cache to provide quick access to areas of memory, the bandwidth of the
    DRAM will be insufficient for the CPU. As both code and data exist in the DRAM
    space, the CPU is effectively instruction throughput limited (how many instructions
    it executes in a given timeframe) if it cannot fetch either the program or data
    from the DRAM fast enough.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天处理器中的DDR-3 DRAM的标准运行频率为1.6 GHz，尽管使用某些高速模块和正确的处理器时，频率可以提升至2.6 GHz。然而，通常每个CPU核心的运行频率大约为3
    GHz。如果没有缓存提供快速访问内存区域的能力，DRAM的带宽将无法满足CPU的需求。由于代码和数据都存在于DRAM空间中，如果CPU无法从DRAM中快速提取程序或数据，它实际上会受到指令吞吐量的限制（即在给定时间内执行的指令数量）。
- en: This is the concept of *memory bandwidth*, the amount of data we can read or
    store to DRAM in a given period of time. However, there is another important concept,
    *latency*. Latency is the amount of time it takes to respond to a fetch request.
    This can be hundreds of processor cycles. If the program wants four elements from
    memory it makes sense therefore to issue all requests together and then wait for
    them to arrive, rather than issue one request, wait until it arrives, issue the
    next request, wait, and so on. Without a cache, a processor would be very much
    memory bandwidth *and* latency limited.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*内存带宽*的概念，即我们在给定时间内能够读取或存储到DRAM的数据量。然而，还有一个重要的概念——*延迟*。延迟是指响应取数请求所需的时间。这可能是数百个处理器周期。如果程序需要从内存中获取四个元素，那么发出所有请求并等待它们到达是有意义的，而不是一个一个请求，等待每个请求的结果，再发出下一个请求，如此循环。如果没有缓存，处理器的性能将受到内存带宽*和*延迟的限制。
- en: To think of bandwidth and latency in everyday terms, imagine a supermarket checkout
    process. There are *N* checkouts available in a given store, not all of which
    may be staffed. With only two checkouts active (staffed), a big queue will form
    behind them as the customers back up, having to wait to pay for their shopping.
    The throughput or bandwidth is the number of customers processed in a given time
    period (e.g., one minute). The time the customer has to wait in the queue is a
    measure of the latency, that is, how long after joining the queue did the customer
    wait to pay for his or her shopping and leave.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用日常术语理解带宽和延迟，想象一个超市结账过程。在某个商店里有*N*个结账点，并非所有结账点都有人值守。当只有两个结账点在使用（有人值守）时，顾客排成长龙，等着支付购物款项。吞吐量或带宽是指在给定时间内处理的顾客数量（例如一分钟）。顾客在队列中等待的时间则是延迟的衡量标准，即顾客加入队列后，等待多长时间才得以支付并离开。
- en: As the queue becomes large, the shop owner may open more checkout points and
    the queue disperses between the new checkout points and the old ones. With two
    new checkout points opened, the bandwidth of the checkout area is doubled, because
    now twice as many people can be served in the same time period. The latency is
    also halved, because, on average, the queue is only half as big and everyone therefore
    waits only half the time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着队列变大，店主可能会开设更多的结账点，队列就会在新旧结账点之间分散。随着两个新的结账点的开设，结账区的带宽翻倍，因为现在有两倍的人可以在相同的时间内得到服务。延迟也减半，因为平均而言，队列只有原来的一半大小，因此每个人的等待时间也只有原来的一半。
- en: However, this does not come for free. It costs money to employ more checkout
    assistants and more of the retail space has to be allocated to checkout points
    rather than shelf space for products. The same tradeoff occurs in processor design,
    in terms of the memory bus width and the clock rate of the memory devices. There
    is only so much silicon space on the device and often the width of the external
    memory bus is limited by the number of physical pins on the processor.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是免费的。雇佣更多的结账助理需要花费资金，同时需要将更多的零售空间分配给结账点，而不是用于产品的货架空间。在处理器设计中也存在类似的权衡，涉及到内存总线的宽度和内存设备的时钟频率。设备上的硅空间是有限的，通常外部内存总线的宽度受到处理器上物理引脚数量的限制。
- en: One other concept we also need to think about is *transaction overhead*. There
    is a certain overhead in processing the payment for every customer. Some may have
    two or three items in a basket while others may have overflowing shopping carts.
    The shop owners love the shopping cart shoppers because they can be processed
    efficiently, that is, more of the checkout person’s time is spent checking out
    groceries, rather than in the overhead of processing the payment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们需要考虑的概念是*事务开销*。每个顾客支付时都会有一定的开销。有些人可能只有两三件商品，而另一些人可能购物车里满满当当。店主们喜欢购物车里的顾客，因为他们能更高效地处理，即结账人员的时间更多地用于结账商品，而不是处理支付的开销。
- en: We see the same in GPUs. Some memory transactions are lightweight compared to
    the fixed overhead to process them. The number of memory cells fetched relative
    to the overhead time is low, or, in other words, the percentage of peak efficiency
    is poor. Others are large and take a bunch of time to serve, but can be serviced
    efficiently and achieve near peak memory transfer rates. These translate to byte-based
    memory transactions at one end of the spectrum and to long word-based transactions
    at the other end. To achieve peak memory efficiency, we need lots of large transactions
    and very few, if any, small ones.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 GPU 中也看到了类似的情况。一些内存事务相比于处理它们所需的固定开销来说非常轻量。相对于开销时间，获取的内存单元数量较少，换句话说，峰值效率的百分比较低。其他的则较大，需要花费大量时间来处理，但能够高效地服务并接近峰值内存传输速率。这些事务在一个极端表现为基于字节的内存事务，而在另一个极端则表现为基于长字的事务。为了达到峰值内存效率，我们需要大量的大事务，并且尽量避免小事务，或者说很少出现小事务。
- en: Caches
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: A cache is a high-speed memory bank that is physically close to the processor
    core. Caches are expensive in terms of silicon real estate, which in turn translates
    into bigger chips, lower yields, and more expensive processors. Thus, the Intel
    Xeon chips with the huge L3 caches found in a lot of server machines are far more
    expensive to manufacture than the desktop version that has less cache on the processor
    die.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是与处理器核心物理接近的高速内存单元。缓存在硅面积上非常昂贵，这也意味着更大的芯片、更低的良率和更贵的处理器。因此，许多服务器机器中的英特尔 Xeon
    处理器，配备了巨大的 L3 缓存，其制造成本远高于桌面版本的处理器，后者在处理器芯片上配备较少的缓存。
- en: The maximum speed of a cache is proportional to the size of the cache. The L1
    cache is the fastest, but is limited in size to usually around 16 K, 32 K, or
    64 K. It is usually allocated to a single CPU core. The L2 cache is slower, but
    much larger, typically 256 K to 512 K. The L3 cache may or may not be present
    and is often several megabytes in size. The L2 and/or L3 cache may be shared between
    processor cores or maintained as separate caches linked directly to given processor
    cores. Generally, at least the L3 cache is a shared cache between processor cores
    on a conventional CPU. This allows for fast intercore communication via this shared
    memory within the device.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的最大速度与缓存的大小成正比。L1 缓存是最快的，但其大小通常限制在 16K、32K 或 64K 左右，通常分配给单个 CPU 核心。L2 缓存较慢，但大得多，通常为
    256K 到 512K。L3 缓存可能存在，也可能不存在，通常其大小为几兆字节。L2 和/或 L3 缓存可能是多个处理器核心共享的缓存，或者是分别维持与指定核心直接连接的独立缓存。通常，至少
    L3 缓存是多个处理器核心之间共享的缓存。这允许设备内核心间通过共享内存进行快速通信。
- en: The G80 and GT200 series GPUs have no equivalent CPU-like cache to speak of.
    They do, however, have a hardware-managed cache that behaves largely like a read-only
    CPU cache in terms of constant and texture memory. The GPU relies instead primarily
    on a programmer-managed cache, or shared memory section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: G80 和 GT200 系列 GPU 没有类似 CPU 的缓存。它们确实有一个硬件管理的缓存，在常量和纹理内存方面的行为类似于只读 CPU 缓存。GPU
    主要依赖于程序员管理的缓存或共享内存部分。
- en: The Fermi GPU implementation was the first to introduce the concept of a nonprogrammer-managed
    data cache. The architecture additionally has, per SM, an L1 cache that is both
    programmer managed and hardware managed. It also has a shared L2 cache across
    all SMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi GPU 实现首次引入了非程序员管理的数据缓存的概念。该架构每个 SM 都有一个既可以由程序员管理又可以由硬件管理的 L1 缓存。它还在所有
    SM 之间共享一个 L2 缓存。
- en: So does it matter if the cache is shared across processor cores or SMs? Why
    is this arrangement relevant? This has an interesting implication for communicating
    with other devices using the same shared cache. It allows interprocessor communication,
    without having to go all the way out to global memory. This is particularly useful
    for atomic operations where, because the L2 cache is unified, all SMs see a consistent
    version of the value at a given memory location. The processor does not have to
    write to the slow global memory, to read it back again, just to ensure consistency
    between processor
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，缓存是否需要跨处理器核心或 SM 共享呢？这种安排有何意义？这对于使用相同共享缓存与其他设备通信有着有趣的影响。它允许处理器之间的通信，无需完全访问全局内存。这对于原子操作尤其有用，因为
    L2 缓存是统一的，所有 SM 可以看到给定内存位置的值的最新版本。处理器无需写入慢速的全局内存再读取回来，以确保处理器之间的一致性。
- en: '![image](../images/F000065f06-01-9780124159334.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-01-9780124159334.jpg)'
- en: Figure 6.1 SM L1/L2 data path.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 SM L1/L2 数据路径。
- en: cores. On G80/GT200 series hardware, where there is no unified cache, we see
    exactly this deficiency and consequently quite slow atomic operations compared
    with Fermi and later hardware.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 核心。在G80/GT200系列硬件上，由于没有统一缓存，我们正是看到这种不足，导致与Fermi及之后的硬件相比，原子操作相对较慢。
- en: Caches are useful for most programs. Significant numbers of programmers either
    care little for or have a limited understanding of how to achieve good performance
    in software. Introducing a cache means most programs work reasonably well and
    the programmer does not have to care too much about how the hardware works. This
    ease of programming is useful for initial development, but in most cases you can
    do somewhat better.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存对大多数程序是有用的。许多程序员对如何在软件中实现良好的性能并不关心，或者对这方面的理解有限。引入缓存意味着大多数程序能合理地运行，程序员不需要过多关心硬件是如何工作的。这种编程的简便性对于初期开发很有帮助，但在大多数情况下，你可以做得更好。
- en: The difference between a novice CUDA programmer and someone who is an expert
    can be up to an order of magnitude. I hope that through reading this book, you’ll
    be able to get several times speedup from your existing code and move toward being
    routinely able to write CUDA code, which significantly outperforms the equivalent
    serial code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 新手CUDA程序员与专家之间的差距可以达到一个数量级。我希望通过阅读本书，你能够从现有的代码中获得数倍的加速，并逐步能够编写CUDA代码，这些代码能显著超越等效的串行代码。
- en: Types of data storage
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据存储的类型
- en: On a GPU, we have a number of levels of areas where you can place data, each
    defined by its potential bandwidth and latency, as shown in [Table 6.1](#T0010).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上，我们有多个层级的数据存储区域，每个区域都有其潜在的带宽和延迟，如[表6.1](#T0010)所示。
- en: Table 6.1 Access Time by Memory Type
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 按内存类型划分的访问时间
- en: '![Image](../images/T000065tabT0010.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0010.jpg)'
- en: At the highest and most preferred level are registers inside the device. Then
    we have shared memory, effectively a programmer-managed L1 cache, constant memory,
    texture memory, regular device memory, and finally host memory. Notice how the
    order of magnitude changes between the slowest and fastest type of storage. We
    will now look at the usage of each of these in turn and how you can maximize the
    gain from using each type.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高且最优先级的层级是设备内部的寄存器。接下来是共享内存，实际上是程序员管理的L1缓存，再是常量内存、纹理内存、常规设备内存，最后是主机内存。注意，存储类型之间的速度差异是多么显著。接下来我们将依次讨论每种内存类型的使用，并介绍如何最大化每种类型的使用收益。
- en: Traditionally, most texts would start off by looking at global memory, as this
    often plays a key role in performance. If you get the global memory pattern wrong
    then you can forget anything else until you get the correct pattern. We take a
    different approach here, in that we look first at how to use the device efficiently
    internally, and from there move out toward global and host memory. Thus, you will
    understand efficiency at each level and have an idea of how to extract it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，大多数文献会从查看全局内存开始，因为全局内存通常在性能中扮演关键角色。如果你搞错了全局内存的访问模式，那就可以忘记其他的内容，直到你纠正了这个模式。我们在这里采取不同的方法，首先看如何高效地利用设备内部的资源，然后再向外扩展到全局内存和主机内存。这样，你可以理解每个层级的效率，并知道如何提取这些效率。
- en: Most CUDA programs are developed progressively, using global memory exclusively
    at least initially. Once there is an initial implementation, then the use of other
    memory types such as zero copy and shared, constant, and ultimately registers
    is considered. For an optimal program, you need to be thinking about these issues
    while you are developing a program. Thus, instead of the faster memory types being
    an afterthought, they are considered at the outset and you know exactly where
    and how to improve the program. You should be continuously thinking about not
    only how to access global memory efficiently, but also how those accesses, especially
    for data that is reused in some way, can be eliminated.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数CUDA程序是逐步开发的，至少最初是专门使用全局内存。一旦有了初步的实现，就会考虑使用其他类型的内存，如零拷贝内存、共享内存、常量内存，最终是寄存器。为了获得最佳的程序，你需要在开发过程中就考虑这些问题。因此，快速的内存类型不是事后考虑的，而是在一开始就被考虑进去，你也清楚在哪里以及如何改进程序。你应该持续思考的不仅是如何高效访问全局内存，还包括如何消除那些访问，特别是对于以某种方式被重复使用的数据。
- en: Register Usage
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寄存器使用
- en: The GPU, unlike its CPU cousin, has thousands of registers per SM (streaming
    multiprocessor). An SM can be thought of like a multithreaded CPU core. On a typical
    CPU we have two, four, six, or eight cores. On a GPU we have *N* SM cores. On
    a Fermi GF100 series, there are 16 SMs on the top-end device. The GT200 series
    has up to 32 SMs per device. The G80 series has up to 16 SMs per device.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CPU 不同，GPU 每个 SM（流处理器）有成千上万的寄存器。可以将 SM 看作是一个多线程的 CPU 核心。在典型的 CPU 中，我们有两个、四个、六个或八个核心。而在
    GPU 中，我们有 *N* 个 SM 核心。在费米 GF100 系列中，顶级设备有 16 个 SM。GT200 系列每个设备最多有 32 个 SM。G80
    系列每个设备最多有 16 个 SM。
- en: It may seem strange that Fermi has less SMs than its predecessors. This is until
    you realize that each Fermi SM contains more SPs (streaming processors) and that
    it is these that do the “grunt” work. Due to the different number of SPs per core,
    you see a major difference in the number of threads per core. A typical CPU will
    support one or two hardware threads per core. A GPU by contrast has between 8
    and 192 SPs per core, meaning each SM can at any time be executing this number
    of concurrent hardware threads.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，费米架构的 SM 数量比其前任少，这似乎很奇怪。直到你意识到每个费米 SM 包含更多的 SP（流处理器），而正是这些处理器承担了“大部分”工作。由于每个核心的
    SP 数量不同，你会看到每个核心的线程数量存在显著差异。典型的 CPU 每个核心支持一个或两个硬件线程。相比之下，GPU 每个核心有 8 到 192 个 SP，这意味着每个
    SM 在任何时候都可以执行这一数量的并发硬件线程。
- en: In practice on GPUs, application threads are pipelined, context switched, and
    dispatched to multiple SMs, meaning the number of active threads across all SMs
    in a GPU device is usually in the tens of thousands range.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 中，应用线程是流水线处理的，通过上下文切换，并被分派到多个 SM，这意味着在 GPU 设备的所有 SM 上，活跃线程的数量通常在几万个数量级。
- en: One major difference we see between CPU and GPU architectures is how CPUs and
    GPUs map registers. The CPU runs lots of threads by using register renaming and
    the stack. To run a new task the CPU needs to do a context switch, which involves
    storing the state of all registers onto the stack (the system memory) and then
    restoring the state from the last run of the new thread. This can take several
    hundred CPU cycles. If you load too many threads onto a CPU it will spend all
    of the time simply swapping out and in registers as it context switches. The effective
    throughput of *useful* work rapidly drops off as soon as you load too many threads
    onto a CPU.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 CPU 和 GPU 架构之间看到的一个主要区别是它们如何映射寄存器。CPU 通过寄存器重命名和堆栈来运行大量线程。为了运行新任务，CPU 需要进行上下文切换，这涉及将所有寄存器的状态存储到堆栈（系统内存）中，然后从上次运行的线程恢复状态。这可能需要几百个
    CPU 周期。如果你将太多线程加载到 CPU 上，它将花费大量时间仅仅是在进行上下文切换时交换寄存器。加载过多线程后，*有效*工作吞吐量会迅速下降。
- en: The GPU by contrast is the exact opposite. It uses threads to hide memory fetch
    and instruction execution latency, so too few threads on the GPU means the GPU
    will become idle, usually waiting on memory transactions. The GPU also does not
    use register renaming, but instead dedicates real registers to each and every
    thread. Thus, when a context switch is required, it has near zero overhead. All
    that happens on a context switch is the selector (or pointer) to the current register
    set is updated to point to the register set of the next warp that will execute.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，GPU 则恰恰相反。它利用线程来隐藏内存访问和指令执行的延迟，因此在 GPU 上线程过少意味着 GPU 会处于空闲状态，通常是在等待内存事务。GPU
    也不使用寄存器重命名，而是将真实寄存器分配给每一个线程。因此，当需要进行上下文切换时，它几乎没有任何开销。上下文切换时发生的唯一事情就是更新当前寄存器集的选择器（或指针），使其指向下一个将执行的
    warp 的寄存器集。
- en: Notice I used the concept of warps here, which was covered in detail in the
    [Chapter 5](CHP005.html) on threading. A warp is simply a grouping of threads
    that are scheduled together. In the current hardware, this is 32 threads. Thus,
    we swap in or swap out, and schedule, groups of 32 threads within a single SM.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我在这里使用了“warp”这一概念，详细内容请参考[第 5 章](CHP005.html)中的线程部分。Warp 只是将线程按组调度在一起。在当前硬件中，warp
    包含 32 个线程。因此，我们会在单个 SM 中交换或调度 32 个线程的组。
- en: Each SM can schedule a number of blocks. Blocks at the SM level are simply logical
    groups of independent warps. The number of registers per kernel thread is calculated
    at compile time. All blocks are of the same size and have a known number of threads,
    and the register usage per block is known and fixed. Consequently, the GPU can
    allocate a fixed set of registers for each block scheduled onto the hardware.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个SM可以调度多个块。SM级别的块仅仅是独立warp的逻辑组。每个内核线程所需的寄存器数在编译时计算。所有的块大小相同，并且有已知数量的线程，每个块的寄存器使用量是已知并固定的。因此，GPU可以为每个调度到硬件上的块分配固定数量的寄存器。
- en: At a thread level, this is transparent to the programmer. However, a kernel
    that requests too many registers per thread can limit the number of blocks the
    GPU can schedule on an SM, and thus the total number of threads that will be run.
    Too few threads and you start underutilizing the hardware and the performance
    starts to rapidly drop off. Too many threads can mean you run short of resources
    and whole blocks of threads are dropped from being scheduled to the SM.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在线程级别，这对程序员是透明的。然而，一个请求每个线程过多寄存器的内核可能会限制GPU在SM上调度的块数量，从而限制运行的线程总数。线程太少会导致硬件利用不足，性能会迅速下降。线程太多则可能导致资源不足，整个线程块将被从调度队列中移除，无法被调度到SM。
- en: Be careful of this effect, as it can cause sudden performance drops in the application.
    If previously the application was using four blocks and now it uses more registers,
    causing only three blocks to be available, you may well see a one-quarter drop
    in GPU throughput. You can see this type of problem with various profiling tools
    available, covered in [Chapter 7](CHP007.html) in the profiling section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个效果，因为它可能导致应用程序突然性能下降。如果之前应用程序使用了四个块，现在它使用了更多的寄存器，导致只剩下三个块可用，你可能会看到GPU吞吐量下降四分之一。你可以使用各种性能分析工具来查看此类问题，具体内容可以参见[第7章](CHP007.html)的性能分析部分。
- en: Depending on the particular hardware you are using, there is 8 K, 16 K, 32 K
    or 64 K of register space per SM for *all threads* within an SM. You need to remember
    that one register is required *per thread*. Thus, a simple local float variable
    in C results in *N* registers usage, where *N* is the number of threads that are
    scheduled. With the Fermi-level hardware, you get 32 K of register space per SM.
    With 256 threads per block, you would have ((32,768/4 bytes per register)/256
    threads) = 32 registers per thread available. To achieve the maximum number of
    registers available on Fermi, 64 (128 on G80/GT200), you’d need to half the thread
    count to just 128 threads. You could have a single block per SM, with the maximum
    permitted number of registers in that block. Equally, you could have eight blocks
    of 32 threads (8 × 32 = 256 threads in total), each using the maximum number of
    registers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你所使用的具体硬件，每个SM有8K、16K、32K或64K的寄存器空间供*所有线程*使用。你需要记住，每个线程需要*一个寄存器*。因此，在C语言中，简单的局部浮动变量会使用*N*个寄存器，其中*N*是调度的线程数。对于Fermi级硬件，每个SM有32K的寄存器空间。每个块256个线程时，你将得到（（32,768字节/每个寄存器4字节）/256个线程）=
    每个线程32个寄存器可用。为了达到Fermi上最大可用寄存器数64（G80/GT200上是128），你需要将线程数减少一半，只保留128个线程。你可以为每个SM分配一个单独的块，并在该块中使用最大允许的寄存器数量。同样，你也可以拥有八个由32个线程组成的块（8
    × 32 = 256个线程总数），每个块使用最大数量的寄存器。
- en: If you can make use of the maximum number of registers, for example, using them
    to work on a section of an array, then this approach can work quite well. It works
    because such a set of values is usually *N* elements from a dataset. If each element
    is independent, you can create instruction-level parallelism (ILP) within a single
    thread. This is exploited by the hardware in terms of pipelining many independent
    instructions. You’ll see later an example of this working in practice.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够利用最大数量的寄存器，例如，利用它们来处理数组的某一部分，那么这种方法可以非常有效。它之所以有效，是因为这样的值集通常是数据集中*N*个独立的元素。如果每个元素是独立的，你可以在单个线程中创建指令级并行性（ILP）。硬件会通过流水线处理许多独立指令来利用这一点。稍后你会看到这个方法在实践中的示例。
- en: However, for most kernels, the number of registers required is somewhat lower.
    If you drop your register requirements from 128 to 64, you can schedule another
    block into the same SM. For example, with 32 registers, you can schedule four
    blocks. In doing so, you are increasing the total thread count. On Fermi, you
    can have up to 1536 threads per SM and, for the general case, the higher the level
    of occupancy you can achieve, the faster your program will execute. You will reach
    a point where you have enough thread-level parallelism (TLP) to hide the memory
    latency. To continue to increase performance further, you either need to move
    to larger memory transactions or introduce ILP, that is, process more than one
    element of the dataset within a single thread.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于大多数内核，所需的寄存器数量相对较少。如果你将寄存器需求从128降低到64，你可以将另一个块调度到同一个SM中。例如，使用32个寄存器，你可以调度四个块。通过这样做，你实际上是在增加线程总数。在Fermi架构上，每个SM最多可以有1536个线程，通常情况下，越高的占用率，程序执行速度越快。你会达到一个点，拥有足够的线程级并行性（TLP）来隐藏内存延迟。要进一步提高性能，你需要采用更大的内存事务或引入ILP（指令级并行性），即在单个线程中处理数据集的多个元素。
- en: There is, however, a limit on the number of warps that can be scheduled to an
    SM. Thus, dropping the number of registers from 32 to 16 does not get eight blocks.
    For that we are limited to 192 threads, as shown in [Table 6.2](#T0015).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个SM可以调度的warp数量是有限制的。因此，将寄存器数量从32个减少到16个并不会得到八个块。我们受到192个线程的限制，如[表6.2](#T0015)所示。
- en: Table 6.2 Register Availability by Thread Usage on Fermi
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 Fermi架构按线程使用的寄存器可用性
- en: '![Image](../images/T000065tabT0015.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0015.jpg)'
- en: '[Table 6.2](#T0015) refers to the Fermi architecture. For the Kepler architecture,
    simply double the number of registers and blocks shown here. We’ve used 192 and
    256 threads here as they provide good utilization of the hardware. Notice that
    the kernel usage of 16 versus 20 registers does not introduce any additional blocks
    to the SM. This is due to the limit on the number of warps that can be allocated
    to an SM. So in this case, you can easily increase register usage without impacting
    the total number of threads that are running on a given SM.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.2](#T0015)涉及Fermi架构。对于Kepler架构，只需将这里显示的寄存器和块的数量加倍。我们使用了192和256个线程，因为它们能够较好地利用硬件。请注意，内核使用16个与20个寄存器并不会引入额外的块到SM中。这是因为每个SM可分配的warp数量有限。因此，在这种情况下，你可以轻松增加寄存器的使用而不影响在给定SM上运行的线程总数。'
- en: 'You want to use registers to avoid usage of the slower memory types, but you
    have to be careful that you use them effectively. For example, suppose we had
    a loop that set each bit in turn, depending on the value of some Boolean variable.
    Effectively, we’d be packing and unpacking 32 Booleans into 32 bits of a word.
    We could write this as a loop, each time modifying the memory location by the
    new Boolean, shifted to the correct position within the word, as shown in the
    following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用寄存器来避免使用较慢的内存类型，但必须小心确保有效利用它们。例如，假设我们有一个循环，每次根据某个布尔变量的值设置每一位。实际上，我们将把32个布尔值打包到一个字的32位中。我们可以将其写成一个循环，每次通过将新的布尔值移到字中的正确位置来修改内存位置，如下所示：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we are reading array element `i` from an array of elements to pack into
    an integer, `packed_result`. We’re left shifting the Boolean by the necessary
    number of bits and then using a bitwise `or` operation with the previous result.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们从一个数组中读取元素`i`，将其打包成一个整数`packed_result`。我们将布尔值按必要的位数左移，然后使用按位`或`运算符与先前的结果进行合并。
- en: If the parameter `packed_result` exists in memory, you’d be doing 32 memory
    read and writes. We could equally place the parameter `packed_result` in a local
    variable, which in turn the compiler would place into a register. As we accumulate
    into the register instead of in main memory, and later write only the *result*
    to main memory, we save 31 of the 32 memory reads and writes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数`packed_result`存在于内存中，那么你将进行32次内存读写操作。我们也可以将参数`packed_result`放入一个局部变量中，编译器将其放入寄存器中。由于我们将数据累积到寄存器中，而不是主内存中，然后只将*结果*写入主内存，因此可以节省32次内存读写中的31次。
- en: Looking back at [Table 6.1](#T0010), you can see it takes several hundred cycles
    to do a global memory operation. Let’s assume 500 cycles for one global memory
    read or write operation. For every value you’d need to read, apply the `or` operation,
    and write the result back. Therefore, you’d have 32 × read + 32 × write = 64 ×
    500 cycles = 32,000 cycles. The register version would eliminate 31 read and 32
    write operations, replacing the 500-cycle operations with single-cycle operations.
    Thus, you’d have
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下[表6.1](#T0010)，你可以看到进行全局内存操作需要几百个周期。假设一次全局内存读写操作需要500个周期。对于每个值，你需要读取、应用`or`操作，然后将结果写回。因此，你将需要32次读取
    + 32次写入 = 64 × 500周期 = 32,000周期。寄存器版本将消除31次读取和32次写入操作，将500周期的操作替换为单周期操作。因此，你将得到
- en: '![image](../images/F000065si1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065si1.png)'
- en: '![image](../images/F000065si2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065si2.png)'
- en: Clearly, this is a huge reduction in the number of cycles. We have a 31 times
    improvement to perform a relatively common operation in certain problem domains.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这在周期数上是一个巨大的减少。我们在某些问题领域中执行相对常见的操作时，性能提升了31倍。
- en: We see similar relationships with common reduction operations like `sum`, `min`,
    `max`, etc. A reduction operation is where a dataset is reduced by some function
    to a smaller set, typically a single item. Thus, `max (10, 12, 1, 4, 5)` would
    return a single value, 12, the maximum of the given dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到类似的关系，常见的归约操作如`sum`、`min`、`max`等也是如此。归约操作是通过某个函数将数据集缩减为更小的集合，通常是单个项。因此，`max
    (10, 12, 1, 4, 5)`将返回一个单一的值12，即给定数据集中的最大值。
- en: Accumulating into a register saves huge numbers of memory writes. In our bit
    packing example, we reduce our memory writes by a factor of 31\. Whether you are
    using a CPU or GPU, this type of register optimization will make a huge difference
    in the speed of execution of your programs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据累积到寄存器中节省了大量的内存写入。在我们的位打包示例中，我们将内存写入减少了31倍。无论你是使用CPU还是GPU，这种寄存器优化都将极大提高程序的执行速度。
- en: However, this burdens the programmer with having to think about which parameters
    are in registers and which are in memory, which registers need to be copied back
    to memory, etc. This might seem like quite a bit of trouble to go to, and for
    the average programmer, often it is. Therefore, we see a proliferation of code
    that works directly on memory. For the most part, cache memory you find on CPUs
    significantly masks this problem. The accumulated value is typically held in the
    L1 cache. If a write-back policy is used on the cache, where the values do not
    need to be written out to main memory until later, the performance is not too
    bad. Note that the L1 cache is still slower than registers, so the solution will
    be suboptimal and may be several times slower than it could be.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这给程序员带来了负担，需要考虑哪些参数在寄存器中，哪些在内存中，哪些寄存器需要复制回内存等。这似乎是相当麻烦的事，对于普通程序员来说，通常也是如此。因此，我们看到大量的代码直接操作内存。大多数情况下，CPU上的缓存内存显著掩盖了这个问题。累积的值通常保存在L1缓存中。如果缓存使用写回策略，其中值不需要立即写回主内存，而是在稍后进行，这样性能还算可以。需要注意的是，L1缓存仍然比寄存器慢，因此这种解决方案仍然是次优的，可能比实际应该有的速度慢几倍。
- en: Some compilers may detect such inefficiencies and implement a load into a register
    during the optimizer phase. Others may not. Relying on the optimizer to fix poor
    programming puts you at the mercy of how good the compiler is, or is not. You
    may find that, as the optimization level is increased, errors creep into the program.
    This may not be the fault of the compiler. The C language definition is quite
    complex. As the optimization level is increased, subtle bugs may appear due to
    a missed volatile qualifier or the like. Automatic test scripts and back-to-back
    testing against a nonoptimized version are good solutions to ensure correctness.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一些编译器可能会检测到这样的低效，并在优化阶段实现将数据加载到寄存器中。其他编译器则可能不会。依赖编译器的优化来修复糟糕的编程可能会让你受到编译器优化质量的制约。你可能会发现，随着优化级别的提高，程序中会出现错误。这可能并不是编译器的错。C语言的定义相当复杂。随着优化级别的提升，可能会因为漏掉`volatile`关键字等原因，出现细微的错误。自动化测试脚本和与未优化版本进行对比测试是确保正确性的一种有效解决方案。
- en: You should also be aware that optimizing compiler vendors don’t always choose
    to implement the best solution. If just 1% of programs fail when a certain optimization
    strategy is employed by the compiler vendor, then it’s unlikely to be employed
    due to the support issues this may generate.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该意识到，优化编译器的供应商并不总是选择实现最佳的解决方案。如果某个优化策略被编译器供应商采用后，仅有1%的程序会失败，那么由于可能产生的支持问题，该策略不太可能被采用。
- en: The GPU has a computation rate many times in excess of its memory bandwidth
    capacity. The Fermi hardware has around 190 GB/s peak bandwidth to memory, with
    a peak compute performance of over one teraflop. This is over five times the memory
    bandwidth. On the Kepler GTX680/Tesla K10 the compute power increases to 3 Teraflops,
    yet with a memory bandwidth almost identical to the GTX580\. In the bit packing
    example, without register optimization and on a system with no cache, you would
    require one read and one write per loop iteration. Each integer or floating-point
    value is 4 bytes in length. The best possible performance we could, theoretically,
    achieve in this example, due to the need to read and write a total of 8 bytes,
    would be one-eighth of the memory bandwidth. Using the 190 GB/s figure, this would
    equate to around 25 billion operations per second.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GPU的计算速率远超其内存带宽能力。Fermi硬件的内存峰值带宽约为190 GB/s，计算性能峰值超过1 teraflop。这是内存带宽的五倍以上。在Kepler
    GTX680/Tesla K10上，计算能力提升到3 teraflops，但内存带宽几乎与GTX580相同。在位打包示例中，如果没有寄存器优化，并且系统没有缓存，你每次循环迭代将需要一次读取和一次写入。每个整数或浮点数值的大小为4字节。由于需要读取和写入总共8字节，在理论上，我们在这个示例中能够实现的最佳性能是内存带宽的八分之一。以190
    GB/s为例，这相当于每秒约250亿次操作。
- en: In practice you’d never get near this, because there are loop indexes and iterations
    to take into account as well as simply the raw memory bandwidth. However, this
    sort of back-of-the-envelope calculation provides you with some idea of the upper
    bounds of your application before you start coding anything.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，你永远无法接近这个数值，因为需要考虑循环索引和迭代次数，以及原始内存带宽。然而，这种背面的计算方式在你开始编写任何代码之前，能够为你提供一些关于应用程序上限的概念。
- en: Applying our factor of 31 reductions to the number of memory operations allows
    you to achieve a theoretical peak of 31 times this figure, some 775 billion iterations
    per second. We’ll in practice hit other limits, within the device. However, you
    can see we’d easily achieve many times better performance than a simple global
    memory version by simply accumulating to or making use of registers wherever possible.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的31倍减少因子应用到内存操作次数上，可以实现理论上的峰值性能，达到此数值的31倍，即每秒大约775亿次迭代。实际上，我们会遇到设备内的其他限制。然而，你可以看到，通过在可能的地方积累或利用寄存器，我们能够轻松地实现比简单的全局内存版本好得多的性能。
- en: 'To get some real figures here, we’ll write a program to do this bit packing
    on global memory and then with registers. The results are as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一些真实的数据，我们将编写一个程序，分别在全局内存和寄存器上进行位打包操作。结果如下：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The two kernels to generate these are as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 生成这些结果的两个内核如下：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`  }`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`  }`'
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The only difference in the two kernels is that one uses a global variable, `d_tmp`,
    while the other uses a local register. Looking at the results you can see the
    speedups in [Table 6.3](#T0020). You see an average speedup of 7.7 times. Perhaps,
    most surprisingly, the fastest speedup comes from the devices that have the largest
    number of SMs, which points to a problem that I hope you may have spotted. In
    the global memory version of the kernel, every thread from every block reads and
    writes to `d_tmp`. There is no guarantee as to in which order this will happen,
    so the program’s output is indeterminate. The kernel executes perfectly well,
    with no CUDA errors detected, yet the answer will always be nonsense. This type
    of error is a remarkably common type of mistake when converting serial code to
    parallel code.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个内核的唯一区别在于一个使用了全局变量`d_tmp`，另一个使用了本地寄存器。查看结果，你可以看到表6.3中的加速比。你会看到平均加速比为7.7倍。也许最令人惊讶的是，最快的加速来自于那些SM数量最多的设备，这提示了一个我希望你已经发现的问题。在全局内存版本的内核中，每个线程来自每个块都要读取和写入`d_tmp`。无法保证这些操作的执行顺序，因此程序的输出是不确定的。该内核执行得非常顺利，且没有检测到CUDA错误，但答案总是无意义的。这种类型的错误是在将串行代码转换为并行代码时，非常常见的错误。
- en: Table 6.3 Speedup Using Registers over GMEM
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 使用寄存器相对于全局内存的加速比
- en: '![Image](../images/T000065tabT0020.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0020.jpg)'
- en: Strange answers should always point you toward something being wrong. So how
    is this issue corrected? In the register version, each thread writes to a unique
    register. In the GMEM (Global Memory) version, it must do the same. Therefore,
    you simply replace the original definition of `d_tmp:`
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的答案总是表明某些地方出了问题。那么如何修正这个问题呢？在寄存器版本中，每个线程写入一个唯一的寄存器。在GMEM（全局内存）版本中，它必须做同样的事情。因此，你只需要替换原始的`d_tmp`定义：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The kernel needs to be updated as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 内核需要更新如下：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now each thread gets to read and write from an independent area of global memory.
    What of the speedup now? See [Table 6.4](#T0025).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个线程可以读取和写入一个独立的全局内存区域。那么现在的加速比如何呢？请参见[表6.4](#T0025)。
- en: Table 6.4 Real Speedup from Using Registers over GMEM
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4 使用寄存器代替GMEM的实际加速比
- en: '![Image](../images/T000065tabT0025.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0025.jpg)'
- en: As you can see from [Table 6.4](#T0025), the average speedup drops to just 1.7
    times. If it were not for the 9800GT (a compute 1.1 device) you’d see the average
    almost hit two times speedup in this simple piece of code. Where possible, you
    always need to avoid global memory writes through some other means. Converging
    on a single memory address, as in the first example, forces the hardware to serialize
    the memory operations, leading to terrible performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从[表6.4](#T0025)中看到的，平均加速比降至仅1.7倍。如果没有9800GT（一个Compute 1.1设备），你会看到在这段简单的代码中，平均加速几乎达到两倍。在可能的情况下，你总是需要通过其他方式避免全局内存写入。像第一个例子那样趋向于单一的内存地址，会迫使硬件串行化内存操作，从而导致糟糕的性能。
- en: Now it’s quite easy to make this code even faster. Loops are typically very
    inefficient, in that they cause branching, which can cause pipeline stalls. More
    importantly, they consume instructions that don’t contribute to the final result.
    The loop code will contain an increment for the loop counter, a test of the end
    loop condition, and a branch for every iteration. In comparison, the useful instructions
    per iteration will load the value from `pack_array`, shift it left *N* bits, and
    `or` it with the existing `d_tmp` value. Just looking at the operations, we see
    50% or so of the operations are based around the loop. You can look directly at
    the following PTX (Parallel Thread eXecution) code to verify this. The instructions
    that perform the loop, to make reading the virtual assembly code easier, are highlighted
    in bold.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让这段代码更快变得相当简单。循环通常效率非常低，因为它们会导致分支，从而可能导致管道停顿。更重要的是，它们会消耗对最终结果没有贡献的指令。循环代码将包含循环计数器的增量、结束循环条件的测试，以及每次迭代的分支。相比之下，每次迭代的有用指令将从`pack_array`加载值，将其左移*N*位，并与现有的`d_tmp`值进行`or`操作。单从操作来看，我们可以看到大约50%的操作是围绕循环展开的。你可以直接查看下面的PTX（并行线程执行）代码来验证这一点。为了便于阅读虚拟汇编代码，执行循环的指令已用粗体标出。
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '` or.b32  %r20, %r18, %r19;`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '` or.b32  %r20, %r18, %r19;`'
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Thus, the PTX code first tests if the `for` loop will actually enter the loop.
    This is done in the block labeled `$LDWbeginblock_180_5`. The code at the `$Lt_0_1794`
    label then performs the loop operation, jumping back to label `$L_0_3330` until
    such time as the loop has completed 32 iterations. The other code in the section
    labeled `$L_0_3330` performs the operation:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PTX代码首先测试`for`循环是否实际会进入循环。这是在标记为`$LDWbeginblock_180_5`的块中完成的。标记为`$Lt_0_1794`的代码然后执行循环操作，跳回标记`$L_0_3330`，直到循环完成32次迭代。标记为`$L_0_3330`的其他代码执行该操作：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice, in addition to the loop overhead, because `packed_array` is indexed
    by a variable the code has to work out the address on every iteration of the loop:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了循环开销外，由于`packed_array`是通过变量索引的，代码必须在每次迭代时计算地址：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is rather inefficient. Compare this to a loop unrolled version and we
    see something quite interesting:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做相当低效。与展开后的版本相比，我们可以看到一些相当有趣的情况：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '` .reg .u64 %rd<6>;`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '` .reg .u64 %rd<6>;`'
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Almost all the instructions now contribute to the result. The loop overhead
    is gone. The address calculation for `packed_array` is reduced to a compile time–resolved
    base plus offset type address. Everything is much simpler, but much longer, both
    in the C code and also in the virtual PTX assembly code.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在几乎所有指令都对结果有所贡献。循环开销已消失。`packed_array`的地址计算简化为一个编译时解决的基地址加偏移量类型的地址。一切变得更加简洁，但在C代码和虚拟PTX汇编代码中都变得更长。
- en: The point here is not to understand PTX, but to see the vast difference small
    changes in C code can have on the virtual assembly generated. It’s to understand
    that techniques like loop unrolling can be hugely beneficial in many cases. We
    look at PTX and how it gets translated in the actual code that gets executed in
    more detail in [Chapter 9](CHP009.html) on optimization.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重点不是理解PTX，而是看到C代码中微小的变化对生成的虚拟汇编代码的巨大影响。它的目的是让你明白像循环展开这样的技术在许多情况下可以带来巨大的好处。我们在[第9章](CHP009.html)的优化部分中会详细讨论PTX以及它是如何在实际执行的代码中转换的。
- en: So what does this do in terms of speedup? See [Table 6.5](#T0030). You can see
    that on the 9800GT or the GTX260, there was no effect at all. However, on the
    more modern compute 2.x hardware, the GTX460 and GTX470, you see a 2.4× and 3.4×
    speedup, respectively. If you look back to the pure GMEM implementation, on the
    GTX470 this is a 6.4× speedup. To put this in perspective, if the original program
    took six and a half hours to run, then the optimized version would take just one
    hour.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这在加速方面有什么效果呢？请参见[表6.5](#T0030)。你可以看到，在9800GT或GTX260上，几乎没有任何效果。然而，在更现代的2.x架构硬件上，GTX460和GTX470分别获得了2.4倍和3.4倍的加速。如果回顾纯GMEM实现，在GTX470上这是6.4倍的加速。为了让这个对比更加清晰，如果原始程序需要六个半小时才能运行，那么优化后的版本只需一个小时。
- en: Table 6.5 Effects of Loop Unrolling
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5 循环展开的效果
- en: '![Image](../images/T000065tabT0030.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0030.jpg)'
- en: Register optimization can have a huge impact on your code execution timing.
    Take the time to look at the PTX code being generated for the *inner loops* of
    your program. Can you unroll the loop to expand it into a single, or set, of expressions?
    Think about this with your code and you’ll see a huge performance leap. It is
    better to register usage, such as eliminating memory accesses, or provide additional
    ILP as one of the best ways to speed up a GPU kernel.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 寄存器优化对代码执行时间有巨大的影响。花时间查看程序的*内部循环*所生成的PTX代码。你能否展开循环，将其转换成单个或一组表达式？考虑一下你的代码，你会看到显著的性能提升。寄存器的使用更好，比如消除内存访问，或提供额外的ILP，是加速GPU内核的最佳方式之一。
- en: Shared Memory
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享内存
- en: Shared memory is effectively a user-controlled L1 cache. The L1 cache and shared
    memory share a 64 K memory segment per SM. In Kepler this can be configured in
    16 K blocks in favor of the L1 or shared memory as you prefer for your application.
    In Fermi the choice is 16 K or 48K in favor of the L1 or shared memory. Pre-Fermi
    hardware (compute 1.×) has a fixed 16 K of shared memory and no L1 cache. The
    shared memory has in the order of 1.5 TB/s bandwidth with extremely low latency.
    Clearly, this is hugely superior to the up to 190 GB/s available from global memory,
    but around one-fifth of the speed of registers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存实际上是一个用户控制的L1缓存。L1缓存和共享内存每个SM共享一个64K的内存段。在Kepler架构中，可以根据应用需求配置为16K块，以偏向L1或共享内存。在Fermi架构中，选择为16K或48K，以偏向L1或共享内存。Fermi之前的硬件（计算能力1.×）有固定的16K共享内存，并且没有L1缓存。共享内存的带宽约为1.5
    TB/s，且延迟极低。显然，这比全局内存的最高190 GB/s要强大得多，但大约是寄存器速度的五分之一。
- en: In practice, global memory speeds on low-end cards are as little as one-tenth
    that of the high-end cards. However, the shared memory speed is driven by the
    core clock rate, which remains much more consistent (around a 20% variation) across
    the entire range of GPUs. This means that to get the most from any card, not just
    the high-end cards, you must use shared memory effectively in addition to using
    registers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，低端显卡的全局内存速度只有高端显卡的十分之一。可是，共享内存的速度由核心时钟频率决定，核心时钟频率在所有GPU中相对保持一致（大约20%的波动）。这意味着，为了从任何显卡中获得最大性能，除了使用寄存器外，还必须有效利用共享内存，而不仅仅是高端显卡。
- en: In fact, just by looking at the bandwidth figures—1.5 TB/s for shared memory
    and 190 GB/s for the best global memory access—you can see that there is a 7:1
    ratio. To put it another way, there is potential for a 7× speedup if you can make
    effective use of shared memory. Clearly, shared memory is a concept that every
    CUDA programmer who cares about performance needs to understand well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，仅通过查看带宽数据——共享内存为1.5 TB/s，最佳全局内存访问为190 GB/s——你可以看到它们之间的比例是7:1。换句话说，如果你能够有效利用共享内存，理论上可以获得7倍的加速。显然，共享内存是每个关心性能的CUDA程序员需要深入了解的概念。
- en: However, the GPU operates a load-store model of memory, in that any operand
    must be loaded into a register prior to any operation. Thus, the loading of a
    value into shared memory, as opposed to just loading it into a register, must
    be justified by data reuse, coalescing global memory, or data sharing between
    threads. Otherwise, better performance is achieved by directly loading the global
    memory values into registers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GPU 采用了加载-存储的内存模型，即任何操作数必须在操作之前先加载到寄存器中。因此，将一个值加载到共享内存中，而不是直接加载到寄存器中，必须通过数据重用、全局内存合并或线程间数据共享来证明其合理性。否则，直接将全局内存的值加载到寄存器中，能实现更好的性能。
- en: Shared memory is a bank-switched architecture. On Fermi it is 32 banks wide,
    and on G200 and G80 hardware it is 16 banks wide. Each bank of data is 4 bytes
    in size, enough for a single-precision floating-point data item or a standard
    32-bit integer value. Kepler also introduces a special 64 bit wide mode so larger
    double precision values no longer span two banks. Each bank can service only a
    *single* operation per cycle, regardless of how many threads initiate this action.
    Thus, if every thread in a warp accesses a separate bank address, every thread’s
    operation is processed in that single cycle. Note there is no need for a one-to-one
    sequential access, just that every thread accesses a separate bank in the shared
    memory. There is, effectively, a crossbar switch connecting any single bank to
    any single thread. This is very useful when you need to swap the words, for example,
    in a sorting algorithm, an example of which we’ll look at later.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存是一种银行切换架构。在 Fermi 上，它有 32 个银行宽，而在 G200 和 G80 硬件上，它有 16 个银行宽。每个数据银行的大小为 4
    字节，足够存储一个单精度浮点数据项或一个标准的 32 位整数值。Kepler 还引入了一种特殊的 64 位宽模式，这样较大的双精度值就不再跨越两个银行。每个银行每个周期只能处理一个*单独*的操作，不管有多少线程发起该操作。因此，如果每个
    warp 中的线程都访问一个独立的银行地址，每个线程的操作都将在同一个周期内处理。需要注意的是，这里并不要求一对一的顺序访问，只要每个线程访问共享内存中的一个独立银行即可。实际上，有一个交叉开关连接着任何一个银行和任何一个线程。当你需要交换数据项时（例如，在排序算法中），这就非常有用，我们稍后将展示一个例子。
- en: There is also one other very useful case with shared memory and that is where
    every thread in a warp reads the *same* bank address. As with constant memory,
    this triggers a broadcast mechanism to all threads within the warp. Usually thread
    zero writes the value to communicate a common value with the other threads in
    the warp. See [Figure 6.2](#F0015).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个非常有用的情况，就是每个线程在一个 warp 中读取*相同*的银行地址。与常量内存类似，这会触发一个广播机制，将值传递给 warp 中的所有线程。通常，线程零会写入该值，以便与
    warp 中的其他线程通信一个公共值。见[图 6.2](#F0015)。
- en: '![image](../images/F000065f06-02-9780124159334.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-02-9780124159334.jpg)'
- en: Figure 6.2 Shared memory patterns.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 共享内存模式。
- en: However, if we have *any other pattern*, we end up with bank conflicts of varying
    degrees. This means you stall the other threads in the warp that idle while the
    threads accessing the shared memory address queue up one after another. One important
    aspect of this is that it is *not* hidden by a switch to another warp, so we do
    in fact stall the SM. Thus, bank conflicts are to be avoided if at all possible
    as the SM will idle until all the bank requests have been fulfilled.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们有*其他任何模式*，就会出现不同程度的银行冲突。这意味着，其他线程会被挂起，处于空闲状态，而访问共享内存地址的线程则一个接一个排队。一个重要的方面是，这*不会*被切换到另一个
    warp 所隐藏，因此我们确实会导致 SM 空闲。因此，尽可能避免银行冲突，因为 SM 会空闲，直到所有的银行请求被处理完毕。
- en: However, this is often not practical, such as in the histogram example we looked
    at in [Chapter 5](CHP005.html). Here the data is unknown, so which bank it falls
    into is entirely dependent on the data pattern.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这在实际中往往不可行，例如我们在[第 5 章](CHP005.html)中看到的直方图例子。在这里，数据是未知的，因此它会落入哪个银行完全取决于数据模式。
- en: The worst case is where every thread writes to the same bank, in which case
    we get 32 serial accesses to the same bank. We see this typically where the thread
    accesses a bank by a stride other than 32\. Where the stride decreases by a power
    of two (e.g., in a parallel reduction), we can also see this, with each successive
    round causing more and more bank conflicts.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最坏的情况是每个线程都写入同一个银行，这时我们会得到 32 次串行访问同一个银行。通常我们会看到这种情况出现在线程以非 32 的步幅访问银行时。在步幅按
    2 的幂递减（例如，在并行归约中）时，我们也会看到这种情况，随着每一轮操作的进行，银行冲突会越来越严重。
- en: Sorting using shared memory
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用共享内存进行排序
- en: Let’s introduce a practical example here, using sorting. A sorting algorithm
    works by taking a random dataset and generating a sorted dataset. We thus need
    *N* input data items and *N* output data items. The key aspect with sorting is
    to ensure you minimize the number of reads and writes to memory. Many sorting
    algorithms are actually multipass, meaning we read every element of *N*, *M* times,
    which is clearly not good.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里引入一个实际的例子，使用排序。排序算法通过获取一个随机数据集并生成一个已排序的数据集来工作。因此，我们需要*N*个输入数据项和*N*个输出数据项。排序的关键在于确保最小化对内存的读取和写入次数。许多排序算法实际上是多次通过的，这意味着我们会读取*N*中的每个元素，读取*M*次，这显然不是一个好选择。
- en: The quicksort algorithm is the preferred algorithm for sorting in the serial
    world. Being a divide-and-conquer algorithm, it would appear to be a good choice
    for a parallel approach. However, by default it uses recursion, which is only
    supported in CUDA compute 2.x devices. Typical parallel implementations spawn
    a new thread for every split of the data. The current CUDA model (see also discussion
    on Kepler’s Dynamic Parallelism in [Chapter 12](CHP012.html)) requires a specification
    of the total number of threads at kernel launch, or a series of kernel launches
    per level. The data causes significant branch divergence, which again is not good
    for GPUs. There are ways to address some of these issues. However, these issues
    make quicksort not the best algorithm to use on a pre-Kepler GK110/ Tesla K20
    GPU. In fact, you often find the best serial algorithm is not the best parallel
    algorithm and it is better to start off with an open mind about what will work
    best.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 快速排序算法是串行世界中首选的排序算法。作为一种分治算法，它看起来似乎是并行方法的一个好选择。然而，默认情况下它使用递归，这仅在CUDA计算能力为2.x的设备中受到支持。典型的并行实现会为数据的每次分割生成一个新线程。目前的CUDA模型（也请参阅关于Kepler动态并行性的讨论，见[第12章](CHP012.html)）要求在内核启动时指定总线程数，或者每一层级进行一系列内核启动。数据会导致显著的分支分歧，这对于GPU来说也不是一个好情况。虽然有一些方法可以解决这些问题，但这些问题使得快速排序并不是在Kepler之前的GK110/Tesla
    K20 GPU上使用的最佳算法。事实上，你会发现最佳的串行算法并不一定是最佳的并行算法，因此最好一开始就以开放的心态来考虑最适合的方案。
- en: One common algorithm found in the parallel world is the merge sort ([Figure
    6.3](#F0020)). It works by recursively partitioning the data into small and smaller
    packets, until eventually you have only two values to sort. Each sorted list is
    then merged together to produce an entire sorted list.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行世界中，常见的一个算法是归并排序（见[图6.3](#F0020)）。它通过递归地将数据分割成越来越小的包，直到最终只剩下两个值需要排序。然后，每个已排序的列表被合并在一起，形成一个完整的排序列表。
- en: '![image](../images/F000065f06-03-9780124159334.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-03-9780124159334.jpg)'
- en: Figure 6.3 Simple merge sort example.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 简单的归并排序示例。
- en: Recursion is not supported in CUDA prior to compute 2.×, so how can such an
    algorithm be performed? Any recursive algorithm will at some point have a dataset
    of size *N*. On GPUs the thread block size or the warp size is the ideal size
    for *N*. Thus, to implement a recursive algorithm all you have to do is break
    the data into blocks of 32 or larger elements as the smallest case of *N*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算能力低于2.×的CUDA版本中不支持递归，那么如何执行这样的算法呢？任何递归算法最终都会有一个大小为*N*的数据集。在GPU上，线程块大小或warp大小是*N*的理想大小。因此，要实现一个递归算法，你所需要做的就是将数据分解成32个或更多元素的块，作为*N*的最小情况。
- en: With merge sort, if you take a set of elements such as {1,5,2,8,9,3,2,1} we
    can split the data at element four and obtain two datasets, {1,5,2,8} and {9,3,2,1}.
    You can now use two threads to apply a sorting algorithm to the two datasets.
    Instantly you have gone from *p* = 1 to *p* = 2, where *p* is the number of parallel
    execution paths.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用归并排序，如果你取一组元素，如{1,5,2,8,9,3,2,1}，我们可以在第四个元素处分割数据，得到两个数据集{1,5,2,8}和{9,3,2,1}。现在，你可以使用两个线程对这两个数据集应用排序算法。瞬间，你从*p*
    = 1变为*p* = 2，其中*p*是并行执行路径的数量。
- en: 'Splitting the data from two sets into four sets gives you {1,5}, {2,8}, {9,3},
    and {2,1}. It’s now trivial to execute four threads, each of which compares the
    two numbers and swaps them if necessary. Thus, you end up with four sorted datasets:
    {1,5}, {2,8}, {3,9}, and {1,2}. The sorting phase is now complete. The maximum
    parallelism that can be expressed in this phase is *N*/2 independent threads.
    Thus, with a 512 MB dataset, you have 128K 32-bit elements, for which we can use
    a maximum of 64K threads (*N* = 128K, *N*/2 = 64K). Since a GTX580 GPU has 16
    SMs, each of which can support up to 1536 threads, we get up to 24K threads supported
    per GPU. With around two and a half passes, you can therefore iterate through
    the 64K data pairs that need to be sorted with such a decomposition.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将来自两个集合的数据拆分为四个集合，得到{1,5}，{2,8}，{9,3}和{2,1}。现在执行四个线程变得很简单，每个线程比较两个数字，并在必要时交换它们。因此，你将得到四个已排序的数据集：{1,5}，{2,8}，{3,9}和{1,2}。排序阶段现在完成了。此阶段可以表达的最大并行度是*N*/2个独立线程。因此，使用512
    MB的数据集，你有128K个32位元素，对于这些元素，我们最多可以使用64K个线程（*N* = 128K，*N*/2 = 64K）。由于GTX580 GPU有16个SM，每个SM最多支持1536个线程，因此每个GPU最多支持24K个线程。通过约两到两次半的遍历，你可以通过这种分解方式迭代排序64K个数据对。
- en: However, you now run into the classic problem with merge sort, the merge phase.
    Here the lists are combined by moving the smallest element of each list into the
    output list. This is then repeated until all members of the input lists are consumed.
    With the previous example, the sorted lists are {1,5}, {2,8}, {3,9}, and {1,2}.
    In a traditional merge sort, these get combined into {1,2,5,8} and {1,2,3,9}.
    These two lists are then further combined in the same manner to produce one final
    sorted list, {1,1,2,2,3,5,8,9}.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你现在遇到了合并排序的经典问题——合并阶段。这里，通过将每个列表中的最小元素移动到输出列表中来合并这些列表。然后重复此过程，直到所有输入列表中的元素都被处理完。以之前的例子为例，已排序的列表是{1,5}，{2,8}，{3,9}和{1,2}。在传统的合并排序中，这些列表会被合并为{1,2,5,8}和{1,2,3,9}。这两个列表随后会以相同的方式进一步合并，最终生成一个已排序的列表{1,1,2,2,3,5,8,9}。
- en: Thus, as each merge stage is completed, the amount of available parallelism
    halves. As an alternative approach where *N* is small, you can simply scan *N*
    sets of lists and immediately place the value in the correct output list, skipping
    any intermediate merge stages as shown in [Figure 6.4](#F0025). The issue is that
    the sort performed at the stage highlighted for elimination in [Figure 6.4](#F0025)
    is typically done with two threads. As anything below 32 threads means we’re using
    less than one warp, this is inefficient on a GPU.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着每个合并阶段的完成，可用的并行度会减半。作为一种替代方法，当*N*较小的时候，你可以简单地扫描*N*个列表集，并立即将值放入正确的输出列表，跳过任何中间的合并阶段，如[图6.4](#F0025)所示。问题在于，图6.4中高亮显示的阶段所执行的排序通常是用两个线程来完成的。由于32个线程以下意味着我们使用的不到一个warp，这在GPU上效率低下。
- en: '![image](../images/F000065f06-04-9780124159334.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-04-9780124159334.jpg)'
- en: Figure 6.4 Merging *N* lists simultaneously.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 同时合并*N*个列表。
- en: The downside of this approach if that it means you would need to read the first
    element of the sorted list set from every set. With 64 K sets, this is 64 K reads,
    or 256 MB of data that has to be fetched from memory. Clearly, this is not a good
    solution when the number of lists is very large.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点在于，你需要从每个集合中读取已排序列表集的第一个元素。对于64K个集合来说，这就是64K次读取，或者说需要从内存中获取256MB的数据。显然，当列表数量非常大时，这不是一个好的解决方案。
- en: Thus, our approach is to try to achieve a much better solution to the merge
    problem by limiting the amount of recursion applied to the original problem and
    stopping at the number of threads in a warp, 32, instead of two elements per sorted
    set, as with a traditional merge sort. This reduces the number of sets in the
    previous example from 64 K sorted sets to just 4 K sets. It also increases the
    maximum amount of parallelism available from *N*/2 to *N*/32\. In the 128 K element
    example we looked at previously, this would mean we would need 4 K processing
    elements. This would distribute 256 processing elements (warps) to every SM on
    a GTX580\. As each Fermi SM can execute a maximum of 48 warps, multiple blocks
    will need to be iterated through, which allows for smaller problem sizes and speedups
    on future hardware. See [Figure 6.5](#F0030).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的方法是通过限制递归的应用来解决合并问题，从而比传统的归并排序提供更好的解决方案，并在一个warp中的线程数32时停止，而不是每个排序集合有两个元素。这将减少前面示例中集合的数量，从64
    K个排序集合减少到仅4 K个集合。它还将可用的最大并行度从*N*/2提高到*N*/32。在我们之前看到的128 K元素示例中，这意味着我们将需要4 K个处理元素。这将把256个处理元素（warps）分配到每个GTX580上的SM。由于每个Fermi
    SM最多可以执行48个warps，因此需要通过多个块进行迭代，这使得在未来硬件上可以处理更小的问题规模并获得加速。见[图6.5](#F0030)。
- en: '![image](../images/F000065f06-05-9780124159334.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-05-9780124159334.jpg)'
- en: Figure 6.5 Shared memory–based decomposition.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 基于共享内存的分解。
- en: Shared memory is bank switched. We have 32 threads within a single warp. However,
    if any of those threads access the same bank, there will be a bank conflict. If
    any of the threads diverge in execution flow, you could be running at up to 1/32
    of the speed in the worst case. Threads can use registers that are private to
    a thread. They can only communicate with one another using shared memory.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存是银行切换的。我们在一个warp中有32个线程。然而，如果这些线程中的任何一个访问同一个银行，就会发生银行冲突。如果线程在执行流中发生分歧，最坏的情况下，执行速度可能会降到原来的1/32。线程可以使用私有于线程的寄存器，它们只能通过共享内存互相通信。
- en: By arranging a dataset in rows of 32 elements in the shared memory, and accessing
    it in columns by thread, you can achieve bank conflict–free access to the memory
    ([Figure 6.6](#F0035)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在共享内存中按32个元素的行排列数据集，并按线程访问这些列，你可以实现无银行冲突的内存访问（[图6.6](#F0035)）。
- en: '![image](../images/F000065f06-06-9780124159334.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-06-9780124159334.jpg)'
- en: Figure 6.6 Shared memory bank access.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 共享内存银行访问。
- en: For coalesced access to global memory, something we’ll cover in the next section,
    you’d need to fetch the data from global memory in rows of 32 elements. Then you
    can apply any sorting algorithm to the column without worrying about shared memory
    conflicts. The only thing you need to consider is branch divergence. You need
    to try to ensure that every thread follows the same execution flow, even though
    they are processing quite different data elements.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对全局内存的合并访问（我们将在下一节中讨论），你需要按32个元素为一行从全局内存中获取数据。然后，你可以对该列应用任何排序算法，而无需担心共享内存冲突。你需要考虑的唯一问题是分支分歧。你需要尽量确保每个线程遵循相同的执行流，即使它们处理的数据元素差异很大。
- en: One side effect of this strategy is we will end up having to make a tradeoff.
    Assuming we have a single warp per SM, we will have no shared memory bank conflicts.
    However, a single warp per SM will not hide the latency of global memory reads
    and writes. At least for the memory fetch and write-back stage, we need lots of
    threads. However, during the sort phase, multiple warps may conflict with one
    another. A single warp would not have any bank conflicts, yet this would not hide
    the instruction execution latency. So in practice, we’ll need multiple warps in
    all phases of the sort.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的一个副作用是我们最终需要做出折衷。假设每个SM只有一个warp，我们将不会有共享内存银行冲突。然而，单个warp在SM上不会隐藏全局内存读取和写入的延迟。至少在内存获取和写回阶段，我们需要大量的线程。然而，在排序阶段，多个warps可能会互相冲突。单个warp不会有任何银行冲突，但这并不能隐藏指令执行的延迟。所以实际上，我们在排序的所有阶段都需要多个warps。
- en: Radix sort
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基数排序
- en: 'One algorithm that has a fixed number of iterations and a consistent execution
    flow is the radix sort. It works by sorting based on the least significant bit
    and then working up to the most significant bit. With a 32-bit integer, using
    a single radix bit, you will have 32 iterations of the sort, no matter how large
    the dataset. Let’s consider an example with the following dataset:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有固定迭代次数和一致执行流程的算法是基数排序。它通过根据最低有效位进行排序，然后向最高有效位进行排序。对于一个 32 位整数，使用单个基数位，你将有
    32 次排序迭代，无论数据集有多大。让我们考虑一个包含以下数据集的示例：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The binary representation of each of these would be
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些的二进制表示为
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '` 9 = 00001001`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '` 9 = 00001001`'
- en: In the first pass of the list, all elements with a 0 in the least significant
    bit (the right side) would form the first list. Those with a 1 as the least significant
    bit would form the second list. Thus, the two lists are
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表的第一次传递中，所有最低有效位（右侧）为 0 的元素将形成第一个列表。那些最低有效位为 1 的元素将形成第二个列表。因此，两个列表是
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The two lists are appended in this order, becoming
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个列表按照这个顺序连接，变成
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The process is then repeated for bit one, generating the next two lists based
    on the ordering of the previous cycle:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程随后在第一位重复，根据上一个周期的排序生成下两个列表：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The combined list is then
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后合并的列表是
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Scanning the list by bit two, we generate
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 按照第二位扫描列表，我们生成
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And so the program continues until it has processed all 32 bits of the list
    in 32 passes. To build the list you need *N* + 2*N* memory cells, one for the
    source data, one of the 0 list, and one of the 1 list. We do not strictly need
    2*N* additional cells, as we could, for example, count from the start of the memory
    for the 0 list and count backward from the end of the memory for the 1 list. However,
    to keep it simple, we’ll use two separate lists.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 程序继续运行，直到它在 32 次传递中处理完列表的所有 32 位。为了构建列表，你需要 *N* + 2*N* 个内存单元，一个用于源数据，一个用于 0
    列表，一个用于 1 列表。我们不严格需要 2*N* 额外的单元，因为我们可以例如从内存的开始处为 0 列表计数，并从内存的末尾处为 1 列表计数。然而，为了简化，我们将使用两个独立的列表。
- en: 'The serial code for the radix sort is shown as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 基数排序的串行代码如下所示：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`   {`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`   {`'
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The code works by being passed two values, a pointer to the data to sort and
    the number of elements in the dataset. It overwrites the unsorted data so the
    returned set is sorted. The outer loop iterates over all 32 bits in a 32-bit integer
    word and the inner loop iterates over all elements in the list. Thus, the algorithm
    requires 32*N* iterations in which the entire dataset will be read and written
    32 times.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码通过传递两个值来工作，一个是待排序数据的指针，另一个是数据集中的元素数量。它会覆盖未排序的数据，因此返回的集合是已排序的。外部循环遍历 32 位整数字中的所有
    32 位，而内部循环遍历列表中的所有元素。因此，该算法需要 32*N* 次迭代，其中整个数据集会被读取和写入 32 次。
- en: Where the size of the data is less than 32 bits (e.g., with 16- or 8-bit integer
    values), the sort runs two or four times faster due to having to do one-half and
    one-quarter of the work, respectively. An implementation of the radix sort is
    available in the Thrust library shipped with v4.0 onwards of the CUDA SDK so you
    don’t have to implement your own radix sort ([Figure 6.7](#F0040)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据的大小小于 32 位（例如，16 位或 8 位整数值）时，由于只需要做一半或四分之一的工作，排序速度会快两倍或四倍。基数排序的实现可以在 CUDA
    SDK v4.0 及以后版本的 Thrust 库中找到，因此你不必自己实现基数排序（[图 6.7](#F0040)）。
- en: '![image](../images/F000065f06-07-9780124159334.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-07-9780124159334.jpg)'
- en: Figure 6.7 Simple radix sort.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 简单的基数排序。
- en: Within the inner loop the data is split into two lists, the 0 list and the 1
    list depending on which bit of the word is being processed. The data is then reconstructed
    from the two lists, the 0 list always being written before the 1 list.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在内循环中，数据被分成两个列表，0 列表和 1 列表，具体取决于正在处理的字节位。然后从这两个列表中重建数据，0 列表总是被先写入，1 列表随后写入。
- en: The GPU version is a little more complex, in that we need to take care of multiple
    threads.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 版本稍微复杂一些，因为我们需要处理多个线程。
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The GPU kernel is written here as a device function, a function only capable
    of being called within a GPU kernel. This is the equivalent of declaring a function
    as “static” in C or “private” in C++.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 内核在这里作为设备函数编写，这是一个只能在 GPU 内核内调用的函数。这相当于在 C 语言中声明一个函数为“static”，或者在 C++ 中声明为“private”。
- en: Notice the inner loop has changed and instead of incrementing by one, the program
    increments by `num_lists` a value passed into the function. This is the number
    of independent lists of data the radix sort should produce. This value should
    equal the number of threads used to invoke the kernel block. The ideal value to
    avoid bank conflicts will be the warp size, 32\. However, this is a less than
    ideal value in terms of hiding instruction and memory latency.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，内层循环已经改变，程序不再按1递增，而是按`num_lists`递增，这是传递给函数的一个值。这个值表示基数排序应生成的独立数据列表的数量。该值应该等于用于调用内核块的线程数。为了避免银行冲突，理想的值是warp大小，32。然而，从隐藏指令和内存延迟的角度来看，这个值并不理想。
- en: What this GPU version of this radix sort will produce is `num_lists` of independent
    sorted lists using `num_lists` threads. Since the SM in the GPU can run 32 threads
    at the same speed as just one thread and it has 32 shared memory banks, you might
    imagine the ideal value for `num_lists` would be 32\. See [Table 6.6](#T0035)
    and [Figure 6.8](#F0045).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个GPU版本的基数排序将使用`num_lists`个线程生成`num_lists`个独立的排序列表。由于GPU中的SM可以以与单个线程相同的速度运行32个线程，并且它有32个共享内存银行，您可以想象，`num_lists`的理想值是32。请参见[表6.6](#T0035)和[图6.8](#F0045)。
- en: Table 6.6 Parallel Radix Sort Results (ms)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.6 并行基数排序结果（毫秒）
- en: '![Image](../images/T000065tabT0035.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0035.jpg)'
- en: '![image](../images/F000065f06-08-9780124159334.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-08-9780124159334.jpg)'
- en: Figure 6.8 Parallel radix sort graph.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 并行基数排序图。
- en: As you can see from the table and figure, the radix sort is actually very efficient.
    You see an approximate linear speedup, up to 128 threads. This is not too surprising
    because each doubling of the number of threads results in each thread processing
    half as much data as before. The point of interest is where this linear relationship
    stops because it shows us when we have hit some limit in the hardware. At 256
    threads it starts to tail off with only a two-thirds speedup, so we know the ideal
    case is 128 threads. However, we also have to consider how using 128 threads might
    limit the usage in the SM, in particular in compute 2.x hardware. Therefore, we
    might select 256 threads depending on how multiple blocks interact. As it happens,
    shared memory is the main factor we need to consider limiting the number of blocks
    we’re likely to be able to put into each SM.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从表格和图形中所见，基数排序实际上非常高效。您会看到近似线性的加速，最多支持128个线程。这并不令人惊讶，因为每次将线程数加倍，意味着每个线程处理的数据量是之前的一半。值得注意的是，这种线性关系的停止点，它告诉我们硬件达到了某种限制。在256个线程时，加速开始趋于平稳，只有2/3的加速效果，因此我们知道理想的情况是128个线程。然而，我们还必须考虑使用128个线程可能会限制SM的使用，特别是在计算2.x硬件中。因此，根据多个块如何交互，我们可能会选择256个线程。实际上，共享内存是我们需要考虑的主要因素，限制了我们能够放入每个SM中的块数。
- en: If you look at the initial radix sort function, it is not very efficient. How
    would you optimize this function? The most obvious change is that you do not need
    separate 0 and 1 lists. The 0 list can be created from reusing the space in the
    original list. This not only allows you to discard the 1 list, but also removes
    a copy back to the source list. This saves a lot of unnecessary work.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看最初的基数排序函数，您会发现它并不高效。您会如何优化这个函数？最明显的改动是，您不需要单独的0和1列表。可以通过重用原始列表中的空间来创建0列表。这不仅允许您丢弃1列表，还可以避免将数据复制回源列表。这可以节省大量不必要的工作。
- en: Finally, did you notice that the bit mask is actually constant within a single
    iteration of the `bit` loop? It is thus an invariant within the `i` loop and can
    be moved out to the `bit` loop. This is a standard compiler optimization called
    invariant analysis. Most compilers would move this outside the `i` loop. Compiler
    optimization is notoriously badly documented and can change from one compiler
    to another and even between versions of compilers. Relying on optimization steps
    of compilers is therefore, generally, bad programming practice and best avoided.
    Therefore, we’ll explicitly move the calculation to ensure it gets executed in
    the correct place. See [Chapter 9](CHP009.html) on optimization for coverage of
    typical compiler optimizations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您是否注意到位掩码在`bit`循环的单次迭代中其实是常量？因此，它在`i`循环内是一个不变量，可以被移到`bit`循环外。这是一个标准的编译器优化技术，称为不变量分析。大多数编译器会将其移到`i`循环之外。编译器优化通常文档支持不佳，并且可能在不同的编译器之间甚至编译器的不同版本之间有所变化。因此，依赖编译器的优化步骤通常是糟糕的编程实践，最好避免。为了确保计算在正确的位置执行，我们将明确地将其移出。有关典型编译器优化的内容，请参见优化的[第9章](CHP009.html)。
- en: 'The slightly more optimal code we end up with is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到的稍微更优化的代码如下：
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`  // Copy data back to source from the one’s list`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`  // 从一个列表复制数据回源`'
- en: '[PRE24]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There are further optimizations that can be made, but the key issue here is
    that we’re now using only one temporary storage area, which in turn allows the
    processing of more elements. This is important because, as we’ll see later, the
    number of lists is an important factor. So how do these changes affect the performance
    of the radix sort?
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 还有进一步的优化可以进行，但这里的关键问题是我们现在只使用一个临时存储区，这反过来允许处理更多的元素。这很重要，因为正如我们稍后将看到的，列表的数量是一个重要的因素。那么这些变化如何影响基数排序的性能呢？
- en: If you look at [Table 6.7](#T0040), you’ll see the worst case, using a single
    thread, has come down from 82 ms to 52 ms. The best case in the previous run,
    0.26 ms, has come down to 0.21 ms, which is about a 20% improvement in execution
    speed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看[表 6.7](#T0040)，你会看到最坏的情况，使用单个线程，已经从82毫秒降到了52毫秒。上一轮中的最佳情况，0.26毫秒，已经降到了0.21毫秒，执行速度大约提高了20%。
- en: Table 6.7 Optimized Radix Sort Results (ms)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.7 优化的基数排序结果（毫秒）
- en: '![Image](../images/T000065tabT0040.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0040.jpg)'
- en: Merging lists
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合并列表
- en: Merge lists of sorted elements is another algorithm that is commonly used in
    parallel programming. However, let’s start by looking at some serial code to merge
    an arbitrary number of sorted lists into a single sorted list, as this is the
    simplest case.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 合并已排序元素的列表是并行编程中常用的另一个算法。然而，让我们先从一些串行代码开始，将任意数量的已排序列表合并成一个已排序的列表，因为这是最简单的情况。
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '` for (u32 i=0; i<num_elements;i++)`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '` for (u32 i=0; i<num_elements;i++)`'
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Assuming there are `num_lists` lists to collect data from, you need some way
    to track where we are in the list. The program uses the array `list_indexes` for
    this. As the number of lists is likely to be small, you can use the stack and
    thus declare the array as a local variable. Note this would be a bad idea with
    a GPU kernel, as the stack allocation may get placed into slow global memory,
    depending on the particular GPU variant. Shared memory would likely be the optimal
    location on the GPU, depending on the number of lists needed.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有`num_lists`个列表需要收集数据，你需要某种方式来追踪我们在列表中的位置。程序使用`list_indexes`数组来实现这一点。由于列表的数量可能较少，你可以使用栈，因此将数组声明为局部变量。请注意，在GPU内核中这样做是一个坏主意，因为栈分配可能会被放入较慢的全局内存中，这取决于特定的GPU型号。根据所需列表的数量，共享内存可能是GPU上的最佳位置。
- en: '![image](../images/F000065f06-09-9780124159334.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-09-9780124159334.jpg)'
- en: Figure 6.9 Multiple lists partially merged.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 多个部分合并的列表。
- en: First, the index values are all set to zero. Then the program iterates over
    all elements and assigns the value in the sorted array from the result of a function,
    `find_min`. The `find_min` function identifies the smallest value from a set of
    `num_lists` values.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所有的索引值都设置为零。然后程序遍历所有元素，并从`find_min`函数的结果中为已排序的数组分配值。`find_min`函数从`num_lists`个值中找出最小的值。
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The function works by iterating through the lists of sorted values and maintaining
    an index into where it is in each list. If it identifies a smaller value than
    `min_val`, it simply updates `min_val` to this new value. When it has scanned
    all the lists, it increments the relevant list index and returns the value it found.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过遍历已排序值的列表并保持每个列表的索引来工作。如果它识别出比`min_val`更小的值，它会将`min_val`更新为这个新值。当它扫描完所有列表后，它会递增相关的列表索引并返回找到的值。
- en: 'Now let’s look at the GPU implementation of this algorithm. First, the top-level
    function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这个算法在GPU上的实现。首先是顶层函数：
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '` const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '` const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
- en: '[PRE29]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is quite a simple program for now. It will be invoked with a single block
    of *N* threads. We’ll develop this as an example of how to use shared memory.
    Looking at the first function, we see the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是一个相当简单的程序。它将用一个*N*线程的单个块来调用。我们将开发这个作为如何使用共享内存的示例。查看第一个函数，我们看到以下内容：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here the program reads data from global memory in rows and not columns into
    the shared memory. This step is important for two reasons. First, the program
    will repeatedly read and write from this memory. Therefore, you want the fastest
    store possible, so we need to use shared memory instead of global memory. Second,
    global memory provides the best performance when accessed by rows. Column access
    produces a scattered memory pattern that the hardware is unable to coalesce, unless
    every thread accesses the same column value and these addresses are adjacent.
    Thus, in most cases the GPU has to issue far more memory fetch operations than
    are necessary and the speed of the program will drop by an order of magnitude.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，程序从全局内存按行而不是按列读取数据到共享内存中。这一步骤很重要，原因有二。首先，程序会反复从该内存中读取和写入数据。因此，为了获得最快的存储速度，我们需要使用共享内存而不是全局内存。其次，全局内存在按行访问时提供最佳性能。按列访问会产生硬件无法合并的分散式内存模式，除非每个线程都访问相同的列值，并且这些地址是相邻的。因此，在大多数情况下，GPU必须执行比实际需要的更多内存读取操作，导致程序的速度下降一个数量级。
- en: When you compile this program, if you have the `-v` flag set on the nvcc compiler
    options, it will print an innocent looking message saying it created a stack frame.
    For example,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编译这个程序时，如果你在 nvcc 编译器选项中设置了 `-v` 标志，它会输出一个看似无害的消息，表示它创建了一个堆栈帧。例如：
- en: '[PRE31]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When a function makes a call into a subfunction and passes parameters, those
    parameters must somehow be provided to the called function. The program makes
    just such a call:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个函数调用另一个子函数并传递参数时，这些参数必须以某种方式传递给被调用函数。程序就是这样进行调用的：
- en: '`dest_array[i] = find_min(src_array,`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`dest_array[i] = find_min(src_array,`'
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are two options that can be employed, to pass the necessary values through
    registers, or to create an area of memory called a stack frame. Most modern processors
    have a large register set (32 or more registers). Thus, for a single level of
    calls, often this is enough. Older architectures use stack frames and push the
    values onto the stack. The called function then pops the values off the stack.
    As you require memory to do this, on the GPU this would mean using “local” memory,
    which is local only in terms of which thread can access it. In fact, “local” memory
    can be held in global memory, so this is hugely inefficient, especially on the
    older architectures (1.x) where it’s not cached. At this point we need to rewrite
    the merge routine to avoid the function call. The new routine is thus:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以使用：通过寄存器传递必要的值，或者创建一个称为堆栈帧的内存区域。大多数现代处理器都有一个较大的寄存器集合（32个或更多寄存器）。因此，对于单级调用，这通常是足够的。较旧的架构使用堆栈帧，并将值推入堆栈中。被调用的函数然后从堆栈中弹出这些值。由于你需要内存来实现这一点，在GPU上这意味着使用“本地”内存，这在本质上是指哪个线程可以访问它。实际上，“本地”内存可以存放在全局内存中，因此在效率上非常低下，尤其是在较老的架构（1.x）上，因为它没有缓存。此时，我们需要重写归并例程以避免函数调用。新的例程如下：
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`     const u32 data = src_array[src_idx];`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`     const u32 data = src_array[src_idx];`'
- en: '[PRE34]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This function now combines the original `merge_arrary` function and its `find_min`
    function. Recompiling now results in no additional stack frame. Running the code,
    we find the results shown in [Table 6.8](#T0045). If you graph this, it’s somewhat
    easier to see ([Figure 6.10](#F0055)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数现在将原来的 `merge_arrary` 函数和它的 `find_min` 函数合并在一起。重新编译后不会再创建额外的堆栈帧。运行代码后，我们可以看到结果如[表
    6.8](#T0045)所示。如果将其绘制成图形，观察会更容易一些（见[图 6.10](#F0055)）。
- en: Table 6.8 Initial Single-Thread Merge Sort Results
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.8 初始单线程归并排序结果
- en: '![Image](../images/T000065tabT0045.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000065tabT0045.jpg)'
- en: '![image](../images/F000065f06-10-9780124159334.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000065f06-10-9780124159334.jpg)'
- en: Figure 6.10 Initial single-thread merge sort graph.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 初始单线程归并排序图。
- en: What is surprising from this graph is the worst performer is actually the GTX260,
    which is slower than the previous generation 9800GT. It’s interesting to also
    note the GTX460 is faster than the GTX470 in this particular test. To understand
    this, you need to look at the specific devices used, as shown in [Table 6.9](#T0050).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中令人惊讶的是，表现最差的是 GTX260，它比前一代的 9800GT 更慢。有趣的是，GTX460 在这个特定的测试中比 GTX470 更快。要理解这一点，你需要查看所用的具体设备，如[表
    6.9](#T0050)所示。
- en: Table 6.9 Device Clock Rate and Bandwidth
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.9 设备时钟频率与带宽
- en: '![Image](../images/T000065tabT0050.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000065tabT0050.jpg)'
- en: You can see the 9800GT has a higher internal clock rate than the GTX260\. The
    same is true of the GTX460 and GTX470\. Since the program is using just a single
    SM, and memory access is dominated by shared memory access time, this is entirely
    to be expected.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，9800GT的内核时钟频率高于GTX260，GTX460和GTX470也一样。由于程序只使用了一个SM，并且内存访问主要受共享内存访问时间的支配，所以这是完全可以预见的。
- en: However, perhaps the most interesting feature you can see from the graph is
    that increasing the number of threads beyond a certain point actually makes the
    calculation go slower. This initially seems counterintuitive if you have never
    seen this relationship before. What this type of result points to is that there
    is some conflict of resources or the problem does not scale in a linear manner
    when you increase the number of threads.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也许你从图表中能看到的最有趣的特点是，当线程数量增加到一定程度后，计算反而变得更慢。如果你以前没有见过这种关系，最初这似乎是违反直觉的。这个结果表明，增加线程数时，资源可能存在冲突，或者问题的扩展性并非线性。
- en: The problem is the latter. The merge step is single thread and must look at
    *N* lists for every element. As the number of lists are increased, the problem
    space becomes 2*N*, 4*N*, 8*N,* etc. in line with the number of threads. The optimal
    point for this algorithm, based on the timings, is actually between four and eight
    lists of data. This is not very good, as it considerably limits the potential
    amount of parallelism.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于后者。合并步骤是单线程的，并且必须查看每个元素的*N*个列表。随着列表数量的增加，问题空间变为2*N*、4*N*、8*N*等，随着线程数的增加而增加。根据计时结果，该算法的最佳点实际上是在四到八个数据列表之间。这并不好，因为它大大限制了潜在的并行度。
- en: Parallel merging
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行合并
- en: For better performance, clearly more than one thread in the merge stage is required.
    However, this introduces a problem, in that we’re writing to a single list. To
    do this, the threads need to cooperate in some manner. This makes the merge somewhat
    more complex.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的性能，显然在合并阶段需要使用多个线程。然而，这会引入一个问题，因为我们正在写入一个单一的列表。为了解决这个问题，线程需要以某种方式进行协作，这使得合并过程变得更加复杂。
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '` // Wait for list_indexes[tid] to be cleared`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`  // 等待list_indexes[tid]被清空`'
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`  __syncthreads();`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`   __syncthreads();`'
- en: '[PRE37]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This version uses `num_lists` threads to do the merge operation. However, only
    a single thread writes to the output data list at a time, thus ensuring the single
    output list is correct at all times.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 该版本使用`num_lists`线程进行合并操作。然而，只有一个线程会同时写入输出数据列表，从而确保单个输出列表始终是正确的。
- en: It makes use of the `atomicMin` function. Instead of one thread reading all
    the values from the lists and computing the minimum, each thread calls `atomicMin`
    with the value of its list entry. Once all threads have called the `atomicMin`
    function, each thread reads it back and compares this with the value it tried
    to write. If the values are the same, then the thread was the winning thread.
    However, there is one further problem in that there may be several winning threads,
    because the data item can be repeated in one or more lists. Thus, a second elimination
    step is required by only those threads with identical data. Most of the time,
    this second step will not be necessary. However, in the worst case of sorting
    a list of identical numbers, it would cause every thread to have to go through
    two elimination steps.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 它利用了`atomicMin`函数。不是一个线程从所有列表中读取所有值并计算最小值，而是每个线程调用`atomicMin`函数，传入其列表项的值。当所有线程都调用了`atomicMin`函数后，每个线程会读取返回值并与自己尝试写入的值进行比较。如果两个值相同，则说明该线程是“胜利”线程。然而，仍然存在一个问题，即可能有多个胜利线程，因为数据项可能在一个或多个列表中重复。因此，只有那些具有相同数据的线程需要进行第二次筛选。通常情况下，这第二步是不必要的。然而，在排序一个由相同数字组成的列表时，最坏的情况是每个线程都必须进行两次筛选步骤。
- en: So how does this version perform? As you can see from [Table 6.10](#T0055) and
    [Figure 6.11](#F0060), we have reduced the total execution time using the larger
    number of threads (128 and 256 threads) by a factor of 10\. However, single-thread
    timing is unchanged. More important, however, is the fastest time has moved from
    the 8- to the 16-thread version and has halved in terms of absolute time.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个版本的表现如何呢？正如你从[表6.10](#T0055)和[图6.11](#F0060)中看到的，通过增加线程数量（128和256线程），我们将总执行时间缩短了10倍。然而，单线程的计时保持不变。更重要的是，最快的时间已经从8线程版本移动到了16线程版本，并且在绝对时间上缩短了一半。
- en: Table 6.10 `atomicMin` Parallel Merge Sort Results (ms)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.10 `atomicMin` 并行合并排序结果（毫秒）
- en: '![Image](../images/T000065tabT0055.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0055.jpg)'
- en: '![image](../images/F000065f06-11-9780124159334.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-11-9780124159334.jpg)'
- en: Figure 6.11 `atomicMin` parallel merge sort graph.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 `atomicMin` 并行归并排序图。
- en: One thing I should mention here is that `atomicMin` on shared memory requires
    a compute 1.2 device or higher. The 9800GT is only a compute 1.1 device, so is
    not shown here as it cannot run the kernel.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里需要提到的是，`atomicMin` 在共享内存上的使用要求设备支持计算能力 1.2 或更高版本。9800GT 只是一个计算能力为 1.1 的设备，因此未显示在这里，因为它无法运行该内核。
- en: If we look a little closer at the hardware counters with a tool like Parallel
    Nsight, we can see that beyond 32 threads the number of divergent branches and
    the number of shared memory accesses start to grow very rapidly. We currently
    have a good solution, but what alternative approaches are there and are they any
    quicker?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用像 Parallel Nsight 这样的工具仔细查看硬件计数器，可以看到，超过 32 个线程后，分支的数量和共享内存访问的数量会迅速增加。目前我们有一个不错的解决方案，但还有哪些替代方法？它们更快吗？
- en: Parallel reduction
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行归约
- en: One common approach to this problem is parallel reduction. This can be applied
    for many problems, a `min` operation being just one of them. It works by using
    half the number of threads of the elements in the dataset. Every thread calculates
    the minimum of its own element and some other element. The resultant element is
    forwarded to the next round. The number of threads is then reduced by half and
    the process repeated until there is just a single element remaining, which is
    the result of the operation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种常见方法是并行归约。这种方法可以应用于许多问题，`min` 操作只是其中之一。它通过使用数据集中的元素数的一半的线程来工作。每个线程计算它自己的元素与其他某个元素之间的最小值。结果元素会被传递到下一轮。然后，线程数减半，过程重复进行，直到只剩下一个元素，这就是操作的结果。
- en: With CUDA you must remember that the execution unit for a given SM is a warp.
    Thus, any amount of threads less than one warp is underutilizing the hardware.
    Also, while divergent threads must all be executed, divergent warps do not have
    to be.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CUDA 时，必须记住给定 SM 的执行单元是 warp。因此，任何少于一个 warp 的线程数量都会导致硬件利用率低下。此外，尽管必须执行所有的分支线程，但分支的
    warp 不必都执行。
- en: When selecting the “other element” for a given thread to work with, you can
    do so to do a reduction within the warp, thus causing significant branch divergence
    within it. This will hinder the performance, as each divergent branch doubles
    the work for the SM. A better approach is to drop whole warps by selecting the
    other element from the other half of the dataset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在为给定线程选择“其他元素”时，你可以这样做，以便在 warp 内进行归约，这样会在其中引发显著的分支分歧。这会影响性能，因为每一个分支分歧都会让 SM
    的工作量加倍。更好的方法是通过从数据集的另一半选择其他元素，丢弃整个 warp。
- en: In [Figure 6.12](#F0065) you see the item being compared with one from the other
    half of the dataset. Shaded cells show the active threads.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6.12](#F0065)中，你可以看到当前项与数据集另一半中的项进行比较。阴影部分表示活动线程。
- en: '![image](../images/F000065f06-12-9780124159334.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-12-9780124159334.jpg)'
- en: Figure 6.12 Final stages of GPU parallel reduction.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 GPU 并行归约的最后阶段。
- en: '[PRE38]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`  // emptied then ignore it`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`  // 清空后忽略它`'
- en: '[PRE39]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`   __syncthreads();`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`   __syncthreads();`'
- en: '[PRE40]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This code works by creating a temporary list of data in shared memory, which
    it populates with a dataset from each cycle from the `num_list` datasets. Where
    a list has already been emptied, the dataset is populated with `0xFFFFFFFF`, which
    will exclude the value from the list. The `while` loop gradually reduces the number
    of active threads until there is only a single thread active, thread zero. This
    then copies the data and increments the list indexes to ensure the value is not
    processed twice.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码通过在共享内存中创建一个临时数据列表来工作，并通过每个周期从 `num_list` 数据集填充该列表。当一个列表已经被清空时，数据集会填充 `0xFFFFFFFF`，这将排除该值。`while`
    循环逐渐减少活动线程的数量，直到只剩下一个活动线程，即线程零。然后，它会复制数据并递增列表索引，确保该值不会被重复处理。
- en: Notice the use of the `__syncthreads` directive within the loop and at the end.
    The program needs to sync across warps when there are more than 32 threads (one
    warp) in use.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在循环内及循环结束时使用 `__syncthreads` 指令。当使用的线程数超过 32 个（即一个 warp）时，程序需要在 warps 之间同步。
- en: So how does this perform? As you can see from [Table 6.11](#T0060) and [Figure
    6.13](#F0070), this approach is significantly slower than the `atomicMin` version,
    the fastest reduction being 8.4 ms versus the 5.86 ms `atomicMin` (GTX460, 16
    threads). This is almost 50% slower than the `atomicMin` version. However, one
    thing to note is that it’s a little under twice the speed of the `atomicMin` when
    using 256 threads (12.27 ms versus 21.58 ms). This is, however, still twice as
    slow as the 16-thread version.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 那么它的表现如何呢？正如你在[表6.11](#T0060)和[图6.13](#F0070)中看到的，这种方法比`atomicMin`版本慢得多，最快的减少时间为8.4毫秒，而`atomicMin`为5.86毫秒（GTX460，16线程）。这比`atomicMin`版本慢了将近50%。然而，需要注意的一点是，当使用256个线程时，它的速度大约是`atomicMin`的两倍（12.27毫秒对比21.58毫秒）。不过，这仍然比16线程版本慢了一倍。
- en: Table 6.11 Parallel Reduction Results (ms)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.11 并行减少结果（毫秒）
- en: '![Image](../images/T000065tabT0060.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000065tabT0060.jpg)'
- en: '![image](../images/F000065f06-13-9780124159334.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000065f06-13-9780124159334.jpg)'
- en: Figure 6.13 Parallel reduction graph.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 并行减少图
- en: Although this version is slower, it has the advantage of not requiring the use
    of the `atomicMin` function. This function is only available on compute 1.2 devices,
    which is generally only an issue if you need to consider the consumer market or
    you need to support *really* old Tesla systems. The main issue, however, is that
    `atomicMin` can only be used with integer values. A significant number of real-world
    problems are floating-point based. In such cases we need both algorithms.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个版本较慢，但它的优势在于不需要使用`atomicMin`函数。这个函数仅在compute 1.2设备上可用，通常只有在需要考虑消费者市场或者需要支持*非常*老旧的Tesla系统时才会成为问题。然而，主要问题是`atomicMin`只能用于整数值。许多实际问题是基于浮点数的，在这种情况下我们需要两种算法。
- en: However, what we can take from both the `atomicMin` and the parallel reduction
    method is that the traditional merge sort using two lists is not the ideal case
    on a GPU. You get increasing performance from the increasing parallelism in the
    radix sort as you increase the number of lists. However, you get decreasing performance
    from the merge stage as you increase the parallelism and move beyond 16 lists.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从`atomicMin`和并行减少方法中我们可以得出结论，传统的使用两个列表的归并排序并不是GPU上的理想情况。当你增加列表的数量时，基数排序的并行性会提高，性能也随之增加。然而，当你增加并行性并超过16个列表时，归并阶段的性能却会下降。
- en: A hybrid approach
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合方法
- en: 'There is potential here to exploit the benefits of both algorithms by creating
    a hybrid approach. We can rewrite the merge sort as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有潜力通过创建混合方法来利用两种算法的优势。我们可以按照以下方式重写归并排序：
- en: '[PRE41]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '` const u32 num_reductions = num_lists >> REDUCTION_SIZE_BIT_SHIFT;`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`const u32 num_reductions = num_lists >> REDUCTION_SIZE_BIT_SHIFT;`'
- en: '[PRE42]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`   }`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`   }`'
- en: '[PRE43]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: One of the main problems of the simple 1-to-*N* reduction is it becomes increasingly
    slower as the value of *N* increases. We can see from previous tests that the
    ideal value of *N* is around 16 elements. The kernel works by creating a partial
    reduction of *N* values and then a final reduction of those *N* values into a
    single value. In this way it’s similar to the reduction example, but skips most
    of the iterations.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的1对*N*减少的主要问题之一是，随着*N*值的增加，其速度会变得越来越慢。从之前的测试中我们可以看到，理想的*N*值大约是16个元素。这个内核通过创建*N*个值的部分减少，然后将这些*N*个值最终减少成一个单一的值。这样，它类似于减少示例，但跳过了大多数迭代。
- en: Notice that `min_val` has been extended from a single value into an array of
    shared values. This is necessary so each independent thread can minimize the values
    over its dataset. Each `min` value is 32 bits wide so it exists in a separate
    shared memory bank, meaning there are no bank conflicts provided the maximum number
    of first-level reductions results in 32 or less elements.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`min_val`已经从单一的值扩展成一个共享值的数组。这是必要的，因为每个独立的线程都可以在其数据集上最小化值。每个`min`值的宽度为32位，因此它存在于一个独立的共享内存银行中，这意味着只要第一次级减少的最大元素数不超过32，就不会发生内存银行冲突。
- en: The value of `REDUCTION_SIZE` has been set to eight, which means the program
    will do a `min` over groups of eight values prior to a final `min`. With the maximum
    of 256 elements, we get exactly 32
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`REDUCTION_SIZE`的值已经设置为八，这意味着程序将在最终的`min`之前对八个值的组进行`min`操作。最大值为256个元素时，结果恰好为32。'
- en: '![image](../images/F000065f06-14-9780124159334.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000065f06-14-9780124159334.jpg)'
- en: Figure 6.14 Hybrid parallel reduction.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 混合并行减少
- en: seperate banks being used to do the reduction. In the 256 elements we have a
    256:32:1 reduction. With a 128-element list we have a 128:16:1 reduction, etc.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的内存银行进行归约。在256个元素中，我们有一个256:32:1的归约。对于128个元素的列表，我们有一个128:16:1的归约，依此类推。
- en: The other major change is now only the thread that writes out the winning element
    reads a new value into `data`, a register-based value that is per thread. Previously,
    all threads re-read in the value from their respective lists. As only one thread
    won each round, only one list pointer changed. Thus, as *N* increased, this became
    increasingly inefficent. However, this doesn’t help as much as you might at first
    imagine.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要的变化是，现在只有写出获胜元素的线程会将新值读取到`data`中，这是每个线程的寄存器值。以前，所有线程都会重新读取其各自列表中的值。由于每轮只有一个线程获胜，因此只有一个列表指针发生了变化。因此，随着*N*的增加，这变得越来越低效。然而，这并不像你最初想象的那样有太大帮助。
- en: So how does this version perform? Notice in [Table 6.12](#T0065) that the minimum
    time, 5.86 ms from the `atomicMin` example, has fallen to 5.67 ms. This is not
    spectacular, but what is interesting to note is the shape of the graph ([Figure
    6.15](#F0080)). No longer is the graph such an inclined U shape. Both the 32-
    and 64-thread versions beat the simple `atomicMin` based on 16 threads. We’re
    starting to smooth out the upward incline introduced by the merge step as shown
    in [table 6.12](#T0065) and [figure 6.15](#F0080).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个版本表现如何呢？请注意在[表6.12](#T0065)中，`atomicMin`示例中的最小时间5.86毫秒已经降至5.67毫秒。这并不显得非常出色，但有趣的是图形的形状（[图6.15](#F0080)）。图形不再呈现出一个倾斜的U形。32线程和64线程版本都超过了基于16线程的简单`atomicMin`。我们开始平滑[表6.12](#T0065)和[图6.15](#F0080)中显示的归约步骤所引入的上升斜率。
- en: Table 6.12 Hybrid Atomic and Parallel Reductions Results (ms)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.12 混合原子和并行归约结果（毫秒）
- en: '![Image](../images/T000065tabT0065.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0065.jpg)'
- en: '![image](../images/F000065f06-15-9780124159334.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-15-9780124159334.jpg)'
- en: Figure 6.15 Hybrid atomic and parallel reduction graph.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 混合原子和并行归约图。
- en: Shared memory on different GPUs
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同GPU上的共享内存
- en: Not all GPUs are created equal. With the move to compute 2.x devices, the amount
    of shared memory became configurable. By default, compute 2.x (Fermi) devices
    are configured to provide 48K of shared memory instead of the 16 K of shared memory
    on compute 1.x devices.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有的GPU都相同。随着向计算2.x设备的过渡，共享内存的数量变得可以配置。默认情况下，计算2.x（Fermi）设备配置提供48K的共享内存，而计算1.x设备则只有16K的共享内存。
- en: 'The amount of shared memory can change between hardware releases. To write
    programs that scale in performance with new GPU releases, you have to write portable
    code. To support this, CUDA allows you to query the device for the amount of shared
    memory available with the following code:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存的数量可以随着硬件版本的不同而变化。为了编写能够在新的GPU发布时提高性能的程序，你必须编写可移植的代码。为此，CUDA允许你通过以下代码查询设备上可用的共享内存数量：
- en: '[PRE44]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Having more shared memory available allows us to select one of two strategies.
    We can either extend the amount of shared memory used from 16 K to 48 K or we
    can simply schedule more blocks into a single SM. The best choice will really
    depend on the application at hand. With our sorting example, 48 K of shared memory
    would allow the number of lists per SM to be reduced by a factor of three. As
    we saw earlier, the number of lists to merge has a significant impact on the overall
    execution time.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 有更多共享内存可用使我们能够选择两种策略之一。我们可以将使用的共享内存从16K扩展到48K，或者我们可以简单地将更多的块调度到单个SM中。最好的选择实际上取决于具体应用的需求。在我们的排序示例中，48K的共享内存可以将每个SM中的列表数量减少三倍。正如我们之前看到的，合并的列表数量对整体执行时间有显著影响。
- en: Shared memory summary
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享内存总结
- en: So far we have looked only at sorting within a single SM, in fact within a single
    block. Moving from a single-block version to a multiple-block version introduces
    another set of merges. Each block will produce an independent sorted list. These
    lists then have to be merged, but this time in global memory. The list size moves
    outside that which can be held in shared memory. The same then becomes true when
    using multiple GPUs—you generate *N* or more sorted lists where *N* equals the
    number of GPUs in the system.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了在单个SM中进行排序，实际上是在单个块中排序。从单块版本转换为多块版本引入了另一组归约步骤。每个块将生成一个独立的排序列表。这些列表然后需要合并，但这次是在全局内存中。列表的大小超出了共享内存所能容纳的范围。当使用多个GPU时，情况也一样——你会生成*N*个或更多的排序列表，其中*N*等于系统中的GPU数量。
- en: We’ve looked primarily at interthread cooperation with shared memory in this
    section. The merging example was selected to demonstrate this in a manner that
    was not too complex and easy to follow. Parallel sorting has a large body of research
    behind it. More complex algorithms may well be more efficient, in terms of the
    memory usage and/or SM utilization. The point here was to use a practical example
    that could be easily followed and process lots of data that did not simply reduce
    to a single value.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们主要讨论了使用共享内存的线程间协作。选择合并示例是为了以一种不太复杂且易于理解的方式演示这一点。并行排序已经有大量的研究支持。更复杂的算法在内存使用和/或共享内存（SM）利用率方面可能会更高效。这里的重点是使用一个实践性的示例，该示例既易于跟随，又能处理大量数据，而不仅仅是简化为一个单一的值。
- en: We’ll continue to look at sorting later and look at how interblock communication
    and coordination can be achieved in addition to thread-level communication.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续讨论排序，并了解如何在线程级通信之外实现块间通信与协调。
- en: Questions on shared memory
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享内存问题
- en: 1. Looking at the `radix_sort` algorithm, how might the use of shared memory
    be reduced? Why would this be useful?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 看`radix_sort`算法，如何减少共享内存的使用？这样做有什么好处？
- en: 2. Are all the synchronization points necessary? In each instance a synchronization
    primitive is used. Discuss why. Are there conditions where they are not necessary?
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 所有同步点都是必要的吗？每次使用同步原语时，讨论一下原因。是否存在不需要同步点的情况？
- en: 3. What would be the effect of using C++ templates in terms of execution time?
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 使用C++模板会对执行时间产生什么影响？
- en: 4. How would you further optimize this sorting algorithm?
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 如何进一步优化这个排序算法？
- en: Answers for shared memory
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享内存答案
- en: 1. There are a number of solutions. One is to use only the memory allocated
    to the sort. This can be done using an MSB radix sort and swapping the 1s with
    elements at the end of the list. The 0 list counts forward and the 1 list counts
    backward. When they meet, the next digit is sorted until the LSB is sorted. Reducing
    the memory usage is useful because it allows larger lists in the shared memory,
    reducing the total number of lists needed, which significantly impacts execution
    time.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 有多种解决方案。一种方法是仅使用分配给排序的内存。这可以通过MSB基数排序实现，并将1与列表末尾的元素交换。0列表从前向后计数，1列表从后向前计数。当它们相遇时，排序下一个数字，直到排序完LSB。减少内存使用是有益的，因为它可以在共享内存中容纳更大的列表，减少所需的列表总数，从而显著影响执行时间。
- en: 2. The main concept to understand here is the synchronization points are necessary
    only when more than one warp is used. Within a warp all instructions execute synchronously.
    A branch causes the nonbranched threads to stall. At the point the branch converges,
    you are guaranteed all instructions are in sync, although the warps can then instantly
    diverge again. Note that memory must be declared as volatile or you must have
    syncthread points within the warp if you wish to guarantee visibility of writes
    between threads. See [Chapter 12](CHP012.html) on common problems for a discussion
    on the use of the volatile qualifier.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 这里需要理解的主要概念是，只有在使用多个warp时才需要同步点。在一个warp内，所有指令都是同步执行的。一个分支会导致未分支的线程停顿。当分支汇聚时，保证所有指令同步执行，尽管warp随后可能立即再次分裂。请注意，若希望保证线程之间写操作的可见性，必须将内存声明为volatile，或者在warp内使用syncthread同步点。关于volatile限定符的使用，请参见[第12章](CHP012.html)中的常见问题讨论。
- en: 3. Templates would allow much of the runtime evaluation of the `num_lists` parameter
    to be replaced with compile time substitution. The parameter must always be a
    power of 2, and in practice will be limited to a maximum of 256\. Thus, a number
    of templates can be created and the appropriate function called at runtime. Given
    a fixed number of iterations known at compiler time instead of runtime, the compiler
    can efficiently unroll loops and substitute variable reads with literals. Additionally,
    templates can be used to support multiple implementations for different data types,
    for example, using the `atomicMin` version for integer data while using a parallel
    reduction for floating-point data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 模板将允许在编译时替代`num_lists`参数的许多运行时评估。该参数必须始终是2的幂，并且在实践中通常限制为最大256。因此，可以创建多个模板，并在运行时调用相应的函数。由于在编译时已知固定的迭代次数，而非在运行时，编译器可以高效地展开循环并将变量读取替换为常量。此外，模板还可以用于支持不同数据类型的多种实现，例如，对于整数数据使用`atomicMin`版本，而对于浮点数据则使用并行归约。
- en: 4. This is rather an open-ended question. There are many valid answers. As the
    number of sorted lists to merge increases, the problem becomes significantly larger.
    Elimination of the merge step would be a good solution. This could be achieved
    by partially sorting the original list into *N* sublists by value. Each sublist
    can then be sorted and the lists concatenated, rather than merged. This approach
    is the basis of another type of sort, sample sort, an algorithm we look at later
    in this chapter.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 这是一个开放性的问题，存在许多有效的答案。随着需要合并的已排序列表数量的增加，问题变得显著更大。消除合并步骤将是一个不错的解决方案。这可以通过将原始列表按值部分排序成*N*个子列表来实现。然后可以对每个子列表进行排序，并将列表连接起来，而不是合并。这个方法是另一种排序类型——样本排序的基础，这是我们在本章后面会讨论的算法。
- en: Consider also the size of the dataset in the example, 1024 elements. With 256
    threads there are just four elements per list. A radix sort using a single bit
    is very inefficient for this number of elements, requiring 128 iterations. A comparison-based
    sort is much quicker for such small values of *N.*
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要考虑示例中数据集的大小，1024个元素。使用256个线程时，每个列表只有四个元素。对于这个元素数量，使用单个比特的基数排序非常低效，需要128次迭代。基于比较的排序对于这种小的*N*值要快得多。
- en: In this example, we used a single bit for the radix sort. Multiple bits can
    be used, which reduces the number of passes over the dataset at the expense of
    more intermediate storage. We currently use an iterative method to sort elements
    into sequential lists. It’s quite possible to work where the data will move to
    by counting the radix bits and using a `prefix sum` calculation to work out the
    index of where the data should be written. We look at `prefix sum` later in this
    chapter.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了一个比特来进行基数排序。可以使用多个比特，这样可以减少对数据集的遍历次数，但需要更多的中间存储。我们目前使用迭代方法将元素排序到顺序列表中。在这种方法中，通过计算基数位并使用`前缀和`计算来确定数据应该写入的索引，是完全有可能工作的。我们将在本章后面讨论`前缀和`。
- en: Constant Memory
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常量内存
- en: Constant memory is a form of virtual addressing of global memory. There is no
    special reserved constant memory block. Constant memory has two special properties
    you might be interested in. First, it is cached, and second, it supports broadcasting
    a single value to all the elements within a warp.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存是一种全局内存的虚拟寻址形式。没有特别预留的常量内存块。常量内存有两个特殊属性，您可能会感兴趣。首先，它是缓存的，其次，它支持将单个值广播到warp中的所有元素。
- en: Constant memory, as its name suggests, is for read-only memory. This is memory
    that is either declared at compile time as read only or defined at runtime as
    read only by the host. It is, therefore, constant only in respect of the GPU’s
    view onto memory. The size of constant memory is restricted to 64 K.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，常量内存是用于只读内存的。这是指在编译时声明为只读的内存，或由主机在运行时定义为只读的内存。因此，它在GPU视图中的内存是常量的。常量内存的大小限制为64
    K。
- en: 'To declare a section of memory as constant at compile time, you simply use
    the `__constant__` keyword. For example:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 要在编译时声明一部分内存为常量，只需使用`__constant__`关键字。例如：
- en: '[PRE45]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: To change the contents of the constant memory section at runtime, you simply
    use the `cudaCopyToSymbol` function call prior to invoking the GPU kernel. If
    you do not define the constant memory at either compile time or host runtime then
    the contents of the memory section are undefined.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 要在运行时更改常量内存部分的内容，只需在调用GPU内核之前使用`cudaCopyToSymbol`函数调用。如果在编译时或主机运行时都没有定义常量内存，则该内存部分的内容是未定义的。
- en: Constant memory caching
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常量内存缓存
- en: Compute 1.x devices
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算设备1.x
- en: On compute 1.x devices (pre-Fermi), constant memory has the property of being
    cached in a small 8K L1 cache, so *subsequent* accesses can be very fast. This
    is providing that there is some potential for data reuse in the memory pattern
    the application is using. It is also highly optimized for broadcast access such
    that threads accessing the same memory address can be serviced in a single cycle.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算设备1.x（Fermi之前）上，常量内存具有被缓存到一个小的8K L1缓存中的特性，因此*后续*访问可以非常快速。这是建立在应用程序使用的内存模式中有一定数据重用潜力的前提下的。它还针对广播访问进行了高度优化，以便访问相同内存地址的线程可以在一个周期内得到服务。
- en: With a 64 K segment size and an 8 K cache size, you have an 8:1 ratio of memory
    size to cache, which is really very good. If you can contain or localize accesses
    to 8 K chunks within this constant section you’ll achieve very good program performance.
    On certain devices you will find localizing the data to even smaller chunks will
    provide higher performance.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在 64 K 的段大小和 8 K 的缓存大小下，你的内存与缓存的比例为 8:1，这真的是非常好的。如果你能够将对 8 K 块的访问限制或局部化到这个常量区域内，你将获得非常好的程序性能。在某些设备上，你会发现将数据局部化到更小的块会提供更高的性能。
- en: With a nonuniform access to constant memory a cache miss results in *N* fetches
    from global memory in addition to the fetch from the constant cache. Thus, a memory
    pattern that exhibits poor locality and/or poor data reuse should not be accessed
    as constant memory. Also, each divergence in the memory fetch pattern causes serialization
    in terms of having to wait for the constant memory. Thus, a warp with 32 separate
    fetches to the constant cache would take at least 32 times longer than an access
    to a single data item. This would grow significantly if it also included cache
    misses.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果常量内存的访问是不均匀的，缓存未命中将导致从全局内存获取 *N* 次数据，此外还需要从常量缓存中获取数据。因此，展示出差的局部性和/或差的数据重用的内存模式不应当作为常量内存来访问。此外，每次内存访问模式的分歧都会导致在等待常量内存时的序列化。因此，一个具有
    32 次独立常量缓存访问的 warp 将需要比访问单个数据项至少多 32 倍的时间。如果它还包括缓存未命中，时间将显著增加。
- en: Single-cycle access is a huge improvement on the several hundred cycles required
    for a fetch from global memory. However, the several hundred–cycle access to global
    memory will likely be hidden by task switches to other warps, if there are enough
    available warps for the SM to execute. Thus, the benefit of using constant memory
    for its cache properties relies on the time taken to fetch data from global memory
    and the amount of data reuse the algorithm has. As with shared memory, the low-end
    devices have much less global memory bandwidth, so they benefit proportionally
    more from such techniques than the high-end devices.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 单周期访问比从全局内存获取数据所需的几百个周期要好得多。然而，如果 SM 有足够的可用 warps 来执行任务，几百个周期的全局内存访问可能会被任务切换隐藏。因此，使用常量内存的缓存特性带来的好处依赖于从全局内存获取数据的时间和算法的数据重用量。与共享内存一样，低端设备的全局内存带宽要小得多，因此它们比高端设备更能从这些技术中受益。
- en: Most algorithms can have their data broken down into “tiles” (i.e., smaller
    datasets) from a much larger problem. In fact, as soon as you have a problem that
    can’t physically fit on one machine, you have to do tiling of the data. The same
    tiling can be done on a multicore CPU with each one of the *N* cores taking 1/*N*
    of the data. You can think of each SM on the GPU as being a core on a CPU that
    is able to support hundreds of threads.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数算法可以将它们的数据分解为“瓦片”（即，更小的数据集），从一个更大的问题中拆分出来。事实上，只要你遇到一个无法物理地放入一台机器的问题，你就必须对数据进行瓦片化。相同的瓦片化可以在多核
    CPU 上进行，每个 *N* 个核心处理 1/*N* 的数据。你可以将 GPU 上的每个 SM 看作是一个支持数百个线程的 CPU 核心。
- en: Imagine overlaying a grid onto the data you are processing where the total number
    of cells, or blocks, in the grid equals the number of cores (SMs) you wish to
    split the data into. Take these SM-based blocks and further divide them into at
    least eight additional blocks. You’ve now decomposed your data area into *N* SMs,
    each of which is allocated *M* blocks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在你处理的数据上覆盖一个网格，网格中的单元格或块的总数等于你希望将数据拆分成的核心数（SM）。将这些基于 SM 的块进一步划分为至少八个额外的块。现在，你已经将数据区域分解成*N*个
    SM，每个 SM 分配 *M* 个块。
- en: In practice, this split is usually too large and would not allow for future
    generations of GPUs to increase the number of SMs or the number of available blocks
    and see any benefit. It also does not work well where the number of SMs is unknown,
    for example, when writing a commercial program that will be run on consumer hardware.
    The largest number of SMs per device to date has been 32 (GT200 series). The Kepler
    and Fermi range aimed at compyte have a maximum of 15 and 16 SMs respectively.
    The range designed primarily for gaming have up to 8 SMs.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种拆分通常太大，无法允许未来的 GPU 增加 SM 的数量或可用块的数量并获得任何好处。在 SM 数量未知的情况下，它也无法很好地工作，例如，在编写将运行在消费硬件上的商业程序时。迄今为止，每个设备的最大
    SM 数量为 32（GT200 系列）。针对计算的 Kepler 和 Fermi 系列分别最多有 15 和 16 个 SM。主要为游戏设计的系列最多有 8
    个 SM。
- en: One other important consideration is what interthread communication you need,
    if any. This can only reasonably be done using threads and these are limited to
    1024 per block on Fermi and Kepler, less on earlier devices. You can, of course,
    process multiple items of data per thread, so this is not such a hard limit as
    it might first appear.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是你是否需要线程间通信。如果需要，这通常只能通过线程实现，并且在Fermi和Kepler架构上，每个块的线程数限制为1024，早期设备的限制更少。当然，你可以在每个线程中处理多个数据项，因此这并不是一个像初看时那样严格的限制。
- en: Finally, you need to consider load balancing. Many of the early card releases
    of GPU families had non power of two numbers of SMs (GTX460 = 7, GTX260 = 30,
    etc.). Therefore, using too few blocks leads to too little granularity and thus
    unoccupied SMs in the final stages of computation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要考虑负载均衡。许多早期发布的GPU卡有非二的SM数量（GTX460 = 7，GTX260 = 30等）。因此，使用太少的块会导致粒度过小，从而在计算的最后阶段出现未占用的SM。
- en: Tiling, in terms of constant memory, means splitting the data into blocks of
    no more than 64 K each. Ideally, the tiles should be 8 K or less. Sometimes tiling
    involves having to deal with halo or ghost cells that occupy the boundaries, so
    values have to be propagated between tiles. Where halos are required, larger block
    sizes work better than smaller cells because the area that needs to communicated
    between blocks is much smaller.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在常量内存方面，瓦片化意味着将数据拆分成每个不超过64 K的块。理想情况下，瓦片大小应为8 K或更小。有时，瓦片化需要处理占据边界的光晕或幽灵单元，因此需要在瓦片之间传播数值。当需要光晕时，较大的块大小比较小的单元更有效，因为需要在块之间通信的区域要小得多。
- en: When using tiling there is actually quite a lot to think about. Often the best
    solution is simply to run through all combinations of number of threads, elements
    processed per thread, number of blocks, and tile widths, and search for the optimal
    solution for the given problem. We look at how to do this in [Chapter 9](CHP009.html)
    on optimization.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用瓦片化时，实际上有很多因素需要考虑。通常，最佳解决方案是简单地遍历所有线程数、每个线程处理的元素数、块数和瓦片宽度的组合，并寻找适合给定问题的最佳方案。我们将在[第9章](CHP009.html)中讨论如何进行优化。
- en: Compute 2.x devices
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算2.x设备
- en: On Fermi (compute 2.x) hardware and later, there is a level two (L2) cache.
    Fermi uses an L2 cache shared between each SM. All memory accesses are cached
    automatically by the L2 cache. Additionally, the L1 cache size can be increased
    from 16 K to 48 K by sacrificing 32 K of the shared memory per SM. Because all
    memory is cached on Fermi, how constant memory is used needs some consideration.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fermi（计算2.x）及更高版本的硬件上，有一个二级缓存（L2缓存）。Fermi使用的是一个在每个SM之间共享的L2缓存。所有内存访问都会自动缓存到L2缓存中。此外，通过牺牲每个SM的32
    K共享内存，L1缓存的大小可以从16 K增加到48 K。由于Fermi上所有内存都被缓存，因此使用常量内存时需要仔细考虑。
- en: Fermi, unlike compute 1.x devices, allows *any* constant section of data to
    be treated as constant memory, even if it is not explicitly declared as such.
    Constant memory on 1.x devices has to be explicitly managed with special-purpose
    calls like `cudaMemcpyToSymbol` or declared at compile time. With Fermi, any nonthread-based
    access to an area of memory declared as constant (simply with the standard `const`
    keyword) goes through the constant cache. By nonthread-based access, this is an
    access that does not include `threadIdx.x` in the array indexing calculation.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算1.x设备不同，Fermi允许*任何*常量数据部分被视为常量内存，即使它没有显式声明为常量。1.x设备上的常量内存必须通过特殊调用（如`cudaMemcpyToSymbol`）显式管理，或者在编译时声明。对于Fermi，任何声明为常量的内存区域（只需使用标准`const`关键字）都将通过常量缓存进行访问。非线程基础的访问是指不包含`threadIdx.x`的数组索引计算的访问。
- en: If you need access to constant data on a per-thread-based access, then you need
    to use the compile time (`__constant__`) or runtime function (`cudaMemcpyToSymbol`)
    as with compute 1.x devices.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要基于每个线程访问常量数据，则需要使用编译时（`__constant__`）或运行时函数（`cudaMemcpyToSymbol`），就像计算1.x设备一样。
- en: However, be aware that the L2 cache will still be there and is much larger than
    the constant cache. If you are implementing a tiling algorithm that needs halo
    or ghost cells between blocks, the solution will often involve copying the halo
    cells into constant or shared memory. Due to Fermi’s L2 cache, this strategy will
    usually be slower than simply copying the tiled cells to shared or constant memory
    and then accessing the halo cells from global memory. The L2 cache will have collected
    the halo cells from the prior block’s access of the memory. Therefore, the halo
    cells are quickly available from the L2 cache and come into the device much quicker
    than you would on compute 1.x hardware where a global memory fetch would have
    to go all the way out to the global memory.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，L2缓存仍然存在，而且比常量缓存大得多。如果你正在实现一个需要在块之间传递halo或ghost单元的平铺算法，解决方案通常会涉及将halo单元复制到常量或共享内存中。由于Fermi的L2缓存，这种策略通常会比直接将平铺单元复制到共享或常量内存中，然后从全局内存访问halo单元更慢。L2缓存会从前一个块访问内存时收集到halo单元。因此，halo单元可以快速从L2缓存中获得，并比在compute
    1.x硬件上更快地进入设备，因为在那种硬件上，全局内存提取必须完全访问全局内存。
- en: Constant memory broadcast
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常量内存广播
- en: Constant memory has one very useful feature. It can be used for the purpose
    of distributing, or broadcasting, data to every thread in a warp. This broadcast
    takes place in just a *single* cycle, making this ability very useful. In comparison,
    a coalesced access to global memory on compute 1.x hardware would require a memory
    fetch taking hundreds of cycles of latency to complete. Once it has arrived from
    the memory subsystem, it would be distributed in the same manner to all threads,
    but only after a significant wait for the memory subsystem to provide the data.
    Unfortunately, this is an all too common problem, in that memory speeds have failed
    to keep pace with processor clock speeds.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存有一个非常有用的特点。它可以用于将数据分发或广播到warp内的每个线程。这个广播只需要一个*周期*，使得这一功能非常有用。相比之下，在compute
    1.x硬件上对全局内存的合并访问需要经历几百个周期的延迟才能完成。一旦从内存子系统获取数据后，它会以相同的方式分发给所有线程，但只有在经过显著等待内存子系统提供数据后才能开始。遗憾的是，这是一个非常普遍的问题，因为内存速度未能跟上处理器时钟速度的提升。
- en: Think of fetching data from global memory in the same terms as you might consider
    fetching data from disk. You would never write a program that fetched the data
    from disk multiple times, because it would be far too slow. You have to think
    about what data to fetch, and once you have it, how to reuse that data as much
    as possible, while some background process triggers the next block of data to
    be brought in from the disk.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 将从全局内存获取数据的过程视为与从磁盘获取数据类似的操作。你永远不会编写一个程序，使其多次从磁盘获取数据，因为那样会非常慢。你需要考虑获取哪些数据，一旦获得数据后，如何尽可能地重用这些数据，同时一些后台进程触发下一块数据从磁盘加载。
- en: By using the broadcast mechanism, which is also present on Fermi for L2 cache–based
    accesses, you can distribute data very quickly to multiple threads within a warp.
    This is particularly useful where you have some common transformation being performed
    by all threads. Each thread reads element *N* from constant memory, which triggers
    a broadcast to all threads in the warp. Some processing is performed on the value
    fetched from constant memory, perhaps in combination with a read/write to global
    memory. You then fetch element *N* + 1 from constant memory, again via a broadcast,
    and so on. As the constant memory area is providing almost L1 cache speeds, this
    type of algorithm works well.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用广播机制——这在Fermi架构中也存在，用于基于L2缓存的访问——你可以非常快速地将数据分发到warp内的多个线程。这在所有线程都执行某个共同转换时尤其有用。每个线程从常量内存中读取元素*N*，这会触发对warp内所有线程的广播。然后对从常量内存获取的值进行一些处理，可能会结合对全局内存的读/写操作。接着你再从常量内存中获取元素*N*
    + 1，仍然通过广播进行，以此类推。由于常量内存区域几乎提供了L1缓存的速度，这种算法效果很好。
- en: 'However, be aware that if a constant is really a literal value, it is better
    to define it as a literal value using a `#define` statement, as this frees up
    constant memory. So don’t place literals like PI into constant memory, rather
    define them as literal `#define` instead. In practice, it makes little difference
    in speed, only memory usage, as to which method is chosen. Let’s look at an example
    program:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，如果常量确实是一个字面量值，最好使用`#define`语句将其定义为字面量值，因为这样可以释放常量内存。因此，不要将像PI这样的字面量放入常量内存，而应将其定义为字面量`#define`。实际上，选择哪种方法对速度几乎没有影响，只有内存使用会有所不同。让我们来看一个示例程序：
- en: '[PRE46]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '`  }`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`  }`'
- en: '[PRE47]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`   char device_prefix[261];`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`   char device_prefix[261];`'
- en: '[PRE48]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`   // printf("\nConst Elapsed time: %.3fms", delta_time2);`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`   // printf("\nConst Elapsed time: %.3fms", delta_time2);`'
- en: '[PRE49]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This program consists of two GPU kernels, `const_test_gpu_literal` and `const_test_gpu_const`.
    Notice how each is declared with the `__global__` prefix to say this function
    has public scope. Each of these kernels fetches some data as either constant data
    or literal data within the `for` loop, and uses it to manipulate the local variable
    `d`. It then writes this manipulated value out to global memory. This is necessary
    only to avoid the compiler optimizing away the code.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序由两个GPU内核组成，`const_test_gpu_literal`和`const_test_gpu_const`。请注意，每个内核都以`__global__`前缀声明，表示此函数具有公共作用域。这些内核中的每一个都会在`for`循环中提取一些数据，作为常量数据或字面量数据，并使用它来操作局部变量`d`。然后，它将这个操作过的值写入全局内存。这样做是必要的，以避免编译器优化掉这些代码。
- en: The next section of code gets the number of CUDA devices present and iterates
    through the devices using the `cudaSetDevice` call. Note that this is possible
    because at the end of the loop the host code calls `cudaDeviceReset` to clear
    the current context.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分获取当前存在的CUDA设备数量，并使用`cudaSetDevice`调用遍历这些设备。请注意，这是可能的，因为在循环结束时，主机代码调用`cudaDeviceReset`来清除当前上下文。
- en: Having set the device, the program allocates some global memory and creates
    two events, a start and a stop timer event. These events are fed into the execution
    stream, along with the kernel call. Thus, you end up with the stream containing
    a start event, a kernel call, and a stop event. These events would normally happen
    asynchronously with the CPU, that is, they do not block the execution of the CPU
    and execute in parallel. This causes some problems when trying to do timing, as
    a CPU timer would see no elapsed time. The program, therefore, calls `cudaEventSynchronize`
    to wait on the last event, the kernel stop event, to complete. It then calculates
    the delta time between the start and stop events and thus knows the execution
    time of the kernel.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好设备后，程序分配一些全局内存并创建两个事件，一个开始计时事件和一个停止计时事件。这些事件被送入执行流中，与内核调用一起。因此，您最终会得到包含开始事件、内核调用和停止事件的流。这些事件通常会与CPU异步发生，即它们不会阻塞CPU的执行，并且是并行执行的。当尝试进行计时时，会出现一些问题，因为CPU计时器不会看到经过的时间。因此，程序调用`cudaEventSynchronize`来等待最后一个事件，即内核停止事件完成。然后，它计算开始和停止事件之间的时间差，从而知道内核的执行时间。
- en: 'This is repeated for the constant and literal kernels, including the execution
    of a warm-up call to avoid any initial effects of filling any caches. The results
    are shown as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于常量和字面量内核重复执行，包括执行预热调用以避免任何初始的缓存填充效果。结果如下所示：
- en: '[PRE50]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`ID:0 GeForce GTX 470:Constant version is faster by: 14.30ms (C=345.23ms, L=330.94ms)`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '`ID:0 GeForce GTX 470:常量版本比：14.30ms（C=345.23ms, L=330.94ms）`'
- en: '[PRE51]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'What is interesting to note is that there is very little, if any, difference
    in the execution time if you look at this as a percentage of the total execution
    time. Consequently we see a fairly random distribution as to which version, the
    constant or the literal, is faster. Now how does this compare with using global
    memory? To test this, we simply replace the literal kernel with one that uses
    global memory as shown in the following:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，如果将其作为总执行时间的百分比来查看，执行时间几乎没有区别。因此，我们会看到常量版本和字面量版本哪个更快的分布非常随机。那么，这与使用全局内存相比如何呢？为了测试这一点，我们简单地将字面量内核替换为使用全局内存的内核，如下所示：
- en: '[PRE52]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`  data[tid] = d;`'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`  data[tid] = d;`'
- en: '[PRE53]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Notice that to declare a global variable in the GPU memory space, you simply
    prefix it by a `__device__` specifier. We have fairly much the same kernel as
    before, reading four values from memory *N* times. However, in this example, I’ve
    had to reduce `KERNEL_LOOP` from 64 K down to 4 K as otherwise the kernel takes
    a *very* long time to execute. So when comparing the timings, remember we’re doing
    just one-sixteenth of the work. The results are interesting.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 GPU 内存空间中声明全局变量时，您只需在前面加上 `__device__` 修饰符。我们有一个与之前非常相似的内核，从内存中读取四个值 *N*
    次。然而，在这个例子中，我不得不将 `KERNEL_LOOP` 从 64K 减少到 4K，否则内核执行时间会非常长。所以在比较时间时，请记住我们只做了原工作量的六分之一。结果很有趣。
- en: '[PRE54]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Notice that on every generation of hardware the constant cache performs better
    than the global memory access. On the compute 1.1 hardware (9800GT) you have a
    40:1 speedup. On the compute 1.3 hardware (GTX260) you have a 3:1 speedup. On
    the compute 2.0 hardware (GTX470) you have a 1.8:1 speedup. On the compute 2.1
    hardware (GTX460) you have a 1.6:1 speedup.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在每一代硬件上，常量缓存的表现都优于全局内存访问。在计算 1.1 硬件（9800GT）上，速度提升为 40:1。在计算 1.3 硬件（GTX260）上，速度提升为
    3:1。在计算 2.0 硬件（GTX470）上，速度提升为 1.8:1。在计算 2.1 硬件（GTX460）上，速度提升为 1.6:1。
- en: What is perhaps most interesting is that the Fermi devices (GTX460 and GTX470)
    would appear to show significant speedups using the constant cache, rather than
    the L1/L2 cache used for global memory access. Thus, even with Fermi, the use
    of constant cache appears to significantly improve throughput. However, is this
    really the case?
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最有趣的是，Fermi 设备（GTX460 和 GTX470）在使用常量缓存时，速度提升显著，相较于使用 L1/L2 缓存进行全局内存访问的情况。因此，即使是
    Fermi，使用常量缓存似乎也能显著提高吞吐量。然而，真的是这样吗？
- en: 'To examine this further, you need to look at the PTX (virtual assembly) code
    generated. To see this, you need to use the `-keep` option for the compiler. For
    the constant kernel, the PTX code for this single function is shown as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步研究，您需要查看生成的 PTX（虚拟汇编）代码。要查看此内容，您需要在编译器中使用 `-keep` 选项。对于常量内核，这个单独函数的 PTX
    代码如下所示：
- en: '[PRE55]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '` .loc 16 50 0`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '` .loc 16 50 0`'
- en: '[PRE56]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Understanding the exact meaning of the assembly code is not necessary. We’ve
    shown the function in full to give you some idea of how a small section of C code
    actually expands to the assembly level. PTX code uses the format
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 理解汇编代码的确切含义并不是必须的。我们展示了完整的函数，以便让您大致了解一小段 C 代码是如何展开成汇编级别的。PTX 代码使用的格式是：
- en: <operator> <destination register> <source reg A> <source reg B>
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: <操作符> <目标寄存器> <源寄存器 A> <源寄存器 B>
- en: Thus,
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '[PRE57]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'takes the value in register 15 and does a 32-bit, bitwise `xor` operation with
    the literal value 1431655765\. It then stores the result in register 16\. Notice
    the numbers highlighted in bold within the previous PTX listing. The compiler
    has replaced the constant values used on the kernel with literals. This is why
    it’s always worthwhile looking into what is going on if the results are not what
    are expected. An extract of the GMEM PTX code for comparison is as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 取寄存器 15 中的值，并与字面值 1431655765 进行 32 位按位 `xor` 操作。然后将结果存储到寄存器 16 中。注意，在之前的 PTX
    列表中加粗的数字。编译器已将内核中使用的常量值替换为字面值。这就是为什么如果结果与预期不符，值得深入查看发生了什么的原因。为了做对比，以下是 GMEM PTX
    代码的摘录：
- en: '[PRE58]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The program is now loading a value from global memory. The constant version
    was not actually doing any memory reads at all. The compiler had done a substitution
    of the constant values for literal values when translating the C code into PTX
    assembly. This can be solved by declaring the constant version as an array, rather
    than a number of scalar variables. Thus, the new function becomes:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在正在从全局内存加载一个值。常量版本实际上并没有进行任何内存读取。编译器在将 C 代码转换成 PTX 汇编时，已经将常量值替换为字面值。这可以通过将常量版本声明为数组，而不是一组标量变量来解决。因此，新的函数变为：
- en: '[PRE59]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In the generated PTX code you now see
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的 PTX 代码中，您现在看到：
- en: '[PRE60]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: You now have an indexed address from the start of the constant array, which
    is what you’d expect to see. How does this affect the results?
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经从常量数组的起始位置获得了一个索引地址，这是您应该看到的。这个结果如何影响最终表现？
- en: '[PRE61]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now we see the results we’d expect to see: On Fermi (compute 2.x hardware),
    global memory accesses that are within the L1 cache and constant memory accesses
    are the same speed. Constant memory, however, shows significant benefits on compute
    1.x devices where the global memory is not cached.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到了预期的结果：在Fermi（compute 2.x硬件）上，L1缓存内的全局内存访问和常量内存访问速度相同。然而，常量内存在compute
    1.x设备上显示出显著的优势，因为全局内存没有缓存。
- en: Constant memory updates at runtime
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常量内存在运行时更新
- en: Constant memory of the GPU is not really constant memory, in that there is no
    dedicated special area of memory set aside for constant memory. The 64 K limit
    is exactly a 16-bit offset, allowing very quick 16-bit addressing. This presents
    some opportunities and some problems. First, constant memory can be updated in
    chunks or tiles of up to 64 K at a time. This is done with the `cudaMemcpyToSymbol`
    API call. Revising our constant program somewhat, let’s look at how this works.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: GPU的常量内存实际上并不是真正的常量内存，因为并没有专门为常量内存预留出独立的特殊内存区域。64 K的限制实际上是一个16位的偏移量，允许非常快速的16位寻址。这既带来了一些机会，也带来了一些问题。首先，常量内存可以一次更新最多64
    K的块或切片。这个操作是通过`cudaMemcpyToSymbol` API调用完成的。稍微修改我们的常量程序，让我们来看一下它是如何工作的。
- en: '[PRE62]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '`#define CUDA_CALL(x) {const cudaError_t a = (x); if (a != cudaSuccess) { printf("\nCUDA
    Error: %s (err_num=%d) \n", cudaGetErrorString(a), a); cudaDeviceReset(); assert(0);}
    }`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`#define CUDA_CALL(x) {const cudaError_t a = (x); if (a != cudaSuccess) { printf("\nCUDA
    Error: %s (err_num=%d) \n", cudaGetErrorString(a), a); cudaDeviceReset(); assert(0);}
    }`'
- en: '[PRE63]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '` char ch;`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '` char ch;`'
- en: '[PRE64]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '`  CUDA_CALL(cudaEventCreate(&kernel_start1));`'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '` CUDA_CALL(cudaEventCreate(&kernel_start1));`'
- en: '[PRE65]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`   CUDA_CALL(cudaEventRecord(kernel_start2,0));`'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '`   CUDA_CALL(cudaEventRecord(kernel_start2,0));`'
- en: '[PRE66]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Notice how the `cudaMemcpyToSymbol` call works. You can copy to any named global
    symbol on the GPU, regardless of whether that symbol is in global memory or constant
    memory. Thus, if you chunk the data to 64 K chunks, you can access it from the
    constant cache. This is very useful if all threads are accessing the same data
    element, as you get the broadcast and cache effect from the constant memory section.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`cudaMemcpyToSymbol`调用是如何工作的。你可以将数据复制到GPU上任何命名的全局符号，不管这个符号是在全局内存还是常量内存中。因此，如果你将数据分成64
    K的块，就可以从常量缓存中访问它。如果所有线程都访问相同的数据元素，这非常有用，因为你可以从常量内存部分获得广播和缓存效果。
- en: Notice also that the memory allocation, creation of events, destruction of the
    events and freeing of device memory is now done outside the main loop. CUDA API
    calls such as these are actually very costly in terms of CPU time. The CPU load
    of this program drops considerably with this simple change. Always try to set
    up everything at the start and destroy or free it at the end. Never do this in
    the loop body or it will greatly slow down the application.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，内存分配、事件创建、事件销毁和设备内存释放现在都在主循环外部完成。像这样的CUDA API调用在CPU时间方面实际上是非常耗费资源的。通过这一简单的更改，程序的CPU负载显著降低。始终尽量在开始时设置所有内容，并在结束时销毁或释放它。绝不要在循环体内做这些操作，否则会极大地降低应用程序的运行速度。
- en: Constant question
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常量问题
- en: 1. If you have a data structure that is 16 K in size and exhibits a random pattern
    of access per block but a unified access pattern per warp, would it be best to
    place it into registers, constant memory, or shared memory? Why?
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 如果你有一个大小为16 K的数据结构，每个块的访问模式是随机的，但每个warp的访问模式是统一的，那么最好将它放入寄存器、常量内存还是共享内存？为什么？
- en: Constant answer
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常量答案
- en: 1. Although it is a little tricky to get a large array into registers, tiling
    into blocks of registers per thread would allow for the fastest access, regardless
    of access pattern. However, you are limited to 32 K (compute < 1.2), 64 K (compute
    1,2, 1.3), or 128 K (compute 2.x) or 256 K (compute 3.x) register space per SM.
    You have to allocate some of this to working registers on a per-thread basis.
    On Fermi you can have a maximum of 64 registers per thread, so with 32 allocated
    to data and 32 as the working set, you would have just 128 active threads, or
    four active warps. As soon as the program accessed off-chip memory (e.g., global
    memory) the latency may stall the SM. Therefore, the kernel would need a high
    ratio of operations on the register block to make this a good solution.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 尽管将大数组放入寄存器有点棘手，但将其按线程划分为寄存器块可以实现最快的访问，无论访问模式如何。然而，你的每个 SM 只能分配 32 K（计算 <
    1.2）、64 K（计算 1.2、1.3）、128 K（计算 2.x）或 256 K（计算 3.x）寄存器空间。你必须为每个线程分配其中的一部分作为工作寄存器。在
    Fermi 中，每个线程最多可以使用 64 个寄存器，因此，若分配 32 个用于数据，32 个作为工作集，则你只能有 128 个活跃线程，或四个活跃 warp。一旦程序访问了外部内存（例如全局内存），延迟可能会导致
    SM 停滞。因此，内核需要在寄存器块上执行较高比例的操作，以便将其作为一个好的解决方案。
- en: Placing it into shared memory would likely be the best case, although depending
    on the actual access pattern you may see shared memory bank conflicts. The uniform
    warp access would allow broadcast from the shared memory to all the threads in
    a single warp. It is only in the case where the warp from two blocks accessed
    the same bank that would you get a shared memory conflict.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 将其放入共享内存可能是最好的选择，尽管根据实际的访问模式，你可能会看到共享内存银行冲突。统一的 warp 访问将允许从共享内存向单个 warp 中的所有线程广播。只有当来自两个块的
    warp 访问同一个内存银行时，才会发生共享内存冲突。
- en: However, 16 K of shared memory would consume entirely the shared memory in one
    SM on compute 1.x devices and limit you to three blocks maximum on compute 2.x/3.x
    hardware.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在计算 1.x 设备上，16 K 的共享内存将完全占用一个 SM 中的共享内存，并且在计算 2.x/3.x 硬件上最多只能限制为三个块。
- en: Constant memory would also be a reasonable choice on compute 1.x devices. Constant
    memory would have the benefit of broadcast to the threads. However, the 16 K of
    data may well swamp the cache memory. Also, and more importantly, the constant
    cache is optimized for linear access, that is, it fetches cache lines upon a single
    access. Thus, accesses near the original access are cached. Accesses to a noncached
    cache line result in a cache miss penalty that is larger than a fetch to global
    memory without a cache miss.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存对于计算 1.x 设备来说也是一个合理的选择。常量内存的好处在于它能向线程广播。然而，16 K 的数据可能会淹没缓存内存。而且，更重要的是，常量缓存针对线性访问进行了优化，即它在单次访问时就会提取缓存行。因此，接近原始访问的访问会被缓存。访问未缓存的缓存行将导致较大的缓存未命中的惩罚，甚至可能比从全局内存访问没有缓存未命中的数据更糟。
- en: Global memory may well be faster on compute 2.x/3.x devices, as the unified
    access per warp should be translated by the compiler into the uniform warp-level
    global memory access. This provides the broadcast access constant memory would
    have provided on compute 1.x devices.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 2.x/3.x 设备上，全局内存可能会更快，因为每个 warp 的统一访问应由编译器转换为统一的 warp 级别的全局内存访问。这提供了在计算
    1.x 设备上常量内存所提供的广播访问。
- en: Global Memory
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局内存
- en: Global memory is perhaps the most interesting of the memory types in that it’s
    the one you absolutely have to understand. GPU global memory is global because
    it’s writable from both the GPU and the CPU. It can actually be accessed from
    any device on the PCI-E bus. GPU cards can transfer data to and from one another,
    directly, without needing the CPU. This peer-to-peer feature, introduced in the
    CUDA 4.x SDK, is not yet supported on all platforms. Currently, the Windows 7/Vista
    platforms are only supported on Tesla hardware, via the TCC driver model. Those
    using Linux or Windows XP can use this feature with both consumer and Tesla cards.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 全局内存可能是最有趣的内存类型，因为它是你必须完全理解的。GPU 的全局内存之所以被称为全局内存，是因为它可以同时被 GPU 和 CPU 写入。它实际上可以从
    PCI-E 总线上的任何设备访问。GPU 卡之间可以直接相互传输数据，而无需 CPU 的介入。这一点是 CUDA 4.x SDK 引入的对等特性，但并非所有平台都支持。目前，Windows
    7/Vista 平台仅通过 TCC 驱动模型在 Tesla 硬件上得到支持。使用 Linux 或 Windows XP 的用户可以在消费级和 Tesla 卡上都使用此特性。
- en: 'The memory from the GPU is accessible to the CPU host processor in one of three
    ways:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 的内存可以通过以下三种方式之一被 CPU 主机处理器访问：
- en: • Explicitly with a blocking transfer.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: • 明确地通过阻塞传输。
- en: • Explicitly with a nonblocking transfer.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: • 明确地通过非阻塞传输。
- en: • Implicitly using zero memory copy.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: • 隐式地使用零内存拷贝。
- en: The memory on the GPU device sits on the other side of the PCI-E bus. This is
    a bidirectional bus that, in theory, supports transfers of up to 8 GB/s (PCI-E
    2.0) in each direction. In practice, the PCI-E bandwidth is typically 4–5 GB/s
    in each direction. Depending on the hardware you are using, nonblocking and implicit
    memory transfers may not be supported. We’ll look at these issues in more detail
    in [Chapter 9](CHP009.html).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: GPU设备上的内存位于PCI-E总线的另一侧。这是一个双向总线，理论上每个方向支持最高8 GB/s的传输（PCI-E 2.0）。实际上，PCI-E带宽通常在每个方向为4–5
    GB/s。根据所使用的硬件，可能不支持非阻塞和隐式内存传输。我们将在[第9章](CHP009.html)中更详细地讨论这些问题。
- en: The usual model of execution involves the CPU transferring a block of data to
    the GPU, the GPU kernel processing it, and then the CPU initiating a transfer
    of the data back to the host memory. A slightly more advanced model of this is
    where we use streams (covered later) to overlap transfers and kernels to ensure
    the GPU is always kept busy, as shown in [Figure 6.16](#F0085).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的执行模型是CPU将数据块传输到GPU，GPU内核处理数据，然后CPU启动将数据传回主机内存的传输。稍微更高级的模型是使用流（稍后介绍）来重叠传输和内核操作，以确保GPU始终保持忙碌状态，如[图6.16](#F0085)所示。
- en: '![image](../images/F000065f06-16-9780124159334.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-16-9780124159334.jpg)'
- en: Figure 6.16 Overlapping kernel and memory transfers.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 内核与内存传输的重叠。
- en: Once you have the data in the GPU, the question then becomes how do you access
    it efficiently on the GPU? Remember the GPU can be rated at over 3 teraflops in
    terms of compute power, but typically the main memory bandwidth is in the order
    of 190 GB/s down to as little as 25 GB/s. By comparison, a typical Intel I7 Nehalem
    or AMD Phenom CPU achieves in the order of 25–30 GB/s, depending on the particular
    device speed and width of the memory bus used.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据进入GPU，接下来要解决的问题是如何高效地在GPU上访问它？请记住，GPU的计算能力可以超过3 teraflops，但通常主内存带宽在190 GB/s到25
    GB/s之间。相比之下，典型的Intel I7 Nehalem或AMD Phenom CPU的带宽约为25–30 GB/s，这取决于具体设备的速度和内存总线的宽度。
- en: Graphics cards use high-speed GDDR, or graphics dynamic memory, which achieves
    very high sustained bandwidth, but like all memory, has a high latency. Latency
    is the time taken to return the first byte of the data access. Therefore, in the
    same way that we can pipeline kernels, as is shown in [Figure 6.16](#F0085), the
    memory accesses are pipelined. By creating a ratio of typically 10:1 of threads
    to number of memory accesses, you can hide memory latency, but only if you access
    global memory in a pattern that is coalesced.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 显卡使用高速GDDR（图形动态内存），它能够实现非常高的持续带宽，但和所有内存一样，它也有较高的延迟。延迟是指返回数据访问第一个字节所花费的时间。因此，正如我们可以流水线化内核操作一样，正如[图6.16](#F0085)所示，内存访问也是流水线化的。通过创建线程与内存访问次数之间通常为10:1的比例，你可以隐藏内存延迟，但前提是你按照共聚模式访问全局内存。
- en: So what is a coalescable pattern? This is where all the threads access a contiguous
    and aligned memory block, as shown in [Figure 6.17](#F0090). Here we have shown
    `Addr` as the logical address offset from the base location, assuming we are accessing
    byte-based data. TID represents the thread number. If we have a one-to-one sequential
    and aligned access to memory, the address accesses of each thread are combined
    together and a single memory transaction is issued. Assuming we’re accessing a
    single precision float or integer value, each thread will be accessing 4 bytes
    of memory. Memory is coalesced on a warp basis (the older G80 hardware uses half
    warps), meaning we get 32 × 4 = 128 byte access to memory.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是共聚模式呢？这就是所有线程访问一个连续且对齐的内存块，如[图6.17](#F0090)所示。在这里，我们假设访问的是基于字节的数据，并将`Addr`表示为从基址位置开始的逻辑地址偏移量。TID表示线程编号。如果我们有一对一的顺序和对齐的内存访问，每个线程的地址访问会合并在一起，从而发起一个单一的内存事务。假设我们访问的是单精度浮点数或整数值，每个线程将访问4个字节的内存。内存是基于warp进行共聚的（较旧的G80硬件使用半warp），意味着我们可以实现32
    × 4 = 128字节的内存访问。
- en: '![image](../images/F000065f06-17-9780124159334.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-17-9780124159334.jpg)'
- en: Figure 6.17 Addresses accessed by thread ID.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 线程ID访问的地址。
- en: Coalescing sizes supported are 32, 64, and 128 bytes, meaning warp accesses
    to byte, 16- and 32-bit data will always be coalesced if the access is a sequential
    pattern and aligned to a 32-byte boundary.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的共聚大小为32、64和128字节，这意味着如果访问模式是顺序的并且对齐到32字节边界，warp访问字节、16位和32位数据时将始终进行共聚。
- en: 'The alignment is achieved by using a special malloc instruction, replacing
    the standard `cudaMalloc` with `cudaMallocPitch`, which has the following syntax:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐是通过使用特殊的 malloc 指令实现的，将标准的 `cudaMalloc` 替换为 `cudaMallocPitch`，其语法如下：
- en: '[PRE67]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This translates to `cudaMallocPitch` (pointer to device memory pointer, pointer
    to pitch, desired width of the row in bytes, height of the array in bytes).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这会转化为 `cudaMallocPitch`（指向设备内存指针、指向步幅的指针、期望的行宽（字节）、数组的高度（字节））。
- en: Thus, if you have an array of 100 rows of 60 float elements, using the conventional
    `cudaMalloc`, you would allocate 100 × 60 × sizeof(float) bytes, or 100 × 60 ×
    4 = 24,000 bytes. Accessing array index `[1][0]` (i.e., row one, element zero)
    would result in noncoalesced access. This is because the length of a single row
    of 60 elements would be 240 bytes, which is of course not a power of two.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你有一个包含100行60个浮动元素的数组，使用常规的 `cudaMalloc`，你需要分配100 × 60 × sizeof(float) 字节，或者100
    × 60 × 4 = 24,000 字节。访问数组索引 `[1][0]`（即第一行，元素零）将导致非合并访问。这是因为单行60个元素的长度是240字节，而这显然不是2的幂。
- en: The first address in the series of addresses from each thread would not meet
    the alignment requirements for coalescing. Using the `cudaMallocPitch` function
    the size of each row is padded by an amount necessary for the alignment requirements
    of the given device ([Figure 6.18](#F0095)). In our example, it would in most
    cases be extended to 64 elements per row, or 256 bytes. The pitch the device actually
    uses is returned in the pitch parameters passed to `cudaMallocPitch`.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程系列中第一个地址将不满足合并的对齐要求。使用 `cudaMallocPitch` 函数后，每行的大小会通过必要的填充来满足给定设备的对齐要求（[图6.18](#F0095)）。在我们的例子中，它通常会扩展为每行64个元素，或256字节。设备实际使用的步幅会在传递给
    `cudaMallocPitch` 的步幅参数中返回。
- en: '![image](../images/F000065f06-18-9780124159334.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-18-9780124159334.jpg)'
- en: Figure 6.18 Padding achieved with `cudaMallocPitch`.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 使用 `cudaMallocPitch` 实现的填充。
- en: Let’s have a look at how this works in practice. Nonaligned accesses result
    in multiple memory fetches being issued. While waiting for a memory fetch, all
    threads in a warp are stalled until *all* memory fetches are returned from the
    hardware. Thus, to achieve the best throughput you need to issue a small number
    of large memory fetch requests, as a result of aligned and sequential coalesced
    accesses.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个在实践中是如何工作的。非对齐的访问会导致发出多个内存取值请求。在等待内存取值时，warp中的所有线程都会被停顿，直到*所有*内存取值从硬件返回。因此，为了实现最佳的吞吐量，你需要发出少量的大内存取值请求，这样可以通过对齐和顺序合并访问来达到结果。
- en: So what happens if you have data that is interleaved in some way, for example,
    a structure?
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果你的数据以某种方式交织在一起，例如，一个结构体，会发生什么呢？
- en: '[PRE68]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[Figure 6.19](#F0100) shows how C will lay this structure out in memory.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.19](#F0100)显示了C语言如何在内存中布局这个结构。'
- en: '![image](../images/F000065f06-19-9780124159334.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-19-9780124159334.jpg)'
- en: Figure 6.19 Array elements in memory.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 内存中的数组元素。
- en: 'Elements are laid out in memory in the sequence in which they are defined within
    the structure. The access pattern for such a structure is shown in [Figure 6.20](#F0105).
    As you can see from the figure, the addresses of the structure elements are not
    contiguous in memory. This means you get no coalescing and the memory bandwidth
    suddenly drops off by an order of magnitude. Depending on the size of our data
    elements, it may be possible to have each thread read a larger value and then
    internally within the threads mask off the necessary bits. For example, if you
    have byte-based data you can do the following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 元素在内存中按其在结构体中定义的顺序排列。该结构的访问模式如[图6.20](#F0105)所示。如图所示，结构体元素的地址在内存中并不连续。这意味着你无法进行合并访问，内存带宽突然下降一个数量级。根据数据元素的大小，可能每个线程读取较大的值，然后在线程内部屏蔽掉必要的位。例如，如果你有基于字节的数据，你可以执行以下操作：
- en: '![image](../images/F000065f06-20-9780124159334.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-20-9780124159334.jpg)'
- en: Figure 6.20 Words accessed by thread (no coalescing).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 线程访问的单词（无合并）。
- en: '[PRE69]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: It’s also possible to maintain the one thread to one data element mapping by
    simply treating the array of structure elements as an array of words. We can then
    allocate one thread to each element of the structure. This type of solution is,
    however, not suitable if there is some data flow relationship between the structure
    members, so thread 1 needs the *x*, *y*, and *z* coordinate of a structure, for
    example. In this case, it’s best to reorder the data, perhaps in the loading or
    transfer phase on the CPU, into *N* discrete arrays. In this way, the arrays individually
    sit concurrently in memory. We can simply access array `a`, `b`, `c`, or `d` instead
    of the `struct->a` notation we’d use with a structure dereference. Instead of
    an interleaved and uncoalesced pattern, we get four coalesced accesses from each
    thread into different memory regions, maintaining optimal global memory bandwidth
    usage.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过简单地将结构体元素的数组视为一个单词数组，来保持一对一线程与数据元素的映射。我们可以将一个线程分配给结构体的每个元素。然而，如果结构体成员之间存在数据流关系，这种解决方案并不适用，例如，线程
    1 需要结构体的 *x*、*y* 和 *z* 坐标。在这种情况下，最好在 CPU 的加载或传输阶段重新排序数据，或许是将其重组为 *N* 个离散数组。这样，数组将并行地存储在内存中。我们可以简单地访问数组
    `a`、`b`、`c` 或 `d`，而不是使用结构体解引用的 `struct->a` 表示法。这样，我们就避免了交错和未合并的模式，每个线程将从不同的内存区域获取四个合并的访问，从而保持了最佳的全局内存带宽利用率。
- en: Let’s look at an example of global memory reads. In this example, we’ll sum
    the values of all the elements in the structure using the two methods. First,
    we’ll add all the values from an array of structures and then from a structure
    of arrays.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 看看全局内存读取的一个例子。在这个例子中，我们将使用两种方法对结构体中所有元素的值求和。首先，我们从结构体数组中加上所有值，然后从数组结构体中加上所有值。
- en: '[PRE70]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In this section of code, we declare two types; the first is `INTERLEAVED_T`,
    an array of structures of which the members are `a` to `d`. We then declare `NON_INTERLEAVED_T`
    as a structure that contains four arrays, `a` to `d`. As the types are named,
    with the first one we expect the data to be interleaved in memory. With the second
    one, we expect a number of contiguous memory areas.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们声明了两种类型；第一种是 `INTERLEAVED_T`，它是一个包含 `a` 到 `d` 成员的结构体数组。然后我们声明 `NON_INTERLEAVED_T`
    作为一个包含四个数组（`a` 到 `d`）的结构体。根据类型的命名，第一种我们期望数据在内存中是交错排列的，而第二种我们期望的是多个连续的内存区域。
- en: Let’s look first at the CPU code.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 先来看一下 CPU 代码。
- en: '[PRE71]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '` const float delta = get_time() - start_time;`'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '` const float delta = get_time() - start_time;`'
- en: '[PRE72]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The two functions to add the data are broadly similar. Each function iterates
    over all elements in the list `iter` times and adds into the destination data
    structure a value from the source data structure. Each function also returns the
    time it takes to execute. As these will run on the CPU, we use the wall clock
    time on the CPU.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个加数据的函数大致相似。每个函数会遍历列表中的所有元素 `iter` 次，并将源数据结构中的值添加到目标数据结构中。每个函数还会返回执行所需的时间。由于这些将在
    CPU 上运行，我们使用 CPU 上的壁钟时间。
- en: The GPU code is largely similar, with the outer loop, `tid`, replaced with *N*
    threads from invoking a kernel.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 代码大体相似，外部循环中的 `tid` 被从调用内核的 *N* 个线程所替代。
- en: '[PRE73]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '`   dest_ptr->d[tid] += src_ptr->d[tid];`'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '`   dest_ptr->d[tid] += src_ptr->d[tid];`'
- en: '[PRE74]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The caller of the GPU function is a fairly standard copy to device and time
    routine. I’ll list here only the interleaved version, as the two functions are
    largely identical.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 函数的调用者是一个相当标准的设备拷贝和时间例程。我这里只列出交错版本，因为这两个函数基本相同。
- en: '[PRE75]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'When we run this code, we achive the following results:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，我们得到了以下结果：
- en: '[PRE77]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: What we see is quite interesting, and largely to be expected. The interleaved
    memory access pattern has an execution time three to four times longer than the
    noninterleaved pattern on compute 2.x hardware. The compute 1.3 GTX260 demonstrates
    a 3× slow down when using the interleaved memory pattern. The compute 1.1 9800GT,
    however, exhibits an 11× slow down, due to the more stringent coalescing requirements
    for these older devices.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的结果相当有趣，而且大多是预期中的。交错内存访问模式的执行时间比非交错模式长三到四倍，尤其是在计算能力为 2.x 的硬件上。在计算能力为 1.3
    的 GTX260 上，使用交错内存模式时，执行时间慢了三倍。然而，计算能力为 1.1 的 9800GT 显示出 11 倍的性能下降，因为这些老旧设备对内存合并的要求更为严格。
- en: We can look a bit deeper into the memory access pattern between the slow interleaved
    pattern and the much faster noninterleaved pattern with a tool such as Parallel
    Nsight. We can see that the number of memory transactions (CUDA Memory Statistics
    experiment) used in the noninterleaved version is approximately one-quarter that
    of the interleaved version, resulting in the noninterleaved version shifting one-quarter
    of the data to/from memory than the interleaved version does.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过类似Parallel Nsight这样的工具，深入了解慢速交错模式与更快速的非交错模式之间的内存访问模式。我们可以看到，非交错版本所使用的内存事务数量（CUDA内存统计实验）大约是交错版本的四分之一，这意味着非交错版本将数据传输到/从内存的次数仅为交错版本的四分之一。
- en: One other interesting thing to note is the CPU shows exactly the opposite effect.
    This may seem strange, until you think about the access pattern and the cache
    reuse. A CPU accessing element `a` in the interleaved example will have brought
    structure elements `b`, `c`, and `d` into the cache on the access to `a` since
    they will likely be in the same cache line. However, the noninterleaved version
    will be accessing memory in four seperate and physically dispersed areas. There
    would be four times the number of memory bus transactions and any read-ahead policy
    the CPU might be using would not be as effective.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的事情是，CPU表现出完全相反的效果。乍一看这似乎很奇怪，但如果你考虑访问模式和缓存重用，就会明白。CPU在交错示例中访问元素`a`时，会将结构元素`b`、`c`和`d`一同加载到缓存中，因为它们很可能在同一缓存行中。然而，非交错版本将访问四个不同且物理分散的内存区域。这会导致四倍的内存总线事务数量，任何CPU可能使用的预读策略也不会那么有效。
- en: Thus, if your existing CPU application uses an interleaved arrangement of structure
    elements, simply copying it to a GPU will work, but at a considerable cost due
    to poor memory coalescing. Simply reordering the declarations and access mechanism,
    as we’ve done in this example, could allow you to achieve a significant speedup
    for very little effort.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您现有的CPU应用程序使用了交错排列的结构元素，将其直接复制到GPU上是可行的，但由于内存合并不佳，会付出相当大的代价。像我们在这个例子中所做的那样，重新排列声明和访问机制，可以让您以极少的努力获得显著的加速。
- en: Score boarding
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记分板
- en: One other interesting property of global memory is that it works with a scoreboard.
    If we initiate a load from global memory (e.g., `a=some_array[0]`), then all that
    happens is that the memory fetch is initiated and local variable `a` is listed
    as having a pending memory transaction. Unlike traditional CPU code, we do not
    see a stall or even a context switch to another warp until such time as the variable
    `a` is later used in an expression. Only at this time do we actually need the
    contents of variable `a`. Thus, the GPU follows a lazy evaluation model.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 全局内存的另一个有趣特点是它与记分板配合工作。如果我们从全局内存发起加载（例如，`a=some_array[0]`），那么发生的事情只是内存取值被发起，本地变量`a`被列为有一个待处理的内存事务。与传统的CPU代码不同，直到稍后在表达式中使用变量`a`时，我们才会看到停滞或甚至上下文切换到另一个warp。在这个时候，我们才真正需要变量`a`的内容。因此，GPU遵循懒加载（lazy
    evaluation）模型。
- en: You can think of this a bit like ordering a taxi and then getting ready to leave.
    It may take only five minutes to get ready, but the taxi may take up to 15 minutes
    to arrive. By ordering it before we actually need it, it starts its journey while
    we are busy on the task of getting ready to leave. If we wait until we are ready
    before ordering the taxi, we serialize the task of getting ready to leave with
    waiting for the taxi.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把这个情况想象成叫出租车然后准备离开。准备好可能只需要五分钟，但出租车可能需要15分钟才能到达。如果在我们实际上需要它之前就叫出租车，那么它会在我们忙于准备离开时开始行程。如果我们等到准备好才叫出租车，那么我们就将准备离开的任务与等待出租车的任务串行化了。
- en: The same is true of the memory transactions. By comparison, they are like the
    slow taxi, taking forever in terms of GPU cycles to arrive. Until such time as
    we actually need the memory transaction to have arrived, the GPU can be busy calculating
    other aspects of the algorithm. This is achieved very simply by placing the memory
    fetches at the start of the kernel, and then using them much later during the
    kernel. We, in effect, overlap the memory fetch latency with useful GPU computations,
    reducing the effect of memory latency on our kernel.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的情况也适用于内存事务。相比之下，它们就像是慢速的出租车，需要花费大量的GPU周期才能到达。直到我们实际上需要内存事务到达时，GPU才会忙于计算算法的其他方面。这通过简单地将内存取值放在内核的开始位置，并在内核的稍后使用它们来实现。实际上，我们将内存取值的延迟与有用的GPU计算重叠，减少了内存延迟对内核的影响。
- en: Global memory sorting
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局内存排序
- en: Picking up from where we left off with shared memory sorting, how do you think
    the same algorithm would work for global memory–based sorting? What needs to be
    considered? First and foremost, you need to think about memory coalescing. Our
    sorting algorithm was specifically developed to run with the 32 banks of shared
    memory and accesses the shared memory in columns. If you look again at [Figure
    6.8](CHP006.html#F0045), you’ll see this also achieves coalesced access to global
    memory if all threads were to read at once.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在共享内存排序的讨论继续，您认为相同的算法如何应用于基于全局内存的排序？需要考虑哪些因素？首先，您需要考虑内存合并。我们的排序算法是专门为32个共享内存银行开发的，并且按列访问共享内存。如果您再次查看[图6.8](CHP006.html#F0045)，您会看到如果所有线程同时读取，它也能实现对全局内存的合并访问。
- en: The coalesced access occurs during the radix sort, as each thread marches through
    its own list. Every thread’s access is coalesced (combined) together by the hardware.
    Writes are noncoalesced as the 1 list can vary in size. However, the zeros are
    both read and written to the same address range, thus providing coalesced access.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 合并访问发生在基数排序过程中，因为每个线程都在遍历自己的列表。每个线程的访问会由硬件合并（组合）在一起。写操作则不是合并的，因为列表的大小可能会有所不同。然而，零值会同时被读写到相同的地址范围，从而提供合并访问。
- en: In the merge phase, during the startup condition one value from each list is
    read from global into shared memory. In every iteration of the merge, a single
    value is written out to global memory, and a single value is read into shared
    memory to replace the value written out. There is a reasonable amount of work
    being done for every memory access. Thus, despite the poor coalescing, the memory
    latency should be largely hidden. Let’s look at how this works in practice.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并阶段，启动时每个列表中的一个值会从全局内存读取到共享内存中。在每次合并迭代中，一个值会写入全局内存，一个值会读取到共享内存中，替换被写出的值。每次内存访问都需要进行相当多的工作。因此，尽管合并不理想，内存延迟应该大部分被隐藏。让我们来看看实际中是如何工作的。
- en: What you can see from [Table 6.13](#T0070) and [Figure 6.21](#F0110) is that
    32 threads work quite well, but this is marginally beaten by 64 threads on all
    the tested devices. It’s likely that having another warp to execute is hiding
    a small amount of the latency and will also improve slightly the memory bandwidth
    utilization.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表格6.13](#T0070)和[图6.21](#F0110)中可以看到，32个线程效果相当好，但在所有测试的设备上，64个线程略微超过了它。很可能是因为执行另一个warp隐藏了一些延迟，并且稍微提高了内存带宽的利用率。
- en: Table 6.13 Single SM GMEM Sort (1K Elements)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 表格6.13 单个SM全局内存排序（1K元素）
- en: '![Image](../images/T000065tabT0070.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000065tabT0070.jpg)'
- en: '![image](../images/F000065f06-21-9780124159334.jpg)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000065f06-21-9780124159334.jpg)'
- en: Figure 6.21 Graph of single SM GMEM sort (1K elements).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 单个SM全局内存排序（1K元素）图
- en: Moving beyond 64 threads slows things down, so if we now fix the number of threads
    at 64 and increase the dataset size what do we see? See [Table 6.14](#T0075) and
    [Figure 6.22](#F0115) for the results. In fact we see an almost perfect linear
    relationship when using a single SM, as we are currently doing.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 超过64个线程会使性能下降，那么如果我们现在将线程数量固定为64并增加数据集大小，会看到什么结果呢？请参考[表格6.14](#T0075)和[图6.22](#F0115)查看结果。事实上，当使用单个SM时，我们看到几乎完美的线性关系，就像我们当前所做的那样。
- en: Table 6.14 GMEM Sort by Size
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 表格6.14 按大小排序的全局内存排序
- en: '![Image](../images/T000065tabT0075.jpg)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000065tabT0075.jpg)'
- en: '![image](../images/F000065f06-22-9780124159334.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000065f06-22-9780124159334.jpg)'
- en: Figure 6.22 GMEM graph sorted by size.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 按大小排序的全局内存（GMEM）图。
- en: As [Table 6.14](#T0075) shows, 1024 KB (1 MB) of data takes 1486 ms to sort
    on the GTX460\. This means we can sort 1 MB of data in around 1.5 seconds (1521
    ms exactly) and around 40 MB per minute, regardless of the size of the data.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[表格6.14](#T0075)所示，1024 KB（1 MB）的数据在GTX460上排序需要1486毫秒。这意味着我们可以在大约1.5秒（精确为1521毫秒）内排序1MB的数据，每分钟排序约40MB，无论数据的大小如何。
- en: A 1 GB dataset would therefore take around 25–26 minutes to sort, which is not
    very impressive. So what is the issue? Well currently we’re using just a single
    block, which in turn limits us to a single SM. The test GPUs consists of 14 SMs
    on the GTX470, 27 SMs on the GTX260, and 7 SMs on the GTX460\. Clearly, we’re
    using a small fraction of the real potential of the card. This has been done largely
    to simplify the solution, so let’s look now at using multiple blocks.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个1 GB的数据集大约需要25到26分钟来排序，这并不算非常惊人。那么问题出在哪里呢？好吧，目前我们只使用了一个单独的数据块，这限制了我们只能使用一个SM。测试用的GPU在GTX470上有14个SM，在GTX260上有27个SM，在GTX460上有7个SM。显然，我们只利用了显卡实际潜力的一小部分。这样做主要是为了简化解决方案，因此我们现在来看一下使用多个数据块的情况。
- en: The output of one SM is a single linear sorted list. The output of two SMs is
    therefore two linear sorted lists, which is not what we want. Consider the following
    dump of output from a two-block version of the sort. The original values were
    in reverse sorting order from `0x01` to `0x100`. The first value shown is the
    array index, followed by the value at that array index.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 一个SM的输出是一个单一的线性排序列表。因此，两个SM的输出将是两个线性排序列表，这并不是我们想要的。请看下面这个从两块版本的排序中输出的转储。原始值的排序顺序是从`0x01`到`0x100`的倒序。第一个显示的值是数组索引，后面是该数组索引处的值。
- en: '[PRE78]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: We can see there are two sorted lists here, one from `0x41` to `0x80` and the
    other from `0x01` to `0x40`. You might say that’s not a great problem, and we
    just need to merge the list again. This is where we hit the second issue; think
    about the memory access on a per-thread basis.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这里有两个排序列表，一个从`0x41`到`0x80`，另一个从`0x01`到`0x40`。你可能会说这不是一个大问题，我们只需要重新合并这两个列表。但这就是我们遇到第二个问题的地方；考虑一下按每个线程进行内存访问的情况。
- en: Assume we use just two threads, one per list. Thread 0 accesses element 0\.
    Thread 1 accesses element 64\. It’s not possible for the hardware to coalesce
    the two accesses, so the hardware has to issue two independent memory fetches.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们只使用两个线程，每个列表一个线程。线程0访问元素0，线程1访问元素64。硬件无法将这两次访问合并，因此硬件必须发出两次独立的内存获取请求。
- en: Even if we were to do the merge in zero time, assuming we have a maximum of
    16 SMs and using all of them did not flood the bandwidth of the device, in the
    best case we’d get 16 × 40 MB/min = 640 MB/min or around 10.5 MB/s. Perhaps an
    alternative approach is required.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们在零时间内执行合并，假设我们最多有16个SM并且使用所有SM都没有淹没设备的带宽，在最好的情况下，我们将得到16 × 40 MB/min = 640
    MB/min，或者大约10.5 MB/s。也许需要一种替代方法。
- en: Sample sort
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例排序
- en: Sample sort tries to get around the problem of merge sort, that is, that you
    have to perform a merge step. It works on the principle of splitting the data
    into *N* independent blocks of data such that each block is partially sorted and
    we can guarantee the numbers in block *N* are less than those in block *N* + 1
    and larger than those in block *N* − 1.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 示例排序试图绕过归并排序的问题，也就是必须执行一个合并步骤。它的原理是将数据分成*N*个独立的数据块，每个数据块是部分排序的，并且我们可以保证块*N*中的数字小于块*N*
    + 1中的数字，同时大于块*N* − 1中的数字。
- en: We’ll look first at an example using three processors sorting 24 data items.
    The first phase selects *S* equidistant samples from the dataset. *S* is chosen
    as a fraction of *N*, the total number of elements in the entire dataset. It is
    important that *S* is representative of the dataset. Equidistant points are best
    used where the data is reasonably uniformly distributed over the data range. If
    the data contains large peaks that are not very wide in terms of sample points,
    a higher number of samples may have to be used, or one where the samples concentrate
    around the known peaks. We’ll chose equidistant points and assume the more common
    uniform distribution of points.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先看一个例子，使用三个处理器排序24个数据项。第一阶段从数据集中选择*S*个等距的样本。*S*被选定为*N*的一个分数，其中*N*是整个数据集中的元素总数。重要的是*S*能够代表数据集。如果数据相对均匀分布在数据范围内，等距点是最好的选择。如果数据中包含大且不宽的峰值，可能需要使用更多的样本，或者选择集中在已知峰值周围的样本。我们将选择等距点，并假设数据点更常见的是均匀分布的。
- en: The samples are then sorted such that the lowest value is first in the list,
    assuming an ascending order sort. The sample data is then split into bins according
    to how many processors are available. The data is scanned to determine how many
    samples fit in each bin. The number of samples in each bin is then added to form
    a prefix sum that is used to index into an array.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，样本会被排序，确保最低值排在列表的最前面，假设是升序排序。样本数据随后会根据可用的处理器数量被分成多个桶。数据会被扫描以确定每个桶中可以放入多少样本。每个桶中的样本数会被累加形成前缀和，并用来索引数组。
- en: A prefix sum is simply the sum of all elements prior to the current element.
    Looking at the example, we can see nine elements were allocated to bin 0\. Therefore,
    the start of the second dataset is element 9\. The next list size, as it happens
    from the dataset, was also nine. Nine plus the previous sum is 18, and thus we
    know the index of the next dataset and so on.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和就是当前元素之前所有元素的总和。查看示例，我们可以看到九个元素分配给了 bin 0。因此，第二个数据集的起始位置是元素 9。接下来的列表大小，也正好是
    9。9 加上之前的和是 18，因此我们知道下一个数据集的索引，依此类推。
- en: The data is then shuffled, so all the bin 0 elements are written to the first
    index of the prefix sum (zero), bin 1 written to the next, and so on. This achieves
    a partial sort of the data such that all the samples in bin *N* − 1 are less than
    those in bin *N*, which in turn are less than those in bin *N* + 1\. The bins
    are then dispatched to *P* processors that sort the lists in parallel. If an in-place
    sort is used, then the list is sorted once the last block of data is sorted, without
    any merge step. [Figure 6.24](#F0125) is this same example using six processing
    elements.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 数据随后会被打乱，因此所有的 bin 0 元素被写入前缀和的第一个索引（零），bin 1 写入下一个索引，以此类推。这实现了数据的部分排序，使得 bin
    *N* - 1 中的所有样本都小于 bin *N* 中的样本，而 bin *N* 中的样本又小于 bin *N* + 1 中的样本。然后，这些桶会被分发到
    *P* 个处理器上并行排序这些列表。如果使用就地排序，那么在最后一个数据块排序完成时，列表就会被排序，无需任何合并步骤。[图 6.24](#F0125) 是使用六个处理元素的相同示例。
- en: '![image](../images/F000065f06-23-9780124159334.jpg)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-23-9780124159334.jpg)'
- en: Figure 6.23 Sample sort using three processors.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 使用三个处理器的样本排序。
- en: '![image](../images/F000065f06-24-9780124159334.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-24-9780124159334.jpg)'
- en: Figure 6.24 Sample sort using six processors.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24 使用六个处理器的样本排序。
- en: Notice that when we used three processors based on six samples, the bin sizes
    were 9, 9, 6\. With six processors the bin sizes are 6, 3, 5, 4, 1, 5\. What we’re
    actually interested in is the largest value, as on *P* processors the largest
    block will determine the total time taken. In this example, the maximum is
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们基于六个样本使用三个处理器时，桶的大小为 9、9、6。使用六个处理器时，桶的大小为 6、3、5、4、1、5。我们真正关心的是最大值，因为在
    *P* 个处理器上，最大块将决定总耗时。在此示例中，最大值是
- en: reduced from nine elements to six elements, so a doubling of the number of processors
    has reduced the maximum number of data points by only one-third.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 从九个元素减少到六个元素，因此将处理器数量加倍仅减少了三分之一的数据点。
- en: The actual distribution will depend very much on the dataset. The most common
    dataset is actually a mostly sorted list or one that is sorted with some new data
    items that must be added. This tends to give a fairly equal distribution for most
    datasets. For problem datasets it’s possible to adjust the sampling policy accordingly.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的分布将很大程度上依赖于数据集。最常见的数据集实际上是一个大部分已排序的列表，或者是一个经过排序并且必须添加一些新数据项的列表。这通常会给大多数数据集带来相对均匀的分布。对于问题数据集，可以相应地调整采样策略。
- en: With a GPU we don’t just have six processors; we have *N* SMs, each of which
    we need to run a number of blocks on. Each block would ideally be around 256 threads
    based simply on ideal memory latency hiding, although we saw that 64 threads worked
    best with the radix sort we developed earlier in the chapter. With the GTX470
    device, we have 14 SMs with a maximum of eight blocks per SM. Therefore, we need
    at least 112 blocks just to keep every SM busy. We’ll find out in practice which
    is the best in due course. It is likely we will need substantially more blocks
    to load balance the work.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPU 时，我们不仅有六个处理器；我们有*N*个 SM，每个 SM 上需要运行多个块。理想情况下，每个块大约包含 256 个线程，基于理想的内存延迟隐藏，尽管我们在本章之前开发的基数排序中看到
    64 个线程效果最好。使用 GTX470 设备时，我们有 14 个 SM，每个 SM 最多可以有 8 个块。因此，我们至少需要 112 个块来保持每个 SM
    的繁忙状态。实践中，我们将找出哪种方式最好。实际上，我们可能需要更多的块来平衡负载。
- en: The first task, however, is to develop a CPU version of the sample sort algorithm
    and to understand it. We’ll look at each operation in turn and how it could be
    converted to a parallel solution.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，首要任务是开发一个 CPU 版本的样本排序算法并理解它。我们将逐步查看每个操作，并讨论如何将其转化为并行解决方案。
- en: To follow the development of the code in the subsequent sections, it’s important
    you understand the sample sort algorithm we just covered. It’s one of the more
    complex sorting algorithms and was chosen both for performance reasons and also
    because it allows us to look at a real problem involving difficult issues in terms
    of GPU implementation. If you browsed over the algorithm, please re-read the last
    few pages until you are sure you understand how the algorithm works before proceeding.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随后续章节中的代码开发，你需要理解我们刚刚讨论过的样本排序算法。这是一个较为复杂的排序算法，选择它既是为了性能考虑，也因为它使我们能够看到一个涉及
    GPU 实现难题的实际问题。如果你略过了这个算法，请重新阅读最后几页，直到你确保理解了算法的工作原理，再继续进行。
- en: Selecting samples
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择样本
- en: The first part of the sample sort is to select *N* samples from the source data.
    The CPU version works with a standard loop where the source data loop index is
    incremented by `sample_interval` elements. The sample index counter, however,
    is incremented only by one per iteration.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 样本排序的第一部分是从源数据中选择 *N* 个样本。CPU 版本使用标准循环，其中源数据的循环索引每次增加 `sample_interval` 个元素。然而，样本索引计数器每次只增加
    1。
- en: '[PRE79]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '` u32 ∗ const sample_data,`'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '` u32 ∗ const sample_data,`'
- en: '[PRE80]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: In the GPU version we can use a classic loop elimination method and simply create
    one thread per sample point, spread across as many blocks as necessary. Thus,
    the first statement
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 版本中，我们可以使用经典的循环消除方法，为每个样本点创建一个线程，按需要跨多个块分配线程。因此，第一条语句
- en: '[PRE81]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: simply takes the block index and multiplies it by the number of threads per
    block and then adds in the current thread to our combined thread index.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 只需获取块索引，将其乘以每块的线程数，然后将当前线程添加到我们的合并线程索引中。
- en: '[PRE82]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '` cuda_error_check(prefix, "Error invoking select_samples_gpu_kernel");`'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '` cuda_error_check(prefix, "Error invoking select_samples_gpu_kernel");`'
- en: '[PRE84]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Finally, to work out the index into the source data we simply multiply our sample
    data index (`tid`) by the size of the sample interval. For the sake of simplicity
    we’ll only look at the case where the dataset sizes are multiples of one another.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了计算源数据中的索引，我们只需将样本数据索引（`tid`）乘以样本间隔的大小。为了简化起见，我们只考虑数据集大小是彼此倍数的情况。
- en: Notice both the CPU and GPU versions return the time taken for the operation,
    something we’ll do in each section of the sort to know the various timings of
    each operation.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CPU 和 GPU 版本都返回操作所用的时间，这是我们在排序的每一部分中都会做的事情，用来了解每个操作的不同时间。
- en: Sorting the samples
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 排序样本
- en: Next we need to sort the samples we’ve selected. On the CPU we can simply call
    the `qsort` (quicksort) routine from the standard C library.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对已选择的样本进行排序。在 CPU 上，我们可以简单地调用标准 C 库中的 `qsort`（快速排序）例程。
- en: '[PRE85]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: On the GPU, however, these standard libraries are not available, so we’ll use
    the radix sort we developed earlier. Note, radix sort is also provided by the
    Thrust library, so you don’t have to write it as we’ve done here. I won’t replicate
    the code here since we’ve already looked at it in detail in the shared memory
    section.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 GPU 上，这些标准库是不可用的，因此我们将使用之前开发的基数排序。注意，基数排序也可以通过 Thrust 库提供，因此你不必像我们在这里所做的那样编写它。我不会在这里复制代码，因为我们已经在共享内存部分详细讨论过它。
- en: One thing to note, however, is the version we developed before does a radix
    sort on a single SM in shared memory and then uses a shared memory reduction for
    the merge operation. This is not an optimal solution, but we’ll use it for at
    least the initial tests.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，我们之前开发的版本对共享内存中的单个 SM 执行基数排序，然后使用共享内存归约进行合并操作。这并不是一个最优的解决方案，但我们至少会在初步测试中使用它。
- en: Counting the sample bins
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计数样本桶
- en: 'Next we need to know how many values exist in each sample bin. The CPU code
    for this is as follows:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要知道每个样本桶中有多少个值。对应的 CPU 代码如下：
- en: '[PRE86]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '` for (u32 src_idx=0; src_idx<num_elements; src_idx++)`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '` for (u32 src_idx=0; src_idx<num_elements; src_idx++)`'
- en: '[PRE87]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: To count the values in each bin we simply iterate over the source dataset and
    for every element call a search function that identifies in which bin a given
    data value will belong. We then increment the bin counter for that given index.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算每个桶中的值，我们只需遍历源数据集，对于每个元素调用一个搜索函数，确定给定数据值属于哪个桶。然后，我们为该桶的索引增量计数。
- en: 'For the search we have two options: a binary search or a sequential search.
    A binary search takes advantage of the fact we have a sorted list of samples from
    the previous step. It works by dividing the list into two halves and asking whether
    the value it seeks is in the top or bottom half of the dataset. It then divides
    the list again and again until such time as it finds the value.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 对于搜索，我们有两个选项：二分查找或顺序查找。二分查找利用了我们有一个从上一步得到的已排序样本列表的事实。它通过将列表分成两半，并询问它要找的值是在数据集的上半部分还是下半部分来工作。然后它不断地将列表一分为二，直到找到该值为止。
- en: The worst case sort time for a binary search is log[2](*N*). We’ll hit the worst
    case in many instances because most of the data is missing from the sample list.
    Therefore, we’ll assume we’ll hit the worst case in all cases when comparing the
    two approaches.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 二分查找的最坏情况排序时间是log[2](*N*)。在许多情况下，我们会遇到最坏情况，因为大多数数据都缺失于样本列表中。因此，我们将假设在比较这两种方法时，所有情况都会达到最坏情况。
- en: The sequential search worst case is *N*. That is, we start at the beginning
    of the list and do not find the item at all, having transversed the list from
    start to finish. However, with a sorted list and a uniform distribution of data
    the most likely case is *N*/2\. Thus, for a sample set of 1024 elements, a binary
    search would take just 10 iterations compared with 512 iterations for the sequential
    search. Clearly, the binary search is the best approach in terms of the search
    space covered.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序查找的最坏情况是*N*。也就是说，我们从列表的开始处开始，并且完全没有找到该项，已经从头到尾遍历了整个列表。然而，对于一个已排序的列表，并且数据均匀分布的情况下，最有可能的情况是*N*/2。因此，对于一个包含1024个元素的样本集，二分查找只需要10次迭代，而顺序查找需要512次迭代。显然，从搜索空间的覆盖角度来看，二分查找是最好的方法。
- en: However, we have to consider that a binary search is not very good for a GPU
    from the perspective of coalesced memory accesses and branch divergence. As soon
    as one thread diverges in a warp, the hardware needs two control paths. We may
    well have the situation where the warps diverge such that we have entirely independent
    control for each thread. In this case we can multiply the time taken by the number
    of divergent threads. This will always be a maximum of the number of iterations,
    which is the log[2](*N*). Thus, our sample size needs to be huge before we see
    anything like the maximum amount of divergence—all threads in a warp.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须考虑到，从联合内存访问和分支分歧的角度来看，二分查找对GPU并不是很有效。只要一个线程在warp中发生分歧，硬件就需要两个控制路径。我们可能会遇到这种情况，即warps发生分歧，以至于每个线程都拥有完全独立的控制。在这种情况下，我们可能会将所需时间乘以分歧线程的数量。这将始终是最大值，最多为迭代次数，即log[2](*N*)。因此，我们的样本大小需要非常大，才能看到类似最大分歧的情况——即warp中的所有线程都分歧。
- en: Each thread is accessing potentially a different area of memory in the sample
    set, so there is no coalescing and therefore there is a drop of an order of magnitude
    in terms of global memory bandwidth. In practice, this should be largely hidden
    by the L1 and L2 cache on compute 2.x devices, depending on the size of the sample
    space. We could also store the sample space in shared memory, meaning we can discount
    the coalescing issues.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程可能会访问样本集中的不同区域，因此没有联合访问，因此在全局内存带宽方面会下降一个数量级。在实践中，这应该在计算2.x设备上通过L1和L2缓存大部分隐藏，具体取决于样本空间的大小。我们还可以将样本空间存储在共享内存中，这样我们就可以忽略联合访问问题。
- en: The standard C library again provides a `bsearch` function, which returns the
    value it finds in the array. However, we’re not interested in the nearest value,
    but actually the array index. Therefore, we’ll write a basic binary search function
    and use this on both the GPU and CPU. Notice the use of both `__host__` and `__device__`
    specifiers to run the identical source, but not binary, code on both the CPU and
    GPU.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 标准C库再次提供了一个`bsearch`函数，它返回在数组中找到的值。然而，我们并不关心最接近的值，而是实际的数组索引。因此，我们将编写一个基本的二分查找函数，并在GPU和CPU上都使用它。请注意，使用了`__host__`和`__device__`修饰符，以便在CPU和GPU上运行相同的源代码，但不运行二进制代码。
- en: '[PRE88]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '` const u32 search_value,`'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '` const u32 search_value,`'
- en: '[PRE89]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The binary search routine works by reducing the `size` parameter to zero. It
    returns the index or the bin in which the search value should be placed.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 二分查找例程通过将`size`参数减少到零来工作。它返回索引或应该放置搜索值的区间。
- en: '[PRE90]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '`__host__ TIMER_T count_bins_gpu(`'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '`__host__ TIMER_T count_bins_gpu(`'
- en: '[PRE91]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Unlike the function to select samples where the maximum number of threads was
    limited by the number of samples, here we are limited only by the number of elements
    in the source array. Thus, the host function launches a kernel that contains one
    thread per element.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 与选择样本的函数不同，其中线程的最大数量受样本数的限制，在这里我们只受源数组中元素数量的限制。因此，主机函数启动一个内核，其中每个元素对应一个线程。
- en: The kernel function works out its element, and reads it from the source dataset
    in a nice coalesced manner. Using more threads per block here allows for increased
    read bandwidth from the global memory.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 核心函数计算其元素，并以一种良好的合并方式从源数据集中读取它。每个块使用更多线程可以增加从全局内存读取的带宽。
- en: Each thread of a warp will jump off into the binary search, and will, after
    not too many iterations, return. With a random list of elements you get some thread
    divergence. However, in the more common case of a mostly sorted list, all threads
    tend to follow the same route, thus causing very little thread divergence in practice.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 warp 的线程将跳入二分查找，经过不多的迭代后返回。对于一个随机的元素列表，你会遇到线程分歧。然而，在更常见的、基本已排序的列表中，所有线程往往遵循相同的路径，因此在实践中线程分歧很少。
- en: When all the threads of a warp have returned from the binary search, they increment
    the values in one of *N* bins held in global memory via an atomic write. Atomic
    operations to global memory are operations that are guaranteed to complete, uninterrupted,
    regardless of which thread on which SM initiated the action. Thus, we can safely
    have many threads write to the same address. Obviously only one can physically
    write, so any clash of values results in serialization of the requests.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 warp 的所有线程从二分查找返回时，它们通过原子写入操作增加全局内存中一个 *N* 个箱子中的值。对全局内存的原子操作是指无论哪一个 SM 上的线程发起该操作，都能保证操作完成且不中断。因此，我们可以安全地让多个线程写入相同的地址。显然，只有一个线程可以实际写入，因此任何值的冲突都会导致请求的串行化。
- en: Unfortunately with a mostly sorted list we find that, because blocks are allocated
    in turn, most active blocks are in a similar area of memory. While this is very
    good for locality, it does mean all the threads are hitting the same memory area
    for the writes. With a sorted list we thus see a degradation of speed in this
    approach, but not a significant one, as we’ll see later.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于一个基本排序的列表，我们发现，由于块是依次分配的，大多数活动块位于内存的相似区域。虽然这对局部性非常有利，但这意味着所有线程都在写入时访问相同的内存区域。因此，对于一个排序的列表，我们会看到这种方法的速度有所下降，但不会显著下降，正如我们稍后所见。
- en: Prefix sum
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前缀和
- en: A prefix sum is useful in that it can be used to create a table of values that
    index into an array that has variable-length records. The size of each bin in
    our case is a variable length and each bin is stored sequentially in memory one
    after another. Thus, we can calculate a prefix sum array and then use array element
    0 to access the start of bin 0, array element one to access the start of bin one, etc.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和的用途在于它可以用来创建一个值表，该表可以索引到具有可变长度记录的数组。在我们的案例中，每个箱子的大小是一个可变长度，并且每个箱子是按顺序一个接一个地存储在内存中的。因此，我们可以计算一个前缀和数组，然后使用数组元素
    0 来访问箱子 0 的开始，使用数组元素 1 来访问箱子 1 的开始，依此类推。
- en: 'The code for the prefix sum on the CPU is quite simple:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上计算前缀和的代码相当简单：
- en: '[PRE92]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Here we simply iterate over the array `bin_count`, which contains how many elements
    there are in each bin. The prefix sum starts at zero and we store to the array
    `prefix_sum` the sum of the bin counts the loop has seen so far.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是简单地遍历数组`bin_count`，该数组包含每个箱子中的元素数量。前缀和从零开始，我们将循环到目前为止所看到的箱子计数之和存储到数组`prefix_sum`中。
- en: The main problem with this piece of code and with prefix sum in general is that
    at first it seems like an inherently serial problem. You cannot calculate the
    last value without its prior value. A loop iterating over all elements is actually
    a very efficient way to calculate this for a single-processor system. So how can
    a prefix sum be calculated in a parallel way so we can make use of more than just
    one SM?
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码和前缀和的主要问题在于，起初它看起来像是一个固有的串行问题。你不能计算出最后一个值而不依赖于它之前的值。对于单处理器系统，遍历所有元素的循环实际上是计算这个问题的非常高效的方法。那么，如何以并行方式计算前缀和，从而不仅仅利用一个
    SM 呢？
- en: It turns out that this simple implementation of prefix sum is actually quite
    fast for small numbers of elements. However, as the number of sample elements
    becomes larger (4096 plus), a somewhat faster and more complex approach is needed.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这种简单的前缀和实现对于少量元素是相当快的。然而，随着样本元素数量的增加（4096及以上），需要一种更快且更复杂的方法。
- en: You can calculate prefix sum in parallel by splitting the array into a number
    of blocks and calculating the prefix sum on those blocks. The end point of each
    prefix sum block is placed into another array. Another prefix sum is then done,
    in place, on this array. The result of this prefix sum is then added to each element
    in the original prefix sum calculation. This produces a parallel prefix sum that
    we can easily use on the GPU ([Figure 6.25](#F0130)).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将数组拆分为多个块，并在这些块上计算前缀和，来并行计算前缀和。每个前缀和块的终点被放入另一个数组中。接着，在这个数组上再进行一次前缀和计算。这次前缀和的结果会加到原始前缀和计算中的每个元素上。这样就生成了一个并行的前缀和，我们可以轻松地在GPU上使用它（见[图
    6.25](#F0130)）。
- en: '![image](../images/F000065f06-25-9780124159334.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-25-9780124159334.jpg)'
- en: Figure 6.25 Parallel prefix sum.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.25 并行前缀和。
- en: For the prefix sum blocks we’ll use a single thread per block. As each thread
    processes the same number of elements and simply iterates around a loop, there
    is no thread divergence. However, the read memory access is poorly coalesced because
    thread 0 will be accessing addresses starting at a zero offset, while thread 1
    will be accessing addresses starting at a `(NUM_SAMPLES/NUM_BLOCKS)` offset.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前缀和块，我们将使用每个块一个线程。由于每个线程处理相同数量的元素，并且仅仅是围绕循环迭代，因此不存在线程分歧。然而，读内存访问的聚合性较差，因为线程0将访问从零偏移开始的地址，而线程1将访问从`(NUM_SAMPLES/NUM_BLOCKS)`偏移开始的地址。
- en: We want to run this on multiple SMs, which in turn means having to create multiple
    blocks. We need a synchronization point in the center where we do a prefix sum.
    This can’t happen until all the blocks have completed. Therefore, we will need
    to launch a kernel to do the initial prefix sum, another to do the prefix sum
    over the results, and a final kernel to do the addition step.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在多个SM上运行这段代码，这意味着需要创建多个块。我们需要一个同步点，在中间进行前缀和计算。这个操作必须等到所有块完成后才能进行。因此，我们将需要启动一个内核来进行初始前缀和计算，另一个内核来对结果进行前缀和计算，再有一个内核来进行加法步骤。
- en: This is actually quite beneficial as it gives us the opportunity to change the
    number of blocks and threads used. While we might use one thread per prefix sum
    block, the addition kernel parallelism is limited only by the number of sample
    points. Thus, we can run *N* blocks of *M* threads where *N* × *M* is the number
    of samples, maximizing the usage of the GPU.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是非常有益的，因为它给了我们改变使用的块数和线程数的机会。虽然我们可能为每个前缀和块使用一个线程，但加法内核的并行性仅受样本点数量的限制。因此，我们可以运行*N*个块，每个块有*M*个线程，其中*N*
    × *M*就是样本数量，从而最大化GPU的使用率。
- en: As with most algorithms that are more complex, there is a tradeoff point where
    the simpler algorithm is faster. For the serial prefix sum versus the blocked
    prefix sum, this is around 4096 sample points. We could take this further and
    implement a more complex prefix sum in the first phase, but unless we have really
    large datasets, the prefix sum will not be a key factor in the sorting time.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数更复杂的算法一样，总有一个权衡点，简单的算法反而更快。对于串行前缀和与分块前缀和来说，这个权衡点大约在4096个样本点左右。我们本可以在第一阶段实现更复杂的前缀和，但除非数据集非常大，否则前缀和在排序时间中不会成为关键因素。
- en: Let’s look at the GPU code in detail.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看GPU代码。
- en: '[PRE93]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '` {`'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '` {`'
- en: '[PRE94]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: First, we calculate our `tid` number based on the block and thread. Then we
    calculate the `tid_offset` based on the number of samples a thread will be calculating
    the prefix sum for.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们根据块和线程计算我们的`tid`编号。然后，我们根据每个线程需要计算前缀和的样本数量计算`tid_offset`。
- en: Then for the first block the result must be zero. For the others, we include
    the first element of the bin count. We then simply implement the code we saw earlier,
    but add in `tid_offset` to read/write to the appropriate elements.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于第一个块，结果必须为零。对于其他块，我们包括第一元素的箱数。然后我们只需实现之前看到的代码，但加上`tid_offset`来读写适当的元素。
- en: '[PRE95]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: The addition kernel is very simple. The program simply calculates the threads
    individual `tid` and uses this to index into the destination array. The value
    to add is taken from the block count. Implicit in this implementation is the assumption
    the caller invokes *N* threads where *N* is the number of samples per thread used
    in the previous kernel. We do this explicitly because it allows the use of `blockIdx.x`
    (the block number) without the need to access a thread index. This allows the
    fetch to fall into the unified constant cache and cause a broadcast operation
    to all elements within the thread block.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 加法内核非常简单。程序只是计算线程的`tid`，并使用它在目标数组中进行索引。要添加的值来自块的计数。在此实现中隐含的假设是调用者调用了*N*个线程，其中*N*是之前内核中每个线程使用的样本数。我们显式地这样做，因为这使得我们可以使用`blockIdx.x`（块编号），而不需要访问线程索引。这使得数据抓取可以落入统一常量缓存，并对线程块中的所有元素进行广播操作。
- en: In addition, we have the simple prefix sum kernel, called when there are a small
    number of elements to process. The parallel version, because it has to do an additional
    block prefix step, another addition, plus synchronization, takes longer in such
    cases. Only with larger block sizes where we can make better use of the hardware
    do we see a significant speedup with the more complex version.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还有简单的前缀和内核，在需要处理少量元素时调用。并行版本因为需要执行额外的块前缀步骤、再加一次加法和同步，所以在这种情况下会花费更多时间。只有当块大小较大时，我们能更好地利用硬件，才能看到更复杂版本的显著加速。
- en: '[PRE96]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'And finally the host function that sequences the kernels:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是主机函数，用于调度内核：
- en: '[PRE97]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '`  // Calculate prefix for the block sums`'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '`  // 计算块总和的前缀和`'
- en: '[PRE98]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: In this function we first check if it is best to use the simple prefix sum or
    the more complex prefix sum calculation. For the more complex solution, we work
    out how many elements each thread will initially process. We then call the three
    kernels in sequence. The function parameterized `num_threads_per_block` and `num_blocks`
    allow us to vary these parameters to allow for tuning.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们首先检查是使用简单的前缀和还是更复杂的前缀和计算。对于更复杂的解决方案，我们计算每个线程最初要处理多少元素。然后我们按顺序调用三个内核。函数参数`num_threads_per_block`和`num_blocks`允许我们调整这些参数，以便进行调优。
- en: At 4K sample points we see a transition between the two functions where the
    simpler version is around the same speed as the more complex version. As we get
    up to 16 K samples, the more complex version is already faster by a factor of
    four.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在4K样本点时，我们看到两个函数之间的过渡，其中简单版本的速度与复杂版本相当。当样本数增加到16K时，复杂版本的速度已经快了四倍。
- en: Sorting into bins
  id: totrans-556
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将数据排序到各个桶中
- en: 'To avoid the merge operation, the samples must be pre-sorted into *N* bins.
    This involves at least one run through the entire array and a shuffle of data
    into the correct bins. The CPU code for this is as follows:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免合并操作，样本必须预先排序到*N*个桶中。这至少需要遍历整个数组一次，并将数据重新排列到正确的桶中。对应的CPU代码如下：
- en: '[PRE99]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '`  const u32 bin = bin_search3(sample_data,`'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '`  const u32 bin = bin_search3(sample_data,`'
- en: '[PRE100]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Each data point in the source array needs to be placed into one of *N* bins
    that are linear in memory. The start and end of each bin has been calculated as
    an offset into the array. We need to preserve this data, but at the same time
    create *N* index pointers that track where we are in each bin. Thus, initially
    a copy of the `dest_bin_idx` array, the array storing the prefix indexes, must
    be made.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 源数组中的每个数据点需要被放入*N*个线性存储的桶中。每个桶的开始和结束位置已计算为数组中的偏移量。我们需要保留这些数据，同时创建*N*个索引指针来跟踪我们在每个桶中的位置。因此，必须先复制`dest_bin_idx`数组，这个数组存储前缀索引。
- en: We then iterate over all the source points. For every source point a binary
    search is used to identify in which bin the data point should be placed. We then
    copy the data to the appropriate bin and increment the bin index pointer for that
    bin.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们遍历所有源点。对于每个源点，使用二分查找来确定数据点应该放入哪个桶。然后我们将数据复制到相应的桶中，并递增该桶的桶索引指针。
- en: When trying to convert this algorithm to a parallel one, you hit the common
    problem of multiple threads trying to write to the same data item. There are two
    choices in this case. The first is to separate the data into *N* separate blocks
    and process each separately and then merge the final output. This was the approach
    used in the prefix sum kernel we looked at previously. There is, however, an alternative
    approach, which we’ll use here.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试将这个算法转换为并行算法时，你会遇到多个线程尝试写入相同数据项的常见问题。在这种情况下，有两种选择。第一种是将数据分离成*N*个独立的块，分别处理每个块，然后合并最终输出。这就是我们之前查看的前缀和内核中使用的方法。然而，还有另一种方法，我们将在这里使用。
- en: '[PRE101]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '`                             data,`'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '`                             data,`'
- en: '[PRE102]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: This is the approach of using atomics that in most cases allows for a much simpler
    implementation. However, this usually comes at the cost of performance. We can,
    of course, at a later date simply replace the atomic usage with an algorithm that
    splits and then merges the data. It’s a tradeoff between programming effort in
    terms of higher complexity, which means longer development time and a higher number
    of errors, versus the sometimes very small gain in performance. If you have sufficient
    time, try both approaches. At the very least this provides a solution for older
    hardware where atomic support is somewhat limited.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种使用原子操作的方法，在大多数情况下可以实现更简单的实现。然而，这通常会以牺牲性能为代价。当然，我们可以在后期将原子的使用替换为一种拆分并合并数据的算法。这是在编程复杂度较高、意味着更长的开发时间和更多的错误的情况下，与有时性能增益很小之间的权衡。如果有足够的时间，可以尝试这两种方法。至少，这为旧硬件提供了一个解决方案，因为旧硬件的原子支持通常有限。
- en: The atomic `sort_to_bins_gpu_kernel` function simply unrolls the loop construct
    over the number of source elements from the CPU code into *N* parallel threads.
    These are then implemented as a combination of threads and blocks to invoke one
    thread per data element.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 原子`sort_to_bins_gpu_kernel`函数简单地将CPU代码中的循环结构展开到*N*个并行线程。这些线程随后通过线程和块的组合实现，每个数据元素由一个线程调用。
- en: The thread reads the source element and does a binary search on the sample data
    space to find the appropriate bin for the element. We, however, then need single-thread
    access to increment the counter that stores the index into which the element must
    be written. You cannot simply increment the counter as shown in the CPU code,
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 线程读取源元素并对样本数据空间进行二分查找，以找到适当的箱子。然而，我们随后需要单线程访问来递增存储索引的计数器，以便将元素写入该索引。你不能像在CPU代码中那样简单地递增计数器，
- en: '[PRE103]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Instead, we use an atomic call, `atomicAdd`:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们使用原子调用，`atomicAdd`：
- en: '[PRE104]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: The `atomicAdd` function, when used on global memory, will add the second formal
    parameter, in this case 1, to the value at the address of the first parameter.
    If more than one thread calls this function, we’re guaranteed that every addition
    will be completed. The `atomicAdd` function returns the value that it held prior
    to the addition. Thus, we can use the return value as a unique index into the
    array to write the new value to the bin.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '`atomicAdd`函数在全局内存中使用时，会将第二个形式参数，在这种情况下是1，添加到第一个参数的地址处的值。如果多个线程调用这个函数，我们可以保证每次加法操作都会完成。`atomicAdd`函数返回在加法操作之前它所持有的值。因此，我们可以使用返回值作为数组中的唯一索引，将新值写入箱子。'
- en: However, be aware that this algorithm will change the ordering of the elements
    within the bins, as the blocks may run in any order. Thus, this is not a simple
    memory copy, due to the potential for more than one thread to try to write to
    the same bin at once. Also note that with a mostly sorted list, most threads will
    be hitting the same atomic address. This causes a slower execution, as you might
    expect, compared with that where the data is uniformly distributed.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请注意，这个算法会改变箱子内元素的顺序，因为块的执行顺序可能是任意的。因此，这不仅仅是简单的内存复制，因为可能有多个线程尝试同时写入同一个箱子。还要注意的是，在一个几乎已排序的列表中，大多数线程会访问相同的原子地址。正如你预期的那样，这会导致比数据均匀分布时更慢的执行。
- en: Sorting the bins
  id: totrans-575
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对箱子进行排序
- en: Having sorted the data into bins, we then need to sort each individual bin in
    some parallel manner. On the CPU side we simply call `qsort` (quick sort) on each
    bin. On the GPU side we use the radix sort.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据分类到箱子中之后，我们需要以某种并行方式对每个单独的箱子进行排序。在CPU端，我们仅对每个箱子调用`qsort`（快速排序）。在GPU端，我们使用基数排序。
- en: '`__host__ TIMER_T sort_bins_gpu(`'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '`__host__ TIMER_T sort_bins_gpu(`'
- en: '[PRE105]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: We use a host function to invoke `num_samples` threads that are split into blocks
    depending on the number of threads requested per block.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个主机函数来调用`num_samples`个线程，这些线程根据每个块请求的线程数被分成多个块。
- en: '[PRE106]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: The kernel is a two-level kernel, as the array `dest_bin_idx` holds only the
    start index. For the last element, accessing `[tid+1]` would cause an array overflow
    issue, so the very last thread needs to be handled slightly differently.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 核心是一个两级内核，因为数组`dest_bin_idx`只保存了起始索引。对于最后一个元素，访问`[tid+1]`会导致数组溢出问题，因此最后一个线程需要稍微不同的处理。
- en: Sorting the multiple blocks is done with a modified version of the `radix_sort`
    kernel we developed in [Chapter 5](CHP005.html).
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 排序多个块是通过我们在[第5章](CHP005.html)中开发的修改版`radix_sort`内核完成的。
- en: '[PRE107]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: The radix sort simply iterates over the dataset it has been provided for a given
    block. For each bit it places the value into either the 0 or 1 list. The caller
    defines the start and end indexes of the array over which the sort will take place.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 基数排序只是遍历给定块的数据集。对于每一位，它将值放入0或1的列表中。调用者定义了排序将进行的数组的起始和结束索引。
- en: Analyzing the results
  id: totrans-585
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析结果
- en: 'With a sample size of 16 K and a source dataset size of 1 MB we see the following
    results on a mostly sorted list:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本大小为16K，源数据集大小为1MB的情况下，我们在一个基本已排序的列表上得到了以下结果：
- en: '[PRE108]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '`Qsort Time   - CPU: 186.13 GPU:N/A`'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '`Qsort 时间 - CPU: 186.13 GPU:N/A`'
- en: Notice how the entire sort process (224–245 ms) is dominated by the sorting
    of the sample dataset on the GPU (~125 ms). As the sample dataset becomes large
    the sort-and-merge approach used for this phase doesn’t work well.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，整个排序过程（224–245毫秒）主要被GPU上样本数据集的排序所主导（约125毫秒）。随着样本数据集变大，排序和合并的方式在此阶段表现不佳。
- en: One solution to this problem would be to run the sample sort on the sample data;
    where the sample dataset is large, this is a good approach. However, for a 16
    K sample set, it takes around 9 ms to run the sample sort compared with a 2 ms
    quick sort time from the CPU.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是对样本数据运行样本排序；当样本数据集较大时，这是一种有效的方法。然而，对于16K的样本集，样本排序的运行时间大约是9毫秒，而CPU的快速排序时间为2毫秒。
- en: It always makes sense to use whatever device works best at a given solution.
    For small sample sizes, the CPU will usually be faster than the GPU. The GPU requires
    reasonably sized datasets, after which point it easily surpasses the CPU. Therefore,
    the optimal solution is simply to run quick sort on the sample set on the CPU
    and then transfer this to the GPU for the large-scale parallel “grunt” work of
    the sorting.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 总是使用在给定解决方案中表现最佳的设备是有意义的。对于小样本集，CPU通常会比GPU更快。GPU需要较为合理大小的数据集，在此之后它会轻松超越CPU。因此，最佳方案是先在CPU上对样本集进行快速排序，然后将其转移到GPU上进行大规模并行的“重”工作（排序）。
- en: When we use this approach the timings drop significantly.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用这种方法时，时间显著下降。
- en: '[PRE109]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: You can see the sample sort time is around 55% of the time of the quick sort
    on the CPU with a 16 K sample size (101 ms GPU, 185 ms CPU). If we vary the sample
    size, we increase the amount of available parallelism in the problem. See [Table
    6.15](#T0080) and [Figure 6.26](#F0135).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，样本排序的时间大约是CPU快速排序时间的55%（16K样本时GPU为101毫秒，CPU为185毫秒）。如果我们改变样本大小，就能增加问题中的并行性。见[表6.15](#T0080)和[图6.26](#F0135)。
- en: Table 6.15 Sample Sort Results (ms)
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.15 样本排序结果（毫秒）
- en: '![Image](../images/T000065tabT0080.jpg)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000065tabT0080.jpg)'
- en: '![image](../images/F000065f06-26-9780124159334.jpg)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000065f06-26-9780124159334.jpg)'
- en: Figure 6.26 Graph of sample sort results.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 样本排序结果图。
- en: What you can see from [Table 6.15](#T0080) and [Figure 6.26](#F0135) is that
    as the number of samples increases, the time drops dramatically. The best time
    is achieved for the GTX460 at 128K samples, or one-eighth of the number of the
    data to be sorted. The GTX470, with its much larger number of SMs, starts to rapidly
    outperform the GTX460 from 2048 sample points onward. The GTX260 by comparison
    (the previous generation of hardware) needs many more sample points to come close
    to the Fermi performance.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表6.15](#T0080)和[图6.26](#F0135)可以看到，随着样本数量的增加，时间显著下降。在128K样本时，GTX460达到了最佳时间，即排序数据量的八分之一。GTX470由于拥有更多的SM单元，从2048个样本点开始，性能迅速超过了GTX460。相比之下，GTX260（上一代硬件）需要更多的样本点才能接近Fermi的性能。
- en: At 128K sample points the sorting of the samples again becomes significant (see
    [Table 6.16](#T0085)) and our strategy of using quick sort on the CPU becomes
    the bottleneck. If we look in detail at the results from the GTX470, we see that
    at 256K sample points up to 50% of the time is spent sorting the sample data.
    At this point a sample sort of the sample data becomes a good option ([Table 6.16](#T0085)).
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 在128K样本点时，样本排序变得再次重要（参见[表 6.16](#T0085)），我们使用CPU上的快速排序策略成为瓶颈。如果我们详细查看GTX470的结果，会发现256K样本点时，最多50%的时间都花费在排序样本数据上。此时，样本数据的样本排序成为一个不错的选择（参见[表
    6.16](#T0085)）。
- en: Table 6.16 GTX470 Sample Sort Results (Mostly Sorted Data)
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.16 GTX470样本排序结果（大致排序数据）
- en: '![Image](../images/T000065tabT0085.jpg)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0085.jpg)'
- en: To give some comparison with almost sorted data versus entirely random data,
    we’ll run the same test over a random dataset ([Table 6.17](#T0090)). Various
    tests have shown the best performance was achieved with 128 threads per block.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与几乎排序的数据与完全随机数据进行对比，我们将在一个随机数据集上运行相同的测试（参见[表 6.17](#T0090)）。各种测试表明，最佳性能是在每个块128个线程时实现的。
- en: Table 6.17 Sample Sort on Random Data
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.17 随机数据上的样本排序
- en: '![Image](../images/T000065tabT0090.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000065tabT0090.jpg)'
- en: As you can see from [Table 6.17](#T0090) the faster run was the GTX470 at 67
    ms. This is five times faster than the serial quick sort on the CPU host. Around
    32 K samples with 128 threads per block would appear to be the optimal launch
    configuration for 1 MB of data. See [Figure 6.27](#F0140).
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从[表 6.17](#T0090)中看到的，最快的运行时间是GTX470，耗时67毫秒。这个速度比CPU主机上的串行快速排序快五倍。使用128个线程每个块处理大约32K个样本似乎是1MB数据的最佳启动配置。参见[图
    6.27](#F0140)。
- en: '![image](../images/F000065f06-27-9780124159334.jpg)'
  id: totrans-607
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-27-9780124159334.jpg)'
- en: Figure 6.27 Chart of sample sort on random data.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.27 随机数据上样本排序的图表。
- en: Questions on global memory
  id: totrans-609
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于全局内存的问题
- en: 1. Discuss the reasons why sample sort is quicker when the list is mostly sorted?
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 讨论当列表大致排序时，样本排序更快的原因？
- en: 2. How might you improve the sample sort algorithm presented here?
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 你如何改进这里展示的样本排序算法？
- en: 3. Do you foresee any problems using larger dataset sizes? What might you have
    to change to run larger datasets?
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 你预计使用更大数据集时会遇到什么问题？你可能需要做哪些更改才能运行更大的数据集？
- en: Answers on global memory
  id: totrans-613
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于全局内存的回答
- en: 1. Sample sort is quicker when using a mostly sorted list because the thread
    divergence is significantly less. Each bin has almost the same number of values.
    We end up with a near-optimal distribution of work to each of the SMs providing
    there are enough samples chosen from the dataset to generate a reasonable number
    of blocks.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 当使用大致排序的列表时，样本排序更快，因为线程分歧显著减少。每个桶中的值几乎相同。只要从数据集中选择足够的样本以生成合理数量的块，我们最终将每个SM分配的工作量接近最优。
- en: 2. One of the key issues with the algorithm is the noncoalesced access to global
    memory during the radix sort. This is caused by doing an in-place sort using the
    prefix calculation and the lower and upper bounds for each block. If you instead
    split each sample set so that it was interleaved by the number of threads, as
    was the radix sort in the shared memory example, we’d get coalesced access for
    most of the sort. The drawback of this is potentially wasted memory since some
    lists are a few entries long and others can be hundreds of entries long.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 该算法的一个关键问题是在基数排序过程中对全局内存的非合并访问。这是由于使用前缀计算和每个块的上下边界进行就地排序所造成的。如果你改为将每个样本集拆分，使其按线程数量交错，就像共享内存示例中的基数排序那样，我们将在大多数排序过程中获得合并访问。这样做的缺点是可能浪费内存，因为某些列表只有几项，而其他列表可能有数百项。
- en: The other obvious solution is to improve the sorting of the samples. At 128
    K samples, the sorting of sample data is contributing 43% of the total sort time.
    However, in practice, we’d never want to use so many samples, and the 32 K results
    are a more realistic use case. At this point sorting the samples contributes just
    7% (see [Table 6.16](#T0085)). The largest contributors are sorting the bins (62%),
    sorting to the bins (14%), and counting the bins (14%). The radix sort is clearly
    the place to start.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个明显的解决方案是改善样本排序。在128K样本时，样本数据的排序占总排序时间的43%。然而，在实际操作中，我们永远不会使用如此多的样本，32K的结果才是更现实的使用案例。此时，排序样本仅占7%（参见[表
    6.16](#T0085)）。最大的贡献者是排序桶（62%）、排序到桶中（14%）和计数桶（14%）。基数排序显然是最好的起点。
- en: 3. As the data size increases, you rapidly hit the maximum number of allowed
    blocks (65,535 on compute 2.x or lower platforms) using a single dimension. At
    this point you need to convert the `num_block` calculation in the various kernel
    invocations to a `dim3` type to include an `x` and `y` components in the block
    layout and possibly multiple grids, if the data size is really large. You then,
    of course, also need to modify the kernels to calculate correctly the block index
    based on block dimension and grid size.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加，使用单一维度时，您很快会达到允许的最大块数（在计算 2.x 或更低平台上为 65,535）。此时，您需要将各种内核调用中的`num_block`计算转换为`dim3`类型，以便在块布局中包括`x`和`y`组件，并且如果数据量非常大，可能还需要多个网格。然后，您当然还需要修改内核，以便根据块维度和网格大小正确计算块索引。
- en: Texture Memory
  id: totrans-618
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 纹理内存
- en: 'Texture memory is not something we will cover in any detail in this text. However,
    we will mention it for some of the special uses it has in case it may be of some
    use in your applications. Texture memory can be used for two primary purposes:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 本文不会详细介绍纹理内存。然而，我们会提及它的一些特殊用途，以防它对您的应用有用。纹理内存可以用于两种主要用途：
- en: • Caching on compute 1.x and 3.x hardware.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: • 计算 1.x 和 3.x 硬件上的缓存。
- en: • Hardware-based manipulation of memory reads.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: • 基于硬件的内存读取操作。
- en: Texture caching
  id: totrans-622
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 纹理缓存
- en: As compute 1.x hardware has no cache to speak of, the 6–8K of texture memory
    per SM provides the only method to truly cache data on such devices. However,
    with the advent of Fermi and its up to 48 K L1 cache and up to 768 K shared L2
    cache, this made the usage of texture memory for its cache properties largely
    obsolete. The texture cache is still present on Fermi to ensure backward compatibility
    with previous generations of code.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算 1.x 硬件几乎没有缓存可言，因此每个 SM 6-8K 的纹理内存提供了在此类设备上真正缓存数据的唯一方法。然而，随着 Fermi 的出现及其高达
    48K 的 L1 缓存和高达 768K 的共享 L2 缓存，纹理内存作为缓存属性的使用基本上已经过时。纹理缓存仍然存在于 Fermi 上，以确保与先前版本代码的向后兼容性。
- en: The texture cache is optimized for locality, that is, it expects data to be
    provided to adjacent threads. This is largely the same cache policy as the L1
    cache on Fermi. Unless you are using the other aspects of texture memory, texture
    memory brings you little benefit for the considerable programming effort required
    to use it on Fermi. However, on Kepler, the texture cache gets a special compute
    path, removing the complexity associated with programming it. See Kepler in [Chapter
    12](CHP012.html) for details. Note the constant memory cache is the only other
    cache on compute 1.x hardware that is organized for broadcast access, that is,
    all threads accessing the same memory address.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理缓存针对局部性进行了优化，即它期望将数据提供给相邻的线程。这在很大程度上与 Fermi 上的 L1 缓存策略相同。除非您使用纹理内存的其他方面，否则在
    Fermi 上使用它所需的编程工作量与它能带来的好处相比相对较小。然而，在 Kepler 上，纹理缓存获得了一条专门的计算路径，消除了与编程相关的复杂性。有关详细信息，请参见[第12章](CHP012.html)。请注意，常量内存缓存是计算
    1.x 硬件上唯一一个为广播访问组织的缓存，即所有线程访问相同的内存地址。
- en: On compute 1.x hardware, however, the texture cache can be of considerable use.
    If you consider a memory read that exhibits some locality, you can save a considerable
    number of memory fetches. Suppose we needed to perform a gather operation from
    memory, that is, to read an out-of-sequence set of memory addresses into *N* threads.
    Unless the thread pattern creates an aligned and sequential memory pattern, the
    coalescing hardware will issue multiple reads. If we instead load the data via
    the texture memory, most of the reads will hit the texture cache, resulting in
    a considerable performance benefit.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在计算 1.x 硬件上，纹理缓存可以大有帮助。如果您考虑执行具有一定局部性的内存读取，您可以节省大量内存提取。例如，如果我们需要从内存执行聚集操作，即将一组无序的内存地址读取到
    *N* 个线程中。除非线程模式创建对齐且连续的内存模式，否则合并硬件将发出多个读取操作。如果我们改为通过纹理内存加载数据，大多数读取将命中纹理缓存，从而带来显著的性能提升。
- en: You can, of course, equally use shared memory for this purpose, reading in a
    coalesced way from memory and then performing a read from the shared memory. As
    the shared memory of a compute 1.x device is limited to 16 K, you may decide to
    allocate shared memory to a specific purpose and use texture memory where the
    memory pattern is not so deterministic.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您也可以同样使用共享内存来实现此目的，先以合并的方式从内存读取数据，然后从共享内存中执行读取操作。由于计算 1.x 设备的共享内存仅限于 16K，您可能决定将共享内存分配给特定目的，并在内存模式不那么确定的情况下使用纹理内存。
- en: Hardware manipulation of memory fetches
  id: totrans-627
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件操作内存提取
- en: The second and perhaps more useful aspect of texture-based memory is that it
    allows some of the hardware aspects of GPUs to be automatically applied when accessing
    memory cells.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理内存的第二个，或许更有用的方面是，当访问内存单元时，它允许自动应用一些GPU硬件特性。
- en: One useful feature is a low-resolution linear interpolation in hardware. Typically,
    linear interpolation is used to represent a function where the output is not easy
    or is computationally expensive to express mathematically. Thus, the input from
    a sensor might have a correction applied to its value at the low or high end of
    its range. Rather than model this you simply place a number of points in a table
    that represent discrete values across the range. For the points falling between
    the real points you use linear interpolation to work out the approximate value.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用的功能是硬件中的低分辨率线性插值。通常，线性插值用于表示一个函数，该函数的输出不容易表示或在计算上代价较高。因此，来自传感器的输入可能会在其范围的低端或高端应用修正。与其建模这个，你可以简单地在一个表格中放置一些代表跨越范围的离散值的点。对于位于实际点之间的点，你可以使用线性插值来计算近似值。
- en: Consider an interpolation table of
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个插值表
- en: '[PRE110]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: If we have a new value, 5 for `X`, what is its interpolated value of `P`? The
    value 5 falls exactly halfway between the two points we have defined, 2 and 4\.
    As the value for 2 is 20 and the value for 4 is 40 we can easily calculate the
    value for 5 as 30\. See [Figure 6.28](#F0145).
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个新值，`X`为5，那么它的插值`P`值是多少？值5恰好位于我们定义的两个点之间，2和4之间。由于2的值为20，4的值为40，我们可以很容易地计算出5的值为30。见[图
    6.28](#F0145)。
- en: '![image](../images/F000065f06-28-9780124159334.jpg)'
  id: totrans-633
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000065f06-28-9780124159334.jpg)'
- en: Figure 6.28 Interpolation.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.28 插值。
- en: With texture memory, you can set it up such that `P` is defined as an array
    normalized from the value 0 to 1 or −1 to +1\. Fetches are then automatically
    interpolated in hardware. Combined with the cache properties, this can be a quick
    method of handling data that is not easily represented as a pure calculation.
    Bilinear and trilinear interpolation in hardware is also supported for two-dimensional
    and three-dimensional arrays, respectively.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 使用纹理内存，你可以将`P`设置为一个从0到1或−1到+1归一化的数组。然后，提取会在硬件中自动插值。结合缓存特性，这可以是处理那些不容易表示为纯计算的数据的快速方法。硬件中还支持二维和三维数组的双线性和三线性插值。
- en: One other nice feature of textures is the automatic handling of boundary conditions
    on array indexes. You can configure the handling of texture arrays to either wrap
    around or clamp at the array boundary. This can be useful, as it allows the normal
    case to be handled for all elements without having to embed special edge handling
    code. Special case code typically causes thread divergence and may not be necessary
    at all with the caching features of Fermi (see [Chapter 9](CHP009.html) on optimization).
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理的另一个优点是自动处理数组索引的边界条件。你可以配置纹理数组的处理方式，要么是环绕数组边界，要么在数组边界处夹紧。这很有用，因为它允许正常情况处理所有元素，而无需嵌入特殊的边缘处理代码。特殊情况代码通常会导致线程分歧，而且在Fermi的缓存特性下，可能完全不需要这些代码（见[第9章](CHP009.html)关于优化的内容）。
- en: Restrictions using textures
  id: totrans-637
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用纹理的限制
- en: Textures come from the graphics world of the GPU and therefore are less flexible
    than the standard CUDA types. Textures must be declared as a fixed type, i.e.
    one of the various aligned vector types (u8, u16, u32, s8, s16, s32) at compile
    time. How the values are interpreted is specified at runtime. Texture memory is
    read only to the GPU kernel and must be explicitly accessed via a special texture
    API (e.g., `tex1Dfetch()`, etc.) and arrays bound to textures.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理来自GPU的图形世界，因此比标准CUDA类型灵活性差。纹理必须在编译时声明为固定类型，即各种对齐向量类型之一（u8、u16、u32、s8、s16、s32）。值如何被解释在运行时指定。纹理内存仅对GPU内核可读，必须通过特殊的纹理API（例如，`tex1Dfetch()`等）和绑定到纹理的数组显式访问。
- en: Textures have their uses, especially on compute 1.x hardware. The uses for textures
    are quite specific and not always worth the trouble of learning yet another API.
    Thus, we have not covered the API side in this section, but simply stated some
    of the typical uses for textures. Concentrate on getting global/shared memory
    and register usage mastered and then look at texture memory, if it’s applicable
    to your application.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理有其用途，尤其是在计算1.x硬件上。纹理的用途是相当特定的，并且并不总是值得学习另一个API。因此，我们在本节中没有涉及API方面，而是简单列出了纹理的一些典型用途。集中精力掌握全局/共享内存和寄存器的使用，然后再考虑纹理内存，前提是它适用于你的应用程序。
- en: For further information on textures, see the CUDA C Programming Guide.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 有关纹理的更多信息，请参见CUDA C编程指南。
- en: Conclusion
  id: totrans-641
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We’ve looked at some of the aspects of using the different memory systems within
    the GPU. Program performance, both in the CPU and GPU domains, is generally dominated
    by memory throughput. You should have understood the principle of locality (i.e.,
    the closer to the device the data is, the faster it can be accessed) and the cost
    of accessing off-chip resources.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了在GPU内使用不同内存系统的一些方面。程序性能，无论是在CPU还是GPU领域，通常由内存吞吐量主导。你应该已经理解了局部性原理（即，数据离设备越近，访问速度越快）和访问外部芯片资源的成本。
- en: Understanding the three major classes of storage available—registers, shared
    memory, and global memory—should allow you to write programs that use each type
    efficiently and correctly.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 理解三种主要存储类型——寄存器、共享内存和全局内存——应该能让你编写高效且正确使用每种类型的程序。
- en: With global memory you need to think about generating patterns that provide
    for good coalescing of data and reduce the number of transactions the device needs
    to issue to the memory subsystem.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全局内存时，你需要考虑生成能够提供良好数据合并并减少设备需要发出给内存子系统的事务数量的模式。
- en: Consider using constant memory when you are going to be distributing the same
    value to many threads, or the same value to many blocks of thread.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 当你要将相同的值分发给多个线程，或者将相同的值分发给多个线程块时，可以考虑使用常量内存。
- en: With shared memory you need to think about data reuse. If there is no data reuse,
    then use registers and read directly from constant/global memory. Where there
    is potential for reuse or you need more register space, use shared memory.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享内存时，你需要考虑数据重用。如果没有数据重用，那么使用寄存器并直接从常量/全局内存中读取。若存在重用的潜力，或者你需要更多的寄存器空间，则使用共享内存。
- en: Always use registers when possible, that is, declare data as local variables
    when possible. Think about each read to memory and if it will be reused. Avoid
    multiple writes to memory by writing to a register and writing back to memory
    later. Registers are the only way of achieving near full throughput of the device,
    but are a scarce and valuable resource. Be aware that excessive register usage
    can cause slowdowns due to spilling of registers to ‘local’ memory.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能使用寄存器，也就是说，在可能的情况下将数据声明为局部变量。考虑每次对内存的读取，看看是否会被重用。通过写入寄存器并稍后将数据写回内存来避免多次写入内存。寄存器是实现设备几乎满载吞吐量的唯一方式，但它们是稀缺且宝贵的资源。要意识到过度使用寄存器可能会导致性能下降，因为寄存器会溢出到“本地”内存。
- en: Now that you understand the principles of these memory types, we will, in subsequent
    chapters, look more at optimization and how these memory types can be used in
    practice.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了这些内存类型的原理，接下来的章节中，我们将更深入地探讨优化以及如何在实际中使用这些内存类型。
