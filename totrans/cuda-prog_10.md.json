["```cpp\n#include <stdlib.h>\n#include <stdio.h>\n#include <iostream>\n#include \"cuda.h\"\n#include \"cuda_helper.h\"\n#include \"common_types.h\"\n#include \"timer.h\"\n```", "```cpp\n// NPP Library\n#include \"npp.h\"\n#include \"nppcore.h\"\n#include \"nppdefs.h\"\n#include \"nppi.h\"\n#include \"npps.h\"\n#include \"nppversion.h\"\n#define NPP_CALL(x) {const NppStatus a = (x); if (a != NPP_SUCCESS) { printf(\"\\nNPP Error: (err_num=%d) \\n\", a); cudaDeviceReset(); ASSERT(0);} }\nint main(int argc, char ∗argv[])\n{\n```", "```cpp\n // Declare and allocate memory on the host\n Npp8u ∗ host_src_ptr1 = (u8 ∗) malloc(num_bytes);\n Npp8u ∗ host_src_ptr2 = (u8 ∗) malloc(num_bytes);\n Npp8u ∗ host_dst_ptr1 = (u8 ∗) malloc(num_bytes);\n Npp8u ∗ host_dst_ptr2 = (u8 ∗) malloc(num_bytes);\n // Check memory allocation worked\n if ( (host_src_ptr1 == NULL) ‖ (host_src_ptr2 == NULL) ‖\n      (host_dst_ptr1 == NULL) ‖ (host_dst_ptr2 == NULL) )\n {\n  printf(\"\\nError Allocating host memory\");\n  exit(0);\n }\n // Declare and allocate memory on the device\n Npp8u ∗ device_src_ptr1;\n Npp8u ∗ device_src_ptr2;\n Npp8u ∗ device_dst_ptr1;\n Npp8u ∗ device_dst_ptr2;\n CUDA_CALL(cudaMalloc((void ∗∗) &device_src_ptr1, num_bytes));\n CUDA_CALL(cudaMalloc((void ∗∗) &device_src_ptr2, num_bytes));\n CUDA_CALL(cudaMalloc((void ∗∗) &device_dst_ptr1, num_bytes));\n CUDA_CALL(cudaMalloc((void ∗∗) &device_dst_ptr2, num_bytes));\n // Fill host src memory with random data\n for (u32 i=0; i< num_bytes; i++)\n {\n  host_src_ptr1[i] = (rand() % 255);\n  host_src_ptr2[i] = (rand() % 255);\n }\n // Copy the random data to the device\n CUDA_CALL(cudaMemcpy(device_src_ptr1, host_src_ptr1, num_bytes, cudaMemcpyHostToDevice));\n CUDA_CALL(cudaMemcpy(device_src_ptr2, host_src_ptr2, num_bytes, cudaMemcpyHostToDevice));\n // Call NPP library to perform the XOR operation on the device\n TIMER_T start_time_device = get_time();\n NPP_CALL(nppsXor_8u(device_src_ptr1, device_src_ptr2, device_dst_ptr1, num_bytes));\n NPP_CALL(nppsAnd_8u(device_src_ptr1, device_dst_ptr1, device_dst_ptr2, num_bytes));\n TIMER_T delta_time_device = get_time() - start_time_device;\n // Copy the XOR’d data on the device back to the host\n```", "```cpp\n // Perform the same XOR followed by AND on the host\n TIMER_T start_time_cpu = get_time();\n for (u32 i=0; i< num_bytes; i++)\n {\n  host_dst_ptr2[i] = host_src_ptr1[i] ^ host_src_ptr2[i];\n  host_dst_ptr2[i] &= host_src_ptr1[i];\n }\n TIMER_T delta_time_cpu = get_time() - start_time_cpu;\n // Compare the device data with the host calculated version\n printf(\"\\nComparison between CPU and GPU processing: \");\n if (memcmp(host_dst_ptr1, host_dst_ptr2, num_bytes) == 0)\n {\n  printf(\"Passed\");\n }\n else\n {\n  printf(\"∗∗∗∗ FAILED ∗∗∗∗\");\n }\n printf(\"\\nCPU Time: %f, GPU Time: %f\", delta_time_cpu, delta_time_device);\n // Free host and device memory\n CUDA_CALL(cudaFree(device_src_ptr1));\n CUDA_CALL(cudaFree(device_src_ptr2));\n CUDA_CALL(cudaFree(device_dst_ptr1));\n CUDA_CALL(cudaFree(device_dst_ptr2));\n free(host_src_ptr1);\n free(host_src_ptr2);\n free(host_dst_ptr1);\n free(host_dst_ptr2);\n // Reset the device so it’s clear for next time\n CUDA_CALL(cudaDeviceReset());\n}\n```", "```cpp\nNPP_CALL(nppsXor_8u(device_src_ptr1, device_src_ptr2, device_dst_ptr1, num_bytes));\nNPP_CALL(nppsAnd_8u(device_src_ptr1, device_dst_ptr1, device_dst_ptr2, num_bytes));\n```", "```cpp\nvoid nppSetStream (cudaStream_t hStream);\n```", "```cpp\n// Max for compute 2.x devices is 16\n#define NUM_STREAMS 4\nint main(int argc, char ∗argv[])\n{\n // 64MB\n const int num_bytes = (1024u ∗ 255u ∗ 256) ∗ sizeof(Npp8u);\n // Select the GTX470 in our test setup\n CUDA_CALL(cudaSetDevice(0));\n printf(\"\\nXOR’ing with %d MB\", (num_bytes / 1024) / 1024);\n // Declare and allocate pinned memory on the host\n Npp8u ∗ host_src_ptr1;\n Npp8u ∗ host_src_ptr2;\n Npp8u ∗ host_dst_ptr1[NUM_STREAMS];\n Npp8u ∗ host_dst_ptr2;\n CUDA_CALL(cudaMallocHost((void ∗∗) &host_src_ptr1, num_bytes));\n CUDA_CALL(cudaMallocHost((void ∗∗) &host_src_ptr2, num_bytes));\n CUDA_CALL(cudaMallocHost((void ∗∗) &host_dst_ptr2, num_bytes));\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  CUDA_CALL(cudaMallocHost((void ∗∗) &(host_dst_ptr1[i]), num_bytes));\n }\n // Declare and allocate memory on the device\n Npp8u ∗ device_src_ptr1[NUM_STREAMS];\n Npp8u ∗ device_src_ptr2[NUM_STREAMS];\n Npp8u ∗ device_dst_ptr1[NUM_STREAMS];\n Npp8u ∗ device_dst_ptr2[NUM_STREAMS];\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  CUDA_CALL(cudaMalloc((void ∗∗) &(device_src_ptr1[i]), num_bytes));\n  CUDA_CALL(cudaMalloc((void ∗∗) &(device_src_ptr2[i]), num_bytes));\n```", "```cpp\n  CUDA_CALL(cudaMalloc((void ∗∗) &(device_dst_ptr2[i]), num_bytes));\n }\n // Fill host src memory with random data\n for (u32 i=0; i< num_bytes; i++)\n {\n  host_src_ptr1[i] = (rand() % 255);\n  host_src_ptr2[i] = (rand() % 255);\n }\n TIMER_T start_time_device = get_time();\n printf(\"\\nRunning Device Synchronous version\");\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  // Copy the random data to the device\n  CUDA_CALL(cudaMemcpy(device_src_ptr1[i], host_src_ptr1,\n              num_bytes, cudaMemcpyHostToDevice));\n  CUDA_CALL(cudaMemcpy(device_src_ptr2[i], host_src_ptr2,\n              num_bytes, cudaMemcpyHostToDevice));\n  // Call NPP library to perform the XOR operation on the device\n  NPP_CALL(nppsXor_8u(device_src_ptr1[i], device_src_ptr2[i],\n             device_dst_ptr1[i], num_bytes));\n  // Copy the XOR’d data on the device back to the host\n  CUDA_CALL(cudaMemcpy(host_dst_ptr1[i], device_dst_ptr1[i],\n             num_bytes, cudaMemcpyDeviceToHost));\n }\n // Grab the end time\n // Last memcpy is synchronous, so CPU time is fine\n TIMER_T delta_time_device = get_time() - start_time_device;\n printf(\"\\nRunning Host version\");\n // Perform the same XOR on the host\n TIMER_T start_time_cpu = get_time();\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  for (u32 i=0; i< num_bytes; i++)\n  {\n   host_dst_ptr2[i] = host_src_ptr1[i] ^ host_src_ptr2[i];\n  }\n }\n```", "```cpp\n TIMER_T delta_time_cpu = get_time() - start_time_cpu;\n // Compare the device data with the host calculated version\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  compare_results(host_dst_ptr1[i], host_dst_ptr2, num_bytes,\n             \"\\nSingle Stream Comparison between CPU and GPU processing: \");\n }\n printf(\"\\nRunning Device Asynchronous version\");\n // Now run and alternate streamed version\n // Create a stream to work in\n cudaStream_t async_stream[NUM_STREAMS];\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  CUDA_CALL(cudaStreamCreate(&async_stream[i]));\n }\n // Grab the CPU time again\n start_time_device = get_time();\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  // Tell NPP to use the correct stream\n  NPP_CALL(nppSetStream(async_stream[i]));\n  // Copy the random data to the device using async transfers\n  CUDA_CALL(cudaMemcpyAsync(device_src_ptr1[i], host_src_ptr1, num_bytes,\n             cudaMemcpyHostToDevice, async_stream[i]));\n  CUDA_CALL(cudaMemcpyAsync(device_src_ptr2[i], host_src_ptr2, num_bytes,\n            cudaMemcpyHostToDevice, async_stream[i]));\n  // Call NPP library to perform the XOR operation on the device\n  NPP_CALL(nppsXor_8u(device_src_ptr1[i], device_src_ptr2[i],\n             device_dst_ptr1[i], num_bytes));\n }\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  // Tell NPP to use the correct stream\n  NPP_CALL(nppSetStream(async_stream[i]));\n  // Copy the XOR’d data on the device back to the host using async mode\n  CUDA_CALL(cudaMemcpyAsync(host_dst_ptr1[i], device_dst_ptr1[i], num_bytes,\n            cudaMemcpyDeviceToHost, async_stream[i]));\n```", "```cpp\n // Wait for everything to complete\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  CUDA_CALL(cudaStreamSynchronize(async_stream[i]));\n }\n // Grab the end time\n TIMER_T delta_time_device_async = get_time() - start_time_device;\n // Compare the device data with the host calculated version\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  compare_results(host_dst_ptr1[i], host_dst_ptr2, num_bytes, \"\\nMulti Stream Comparison between CPU and GPU processing: \");\n }\n printf(\"\\nCPU Time: %.1f, GPU Sync Time: %.1f, GPU Async Time: %.1f\", delta_time_cpu, delta_time_device, delta_time_device_async);\n // Free host and device memory\n for (u32 i=0; i< NUM_STREAMS; i++)\n {\n  CUDA_CALL(cudaFree(device_src_ptr1[i]));\n  CUDA_CALL(cudaFree(device_src_ptr2[i]));\n  CUDA_CALL(cudaFree(device_dst_ptr1[i]));\n  CUDA_CALL(cudaFree(device_dst_ptr2[i]));\n  CUDA_CALL(cudaFreeHost(host_dst_ptr1[i]));\n  CUDA_CALL(cudaStreamDestroy(async_stream[i]));\n }\n CUDA_CALL(cudaFreeHost(host_src_ptr1));\n CUDA_CALL(cudaFreeHost(host_src_ptr2));\n CUDA_CALL(cudaFreeHost(host_dst_ptr2));\n // Reset the device so it’s clear for next time\n CUDA_CALL(cudaDeviceReset());\n}\n```", "```cpp\nint sum(const int x, const int y)\n{\n  return x+y;\n}\n```", "```cpp\nClassNameA::my_func();\nClassNameB::my_func();\n```", "```cpp\nthrust::host_vector <float> my_host_float_vector(200);\nthrust::device_vector <int> my_device_int_vector(500);\n```", "```cpp\nfor (int i=0; i < my_device_int_vector.size(); i++)\n{\n  int x = my_device_int_vector[i];\n  …\n}\n```", "```cpp\nthrust::copy(my_host_float_out_vector.begin(), my_host_float_out_vector.end(), my_device_float_results_vector.begin(), my_device_float_results_vector.begin() );\n```", "```cpp\nthrust::sort(device_array.begin(), device_array.end());\n```", "```cpp\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/generate.h>\n#include <thrust/sort.h>\n#include <thrust/copy.h>\n#include <cstdlib>\n// 1M Elements = 4MB Data\n#define NUM_ELEM (1024∗1024)\nint main(void)\n{\n // Declare an array on the host\n printf(\"\\nAllocating memory on host\");\n thrust::host_vector<int> host_array(NUM_ELEM);\n // Populate this array with random numbers\n printf(\"\\nGenerating random numbers on host\");\n```", "```cpp\n // Create a device array and populate it with the host values\n // A PCI-E transfer to device happens here\n printf(\"\\nTransferring to device\");\n thrust::device_vector<int> device_array = host_array;\n // Sort the array on the device\n printf(\"\\nSorting on device\");\n thrust::sort(device_array.begin(), device_array.end());\n // Sort the array on the host\n printf(\"\\nSorting on host\");\n thrust::sort(host_array.begin(), host_array.end());\n // Create a host array and populate it with the sorted device values\n // A PCI-E transfer from the device happens here\n printf(\"\\nTransfering back to host\");\n thrust::host_vector<int> host_array_sorted = device_array;\n printf(\"\\nSorting Complete\");\n return 0;\n}\n```", "```cpp\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/generate.h>\n#include <thrust/sort.h>\n#include <thrust/copy.h>\n#include <cstdlib>\n#include \"cuda_helper.h\"\n#include \"timer.h\"\nvoid display_gpu_name(void)\n{\n int device_num;\n struct cudaDeviceProp prop;\n CUDA_CALL(cudaGetDevice(&device_num));\n // Get the device name\n```", "```cpp\n // Print device name and logical to physical mapping\n printf(\"\\n\\nUsing CUDA Device %u. Device ID: %s on PCI-E %d\",\n  device_num, prop.name, prop.pciBusID);\n}\n// 4M Elements = 16MB Data\n#define NUM_ELEM (1024∗1024∗4)\nint main(void)\n{\n int num_devices;\n CUDA_CALL(cudaGetDeviceCount(&num_devices));\n for (int device_num = 0; device_num < num_devices; device_num++)\n {\n  CUDA_CALL(cudaSetDevice(device_num));\n  display_gpu_name();\n  const size_t size_in_bytes = NUM_ELEM ∗ sizeof(int);\n  printf(\"\\nSorting %lu data items (%lu MB)\", NUM_ELEM, (size_in_bytes/1024/1024));\n  // Allocate timer events to track time\n  float c2d_t, sort_d_t, sort_h_t, c2h_t;\n  cudaEvent_t c2d_start, c2d_stop;\n  cudaEvent_t sort_d_start, sort_d_stop;\n  cudaEvent_t c2h_start, c2h_stop;\n  CUDA_CALL(cudaEventCreate(&c2d_start));\n  CUDA_CALL(cudaEventCreate(&c2d_stop));\n  CUDA_CALL(cudaEventCreate(&sort_d_start));\n  CUDA_CALL(cudaEventCreate(&sort_d_stop));\n  CUDA_CALL(cudaEventCreate(&c2h_start));\n  CUDA_CALL(cudaEventCreate(&c2h_stop));\n  // Declare an array on the host\n  printf(\"\\nAllocating memory on host\");\n  thrust::host_vector<int> host_array(NUM_ELEM);\n  // Populate this array with random numbers\n  printf(\"\\nGenerating random numbers on host\");\n  thrust::generate(host_array.begin(), host_array.end(), rand);\n  // Create a device array and populate it with the host values\n  // A PCI-E transfer to device happens here\n  printf(\"\\nTransferring to device\");\n  CUDA_CALL(cudaEventRecord(c2d_start, 0));\n```", "```cpp\n  CUDA_CALL(cudaEventRecord(c2d_stop, 0));\n  // Sort the array on the device\n  printf(\"\\nSorting on device\");\n  CUDA_CALL(cudaEventRecord(sort_d_start, 0));\n  thrust::sort(device_array.begin(), device_array.end());\n  CUDA_CALL(cudaEventRecord(sort_d_stop, 0));\n  CUDA_CALL(cudaEventSynchronize(sort_d_stop));\n  // Sort the array on the host\n  printf(\"\\nSorting on host\");\n  sort_h_t = get_time();\n  thrust::sort(host_array.begin(), host_array.end());\n  sort_h_t = (get_time() - sort_h_t);\n  // Create a host array and populate it with the sorted device values\n  // A PCI-E transfer from the device happens here\n  printf(\"\\nTransfering back to host\");\n  CUDA_CALL(cudaEventRecord(c2h_start, 0));\n  thrust::host_vector<int> host_array_sorted = device_array;\n  CUDA_CALL(cudaEventRecord(c2h_stop, 0));\n  // Wait for last event to be recorded\n  CUDA_CALL(cudaEventSynchronize(c2h_stop));\n  printf(\"\\nSorting Complete\");\n  // Calculate time for each aspect\n  CUDA_CALL(cudaEventElapsedTime(&c2d_t, c2d_start, c2d_stop));\n  CUDA_CALL(cudaEventElapsedTime(&sort_d_t, sort_d_start, sort_d_stop));\n  CUDA_CALL(cudaEventElapsedTime(&c2h_t, c2h_start, c2h_stop));\n  printf(\"\\nCopy To Device : %.2fms\", c2d_t);\n  printf(\"\\nSort On Device : %.2fms\", sort_d_t);\n  printf(\"\\nCopy From Device : %.2fms\", c2h_t);\n  printf(\"\\nTotal Device Time: %.2fms\", c2d_t + sort_d_t + c2h_t);\n  printf(\"\\n\\nSort On Host : %.2fms\", sort_h_t);\n  CUDA_CALL(cudaEventDestroy(c2d_start));\n  CUDA_CALL(cudaEventDestroy(c2d_stop));\n  CUDA_CALL(cudaEventDestroy(sort_d_start));\n  CUDA_CALL(cudaEventDestroy(sort_d_stop));\n  CUDA_CALL(cudaEventDestroy(c2h_start));\n  CUDA_CALL(cudaEventDestroy(c2h_stop));\n }\n return 0;\n```", "```cpp\nUsing CUDA Device 0\\. Device ID: GeForce GTX 470 on PCI-E 8\nSorting 4194304 data items (16 MB)\nAllocating memory on host\nGenerating random numbers on host\nTransferring to device\nSorting on device\nSorting on host\nTransfering back to host\nExtracting data from Thrust vector\nSorted arrays Match\nRunning single core qsort comparison\nSorting Complete\nCopy To Device : 10.00ms\nSort On Device : 58.55ms\nCopy From Device : 12.17ms\nTotal Device Time : 80.73ms\nThrust Sort On Host: 2398.00ms\nQSort On Host : 949.00ms\n```", "```cpp\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/sequence.h>\n#include <thrust/sort.h>\n#include <thrust/copy.h>\n```", "```cpp\n#include \"cuda_helper.h\"\n#include \"timer.h\"\nvoid display_gpu_name(void)\n{\n int device_num;\n struct cudaDeviceProp prop;\n CUDA_CALL(cudaGetDevice(&device_num));\n // Get the device name\n CUDA_CALL( cudaGetDeviceProperties( &prop, device_num ) );\n // Print device name and logical to physical mapping\n printf(\"\\n\\nUsing CUDA Device %u. Device ID: %s on PCI-E %d\",\n  device_num, prop.name, prop.pciBusID);\n}\nlong int reduce_serial(const int ∗ __restrict__ const host_raw_ptr,\n                       const int num_elements)\n{\n long int sum = 0;\n for (int i=0; i < num_elements; i++)\n  sum += host_raw_ptr[i];\n return sum;\n}\nlong int reduce_openmp(const int ∗ __restrict__ const host_raw_ptr,\n            const int num_elements)\n{\n long int sum = 0;\n#pragma omp parallel for reduction(+:sum) num_threads(4)\n for (int i=0; i < num_elements; i++)\n  sum += host_raw_ptr[i];\n return sum;\n}\n// 1M Elements = 4MB Data\n#define NUM_ELEM_START (1024∗1024)\n```", "```cpp\nint main(void)\n{\n int num_devices;\n CUDA_CALL(cudaGetDeviceCount(&num_devices));\n for (unsigned long num_elem = NUM_ELEM_START; num_elem < NUM_ELEM_END; num_elem ∗=2)\n {\n  const size_t size_in_bytes = num_elem ∗ sizeof(int);\n  for (int device_num = 0; device_num < num_devices; device_num++)\n  {\n   CUDA_CALL(cudaSetDevice(device_num));\n   display_gpu_name();\n   printf(\"\\nReducing %lu data items (%lu MB)\", num_elem, (size_in_bytes/1024/1024));\n   // Allocate timer events to track time\n   float c2d_t, reduce_d_t, reduce_h_t, reduce_h_mp_t, reduce_h_serial_t;\n   cudaEvent_t c2d_start, c2d_stop;\n   cudaEvent_t sort_d_start, sort_d_stop;\n   CUDA_CALL(cudaEventCreate(&c2d_start));\n   CUDA_CALL(cudaEventCreate(&c2d_stop));\n   CUDA_CALL(cudaEventCreate(&sort_d_start));\n   CUDA_CALL(cudaEventCreate(&sort_d_stop));\n   // Declare an array on the host\n   thrust::host_vector<int> host_array(num_elem);\n   // Populate this array with random numbers\n   thrust::sequence(host_array.begin(), host_array.end());\n   // Create a device array and populate it with the host values\n   // A PCI-E transfer to device happens here\n   CUDA_CALL(cudaEventRecord(c2d_start, 0));\n   thrust::device_vector<int> device_array = host_array;\n   CUDA_CALL(cudaEventRecord(c2d_stop, 0));\n   // Sort the array on the device\n   CUDA_CALL(cudaEventRecord(sort_d_start, 0));\n   const long int sum_device = thrust::reduce(device_array.begin(), device_array.end());\n   CUDA_CALL(cudaEventRecord(sort_d_stop, 0));\n   CUDA_CALL(cudaEventSynchronize(sort_d_stop));\n   // Sort the array on the host\n   reduce_h_t = get_time();\n   const long int sum_host = thrust::reduce(host_array.begin(), host_array.end());\n```", "```cpp\n   // Allocate host memory\n   int ∗ const host_raw_ptr_2 = (int ∗) malloc(size_in_bytes);\n   int ∗p2 = host_raw_ptr_2;\n   if ( (host_raw_ptr_2 == NULL) )\n   {\n    printf(\"\\nError allocating host memory for extraction of thrust data\");\n    exit(0);\n   }\n   // Extract data from Thrust vector to normal memory block\n   for (int i=0; i<num_elem; i++)\n   {\n    ∗p2++ = host_array[i];\n   }\n   reduce_h_mp_t = get_time();\n   const long int sum_host_openmp = reduce_openmp(host_raw_ptr_2, num_elem);\n   reduce_h_mp_t = (get_time() - reduce_h_mp_t);\n   reduce_h_serial_t = get_time();\n   const long int sum_host_serial = reduce_serial(host_raw_ptr_2, num_elem);\n   reduce_h_serial_t = (get_time() - reduce_h_serial_t);\n   // Free memory\n   free(host_raw_ptr_2);\n   if ( (sum_host == sum_device) && (sum_host == sum_host_openmp) )\n    printf(\"\\nReduction Matched\");\n   else\n    printf(\"\\n∗∗∗∗ FAILED ∗∗∗∗\");\n   // Calculate time for each aspect\n   CUDA_CALL(cudaEventElapsedTime(&c2d_t, c2d_start, c2d_stop));\n   CUDA_CALL(cudaEventElapsedTime(&reduce_d_t, sort_d_start, sort_d_stop));\n   printf(\"\\nCopy To Device : %.2fms\", c2d_t);\n   printf(\"\\nReduce On Device : %.2fms\", reduce_d_t);\n   printf(\"\\nTotal Device Time : %.2fms\", c2d_t + reduce_d_t);\n   printf(\"\\n\\nThrust Reduce On Host: %.2fms\", reduce_h_t);\n   printf(\"\\nSerial Reduce On Host: %.2fms\", reduce_h_serial_t);\n   printf(\"\\nOpenMP Reduce On Host: %.2fms\", reduce_h_mp_t);\n   CUDA_CALL(cudaEventDestroy(c2d_start));\n   CUDA_CALL(cudaEventDestroy(c2d_stop));\n   CUDA_CALL(cudaEventDestroy(sort_d_start));\n   CUDA_CALL(cudaEventDestroy(sort_d_stop));\n  }\n }\n```", "```cpp\n}\n```", "```cpp\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/sort.h>\n#include <cstdlib>\n#include \"cuda_helper.h\"\n#include \"timer.h\"\n#include \"common_types.h\"\nvoid display_gpu_name(void)\n{\n int device_num;\n struct cudaDeviceProp prop;\n CUDA_CALL(cudaGetDevice(&device_num));\n // Get the device name\n CUDA_CALL( cudaGetDeviceProperties( &prop, device_num ) );\n // Print device name and logical to physical mapping\n printf(\"\\n\\nUsing CUDA Device %u. Device ID: %s on PCI-E %d\",\n  device_num, prop.name, prop.pciBusID);\n}\n__global__ void fill_memory(int ∗ const __restrict__ data,\n            const int num_elements)\n{\n const int tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n  data[tid] = (num_elements - tid);\n}\n// 4M Elements = 16MB Data\n#define NUM_ELEM (1024∗1024∗4)\nint main(void)\n{\n const size_t size_in_bytes = NUM_ELEM ∗ sizeof(int);\n display_gpu_name();\n```", "```cpp\n // Declare an array on the device\n printf(\"\\nAllocating memory on device\");\n int ∗ device_mem_ptr;\n CUDA_CALL(cudaMalloc((void ∗∗) &device_mem_ptr, size_in_bytes));\n const u32 num_threads = 256;\n const u32 num_blocks = (NUM_ELEM + (num_threads-1)) / num_threads;\n printf(\"\\nFilling memory with pattern\");\n fill_memory<<<num_threads, num_blocks>>>(device_mem_ptr, NUM_ELEM);\n // Convert the array to\n printf(\"\\nConverting regular device pointer to thrust device pointer\");\n thrust::device_ptr<int> thrust_dev_ptr(device_mem_ptr);\n // Sort the array on the device\n printf(\"\\nSorting on device\");\n thrust::sort(thrust_dev_ptr, thrust_dev_ptr + NUM_ELEM);\n printf(\"\\nFreeing memory on device\");\n CUDA_CALL(cudaFree(device_mem_ptr));\n return 0;\n}\n```", "```cpp\n#include <curand_kernel.h>\n```", "```cpp\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v4.1\\lib\\x64\\curand.lib\n```", "```cpp\n#include <stdio.h>\n#include <stdlib.h>\n#include <curand_kernel.h>\n#include \"cuda_helper.h\"\n#include \"cuda.h\"\n#define CURAND_CALL(x) {const curandStatus_t a = (x); if (a != CURAND_STATUS_SUCCESS) { printf(\"\\nCuRand Error: (err_num=%d) \\n\", a); cudaDeviceReset(); ASSERT(0);} }\n__host__ void print_array(const float ∗ __restrict__ const data, const int num_elem)\n{\n for (int i=0; i<num_elem; i++)\n {\n  if ( i% 4 == 0)\n```", "```cpp\n  printf(\"%2d: %f \", i, data[i]);\n }\n}\n__host__ int main(int argc, char ∗argv[])\n{\n const int num_elem = 32;\n const size_t size_in_bytes = (num_elem ∗ sizeof(float));\n curandGenerator_t rand_generator_device, rand_generator_host;\n const unsigned long int seed = 987654321;\n const curandRngType_t generator_type = CURAND_RNG_PSEUDO_DEFAULT;\n // Allocate memory on the device\n float ∗ device_ptr;\n CUDA_CALL( cudaMalloc( (void ∗∗) &device_ptr, size_in_bytes ));\n // Allocate memory on the host for the device copy\n float ∗ host_ptr;\n CUDA_CALL( cudaMallocHost( (void ∗∗) &host_ptr, size_in_bytes ));\n // Allocate memory on the host for the host version\n float ∗ host_gen_ptr = (float ∗) malloc(size_in_bytes);\n if (host_gen_ptr == NULL)\n {\n  printf(\"\\nFailed to allocation memory on host\");\n  exit(0);\n }\n // Print library version number\n int version;\n CURAND_CALL(curandGetVersion(&version));\n printf(\"\\nUsing CuRand Version: %d and generator: CURAND_RNG_PSEUDO_DEFAULT\", version);\n // Register the generator - note the different function calls\n CURAND_CALL(curandCreateGenerator(&rand_generator_device, generator_type));\n CURAND_CALL(curandCreateGeneratorHost(&rand_generator_host, generator_type));\n // Set the seed for the random number generators\n CURAND_CALL(curandSetPseudoRandomGeneratorSeed(rand_generator_device, seed));\n CURAND_CALL(curandSetPseudoRandomGeneratorSeed(rand_generator_host, seed));\n // Create a set of random numbers on the device and host\n CURAND_CALL(curandGenerateUniform(rand_generator_device, device_ptr, num_elem));\n CURAND_CALL(curandGenerateUniform(rand_generator_host, host_gen_ptr, num_elem));\n // Copy the set of device generated data to the host\n```", "```cpp\n printf(\"\\n\\nRandom numbers from GPU\");\n print_array(host_ptr, num_elem);\n printf(\"\\n\\nRandom numbers from Host\");\n print_array(host_gen_ptr, num_elem);\n printf(\"\\n\");\n // Free device resources\n CURAND_CALL(curandDestroyGenerator(rand_generator_device));\n CUDA_CALL(cudaFree(device_ptr));\n CUDA_CALL(cudaFreeHost(host_ptr));\n CUDA_CALL(cudaDeviceReset());\n // Free host resources\n CURAND_CALL(curandDestroyGenerator(rand_generator_host));\n free(host_gen_ptr);\n}\n```", "```cpp\nUsing CuRand Version: 4010 and generator: CURAND_RNG_PSEUDO_DEFAULT\nRandom numbers from GPU\n 0: 0.468090  1: 0.660579  2: 0.351722  3: 0.891716\n 4: 0.624544  5: 0.861485  6: 0.662096  7: 0.007847\n 8: 0.179364  9: 0.260115 10: 0.453508 11: 0.711956\n12: 0.973453 13: 0.152303 14: 0.784318 15: 0.948965\n16: 0.214159 17: 0.236516 18: 0.020540 19: 0.175973\n20: 0.085989 21: 0.863053 22: 0.908001 23: 0.539129\n24: 0.849580 25: 0.496193 26: 0.588651 27: 0.361609\n28: 0.025815 29: 0.778294 30: 0.194206 31: 0.478006\nRandom numbers from Host\n 0: 0.468090  1: 0.660579  2: 0.351722  3: 0.891716\n 4: 0.624544  5: 0.861485  6: 0.662096  7: 0.007847\n 8: 0.179364  9: 0.260115 10: 0.453508 11: 0.711956\n12: 0.973453 13: 0.152303 14: 0.784318 15: 0.948965\n16: 0.214159 17: 0.236516 18: 0.020540 19: 0.175973\n20: 0.085989 21: 0.863053 22: 0.908001 23: 0.539129\n24: 0.849580 25: 0.496193 26: 0.588651 27: 0.361609\n28: 0.025815 29: 0.778294 30: 0.194206 31: 0.478006\n```", "```cpp\n#include <stdio.h>\n#include <stdlib.h>\n#include <cublas_v2.h>\n#include \"cuda_helper.h\"\n#include \"cuda.h\"\n#define CUBLAS_CALL(x) {const cublasStatus_t a = (x); if (a != CUBLAS_STATUS_SUCCESS) { printf(\"\\nCUBLAS Error: (err_num=%d) \\n\", a); cudaDeviceReset(); ASSERT(0);} }\n__host__ void print_array(const float ∗ __restrict__ const data1,\n                          const float ∗ __restrict__ const data2,\n                          const float ∗ __restrict__ const data3,\n                          const int num_elem,\n                          const char ∗ const prefix)\n{\n printf(\"\\n%s\", prefix);\n for (int i=0; i<num_elem; i++)\n {\n  printf(\"\\n%2d: %2.4f %2.4f %2.4f \", i+1, data1[i], data2[i], data3[i]);\n }\n}\n__host__ int main(int argc, char ∗argv[])\n{\n const int num_elem = 8;\n const size_t size_in_bytes = (num_elem ∗ sizeof(float));\n // Allocate memory on the device\n float ∗ device_src_ptr_A;\n CUDA_CALL( cudaMalloc( (void ∗∗) &device_src_ptr_A, size_in_bytes ));\n float ∗ device_src_ptr_B;\n CUDA_CALL( cudaMalloc( (void ∗∗) &device_src_ptr_B, size_in_bytes ));\n float ∗ device_dest_ptr;\n CUDA_CALL( cudaMalloc( (void ∗∗) &device_dest_ptr, size_in_bytes ));\n // Allocate memory on the host for the device copy\n float ∗ host_src_ptr_A;\n CUDA_CALL( cudaMallocHost( (void ∗∗) &host_src_ptr_A, size_in_bytes ));\n float ∗ host_dest_ptr;\n CUDA_CALL( cudaMallocHost( (void ∗∗) &host_dest_ptr, size_in_bytes ));\n float ∗ host_dest_ptr_A;\n CUDA_CALL( cudaMallocHost( (void ∗∗) &host_dest_ptr_A, size_in_bytes ));\n float ∗ host_dest_ptr_B;\n```", "```cpp\n // Clear destination memory\n memset(host_dest_ptr_A, 0, size_in_bytes);\n memset(host_dest_ptr_B, 0, size_in_bytes);\n memset(host_dest_ptr, 0, size_in_bytes);\n // Init the CUBLAS library\n cublasHandle_t cublas_handle;\n CUBLAS_CALL(cublasCreate(&cublas_handle));\n // Print library version number\n int version;\n CUBLAS_CALL(cublasGetVersion(cublas_handle, &version));\n printf(\"\\nUsing CUBLAS Version: %d\", version);\n // Fill the first host array with known values\n for (int i=0; i < num_elem; i++)\n {\n  host_src_ptr_A[i] = (float) i;\n }\n print_array(host_src_ptr_A, host_dest_ptr_B, host_dest_ptr, num_elem, \"Before Set\");\n const int num_rows = num_elem;\n const int num_cols = 1;\n const size_t elem_size = sizeof(float);\n // Copy a matrix one cell wide by num_elem rows from the CPU to the device\n CUBLAS_CALL(cublasSetMatrix(num_rows, num_cols, elem_size, host_src_ptr_A,\n           num_rows, device_src_ptr_A, num_rows));\n // Clear the memory in the other two\n CUDA_CALL(cudaMemset(device_src_ptr_B, 0, size_in_bytes));\n CUDA_CALL(cudaMemset(device_dest_ptr, 0, size_in_bytes));\n // SAXPY on device based on copied matrix and alpha\n const int stride = 1;\n float alpha = 2.0F;\n CUBLAS_CALL(cublasSaxpy(cublas_handle, num_elem, &alpha, device_src_ptr_A,\n            stride, device_src_ptr_B, stride));\n alpha = 3.0F;\n CUBLAS_CALL(cublasSaxpy(cublas_handle, num_elem, &alpha, device_src_ptr_A,\n           stride, device_dest_ptr, stride));\n // Calculate the index of the max of each maxtrix, writing the result\n // directly to host memory\n```", "```cpp\n CUBLAS_CALL(cublasIsamax(cublas_handle, num_elem, device_src_ptr_A,\n           stride, &host_max_idx_A));\n CUBLAS_CALL(cublasIsamax(cublas_handle, num_elem, device_src_ptr_B,\n           stride, &host_max_idx_B));\n CUBLAS_CALL(cublasIsamax(cublas_handle, num_elem, device_dest_ptr,\n           stride, &host_max_idx_dest));\n // Calculate the sum of each maxtrix, writing the result directly to host memory\n float host_sum_A, host_sum_B, host_sum_dest;\n CUBLAS_CALL(cublasSasum(cublas_handle, num_elem, device_src_ptr_A,\n            stride, &host_sum_A));\n CUBLAS_CALL(cublasSasum(cublas_handle, num_elem, device_src_ptr_B,\n            stride, &host_sum_B));\n CUBLAS_CALL(cublasSasum(cublas_handle, num_elem, device_dest_ptr,\n            stride, &host_sum_dest));\n // Copy device versions back to host to print out\n CUBLAS_CALL(cublasGetMatrix(num_rows, num_cols, elem_size, device_src_ptr_A,\n            num_rows, host_dest_ptr_A, num_rows));\n CUBLAS_CALL(cublasGetMatrix(num_rows, num_cols, elem_size, device_src_ptr_B,\n            num_rows, host_dest_ptr_B, num_rows));\n CUBLAS_CALL(cublasGetMatrix(num_rows, num_cols, elem_size, device_dest_ptr,\n            num_rows, host_dest_ptr, num_rows));\n // Make sure any async calls above are complete before we use the host data\n const int default_stream = 0;\n CUDA_CALL(cudaStreamSynchronize(default_stream));\n // Print out the arrays\n print_array(host_dest_ptr_A, host_dest_ptr_B, host_dest_ptr, num_elem, \"After Set\");\n // Print some stats from the arrays\n printf(\"\\nIDX of max values : %d, %d, %d\", host_max_idx_A,\n  host_max_idx_B, host_max_idx_dest);\n printf(\"\\nSUM of values : %2.2f, %2.2f, %2.2f\", host_sum_A,\n  host_sum_B, host_sum_dest);\n // Free device resources\n CUBLAS_CALL(cublasDestroy(cublas_handle));\n CUDA_CALL(cudaFree(device_src_ptr_A));\n CUDA_CALL(cudaFree(device_src_ptr_B));\n CUDA_CALL(cudaFree(device_dest_ptr));\n // Free host resources\n CUDA_CALL(cudaFreeHost(host_src_ptr_A));\n CUDA_CALL(cudaFreeHost(host_dest_ptr_A));\n```", "```cpp\n CUDA_CALL(cudaFreeHost(host_dest_ptr));\n // Reset ready for next GPU program\n CUDA_CALL(cudaDeviceReset());\n}\n```", "```cpp\ncutilSafeCall(cudaGetDeviceProperties(&deviceProps, devID));\n```", "```cpp\nFound 4 CUDA Capable device(s)\nDevice 0: \"GeForce GTX 470\"\n CUDA Driver Version / Runtime Version          4.1 / 4.1\n CUDA Capability Major/Minor version number:    2.0\n Total amount of global memory:                 1280 MBytes (1342177280 bytes)\n (14) Multiprocessors x (32) CUDA Cores/MP:     448 CUDA Cores\n GPU Clock Speed:                               1.22 GHz\n Memory Clock rate:                             1674.00 Mhz\n Memory Bus Width:                              320-bit\n L2 Cache Size:                                 655360 bytes\n Max Texture Dimension Size (x,y,z)             1D=(65536), 2D=(65536,65535), 3D=(2048,2048,2048)\n Max Layered Texture Size (dim) x layers        1D=(16384) x 2048, 2D=(16384,16384) x 2048\n Total amount of constant memory:               65536 bytes\n Total amount of shared memory per block:       49152 bytes\n Total number of registers available per block: 32768\n Warp size:                                     32\n Maximum number of threads per block:           1024\n Maximum sizes of each dimension of a block:    1024 x 1024 x 64\n Maximum sizes of each dimension of a grid:     65535 x 65535 x 65535\n Maximum memory pitch:                          2147483647 bytes\n Texture alignment:                             512 bytes\n Concurrent copy and execution:                 Yes with 1 copy engine(s)\n Run time limit on kernels:                     No\n Integrated GPU sharing Host Memory:            No\n Support host page-locked memory mapping:       Yes\n Concurrent kernel execution:                   Yes\n Alignment requirement for Surfaces:            Yes\n```", "```cpp\n Device is using TCC driver mode:               No\n Device supports Unified Addressing (UVA):      No\n Device PCI Bus ID / PCI location ID:           8 / 0\n Compute Mode:\n  < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\nDevice 1: \"GeForce 9800 GT\"\n CUDA Capability Major/Minor version number:    1.1\n Total amount of global memory:                 1024 MBytes (1073741824 bytes)\n (14) Multiprocessors x ( 8) CUDA Cores/MP:     112 CUDA Cores\n GPU Clock Speed:                               1.63 GHz\n Memory Clock rate:                             950.00 Mhz\n Memory Bus Width:                              256-bit\n Total amount of shared memory per block:       16384 bytes\n Total number of registers available per block: 8192\n Maximum number of threads per block:           512\n Device PCI Bus ID / PCI location ID:           7 / 0\nDevice 2: \"GeForce GTX 260\"\n CUDA Capability Major/Minor version number:    1.3\n Total amount of global memory:                 896 MBytes (939524096 bytes)\n (27) Multiprocessors x ( 8) CUDA Cores/MP:     216 CUDA Cores\n GPU Clock Speed:                               1.35 GHz\n Memory Clock rate:                             1100.00 Mhz\n Memory Bus Width:                              448-bit\n Total amount of shared memory per block:       16384 bytes\n Total number of registers available per block: 16384\n Maximum number of threads per block:           512\n Device PCI Bus ID / PCI location ID:           1 / 0\nDevice 3: \"GeForce GTX 460\"\n CUDA Capability Major/Minor version number:    2.1\n Total amount of global memory:                 1024 MBytes (1073741824 bytes)\n ( 7) Multiprocessors x (48) CUDA Cores/MP:     336 CUDA Cores\n GPU Clock Speed:                               1.45 GHz\n Memory Clock rate:                             1800.00 Mhz\n Memory Bus Width:                              256-bit\n L2 Cache Size:                                 524288 bytes\n Total amount of shared memory per block:       49152 bytes\n Total number of registers available per block: 32768\n Maximum number of threads per block:           1024\n Device PCI Bus ID / PCI location ID:           2 / 0\n```", "```cpp\nDevice 0: \"GeForce GTX 680\"\n```", "```cpp\n Total amount of global memory:                 2048 MBytes (2146762752 bytes)\n ( 8) Multiprocessors x (192) CUDA Cores/MP:    1536 CUDA Cores\n GPU Clock Speed:                               1006 MHz\n Memory Clock rate:                             3004.00 Mhz\n Memory Bus Width:                              256-bit\n L2 Cache Size:                                 524288 bytes\n Total amount of shared memory per block:       49152 bytes\n Total number of registers available per block: 65536\n Warp size: 32\n Maximum number of threads per block:           1024\n Concurrent copy and execution:                 Yes with 1 copy engine(s)\n```", "```cpp\n>> bandwidthtest --device=0 --memory=pageable\nDevice 0: GeForce GTX 470\n Quick Mode\n Host to Device Bandwidth, 1 Device(s), Paged memory\n  Transfer Size (Bytes) Bandwidth(MB/s)\n  33554432 1833.6\n Device to Host Bandwidth, 1 Device(s), Paged memory\n  Transfer Size (Bytes) Bandwidth(MB/s)\n  33554432 1700.5\n Device to Device Bandwidth, 1 Device(s)\n  Transfer Size (Bytes) Bandwidth(MB/s)\n  33554432 113259.3\n>> bandwidthtest --device=0 --memory=pinned\nHost to Device Bandwidth, 1 Device(s), Pinned memory\n```", "```cpp\n  33554432 2663.6\n Device to Host Bandwidth, 1 Device(s), Pinned memory\n  Transfer Size (Bytes) Bandwidth(MB/s)\n  33554432 3225.8\n Device to Device Bandwidth, 1 Device(s)\n  Transfer Size (Bytes) Bandwidth(MB/s)\n  33554432 113232.3\n```", "```cpp\nstruct cudaDevice device_prop;\nCUDA_CALL(cudaGetDeviceProperties(&device_prop));\nif (device_prop.unifiedAddressing == 1) // If unified addressing is enabled\n```", "```cpp\nint peer_access_avail;\nint src_device = 0;\nint peer_device = 1;\nCUDA_CALL(cudaDeviceCanAccessPeer( &peer_access_avail, src_device, peer_device));\nif (peer_access_avail == 1) // If peer access from device 0 to device 1 is available\n{\n  int flags = 0;\n  CUDA_CALL(cudaSetDevice(peer_device));\n  CUDA_CALL(cudaEnablePeerAccess(peer_device, flags);\n}\n```", "```cpp\ncudaMemcpyPeerAsync( dest_device_ptr, dst_device_num, src_device_ptr, src_device_num, num_bytes, stream );\n```", "```cpp\ncudaDeviceDisablePeerAccess(device_num);\n```", "```cpp\n#include <stdio.h>\n#include <omp.h>\n#include \"cuda_helper.h\"\n#include \"cuda.h\"\n__global__ void increment_kernel(int ∗ __restrict__ const data,\n                                 const int inc_value,\n                                 const int num_elem)\n{\n const int idx = blockIdx.x ∗ blockDim.x + threadIdx.x;\n // Check array index does not overflow the array\n```", "```cpp\n {\n  // Repeat N times - just to make the kernel take some time\n  const int repeat = 512;\n  for (int i=0; i < repeat; i++)\n   data[idx] += inc_value;\n }\n}\n// Max number of devices on any single node is, usually at most, eight\n#define MAX_NUM_DEVICES 8\n__host__ int main(int argc, char ∗argv[])\n{\n const int num_elem = 1024 ∗ 1024 ∗ 16;\n const int size_in_bytes = num_elem ∗ sizeof(int);\n const int increment_value = 1;\n const int loop_iteration_check = 1000000;\n const int shared_mem = 0;\n // Define the number of threads/blocks needed\n const int num_threads = 512;\n const int num_blocks = ((num_elem + (num_threads-1)) / num_threads);\n // One array element per CPU thread\n int host_counter[MAX_NUM_DEVICES];\n float delta_device_time[MAX_NUM_DEVICES];\n cudaDeviceProp device_prop[MAX_NUM_DEVICES];\n int num_devices;\n CUDA_CALL(cudaGetDeviceCount(&num_devices));\n printf(\"\\nIdentified %d devices. Spawning %d threads to calculate %d MB using (%dx%d)\", num_devices, num_devices, ((size_in_bytes/1024)/1024), num_blocks, num_threads );\n // Declare thread private, per thread variables\n int ∗ device_ptr[MAX_NUM_DEVICES];\n int ∗ host_ptr[MAX_NUM_DEVICES];\n cudaEvent_t start_event[MAX_NUM_DEVICES], stop_event[MAX_NUM_DEVICES];\n cudaStream_t async_stream[MAX_NUM_DEVICES];\n // Create all allocations outside of OpenMP in series\n for (int device_num=0; device_num < num_devices; device_num++)\n {\n  // Set the device to a unique device per CPU thread\n  CUDA_CALL(cudaSetDevice(device_num));\n  // Get the current device properties\n```", "```cpp\n  // Allocate the resources necessary\n  CUDA_CALL(cudaMalloc((void ∗∗) &device_ptr[device_num], size_in_bytes));\n  CUDA_CALL(cudaMallocHost((void ∗∗) &host_ptr[device_num], size_in_bytes));\n  CUDA_CALL(cudaEventCreate(&start_event[device_num]));\n  CUDA_CALL(cudaEventCreate(&stop_event[device_num]));\n  CUDA_CALL(cudaStreamCreate(&async_stream[device_num]));\n }\n // Spawn one CPU thread for each device\n#pragma omp parallel num_threads(num_devices)\n {\n  // Variables declared within the OpenMP block are thread private and per thread\n  // Variables outside OpenMP block exist once in memory and are shared between\n  // threads.\n  // Get our current thread number and use this as the device number\n  const int device_num = omp_get_thread_num();\n  // Set the device to a unique device per CPU thread\n  CUDA_CALL(cudaSetDevice(device_num));\n  // Push start timer, memset, kernel, copy back and stop timer into device queue\n  CUDA_CALL(cudaEventRecord(start_event[device_num], async_stream[device_num]));\n  // Copy the data to the device\n  CUDA_CALL(cudaMemsetAsync(device_ptr[device_num], 0, size_in_bytes,\n            async_stream[device_num]));\n  // Invoke the kernel\n  increment_kernel<<<num_blocks, num_threads, shared_mem, async_stream[device_num]>>>(device_ptr[device_num], increment_value, num_elem);\n  // Copy data back from the device\n  CUDA_CALL(cudaMemcpyAsync(host_ptr[device_num], device_ptr[device_num],\n           size_in_bytes, cudaMemcpyDeviceToHost,\n           async_stream[device_num]));\n  // Record the end of the GPU work\n  CUDA_CALL(cudaEventRecord(stop_event[device_num], async_stream[device_num]));\n  // Device work has now been sent to the GPU, so do some CPU work\n  // whilst we’re waiting for the device to complete its work queue\n  // Reset host counter\n  int host_counter_local = 0;\n```", "```cpp\n  // Do some work on the CPU until all the device kernels complete\n  do\n  {\n   // Insert useful CPU work here\n   host_counter_local++;\n   // Check device completion status every loop_iteration_check iterations\n   if ( (host_counter_local % loop_iteration_check) == 0 )\n   {\n    // Assume everything is now complete\n    complete = 1;\n    // Check if all GPU streams have completed. Continue to do more CPU\n    // work if one of more devices have pending work.\n    for ( int device_check_num=0; device_check_num < num_devices;\n     device_check_num++)\n   {\n     if ( cudaEventQuery(stop_event[device_check_num]) == cudaErrorNotReady )\n      complete = 0;\n    }\n   }\n  } while( complete == 0 );\n  // Write out final result\n  host_counter[device_num] = host_counter_local;\n  // Calculate elapsed GPU time\n  CUDA_CALL(cudaEventElapsedTime(&delta_device_time[device_num],\n            start_event[device_num],\n            stop_event[device_num]));\n } // End parallel region\n // Now running as a single CPU thread again\n // Free allocated resources\n // Create all allocations outside of OpenMP in series\n for (int device_num=0; device_num < num_devices; device_num++)\n {\n  // Set the device to a unique device per CPU thread\n  CUDA_CALL(cudaSetDevice(device_num));\n  CUDA_CALL(cudaStreamDestroy(async_stream[device_num]));\n  CUDA_CALL(cudaEventDestroy(stop_event[device_num]));\n  CUDA_CALL(cudaEventDestroy(start_event[device_num]));\n```", "```cpp\n  CUDA_CALL(cudaFree(device_ptr[device_num]));\n  // Reset the device for later use\n  CUDA_CALL(cudaDeviceReset());\n }\n // Print a summary of the results\n for (int device=0; device < num_devices; device++)\n {\n  printf(\"\\n\\nKernel Time for device %s id:%d: %.2fms\",\n         device_prop[device].name, device, delta_device_time[device]);\n  printf(\"\\nCPU count for thread %d: %d\", device, host_counter[device]);\n }\n}\n```", "```cpp\ncudaStream_t async_stream[MAX_NUM_DEVICES];\nCUDA_CALL(cudaSetDevice(device_num));\nCUDA_CALL(cudaStreamCreate(&async_stream[device_num]));\n```", "```cpp\nconst int num_elem = 1024 ∗ 1024 ∗ 16;\nconst int num_threads = 512;\nconst int num_blocks = ((num_elem + (num_threads-1)) / num_threads);\n```", "```cpp\n// Check array index does not overflow the array\nif (idx < num_elem)\n```", "```cpp\nomp_set_num_threads(num_gpus);\n//omp_set_num_threads(2∗num_gpus);\n#pragma omp parallel\n{\n}\n```", "```cpp\n// Spawn one CPU thread for each device\n#pragma omp parallel num_threads(num_devices)\n{\n}\n```", "```cpp\nIdentified 4 devices. Spawning 4 threads to calculate 64 MB using (32768x512)\nKernel Time for device GeForce GTX 470 id:0: 427.74ms\nCPU count for thread 0: 1239000000\nKernel Time for device GeForce 9800 GT id:1: 3300.55ms\nCPU count for thread 1: 1180000000\nKernel Time for device GeForce GTX 285 id:2: 1693.63ms\nCPU count for thread 2: 1229000000\nKernel Time for device GeForce GTX 460 id:3: 662.61ms\nCPU count for thread 3: 1254000000\n```", "```cpp\ntypedef struct __align__(8)\n{\n  unsigned int l, a;\n} LA32;\n```", "```cpp\nlong int reduce_serial(const int ∗ __restrict__ const host_ptr,\n                       const int num_elements)\n{\n long int sum = 0;\n for (int i=0; i < num_elements; i++)\n      sum += host_ptr[i];\n return sum;\n}\nlong int reduce_openmp(const int ∗ __restrict__ const host_ptr,\n                       const int num_elements)\n{\n long int sum = 0;\n#pragma omp parallel for reduction(+:sum)\n for (int i=0; i < num_elements; i++)\n {\n  sum += host_ptr[i];\n }\n return sum;\n}\nlong int reduce_openacc(const int ∗ __restrict__ const host_ptr,\n                        const int num_elements)\n{\n long int sum = 0;\n#pragma acc kernels\n for (int i=0; i < num_elements; i++)\n {\n  sum += host_ptr[i];\n }\n return sum;\n```", "```cpp\nAccelerator kernel generated\n60, #pragma acc loop gang, vector /∗ blockIdx.x threadIdx.x ∗/\nCC 1.3 : 21 registers; 1024 shared, 20 constant, 0 local memory bytes; 100% occupancy\nCC 2.0 : 23 registers; 1048 shared, 40 constant, 0 local memory bytes; 100% occupancy\n```", "```cpp\n#pragma acc data <directives>\n```", "```cpp\n#define NUM_ELEM 32768\n#pragma acc kernels loop gang(64), vector(128)\nfor( int i = 0; i < NUM_ELEM; i++ )\n{\n x[i] += y[i];\n}\n```", "```cpp\n#pragma acc kernels loop async\nfor (i=0; i< num_elem; i++)\n{\n…\n}\n```", "```cpp\n#pragma omp parallel num_thread(4)\n```", "```cpp\n#pragma omp parallel num_thread(4)\n{\nconst int cpu_thread_id = omp_get_thread_num();\nacc_set_device_num( cpu_thread_id, acc_device_nvidia );\n}\n```", "```cpp\nconst int num_gpus = acc_get_num_devices( acc_device_nvidia );\n#pragma omp parallel num_thread(4)\n{\nconst int cpu_thread_id = omp_get_thread_num();\nif (cpu_thread_id < num_gpus)\n{\n// Do CPU and GPU work\nacc_set_device_num( cpu_thread_id, acc_device_nvidia );\n}\nelse\n{\n    // Do CPU only work\n}\n}\n```", "```cpp\nconst int num_gpus = acc_get_num_devices( acc_device_nvidia );\n// Get my MPI virtual process id (rank)\nint my_rank;\nMPI_Comm_rank( MPI_COMM_WORLD, &my_rank );\nif ( my_rank < num_gpus)\n{\n// Do CPU and GPU work e.g. workers\nacc_set_device_num( my_rank, acc_device_nvidia );\n}\nelse\n{\n  // Do CPU only work, e.g. master\n```"]