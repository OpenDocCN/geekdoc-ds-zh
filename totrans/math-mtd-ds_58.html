<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>7.6. Further applications: Gibbs sampling and generating images#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>7.6. Further applications: Gibbs sampling and generating images#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap07_rwmc/06_gibbs/roch-mmids-rwmc-gibbs.html">https://mmids-textbook.github.io/chap07_rwmc/06_gibbs/roch-mmids-rwmc-gibbs.html</a></blockquote>

<p>In this section, we derive an important application of Markov chains known as Markov Chain Monte Carlo (MCMC). We specialize it to Gibbs sampling and apply it to the generation of handwritten digits using a Restricted Boltzmann Machine (RBM).</p>
<section id="markov-chain-monte-carlo-mcmc">
<h2><span class="section-number">7.6.1. </span>Markov chain Monte Carlo (MCMC)<a class="headerlink" href="#markov-chain-monte-carlo-mcmc" title="Link to this heading">#</a></h2>
<p>Suppose we are interested in generating samples from a target distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i \in \S}\)</span> over a set <span class="math notranslate nohighlight">\(\S\)</span>. We have done this before. For instance, we generated samples from a mixture of Gaussians to test <span class="math notranslate nohighlight">\(k\)</span>-means clustering in different dimensions. There are many more applications. A canonical one is to estimate the mean of a function <span class="math notranslate nohighlight">\(f\)</span> under <span class="math notranslate nohighlight">\(\bpi\)</span>: generate <span class="math notranslate nohighlight">\(n\)</span> independent samples <span class="math notranslate nohighlight">\(Z_1,\ldots,Z_n\)</span>, all distributed according to <span class="math notranslate nohighlight">\(\pi\)</span>, then compute</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n f(Z_i),
\]</div>
<p>which is approximately <span class="math notranslate nohighlight">\(\E[f(Z_1)]\)</span> by the law of large numbers, provided <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large. Furthermore, this type of problem plays an important role in <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a>.</p>
<p><strong>Sampling from simple distributions</strong> When <span class="math notranslate nohighlight">\(\bpi\)</span> is a standard distribution or <span class="math notranslate nohighlight">\(\S\)</span> is relatively small, this can be done efficiently by using a random number generator, as we have done previously.</p>
<p><strong>NUMERICAL CORNER:</strong> Recall how this works. We first initialize the random number generator and use a <code class="docutils literal notranslate"><span class="pre">seed</span></code> for reproducibility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To generate, say <span class="math notranslate nohighlight">\(1000\)</span>, samples from a multivariate normal, say with mean <span class="math notranslate nohighlight">\((0, 0)\)</span> and covariance <span class="math notranslate nohighlight">\(\begin{pmatrix}5 &amp; 0\\0 &amp; 1\end{pmatrix}\)</span>, we use <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html#numpy.random.Generator.multivariate_normal"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a> as follows. The <code class="docutils literal notranslate"><span class="pre">.T</span></code> below transposes the output to separate the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> coordinates into individual arrays.</p>
<p><code class="docutils literal notranslate"><span class="pre">rng.multivariate_normal(mean,</span> <span class="pre">cov,</span> <span class="pre">1000)</span></code> returns a <code class="docutils literal notranslate"><span class="pre">(1000,</span> <span class="pre">2)</span></code> array where each row is one sample:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span/><span class="p">[[</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span>
 <span class="p">[</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">],</span>
 <span class="p">[</span><span class="n">x3</span><span class="p">,</span> <span class="n">y3</span><span class="p">],</span>
 <span class="o">...</span>
 <span class="p">[</span><span class="n">x1000</span><span class="p">,</span> <span class="n">y1000</span><span class="p">]]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.T</span></code> transposes this to <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">1000)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span/><span class="p">[[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">x1000</span><span class="p">],</span>
 <span class="p">[</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">y1000</span><span class="p">]]</span>
</pre></div>
</div>
<p>Now <code class="docutils literal notranslate"><span class="pre">x,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">...</span></code> unpacks the two rows, giving you:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">[x1,</span> <span class="pre">x2,</span> <span class="pre">x3,</span> <span class="pre">...,</span> <span class="pre">x1000]</span></code> (all x-coordinates)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">[y1,</span> <span class="pre">y2,</span> <span class="pre">y3,</span> <span class="pre">...,</span> <span class="pre">y1000]</span></code> (all y-coordinates)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>Computing the mean of each component we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-0.035322561120667575
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-0.009499619370100139
</pre></div>
</div>
</div>
</div>
<p>This is somewhat close to the expected answer: <span class="math notranslate nohighlight">\((0,0)\)</span>.</p>
<p>Using a larger number of samples, say <span class="math notranslate nohighlight">\(10,000\)</span>, gives a better result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-0.0076273930440971215
-0.008874190869155479
</pre></div>
</div>
</div>
</div>
<p>Sampling from an arbitrary distribution on a finite set is also straightforward – as long as the set is not too big. This can be done using <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code></a>. Borrowing the example from the documentation, the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">aa_milne_arr</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'pooh'</span><span class="p">,</span> <span class="s1">'rabbit'</span><span class="p">,</span> <span class="s1">'piglet'</span><span class="p">,</span> <span class="s1">'christopher'</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">aa_milne_arr</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['pooh' 'pooh' 'piglet' 'christopher' 'piglet']
</pre></div>
</div>
</div>
</div>
<p>generates <span class="math notranslate nohighlight">\(5\)</span> samples from the set <span class="math notranslate nohighlight">\(\S = \{\tt{pooh}, \tt{rabbit}, \tt{piglet}, \tt{christopher}\}\)</span> with respective probabilities <span class="math notranslate nohighlight">\(0.5, 0.1, 0.1, 0.3\)</span>.</p>
<p>But this may not be practical when the state space <span class="math notranslate nohighlight">\(\S\)</span> is very large. As an example, later in this section, we will learn a “realistic” distribution of handwritten digits. We will do so using the <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>Each image is <span class="math notranslate nohighlight">\(28 \times 28\)</span>, so the total number of (black and white) pixels is <span class="math notranslate nohighlight">\(784\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(28, 28)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n_pixels</span> <span class="o">=</span> <span class="n">nx_pixels</span> <span class="o">*</span> <span class="n">ny_pixels</span>
<span class="n">n_pixels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>784
</pre></div>
</div>
</div>
</div>
<p>To specify a distribution over all possible black and white images of this size, we need in principle to assign a probability to a very large number of states. Our space here is <span class="math notranslate nohighlight">\(\S = \{0,1\}^{784}\)</span>, imagining that <span class="math notranslate nohighlight">\(0\)</span> encodes white and <span class="math notranslate nohighlight">\(1\)</span> encodes black and that we have ordered the pixels in some arbitrary way. How big is this space?</p>
<p>Answer: <span class="math notranslate nohighlight">\(2^{784}\)</span>.</p>
<p>Or in base <span class="math notranslate nohighlight">\(10\)</span>, we compute <span class="math notranslate nohighlight">\(\log_{10}(2^{784})\)</span>, which is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="mi">784</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>236.00751660056122
</pre></div>
</div>
</div>
</div>
<p>So a little more than <span class="math notranslate nohighlight">\(10^{236}\)</span>.</p>
<p>This is much too large to naively plug into <code class="docutils literal notranslate"><span class="pre">rng.choice</span></code>!</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>So how to proceed? Instead we’ll use a Markov chain, as detailed next.</p>
<p><strong>General setting</strong> The idea behind MCMC<span class="math notranslate nohighlight">\(\idx{Markov chain Monte Carlo}\xdi\)</span> is simple. To generate samples from <span class="math notranslate nohighlight">\(\bpi\)</span>, use a Markov chain <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> for which <em>it is the stationary distribution</em>. Indeed, we know from the <em>Convergence to Equilibrium Theorem</em> that if the chain is irreducible and aperiodic, then the distribution at time <span class="math notranslate nohighlight">\(t\)</span> is close to <span class="math notranslate nohighlight">\(\bpi\)</span> when <span class="math notranslate nohighlight">\(t\)</span> is large enough; and this holds for any initial dsitribution. Repeating this multiple times produces many independent, approximate samples from <span class="math notranslate nohighlight">\(\bpi\)</span>.</p>
<p>The question is now:</p>
<ol class="arabic simple">
<li><p>How to construct a transition matrix <span class="math notranslate nohighlight">\(P\)</span> whose stationary distribution is given target distribution <span class="math notranslate nohighlight">\(\bpi\)</span>?</p></li>
<li><p>How to ensure that this Markov chain is relatively easy to simulate?</p></li>
</ol>
<p>Regarding the first question, we have seen how to compute the stationary distribution of a transition matrix (provided it exists and is unique). How do we invert the process? Note one difficulty: many transition matrices can have the same stationary distribution. This is in fact a blessing, as it gives room for designing a “good” Markov chain.</p>
<p><strong>KNOWLEDGE CHECK:</strong> Construct two distinct transition matrices on <span class="math notranslate nohighlight">\(2\)</span> states whose stationary distribution is uniform. <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p>Regarding the second question, note that an obvious chain answering the first question is one that ignores the current state and chooses the next state according to <span class="math notranslate nohighlight">\(\bpi\)</span>. We have already seen that this can be a problematic choice.</p>
<p><strong>KNOWLEDGE CHECK:</strong> Show that this chain has the desired stationary distribution. <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p><strong>Metropolis-Hastings</strong> We develop one standard technique that helps answer these two questions. It is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings algorithm</a><span class="math notranslate nohighlight">\(\idx{Metropolis-Hastings}\xdi\)</span>. It consists in two steps. We assume that <span class="math notranslate nohighlight">\(\bpi &gt; 0\)</span>, that is, <span class="math notranslate nohighlight">\(\pi_i &gt; 0, \forall i \in \S\)</span>.</p>
<p><em>Proposal distribution:</em> We first define a proposal chain, that is, a transition matrix <span class="math notranslate nohighlight">\(Q\)</span> on the space <span class="math notranslate nohighlight">\(\S\)</span>. This chain <em>does not</em> need to have stationary distribution <span class="math notranslate nohighlight">\(\bpi\)</span>. But it is typically a chain that is easy to simulate. A different way to think of this chain is that, for each state <span class="math notranslate nohighlight">\(x \in \S\)</span>, we have a proposal distribution <span class="math notranslate nohighlight">\(Q(x,\,\cdot\,)\)</span> for the next state.</p>
<p>For instance, on the space of <span class="math notranslate nohighlight">\(28 \times 28\)</span> black-and-white images, we might pick a pixel uniformly at random and flip its value with probability <span class="math notranslate nohighlight">\(1/2\)</span>.</p>
<p><strong>KNOWLEDGE CHECK:</strong> In the previous example, what is the stationary distribution?</p>
<p>a) All-white with probability <span class="math notranslate nohighlight">\(1/2\)</span>, all-black with probability <span class="math notranslate nohighlight">\(1/2\)</span>.</p>
<p>b) Uniform.</p>
<p>c) Too complex to compute.</p>
<p>d) What is a stationary distribution?</p>
<p><span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p><em>Hastings correction:</em><span class="math notranslate nohighlight">\(\idx{Hastings correction}\xdi\)</span> At each step, we first pick a state according to <span class="math notranslate nohighlight">\(Q\)</span>, given the current state. Then we accept or reject this move according to a specially defined probability that depends on <span class="math notranslate nohighlight">\(Q\)</span> as well as <span class="math notranslate nohighlight">\(\bpi\)</span>. This is where the target distribution <span class="math notranslate nohighlight">\(\bpi\)</span> enters the picture, and the rejection probability is chosen to ensure that the new chain has the right stationary distribution, as we will see later. But first we define the full algorithm.</p>
<p>Formally, the algorithm goes as follows. Let <span class="math notranslate nohighlight">\(x_0 \in \S\)</span> be an arbitrary starting point and set <span class="math notranslate nohighlight">\(X_0 := x_0\)</span>.</p>
<p>At each time <span class="math notranslate nohighlight">\(t \geq 1\)</span>:</p>
<p>1- Pick a state <span class="math notranslate nohighlight">\(Y\)</span> according to the distribution <span class="math notranslate nohighlight">\(Q(X_{t-1}, \,\cdot\,)\)</span>, that is, row <span class="math notranslate nohighlight">\(X_{t-1}\)</span> of <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>2- With probability</p>
<div class="math notranslate nohighlight">
\[
\min\left\{1, \frac{\pi_{Y}}{\pi_{X_{t-1}}} \frac{Q(Y, X_{t-1})}{Q(X_{t-1}, Y)} \right\}
\]</div>
<p>we set <span class="math notranslate nohighlight">\(X_{t} := Y\)</span> (i.e., we accept the move), and otherwise we set <span class="math notranslate nohighlight">\(X_{t} := X_{t-1}\)</span> (i.e., we reject the move).</p>
<p><strong>KNOWLEDGE CHECK:</strong> Should we worry about the denominator <span class="math notranslate nohighlight">\(\pi_{X_{t-1}} Q(X_{t-1}, Y)\)</span> being <span class="math notranslate nohighlight">\(0\)</span>? <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p>We make three observations:</p>
<ol class="arabic simple">
<li><p>Taking a minimum with <span class="math notranslate nohighlight">\(1\)</span> ensures that acceptance probability is indeed between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p>We only need to know <span class="math notranslate nohighlight">\(\bpi\)</span> <em>up to a scaling factor</em> since the chain depends only on the ratio <span class="math notranslate nohighlight">\(\frac{\pi_{Y}}{\pi_{X_{t-1}}}\)</span>. The scaling factor cancels out. This turns out to be critical in many applications of MCMC. We will see an example in the next subsection.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(Q\)</span> is symmetric, that is, <span class="math notranslate nohighlight">\(Q(x,y) = Q(y,x)\)</span> for all <span class="math notranslate nohighlight">\(x, y \in \S\)</span>, then the ratio <span class="math notranslate nohighlight">\(\frac{Q(Y, X_{t-1})}{Q(X_{t-1}, Y)}\)</span> is just <span class="math notranslate nohighlight">\(1\)</span>, leading to a simpler formula for the acceptance probability. In particular, in that case, moving to a state with a larger probability under <span class="math notranslate nohighlight">\(\bpi\)</span> is <em>always</em> accepted.</p></li>
</ol>
<p><strong>NUMERICAL CORNER:</strong> Suppose <span class="math notranslate nohighlight">\(\S = \{1,\cdots, n\} = [n]\)</span> for some positive integer <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(\bpi\)</span> is proportional to a Poisson distribution with mean <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>. That is,</p>
<div class="math notranslate nohighlight">
\[
\pi_i = C e^{-\lambda} \frac{\lambda^i}{i!}, \quad \forall i \in \S
\]</div>
<p>for some constant <span class="math notranslate nohighlight">\(C\)</span> chosen so that <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} \pi_i = 1\)</span>. Recall that we do not need to determine <span class="math notranslate nohighlight">\(C\)</span> as it is enough to know the target distribution up to a scaling factor by the previous remark.</p>
<p>To apply Metropolis-Hastings, we need a proposal chain. Consider the following choice. For each <span class="math notranslate nohighlight">\(1 &lt; i &lt; n\)</span>, move to <span class="math notranslate nohighlight">\(i+1\)</span> or <span class="math notranslate nohighlight">\(i-1\)</span> with probability <span class="math notranslate nohighlight">\(1/2\)</span> each. For <span class="math notranslate nohighlight">\(i=1\)</span> (respectively <span class="math notranslate nohighlight">\(i = n\)</span>), move to <span class="math notranslate nohighlight">\(2\)</span> (respectively <span class="math notranslate nohighlight">\(n-1\)</span>) with probability <span class="math notranslate nohighlight">\(1/2\)</span>, otherwise stay where you are. For instance, if <span class="math notranslate nohighlight">\(n = 4\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q
=
\begin{pmatrix}
1/2 &amp; 1/2 &amp; 0 &amp; 0\\
1/2 &amp; 0 &amp; 1/2 &amp; 0\\
0 &amp; 1/2 &amp; 0 &amp; 1/2\\
0 &amp; 0 &amp; 1/2 &amp; 1/2
\end{pmatrix},
\end{split}\]</div>
<p>which is indeed a stochastic matrix. It is also symmetric, so it does not enter into the acceptance probability by the previous remark.</p>
<p>To compute the acceptance probability, we only need to consider pairs of adjacent integers as they are the only one that have non-zero probability under <span class="math notranslate nohighlight">\(Q\)</span>. Consider state <span class="math notranslate nohighlight">\(1 &lt; i &lt; n\)</span>. Observe that</p>
<div class="math notranslate nohighlight">
\[
\frac{\pi_{i+1}}{\pi_{i}}
= \frac{C e^{-\lambda} \lambda^{i+1}/(i+1)!}{C e^{-\lambda} \lambda^{i}/i!}
= \frac{\lambda}{i+1}
\]</div>
<p>so a move to <span class="math notranslate nohighlight">\(i+1\)</span> happens with probability</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2} \min\left\{1, \frac{\lambda}{i+1}\right\},
\]</div>
<p>where the <span class="math notranslate nohighlight">\(1/2\)</span> factor from the proposal distribution.
Similarly, it can be checked (try it!) that a move to <span class="math notranslate nohighlight">\(i-1\)</span> occurs with probability</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2} \min\left\{1, \frac{i}{\lambda}\right\}.
\]</div>
<p>And we stay at <span class="math notranslate nohighlight">\(i\)</span> with probability <span class="math notranslate nohighlight">\(1 - \frac{1}{2} \min\left\{1, \frac{\lambda}{i+1}\right\} - \frac{1}{2} \min\left\{1, \frac{i}{\lambda}\right\}\)</span>. (Why is this guaranteed to be a probability?)</p>
<p>A similar formula applies to <span class="math notranslate nohighlight">\(i = 1, n\)</span>. (Try it!)</p>
<p>We are ready to apply Metropolis-Hastings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">mh_transition_poisson</span><span class="p">(</span><span class="n">lmbd</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># index starts at 0 rather than 1</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">):</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">lmbd</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">lmbd</span><span class="p">]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">lmbd</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="n">n</span><span class="p">:</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">lmbd</span><span class="p">]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">P</span>
</pre></div>
</div>
</div>
</div>
<p>Take <span class="math notranslate nohighlight">\(\lambda = 1\)</span> and <span class="math notranslate nohighlight">\(n = 6\)</span>. We get the following transition matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">lmbd</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">6</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">P</span> <span class="o">=</span> <span class="n">mh_transition_poisson</span><span class="p">(</span><span class="n">lmbd</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.75       0.25       0.         0.         0.         0.        ]
 [0.5        0.33333333 0.16666667 0.         0.         0.        ]
 [0.         0.5        0.375      0.125      0.         0.        ]
 [0.         0.         0.5        0.4        0.1        0.        ]
 [0.         0.         0.         0.5        0.41666667 0.08333333]
 [0.         0.         0.         0.         0.5        0.5       ]]
</pre></div>
</div>
</div>
</div>
<p><strong>TRY IT!</strong> Rewrite the function <code class="docutils literal notranslate"><span class="pre">mh_transition_poisson</span></code> without an explicit loop by using <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting and vectorization</a>.  (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open in Colab</a>)</p>
<p>We use our simulator from a previous section. We start from the uniform distribution and take <span class="math notranslate nohighlight">\(100\)</span> steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">SamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our sample is the final state of the trajectory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span><span class="p">[</span><span class="n">T</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>2.0
</pre></div>
</div>
</div>
</div>
<p>We repeat <span class="math notranslate nohighlight">\(1000\)</span> times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">N_samples</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># number of repetitions</span>

<span class="n">freq_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># init of frequencies sampled</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_samples</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">SamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">freq_z</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">T</span><span class="p">])</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># adjust for index starting at 0</span>
    
<span class="n">freq_z</span> <span class="o">=</span> <span class="n">freq_z</span><span class="o">/</span><span class="n">N_samples</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the frequencies.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">freq_z</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d6e38b4f33052e917d7a2b6a03db11bd9c511daf60b88e2534f0a8be833345b9.png" src="../Images/4cb2314254f27c2be1f23bd733b95e2d.png" data-original-src="https://mmids-textbook.github.io/_images/d6e38b4f33052e917d7a2b6a03db11bd9c511daf60b88e2534f0a8be833345b9.png"/>
</div>
</div>
<p>If we increase the parameter <span class="math notranslate nohighlight">\(\lambda\)</span> (which is not quite the mean; why?), what would you expect will happen to the sampled distribution?</p>
<p><strong>TRY IT!</strong> Redo the simulations, but this time implement a general Metropolis-Hastings algorithm rather than specifying the transition matrix directly. That is, implement the algorithm for an arbitrary <span class="math notranslate nohighlight">\(\bpi\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>. Assume the state space is <span class="math notranslate nohighlight">\([n]\)</span>. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open in Colab</a>)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>It remains to prove that <span class="math notranslate nohighlight">\(\bpi\)</span> is needed the stationary distribution of the Metropolis-Hastings algorithm. We restrict ourselves to the symmetric case, that is, <span class="math notranslate nohighlight">\(Q(x,y) = Q(y,x)\)</span> for all <span class="math notranslate nohighlight">\(x,y\)</span>.</p>
<p><strong>THEOREM</strong> <strong>(Correctness of Metropolis-Hastings)</strong> <span class="math notranslate nohighlight">\(\idx{correctness of Metropolis-Hastings}\xdi\)</span>
Consider the Metropolis-Hastings algorithm with target distribution <span class="math notranslate nohighlight">\(\bpi\)</span> over finite state space <span class="math notranslate nohighlight">\(\S\)</span> and symmetric proposal chain <span class="math notranslate nohighlight">\(Q\)</span>. Assume further that <span class="math notranslate nohighlight">\(\bpi\)</span> is strictly positive and <span class="math notranslate nohighlight">\(Q\)</span> is irreducible over <span class="math notranslate nohighlight">\(\S\)</span>. The resulting Markov chain is irreducible and reversible with respect to <span class="math notranslate nohighlight">\(\bpi\)</span>.
<span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> It is just a matter of writing down the resulting transition matrix <span class="math notranslate nohighlight">\(P\)</span> and checking the detailed balance conditions. Because of the minimum in the acceptance probability, one has to consider two cases each time.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(P\)</span> denote the transition matrix of the resulting Markov chain. Our first task is to compute <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(x, y \in \S\)</span> be a pair of distinct states such that <span class="math notranslate nohighlight">\(Q(x, y) = Q(y, x) = 0\)</span>. Then, from <span class="math notranslate nohighlight">\(x\)</span> (respectively <span class="math notranslate nohighlight">\(y\)</span>), the proposal chain never picks <span class="math notranslate nohighlight">\(y\)</span> (respectively <span class="math notranslate nohighlight">\(x\)</span>) as the possible next state. Hence <span class="math notranslate nohighlight">\(P(x,y) = P(y, x) = 0\)</span> in that case.</p>
<p>So let <span class="math notranslate nohighlight">\(x, y \in \S\)</span> be a pair of distinct states such that <span class="math notranslate nohighlight">\(Q(x, y) = Q(y, x) &gt; 0\)</span>. Applying the Hastings correction, we get that the overall probability of moving to <span class="math notranslate nohighlight">\(y\)</span> from current state <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[
P(x, y)
= Q(x, y) \left(1 \land \frac{\pi_{y}}{\pi_{x}}\right) &gt; 0,
\]</div>
<p>where we used the symmetry of <span class="math notranslate nohighlight">\(Q\)</span> and the notation <span class="math notranslate nohighlight">\(a \land b = \min\{a,b\}\)</span>. Similarly,</p>
<div class="math notranslate nohighlight">
\[
P(y, x)
= Q(y, x) \left(1 \land \frac{\pi_{x}}{\pi_{y}}\right) &gt; 0.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(P(x,y)\)</span> is strictly positive exactly when <span class="math notranslate nohighlight">\(Q(x,y)\)</span> is strictly positive (for distinct <span class="math notranslate nohighlight">\(x,y\)</span>), the chain <span class="math notranslate nohighlight">\(P\)</span> has the same transition graph as the chain <span class="math notranslate nohighlight">\(Q\)</span>. Hence, because <span class="math notranslate nohighlight">\(Q\)</span> is irreducible, so is <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>It remains to check the detailed balance conditions. There are two cases. Without loss of generality, say <span class="math notranslate nohighlight">\(\pi_x \leq \pi_y\)</span>. Then the previous formulas for <span class="math notranslate nohighlight">\(P\)</span> simplify to</p>
<div class="math notranslate nohighlight">
\[
P(x, y)
= Q(x, y) 
\quad\text{and}\quad
P(y, x)
= Q(y, x) \frac{\pi_{x}}{\pi_{y}}.
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
\pi_x P(x,y) 
= \pi_x Q(x,y)
= \pi_x Q(y,x)
= \pi_x \frac{\pi_y}{\pi_y} Q(y,x)
= \pi_y P(y,x),
\]</div>
<p>where we used the symmetry of <span class="math notranslate nohighlight">\(Q\)</span> to obtain the second equality. That establishes the reversibility of <span class="math notranslate nohighlight">\(P\)</span> and concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The Metropolis-Hastings algorithm can be used for Bayesian inference. Ask your favorite AI chatbot to explain how MCMC methods are used in Bayesian inference and to provide an example of using the Metropolis-Hastings algorithm for parameter estimation in a simple Bayesian model. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="gibbs-sampling">
<h2><span class="section-number">7.6.2. </span>Gibbs sampling<a class="headerlink" href="#gibbs-sampling" title="Link to this heading">#</a></h2>
<p>We have seen that one challenge of the Metropolis-Hastings approach is to choose a good proposal chain. Gibbs sampling<span class="math notranslate nohighlight">\(\idx{Gibbs sampling}\xdi\)</span> is a canonical way of addressing this issue that has many applications. It applies in cases where the states are vectors, typically with a large number of coordinates, and where the target distribution has the kind of conditional independence properties we have encountered previously in the previous chapter.</p>
<p><strong>General setting</strong> Here we will assume that <span class="math notranslate nohighlight">\(\S = \Z^d\)</span> where <span class="math notranslate nohighlight">\(\Z\)</span> is a finite set and <span class="math notranslate nohighlight">\(d\)</span> is the dimension. To emphasize that states are vectors, we will boldface letters, e.g., <span class="math notranslate nohighlight">\(\bx = (x_i)_{i=1}^d\)</span>, <span class="math notranslate nohighlight">\(\by = (y_i)_{i=1}^d\)</span>, etc., to denote them.</p>
<p>We will need the following special notation. For a vector <span class="math notranslate nohighlight">\(\bx \in \Z^d\)</span> and an index <span class="math notranslate nohighlight">\(i \in [d]\)</span>, we write</p>
<div class="math notranslate nohighlight">
\[
\bx_{-i} = (x_1, \ldots,x_{i-1}, x_{i+1}, \ldots, x_d)
\]</div>
<p>for the vector <span class="math notranslate nohighlight">\(\bx\)</span> where the coordinate <span class="math notranslate nohighlight">\(x_i\)</span> is dropped.</p>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is the target distribution, we let <span class="math notranslate nohighlight">\(\pi_i(x_i|\bx_{-i})\)</span>
be the conditional probability that <span class="math notranslate nohighlight">\(X_i = x_i\)</span> given that <span class="math notranslate nohighlight">\(\bX_{-i} = \bx_{-i}\)</span> under the distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, i.e., <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d) \sim \boldsymbol{\pi}\)</span>. We assume that <span class="math notranslate nohighlight">\(\pi_{\bx} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(\bx \in \Z^d\)</span>. As a result, <span class="math notranslate nohighlight">\(\pi_i(x_i|\bx_{-i}) &gt; 0\)</span> as well (prove it!).</p>
<p>A basic version of the Gibbs sampler generates a sequence of vectors <span class="math notranslate nohighlight">\(\bX_0, \bX_1, \ldots, \bX_t, \ldots\)</span> in <span class="math notranslate nohighlight">\(\Z^d\)</span> as follows. We denote the coordinates of <span class="math notranslate nohighlight">\(\bX_t\)</span> by <span class="math notranslate nohighlight">\((X_{t,1}, \ldots, X_{t,d})\)</span>. We denote the vector of all coordinates of <span class="math notranslate nohighlight">\(\bX_t\)</span> except <span class="math notranslate nohighlight">\(i\)</span> by <span class="math notranslate nohighlight">\(\bX_{t,-i}\)</span>.</p>
<p>Pick <span class="math notranslate nohighlight">\(\bX_0\)</span> according to an arbitrary initial distribution <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> over <span class="math notranslate nohighlight">\(\Z^d\)</span>.</p>
<p>At each time <span class="math notranslate nohighlight">\(t \geq 1\)</span>:</p>
<p>1- Pick a coordinate <span class="math notranslate nohighlight">\(i\)</span> uniformly at random in <span class="math notranslate nohighlight">\([d]\)</span>.</p>
<p>2- Update coordinate <span class="math notranslate nohighlight">\(X_{t,i}\)</span> according to <span class="math notranslate nohighlight">\(\pi_i(\,\cdot\,|\bX_{t-1,-i})\)</span> while leaving all other coordinates unchanged.</p>
<p>We will implement it in a special case in the next subsection. But first we argue that it has the desired stationary distribution.</p>
<p>It suffices to establish that the Gibbs sampler is a special case of the Metropolis-Hastings algorithm. For this, we must identify the appropriate proposal chain <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>We claim that the following choice works: for <span class="math notranslate nohighlight">\(\bx \neq \by\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q(\bx, \by)
= 
\begin{cases}
\frac{1}{d} \pi_i(y_i|\bx_{-i}) &amp; \text{if $\by_{-i} = \bx_{-i}$ for some $i \in [d]$}\\
0 &amp; \text{o.w.}
\end{cases}
\end{split}\]</div>
<p>The condition “<span class="math notranslate nohighlight">\(\by_{-i} = \bx_{-i}\)</span> for some <span class="math notranslate nohighlight">\(i \in [d]\)</span>” ensures that we only consider moves that affect a single coordinate <span class="math notranslate nohighlight">\(i\)</span>. The factor <span class="math notranslate nohighlight">\(1/d\)</span> means that we pick that coordinate uniformly at random among all coordinates.</p>
<p>For each <span class="math notranslate nohighlight">\(\bx\)</span>, we stay put with the remaining probability.</p>
<p><strong>KNOWLEDGE CHECK:</strong> Write down explicitly the staying probability <span class="math notranslate nohighlight">\(Q(\bx, \bx)\)</span> and check it is indeed in <span class="math notranslate nohighlight">\([0,1]\)</span>. <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p>In general, this <span class="math notranslate nohighlight">\(Q\)</span> is not symmetric. For <span class="math notranslate nohighlight">\(\bx \neq \by\)</span> with <span class="math notranslate nohighlight">\(Q(\bx, \by) &gt; 0\)</span> where <span class="math notranslate nohighlight">\(i\)</span> is the non-matching coordinate, the acceptance probability is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{Q(\by, \bx)}{Q(\bx, \by)} \right\}
&amp;= \min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\frac{1}{d} \pi_i(x_i|\by_{-i})}{\frac{1}{d} \pi_i(y_i|\bx_{-i})} \right\}\\
&amp;= \min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\pi_i(x_i|\bx_{-i})}{\pi_i(y_i|\bx_{-i})} \right\},
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(\bx_{-i} = \by_{-i}\)</span> in the second equality.</p>
<p>Recall the definition of the conditional probability as a ratio: <span class="math notranslate nohighlight">\(\P[A|B] = \P[A\cap B]/\P[B]\)</span>. Applying that definition, both conditional probabilities
<span class="math notranslate nohighlight">\(\pi_i(x_i|\bx_{-i})\)</span>
and
<span class="math notranslate nohighlight">\(\pi_i(y_i|\bx_{-i})\)</span>
have the <em>same denominator</em>. Their respective numerators on the other hand are <span class="math notranslate nohighlight">\(\pi_{\bx}\)</span> and <span class="math notranslate nohighlight">\(\pi_{\by}\)</span>. Hence,</p>
<div class="math notranslate nohighlight">
\[
\min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\pi_i(x_i|\bx_{-i})}{\pi_i(y_i|\bx_{-i})} \right\}
= \min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\pi_{\bx}}{\pi_{\by}} \right\}
= 1.
\]</div>
<p>In other words, the proposed move is always accepted! Therefore <span class="math notranslate nohighlight">\(P = Q\)</span>, which is indeed the Gibbs sampler. It also establishes by <em>Correctness of Metropolis-Hastings</em> that <span class="math notranslate nohighlight">\(P\)</span> is reversible with respect to <span class="math notranslate nohighlight">\(\pi\)</span>. It is also irreducible (Why?).</p>
<p>Here we picked a coordinate at random. It turns out that other choices are possible. For example, one could update each coordinate in some deterministic order, or one could update blocks of coordinates at a time. Under some conditions, these schemes can still produce an algorithm simulating the desired distribution. We will not detail this here, but our implementation below does use a block scheme.</p>
<p><strong>An example: restricted Boltzmann machines (RBM)</strong> We implement the Gibbs sampler on a specific probabilistic model, a so-called restricted Boltzmann machine (RBM)<span class="math notranslate nohighlight">\(\idx{restricted Boltzmann machine}\xdi\)</span>, and apply it to the generation of random images from a “realistic” distribution. For more on Boltzmann machines, including their restricted and deep versions, see <a class="reference external" href="https://en.wikipedia.org/wiki/Boltzmann_machine">here</a>. We will not describe them in great details here, but only use them as an example of a complex distribution.</p>
<p><em>Probabilistic model:</em> An RBM has <span class="math notranslate nohighlight">\(m\)</span> visible units (i.e., observed variables) and <span class="math notranslate nohighlight">\(n\)</span> hidden units (i.e., hidden variables). It is represented by a complete bipartite graph between the two.</p>
<p><img alt="An RBM (with help from ChatGPT; code converted and adapted from Source)" src="../Images/4fb58292c78ce9956928cadb68b7af09.png" data-original-src="https://mmids-textbook.github.io/_images/rbm.png"/></p>
<p>Visible unit <span class="math notranslate nohighlight">\(i\)</span> is associated a variable <span class="math notranslate nohighlight">\(v_i\)</span> and hidden unit <span class="math notranslate nohighlight">\(j\)</span> is associated a variable <span class="math notranslate nohighlight">\(h_j\)</span>. We define the corresponding vectors <span class="math notranslate nohighlight">\(\bv = (v_1,\ldots,v_m)\)</span> and <span class="math notranslate nohighlight">\(\bh = (h_1,\ldots,h_n)\)</span>. For our purposes, it will suffice to assume that <span class="math notranslate nohighlight">\(\bv \in \{0,1\}^m\)</span> and <span class="math notranslate nohighlight">\(\bh \in \{0,1\}^n\)</span>. These are referred to as binary units.</p>
<p>The probabilistic model has a number of parameters. Each visible unit <span class="math notranslate nohighlight">\(i\)</span> has an offset <span class="math notranslate nohighlight">\(b_i \in \mathbb{R}\)</span> and each hidden unit <span class="math notranslate nohighlight">\(j\)</span> has an offset <span class="math notranslate nohighlight">\(c_j \in \mathbb{R}\)</span>. We write <span class="math notranslate nohighlight">\(\bb = (b_1,\ldots,b_m)\)</span> and <span class="math notranslate nohighlight">\(\bc = (c_1,\ldots,c_n)\)</span> for the offset vectors. For each pair <span class="math notranslate nohighlight">\((i,j)\)</span> of visible and hidden units (or, put differently, for each edge in the complete bipartite graph), there is a weight <span class="math notranslate nohighlight">\(w_{i,j} \in \mathbb{R}\)</span>. We write <span class="math notranslate nohighlight">\(W = (w_{i,j})_{i,j=1}^{m,n}\)</span> for the weight matrix.</p>
<p>To define the probability distribution, we need the so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Energy-based_model">energy</a><span class="math notranslate nohighlight">\(\idx{energy-based model}\xdi\)</span> (as you may have guessed, this terminology comes from related models in <a class="reference external" href="https://en.wikipedia.org/wiki/Boltzmann_distribution">physics</a>): for <span class="math notranslate nohighlight">\(\bv \in \{0,1\}^m\)</span> and <span class="math notranslate nohighlight">\(\bh \in \{0,1\}^n\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\cE(\bv, \bh)
&amp;= 
- \bv^T W \bh
- \bb^T \bv
- \bc^T \bh\\
&amp;= 
- \sum_{i=1}^m \sum_{j=1}^n w_{i,j} v_i h_j
- \sum_{i=1}^m b_i v_i
- \sum_{j=1}^n c_j h_j.
\end{align*}\]</div>
<p>The joint distribution of <span class="math notranslate nohighlight">\(\bv\)</span> and <span class="math notranslate nohighlight">\(\bh\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\pi}(\bv, \bh)
= \frac{1}{Z} \exp\left(- \cE(\bv, \bh)\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span>, the <a class="reference external" href="https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a><span class="math notranslate nohighlight">\(\idx{partition function}\xdi\)</span> (a function of <span class="math notranslate nohighlight">\(W,\bb,\bc\)</span>), ensures that <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> indeed sums to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>We will be interested in sampling from the marginal over visible units, that is,</p>
<div class="math notranslate nohighlight">
\[
\rho(\bv)
= \sum_{\bh \in \{0,1\}^n} \boldsymbol{\pi}(\bv, \bh).
\]</div>
<p>When <span class="math notranslate nohighlight">\(m\)</span> and/or <span class="math notranslate nohighlight">\(n\)</span> are large, computing <span class="math notranslate nohighlight">\(\rho\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> explicitly – or even numerically – is impractical.</p>
<p>We develop the Gibbs sampler for this model next.</p>
<p><em>Gibbs sampling:</em> We sample from the joint distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and observe only <span class="math notranslate nohighlight">\(\bv\)</span>.</p>
<p>We need to compute the conditional probabilities given every other variable. The sigmoid function, <span class="math notranslate nohighlight">\(\sigma(x)\)</span>, will once again make an appearance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Fix a visible unit <span class="math notranslate nohighlight">\(i \in [m]\)</span>. For a pair <span class="math notranslate nohighlight">\((\bv, \bh)\)</span>, we denote by <span class="math notranslate nohighlight">\((\bv_{[i]}, \bh)\)</span> the same pair where coordinate <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(\bv\)</span> is flipped. Given every other variable, i.e., <span class="math notranslate nohighlight">\((\bv_{-i},\bh)\)</span>, and using a superscript <span class="math notranslate nohighlight">\(\text{v}\)</span> to indicate the probability of a visible unit, the conditional probability of <span class="math notranslate nohighlight">\(v_i\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi^{\text{v}}_i(v_i|\bv_{-i},\bh)
&amp;= \frac{\boldsymbol{\pi}(\bv, \bh)}{\boldsymbol{\pi}(\bv, \bh) + \boldsymbol{\pi}(\bv_{[i]}, \bh)}\\
&amp;= \frac{\frac{1}{Z} \exp\left(- \cE(\bv, \bh)\right)}{\frac{1}{Z} \exp\left(- \cE(\bv, \bh)\right) + \frac{1}{Z} \exp\left(- \cE(\bv_{[i]}, \bh)\right)}.
\end{align*}\]</div>
<p>In this last ratio, the partition functions (the <span class="math notranslate nohighlight">\(Z\)</span>’s) cancel out. Moreover, all the terms in the exponentials <em>not depending</em> on the <span class="math notranslate nohighlight">\(i\)</span>-th visible unit actually factor out and cancel out as well – they are identical in all three exponentials. Similarly, the terms in the exponentials <em>depending only on <span class="math notranslate nohighlight">\(\bh\)</span></em> also factor out and cancel out.</p>
<p>What we are left with is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\pi^{\text{v}}_i(v_i|\bv_{-i},\bh)\\
&amp;= \frac{\exp\left(\sum_{j=1}^n w_{i,j} v_i h_j
+ b_i v_i\right)}
{\exp\left(\sum_{j=1}^n w_{i,j} v_i h_j
+ b_i v_i\right) 
+ \exp\left(\sum_{j=1}^n w_{i,j} (1-v_i) h_j
+ b_i (1-v_i)\right)},
\end{align*}\]</div>
<p>where we used the fact that flipping <span class="math notranslate nohighlight">\(v_i \in \{0,1\}\)</span> is the same as setting it to <span class="math notranslate nohighlight">\(1 - v_i\)</span>, a transformation which indeed sends <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>This expression does not depend on <span class="math notranslate nohighlight">\(\bv_{-i}\)</span>. In other words, the <span class="math notranslate nohighlight">\(i\)</span>-th visible unit is conditionally independent of all other visible units given the hidden units.</p>
<p>We simplify the expression</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\pi^{\text{v}}_i(v_i|\bv_{-i},\bh)\\
&amp;= \frac{1}
{1 
+ \exp\left(\sum_{j=1}^n w_{i,j} (1-2 v_i) h_j
+ b_i (1- 2v_i)\right)}\\
&amp;= \sigma\left(\sum_{j=1}^n w_{i,j} (2 v_i-1) h_j
+ b_i (2v_i-1)\right).
\end{align*}\]</div>
<p>In particular, the conditional mean of the <span class="math notranslate nohighlight">\(i\)</span>-th visible unit given everything else is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
0 \cdot \pi^{\text{v}}_i(0|\bv_{-i},\bh) + 1 \cdot \pi^{\text{v}}_i(1|\bv_{-i},\bh)
&amp;= \pi^{\text{v}}_i(1|\bv_{-i},\bh)\\
&amp;= \sigma\left(\sum_{j=1}^n w_{i,j} h_j
+ b_i \right)\\
&amp;= \sigma\left((W \bh + \bb)_i
\right)
\end{align*}\]</div>
<p>Similarly for the conditional probability of the <span class="math notranslate nohighlight">\(j\)</span>-th hidden unit given everything else, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\pi^{\text{h}}_j(h_j|\bv,\bh_{-j})\\
&amp;= \sigma\left(\sum_{i=1}^m w_{i,j} v_i (2h_j -1) 
+ c_j (2h_j -1)\right).
\end{align*}\]</div>
<p>The conditional mean given everything else is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
0 \cdot \pi^{\text{h}}_j(0|\bv,\bh_{-j}) + 1 \cdot \pi^{\text{h}}_j(1|\bv,\bh_{-j})
&amp;= \pi^{\text{h}}_j(1|\bv,\bh_{-j})
= \sigma\left((W^T \bv + \bc)_j \right).
\end{align*}\]</div>
<p>And the <span class="math notranslate nohighlight">\(j\)</span>-th hidden unit is conditionally independent of all other hidden units given the visible units.</p>
<p>We implement the Gibbs sampler for an RBM. Rather than updating the units at random, we use a block approach. Specifically, we update all hidden units independently, given the visible units; then we update all visible units independently, given the hidden units. In each case, this is warranted by the conditional independence structure revealed above.</p>
<p>We first implement the conditional means using the formulas previously derived.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_mean_hidden</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">v</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rbm_mean_visible</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We next implement one step of the sampler, which consists in updating all hidden units, followed by updating all visible units.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_gibbs_update</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">p_hidden</span> <span class="o">=</span> <span class="n">rbm_mean_hidden</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_hidden</span><span class="p">,</span> <span class="n">p_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">p_visible</span> <span class="o">=</span> <span class="n">rbm_mean_visible</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_visible</span><span class="p">,</span> <span class="n">p_visible</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we repeat these steps <code class="docutils literal notranslate"><span class="pre">k</span></code> times. We only return the visible units <code class="docutils literal notranslate"><span class="pre">v</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_gibbs_sampling</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v_0</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v_0</span>
    <span class="k">while</span> <span class="n">counter</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">rbm_gibbs_update</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">v_0</span></code> is the initial visible unit states. We do not need to initialize the hidden ones as this is done automatically in the first update step. In the next subsection, we will take the initial distribution of <span class="math notranslate nohighlight">\(\bv\)</span> to be independent Bernoullis with success probability <span class="math notranslate nohighlight">\(1/2\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We apply our Gibbs sampler to generating images. As mentioned previously, we use the MNIST dataset to learn a “realistic” distribution of handwritten digit images. Here the images are encoded by the visible units of an RBM. Then we sample from this model.</p>
<p>We first need to train the model on the data. We will not show how this is done here, but instead use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html"><code class="docutils literal notranslate"><span class="pre">sklearn.neural_network.BernoulliRBM</span></code></a>. (Some details of how this training is done is provided <a class="reference external" href="https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#stochastic-maximum-likelihood-learning">here</a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">BernoulliRBM</span>

<span class="n">rbm</span> <span class="o">=</span> <span class="n">BernoulliRBM</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To simplify the analysis and speed up the training, we only keep digits <span class="math notranslate nohighlight">\(0\)</span>, <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(5\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We flatten the images (which have already been “rounded” to black-and-white; see the first subsection).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We now fit the model. Choosing the hyperparameters of the training algorithm is tricky. The following seem to work reasonably well. (For a more systematic approach to tuning hyperparameters, see <a class="reference external" href="https://scikit-learn.org/stable/modules/grid_search.html">here</a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">rbm</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.02</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `&amp;lt;a&amp;gt;` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>BernoulliRBM(batch_size=50, learning_rate=0.02, n_components=100, n_iter=20,
             random_state=535)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked="checked"/><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">  BernoulliRBM<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.BernoulliRBM.html">?<span>Documentation for BernoulliRBM</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>BernoulliRBM(batch_size=50, learning_rate=0.02, n_components=100, n_iter=20,
             random_state=535)</pre></div> </div></div></div></div></div></div>
</div>
<p>We are ready to sample from the trained RBM. We extract the learned parameters from <code class="docutils literal notranslate"><span class="pre">rbm</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">W</span> <span class="o">=</span> <span class="n">rbm</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span>
<span class="n">W</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(784, 100)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">b</span> <span class="o">=</span> <span class="n">rbm</span><span class="o">.</span><span class="n">intercept_visible_</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(784,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">c</span> <span class="o">=</span> <span class="n">rbm</span><span class="o">.</span><span class="n">intercept_hidden_</span>
<span class="n">c</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(100,)
</pre></div>
</div>
</div>
</div>
<p>To generate <span class="math notranslate nohighlight">\(25\)</span> samples, we first generate <span class="math notranslate nohighlight">\(25\)</span> independent initial states. We stack them into a matrix, where each row is a different flattened random noise image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_pixels</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>To process all samples simultaneously, we make a small change to the code. We use <code class="docutils literal notranslate"><span class="pre">numpy.newaxis</span></code>
to make the offsets into column vectors, which are then automatically added to all columns of the resulting weighted sum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_mean_hidden</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">v</span> <span class="o">+</span> <span class="n">c</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">rbm_mean_visible</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>For plotting, we use a script <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html">adapted from here</a> (with help from ChatGPT).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">plot_imgs</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">n_imgs</span><span class="p">,</span> <span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span><span class="p">):</span>
    <span class="n">nx_imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_imgs</span><span class="p">))</span>
    <span class="n">ny_imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_imgs</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">nx_imgs</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">ny_imgs</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray_r'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We are now ready to run our Gibbs sampler. The outcome depends on the number of steps we take. After <span class="math notranslate nohighlight">\(100\)</span> steps, the outcome is somewhat realistic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">v_0</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">T</span>
<span class="n">gen_v</span> <span class="o">=</span> <span class="n">rbm_gibbs_sampling</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">v_0</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="n">plot_imgs</span><span class="p">(</span><span class="n">gen_v</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/aab5b59b73dc84d3b82547a838a8eadc75c8099fe134e50728fbf2920a04f55e.png" src="../Images/2abf435d81044deb01535f2ccdd32d63.png" data-original-src="https://mmids-textbook.github.io/_images/aab5b59b73dc84d3b82547a838a8eadc75c8099fe134e50728fbf2920a04f55e.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The RBM can be stacked to form a <a class="reference internal" href="#(https://en.wikipedia.org/wiki/Boltzmann_machine#Deep_Boltzmann_machine)"><span class="xref myst">deep belief network (DBN)</span></a>. Ask your favorite AI chatbot about the process of greedy layer-wise pretraining of a DBN using RBMs. Discuss how this can be used for initializing the weights of a deep neural network and compare the performance with random initialization. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In the context of Markov Chain Monte Carlo (MCMC), what is the primary goal?</p>
<p>a) To find the maximum likelihood estimate of a parameter.</p>
<p>b) To generate samples from a complex target distribution.</p>
<p>c) To optimize a loss function using gradient descent.</p>
<p>d) To cluster data points based on similarity.</p>
<p><strong>2</strong> In the Metropolis-Hastings algorithm, what is the role of the proposal chain <span class="math notranslate nohighlight">\(Q\)</span>?</p>
<p>a) It determines the stationary distribution of the resulting Markov chain.</p>
<p>b) It is used to compute the acceptance probability for the proposed moves.</p>
<p>c) It generates the candidate states for the next move in the Markov chain.</p>
<p>d) It ensures that the resulting Markov chain is irreducible and aperiodic.</p>
<p><strong>3</strong> What is the purpose of the Hastings correction in the Metropolis-Hastings algorithm?</p>
<p>a) To ensure that the proposal chain is symmetric.</p>
<p>b) To make the resulting Markov chain irreducible and aperiodic.</p>
<p>c) To ensure that the resulting Markov chain has the desired stationary distribution.</p>
<p>d) To improve the mixing time of the resulting Markov chain.</p>
<p><strong>4</strong> What is the role of the energy function <span class="math notranslate nohighlight">\(\mathcal{E}(\mathbf{v},\mathbf{h})\)</span> in a Restricted Boltzmann Machine (RBM)?</p>
<p>a) It determines the acceptance probability in the Metropolis-Hastings algorithm.</p>
<p>b) It defines the joint probability distribution of the visible and hidden units.</p>
<p>c) It represents the cost function to be minimized during training.</p>
<p>d) It controls the learning rate of the RBM.</p>
<p><strong>5</strong> What is the partition function <span class="math notranslate nohighlight">\(Z\)</span> used for in the RBM’s joint probability distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}(\mathbf{v}, \mathbf{h})\)</span>?</p>
<p>a) It normalizes the energy function.</p>
<p>b) It scales the weights matrix <span class="math notranslate nohighlight">\(W\)</span>.</p>
<p>c) It ensures that the probability distribution sums to one.</p>
<p>d) It adjusts the biases <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>.</p>
<p>Answer for 1: b. Justification: The text states that “The idea behind MCMC is simple. To generate samples from <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, use a Markov chain for which it is the stationary distribution.”</p>
<p>Answer for 2: c. Justification: The text describes the proposal chain as follows: “We first define a proposal chain, that is, a transition matrix <span class="math notranslate nohighlight">\(Q\)</span> on the space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. This chain does not need to have stationary distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>. But it is typically a chain that is easy to simulate.”</p>
<p>Answer for 3: c. Justification: The text states that the Hastings correction is “where the target distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> enters the picture, and the rejection probability is chosen to ensure that the new chain has the right stationary distribution, as we will see later.”</p>
<p>Answer for 4: b. Justification: The text defines the joint distribution of <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(h\)</span> as <span class="math notranslate nohighlight">\(\boldsymbol{\pi}(\mathbf{v},\mathbf{h}) = \frac{1}{Z} \exp(-\mathcal{E}(\mathbf{v},\mathbf{h}))\)</span>.</p>
<p>Answer for 5: c. Justification: The text explains that <span class="math notranslate nohighlight">\(Z\)</span>, the partition function, “ensures that <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> indeed sums to 1.”</p>
</section>
&#13;

<h2><span class="section-number">7.6.1. </span>Markov chain Monte Carlo (MCMC)<a class="headerlink" href="#markov-chain-monte-carlo-mcmc" title="Link to this heading">#</a></h2>
<p>Suppose we are interested in generating samples from a target distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i \in \S}\)</span> over a set <span class="math notranslate nohighlight">\(\S\)</span>. We have done this before. For instance, we generated samples from a mixture of Gaussians to test <span class="math notranslate nohighlight">\(k\)</span>-means clustering in different dimensions. There are many more applications. A canonical one is to estimate the mean of a function <span class="math notranslate nohighlight">\(f\)</span> under <span class="math notranslate nohighlight">\(\bpi\)</span>: generate <span class="math notranslate nohighlight">\(n\)</span> independent samples <span class="math notranslate nohighlight">\(Z_1,\ldots,Z_n\)</span>, all distributed according to <span class="math notranslate nohighlight">\(\pi\)</span>, then compute</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n f(Z_i),
\]</div>
<p>which is approximately <span class="math notranslate nohighlight">\(\E[f(Z_1)]\)</span> by the law of large numbers, provided <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large. Furthermore, this type of problem plays an important role in <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a>.</p>
<p><strong>Sampling from simple distributions</strong> When <span class="math notranslate nohighlight">\(\bpi\)</span> is a standard distribution or <span class="math notranslate nohighlight">\(\S\)</span> is relatively small, this can be done efficiently by using a random number generator, as we have done previously.</p>
<p><strong>NUMERICAL CORNER:</strong> Recall how this works. We first initialize the random number generator and use a <code class="docutils literal notranslate"><span class="pre">seed</span></code> for reproducibility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To generate, say <span class="math notranslate nohighlight">\(1000\)</span>, samples from a multivariate normal, say with mean <span class="math notranslate nohighlight">\((0, 0)\)</span> and covariance <span class="math notranslate nohighlight">\(\begin{pmatrix}5 &amp; 0\\0 &amp; 1\end{pmatrix}\)</span>, we use <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html#numpy.random.Generator.multivariate_normal"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a> as follows. The <code class="docutils literal notranslate"><span class="pre">.T</span></code> below transposes the output to separate the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> coordinates into individual arrays.</p>
<p><code class="docutils literal notranslate"><span class="pre">rng.multivariate_normal(mean,</span> <span class="pre">cov,</span> <span class="pre">1000)</span></code> returns a <code class="docutils literal notranslate"><span class="pre">(1000,</span> <span class="pre">2)</span></code> array where each row is one sample:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span/><span class="p">[[</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span>
 <span class="p">[</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">],</span>
 <span class="p">[</span><span class="n">x3</span><span class="p">,</span> <span class="n">y3</span><span class="p">],</span>
 <span class="o">...</span>
 <span class="p">[</span><span class="n">x1000</span><span class="p">,</span> <span class="n">y1000</span><span class="p">]]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.T</span></code> transposes this to <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">1000)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span/><span class="p">[[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">x1000</span><span class="p">],</span>
 <span class="p">[</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">y1000</span><span class="p">]]</span>
</pre></div>
</div>
<p>Now <code class="docutils literal notranslate"><span class="pre">x,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">...</span></code> unpacks the two rows, giving you:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">[x1,</span> <span class="pre">x2,</span> <span class="pre">x3,</span> <span class="pre">...,</span> <span class="pre">x1000]</span></code> (all x-coordinates)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">[y1,</span> <span class="pre">y2,</span> <span class="pre">y3,</span> <span class="pre">...,</span> <span class="pre">y1000]</span></code> (all y-coordinates)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>Computing the mean of each component we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-0.035322561120667575
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-0.009499619370100139
</pre></div>
</div>
</div>
</div>
<p>This is somewhat close to the expected answer: <span class="math notranslate nohighlight">\((0,0)\)</span>.</p>
<p>Using a larger number of samples, say <span class="math notranslate nohighlight">\(10,000\)</span>, gives a better result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-0.0076273930440971215
-0.008874190869155479
</pre></div>
</div>
</div>
</div>
<p>Sampling from an arbitrary distribution on a finite set is also straightforward – as long as the set is not too big. This can be done using <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code></a>. Borrowing the example from the documentation, the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">aa_milne_arr</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'pooh'</span><span class="p">,</span> <span class="s1">'rabbit'</span><span class="p">,</span> <span class="s1">'piglet'</span><span class="p">,</span> <span class="s1">'christopher'</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">aa_milne_arr</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['pooh' 'pooh' 'piglet' 'christopher' 'piglet']
</pre></div>
</div>
</div>
</div>
<p>generates <span class="math notranslate nohighlight">\(5\)</span> samples from the set <span class="math notranslate nohighlight">\(\S = \{\tt{pooh}, \tt{rabbit}, \tt{piglet}, \tt{christopher}\}\)</span> with respective probabilities <span class="math notranslate nohighlight">\(0.5, 0.1, 0.1, 0.3\)</span>.</p>
<p>But this may not be practical when the state space <span class="math notranslate nohighlight">\(\S\)</span> is very large. As an example, later in this section, we will learn a “realistic” distribution of handwritten digits. We will do so using the <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>Each image is <span class="math notranslate nohighlight">\(28 \times 28\)</span>, so the total number of (black and white) pixels is <span class="math notranslate nohighlight">\(784\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(28, 28)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n_pixels</span> <span class="o">=</span> <span class="n">nx_pixels</span> <span class="o">*</span> <span class="n">ny_pixels</span>
<span class="n">n_pixels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>784
</pre></div>
</div>
</div>
</div>
<p>To specify a distribution over all possible black and white images of this size, we need in principle to assign a probability to a very large number of states. Our space here is <span class="math notranslate nohighlight">\(\S = \{0,1\}^{784}\)</span>, imagining that <span class="math notranslate nohighlight">\(0\)</span> encodes white and <span class="math notranslate nohighlight">\(1\)</span> encodes black and that we have ordered the pixels in some arbitrary way. How big is this space?</p>
<p>Answer: <span class="math notranslate nohighlight">\(2^{784}\)</span>.</p>
<p>Or in base <span class="math notranslate nohighlight">\(10\)</span>, we compute <span class="math notranslate nohighlight">\(\log_{10}(2^{784})\)</span>, which is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="mi">784</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>236.00751660056122
</pre></div>
</div>
</div>
</div>
<p>So a little more than <span class="math notranslate nohighlight">\(10^{236}\)</span>.</p>
<p>This is much too large to naively plug into <code class="docutils literal notranslate"><span class="pre">rng.choice</span></code>!</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>So how to proceed? Instead we’ll use a Markov chain, as detailed next.</p>
<p><strong>General setting</strong> The idea behind MCMC<span class="math notranslate nohighlight">\(\idx{Markov chain Monte Carlo}\xdi\)</span> is simple. To generate samples from <span class="math notranslate nohighlight">\(\bpi\)</span>, use a Markov chain <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> for which <em>it is the stationary distribution</em>. Indeed, we know from the <em>Convergence to Equilibrium Theorem</em> that if the chain is irreducible and aperiodic, then the distribution at time <span class="math notranslate nohighlight">\(t\)</span> is close to <span class="math notranslate nohighlight">\(\bpi\)</span> when <span class="math notranslate nohighlight">\(t\)</span> is large enough; and this holds for any initial dsitribution. Repeating this multiple times produces many independent, approximate samples from <span class="math notranslate nohighlight">\(\bpi\)</span>.</p>
<p>The question is now:</p>
<ol class="arabic simple">
<li><p>How to construct a transition matrix <span class="math notranslate nohighlight">\(P\)</span> whose stationary distribution is given target distribution <span class="math notranslate nohighlight">\(\bpi\)</span>?</p></li>
<li><p>How to ensure that this Markov chain is relatively easy to simulate?</p></li>
</ol>
<p>Regarding the first question, we have seen how to compute the stationary distribution of a transition matrix (provided it exists and is unique). How do we invert the process? Note one difficulty: many transition matrices can have the same stationary distribution. This is in fact a blessing, as it gives room for designing a “good” Markov chain.</p>
<p><strong>KNOWLEDGE CHECK:</strong> Construct two distinct transition matrices on <span class="math notranslate nohighlight">\(2\)</span> states whose stationary distribution is uniform. <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p>Regarding the second question, note that an obvious chain answering the first question is one that ignores the current state and chooses the next state according to <span class="math notranslate nohighlight">\(\bpi\)</span>. We have already seen that this can be a problematic choice.</p>
<p><strong>KNOWLEDGE CHECK:</strong> Show that this chain has the desired stationary distribution. <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p><strong>Metropolis-Hastings</strong> We develop one standard technique that helps answer these two questions. It is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings algorithm</a><span class="math notranslate nohighlight">\(\idx{Metropolis-Hastings}\xdi\)</span>. It consists in two steps. We assume that <span class="math notranslate nohighlight">\(\bpi &gt; 0\)</span>, that is, <span class="math notranslate nohighlight">\(\pi_i &gt; 0, \forall i \in \S\)</span>.</p>
<p><em>Proposal distribution:</em> We first define a proposal chain, that is, a transition matrix <span class="math notranslate nohighlight">\(Q\)</span> on the space <span class="math notranslate nohighlight">\(\S\)</span>. This chain <em>does not</em> need to have stationary distribution <span class="math notranslate nohighlight">\(\bpi\)</span>. But it is typically a chain that is easy to simulate. A different way to think of this chain is that, for each state <span class="math notranslate nohighlight">\(x \in \S\)</span>, we have a proposal distribution <span class="math notranslate nohighlight">\(Q(x,\,\cdot\,)\)</span> for the next state.</p>
<p>For instance, on the space of <span class="math notranslate nohighlight">\(28 \times 28\)</span> black-and-white images, we might pick a pixel uniformly at random and flip its value with probability <span class="math notranslate nohighlight">\(1/2\)</span>.</p>
<p><strong>KNOWLEDGE CHECK:</strong> In the previous example, what is the stationary distribution?</p>
<p>a) All-white with probability <span class="math notranslate nohighlight">\(1/2\)</span>, all-black with probability <span class="math notranslate nohighlight">\(1/2\)</span>.</p>
<p>b) Uniform.</p>
<p>c) Too complex to compute.</p>
<p>d) What is a stationary distribution?</p>
<p><span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p><em>Hastings correction:</em><span class="math notranslate nohighlight">\(\idx{Hastings correction}\xdi\)</span> At each step, we first pick a state according to <span class="math notranslate nohighlight">\(Q\)</span>, given the current state. Then we accept or reject this move according to a specially defined probability that depends on <span class="math notranslate nohighlight">\(Q\)</span> as well as <span class="math notranslate nohighlight">\(\bpi\)</span>. This is where the target distribution <span class="math notranslate nohighlight">\(\bpi\)</span> enters the picture, and the rejection probability is chosen to ensure that the new chain has the right stationary distribution, as we will see later. But first we define the full algorithm.</p>
<p>Formally, the algorithm goes as follows. Let <span class="math notranslate nohighlight">\(x_0 \in \S\)</span> be an arbitrary starting point and set <span class="math notranslate nohighlight">\(X_0 := x_0\)</span>.</p>
<p>At each time <span class="math notranslate nohighlight">\(t \geq 1\)</span>:</p>
<p>1- Pick a state <span class="math notranslate nohighlight">\(Y\)</span> according to the distribution <span class="math notranslate nohighlight">\(Q(X_{t-1}, \,\cdot\,)\)</span>, that is, row <span class="math notranslate nohighlight">\(X_{t-1}\)</span> of <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>2- With probability</p>
<div class="math notranslate nohighlight">
\[
\min\left\{1, \frac{\pi_{Y}}{\pi_{X_{t-1}}} \frac{Q(Y, X_{t-1})}{Q(X_{t-1}, Y)} \right\}
\]</div>
<p>we set <span class="math notranslate nohighlight">\(X_{t} := Y\)</span> (i.e., we accept the move), and otherwise we set <span class="math notranslate nohighlight">\(X_{t} := X_{t-1}\)</span> (i.e., we reject the move).</p>
<p><strong>KNOWLEDGE CHECK:</strong> Should we worry about the denominator <span class="math notranslate nohighlight">\(\pi_{X_{t-1}} Q(X_{t-1}, Y)\)</span> being <span class="math notranslate nohighlight">\(0\)</span>? <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p>We make three observations:</p>
<ol class="arabic simple">
<li><p>Taking a minimum with <span class="math notranslate nohighlight">\(1\)</span> ensures that acceptance probability is indeed between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p>We only need to know <span class="math notranslate nohighlight">\(\bpi\)</span> <em>up to a scaling factor</em> since the chain depends only on the ratio <span class="math notranslate nohighlight">\(\frac{\pi_{Y}}{\pi_{X_{t-1}}}\)</span>. The scaling factor cancels out. This turns out to be critical in many applications of MCMC. We will see an example in the next subsection.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(Q\)</span> is symmetric, that is, <span class="math notranslate nohighlight">\(Q(x,y) = Q(y,x)\)</span> for all <span class="math notranslate nohighlight">\(x, y \in \S\)</span>, then the ratio <span class="math notranslate nohighlight">\(\frac{Q(Y, X_{t-1})}{Q(X_{t-1}, Y)}\)</span> is just <span class="math notranslate nohighlight">\(1\)</span>, leading to a simpler formula for the acceptance probability. In particular, in that case, moving to a state with a larger probability under <span class="math notranslate nohighlight">\(\bpi\)</span> is <em>always</em> accepted.</p></li>
</ol>
<p><strong>NUMERICAL CORNER:</strong> Suppose <span class="math notranslate nohighlight">\(\S = \{1,\cdots, n\} = [n]\)</span> for some positive integer <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(\bpi\)</span> is proportional to a Poisson distribution with mean <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>. That is,</p>
<div class="math notranslate nohighlight">
\[
\pi_i = C e^{-\lambda} \frac{\lambda^i}{i!}, \quad \forall i \in \S
\]</div>
<p>for some constant <span class="math notranslate nohighlight">\(C\)</span> chosen so that <span class="math notranslate nohighlight">\(\sum_{i=1}^{n} \pi_i = 1\)</span>. Recall that we do not need to determine <span class="math notranslate nohighlight">\(C\)</span> as it is enough to know the target distribution up to a scaling factor by the previous remark.</p>
<p>To apply Metropolis-Hastings, we need a proposal chain. Consider the following choice. For each <span class="math notranslate nohighlight">\(1 &lt; i &lt; n\)</span>, move to <span class="math notranslate nohighlight">\(i+1\)</span> or <span class="math notranslate nohighlight">\(i-1\)</span> with probability <span class="math notranslate nohighlight">\(1/2\)</span> each. For <span class="math notranslate nohighlight">\(i=1\)</span> (respectively <span class="math notranslate nohighlight">\(i = n\)</span>), move to <span class="math notranslate nohighlight">\(2\)</span> (respectively <span class="math notranslate nohighlight">\(n-1\)</span>) with probability <span class="math notranslate nohighlight">\(1/2\)</span>, otherwise stay where you are. For instance, if <span class="math notranslate nohighlight">\(n = 4\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q
=
\begin{pmatrix}
1/2 &amp; 1/2 &amp; 0 &amp; 0\\
1/2 &amp; 0 &amp; 1/2 &amp; 0\\
0 &amp; 1/2 &amp; 0 &amp; 1/2\\
0 &amp; 0 &amp; 1/2 &amp; 1/2
\end{pmatrix},
\end{split}\]</div>
<p>which is indeed a stochastic matrix. It is also symmetric, so it does not enter into the acceptance probability by the previous remark.</p>
<p>To compute the acceptance probability, we only need to consider pairs of adjacent integers as they are the only one that have non-zero probability under <span class="math notranslate nohighlight">\(Q\)</span>. Consider state <span class="math notranslate nohighlight">\(1 &lt; i &lt; n\)</span>. Observe that</p>
<div class="math notranslate nohighlight">
\[
\frac{\pi_{i+1}}{\pi_{i}}
= \frac{C e^{-\lambda} \lambda^{i+1}/(i+1)!}{C e^{-\lambda} \lambda^{i}/i!}
= \frac{\lambda}{i+1}
\]</div>
<p>so a move to <span class="math notranslate nohighlight">\(i+1\)</span> happens with probability</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2} \min\left\{1, \frac{\lambda}{i+1}\right\},
\]</div>
<p>where the <span class="math notranslate nohighlight">\(1/2\)</span> factor from the proposal distribution.
Similarly, it can be checked (try it!) that a move to <span class="math notranslate nohighlight">\(i-1\)</span> occurs with probability</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2} \min\left\{1, \frac{i}{\lambda}\right\}.
\]</div>
<p>And we stay at <span class="math notranslate nohighlight">\(i\)</span> with probability <span class="math notranslate nohighlight">\(1 - \frac{1}{2} \min\left\{1, \frac{\lambda}{i+1}\right\} - \frac{1}{2} \min\left\{1, \frac{i}{\lambda}\right\}\)</span>. (Why is this guaranteed to be a probability?)</p>
<p>A similar formula applies to <span class="math notranslate nohighlight">\(i = 1, n\)</span>. (Try it!)</p>
<p>We are ready to apply Metropolis-Hastings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">mh_transition_poisson</span><span class="p">(</span><span class="n">lmbd</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># index starts at 0 rather than 1</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">):</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">lmbd</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">lmbd</span><span class="p">]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">lmbd</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="n">n</span><span class="p">:</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">lmbd</span><span class="p">]))</span>
            <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">P</span>
</pre></div>
</div>
</div>
</div>
<p>Take <span class="math notranslate nohighlight">\(\lambda = 1\)</span> and <span class="math notranslate nohighlight">\(n = 6\)</span>. We get the following transition matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">lmbd</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">6</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">P</span> <span class="o">=</span> <span class="n">mh_transition_poisson</span><span class="p">(</span><span class="n">lmbd</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.75       0.25       0.         0.         0.         0.        ]
 [0.5        0.33333333 0.16666667 0.         0.         0.        ]
 [0.         0.5        0.375      0.125      0.         0.        ]
 [0.         0.         0.5        0.4        0.1        0.        ]
 [0.         0.         0.         0.5        0.41666667 0.08333333]
 [0.         0.         0.         0.         0.5        0.5       ]]
</pre></div>
</div>
</div>
</div>
<p><strong>TRY IT!</strong> Rewrite the function <code class="docutils literal notranslate"><span class="pre">mh_transition_poisson</span></code> without an explicit loop by using <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting and vectorization</a>.  (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open in Colab</a>)</p>
<p>We use our simulator from a previous section. We start from the uniform distribution and take <span class="math notranslate nohighlight">\(100\)</span> steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">SamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our sample is the final state of the trajectory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span><span class="p">[</span><span class="n">T</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>2.0
</pre></div>
</div>
</div>
</div>
<p>We repeat <span class="math notranslate nohighlight">\(1000\)</span> times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">N_samples</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># number of repetitions</span>

<span class="n">freq_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># init of frequencies sampled</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_samples</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">SamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">freq_z</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">T</span><span class="p">])</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># adjust for index starting at 0</span>
    
<span class="n">freq_z</span> <span class="o">=</span> <span class="n">freq_z</span><span class="o">/</span><span class="n">N_samples</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the frequencies.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">freq_z</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d6e38b4f33052e917d7a2b6a03db11bd9c511daf60b88e2534f0a8be833345b9.png" src="../Images/4cb2314254f27c2be1f23bd733b95e2d.png" data-original-src="https://mmids-textbook.github.io/_images/d6e38b4f33052e917d7a2b6a03db11bd9c511daf60b88e2534f0a8be833345b9.png"/>
</div>
</div>
<p>If we increase the parameter <span class="math notranslate nohighlight">\(\lambda\)</span> (which is not quite the mean; why?), what would you expect will happen to the sampled distribution?</p>
<p><strong>TRY IT!</strong> Redo the simulations, but this time implement a general Metropolis-Hastings algorithm rather than specifying the transition matrix directly. That is, implement the algorithm for an arbitrary <span class="math notranslate nohighlight">\(\bpi\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>. Assume the state space is <span class="math notranslate nohighlight">\([n]\)</span>. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open in Colab</a>)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>It remains to prove that <span class="math notranslate nohighlight">\(\bpi\)</span> is needed the stationary distribution of the Metropolis-Hastings algorithm. We restrict ourselves to the symmetric case, that is, <span class="math notranslate nohighlight">\(Q(x,y) = Q(y,x)\)</span> for all <span class="math notranslate nohighlight">\(x,y\)</span>.</p>
<p><strong>THEOREM</strong> <strong>(Correctness of Metropolis-Hastings)</strong> <span class="math notranslate nohighlight">\(\idx{correctness of Metropolis-Hastings}\xdi\)</span>
Consider the Metropolis-Hastings algorithm with target distribution <span class="math notranslate nohighlight">\(\bpi\)</span> over finite state space <span class="math notranslate nohighlight">\(\S\)</span> and symmetric proposal chain <span class="math notranslate nohighlight">\(Q\)</span>. Assume further that <span class="math notranslate nohighlight">\(\bpi\)</span> is strictly positive and <span class="math notranslate nohighlight">\(Q\)</span> is irreducible over <span class="math notranslate nohighlight">\(\S\)</span>. The resulting Markov chain is irreducible and reversible with respect to <span class="math notranslate nohighlight">\(\bpi\)</span>.
<span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> It is just a matter of writing down the resulting transition matrix <span class="math notranslate nohighlight">\(P\)</span> and checking the detailed balance conditions. Because of the minimum in the acceptance probability, one has to consider two cases each time.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(P\)</span> denote the transition matrix of the resulting Markov chain. Our first task is to compute <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(x, y \in \S\)</span> be a pair of distinct states such that <span class="math notranslate nohighlight">\(Q(x, y) = Q(y, x) = 0\)</span>. Then, from <span class="math notranslate nohighlight">\(x\)</span> (respectively <span class="math notranslate nohighlight">\(y\)</span>), the proposal chain never picks <span class="math notranslate nohighlight">\(y\)</span> (respectively <span class="math notranslate nohighlight">\(x\)</span>) as the possible next state. Hence <span class="math notranslate nohighlight">\(P(x,y) = P(y, x) = 0\)</span> in that case.</p>
<p>So let <span class="math notranslate nohighlight">\(x, y \in \S\)</span> be a pair of distinct states such that <span class="math notranslate nohighlight">\(Q(x, y) = Q(y, x) &gt; 0\)</span>. Applying the Hastings correction, we get that the overall probability of moving to <span class="math notranslate nohighlight">\(y\)</span> from current state <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[
P(x, y)
= Q(x, y) \left(1 \land \frac{\pi_{y}}{\pi_{x}}\right) &gt; 0,
\]</div>
<p>where we used the symmetry of <span class="math notranslate nohighlight">\(Q\)</span> and the notation <span class="math notranslate nohighlight">\(a \land b = \min\{a,b\}\)</span>. Similarly,</p>
<div class="math notranslate nohighlight">
\[
P(y, x)
= Q(y, x) \left(1 \land \frac{\pi_{x}}{\pi_{y}}\right) &gt; 0.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(P(x,y)\)</span> is strictly positive exactly when <span class="math notranslate nohighlight">\(Q(x,y)\)</span> is strictly positive (for distinct <span class="math notranslate nohighlight">\(x,y\)</span>), the chain <span class="math notranslate nohighlight">\(P\)</span> has the same transition graph as the chain <span class="math notranslate nohighlight">\(Q\)</span>. Hence, because <span class="math notranslate nohighlight">\(Q\)</span> is irreducible, so is <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>It remains to check the detailed balance conditions. There are two cases. Without loss of generality, say <span class="math notranslate nohighlight">\(\pi_x \leq \pi_y\)</span>. Then the previous formulas for <span class="math notranslate nohighlight">\(P\)</span> simplify to</p>
<div class="math notranslate nohighlight">
\[
P(x, y)
= Q(x, y) 
\quad\text{and}\quad
P(y, x)
= Q(y, x) \frac{\pi_{x}}{\pi_{y}}.
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
\pi_x P(x,y) 
= \pi_x Q(x,y)
= \pi_x Q(y,x)
= \pi_x \frac{\pi_y}{\pi_y} Q(y,x)
= \pi_y P(y,x),
\]</div>
<p>where we used the symmetry of <span class="math notranslate nohighlight">\(Q\)</span> to obtain the second equality. That establishes the reversibility of <span class="math notranslate nohighlight">\(P\)</span> and concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The Metropolis-Hastings algorithm can be used for Bayesian inference. Ask your favorite AI chatbot to explain how MCMC methods are used in Bayesian inference and to provide an example of using the Metropolis-Hastings algorithm for parameter estimation in a simple Bayesian model. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
&#13;

<h2><span class="section-number">7.6.2. </span>Gibbs sampling<a class="headerlink" href="#gibbs-sampling" title="Link to this heading">#</a></h2>
<p>We have seen that one challenge of the Metropolis-Hastings approach is to choose a good proposal chain. Gibbs sampling<span class="math notranslate nohighlight">\(\idx{Gibbs sampling}\xdi\)</span> is a canonical way of addressing this issue that has many applications. It applies in cases where the states are vectors, typically with a large number of coordinates, and where the target distribution has the kind of conditional independence properties we have encountered previously in the previous chapter.</p>
<p><strong>General setting</strong> Here we will assume that <span class="math notranslate nohighlight">\(\S = \Z^d\)</span> where <span class="math notranslate nohighlight">\(\Z\)</span> is a finite set and <span class="math notranslate nohighlight">\(d\)</span> is the dimension. To emphasize that states are vectors, we will boldface letters, e.g., <span class="math notranslate nohighlight">\(\bx = (x_i)_{i=1}^d\)</span>, <span class="math notranslate nohighlight">\(\by = (y_i)_{i=1}^d\)</span>, etc., to denote them.</p>
<p>We will need the following special notation. For a vector <span class="math notranslate nohighlight">\(\bx \in \Z^d\)</span> and an index <span class="math notranslate nohighlight">\(i \in [d]\)</span>, we write</p>
<div class="math notranslate nohighlight">
\[
\bx_{-i} = (x_1, \ldots,x_{i-1}, x_{i+1}, \ldots, x_d)
\]</div>
<p>for the vector <span class="math notranslate nohighlight">\(\bx\)</span> where the coordinate <span class="math notranslate nohighlight">\(x_i\)</span> is dropped.</p>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is the target distribution, we let <span class="math notranslate nohighlight">\(\pi_i(x_i|\bx_{-i})\)</span>
be the conditional probability that <span class="math notranslate nohighlight">\(X_i = x_i\)</span> given that <span class="math notranslate nohighlight">\(\bX_{-i} = \bx_{-i}\)</span> under the distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, i.e., <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d) \sim \boldsymbol{\pi}\)</span>. We assume that <span class="math notranslate nohighlight">\(\pi_{\bx} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(\bx \in \Z^d\)</span>. As a result, <span class="math notranslate nohighlight">\(\pi_i(x_i|\bx_{-i}) &gt; 0\)</span> as well (prove it!).</p>
<p>A basic version of the Gibbs sampler generates a sequence of vectors <span class="math notranslate nohighlight">\(\bX_0, \bX_1, \ldots, \bX_t, \ldots\)</span> in <span class="math notranslate nohighlight">\(\Z^d\)</span> as follows. We denote the coordinates of <span class="math notranslate nohighlight">\(\bX_t\)</span> by <span class="math notranslate nohighlight">\((X_{t,1}, \ldots, X_{t,d})\)</span>. We denote the vector of all coordinates of <span class="math notranslate nohighlight">\(\bX_t\)</span> except <span class="math notranslate nohighlight">\(i\)</span> by <span class="math notranslate nohighlight">\(\bX_{t,-i}\)</span>.</p>
<p>Pick <span class="math notranslate nohighlight">\(\bX_0\)</span> according to an arbitrary initial distribution <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> over <span class="math notranslate nohighlight">\(\Z^d\)</span>.</p>
<p>At each time <span class="math notranslate nohighlight">\(t \geq 1\)</span>:</p>
<p>1- Pick a coordinate <span class="math notranslate nohighlight">\(i\)</span> uniformly at random in <span class="math notranslate nohighlight">\([d]\)</span>.</p>
<p>2- Update coordinate <span class="math notranslate nohighlight">\(X_{t,i}\)</span> according to <span class="math notranslate nohighlight">\(\pi_i(\,\cdot\,|\bX_{t-1,-i})\)</span> while leaving all other coordinates unchanged.</p>
<p>We will implement it in a special case in the next subsection. But first we argue that it has the desired stationary distribution.</p>
<p>It suffices to establish that the Gibbs sampler is a special case of the Metropolis-Hastings algorithm. For this, we must identify the appropriate proposal chain <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>We claim that the following choice works: for <span class="math notranslate nohighlight">\(\bx \neq \by\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q(\bx, \by)
= 
\begin{cases}
\frac{1}{d} \pi_i(y_i|\bx_{-i}) &amp; \text{if $\by_{-i} = \bx_{-i}$ for some $i \in [d]$}\\
0 &amp; \text{o.w.}
\end{cases}
\end{split}\]</div>
<p>The condition “<span class="math notranslate nohighlight">\(\by_{-i} = \bx_{-i}\)</span> for some <span class="math notranslate nohighlight">\(i \in [d]\)</span>” ensures that we only consider moves that affect a single coordinate <span class="math notranslate nohighlight">\(i\)</span>. The factor <span class="math notranslate nohighlight">\(1/d\)</span> means that we pick that coordinate uniformly at random among all coordinates.</p>
<p>For each <span class="math notranslate nohighlight">\(\bx\)</span>, we stay put with the remaining probability.</p>
<p><strong>KNOWLEDGE CHECK:</strong> Write down explicitly the staying probability <span class="math notranslate nohighlight">\(Q(\bx, \bx)\)</span> and check it is indeed in <span class="math notranslate nohighlight">\([0,1]\)</span>. <span class="math notranslate nohighlight">\(\checkmark\)</span></p>
<p>In general, this <span class="math notranslate nohighlight">\(Q\)</span> is not symmetric. For <span class="math notranslate nohighlight">\(\bx \neq \by\)</span> with <span class="math notranslate nohighlight">\(Q(\bx, \by) &gt; 0\)</span> where <span class="math notranslate nohighlight">\(i\)</span> is the non-matching coordinate, the acceptance probability is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{Q(\by, \bx)}{Q(\bx, \by)} \right\}
&amp;= \min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\frac{1}{d} \pi_i(x_i|\by_{-i})}{\frac{1}{d} \pi_i(y_i|\bx_{-i})} \right\}\\
&amp;= \min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\pi_i(x_i|\bx_{-i})}{\pi_i(y_i|\bx_{-i})} \right\},
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(\bx_{-i} = \by_{-i}\)</span> in the second equality.</p>
<p>Recall the definition of the conditional probability as a ratio: <span class="math notranslate nohighlight">\(\P[A|B] = \P[A\cap B]/\P[B]\)</span>. Applying that definition, both conditional probabilities
<span class="math notranslate nohighlight">\(\pi_i(x_i|\bx_{-i})\)</span>
and
<span class="math notranslate nohighlight">\(\pi_i(y_i|\bx_{-i})\)</span>
have the <em>same denominator</em>. Their respective numerators on the other hand are <span class="math notranslate nohighlight">\(\pi_{\bx}\)</span> and <span class="math notranslate nohighlight">\(\pi_{\by}\)</span>. Hence,</p>
<div class="math notranslate nohighlight">
\[
\min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\pi_i(x_i|\bx_{-i})}{\pi_i(y_i|\bx_{-i})} \right\}
= \min\left\{1, \frac{\pi_{\by}}{\pi_{\bx}} \frac{\pi_{\bx}}{\pi_{\by}} \right\}
= 1.
\]</div>
<p>In other words, the proposed move is always accepted! Therefore <span class="math notranslate nohighlight">\(P = Q\)</span>, which is indeed the Gibbs sampler. It also establishes by <em>Correctness of Metropolis-Hastings</em> that <span class="math notranslate nohighlight">\(P\)</span> is reversible with respect to <span class="math notranslate nohighlight">\(\pi\)</span>. It is also irreducible (Why?).</p>
<p>Here we picked a coordinate at random. It turns out that other choices are possible. For example, one could update each coordinate in some deterministic order, or one could update blocks of coordinates at a time. Under some conditions, these schemes can still produce an algorithm simulating the desired distribution. We will not detail this here, but our implementation below does use a block scheme.</p>
<p><strong>An example: restricted Boltzmann machines (RBM)</strong> We implement the Gibbs sampler on a specific probabilistic model, a so-called restricted Boltzmann machine (RBM)<span class="math notranslate nohighlight">\(\idx{restricted Boltzmann machine}\xdi\)</span>, and apply it to the generation of random images from a “realistic” distribution. For more on Boltzmann machines, including their restricted and deep versions, see <a class="reference external" href="https://en.wikipedia.org/wiki/Boltzmann_machine">here</a>. We will not describe them in great details here, but only use them as an example of a complex distribution.</p>
<p><em>Probabilistic model:</em> An RBM has <span class="math notranslate nohighlight">\(m\)</span> visible units (i.e., observed variables) and <span class="math notranslate nohighlight">\(n\)</span> hidden units (i.e., hidden variables). It is represented by a complete bipartite graph between the two.</p>
<p><img alt="An RBM (with help from ChatGPT; code converted and adapted from Source)" src="../Images/4fb58292c78ce9956928cadb68b7af09.png" data-original-src="https://mmids-textbook.github.io/_images/rbm.png"/></p>
<p>Visible unit <span class="math notranslate nohighlight">\(i\)</span> is associated a variable <span class="math notranslate nohighlight">\(v_i\)</span> and hidden unit <span class="math notranslate nohighlight">\(j\)</span> is associated a variable <span class="math notranslate nohighlight">\(h_j\)</span>. We define the corresponding vectors <span class="math notranslate nohighlight">\(\bv = (v_1,\ldots,v_m)\)</span> and <span class="math notranslate nohighlight">\(\bh = (h_1,\ldots,h_n)\)</span>. For our purposes, it will suffice to assume that <span class="math notranslate nohighlight">\(\bv \in \{0,1\}^m\)</span> and <span class="math notranslate nohighlight">\(\bh \in \{0,1\}^n\)</span>. These are referred to as binary units.</p>
<p>The probabilistic model has a number of parameters. Each visible unit <span class="math notranslate nohighlight">\(i\)</span> has an offset <span class="math notranslate nohighlight">\(b_i \in \mathbb{R}\)</span> and each hidden unit <span class="math notranslate nohighlight">\(j\)</span> has an offset <span class="math notranslate nohighlight">\(c_j \in \mathbb{R}\)</span>. We write <span class="math notranslate nohighlight">\(\bb = (b_1,\ldots,b_m)\)</span> and <span class="math notranslate nohighlight">\(\bc = (c_1,\ldots,c_n)\)</span> for the offset vectors. For each pair <span class="math notranslate nohighlight">\((i,j)\)</span> of visible and hidden units (or, put differently, for each edge in the complete bipartite graph), there is a weight <span class="math notranslate nohighlight">\(w_{i,j} \in \mathbb{R}\)</span>. We write <span class="math notranslate nohighlight">\(W = (w_{i,j})_{i,j=1}^{m,n}\)</span> for the weight matrix.</p>
<p>To define the probability distribution, we need the so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Energy-based_model">energy</a><span class="math notranslate nohighlight">\(\idx{energy-based model}\xdi\)</span> (as you may have guessed, this terminology comes from related models in <a class="reference external" href="https://en.wikipedia.org/wiki/Boltzmann_distribution">physics</a>): for <span class="math notranslate nohighlight">\(\bv \in \{0,1\}^m\)</span> and <span class="math notranslate nohighlight">\(\bh \in \{0,1\}^n\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\cE(\bv, \bh)
&amp;= 
- \bv^T W \bh
- \bb^T \bv
- \bc^T \bh\\
&amp;= 
- \sum_{i=1}^m \sum_{j=1}^n w_{i,j} v_i h_j
- \sum_{i=1}^m b_i v_i
- \sum_{j=1}^n c_j h_j.
\end{align*}\]</div>
<p>The joint distribution of <span class="math notranslate nohighlight">\(\bv\)</span> and <span class="math notranslate nohighlight">\(\bh\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\pi}(\bv, \bh)
= \frac{1}{Z} \exp\left(- \cE(\bv, \bh)\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span>, the <a class="reference external" href="https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29">partition function</a><span class="math notranslate nohighlight">\(\idx{partition function}\xdi\)</span> (a function of <span class="math notranslate nohighlight">\(W,\bb,\bc\)</span>), ensures that <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> indeed sums to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>We will be interested in sampling from the marginal over visible units, that is,</p>
<div class="math notranslate nohighlight">
\[
\rho(\bv)
= \sum_{\bh \in \{0,1\}^n} \boldsymbol{\pi}(\bv, \bh).
\]</div>
<p>When <span class="math notranslate nohighlight">\(m\)</span> and/or <span class="math notranslate nohighlight">\(n\)</span> are large, computing <span class="math notranslate nohighlight">\(\rho\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> explicitly – or even numerically – is impractical.</p>
<p>We develop the Gibbs sampler for this model next.</p>
<p><em>Gibbs sampling:</em> We sample from the joint distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> and observe only <span class="math notranslate nohighlight">\(\bv\)</span>.</p>
<p>We need to compute the conditional probabilities given every other variable. The sigmoid function, <span class="math notranslate nohighlight">\(\sigma(x)\)</span>, will once again make an appearance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Fix a visible unit <span class="math notranslate nohighlight">\(i \in [m]\)</span>. For a pair <span class="math notranslate nohighlight">\((\bv, \bh)\)</span>, we denote by <span class="math notranslate nohighlight">\((\bv_{[i]}, \bh)\)</span> the same pair where coordinate <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(\bv\)</span> is flipped. Given every other variable, i.e., <span class="math notranslate nohighlight">\((\bv_{-i},\bh)\)</span>, and using a superscript <span class="math notranslate nohighlight">\(\text{v}\)</span> to indicate the probability of a visible unit, the conditional probability of <span class="math notranslate nohighlight">\(v_i\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi^{\text{v}}_i(v_i|\bv_{-i},\bh)
&amp;= \frac{\boldsymbol{\pi}(\bv, \bh)}{\boldsymbol{\pi}(\bv, \bh) + \boldsymbol{\pi}(\bv_{[i]}, \bh)}\\
&amp;= \frac{\frac{1}{Z} \exp\left(- \cE(\bv, \bh)\right)}{\frac{1}{Z} \exp\left(- \cE(\bv, \bh)\right) + \frac{1}{Z} \exp\left(- \cE(\bv_{[i]}, \bh)\right)}.
\end{align*}\]</div>
<p>In this last ratio, the partition functions (the <span class="math notranslate nohighlight">\(Z\)</span>’s) cancel out. Moreover, all the terms in the exponentials <em>not depending</em> on the <span class="math notranslate nohighlight">\(i\)</span>-th visible unit actually factor out and cancel out as well – they are identical in all three exponentials. Similarly, the terms in the exponentials <em>depending only on <span class="math notranslate nohighlight">\(\bh\)</span></em> also factor out and cancel out.</p>
<p>What we are left with is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\pi^{\text{v}}_i(v_i|\bv_{-i},\bh)\\
&amp;= \frac{\exp\left(\sum_{j=1}^n w_{i,j} v_i h_j
+ b_i v_i\right)}
{\exp\left(\sum_{j=1}^n w_{i,j} v_i h_j
+ b_i v_i\right) 
+ \exp\left(\sum_{j=1}^n w_{i,j} (1-v_i) h_j
+ b_i (1-v_i)\right)},
\end{align*}\]</div>
<p>where we used the fact that flipping <span class="math notranslate nohighlight">\(v_i \in \{0,1\}\)</span> is the same as setting it to <span class="math notranslate nohighlight">\(1 - v_i\)</span>, a transformation which indeed sends <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>This expression does not depend on <span class="math notranslate nohighlight">\(\bv_{-i}\)</span>. In other words, the <span class="math notranslate nohighlight">\(i\)</span>-th visible unit is conditionally independent of all other visible units given the hidden units.</p>
<p>We simplify the expression</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\pi^{\text{v}}_i(v_i|\bv_{-i},\bh)\\
&amp;= \frac{1}
{1 
+ \exp\left(\sum_{j=1}^n w_{i,j} (1-2 v_i) h_j
+ b_i (1- 2v_i)\right)}\\
&amp;= \sigma\left(\sum_{j=1}^n w_{i,j} (2 v_i-1) h_j
+ b_i (2v_i-1)\right).
\end{align*}\]</div>
<p>In particular, the conditional mean of the <span class="math notranslate nohighlight">\(i\)</span>-th visible unit given everything else is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
0 \cdot \pi^{\text{v}}_i(0|\bv_{-i},\bh) + 1 \cdot \pi^{\text{v}}_i(1|\bv_{-i},\bh)
&amp;= \pi^{\text{v}}_i(1|\bv_{-i},\bh)\\
&amp;= \sigma\left(\sum_{j=1}^n w_{i,j} h_j
+ b_i \right)\\
&amp;= \sigma\left((W \bh + \bb)_i
\right)
\end{align*}\]</div>
<p>Similarly for the conditional probability of the <span class="math notranslate nohighlight">\(j\)</span>-th hidden unit given everything else, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\pi^{\text{h}}_j(h_j|\bv,\bh_{-j})\\
&amp;= \sigma\left(\sum_{i=1}^m w_{i,j} v_i (2h_j -1) 
+ c_j (2h_j -1)\right).
\end{align*}\]</div>
<p>The conditional mean given everything else is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
0 \cdot \pi^{\text{h}}_j(0|\bv,\bh_{-j}) + 1 \cdot \pi^{\text{h}}_j(1|\bv,\bh_{-j})
&amp;= \pi^{\text{h}}_j(1|\bv,\bh_{-j})
= \sigma\left((W^T \bv + \bc)_j \right).
\end{align*}\]</div>
<p>And the <span class="math notranslate nohighlight">\(j\)</span>-th hidden unit is conditionally independent of all other hidden units given the visible units.</p>
<p>We implement the Gibbs sampler for an RBM. Rather than updating the units at random, we use a block approach. Specifically, we update all hidden units independently, given the visible units; then we update all visible units independently, given the hidden units. In each case, this is warranted by the conditional independence structure revealed above.</p>
<p>We first implement the conditional means using the formulas previously derived.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_mean_hidden</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">v</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rbm_mean_visible</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We next implement one step of the sampler, which consists in updating all hidden units, followed by updating all visible units.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_gibbs_update</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">p_hidden</span> <span class="o">=</span> <span class="n">rbm_mean_hidden</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_hidden</span><span class="p">,</span> <span class="n">p_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">p_visible</span> <span class="o">=</span> <span class="n">rbm_mean_visible</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_visible</span><span class="p">,</span> <span class="n">p_visible</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we repeat these steps <code class="docutils literal notranslate"><span class="pre">k</span></code> times. We only return the visible units <code class="docutils literal notranslate"><span class="pre">v</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_gibbs_sampling</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v_0</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v_0</span>
    <span class="k">while</span> <span class="n">counter</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">rbm_gibbs_update</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">v_0</span></code> is the initial visible unit states. We do not need to initialize the hidden ones as this is done automatically in the first update step. In the next subsection, we will take the initial distribution of <span class="math notranslate nohighlight">\(\bv\)</span> to be independent Bernoullis with success probability <span class="math notranslate nohighlight">\(1/2\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We apply our Gibbs sampler to generating images. As mentioned previously, we use the MNIST dataset to learn a “realistic” distribution of handwritten digit images. Here the images are encoded by the visible units of an RBM. Then we sample from this model.</p>
<p>We first need to train the model on the data. We will not show how this is done here, but instead use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html"><code class="docutils literal notranslate"><span class="pre">sklearn.neural_network.BernoulliRBM</span></code></a>. (Some details of how this training is done is provided <a class="reference external" href="https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#stochastic-maximum-likelihood-learning">here</a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">BernoulliRBM</span>

<span class="n">rbm</span> <span class="o">=</span> <span class="n">BernoulliRBM</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To simplify the analysis and speed up the training, we only keep digits <span class="math notranslate nohighlight">\(0\)</span>, <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(5\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We flatten the images (which have already been “rounded” to black-and-white; see the first subsection).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We now fit the model. Choosing the hyperparameters of the training algorithm is tricky. The following seem to work reasonably well. (For a more systematic approach to tuning hyperparameters, see <a class="reference external" href="https://scikit-learn.org/stable/modules/grid_search.html">here</a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">rbm</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.02</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">rbm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `&amp;lt;a&amp;gt;` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>BernoulliRBM(batch_size=50, learning_rate=0.02, n_components=100, n_iter=20,
             random_state=535)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked="checked"/><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">  BernoulliRBM<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.BernoulliRBM.html">?<span>Documentation for BernoulliRBM</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>BernoulliRBM(batch_size=50, learning_rate=0.02, n_components=100, n_iter=20,
             random_state=535)</pre></div> </div></div></div></div></div></div>
</div>
<p>We are ready to sample from the trained RBM. We extract the learned parameters from <code class="docutils literal notranslate"><span class="pre">rbm</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">W</span> <span class="o">=</span> <span class="n">rbm</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span>
<span class="n">W</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(784, 100)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">b</span> <span class="o">=</span> <span class="n">rbm</span><span class="o">.</span><span class="n">intercept_visible_</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(784,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">c</span> <span class="o">=</span> <span class="n">rbm</span><span class="o">.</span><span class="n">intercept_hidden_</span>
<span class="n">c</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(100,)
</pre></div>
</div>
</div>
</div>
<p>To generate <span class="math notranslate nohighlight">\(25\)</span> samples, we first generate <span class="math notranslate nohighlight">\(25\)</span> independent initial states. We stack them into a matrix, where each row is a different flattened random noise image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_pixels</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>To process all samples simultaneously, we make a small change to the code. We use <code class="docutils literal notranslate"><span class="pre">numpy.newaxis</span></code>
to make the offsets into column vectors, which are then automatically added to all columns of the resulting weighted sum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">rbm_mean_hidden</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">v</span> <span class="o">+</span> <span class="n">c</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">rbm_mean_visible</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>For plotting, we use a script <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html">adapted from here</a> (with help from ChatGPT).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">plot_imgs</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">n_imgs</span><span class="p">,</span> <span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span><span class="p">):</span>
    <span class="n">nx_imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_imgs</span><span class="p">))</span>
    <span class="n">ny_imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_imgs</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">nx_imgs</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">ny_imgs</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray_r'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We are now ready to run our Gibbs sampler. The outcome depends on the number of steps we take. After <span class="math notranslate nohighlight">\(100\)</span> steps, the outcome is somewhat realistic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">v_0</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">T</span>
<span class="n">gen_v</span> <span class="o">=</span> <span class="n">rbm_gibbs_sampling</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">v_0</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="n">plot_imgs</span><span class="p">(</span><span class="n">gen_v</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">nx_pixels</span><span class="p">,</span> <span class="n">ny_pixels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/aab5b59b73dc84d3b82547a838a8eadc75c8099fe134e50728fbf2920a04f55e.png" src="../Images/2abf435d81044deb01535f2ccdd32d63.png" data-original-src="https://mmids-textbook.github.io/_images/aab5b59b73dc84d3b82547a838a8eadc75c8099fe134e50728fbf2920a04f55e.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The RBM can be stacked to form a <a class="reference internal" href="#(https://en.wikipedia.org/wiki/Boltzmann_machine#Deep_Boltzmann_machine)"><span class="xref myst">deep belief network (DBN)</span></a>. Ask your favorite AI chatbot about the process of greedy layer-wise pretraining of a DBN using RBMs. Discuss how this can be used for initializing the weights of a deep neural network and compare the performance with random initialization. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In the context of Markov Chain Monte Carlo (MCMC), what is the primary goal?</p>
<p>a) To find the maximum likelihood estimate of a parameter.</p>
<p>b) To generate samples from a complex target distribution.</p>
<p>c) To optimize a loss function using gradient descent.</p>
<p>d) To cluster data points based on similarity.</p>
<p><strong>2</strong> In the Metropolis-Hastings algorithm, what is the role of the proposal chain <span class="math notranslate nohighlight">\(Q\)</span>?</p>
<p>a) It determines the stationary distribution of the resulting Markov chain.</p>
<p>b) It is used to compute the acceptance probability for the proposed moves.</p>
<p>c) It generates the candidate states for the next move in the Markov chain.</p>
<p>d) It ensures that the resulting Markov chain is irreducible and aperiodic.</p>
<p><strong>3</strong> What is the purpose of the Hastings correction in the Metropolis-Hastings algorithm?</p>
<p>a) To ensure that the proposal chain is symmetric.</p>
<p>b) To make the resulting Markov chain irreducible and aperiodic.</p>
<p>c) To ensure that the resulting Markov chain has the desired stationary distribution.</p>
<p>d) To improve the mixing time of the resulting Markov chain.</p>
<p><strong>4</strong> What is the role of the energy function <span class="math notranslate nohighlight">\(\mathcal{E}(\mathbf{v},\mathbf{h})\)</span> in a Restricted Boltzmann Machine (RBM)?</p>
<p>a) It determines the acceptance probability in the Metropolis-Hastings algorithm.</p>
<p>b) It defines the joint probability distribution of the visible and hidden units.</p>
<p>c) It represents the cost function to be minimized during training.</p>
<p>d) It controls the learning rate of the RBM.</p>
<p><strong>5</strong> What is the partition function <span class="math notranslate nohighlight">\(Z\)</span> used for in the RBM’s joint probability distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}(\mathbf{v}, \mathbf{h})\)</span>?</p>
<p>a) It normalizes the energy function.</p>
<p>b) It scales the weights matrix <span class="math notranslate nohighlight">\(W\)</span>.</p>
<p>c) It ensures that the probability distribution sums to one.</p>
<p>d) It adjusts the biases <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>.</p>
<p>Answer for 1: b. Justification: The text states that “The idea behind MCMC is simple. To generate samples from <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, use a Markov chain for which it is the stationary distribution.”</p>
<p>Answer for 2: c. Justification: The text describes the proposal chain as follows: “We first define a proposal chain, that is, a transition matrix <span class="math notranslate nohighlight">\(Q\)</span> on the space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. This chain does not need to have stationary distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>. But it is typically a chain that is easy to simulate.”</p>
<p>Answer for 3: c. Justification: The text states that the Hastings correction is “where the target distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> enters the picture, and the rejection probability is chosen to ensure that the new chain has the right stationary distribution, as we will see later.”</p>
<p>Answer for 4: b. Justification: The text defines the joint distribution of <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(h\)</span> as <span class="math notranslate nohighlight">\(\boldsymbol{\pi}(\mathbf{v},\mathbf{h}) = \frac{1}{Z} \exp(-\mathcal{E}(\mathbf{v},\mathbf{h}))\)</span>.</p>
<p>Answer for 5: c. Justification: The text explains that <span class="math notranslate nohighlight">\(Z\)</span>, the partition function, “ensures that <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> indeed sums to 1.”</p>
    
</body>
</html>