- en: '**Chapter 11 CUDA C on Multiple GPUs**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第11章 多GPU上的CUDA C**'
- en: 'There is an old saying that goes something like this: “The only thing better
    than computing on a GPU is computing on two GPUs.” Systems containing multiple
    graphics processors have become more and more common in recent years. Of course,
    in some ways multi-GPU systems are similar to multi-CPU systems in that they are
    still far from the common system configuration, but it has gotten quite easy to
    end up with more than one GPU in your system. Products such as the GeForce GTX
    295 contain two GPUs on a single card. NVIDIA’s Tesla S1070 contains a whopping
    four CUDA-capable graphics processors in it. Systems built around a recent NVIDIA
    chipset will have an integrated, CUDA-capable GPU on the motherboard. Adding a
    discrete NVIDIA GPU in one of the PCI Express slots will make this system multi-GPU.
    Neither of these scenarios is very farfetched, so we would be best served by learning
    to exploit the resources of a system with multiple GPUs in it.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 有一句古话是这样说的：“比在GPU上计算更好的事情，就是在两个GPU上计算。”近年来，包含多个图形处理器的系统变得越来越普遍。当然，从某些方面来看，多GPU系统与多CPU系统相似，因为它们仍然远未成为常见的系统配置，但如今在你的系统中拥有多个GPU变得相当容易。例如，GeForce
    GTX 295这类产品就包含了两个GPU在一张卡上。NVIDIA的Tesla S1070包含了四个支持CUDA的图形处理器。基于最近NVIDIA芯片组构建的系统会在主板上集成一个支持CUDA的GPU。在PCI
    Express插槽中添加一块独立的NVIDIA GPU会使该系统成为多GPU系统。这些场景都并不牵强，所以我们最好学习如何利用拥有多个GPU的系统资源。
- en: '**11.1 Chapter Objectives**'
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.1 章节目标**'
- en: 'Through the course of this chapter, you will accomplish the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将完成以下内容：
- en: • You will learn how to allocate and use *zero-copy* memory.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: • 你将学习如何分配和使用*零拷贝*内存。
- en: • You will learn how to use multiple GPUs within the same application.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: • 你将学习如何在同一个应用程序中使用多个GPU。
- en: • You will learn how to allocate and use *portable* pinned memory.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: • 你将学习如何分配和使用*可移植*的固定内存。
- en: '**11.2 Zero-Copy Host Memory**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.2 零拷贝主机内存**'
- en: 'In [Chapter 10](ch10.html#ch10), we examined pinned or page-locked memory,
    a new type of host memory that came with the guarantee that the buffer would never
    be swapped out of physical memory. If you recall, we allocated this memory by
    making a call to `cudaHostAlloc()` and passing `cudaHostAllocDefault` to get default,
    pinned memory. We promised that in the next chapter, you would see other more
    exciting means by which you can allocate pinned memory. Assuming that this is
    the only reason you’ve continued reading, you will be glad to know that the wait
    is over. The flag `cudaHostAllocMapped` can be passed instead of `cudaHostAllocDefault`.
    The host memory allocated using `cudaHostAllocMapped` is *pinned* in the same
    sense that memory allocated with `cudaHostAllocDefault` is pinned, specifically
    that it cannot be paged out of or relocated within physical memory. But in addition
    to using this memory from the host for memory copies to and from the GPU, this
    new kind of host memory allows us to violate one of the first rules we presented
    in [Chapter 3](ch03.html#ch03) concerning host memory: We can access this host
    memory directly from within CUDA C kernels. Because this memory does not require
    copies to and from the GPU, we refer to it as *zero-copy* memory.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#ch10)中，我们讨论了固定或页面锁定内存，这是一种新的主机内存类型，保证该缓冲区永远不会被换出物理内存。如果你还记得，我们通过调用`cudaHostAlloc()`并传递`cudaHostAllocDefault`来分配这类内存，从而获取默认的固定内存。我们承诺在下一章中，你将看到更多令人兴奋的分配固定内存的方法。如果这就是你继续阅读的唯一原因，那么你会很高兴地知道，等待终于结束了。可以传递`cudaHostAllocMapped`标志来代替`cudaHostAllocDefault`。使用`cudaHostAllocMapped`分配的主机内存与使用`cudaHostAllocDefault`分配的内存一样是*固定*的，具体来说，它不能被换出或在物理内存中重新定位。但除了从主机进行GPU之间的内存拷贝外，这种新的主机内存还允许我们违反我们在[第3章](ch03.html#ch03)中提出的关于主机内存的第一条规则：我们可以直接从CUDA
    C内核中访问这块主机内存。由于这块内存不需要进行拷贝，我们称其为*零拷贝*内存。
- en: '**11.2.1 Zero-Copy Dot Product**'
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**11.2.1 零拷贝点积**'
- en: 'Typically, our GPU accesses only GPU memory, and our CPU accesses only host
    memory. But in some circumstances, it’s better to break these rules. To see an
    instance where it’s better to have the GPU manipulate host memory, we’ll revisit
    our favorite reduction: the vector dot product. If you’ve managed to read this
    entire book, you may recall our first attempt at the dot product. We copied the
    two input vectors to the GPU, performed the computation, copied the intermediate
    results back to the host, and completed the computation on the CPU.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，GPU 只访问 GPU 内存，而 CPU 只访问主机内存。但在某些情况下，打破这些规则会更好。为了看到让 GPU 操作主机内存的情况，我们将再次回顾我们最喜欢的归约操作：向量点积。如果你已经读完了这本书，你可能会记得我们第一次尝试点积时的做法。我们将两个输入向量复制到
    GPU，执行计算，将中间结果复制回主机，并在 CPU 上完成计算。
- en: In this version, we’ll skip the explicit copies of our input up to the GPU and
    instead use zero-copy memory to access the data directly from the GPU. This version
    of dot product will be set up exactly like our pinned memory test. Specifically,
    we’ll write two functions; one will perform the test with standard host memory,
    and the other will finish the reduction on the GPU using zero-copy memory to hold
    the input and output buffers. First let’s take a look at the standard host memory
    version of the dot product. We start in the usual fashion by creating timing events,
    allocating input and output buffers, and filling our input buffers with data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本中，我们将跳过将输入显式复制到 GPU 的步骤，而是使用零拷贝内存直接从 GPU 访问数据。这个版本的点积将与我们之前的固定内存测试设置完全相同。具体来说，我们将编写两个函数；一个将使用标准主机内存进行测试，另一个将在
    GPU 上使用零拷贝内存来完成归约操作，并保存输入和输出缓冲区。首先让我们来看看使用标准主机内存版本的点积。我们按照常规方式开始，通过创建计时事件，分配输入和输出缓冲区，并将数据填充到输入缓冲区中。
- en: '![image](graphics/p0215-01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0215-01.jpg)'
- en: After the allocations and data creation, we can begin the computations. We start
    our timer, copy our inputs to the GPU, execute the dot product kernel, and copy
    the partial results back to the host.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成分配和数据创建后，我们可以开始计算。我们启动计时器，将输入数据复制到 GPU，执行点积核函数，并将部分结果复制回主机。
- en: '![image](graphics/p0216-01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0216-01.jpg)'
- en: 'Now we need to finish up our computations on the CPU as we did in [Chapter
    5](ch05.html#ch05). Before doing this, we’ll stop our event timer because it only
    measures work that’s being performed on the GPU:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要像在[第 5 章](ch05.html#ch05)中那样在 CPU 上完成计算。在执行此操作之前，我们会停止我们的事件计时器，因为它只测量
    GPU 上执行的工作：
- en: '![image](graphics/p0216-02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0216-02.jpg)'
- en: Finally, we sum our partial results and free our input and output buffers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将部分结果求和，并释放输入和输出缓冲区。
- en: '![image](graphics/p0216-03.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0216-03.jpg)'
- en: 'The version that uses zero-copy memory will be remarkably similar, with the
    exception of memory allocation. So, we start by allocating our input and output,
    filling the input memory with data as before:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零拷贝内存的版本将与此版本非常相似，唯一不同的是内存分配。因此，我们首先分配输入和输出内存，并像以前一样填充输入内存数据：
- en: '![image](graphics/p0217-01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0217-01.jpg)'
- en: As with [Chapter 10](ch10.html#ch10), we see `cudaHostAlloc()` in action again,
    although we’re now using the `flags` argument to specify more than just default
    behavior. The flag `cudaHostAllocMapped` tells the runtime that we intend to access
    this buffer from the GPU. In other words, this flag is what makes our buffer *zero-copy*.
    For the two input buffers, we specify the flag `cudaHostAllocWriteCombined`. This
    flag indicates that the runtime should allocate the buffer as write-combined with
    respect to the CPU cache. This flag will not change functionality in our application
    but represents an important performance enhancement for buffers that will be read
    only by the GPU. However, write-combined memory can be extremely inefficient in
    scenarios where the CPU also needs to perform reads from the buffer, so you will
    have to consider your application’s likely access patterns when making this decision.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与[第 10 章](ch10.html#ch10)一样，我们再次看到 `cudaHostAlloc()` 的使用，尽管现在我们使用 `flags` 参数来指定不同于默认行为的设置。标志
    `cudaHostAllocMapped` 告诉运行时我们打算从 GPU 访问此缓冲区。换句话说，正是这个标志使我们的缓冲区变成了 *零拷贝*。对于两个输入缓冲区，我们指定了标志
    `cudaHostAllocWriteCombined`。该标志表示运行时应该将缓冲区分配为与 CPU 缓存写合并的方式。这个标志不会改变我们应用程序的功能，但对于只会被
    GPU 读取的缓冲区，它代表了一个重要的性能提升。然而，在 CPU 也需要从缓冲区读取数据的场景中，写合并内存可能会非常低效，因此在做出这个决策时，你需要考虑应用程序可能的访问模式。
- en: 'Since we’ve allocated our host memory with the flag `cudaHostAllocMapped`,
    the buffers can be accessed from the GPU. However, the GPU has a different virtual
    memory space than the CPU, so the buffers will have different addresses when they’re
    accessed on the GPU as compared to the CPU. The call to `cudaHostAlloc()` returns
    the CPU pointer for the memory, so we need to call `cudaHostGetDevicePointer()`
    in order to get a valid GPU pointer for the memory. These pointers will be passed
    to the kernel and then used by the GPU to read from and write to our host allocations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经使用标志`cudaHostAllocMapped`分配了主机内存，这些缓冲区可以从GPU访问。然而，GPU的虚拟内存空间与CPU不同，因此在GPU上访问时，这些缓冲区的地址会与在CPU上访问时不同。`cudaHostAlloc()`调用返回内存的CPU指针，因此我们需要调用`cudaHostGetDevicePointer()`来获取有效的GPU指针。然后，这些指针会传递给内核，供GPU读取和写入我们的主机分配：
- en: '![image](graphics/p0219-01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0219-01.jpg)'
- en: With valid device pointers in hand, we’re ready to start our timer and launch
    our kernel.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 手中有了有效的设备指针后，我们就可以开始计时并启动我们的内核了。
- en: '![image](graphics/p0219-02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0219-02.jpg)'
- en: Even though the pointers `dev_a`, `dev_b`, and `dev_partial_c` all reside on
    the host, they will look to our kernel as if they are GPU memory, thanks to our
    calls to `cudaHostGetDevicePointer()`. Since our partial results are already on
    the host, we don’t need to bother with a `cudaMemcpy()` from the device. However,
    you will notice that we’re synchronizing the CPU with the GPU by calling `cudaThreadSynchronize()`.
    The contents of zero-copy memory are undefined during the execution of a kernel
    that potentially makes changes to its contents. After synchronizing, we’re sure
    that the kernel has completed and that our zero-copy buffer contains the results
    so we can stop our timer and finish the computation on the CPU as we did before.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 即使指针`dev_a`、`dev_b`和`dev_partial_c`都驻留在主机上，它们对于我们的内核来说看起来就像是GPU内存，这是因为我们调用了`cudaHostGetDevicePointer()`。由于我们的部分结果已经在主机上，因此我们不需要进行`cudaMemcpy()`从设备拷贝。然而，你会注意到，我们通过调用`cudaThreadSynchronize()`将CPU与GPU进行了同步。由于内核可能会更改其内容，零拷贝内存的内容在执行过程中是未定义的。同步之后，我们可以确保内核已经完成，并且我们的零拷贝缓冲区包含了结果，这样我们就可以停止计时器并像之前一样在CPU上完成计算。
- en: '![image](graphics/p0219-03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0219-03.jpg)'
- en: The only thing remaining in the `cudaHostAlloc()` version of the dot product
    is cleanup.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在`cudaHostAlloc()`版本的点积中，唯一剩下的就是清理工作。
- en: '![image](graphics/p0220-01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0220-01.jpg)'
- en: You will notice that no matter what flags we use with `cudaHostAlloc()`, the
    memory always gets freed in the same way. Specifically, a call to `cudaFreeHost()`
    does the trick.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，无论我们在`cudaHostAlloc()`中使用什么标志，内存总是以相同的方式被释放。具体来说，调用`cudaFreeHost()`就能完成这项操作。
- en: And that’s that! All that remains is to look at how `main()` ties all of this
    together. The first thing we need to check is whether our device supports mapping
    host memory. We do this the same way we checked for device overlap in the previous
    chapter, with a call to `cudaGetDeviceProperties()`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！剩下的就是看看`main()`是如何将这一切结合在一起的。我们需要检查的第一件事是我们的设备是否支持映射主机内存。我们可以像在上一章中检查设备重叠那样，通过调用`cudaGetDeviceProperties()`来完成这项检查。
- en: '![image](graphics/p0220-02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0220-02.jpg)'
- en: 'Assuming that our device supports zero-copy memory, we place the runtime into
    a state where it will be able to allocate zero-copy buffers for us. We accomplish
    this by a call to `cudaSetDeviceFlags()` and by passing the flag `cudaDeviceMapHost`
    to indicate that we want the device to be allowed to map host memory:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的设备支持零拷贝内存，我们将运行时环境置于一种能够为我们分配零拷贝缓冲区的状态。我们通过调用`cudaSetDeviceFlags()`并传递标志`cudaDeviceMapHost`来实现这一点，表示我们希望设备能够映射主机内存：
- en: '![image](graphics/p0221-01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0221-01.jpg)'
- en: 'That’s really all there is to `main()`. We run our two tests, display the elapsed
    time, and exit the application:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`main()`的全部内容。我们运行两个测试，显示经过的时间，然后退出应用程序：
- en: '![image](graphics/p0221-02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0221-02.jpg)'
- en: 'The kernel itself is unchanged from [Chapter 5](ch05.html#ch05), but for the
    sake of completeness, here it is in its entirety:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 核心代码本身与[第5章](ch05.html#ch05)相同，但为了完整起见，这里将其完整展示：
- en: '![image](graphics/p0221-03.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0221-03.jpg)'
- en: '![image](graphics/p0222-01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0222-01.jpg)'
- en: '**11.2.2 Zero-Copy Performance**'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**11.2.2 零拷贝性能**'
- en: 'What should we expect to gain from using zero-copy memory? The answer to this
    question is different for discrete GPUs and integrated GPUs. *Discrete GPUs* are
    graphics processors that have their own dedicated DRAMs and typically sit on separate
    circuit boards from the CPU. For example, if you have ever installed a graphics
    card into your desktop, this GPU is a discrete GPU. *Integrated GPUs* are graphics
    processors built into a system’s chipset and usually share regular system memory
    with the CPU. Many recent systems built with NVIDIA’s nForce media and communications
    processors (MCPs) contain CUDA-capable integrated GPUs. In addition to nForce
    MCPs, all the netbook, notebook, and desktop computers based on NVIDIA’s new ION
    platform contain integrated, CUDA-capable GPUs. For integrated GPUs, the use of
    zero-copy memory is *always* a performance win because the memory is physically
    shared with the host anyway. Declaring a buffer as zero-copy has the sole effect
    of preventing unnecessary copies of data. But remember that nothing is free and
    that zero-copy buffers are still constrained in the same way that all pinned memory
    allocations are constrained: Each pinned allocation carves into the system’s available
    physical memory, which will eventually degrade system performance.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零拷贝内存我们应该期望获得什么样的收益？这个问题的答案在离散GPU和集成GPU之间是不同的。*离散GPU*是拥有自己专用DRAM的图形处理器，通常与CPU位于不同的电路板上。例如，如果你曾经在台式机中安装过显卡，这块GPU就是离散GPU。*集成GPU*是内置在系统芯片组中的图形处理器，通常与CPU共享常规系统内存。许多近期基于NVIDIA
    nForce媒体和通信处理器（MCPs）构建的系统都包含支持CUDA的集成GPU。除了nForce MCPs外，所有基于NVIDIA新ION平台的上网本、笔记本和台式电脑都包含集成的、支持CUDA的GPU。对于集成GPU，使用零拷贝内存*总是*能带来性能提升，因为这些内存本就与主机共享。将缓冲区声明为零拷贝只会防止不必要的数据拷贝。但请记住，世上没有免费的午餐，零拷贝缓冲区仍然会像所有固定内存分配一样受到约束：每个固定的分配都会占用系统的物理内存，最终会降低系统性能。
- en: In cases where inputs and outputs are used exactly once, we will even see a
    performance enhancement when using zero-copy memory with a discrete GPU. Since
    GPUs are designed to excel at hiding the latencies associated with memory access,
    performing reads and writes over the PCI Express bus can be mitigated to some
    degree by this mechanism, yielding a noticeable performance advantage. But since
    the zero-copy memory is not cached on the GPU, in situations where the memory
    gets read multiple times, we will end up paying a large penalty that could be
    avoided by simply copying the data to the GPU first.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入和输出仅使用一次的情况下，当使用零拷贝内存与离散GPU配合时，我们甚至能看到性能提升。由于GPU设计上擅长隐藏与内存访问相关的延迟，通过PCI Express总线进行读写时，这个机制能在一定程度上缓解延迟，从而带来明显的性能优势。但由于零拷贝内存并未在GPU上进行缓存，当内存被多次读取时，我们将面临较大的性能损失，这本可以通过先将数据拷贝到GPU来避免。
- en: How do you determine whether a GPU is integrated or discrete? Well, you can
    open up your computer and look, but this solution is fairly unworkable for your
    CUDA C application. Your code can check this property of a GPU by, not surprisingly,
    looking at the structure returned by `cudaGetDeviceProperties()`. This structure
    has a field named `integrated`, which will be `true` if the device is an integrated
    GPU and `false` if it’s not.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如何判断一个GPU是集成的还是离散的呢？嗯，你可以打开电脑看一看，但对于你的CUDA C应用程序来说，这种方法并不实用。你的代码可以通过检查`cudaGetDeviceProperties()`返回的结构来判断这一属性。这个结构中有一个名为`integrated`的字段，如果设备是集成GPU，它的值为`true`，如果不是，则为`false`。
- en: Since our dot product application satisfies the “read and/or write exactly once”
    constraint, it’s possible that it will enjoy a performance boost when run with
    zero-copy memory. And in fact, it does enjoy a slight boost in performance. On
    a GeForce GTX 285, the execution time improves by more than 45 percent, dropping
    from 98.1ms to 52.1ms when migrated to zero-copy memory. A GeForce GTX 280 enjoys
    a similar improvement, speeding up by 34 percent from 143.9 ms to 94.7ms. Of course,
    different GPUs will exhibit different performance characteristics because of varying
    ratios of computation to bandwidth, as well as because of variations in effective
    PCI Express bandwidth across chipsets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的点积应用满足“只读/写一次”的限制，因此在使用零拷贝内存时，可能会获得性能提升。事实上，它确实享受到了轻微的性能提升。在一台GeForce GTX
    285上，执行时间提升超过45%，从98.1ms下降到52.1ms，迁移到零拷贝内存后。GeForce GTX 280也享受到了类似的提升，执行时间从143.9ms提升到94.7ms，提升了34%。当然，由于计算与带宽的比例不同，以及各芯片组的有效PCI
    Express带宽的差异，不同的GPU会呈现出不同的性能特征。
- en: '**11.3 Using Multiple GPUs**'
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.3 使用多个GPU**'
- en: In the previous section, we mentioned how devices are either integrated or discrete
    GPUs, where the former is built into the system’s chipset and the latter is typically
    an expansion card in a PCI Express slot. More and more systems contain *both*
    integrated and discrete GPUs, meaning that they also have multiple CUDA-capable
    processors. NVIDIA also sells products, such as the GeForce GTX 295, that contain
    more than one GPU. A GeForce GTX 295, while physically occupying a single expansion
    slot, will appear to your CUDA applications as two separate GPUs. Furthermore,
    users can also add multiple GPUs to separate PCI Express slots, connecting them
    with bridges using NVIDIA’s *scalable link interface* (SLI) technology. As a result
    of these trends, it has become relatively common to have a CUDA application running
    on a system with multiple graphics processors. Since our CUDA applications tend
    to be very parallelizable to begin with, it would be excellent if we could use
    every CUDA device in the system to achieve maximum throughput. So, let’s figure
    out how we can accomplish this.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们提到过设备有集成GPU和独立GPU两种类型，前者内置于系统芯片组中，后者通常是安装在PCI Express插槽中的扩展卡。越来越多的系统同时包含*集成*和独立GPU，这意味着它们也有多个CUDA支持的处理器。NVIDIA还销售如GeForce
    GTX 295这样的产品，内含多个GPU。尽管GeForce GTX 295物理上只占用一个扩展插槽，但它会在CUDA应用程序中显示为两个独立的GPU。此外，用户还可以将多个GPU添加到不同的PCI
    Express插槽，并使用NVIDIA的*可扩展链路接口*（SLI）技术将它们连接起来。由于这些趋势，CUDA应用程序在多显卡系统中运行变得相对常见。由于我们的CUDA应用程序通常具有很强的并行性，因此如果能利用系统中的每一个CUDA设备以实现最大吞吐量，那将是非常棒的。那么，让我们来看看如何实现这一目标。
- en: To avoid learning a new example, let’s convert our dot product to use multiple
    GPUs. To make our lives easier, we will summarize all the data necessary to compute
    a dot product in a single structure. You’ll see momentarily exactly why this will
    make our lives easier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免学习一个新的例子，让我们将点积操作转换为使用多个GPU。为了让我们更加轻松，我们将所有计算点积所需的数据汇总到一个结构体中。你马上就会明白为什么这样做能让我们的工作变得更容易。
- en: '![image](graphics/p0224-01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0224-01.jpg)'
- en: This structure contains the identification for the device on which the dot product
    will be computed; it contains the size of the input buffers as well as pointers
    to the two inputs `a` and `b`. Finally, it has an entry to store the value computed
    as the dot product of `a` and `b`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该结构体包含计算点积的设备的标识符；它包含输入缓冲区的大小，以及指向两个输入`a`和`b`的指针。最后，它有一个条目用于存储计算出的`a`和`b`的点积值。
- en: To use `N` GPUs, we first would like to know exactly what value of `N` we’re
    dealing with. So, we start our application with a call to `cudaGetDeviceCount()`
    in order to determine how many CUDA-capable processors have been installed in
    our system.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`N`个GPU，我们首先需要知道我们正在处理的`N`的确切值。因此，我们通过调用`cudaGetDeviceCount()`来启动应用程序，以确定系统中安装了多少个CUDA支持的处理器。
- en: '![image](graphics/p0225-01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0225-01.jpg)'
- en: This example is designed to show multi-GPU usage, so you’ll notice that we simply
    exit if the system has only one CUDA device (not that there’s anything wrong with
    that). This is not encouraged as a best practice for obvious reasons. To keep
    things as simple as possible, we’ll allocate standard host memory for our inputs
    and fill them with data exactly how we’ve done in the past.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本例旨在展示多GPU的使用，因此您会注意到，如果系统只有一个CUDA设备，我们会直接退出（并不是说这样有问题）。显然，出于种种原因，这种做法并不推荐作为最佳实践。为了尽可能简单，我们将为输入分配标准主机内存，并按以往的方式填充数据。
- en: '![image](graphics/p0225-02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0225-02.jpg)'
- en: 'We’re now ready to dive into the multi-GPU code. The trick to using multiple
    GPUs with the CUDA runtime API is realizing that each GPU needs to be controlled
    by a different CPU thread. Since we have used only a single GPU before, we haven’t
    needed to worry about this. We have moved a lot of the annoyance of multithreaded
    code to our file of auxiliary code, `book.h`. With this code tucked away, all
    we need to do is fill a structure with data necessary to perform the computations.
    Although the system could have any number of GPUs greater than one, we will use
    only two of them for clarity:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备深入了解多GPU代码。使用CUDA运行时API来操作多个GPU的关键是认识到每个GPU需要由不同的CPU线程控制。由于之前我们只使用了一个GPU，所以不需要考虑这一点。我们将多线程代码中的大部分麻烦都移到了辅助代码文件`book.h`中。通过将这些代码隐藏起来，我们只需填充一个结构体，提供执行计算所需的数据。尽管系统中可能有多个GPU，但为了清晰起见，我们只使用其中的两个：
- en: '![image](graphics/p0226-01.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0226-01.jpg)'
- en: To proceed, we pass one of the `DataStruct` variables to a utility function
    we’ve named `start_thread()`. We also pass `start_thread()` a pointer to a function
    to be called by the newly created thread; this example’s thread function is called
    `routine()`. The function `start_thread()` will create a new thread that then
    calls the specified function, passing the `DataStruct` to this function. The other
    call to `routine()` gets made from the default application thread (so we’ve created
    only one *additional* thread).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将一个`DataStruct`变量传递给一个名为`start_thread()`的工具函数。我们还将一个指向函数的指针传递给`start_thread()`，该函数将在新创建的线程中被调用；本例中的线程函数名为`routine()`。`start_thread()`函数将创建一个新线程，随后该线程调用指定的函数，并将`DataStruct`传递给该函数。另一个对`routine()`的调用将由默认的应用线程执行（因此我们只创建了一个*额外*线程）。
- en: '![image](graphics/p0226-02.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0226-02.jpg)'
- en: Before we proceed, we have the main application thread wait for the other thread
    to finish by calling `end_thread()`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们让主应用线程通过调用`end_thread()`等待其他线程完成。
- en: '![image](graphics/p0226-03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0226-03.jpg)'
- en: Since both threads have completed at this point in `main()`, it’s safe to clean
    up and display the result.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此时`main()`中的两个线程都已完成，所以可以安全地进行清理并显示结果。
- en: '![image](graphics/p0227-01.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0227-01.jpg)'
- en: Notice that we sum the results computed by each thread. This is the last step
    in our dot product reduction. In another algorithm, this combination of multiple
    results may involve other steps. In fact, in some applications, the two GPUs may
    be executing completely different code on completely different data sets. For
    simplicity’s sake, this is not the case in our dot product example.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将每个线程计算的结果相加。这是我们点积归约的最后一步。在另一种算法中，多个结果的组合可能涉及其他步骤。实际上，在某些应用中，两个GPU可能会在完全不同的数据集上执行完全不同的代码。为了简单起见，在我们的点积示例中并非如此。
- en: 'Since the dot product routine is identical to the other versions you’ve seen,
    we’ll omit it from this section. However, the contents of `routine()` may be of
    interest. We declare `routine()` as taking and returning a `void*` so that you
    can reuse the `start_thread()` code with arbitrary implementations of a thread
    function. Although we’d love to take credit for this idea, it’s fairly standard
    procedure for callback functions in C:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点积例程与您之前看到的其他版本完全相同，我们将在本节中省略它。然而，`routine()`的内容可能会引起您的兴趣。我们声明`routine()`函数接受并返回一个`void*`，这样您就可以用任意线程函数的实现重用`start_thread()`代码。虽然我们很愿意为这个想法归功于自己，但这其实是C语言中回调函数的标准做法：
- en: '![image](graphics/p0227-02.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0227-02.jpg)'
- en: Each thread calls `cudaSetDevice()`, and each passes a different ID to this
    function. As a result, we know each thread will be manipulating a different GPU.
    These GPUs may have identical performance, as with the dual-GPU GeForce GTX 295,
    or they may be different GPUs as would be the case in a system that has both an
    integrated GPU and a discrete GPU. These details are not important to our application,
    though they might be of interest to you. Particularly, these details prove useful
    if you depend on a certain minimum compute capability to launch your kernels or
    if you have a serious desire to load balance your application across the system’s
    GPUs. If the GPUs are different, you will need to do some work to partition the
    computations so that each GPU is occupied for roughly the same amount of time.
    For our purposes in this example, however, these are piddling details with which
    we won’t worry.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程都会调用`cudaSetDevice()`，并为此函数传递不同的ID。结果是，我们知道每个线程将操作不同的GPU。这些GPU可能具有相同的性能，如双GPU
    GeForce GTX 295，或者它们可能是不同的GPU，例如在一个同时拥有集成GPU和离散GPU的系统中。然而，这些细节对于我们的应用程序并不重要，尽管它们可能对你有兴趣。特别地，如果你依赖于某个最低的计算能力来启动内核，或者你有强烈的愿望在系统的GPU之间负载平衡，这些细节就非常有用。如果GPU不同，你需要做一些工作来划分计算任务，以确保每个GPU大致占用相同的时间。然而，对于我们在这个示例中的目的来说，这些都是微不足道的细节，我们不必担心。
- en: 'Outside the call to `cudaSetDevice()` to specify which CUDA device we intend
    to use, this implementation of `routine()` is remarkably similar to the vanilla
    `malloc_test()` from Section 11.2.1: Zero-Copy Dot Product. We allocate buffers
    for our GPU copies of the input and a buffer for our partial results followed
    by a `cudaMemcpy()` of each input array to the GPU.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调用`cudaSetDevice()`来指定我们打算使用的CUDA设备之外，`routine()`的实现与第11.2.1节：零拷贝点积中的原始`malloc_test()`非常相似。我们为GPU上输入数据的副本分配缓冲区，并为部分结果分配一个缓冲区，然后通过`cudaMemcpy()`将每个输入数组复制到GPU。
- en: '![image](graphics/p0228-01.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0228-01.jpg)'
- en: We then launch our dot product kernel, copy the results back, and finish the
    computation on the CPU.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们启动点积内核，将结果复制回去，并在CPU上完成计算。
- en: '![image](graphics/p0229-01.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0229-01.jpg)'
- en: As usual, we clean up our GPU buffers and return the dot product we’ve computed
    in the `returnValue` field of our `DataStruct`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们清理GPU缓冲区，并在`DataStruct`的`returnValue`字段中返回我们计算的点积结果。
- en: '![image](graphics/p0229-02.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0229-02.jpg)'
- en: So when we get down to it, outside of the host thread management issue, using
    multiple GPUs is not too much tougher than using a single GPU. Using our helper
    code to create a thread and execute a function on that thread, this becomes significantly
    more manageable. If you have your own thread libraries, you should feel free to
    use them in your own applications. You just need to remember that each GPU gets
    its own thread, and everything else is cream cheese.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，归根结底，除了主机线程管理的问题，使用多个GPU并不会比使用单个GPU困难太多。利用我们的辅助代码创建一个线程并在该线程上执行函数后，这变得更加可管理。如果你有自己的线程库，可以在自己的应用程序中自由使用。你只需要记住，每个GPU都有自己的线程，其他的一切就像奶油芝士一样简单。
- en: '**11.4 Portable Pinned Memory**'
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.4 可移植的固定内存**'
- en: The last important piece to using multiple GPUs involves the use of pinned memory.
    We learned in [Chapter 10](ch10.html#ch10) that pinned memory is actually host
    memory that has its pages locked in physical memory to prevent it from being paged
    out or relocated. However, it turns out that pages can appear pinned to a single
    CPU thread only. That is, they will remain page-locked if *any* thread has allocated
    them as pinned memory, but they will only *appear* page-locked to the thread that
    allocated them. If the pointer to this memory is shared between threads, the other
    threads will see the buffer as standard, pageable data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个GPU的最后一个重要部分涉及使用固定内存（pinned memory）。我们在[第10章](ch10.html#ch10)中了解到，固定内存实际上是主机内存，它的页面被锁定在物理内存中，以防止被换出或重新定位。然而，事实证明，页面只能被锁定在单个CPU线程上。也就是说，*任何*线程只要将它们分配为固定内存，这些页面就会保持页面锁定，但它们仅对分配它们的线程*看起来*是页面锁定的。如果该内存的指针被多个线程共享，其他线程会将该缓冲区视为标准的可分页数据。
- en: As a side effect of this behavior, when a thread that did not allocate a pinned
    buffer attempts to perform a `cudaMemcpy()` using it, the copy will be performed
    at standard pageable memory speeds. As we saw in [Chapter 10](ch10.html#ch10),
    this speed can be roughly 50 percent of the maximum attainable transfer speed.
    What’s worse, if the thread attempts to enqueue a `cudaMemcpyAsync()` call into
    a CUDA stream, this operation will fail because it requires a pinned buffer to
    proceed. Since the buffer appears pageable from the thread that didn’t allocate
    it, the call dies a grisly death. Even in the future nothing works!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种行为的副作用，当一个没有分配固定缓冲区的线程尝试使用它执行 `cudaMemcpy()` 时，复制将以标准的可分页内存速度执行。正如我们在[第
    10 章](ch10.html#ch10)中看到的那样，这个速度大约是最大传输速度的 50%。更糟糕的是，如果线程尝试将 `cudaMemcpyAsync()`
    调用加入到 CUDA 流中，这个操作将失败，因为它需要一个固定缓冲区才能继续。由于该缓冲区在没有分配的线程看来是可分页的，这个调用就会死于非命。即使是未来，也无法正常工作！
- en: 'But there is a remedy to this problem. We can allocate pinned memory as *portable*,
    meaning that we will be allowed to migrate it between host threads and allow any
    thread to view it as a pinned buffer. To do so, we use our trusty `cudaHostAlloc()`
    to allocate the memory, but we call it with a new flag: `cudaHostAllocPortable`.
    This flag can be used in concert with the other flags you’ve seen, such as `cudaHostAllocWriteCombined`
    and `cudaHostAllocMapped`. This means that you can allocate your host buffers
    as any combination of portable, zero-copy and write-combined.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有一种解决方法。我们可以将固定内存分配为 *便携式*，这意味着我们将允许它在主机线程之间迁移，并允许任何线程将其视为固定缓冲区。为此，我们使用我们可靠的
    `cudaHostAlloc()` 来分配内存，但我们用一个新标志来调用它：`cudaHostAllocPortable`。这个标志可以与其他标志一起使用，比如
    `cudaHostAllocWriteCombined` 和 `cudaHostAllocMapped`。这意味着你可以将主机缓冲区分配为便携式、零拷贝和写入合并的任何组合。
- en: To demonstrate portable pinned memory, we’ll enhance our multi-GPU dot product
    application. We’ll adapt our original zero-copy version of the dot product, so
    this version begins as something of a mash-up of the zero-copy and multi-GPU versions.
    As we have throughout this chapter, we need to verify that there are at least
    two CUDA-capable GPUs and that both can handle zero-copy buffers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示便携式固定内存，我们将增强我们的多 GPU 点积应用程序。我们将改编原始的零拷贝版本点积，因此这个版本在某种程度上是零拷贝版本和多 GPU 版本的混合体。正如我们在本章中所做的那样，我们需要验证是否至少有两块支持
    CUDA 的 GPU，并且两块 GPU 都能处理零拷贝缓冲区。
- en: '![image](graphics/p0231-01.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0231-01.jpg)'
- en: 'In previous examples, we’d be ready to start allocating memory on the host
    to hold our input vectors. To allocate portable pinned memory, however, it’s necessary
    to first set the CUDA device on which we intend to run. Since we intend to use
    the device for zero-copy memory as well, we follow the `cudaSetDevice()` call
    with a call to `cudaSetDeviceFlags()`, as we did in Section 11.2.1: Zero-Copy
    Dot Product.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们准备开始分配主机内存以存储输入向量。然而，为了分配便携式固定内存，首先必须设置我们打算运行的 CUDA 设备。由于我们还打算使用该设备进行零拷贝内存操作，因此在调用
    `cudaSetDevice()` 后，我们会接着调用 `cudaSetDeviceFlags()`，就像我们在第 11.2.1 节《零拷贝点积》中所做的那样。
- en: '![image](graphics/p0231-02.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0231-02.jpg)'
- en: Earlier in this chapter, we called `cudaSetDevice()` but not until we had already
    allocated our memory and created our threads. One of the requirements of allocating
    portable page-locked memory with `cudaHostAlloc()`, though, is that we have initialized
    the device first by calling `cudaSetDevice()`. You will also notice that we pass
    our newly learned flag, `cudaHostAllocPortable`, to both allocations. Since these
    were allocated after calling `cudaSetDevice(0)`, only CUDA device zero would see
    these buffers as pinned memory if we had not specified that they were to be portable
    allocations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早些时候，我们调用了 `cudaSetDevice()`，但直到我们已经分配了内存并创建了线程才执行。需要注意的是，使用 `cudaHostAlloc()`
    分配便携式页锁定内存的一个要求是，必须先通过调用 `cudaSetDevice()` 初始化设备。不过，你也会注意到，我们将新学习的标志 `cudaHostAllocPortable`
    传递给了两个内存分配函数。由于这些内存是在调用 `cudaSetDevice(0)` 后分配的，如果我们没有指定它们为便携式分配，那么只有 CUDA 设备零会将这些缓冲区视为固定内存。
- en: 'We continue the application as we have in the past, generating data for our
    input vectors and preparing our `DataStruct` structures as we did in the multi-GPU
    example in [Section 11.3](ch11.html#ch11lev3): Using Multiple GPUs.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像过去一样继续应用程序，为我们的输入向量生成数据，并准备我们的 `DataStruct` 结构，就像我们在[第 11.3 节](ch11.html#ch11lev3)《使用多个
    GPU》的多 GPU 示例中所做的那样。
- en: '![image](graphics/p0232-01.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0232-01.jpg)'
- en: We can then create our secondary thread and call `routine()` to begin computing
    on each device.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建我们的辅助线程并调用`routine()`，以便在每个设备上开始计算。
- en: '![image](graphics/p0233-01.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0233-01.jpg)'
- en: Because our host memory was allocated by the CUDA runtime, we use `cudaFreeHost()`
    to free it. Other than no longer calling `free()`, we have seen all there is to
    see in `main()`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的主机内存是由CUDA运行时分配的，所以我们使用`cudaFreeHost()`来释放它。除此之外，我们不再调用`free()`，`main()`中的内容也就到此为止。
- en: '![image](graphics/p0233-02.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0233-02.jpg)'
- en: To support portable pinned memory and zero-copy memory in our multi-GPU application,
    we need to make two notable changes in the code for `routine()`. The first is
    a bit subtle, and in no way should this have been obvious.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持在我们的多GPU应用程序中使用可移植的固定内存和零拷贝内存，我们需要对`routine()`函数的代码进行两处显著的更改。第一个更改有点微妙，且不应该是显而易见的。
- en: '![image](graphics/p0233-03.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0233-03.jpg)'
- en: You may recall in our multi-GPU version of this code, we need a call to `cudaSetDevice()`
    in `routine()` in order to ensure that each participating thread controls a different
    GPU. On the other hand, in this example we have already made a call to `cudaSetDevice()`
    from the main thread. We did so in order to allocate pinned memory in `main()`.
    As a result, we only want to call `cudaSetDevice()` and `cudaSetDeviceFlags()`
    on devices where we have not made this call. That is, we call these two functions
    if the `deviceID` is not zero. Although it would yield cleaner code to simply
    repeat these calls on device zero, it turns out that this is in fact an error.
    Once you have set the device on a particular thread, you cannot call `cudaSetDevice()`
    again, even if you pass the same device identifier. The highlighted `if()` statement
    helps us avoid this little nasty-gram from the CUDA runtime, so we move on to
    the next important change to `routine()`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得在我们的多GPU版本代码中，我们需要在`routine()`中调用`cudaSetDevice()`，以确保每个参与的线程控制一个不同的GPU。另一方面，在这个示例中，我们已经从主线程中调用了`cudaSetDevice()`，目的是在`main()`中分配固定内存。因此，我们只希望在尚未调用该函数的设备上调用`cudaSetDevice()`和`cudaSetDeviceFlags()`。也就是说，当`deviceID`不为零时，我们才调用这两个函数。虽然重复在设备零上调用这两个函数会使代码更简洁，但实际上这会导致错误。一旦你在某个线程上设置了设备，就不能再次调用`cudaSetDevice()`，即使传递的是相同的设备标识符。高亮显示的`if()`语句帮助我们避免了来自CUDA运行时的这个小错误，因此我们继续对`routine()`进行下一个重要的更改。
- en: In addition to using portable pinned memory for the host-side memory, we are
    using zero-copy in order to access these buffers directly from the GPU. Consequently,
    we no longer use `cudaMemcpy()` as we did in the original multi-GPU application,
    but we use `cudaHostGetDevicePointer()` to get valid device pointers for the host
    memory as we did in the zero-copy example. However, you will notice that we use
    standard GPU memory for the partial results. As always, this memory gets allocated
    using `cudaMalloc()`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为主机端内存使用可移植的固定内存外，我们还使用零拷贝直接从GPU访问这些缓冲区。因此，我们不再像原来的多GPU应用程序中那样使用`cudaMemcpy()`，而是使用`cudaHostGetDevicePointer()`来获取主机内存的有效设备指针，就像在零拷贝示例中一样。然而，你会注意到我们使用了标准的GPU内存来存储部分结果。和往常一样，这块内存是通过`cudaMalloc()`分配的。
- en: '![image](graphics/p0234-01.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0234-01.jpg)'
- en: At this point, we’re pretty much ready to go, so we launch our kernel and copy
    our results back from the GPU.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时刻，我们基本上已经准备好开始，因此我们启动内核并将结果从GPU复制回来。
- en: '![image](graphics/p0235-01.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0235-01.jpg)'
- en: We conclude as we always have in our dot product example by summing our partial
    results on the CPU, freeing our temporary storage, and returning to `main()`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像在点积示例中一样，通过在CPU上汇总部分结果、释放临时存储并返回`main()`来结束。
- en: '![image](graphics/p0235-02.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/p0235-02.jpg)'
- en: '**11.5 Chapter Review**'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.5 章节回顾**'
- en: We have seen some new types of host memory allocations, all of which get allocated
    with a single call, `cudaHostAlloc()`. Using a combination of this one entry point
    and a set of argument flags, we can allocate memory as any combination of zero-copy,
    portable, and/or write-combined. We used *zero-copy* buffers to avoid making explicit
    copies of data to and from the GPU, a maneuver that potentially speeds up a wide
    class of applications. Using a support library for threading, we manipulated multiple
    GPUs from the same application, allowing our dot product computation to be performed
    across multiple devices. Finally, we saw how multiple GPUs could share pinned
    memory allocations by allocating them as *portable* pinned memory. Our last example
    used portable pinned memory, multiple GPUs, and zero-copy buffers in order to
    demonstrate a turbocharged version of the dot product we started toying with back
    in [Chapter 5](ch05.html#ch05). As multiple-device systems gain popularity, these
    techniques should serve you well in harnessing the computational power of your
    target platform in its entirety.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一些新的主机内存分配方式，所有这些都通过一个单独的调用`cudaHostAlloc()`进行分配。通过组合这个入口点和一组参数标志，我们可以将内存分配为零拷贝、可移植和/或写合并的任意组合。我们使用了*零拷贝*缓冲区，以避免显式地将数据复制到GPU或从GPU复制数据，这一操作有可能加速广泛应用程序的执行。通过使用支持库进行线程管理，我们从同一个应用程序操作多个GPU，使得我们的点积计算可以在多个设备之间进行。最后，我们看到了如何通过将内存分配为*可移植*固定内存，使多个GPU共享固定内存分配。我们的最后一个例子使用了可移植固定内存、多个GPU和零拷贝缓冲区，以展示我们在[第5章](ch05.html#ch05)开始探索的点积的加速版本。随着多设备系统的日益普及，这些技术应能帮助你充分利用目标平台的计算能力。
