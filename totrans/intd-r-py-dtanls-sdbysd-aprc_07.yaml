- en: Chapter 4 Numpy ndarrays Versus R’s matrix and array Types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://randpythonbook.netlify.app/numpy-ndarrays-versus-rs-matrix-and-array-types](https://randpythonbook.netlify.app/numpy-ndarrays-versus-rs-matrix-and-array-types)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sometimes you want a collection of elements that are *all the same type*, but
    you want to store them in a two- or three-dimensional structure. For instance,
    say you need to use matrix multiplication for some linear regression software
    you’re writing, or that you need to use tensors for a computer vision project
    you’re working on.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Numpy `ndarray`s In Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Python, you could still use arrays for these kinds of tasks. You will be
    pleased to learn that the Numpy `array`s we discussed earlier are a special case
    of [Numpy’s N-dimensional arrays](https://numpy.org/doc/stable/reference/arrays.ndarray.html).
    Each array will come with an enormous amount of [methods](https://numpy.org/doc/stable/reference/arrays.ndarray.html#array-methods)
    and [attributes](https://numpy.org/doc/stable/reference/arrays.ndarray.html#array-attributes)
    (more on object-oriented program in chapter [14](/an-introduction-to-object-oriented-programming#an-introduction-to-object-oriented-programming))
    attached to it. A few are demonstrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Matrix and elementwise multiplication is often useful, too.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I should mention that there is also a `matrix` type in Numpy; however, this
    is not described in this text because it is preferable to work with Numpy `array`s
    (Albon [2018](#ref-ml_with_python_cookbook)).
  prefs: []
  type: TYPE_NORMAL
- en: In both R and Python, there are `matrix` types and `array` types. In R, it is
    more common to work with `matrix`s than `array`s, and the opposite is true in
    Python!
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 The `matrix` and `array` classes in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Python, adding a dimension to your “container” is simple. You keep using
    Numpy arrays, and you just change the `.shape` attribute (perhaps with a call
    to `.reshape()` or something similar). In R, there is a stronger distinction between
    1-,2-, and 3-dimensional containers. Each has its own class. 2-dimensional containers
    that store objects of the same type are of the `matrix` class. Containers with
    3 or more dimensions are of the `array` class[⁷](#fn7). In this section, I will
    provide a quick introduction to using these two classes. For more information,
    see chapter 3 of (Matloff [2011](#ref-matloff_r_book)).
  prefs: []
  type: TYPE_NORMAL
- en: Just like `vector`s, `matrix` objects do not necessarily have to be used to
    perform matrix arithmetic. Yes, they require all the elements are of the same
    type, but it doesn’t really make sense to “multiply” `matrix` objects that hold
    onto `character`s.
  prefs: []
  type: TYPE_NORMAL
- en: I usually create `matrix` objects with the `matrix()` function or the `as.matrix()`
    function. `matrix()` is to be preferred in my opinion. The first argument is explicitly
    a `vector` of all the flattened data that you want in your `matrix`. On the other
    hand, `as.matrix()` is more flexible; it takes in a variety of R objects (e.g. `data.frame`s),
    and tries to figure out what to do with them on a case-by-case basis. In other
    words, `as.matrix()` is a *generic function*. More information about generic functions
    is provided in [14.2.2](/an-introduction-to-object-oriented-programming#using-s3-objects).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other things to remember with `matrix()`: `byrow=` is `FALSE` by default,
    and you will also need to specify either `ncol=` and/or `nrow=` if you want anything
    that isn’t a 1-column `matrix`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`array()` is used to create `array` objects. This type is used less than the
    `matrix` type, but this doesn’t mean you should avoid learning about it. This
    is mostly a reflection of what kind of data sets people prefer to work with, and
    the fact that matrix algebra is generally better understood than tensor algebra.
    You won’t be able to avoid 3-d data sets (3-dimensions, not a 3-column `matrix`)
    forever, though, particularly if you’re working in an area such as neuroimaging
    or computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can matrix-multiply `matrix` objects together with the `%*%` operator. If
    you’re working on this, then the transpose operator (i.e. `t()`) comes in handy,
    too. You can still use element-wise (Hadamard) multiplication. This is defined
    with the more familiar multiplication operator `*`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes you need to access or modify individual elements of a `matrix` object.
    You can use the familiar `[` and `[<-` operators to do this. Here is a setting
    example. You don’t need to worry about coercion to different types here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here are some extraction examples. Notice that, if it can, `[` will coerce a
    `matrix` to `vector`. If you wish to avoid this, you can specify `drop=FALSE`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are other functions that operate on one or more `matrix` objects in more
    interesting ways, but much of this will be covered in future sections. For instance,
    we will describe how `apply()` works with `matrix`s in section [15](/an-introduction-to-functional-programming#an-introduction-to-functional-programming),
    and we will discuss combining `matrix` objects in different ways in section [12](/reshaping-and-combining-data-sets#reshaping-and-combining-data-sets).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.3.1 R Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the following data set. Let \(N = 20\) be the number of rows. For \(i=1,\ldots,N\),
    define \(\mathbf{x}_i \in \mathbb{R}^4\) as the data in row \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For the following problems, make sure to only use the transpose function `t()`,
    matrix multiplication (i.e. `%*%`), and scalar multiplication/division. You may
    use other functions in interactive mode to check your work, but please do not
    use them in your submission.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the sample mean \(\bar{\mathbf{x}} = \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i\).
    Check your work with `colMeans()`, **but don’t use that function in your submitted
    code.** Assign it to the variable `xbar`. Make sure it is a \(4 \times 1\) `matrix`
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the \(4 \times 4\) sample covariance of the following data. Call the
    variable `mySampCov`, and make sure it is also a `matrix` object. **You can check
    your work with `cov()`, but don’t use it in your submitted code.** A formula for
    the sample covariance is \[\begin{equation} \frac{1}{N-1} \sum_{i=1}^N (\mathbf{x}_i
    - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^\intercal \end{equation}\]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `matrix` called `P` that has one hundred rows, one hundred columns,
    all of its elements nonnegative, \(1/10\) on every diagonal element, and all rows
    summing to one. This matrix is called **stochastic** and it describes how a Markov
    chain moves randomly through time.
  prefs: []
  type: TYPE_NORMAL
- en: Create a `matrix` called `X` that has one thousand rows, four columns, has every
    element set to either \(0\) or \(1\), has its first column set to all \(1\)s,
    has the second column set to \(1\) in the second \(250\) elements and \(0\) elsewhere,
    has the third column set to \(1\) in the third \(250\) spots and \(0\) elsewhere,
    and has the fourth column set to \(1\) in the last \(250\) spots and \(0\) elsewhere.
    In other words, it looks something like
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{equation} \begin{bmatrix} \mathbf{1}_{250} & \mathbf{0}_{250} & \mathbf{0}_{250}
    & \mathbf{0}_{250} \\ \mathbf{1}_{250} & \mathbf{1}_{250} & \mathbf{0}_{250} &
    \mathbf{0}_{250} \\ \mathbf{1}_{250} & \mathbf{0}_{250} & \mathbf{1}_{250} & \mathbf{0}_{250}
    \\ \mathbf{1}_{250} & \mathbf{0}_{250} & \mathbf{0}_{250} & \mathbf{1}_{250} \\
    \end{bmatrix} \end{equation}\] where \(\mathbf{1}_{250}\) and \(\mathbf{0}_{250}\)
    are length \(250\) column vectors with all of their elements set to \(1\) or \(0\),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the **projection (or hat) matrix** \(\mathbf{H} := \mathbf{X}\left(\mathbf{X}^\intercal
    \mathbf{X}\right)^{-1} \mathbf{X}^\intercal\). Make it a `matrix` and call it
    `H`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An **exchangeable** covariance matrix for a random vector is a covariance matrix
    that has all the same variances, and all the same covariances. In other words,
    it has two unique elements: the diagonal elements should be the same, and the
    off-diagonals should be the same. In R, generate ten \(100 \times 100\) **exchangeable**
    covariance matrices, each with \(2\) as the variance, and have the possible covariances
    take values in the collection \(0,.01,.02, ..., .09.\) Store these ten covariance
    matrices in a three-dimensional array. The first index should be each matrix’s
    row index, the second should be the column index of each matrix, and the third
    index should be the “layer” or “slice” indicating which of the \(10\) matrices
    you have. Name this array `myCovMats`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In R, generate one hundred \(10 \times 10\) **exchangeable** covariance matrices,
    each with \(2\) as the variance, and have the possible covariances take values
    in the collection \(0,0.0009090909, ..., 0.0890909091, .09.\) Store these \(100\)
    covariance matrices in a three-dimensional array. The first index should be each
    matrix’s row index, the second should be the column index of each matrix, and
    the third index should be the “layer” or “slice” indicating which of the \(100\)
    matrices you have. Name this array `myCovMats2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4.3.2 Python Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let \(\mathbf{X}\) be an \(n \times 1\) random vector. It has a multivariate
    normal distribution with mean vector \(\mathbf{m}\) and positive definite covariance
    matrix \(\mathbf{C}\) if its probability density function can be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{equation} f(\mathbf{x}; \mathbf{m}, \mathbf{C}) = (2\pi)^{-n/2}\text{det}\left(
    \mathbf{C} \right)^{-1/2}\exp\left[- \frac{1}{2} (\mathbf{x}- \mathbf{m})^\intercal
    \mathbf{C}^{-1} (\mathbf{x}- \mathbf{m}) \right] \end{equation}\]
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating this density should be done with care. There is no one function that
    is optimal for all situations. Here are a couple quick things to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Inverting very large matrices with either [`np.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)
    or [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)
    becomes very slow if the covariance matrix is high-dimensional. If you have special
    assumptions about the structure of the covariance matrix, use it! Also, it’s a
    good idea to be aware of what happens when you try to invert noninvertible matrices.
    For instance, can you rely on errors to be thrown, or will it return a bogus answer?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall from the last lab that exponentiating numbers close to \(-\infty\) risks
    numerical underflow. It’s better to prefer evaluating log densities (base \(e\),
    the natural logarithm). There are also [special functions that evaluate log determinants](https://numpy.org/doc/stable/reference/generated/numpy.linalg.slogdet.html)
    that are less likely to underflow/overflow, too!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complete the following problems. **Do not use pre-made functions such as [`scipy.stats.norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html)
    and [`scipy.stats.multivariate_normal`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html)
    in your submission, but you may use them to check your work. Use only “standard”
    functions and Numpy n-dimensional arrays.** Use the following definitions for
    \(\mathbf{x}\) and \(\mathbf{m}\):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let \(\mathbf{C} = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 10 & 0 \\ 0 & 0 & 10 \end{bmatrix}\).
    Evaluate and assign the log density to a `float`-like called `log_dens1`. Can
    you do this without defining a numpy array for \(\mathbf{C}\)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let \(\mathbf{C} = \begin{bmatrix} 10 & 0 & 0 \\ 0 & 11 & 0 \\ 0 & 0 & 12 \end{bmatrix}\).
    Evaluate and assign the log density to a `float`-like called `log_dens2`. Can
    you do this without defining a numpy array for \(\mathbf{C}\)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let \(\mathbf{C} = \begin{bmatrix} 10 & -.9 & -.9 \\ -.9 & 11 & -.9 \\ -.9 &
    -.9 & 12 \end{bmatrix}\). Evaluate and assign the log density to a `float`-like
    called `log_dens3`. Can you do this without defining a numpy array for \(\mathbf{C}\)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider this [wine data set](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)
    from (Cortez et al. [2009](#ref-wine_data)) hosted by (Dua and Graff [2017](#ref-uci_data)).
    Read it in with the following code. Note that you might need to use `os.chdir()`
    first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Create the **design matrix** (denoted mathematically by \(\mathbf{X}\)) by removing
    the `"quality"` column, and subtracting the column mean from each element. Call
    the variable `X`, and make sure that it is a Numpy `ndarray`, not a Pandas `DataFrame`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the **spectral decomposition** of \(\mathbf{X}^\intercal \mathbf{X}\).
    In other words, find “special” matrices[⁸](#fn8) \(\mathbf{V}\) and \(\boldsymbol{\Lambda}\)
    such that \(\mathbf{X}^\intercal \mathbf{X} = \mathbf{V} \boldsymbol{\Lambda}
    \mathbf{V}^\intercal\). Note that the *eigenvectors* are stored as columns in
    a matrix \(\mathbf{V} := \begin{bmatrix} \mathbf{V}_1 & \cdots & \mathbf{V}_{11}\end{bmatrix}\),
    and the scalar *eigenvalues* are stored as diagonal elements \(\boldsymbol{\Lambda}
    = \text{diag}(\lambda_1, \ldots, \lambda_{11})\). Store the eigenvectors in an
    `ndarray` called `eig_vecs`, and store the eigenvalues in a Numpy `array` called
    `eig_vals`. Hint: use [`np.linalg.eig()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).
    Also, if you’re rusty with your linear algebra, don’t worry too much about refreshing
    your memory about what eigenvectors and eigenvalues are.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the **singular value decomposition** of \(\mathbf{X}\). In other words,
    find “special”[⁹](#fn9) matrices \(\mathbf{U}\), \(\mathbf{\Sigma}\), and \(\mathbf{V}\)
    such that \(\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\intercal\). Use
    [`np.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html),
    and don’t worry too much about the mathematical details. These two decompositions
    are related. If you do it correctly, the two \(\mathbf{V}\) matrices should be
    the same, and the elements of \(\boldsymbol{\Sigma}\) should be the square roots
    of the elements of \(\boldsymbol{\Lambda}\). Store the eigenvectors as columns
    in an `ndarray` called `eig_vecs_v2`, and store the singular values (diagonal
    elements of \(\boldsymbol{\Sigma}\)) in a Numpy `array` called `sing_vals`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the **first principal component** vector, and call it `first_pc_v1`.
    The mathematical formula is \(\mathbf{X} \mathbf{U}_1\) where \(\mathbf{U}_1\)
    is the eigenvector associated with the largest eigenvalue \(\lambda_1\). This
    can be thought of as, in a sense, the most informative predictor that you can
    create by averaging together all other predictors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Albon, Chris. 2018\. *Machine Learning with Python Cookbook: Practical Solutions
    from Preprocessing to Deep Learning*. 1st ed. O’Reilly Media, Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cortez, Paulo, António Cerdeira, Fernando Almeida, Telmo Matos, and José Reis.
    2009\. “Modeling Wine Preferences by Data Mining from Physicochemical Properties.”
    *Decis. Support Syst.* 47 (4): 547–53\. [http://dblp.uni-trier.de/db/journals/dss/dss47.html#CortezCAMR09](http://dblp.uni-trier.de/db/journals/dss/dss47.html#CortezCAMR09).'
  prefs: []
  type: TYPE_NORMAL
- en: Dua, Dheeru, and Casey Graff. 2017\. “UCI Machine Learning Repository.” University
    of California, Irvine, School of Information; Computer Sciences. [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Matloff, Norman. 2011\. *The Art of R Programming: A Tour of Statistical Software
    Design*. 1st ed. USA: No Starch Press.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Technically, the distinction between all of these containers is more subtle.
    An `array` in R can have one, two or more dimensions, and it is just a vector
    which is stored with additional dimension attributes. Moreover, a 2-dimensional
    array is the same as a `matrix`.[↩](/numpy-ndarrays-versus-rs-matrix-and-array-types#fnref7)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do not worry too much about the properties of these matrices for this problem[↩](/numpy-ndarrays-versus-rs-matrix-and-array-types#fnref8)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, do not worry too much about the properties of these matrices for this
    problem.[↩](/numpy-ndarrays-versus-rs-matrix-and-array-types#fnref9)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
