- en: 5.4\. Spectral properties of the Laplacian matrix#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html](https://mmids-textbook.github.io/chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this section, we look at the spectral properties of the Laplacian of a graph.
  prefs: []
  type: TYPE_NORMAL
- en: '5.4.1\. Eigenvalues of the Laplacian matrix: first observations[#](#eigenvalues-of-the-laplacian-matrix-first-observations
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let \(G = (V, E)\) be a graph with \(n = |V|\) vertices. Two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- Since the Laplacian matrix \(L\) of \(G\) is symmetric, by the *Spectral
    Theorem*, it has a spectral decomposition
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{y}_i\)’s form an orthonormal basis of \(\mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: 2- Further, because \(L\) is positive semidefinite, the eigenvalues are nonnegative.
    By convention, we assume
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is the opposite order to we used in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Let \(G = (V, E)\) be a graph with \(n = |V|\) vertices and Laplacian
    matrix \(L\). The constant unit vector'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{y}_1 = \frac{1}{\sqrt{n}} (1, \ldots, 1) \]
  prefs: []
  type: TYPE_NORMAL
- en: is an eigenvector of \(L\) with eigenvalue \(0\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(B\) be an oriented incidence matrix of \(G\) recall that \(L
    = B B^T\). By construction \(B^T \mathbf{y}_1 = \mathbf{0}\) since each column
    of \(B\) has exactly one \(1\) and one \(-1\). So \(L \mathbf{y}_1 = B B^T \mathbf{y}_1
    = \mathbf{0}\) as claimed. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the constant vector may not be the only eigenvector with eigenvalue
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** One use of the spectral decomposition of the Laplacian
    matrix is in graph drawing\(\idx{graph drawing}\xdi\). We illustrate this next.
    Given a graph \(G = (V, E)\), it is not clear a priori how to draw it in the plane
    since the only information available are adjacencies of vertices. One approach
    is just to position the vertices at random. The function [`networkx.draw`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw.html)
    or [`networkx.draw_networkx`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html#networkx.drawing.nx_pylab.draw_networkx)
    can take as input different [graph layout](https://networkx.org/documentation/stable/reference/drawing.html#module-networkx.drawing.layout)
    functions that return an \(x\) and \(y\)-coordinate for each vertex.'
  prefs: []
  type: TYPE_NORMAL
- en: We will test this on a grid graph. We use [`networkx.grid_2d_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.lattice.grid_2d_graph.html)
    to construct such a graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: One layout approach is to choose random locations for the nodes. Specifically,
    for every node, a position is generated by choosing each coordinate uniformly
    at random on the interval \([0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e71c11388af95019112c93563223107aea5d4aa3b91dbab26b654c57b7ad303b.png](../Images/f62f7cee4755a1c33c92335ca3d78f91.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, this is hard to read.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to map the vertices to two eigenvectors, similarly to what
    we did for dimensionality reduction. The eigenvector associated to \(\mu_1\) is
    constant and therefore not useful for drawing. We try the next two. We use the
    Laplacian matrix. This is done using [`networkx.spectral_layout`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.layout.spectral_layout.html#networkx.drawing.layout.spectral_layout).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/d9f18189eb6a51676466d9a2ffe9aa688e95d0f15f8697b68ee0c7c9ae2ada45.png](../Images/0ecdf40ac77cc8a246dd3cb1077c885d.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, the outcome is provides a much more natural drawing of the graph,
    revealing its underlying structure as a grid. We will come back later to try to
    explain this, after we have developed further understanding of the spectral properties
    of the Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2\. Laplacian matrix and connectivity[#](#laplacian-matrix-and-connectivity
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we indicated before, the Laplacian matrix contains information about the
    connectedness of \(G\). We elaborate on a first concrete connection here. But
    first we will need a useful form of the Laplaican quadratic form \(\mathbf{x}^T
    L \mathbf{x}\) which enters in the variational charaterization of the eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Laplacian Quadratic Form)** \(\idx{Laplacian quadratic form lemma}\xdi\)
    Let \(G = (V, E)\) be a graph with \(n = |V|\) vertices and Laplacian matrix \(L\).
    We have the following formula for the Laplacian quadratic form'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L \mathbf{x} = \sum_{e = \{i,j\} \in E} (x_i - x_j)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: Here is an intuitive way of interpreting this lemma. If one thinks of \(\mathbf{x}
    = (x_1, \ldots, x_n) \in \mathbb{R}^n\) as a real-valued function over the vertices
    (i.e., it associates a real value \(x_i\) to vertex \(i\) for each \(i\)), then
    the Laplacian quadratic form measures how “smooth” the function is over the graph
    in the following sense. A small value of \(\mathbf{x}^T L \mathbf{x}\) indicates
    that adjacent vertices tend to get assigned close values.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(B\) be an oriented incidence matrix of \(G\). We have that \(L
    = B B^T\). Thus, for any \(\mathbf{x}\), we have \((B^T \mathbf{x})_k = x_v -
    x_u\) if the edge \(e_k = \{u, v\}\) is oriented as \((u,v)\) under \(B\). That
    implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L \mathbf{x} = \mathbf{x}^T B B^T \mathbf{x} = \|B^T \mathbf{x}\|^2
    = \sum_{e = \{i,j\} \in E} (x_i - x_j)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Since the latter is always nonnegative, it also implies that \(L\) is positive
    semidefinite. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to derive connectivity consequences. Recall that, for any graph
    \(G\), the Laplacian eigenvalue \(\mu_1 = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Laplacian and Connectivity)** \(\idx{Laplacian and connectivity
    lemma}\xdi\) If \(G\) is connected, then the Laplacian eigenvalue \(\mu_2 > 0\).
    \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(G = (V, E)\) with \(n = |V|\) and let \(L = \sum_{i=1}^n \mu_i
    \mathbf{y}_i \mathbf{y}_i^T\) be a spectral decomposition of its Laplacian \(L\)
    with \(0 = \mu_1 \leq \cdots \leq \mu_n\). Suppose by way of contradiction that
    \(\mu_2 = 0\). Any eigenvector \(\mathbf{y} = (y_{1}, \ldots, y_{n})\) with \(0\)
    eigenvalue satisfies \(L \mathbf{y} = \mathbf{0}\) by definition. By the *Laplacian
    Quadratic Form Lemma* then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \mathbf{y}^T L \mathbf{y} = \sum_{e = \{i, j\} \in E} (y_{i} - y_{j})^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 1- In order for this to hold, it must be that any two adjacent vertices \(i\)
    and \(j\) have \(y_{i} = y_{j}\). That is, \(\{i,j\} \in E\) implies \(y_i = y_j\).
  prefs: []
  type: TYPE_NORMAL
- en: 2- Furthermore, because \(G\) is connected, between any two of its vertices
    \(u\) and \(v\) - adjacent or not - there is a path \(u = w_0 \sim \cdots \sim
    w_k = v\) along which the \(y_{w}\)’s must be the same. Thus \(\mathbf{y}\) is
    a constant vector.
  prefs: []
  type: TYPE_NORMAL
- en: But that is a contradiction since the eigenvectors \(\mathbf{y}_1, \ldots, \mathbf{y}_n\)
    are in fact linearly independent, so that \(\mathbf{y}_1\) and \(\mathbf{y}_2\)
    cannot both be a constant vector. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The quantity \(\mu_2\) is sometimes referred to as the [algebraic connectivity](https://mathworld.wolfram.com/AlgebraicConnectivity.html)\(\idx{algebraic
    connectivity}\xdi\) of the graph. The corresponding eigenvector, \(\mathbf{y}_2\),
    is known as the [Fiedler vector](https://mathworld.wolfram.com/FiedlerVector.html)\(\idx{Fiedler
    vector}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: We state the following (more general) converse result without proof.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** If \(\mu_{k+1}\) is the smallest nonzero Laplacian eigenvalue of
    \(G\), then \(G\) has \(k\) connected components. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: We will be interested in more quantitative results of this type. Before proceeding,
    we start with a simple observation. By our proof of the *Spectral Theorem*, the
    largest eigenvalue \(\mu_n\) of the Laplacian matrix \(L\) is the solution to
    the optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_n = \max\{\langle \mathbf{x}, L \mathbf{x}\rangle:\|\mathbf{x}\| = 1\}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Such extremal characterization is useful in order to bound the eigenvalue \(\mu_n\),
    since any choice of \(\mathbf{x}\) with \(\|\mathbf{x}\| =1\) gives a lower bound
    through the quantity \(\langle \mathbf{x}, L \mathbf{x}\rangle\). That perspective
    will be key to our application to graph partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we give a simple consequence.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Laplacian and Maximum Degree)** \(\idx{Laplacian and maximum degree
    lemma}\xdi\) Let \(G = (V, E)\) be a graph with maximum degree \(\bar{\delta}\).
    Let \(\mu_n\) be the largest eigenvalue of its Laplacian matrix \(L\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bar{\delta}+1 \leq \mu_n \leq 2 \bar{\delta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* As explained before the statement of the lemma, for the lower
    bound it suffices to find a good test unit vector \(\mathbf{x}\) to plug into
    \(\langle \mathbf{x}, L \mathbf{x}\rangle\). A clever choice does the trick.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We start with the lower bound. Let \(u \in V\) be a vertex with degree
    \(\bar{\delta}\). Let \(\mathbf{z}\) be the vector with entries'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} z_i = \begin{cases} \bar{\delta} & \text{if $i = u$}\\ -1 &
    \text{if $\{i,u\} \in E$}\\ 0 & \text{o.w.} \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and let \(\mathbf{x}\) be the unit vector \(\mathbf{z}/\|\mathbf{z}\|\). By
    definition of the degree of \(u\), \(\|\mathbf{z}\|^2 = \bar{\delta}^2 + \bar{\delta}(-1)^2
    = \bar{\delta}(\bar{\delta}+1)\). Using the *Laplacian Quadratic Form Lemma*,
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \langle \mathbf{z}, L \mathbf{z}\rangle = \sum_{e = \{i, j\} \in E} (z_i
    - z_j)^2 \geq \sum_{i: \{i, u\} \in E} (z_i - z_u)^2 = \sum_{i: \{i, u\} \in E}
    (-1 - \bar{\delta})^2 = \bar{\delta} (\bar{\delta}+1)^2 \]'
  prefs: []
  type: TYPE_NORMAL
- en: where we restricted the sum to those edges incident with \(u\) and used the
    fact that all terms in the sum are nonnegative. Finally
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{x}, L \mathbf{x}\rangle = \left\langle \frac{\mathbf{z}}{\|\mathbf{z}\|},
    L \frac{\mathbf{z}}{\|\mathbf{z}\|}\right\rangle = \frac{1}{\|\mathbf{z}\|^2}
    \langle \mathbf{z}, L \mathbf{z}\rangle = \frac{\bar{\delta} (\bar{\delta}+1)^2}{\bar{\delta}(\bar{\delta}+1)}
    = \bar{\delta}+1 \]
  prefs: []
  type: TYPE_NORMAL
- en: so that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_n = \max\{\langle \mathbf{x}', L \mathbf{x}'\rangle:\|\mathbf{x}'\| =
    1\} \geq \langle \mathbf{x}, L \mathbf{x}\rangle = \bar{\delta}+1 \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: We proceed with the lower bound. For any unit vector \(\mathbf{x}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \langle \mathbf{x}, L \mathbf{x}\rangle &= \sum_{i,j} L_{ij}
    x_i x_j\\ &\leq \sum_{i,j} |L_{ij}| |x_i| |x_j|\\ &= \sum_{i,j} (D_{ij} + A_{ij})
    |x_i| |x_j|\\ &= \sum_{i} \delta(i) \,x_i^2 + \sum_{i,j} A_{ij} |x_i| |x_j|. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Cauchy-Schwarz inequality*, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq \bar{\delta} + \left(\sum_{i,j} A_{ij} x_i^2\right)^{1/2}
    \left(\sum_{i,j} A_{ij} x_j^2\right)^{1/2}\\ &\leq \bar{\delta} + \left( \bar{\delta}
    \sum_{i} x_i^2\right)^{1/2} \left(\bar{\delta} \sum_{j} x_j^2\right)^{1/2}\\ &\leq
    2\bar{\delta}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We construct a graph with two connected components and
    check the results above. We work directly with the adjacency matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note the block structure.
  prefs: []
  type: TYPE_NORMAL
- en: The degrees can be obtained by summing the rows of the adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Observe that (up to numerical error) there are two \(0\) eigenvalues and that
    the largest eigenvalue is greater or equal than the maximum degree plus one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the Laplacian matrix, one can also use the function [`networkx.laplacian_matrix`](https://networkx.org/documentation/stable/reference/generated/networkx.linalg.laplacianmatrix.laplacian_matrix.html).
    For example, the Laplacian of the Petersen graph is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3\. Variational characterization of second Laplacian eigenvalue[#](#variational-characterization-of-second-laplacian-eigenvalue
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The definition \(A \mathbf{x} = \lambda \mathbf{x}\) is perhaps not the best
    way to understand why the eigenvectors of the Laplacian matrix are useful. Instead
    the following application of the *Courant-Fischer theorem*\(\idx{Courant-Fischer
    Theorem}\xdi\) provides much insight, as we will see in the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Variational Characterization of \(\mu_2\))** \(\idx{variational
    characterization of the algebraic connectivity}\xdi\) Let \(G = (V, E)\) be a
    graph with \(n = |V|\) vertices. Assume the Laplacian \(L\) of \(G\) has spectral
    decomposition \(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\) with \(0
    = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\) and \(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\).
    Then'
  prefs: []
  type: TYPE_NORMAL
- en: '\[\begin{align*} \mu_2 = \min\left\{ \sum_{\{i, j\} \in E}(x_i - x_j)^2 \,:
    \,\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{i=1}^n x_i = 0, \sum_{j
    = 1}^n x_j^2=1 \right\}. \end{align*}\]'
  prefs: []
  type: TYPE_NORMAL
- en: Taking \(\mathbf{x} = \mathbf{y}_2\) achieves this minimum. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Courant-Fischer Theorem*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u}),
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathcal{V}_{n-1} = \mathrm{span}(\mathbf{y}_2, \ldots, \mathbf{y}_n)
    = \mathrm{span}(\mathbf{y}_1)^\perp\). Observe that, because we reverse the order
    of the eigenvalues compared to the convention used in the *Courant-Fischer theorem*,
    we must adapt the definition of \(\mathcal{V}_{n-1}\) slightly. Moreover we know
    that \(\mathcal{R}_L(\mathbf{y}_2) = \mu_2\). We make a simple transformation
    of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: We claim that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1,
    \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}. \qquad (*) \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, if \(\mathbf{u} \in \mathrm{span}(\mathbf{y}_1)^\perp\) has unit norm,
    i.e., \(\|\mathbf{u}\| = 1\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{R}_L(\mathbf{u}) = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle
    \mathbf{u},\mathbf{u}\rangle} = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
    = \langle \mathbf{u}, L \mathbf{u}\rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u})
    \leq \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1,
    \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: To prove the other direction, for any \(\mathbf{u} \neq \mathbf{0}\), we can
    normalize it by defining \(\mathbf{x} = \mathbf{u}/\|\mathbf{u}\|\) and we note
    that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{R}_L(\mathbf{u}) = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle
    \mathbf{u},\mathbf{u}\rangle} = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
    = \left\langle \frac{\mathbf{u}}{\|\mathbf{u}\|}, L \frac{\mathbf{u}}{\|\mathbf{u}\|}\right\rangle
    = \langle \mathbf{x}, L \mathbf{x}\rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover \(\langle \mathbf{u}, \mathbf{y}_1\rangle = 0\) if only if \(\langle
    \mathbf{x}, \mathbf{y}_1\rangle = 0\). That establishes \((*)\), since any objective
    value achieved in the original formulation can be achieved in the new one.
  prefs: []
  type: TYPE_NORMAL
- en: Using that \(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\), the condition
    \(\langle \mathbf{x}, \mathbf{y}_1 \rangle = 0\), i.e., \(\sum_{i=1}^n (x_i/\sqrt{n})
    = 0\), is equivalent to \(\sum_{i=1}^n x_i = 0\). Similary, the condition \(\|\mathbf{x}\|=1\)
    is equivalent, after squaring each side, to \(\sum_{j=1}^n x_j^2 = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the claim follows from the *Laplacian Quadratic Form Lemma*. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: One application of this extremal characterization is the graph drawing heuristic
    we described previously. Consider the entries of the second Laplacian eigenvector
    \(\mathbf{y}_2\). Its entries are centered around \(0\) by the condition \(\langle
    \mathbf{y}_1, \mathbf{y}_2\rangle = 0\). Because it minimizes the following quantity
    over all centered unit vectors,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\{i, j\} \in E} (x_i - x_j)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: the eigenvector \(\mathbf{y}_2\) tends to assign similar coordinates to adjacent
    vertices. A similar reasoning applies to the third Laplacian eigenvector, which
    in addition is orthogonal to the second one. So coordinates based on the second
    and third Laplacian eigenvectors should be expected to position adjacent vertices
    close-by and hence minimizing the need for long-range edges in the visualization.
    In particular, it reveals some of the underlying Euclidean geometry of the graph,
    as the next example shows.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** This is perhaps easiest to see on a path graph. Recall
    that NetworkX numbers vertices \(0,\ldots,n-1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We plot the second Laplacian eigenvector (i.e., the eigenvector of the Laplacian
    matrix corresponding to the second smallest eigenvalue). We use [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)
    to find the index of the second smallest eigenvalue. Because indices start at
    `0`, we want entry `1` of the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/c346aff858ca3853432ca68560966ff40a69ed45112737ae2d576de9ef3cebb7.png](../Images/aa1979a98f2c16cedc4afdcc1b494809.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Two-Component Graph)** Let \(G=(V,E)\) be a graph with two
    connected components \(\emptyset \neq V_1, V_2 \subseteq V\). By the properties
    of connected components, we have \(V_1 \cap V_2 = \emptyset\) and \(V_1 \cup V_2
    = V\). Assume the Laplacian \(L\) of \(G\) has spectral decomposition \(L = \sum_{i=1}^n
    \mu_i \mathbf{y}_i \mathbf{y}_i^T\) with \(0 = \mu_1 \leq \mu_2 \leq \cdots \leq
    \mu_n\) and \(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\). We claimed earlier
    that for such a graph \(\mu_2 = 0\). We prove this here using the *Variational
    Characterization of \(\mu_2\)*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min\left\{ \sum_{\{u, v\} \in E} (x_u - x_v)^2 \,:\, \mathbf{x}
    = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{u=1}^n x_u = 0, \sum_{u = 1}^n x_u^2=1
    \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Based on this characterization, it suffices to find a vector \(\mathbf{x}\)
    satisfying \(\sum_{u=1}^n x_u = 0\) and \(\sum_{u = 1}^n x_u^2=1\) such that \(\sum_{\{u,
    v\} \in E} (x_u - x_v)^2 = 0\). Indeed, since \(\mu_2 \geq 0\) and any such \(\mathbf{x}\)
    gives an upper bound on \(\mu_2\), we then necessarily have that \(\mu_2 = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: For \(\sum_{\{u, v\} \in E} (x_u - x_v)^2\) to be \(0\), one might be tempted
    to take a constant vector \(\mathbf{x}\). But then we could not satisfy \(\sum_{u=1}^n
    x_u = 0\) and \(\sum_{u = 1}^n x_u^2=1\) simultaneously. Instead, we modify this
    guess slightly. Because the graph has two connected components, there is no edge
    between \(V_1\) and \(V_2\). Hence we can assign a different value to each component
    and still get \(\sum_{\{u, v\} \in E} (x_u - x_v)^2 = 0\). So we look for a vector
    \(\mathbf{x} = (x_1, \ldots, x_n)\) of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} x_u = \begin{cases} \alpha, & \text{if $u \in V_1$,}\\ \beta,
    & \text{if $u \in V_2$.} \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: To satisfy the constraints on \(\mathbf{x}\), we require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{u=1}^n x_u = \sum_{u \in V_1} \alpha + \sum_{u \in V_2} \beta = |V_1|
    \alpha + |V_2| \beta = 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{u=1}^n x_u^2 = \sum_{u \in V_1} \alpha^2 + \sum_{u \in V_2} \beta^2
    = |V_1| \alpha^2 + |V_2| \beta^2 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Replacing the first equation in the second one, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ |V_1| \left(\frac{-|V_2|\beta}{|V_1|}\right)^2 + |V_2| \beta^2 = \frac{|V_2|^2
    \beta^2}{|V_1|} + |V_2| \beta^2 = 1, \]
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: \[ \beta^2 = \frac{|V_1|}{|V_2|(|V_2| + |V_1|)} = \frac{|V_1|}{n |V_2|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Take
  prefs: []
  type: TYPE_NORMAL
- en: \[ \beta = - \sqrt{\frac{|V_1|}{n |V_2|}}, \qquad \alpha = \frac{-|V_2|\beta}{|V_1|}
    = \sqrt{\frac{|V_2|}{n |V_1|}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The vector \(\mathbf{x}\) we constructed is in fact an eigenvector of \(L\).
    Indeed, let \(B\) be an oriented incidence matrix of \(G\). Then, for \(e_k =
    \{u,v\}\), \((B^T \mathbf{x})_k\) is either \(x_u - x_v\) or \(x_v - x_u\). In
    both cases, that is \(0\). So \(L \mathbf{x} = B B^T \mathbf{x} = \mathbf{0}\),
    that is, \(\mathbf{x}\) is an eigenvector of \(L\) with eigenvalue \(0\).
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that \(\mu_2 = 0\) when \(G\) has two connected components. A
    slight modification of this argument shows that \(\mu_2 = 0\) whenever \(G\) is
    not connected. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is NOT a property of the Laplacian matrix \(L\)
    of a graph \(G\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(L\) is symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: b) \(L\) is positive semidefinite.
  prefs: []
  type: TYPE_NORMAL
- en: c) The constant unit vector \(\frac{1}{\sqrt{n}}(1,\ldots,1)\) is an eigenvector
    of \(L\) with eigenvalue 0.
  prefs: []
  type: TYPE_NORMAL
- en: d) \(L\) is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which vector is known as the Fiedler vector?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The eigenvector corresponding to the largest eigenvalue of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: b) The eigenvector corresponding to the smallest eigenvalue of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: c) The eigenvector corresponding to the second smallest eigenvalue of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: d) The eigenvector corresponding to the average of all eigenvalues of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** For a connected graph \(G\), which of the following statements about
    the second smallest eigenvalue \(\mu_2\) of its Laplacian matrix is true?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mu_2 = 0\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mu_2 < 0\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mu_2 > 0\)
  prefs: []
  type: TYPE_NORMAL
- en: d) The value of \(\mu_2\) cannot be determined without additional information.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** The Laplacian quadratic form \(\mathbf{x}^T L \mathbf{x}\) for a graph
    \(G\) with Laplacian matrix \(L\) can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L \mathbf{x} = \sum_{\{i,j\} \in E} (x_i - x_j)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: What does this quadratic form measure?
  prefs: []
  type: TYPE_NORMAL
- en: a) The average distance between vertices in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: b) The number of connected components in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: c) The “smoothness” of the function \(x\) over the graph.
  prefs: []
  type: TYPE_NORMAL
- en: d) The degree of each vertex in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** The Laplacian matrix \(L\) of a graph \(G\) can be decomposed as \(L
    = B B^T\), where \(B\) is an oriented incidence matrix. What does this decomposition
    imply about \(L\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(L\) is positive definite
  prefs: []
  type: TYPE_NORMAL
- en: b) \(L\) is symmetric and positive semidefinite
  prefs: []
  type: TYPE_NORMAL
- en: c) \(L\) is antisymmetric
  prefs: []
  type: TYPE_NORMAL
- en: d) \(L\) is a diagonal matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: d. Justification: The text states that “because \(L\) is positive
    semidefinite, the eigenvalues are nonnegative,” but it does not claim that \(L\)
    is positive definite.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: The text refers to the eigenvector corresponding
    to \(\mu_2\) (the second smallest eigenvalue) as the Fiedler vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text proves that “If \(G\) is connected,
    then the Laplacian eigenvalue \(\mu_2 > 0\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text states that “the Laplacian quadratic
    form measures how ‘smooth’ the function \(\mathbf{x}\) is over the graph in the
    following sense. A small value of \(\mathbf{x}^T L \mathbf{x}\) indicates that
    adjacent vertices tend to get assigned close values.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states, “Let \(B\) be an oriented
    incidence matrix of \(G\). By construction, \(L = B B^T\). This implies that \(L\)
    is symmetric and positive semidefinite.”'
  prefs: []
  type: TYPE_NORMAL
- en: '5.4.1\. Eigenvalues of the Laplacian matrix: first observations[#](#eigenvalues-of-the-laplacian-matrix-first-observations
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let \(G = (V, E)\) be a graph with \(n = |V|\) vertices. Two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- Since the Laplacian matrix \(L\) of \(G\) is symmetric, by the *Spectral
    Theorem*, it has a spectral decomposition
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{y}_i\)’s form an orthonormal basis of \(\mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: 2- Further, because \(L\) is positive semidefinite, the eigenvalues are nonnegative.
    By convention, we assume
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is the opposite order to we used in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Let \(G = (V, E)\) be a graph with \(n = |V|\) vertices and Laplacian
    matrix \(L\). The constant unit vector'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{y}_1 = \frac{1}{\sqrt{n}} (1, \ldots, 1) \]
  prefs: []
  type: TYPE_NORMAL
- en: is an eigenvector of \(L\) with eigenvalue \(0\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(B\) be an oriented incidence matrix of \(G\) recall that \(L
    = B B^T\). By construction \(B^T \mathbf{y}_1 = \mathbf{0}\) since each column
    of \(B\) has exactly one \(1\) and one \(-1\). So \(L \mathbf{y}_1 = B B^T \mathbf{y}_1
    = \mathbf{0}\) as claimed. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the constant vector may not be the only eigenvector with eigenvalue
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** One use of the spectral decomposition of the Laplacian
    matrix is in graph drawing\(\idx{graph drawing}\xdi\). We illustrate this next.
    Given a graph \(G = (V, E)\), it is not clear a priori how to draw it in the plane
    since the only information available are adjacencies of vertices. One approach
    is just to position the vertices at random. The function [`networkx.draw`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw.html)
    or [`networkx.draw_networkx`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html#networkx.drawing.nx_pylab.draw_networkx)
    can take as input different [graph layout](https://networkx.org/documentation/stable/reference/drawing.html#module-networkx.drawing.layout)
    functions that return an \(x\) and \(y\)-coordinate for each vertex.'
  prefs: []
  type: TYPE_NORMAL
- en: We will test this on a grid graph. We use [`networkx.grid_2d_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.lattice.grid_2d_graph.html)
    to construct such a graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: One layout approach is to choose random locations for the nodes. Specifically,
    for every node, a position is generated by choosing each coordinate uniformly
    at random on the interval \([0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e71c11388af95019112c93563223107aea5d4aa3b91dbab26b654c57b7ad303b.png](../Images/f62f7cee4755a1c33c92335ca3d78f91.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, this is hard to read.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to map the vertices to two eigenvectors, similarly to what
    we did for dimensionality reduction. The eigenvector associated to \(\mu_1\) is
    constant and therefore not useful for drawing. We try the next two. We use the
    Laplacian matrix. This is done using [`networkx.spectral_layout`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.layout.spectral_layout.html#networkx.drawing.layout.spectral_layout).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/d9f18189eb6a51676466d9a2ffe9aa688e95d0f15f8697b68ee0c7c9ae2ada45.png](../Images/0ecdf40ac77cc8a246dd3cb1077c885d.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, the outcome is provides a much more natural drawing of the graph,
    revealing its underlying structure as a grid. We will come back later to try to
    explain this, after we have developed further understanding of the spectral properties
    of the Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2\. Laplacian matrix and connectivity[#](#laplacian-matrix-and-connectivity
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we indicated before, the Laplacian matrix contains information about the
    connectedness of \(G\). We elaborate on a first concrete connection here. But
    first we will need a useful form of the Laplaican quadratic form \(\mathbf{x}^T
    L \mathbf{x}\) which enters in the variational charaterization of the eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Laplacian Quadratic Form)** \(\idx{Laplacian quadratic form lemma}\xdi\)
    Let \(G = (V, E)\) be a graph with \(n = |V|\) vertices and Laplacian matrix \(L\).
    We have the following formula for the Laplacian quadratic form'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L \mathbf{x} = \sum_{e = \{i,j\} \in E} (x_i - x_j)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: Here is an intuitive way of interpreting this lemma. If one thinks of \(\mathbf{x}
    = (x_1, \ldots, x_n) \in \mathbb{R}^n\) as a real-valued function over the vertices
    (i.e., it associates a real value \(x_i\) to vertex \(i\) for each \(i\)), then
    the Laplacian quadratic form measures how “smooth” the function is over the graph
    in the following sense. A small value of \(\mathbf{x}^T L \mathbf{x}\) indicates
    that adjacent vertices tend to get assigned close values.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(B\) be an oriented incidence matrix of \(G\). We have that \(L
    = B B^T\). Thus, for any \(\mathbf{x}\), we have \((B^T \mathbf{x})_k = x_v -
    x_u\) if the edge \(e_k = \{u, v\}\) is oriented as \((u,v)\) under \(B\). That
    implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L \mathbf{x} = \mathbf{x}^T B B^T \mathbf{x} = \|B^T \mathbf{x}\|^2
    = \sum_{e = \{i,j\} \in E} (x_i - x_j)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Since the latter is always nonnegative, it also implies that \(L\) is positive
    semidefinite. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to derive connectivity consequences. Recall that, for any graph
    \(G\), the Laplacian eigenvalue \(\mu_1 = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Laplacian and Connectivity)** \(\idx{Laplacian and connectivity
    lemma}\xdi\) If \(G\) is connected, then the Laplacian eigenvalue \(\mu_2 > 0\).
    \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(G = (V, E)\) with \(n = |V|\) and let \(L = \sum_{i=1}^n \mu_i
    \mathbf{y}_i \mathbf{y}_i^T\) be a spectral decomposition of its Laplacian \(L\)
    with \(0 = \mu_1 \leq \cdots \leq \mu_n\). Suppose by way of contradiction that
    \(\mu_2 = 0\). Any eigenvector \(\mathbf{y} = (y_{1}, \ldots, y_{n})\) with \(0\)
    eigenvalue satisfies \(L \mathbf{y} = \mathbf{0}\) by definition. By the *Laplacian
    Quadratic Form Lemma* then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \mathbf{y}^T L \mathbf{y} = \sum_{e = \{i, j\} \in E} (y_{i} - y_{j})^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 1- In order for this to hold, it must be that any two adjacent vertices \(i\)
    and \(j\) have \(y_{i} = y_{j}\). That is, \(\{i,j\} \in E\) implies \(y_i = y_j\).
  prefs: []
  type: TYPE_NORMAL
- en: 2- Furthermore, because \(G\) is connected, between any two of its vertices
    \(u\) and \(v\) - adjacent or not - there is a path \(u = w_0 \sim \cdots \sim
    w_k = v\) along which the \(y_{w}\)’s must be the same. Thus \(\mathbf{y}\) is
    a constant vector.
  prefs: []
  type: TYPE_NORMAL
- en: But that is a contradiction since the eigenvectors \(\mathbf{y}_1, \ldots, \mathbf{y}_n\)
    are in fact linearly independent, so that \(\mathbf{y}_1\) and \(\mathbf{y}_2\)
    cannot both be a constant vector. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The quantity \(\mu_2\) is sometimes referred to as the [algebraic connectivity](https://mathworld.wolfram.com/AlgebraicConnectivity.html)\(\idx{algebraic
    connectivity}\xdi\) of the graph. The corresponding eigenvector, \(\mathbf{y}_2\),
    is known as the [Fiedler vector](https://mathworld.wolfram.com/FiedlerVector.html)\(\idx{Fiedler
    vector}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: We state the following (more general) converse result without proof.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** If \(\mu_{k+1}\) is the smallest nonzero Laplacian eigenvalue of
    \(G\), then \(G\) has \(k\) connected components. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: We will be interested in more quantitative results of this type. Before proceeding,
    we start with a simple observation. By our proof of the *Spectral Theorem*, the
    largest eigenvalue \(\mu_n\) of the Laplacian matrix \(L\) is the solution to
    the optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_n = \max\{\langle \mathbf{x}, L \mathbf{x}\rangle:\|\mathbf{x}\| = 1\}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Such extremal characterization is useful in order to bound the eigenvalue \(\mu_n\),
    since any choice of \(\mathbf{x}\) with \(\|\mathbf{x}\| =1\) gives a lower bound
    through the quantity \(\langle \mathbf{x}, L \mathbf{x}\rangle\). That perspective
    will be key to our application to graph partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we give a simple consequence.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Laplacian and Maximum Degree)** \(\idx{Laplacian and maximum degree
    lemma}\xdi\) Let \(G = (V, E)\) be a graph with maximum degree \(\bar{\delta}\).
    Let \(\mu_n\) be the largest eigenvalue of its Laplacian matrix \(L\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bar{\delta}+1 \leq \mu_n \leq 2 \bar{\delta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* As explained before the statement of the lemma, for the lower
    bound it suffices to find a good test unit vector \(\mathbf{x}\) to plug into
    \(\langle \mathbf{x}, L \mathbf{x}\rangle\). A clever choice does the trick.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We start with the lower bound. Let \(u \in V\) be a vertex with degree
    \(\bar{\delta}\). Let \(\mathbf{z}\) be the vector with entries'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} z_i = \begin{cases} \bar{\delta} & \text{if $i = u$}\\ -1 &
    \text{if $\{i,u\} \in E$}\\ 0 & \text{o.w.} \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and let \(\mathbf{x}\) be the unit vector \(\mathbf{z}/\|\mathbf{z}\|\). By
    definition of the degree of \(u\), \(\|\mathbf{z}\|^2 = \bar{\delta}^2 + \bar{\delta}(-1)^2
    = \bar{\delta}(\bar{\delta}+1)\). Using the *Laplacian Quadratic Form Lemma*,
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \langle \mathbf{z}, L \mathbf{z}\rangle = \sum_{e = \{i, j\} \in E} (z_i
    - z_j)^2 \geq \sum_{i: \{i, u\} \in E} (z_i - z_u)^2 = \sum_{i: \{i, u\} \in E}
    (-1 - \bar{\delta})^2 = \bar{\delta} (\bar{\delta}+1)^2 \]'
  prefs: []
  type: TYPE_NORMAL
- en: where we restricted the sum to those edges incident with \(u\) and used the
    fact that all terms in the sum are nonnegative. Finally
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{x}, L \mathbf{x}\rangle = \left\langle \frac{\mathbf{z}}{\|\mathbf{z}\|},
    L \frac{\mathbf{z}}{\|\mathbf{z}\|}\right\rangle = \frac{1}{\|\mathbf{z}\|^2}
    \langle \mathbf{z}, L \mathbf{z}\rangle = \frac{\bar{\delta} (\bar{\delta}+1)^2}{\bar{\delta}(\bar{\delta}+1)}
    = \bar{\delta}+1 \]
  prefs: []
  type: TYPE_NORMAL
- en: so that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_n = \max\{\langle \mathbf{x}', L \mathbf{x}'\rangle:\|\mathbf{x}'\| =
    1\} \geq \langle \mathbf{x}, L \mathbf{x}\rangle = \bar{\delta}+1 \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: We proceed with the lower bound. For any unit vector \(\mathbf{x}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \langle \mathbf{x}, L \mathbf{x}\rangle &= \sum_{i,j} L_{ij}
    x_i x_j\\ &\leq \sum_{i,j} |L_{ij}| |x_i| |x_j|\\ &= \sum_{i,j} (D_{ij} + A_{ij})
    |x_i| |x_j|\\ &= \sum_{i} \delta(i) \,x_i^2 + \sum_{i,j} A_{ij} |x_i| |x_j|. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Cauchy-Schwarz inequality*, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq \bar{\delta} + \left(\sum_{i,j} A_{ij} x_i^2\right)^{1/2}
    \left(\sum_{i,j} A_{ij} x_j^2\right)^{1/2}\\ &\leq \bar{\delta} + \left( \bar{\delta}
    \sum_{i} x_i^2\right)^{1/2} \left(\bar{\delta} \sum_{j} x_j^2\right)^{1/2}\\ &\leq
    2\bar{\delta}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We construct a graph with two connected components and
    check the results above. We work directly with the adjacency matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note the block structure.
  prefs: []
  type: TYPE_NORMAL
- en: The degrees can be obtained by summing the rows of the adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Observe that (up to numerical error) there are two \(0\) eigenvalues and that
    the largest eigenvalue is greater or equal than the maximum degree plus one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the Laplacian matrix, one can also use the function [`networkx.laplacian_matrix`](https://networkx.org/documentation/stable/reference/generated/networkx.linalg.laplacianmatrix.laplacian_matrix.html).
    For example, the Laplacian of the Petersen graph is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3\. Variational characterization of second Laplacian eigenvalue[#](#variational-characterization-of-second-laplacian-eigenvalue
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The definition \(A \mathbf{x} = \lambda \mathbf{x}\) is perhaps not the best
    way to understand why the eigenvectors of the Laplacian matrix are useful. Instead
    the following application of the *Courant-Fischer theorem*\(\idx{Courant-Fischer
    Theorem}\xdi\) provides much insight, as we will see in the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Variational Characterization of \(\mu_2\))** \(\idx{variational
    characterization of the algebraic connectivity}\xdi\) Let \(G = (V, E)\) be a
    graph with \(n = |V|\) vertices. Assume the Laplacian \(L\) of \(G\) has spectral
    decomposition \(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\) with \(0
    = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\) and \(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\).
    Then'
  prefs: []
  type: TYPE_NORMAL
- en: '\[\begin{align*} \mu_2 = \min\left\{ \sum_{\{i, j\} \in E}(x_i - x_j)^2 \,:
    \,\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{i=1}^n x_i = 0, \sum_{j
    = 1}^n x_j^2=1 \right\}. \end{align*}\]'
  prefs: []
  type: TYPE_NORMAL
- en: Taking \(\mathbf{x} = \mathbf{y}_2\) achieves this minimum. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Courant-Fischer Theorem*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u}),
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathcal{V}_{n-1} = \mathrm{span}(\mathbf{y}_2, \ldots, \mathbf{y}_n)
    = \mathrm{span}(\mathbf{y}_1)^\perp\). Observe that, because we reverse the order
    of the eigenvalues compared to the convention used in the *Courant-Fischer theorem*,
    we must adapt the definition of \(\mathcal{V}_{n-1}\) slightly. Moreover we know
    that \(\mathcal{R}_L(\mathbf{y}_2) = \mu_2\). We make a simple transformation
    of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: We claim that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1,
    \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}. \qquad (*) \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, if \(\mathbf{u} \in \mathrm{span}(\mathbf{y}_1)^\perp\) has unit norm,
    i.e., \(\|\mathbf{u}\| = 1\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{R}_L(\mathbf{u}) = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle
    \mathbf{u},\mathbf{u}\rangle} = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
    = \langle \mathbf{u}, L \mathbf{u}\rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u})
    \leq \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1,
    \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: To prove the other direction, for any \(\mathbf{u} \neq \mathbf{0}\), we can
    normalize it by defining \(\mathbf{x} = \mathbf{u}/\|\mathbf{u}\|\) and we note
    that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{R}_L(\mathbf{u}) = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle
    \mathbf{u},\mathbf{u}\rangle} = \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
    = \left\langle \frac{\mathbf{u}}{\|\mathbf{u}\|}, L \frac{\mathbf{u}}{\|\mathbf{u}\|}\right\rangle
    = \langle \mathbf{x}, L \mathbf{x}\rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover \(\langle \mathbf{u}, \mathbf{y}_1\rangle = 0\) if only if \(\langle
    \mathbf{x}, \mathbf{y}_1\rangle = 0\). That establishes \((*)\), since any objective
    value achieved in the original formulation can be achieved in the new one.
  prefs: []
  type: TYPE_NORMAL
- en: Using that \(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\), the condition
    \(\langle \mathbf{x}, \mathbf{y}_1 \rangle = 0\), i.e., \(\sum_{i=1}^n (x_i/\sqrt{n})
    = 0\), is equivalent to \(\sum_{i=1}^n x_i = 0\). Similary, the condition \(\|\mathbf{x}\|=1\)
    is equivalent, after squaring each side, to \(\sum_{j=1}^n x_j^2 = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the claim follows from the *Laplacian Quadratic Form Lemma*. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: One application of this extremal characterization is the graph drawing heuristic
    we described previously. Consider the entries of the second Laplacian eigenvector
    \(\mathbf{y}_2\). Its entries are centered around \(0\) by the condition \(\langle
    \mathbf{y}_1, \mathbf{y}_2\rangle = 0\). Because it minimizes the following quantity
    over all centered unit vectors,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\{i, j\} \in E} (x_i - x_j)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: the eigenvector \(\mathbf{y}_2\) tends to assign similar coordinates to adjacent
    vertices. A similar reasoning applies to the third Laplacian eigenvector, which
    in addition is orthogonal to the second one. So coordinates based on the second
    and third Laplacian eigenvectors should be expected to position adjacent vertices
    close-by and hence minimizing the need for long-range edges in the visualization.
    In particular, it reveals some of the underlying Euclidean geometry of the graph,
    as the next example shows.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** This is perhaps easiest to see on a path graph. Recall
    that NetworkX numbers vertices \(0,\ldots,n-1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We plot the second Laplacian eigenvector (i.e., the eigenvector of the Laplacian
    matrix corresponding to the second smallest eigenvalue). We use [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)
    to find the index of the second smallest eigenvalue. Because indices start at
    `0`, we want entry `1` of the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/c346aff858ca3853432ca68560966ff40a69ed45112737ae2d576de9ef3cebb7.png](../Images/aa1979a98f2c16cedc4afdcc1b494809.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Two-Component Graph)** Let \(G=(V,E)\) be a graph with two
    connected components \(\emptyset \neq V_1, V_2 \subseteq V\). By the properties
    of connected components, we have \(V_1 \cap V_2 = \emptyset\) and \(V_1 \cup V_2
    = V\). Assume the Laplacian \(L\) of \(G\) has spectral decomposition \(L = \sum_{i=1}^n
    \mu_i \mathbf{y}_i \mathbf{y}_i^T\) with \(0 = \mu_1 \leq \mu_2 \leq \cdots \leq
    \mu_n\) and \(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\). We claimed earlier
    that for such a graph \(\mu_2 = 0\). We prove this here using the *Variational
    Characterization of \(\mu_2\)*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min\left\{ \sum_{\{u, v\} \in E} (x_u - x_v)^2 \,:\, \mathbf{x}
    = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{u=1}^n x_u = 0, \sum_{u = 1}^n x_u^2=1
    \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Based on this characterization, it suffices to find a vector \(\mathbf{x}\)
    satisfying \(\sum_{u=1}^n x_u = 0\) and \(\sum_{u = 1}^n x_u^2=1\) such that \(\sum_{\{u,
    v\} \in E} (x_u - x_v)^2 = 0\). Indeed, since \(\mu_2 \geq 0\) and any such \(\mathbf{x}\)
    gives an upper bound on \(\mu_2\), we then necessarily have that \(\mu_2 = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: For \(\sum_{\{u, v\} \in E} (x_u - x_v)^2\) to be \(0\), one might be tempted
    to take a constant vector \(\mathbf{x}\). But then we could not satisfy \(\sum_{u=1}^n
    x_u = 0\) and \(\sum_{u = 1}^n x_u^2=1\) simultaneously. Instead, we modify this
    guess slightly. Because the graph has two connected components, there is no edge
    between \(V_1\) and \(V_2\). Hence we can assign a different value to each component
    and still get \(\sum_{\{u, v\} \in E} (x_u - x_v)^2 = 0\). So we look for a vector
    \(\mathbf{x} = (x_1, \ldots, x_n)\) of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} x_u = \begin{cases} \alpha, & \text{if $u \in V_1$,}\\ \beta,
    & \text{if $u \in V_2$.} \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: To satisfy the constraints on \(\mathbf{x}\), we require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{u=1}^n x_u = \sum_{u \in V_1} \alpha + \sum_{u \in V_2} \beta = |V_1|
    \alpha + |V_2| \beta = 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{u=1}^n x_u^2 = \sum_{u \in V_1} \alpha^2 + \sum_{u \in V_2} \beta^2
    = |V_1| \alpha^2 + |V_2| \beta^2 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Replacing the first equation in the second one, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ |V_1| \left(\frac{-|V_2|\beta}{|V_1|}\right)^2 + |V_2| \beta^2 = \frac{|V_2|^2
    \beta^2}{|V_1|} + |V_2| \beta^2 = 1, \]
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: \[ \beta^2 = \frac{|V_1|}{|V_2|(|V_2| + |V_1|)} = \frac{|V_1|}{n |V_2|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Take
  prefs: []
  type: TYPE_NORMAL
- en: \[ \beta = - \sqrt{\frac{|V_1|}{n |V_2|}}, \qquad \alpha = \frac{-|V_2|\beta}{|V_1|}
    = \sqrt{\frac{|V_2|}{n |V_1|}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The vector \(\mathbf{x}\) we constructed is in fact an eigenvector of \(L\).
    Indeed, let \(B\) be an oriented incidence matrix of \(G\). Then, for \(e_k =
    \{u,v\}\), \((B^T \mathbf{x})_k\) is either \(x_u - x_v\) or \(x_v - x_u\). In
    both cases, that is \(0\). So \(L \mathbf{x} = B B^T \mathbf{x} = \mathbf{0}\),
    that is, \(\mathbf{x}\) is an eigenvector of \(L\) with eigenvalue \(0\).
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that \(\mu_2 = 0\) when \(G\) has two connected components. A
    slight modification of this argument shows that \(\mu_2 = 0\) whenever \(G\) is
    not connected. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is NOT a property of the Laplacian matrix \(L\)
    of a graph \(G\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(L\) is symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: b) \(L\) is positive semidefinite.
  prefs: []
  type: TYPE_NORMAL
- en: c) The constant unit vector \(\frac{1}{\sqrt{n}}(1,\ldots,1)\) is an eigenvector
    of \(L\) with eigenvalue 0.
  prefs: []
  type: TYPE_NORMAL
- en: d) \(L\) is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which vector is known as the Fiedler vector?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The eigenvector corresponding to the largest eigenvalue of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: b) The eigenvector corresponding to the smallest eigenvalue of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: c) The eigenvector corresponding to the second smallest eigenvalue of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: d) The eigenvector corresponding to the average of all eigenvalues of the Laplacian
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** For a connected graph \(G\), which of the following statements about
    the second smallest eigenvalue \(\mu_2\) of its Laplacian matrix is true?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mu_2 = 0\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mu_2 < 0\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mu_2 > 0\)
  prefs: []
  type: TYPE_NORMAL
- en: d) The value of \(\mu_2\) cannot be determined without additional information.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** The Laplacian quadratic form \(\mathbf{x}^T L \mathbf{x}\) for a graph
    \(G\) with Laplacian matrix \(L\) can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L \mathbf{x} = \sum_{\{i,j\} \in E} (x_i - x_j)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: What does this quadratic form measure?
  prefs: []
  type: TYPE_NORMAL
- en: a) The average distance between vertices in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: b) The number of connected components in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: c) The “smoothness” of the function \(x\) over the graph.
  prefs: []
  type: TYPE_NORMAL
- en: d) The degree of each vertex in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** The Laplacian matrix \(L\) of a graph \(G\) can be decomposed as \(L
    = B B^T\), where \(B\) is an oriented incidence matrix. What does this decomposition
    imply about \(L\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(L\) is positive definite
  prefs: []
  type: TYPE_NORMAL
- en: b) \(L\) is symmetric and positive semidefinite
  prefs: []
  type: TYPE_NORMAL
- en: c) \(L\) is antisymmetric
  prefs: []
  type: TYPE_NORMAL
- en: d) \(L\) is a diagonal matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: d. Justification: The text states that “because \(L\) is positive
    semidefinite, the eigenvalues are nonnegative,” but it does not claim that \(L\)
    is positive definite.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: The text refers to the eigenvector corresponding
    to \(\mu_2\) (the second smallest eigenvalue) as the Fiedler vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text proves that “If \(G\) is connected,
    then the Laplacian eigenvalue \(\mu_2 > 0\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text states that “the Laplacian quadratic
    form measures how ‘smooth’ the function \(\mathbf{x}\) is over the graph in the
    following sense. A small value of \(\mathbf{x}^T L \mathbf{x}\) indicates that
    adjacent vertices tend to get assigned close values.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states, “Let \(B\) be an oriented
    incidence matrix of \(G\). By construction, \(L = B B^T\). This implies that \(L\)
    is symmetric and positive semidefinite.”'
  prefs: []
  type: TYPE_NORMAL
