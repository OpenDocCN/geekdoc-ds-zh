- en: 5.8\. Online supplementary materials#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap05_specgraph/supp/roch-mmids-specgraph-supp.html](https://mmids-textbook.github.io/chap05_specgraph/supp/roch-mmids-specgraph-supp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_specgraph_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_specgraph_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_specgraph_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 5.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-specgraph-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-specgraph-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.1** \(V = \{1, 2, 3, 4\}\), \(E = \{\{1, 2\}, \{1, 3\}, \{2, 4\}, \{3,
    4\}\}\). This follows directly from the definition of a graph as a pair \(G =
    (V, E)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.3** One possible path is \(1 \sim 2 \sim 4\). Its length is 2, since
    it consists of two edges.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.5** \(\delta^-(2) = 1\), \(\delta^+(2) = 2\). The in-degree of a vertex
    \(v\) is the number of edges with destination \(v\), and the out-degree is the
    number of edges with source \(v\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.7** The graph \(G\) is not connected. There is no path between vertex
    1 and vertex 4, indicating that the graph is not connected.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.9** The number of connected components is 1\. There is a path between
    every pair of vertices, so the graph is connected, having only one connected component.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} 1 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 1
    & 0 & 1 \\ 0 & 0 & 1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The entry \(B_{ij}\) is 1 if vertex \(i\) is incident to edge \(j\), and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} -1 & 0 & 1 & 0 \\ 1 & -1 & 0 & -1 \\ 0 &
    1 & -1 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The entry \(B_{ij}\) is 1 if edge \(j\) leaves vertex \(i\), -1 if edge \(j\)
    enters vertex \(i\), and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.15** The Petersen graph is 3-regular, as stated in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.1** We can find the eigenvectors of \(A\) and normalize them to get
    an orthonormal basis. The characteristic polynomial of \(A\) is \((5-\lambda)^2
    - 9 = \lambda^2 - 10\lambda + 16 = (\lambda - 2)(\lambda - 8)\), so the eigenvalues
    are 2 and 8\. For \(\lambda = 2\), an eigenvector is \(\begin{pmatrix} -1 \\ 1
    \end{pmatrix}\), and for \(\lambda = 8\), an eigenvector is \(\begin{pmatrix}
    1 \\ 1 \end{pmatrix}\). Normalizing these vectors, we get the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} W = \begin{pmatrix} -1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} &
    1/\sqrt{2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.3** \(R_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A\mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} = \langle \mathbf{u}, A\mathbf{u} \rangle = \begin{pmatrix}
    \frac{\sqrt{3}}{2} & \frac{1}{2} \end{pmatrix} \begin{pmatrix} 3 & 1 \\ 1 & 2
    \end{pmatrix} \begin{pmatrix} \frac{\sqrt{3}}{2} \\ \frac{1}{2} \end{pmatrix}
    = \frac{7}{2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.5** To find the eigenvalues, we solve the characteristic equation: \(\det(A
    - \lambda I) = \begin{vmatrix} 2-\lambda & -1 \\ -1 & 2-\lambda \end{vmatrix}
    = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = (\lambda - 1)(\lambda - 3) =
    0\). So the eigenvalues are \(\lambda_1 = 3\) and \(\lambda_2 = 1\). To find the
    eigenvectors, we solve \((A - \lambda_i I)\mathbf{v}_i = 0\) for each eigenvalue:
    For \(\lambda_1 = 3\): \(\begin{pmatrix} -1 & -1 \\ -1 & -1 \end{pmatrix}\mathbf{v}_1
    = \mathbf{0}\), which gives \(\mathbf{v}_1 = c\begin{pmatrix} 1 \\ -1 \end{pmatrix}\).
    Normalizing, we get \(\mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1
    \end{pmatrix}\). For \(\lambda_2 = 1\): \(\begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix}\mathbf{v}_2
    = \mathbf{0}\), which gives \(\mathbf{v}_2 = c\begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
    Normalizing, we get \(\mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1
    \end{pmatrix}\). To verify the variational characterization for \(\lambda_1\):
    \(R_A(\mathbf{v}_1) = \frac{\langle \mathbf{v}_1, A\mathbf{v}_1 \rangle}{\langle
    \mathbf{v}_1, \mathbf{v}_1 \rangle} = \frac{1}{2}\begin{pmatrix} 1 & 1 \end{pmatrix}\begin{pmatrix}
    2 & -1 \\ -1 & 2 \end{pmatrix}\begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{1}{2}(2
    - 1 - 1 + 2) = 3 = \lambda_1\). For any unit vector \(\mathbf{u} = \begin{pmatrix}
    \cos\theta \\ \sin\theta \end{pmatrix}\), we have: \(R_A(\mathbf{u}) = \begin{pmatrix}
    \cos\theta & \sin\theta \end{pmatrix}\begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}\begin{pmatrix}
    \cos\theta \\ \sin\theta \end{pmatrix} = 2\cos^2\theta + 2\sin^2\theta - 2\cos\theta\sin\theta
    = 2 - 2\cos\theta\sin\theta \leq 2 + 2|\cos\theta\sin\theta| \leq 3\), with equality
    when \(\theta = \frac{\pi}{4}\), which corresponds to \(\mathbf{u} = \mathbf{v}_1\).
    Thus, \(\lambda_1 = \max_{\mathbf{u} \neq \mathbf{0}} R_A(\mathbf{u})\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.7** First, we find the eigenvalues corresponding to the given eigenvectors:
    \(A\mathbf{v}_1 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 3 \\ 3 \\ 0 \end{pmatrix} = 3\mathbf{v}_1\), so \(\lambda_1 =
    3\). \(A\mathbf{v}_2 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \frac{1}{\sqrt{2}} \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} = \frac{1}{\sqrt{2}}
    \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} = \mathbf{v}_2\), so \(\lambda_2 =
    1\). \(A\mathbf{v}_3 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 3 \end{pmatrix}
    = 3\mathbf{v}_3\), so \(\lambda_3 = 3\).'
  prefs: []
  type: TYPE_NORMAL
- en: So the eigenvalues are \(\lambda_1 = 3\), \(\lambda_2 = 1\), and \(\lambda_3
    = 3\). Note that \(\lambda_2\) is the second smallest eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s verify the variational characterization for \(\lambda_2\). Any vector
    \(\mathbf{u} \in V_2\) can be written as \(\mathbf{u} = c_1 \mathbf{v}_1 + c_2
    \mathbf{v}_2\) for some scalars \(c_1\) and \(c_2\). Then:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\langle \mathbf{u}, \mathbf{u} \rangle = \langle c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2,
    c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 \rangle = c_1^2 \langle \mathbf{v}_1, \mathbf{v}_1
    \rangle + 2c_1c_2 \langle \mathbf{v}_1, \mathbf{v}_2 \rangle + c_2^2 \langle \mathbf{v}_2,
    \mathbf{v}_2 \rangle = c_1^2 + c_2^2\), since \(\mathbf{v}_1\) and \(\mathbf{v}_2\)
    are orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: \(\langle \mathbf{u}, A\mathbf{u} \rangle = \langle c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2,
    A(c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2) \rangle = \langle c_1 \mathbf{v}_1 + c_2
    \mathbf{v}_2, 3c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 \rangle = 3c_1^2 + c_2^2\),
    since \(A\mathbf{v}_1 = 3\mathbf{v}_1\) and \(A\mathbf{v}_2 = \mathbf{v}_2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, \(R_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A\mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} = \frac{3c_1^2 + c_2^2}{c_1^2 + c_2^2} \geq 1\)
    for all \(\mathbf{u} \neq \mathbf{0}\) in \(V_2\), with equality when \(c_1 =
    0\). Thus: \(\lambda_2 = \min_{\mathbf{0} \neq \mathbf{u} \in V_2} R_A(\mathbf{u})
    = 1 = R_A(\mathbf{v}_2)\).'
  prefs: []
  type: TYPE_NORMAL
- en: So indeed, the second smallest eigenvalue \(\lambda_2\) satisfies the variational
    characterization \(\lambda_2 = \min_{\mathbf{0} \neq \mathbf{u} \in V_2} R_A(\mathbf{u})\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.1** The degree matrix \(D\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0
    & 2 & 0 \\ 0 & 0 & 0 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The Laplacian matrix \(L\) is \(L = D - A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{pmatrix} 1 & -1 & 0 & 0 \\ -1 & 3 & -1 & -1 \\ 0
    & -1 & 2 & -1 \\ 0 & -1 & -1 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.3**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{y}_1 = \begin{pmatrix} 2 & -1 & 0 & -1 & 0\\ -1 & 2
    & -1 & 0 & 0\\ 0 & -1 & 3 & -1 & -1\\ -1 & 0 & -1 & 2 & 0\\ 0 & 0 & -1 & 0 & 1
    \end{pmatrix} \cdot \frac{1}{\sqrt{5}}\begin{pmatrix} 1\\ 1\\ 1\\ 1\\ 1 \end{pmatrix}
    = \frac{1}{\sqrt{5}}\begin{pmatrix} 0\\ 0\\ 0\\ 0\\ 0 \end{pmatrix} = 0 \cdot
    \mathbf{y}_1. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.5** Let’s verify that \(\mathbf{y}_1\) and \(\mathbf{y}_2\) are eigenvectors
    of \(L\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{y}_1 = \begin{pmatrix} 1 & -1 & 0 & 0\\ -1 & 2 & -1
    & 0\\ 0 & -1 & 2 & -1\\ 0 & 0 & -1 & 1 \end{pmatrix} \cdot \frac{1}{2}\begin{pmatrix}
    1\\ 1\\ 1\\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix} 0\\ 0\\ 0\\ 0 \end{pmatrix}
    = 0 \cdot \mathbf{y}_1, \end{split}\]\[\begin{split} L\mathbf{y}_2 = \begin{pmatrix}
    1 & -1 & 0 & 0\\ -1 & 2 & -1 & 0\\ 0 & -1 & 2 & -1\\ 0 & 0 & -1 & 1 \end{pmatrix}
    \cdot \frac{1}{2}\begin{pmatrix} 1\\ \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}\\
    -1 \end{pmatrix} = (2 - \sqrt{2}) \cdot \frac{1}{2}\begin{pmatrix} 1\\ \frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}\\ -1 \end{pmatrix} = (2 - \sqrt{2}) \cdot \mathbf{y}_2. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: So, \(\mathbf{y}_1\) is an eigenvector with eigenvalue \(\mu_1 = 0\), and \(\mathbf{y}_2\)
    is an eigenvector with eigenvalue \(\mu_2 = 2 - \sqrt{2}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.7** The maximum degree of \(K_4\) is \(\bar{\delta} = 3\). Using the
    bounds \(\bar{\delta} + 1 \leq \mu_n \leq 2\bar{\delta}\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 4 = \bar{\delta} + 1 \leq \mu_4 \leq 2\bar{\delta} = 6. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.9** The diagonal entry \(L_{ii}\) is the degree of vertex \(i\), and
    the off-diagonal entry \(L_{ij}\) (for \(i \neq j\)) is \(-1\) if vertices \(i\)
    and \(j\) are adjacent, and 0 otherwise. Thus, the sum of the entries in row \(i\)
    is'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \deg(i) - \sum_{j: \{i,j\} \in E} 1 = 0. \]'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.11** For any vector \(\mathbf{x} \in \mathbb{R}^n\), we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L_G \mathbf{x} = \sum_{\{i,j\} \in E} (x_i - x_j)^2 \ge 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Since this holds for all \(\mathbf{x}\), \(L_G\) is positive semidefinite.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.13** In a complete graph, each vertex has degree \(n-1\). Thus, the
    Laplacian matrix is \(L_G = nI - J\), where \(I\) is the identity matrix and \(J\)
    is the all-ones matrix. The eigenvalues of \(J\) are \(n\) (with multiplicity
    1) and, because \(J\) has rank one, 0 (with multiplicity \(n-1\)). Therefore,
    the eigenvalues of \(L_G\) are 0 (with multiplicity 1) and \(n\) (with multiplicity
    \(n-1\)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.1** \(|E(S, S^c)| = 2\) (the edges between \(S\) and \(S^c\) are \(\{1,
    2\}\) and \(\{2, 3\}\)), and \(\min\{|S|, |S^c|\} = 1\). Therefore, \(\phi(S)
    = \frac{|E(S, S^c)|}{\min\{|S|, |S^c|\}} = 2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.3** In a connected graph with \(n\) vertices, the numerator of the cut
    ratio is at least 1, and the denominator is at most \(n/2\), which is achieved
    by cutting the graph into two equal parts. Therefore, the smallest possible value
    of the isoperimetric number is \(\frac{1}{n/2} = \frac{2}{n}\). For \(n = 6\),
    this is \(\frac{1}{3}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.5** The Cheeger Inequality also states that \(\frac{\phi_G^2}{2\bar{\delta}}
    \leq \mu_2\). Therefore, \(\phi_G \leq \sqrt{2\bar{\delta}\mu_2} = \sqrt{2 \cdot
    4 \cdot 0.5} = 2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.7** Let \(\mathbf{v}_1 = (1, 1, 1, 1)/2\), \(\mathbf{v}_2 = (-1, -1,
    1, 1)/2\), \(\mathbf{v}_3 = (1, -1, -1, 1)/2\), and \(\mathbf{v}_4 = (1, -1, 1,
    -1)/2\). These vectors form an orthonormal list. We can verify that these are
    eigenvectors of \(L\) and find their corresponding eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{v}_1 = \begin{pmatrix} 2 & -1 & 0 & -1 \\ -1 & 2 &
    -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix} \frac{1}{2}\begin{pmatrix}
    1 \\ 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}
    = 0 \cdot \mathbf{v}_1, \end{split}\]\[\begin{split} L\mathbf{v}_2 = \begin{pmatrix}
    2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix}
    \frac{1}{2}\begin{pmatrix} -1 \\ -1 \\ 1 \\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
    -2 \\ -2 \\ 2 \\ 2 \end{pmatrix} = 2 \cdot \mathbf{v}_2, \end{split}\]\[\begin{split}
    L\mathbf{v}_3 = \begin{pmatrix} 2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 &
    2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix} \frac{1}{2}\begin{pmatrix} 1 \\ -1 \\
    -1 \\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix} 2 \\ -2 \\ -2 \\ 2 \end{pmatrix}
    = 2 \cdot \mathbf{v}_3, \end{split}\]\[\begin{split} L\mathbf{v}_4 = \begin{pmatrix}
    2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix}
    \frac{1}{2}\begin{pmatrix} 1 \\ -1 \\ 1 \\ -1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
    4 \\ -4 \\ 4 \\ -4 \end{pmatrix} = 4 \cdot \mathbf{v}_4. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the corresponding eigenvalues are \(\mu_1 = 0\), \(\mu_2 = 2\), \(\mu_3
    = 2\), and \(\mu_4 = 4\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.9** Using the Fiedler vector \(\mathbf{v}_2 = (-1, -1, 1, 1)/2\), an
    order is \(\pi(1) = 1\), \(\pi(2) = 2\), \(\pi(3) = 3\), \(\pi(4) = 4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.11** To find the isoperimetric number, we need to consider all possible
    cuts and find the minimum cut ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider all possible cuts:'
  prefs: []
  type: TYPE_NORMAL
- en: \(S = \{1\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 1\), so \(\phi(S)
    = 2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 2\), so \(\phi(S)
    = 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2, 3\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 1\), so \(\phi(S)
    = 2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2, 3, 4\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 0\), so
    \(\phi(S)\) is undefined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum cut ratio is \(1\), achieved by the cut \(S = \{1, 2\}\). Therefore,
    the isoperimetric number of the graph is \(\phi_G = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing this to the results from E5.5.8 and E5.5.10:'
  prefs: []
  type: TYPE_NORMAL
- en: In E5.5.8, we found that the Fiedler vector is either \((-1, -1, 1, 1)/2\),
    which suggests a cut separating vertices \(\{1, 2\}\) from \(\{3, 4\}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In E5.5.10, using the ordering based on the Fiedler vector, we found that the
    cut with the smallest ratio is \(S_2 = \{1, 2\}\), with a cut ratio of \(1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the Fiedler vector and the spectral clustering algorithm based on it correctly
    identify the cut that achieves the isoperimetric number (Cheeger constant) of
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s compare the isoperimetric number to the bounds given by Cheeger’s
    inequality. From E5.5.7, we know that the second smallest eigenvalue of the Laplacian
    matrix is \(\mu_2 = 2\). The maximum degree of the graph is \(\bar{\delta} = 2\).
    Cheeger’s inequality states that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{\phi_G^2}{2\bar{\delta}} \leq \mu_2 \leq 2\phi_G\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging in the values, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{(\frac{1}{2})^2}{2 \cdot 2} \leq 2 \leq 2 \cdot 1\]
  prefs: []
  type: TYPE_NORMAL
- en: 'which simplifies to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{1}{16} \leq 2 \leq 2\]
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the isoperimetric number \(\phi_G = 1\) satisfies the bounds
    given by Cheeger’s inequality.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 2 & 0 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 & 0 \\
    0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The degree matrix \(D\) is a diagonal matrix where each entry \(D_{ii}\) is
    the degree of vertex \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.15**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Cutset} = \{\{1, 3\}, \{2, 3\}, \{2, 4\}\}, \quad |E(S, S^c)| = 3 \]\[
    \phi(S) = \frac{|E(S, S^c)|}{\min(|S|, |S^c|)} = \frac{3}{2} = 1.5 \]
  prefs: []
  type: TYPE_NORMAL
- en: The cutset consists of edges between \(S\) and \(S^c\). The cut ratio \(\phi(S)\)
    is the size of the cutset divided by the size of the smaller subset.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.17**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code creates and displays the graph with the cut edges highlighted in red.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.1** The expected number of edges is \(\mathbb{E}[|E|] = \binom{n}{2}p
    = \binom{6}{2} \cdot 0.4 = 15 \cdot 0.4 = 6\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.3** The expected number of triangles is \(\mathbb{E}[|T_3|] = \binom{n}{3}p^3
    = \binom{10}{3} \cdot 0.3^3 = 120 \cdot 0.027 = 3.24\). The expected triangle
    density is \(\mathbb{E}[|T_3|/\binom{n}{3}] = p^3 = 0.3^3 = 0.027\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.5** The block assignment matrix \(Z\) is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Z = \begin{pmatrix} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 0 &
    1 \\ 0 & 1 \\ 0 & 1 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.7** The degree of a vertex in \(G(4, 0.5)\) follows a binomial distribution
    \(\mathrm{Bin}(3, 0.5)\). The probabilities are:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 0) = (0.5)^3 = 0.125\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 1) = 3 \cdot (0.5)^3 = 0.375\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 2) = 3 \cdot (0.5)^3 = 0.375\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 3) = (0.5)^3 = 0.125\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E5.6.9** The variance is \(\mathrm{Var}[|E|] = \binom{3}{2} \cdot p \cdot
    (1 - p) = 3 \cdot 0.5 \cdot 0.5 = 0.75\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.11** Since vertex 2 is in block \(C_1\) and vertex 4 is in block \(C_2\),
    the probability of an edge between them is \(b_{1,2} = 1/4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.13** The expected degree of each vertex is \((p+q)n/2 = (1)(8)/2 = 4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.15** Let \(A_{i,j}\) be the indicator random variable for the presence
    of edge \(\{i,j\}\). Then the number of edges is \(X = \sum_{i<j} A_{i,j}\). Since
    the edges are independent,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{Var}[X] &= \mathrm{Var}\left[\sum_{i<j} A_{i,j}\right]
    \\ &= \sum_{i<j} \mathrm{Var}[A_{i,j}] \\ &= \sum_{i<j} m_{i,j}(1-m_{i,j}) \\
    &= \frac{1}{2}\left(1-\frac{1}{2}\right) + \frac{1}{4}\left(1-\frac{1}{4}\right)
    + \frac{1}{2}\left(1-\frac{1}{2}\right) \\ &= \frac{11}{16}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define undirected and directed graphs, and identify their key components such
    as vertices, edges, paths, and cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize special types of graphs, including cliques, trees, forests, and directed
    acyclic graphs (DAGs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the connectivity of a graph and identify its connected components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the adjacency matrix, incidence matrix, and Laplacian matrix representations
    of a graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove key properties of the Laplacian matrix, such as symmetry and positive
    semidefiniteness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the concepts of graphs and their matrix representations to weighted graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement graph representations and algorithms using the NetworkX package in
    Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply graph theory concepts to model and analyze real-world networks and relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outline the proof of the Spectral Theorem using a sequence of orthogonal transformations
    to diagonalize a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Rayleigh quotient and explain its relationship to eigenvectors and
    eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the variational characterizations of the largest, smallest, and second
    smallest eigenvalues of a symmetric matrix using the Rayleigh quotient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Courant-Fischer Theorem and interpret its local and global formulas
    for the eigenvalues of a symmetric matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Courant-Fischer Theorem to characterize the third smallest eigenvalue
    of a symmetric matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the key properties of the Laplacian matrix, including symmetry, positive
    semidefiniteness, and the constant eigenvector associated with the zero eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the relationship between graph connectivity and the second smallest eigenvalue
    (algebraic connectivity) of the Laplacian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive bounds on the largest eigenvalue of the Laplacian matrix using the maximum
    degree of the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the variational characterization of the second smallest eigenvalue
    of the Laplacian matrix as a constrained optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how the eigenvectors of the Laplacian matrix can be used for graph drawing
    and revealing the underlying geometry of the graph, using the variational characterization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the Laplacian matrix and its eigenvalues and eigenvectors for simple
    graphs, and interpret the results in terms of graph connectivity and geometry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the cut ratio and the isoperimetric number (Cheeger constant) for a
    given graph cut.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Cheeger Inequality and explain its significance in relating the isoperimetric
    number to the second smallest Laplacian eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the main steps of the graph-cutting algorithm based on the Fiedler
    vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the performance guarantees of the Fiedler vector-based graph-cutting
    algorithm and compare them to the optimal cut.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply spectral clustering techniques to identify communities within a graph,
    and evaluate the quality of the resulting partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the minimum bisection problem as a discrete optimization problem and
    relax it into a continuous optimization problem related to the Laplacian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the inhomogeneous Erdős-Rényi (ER) random graph model and explain how
    it generalizes the standard ER model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate inhomogeneous ER graphs and analyze their properties, such as edge
    density and the probability of connectivity, using Python and NetworkX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the expected number of edges and triangles in an ER random graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the stochastic blockmodel (SBM) and its role in creating random graphs
    with planted partitions, and explain how it relates to the concept of homophily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct SBMs with specified block assignments and edge probabilities, and
    visualize the resulting graphs using Python and NetworkX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the expected adjacency matrix of an SBM given the block assignment matrix
    and connection probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply spectral clustering algorithms to SBMs and evaluate their performance
    in recovering the ground truth community structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the simplified symmetric stochastic blockmodel (SSBM) and derive the
    spectral decomposition of its expected adjacency matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between the eigenvectors of the expected Laplacian
    matrix and the community structure in the SSBM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigate the behavior of the eigenvalues of the Laplacian matrix for large
    random graphs and discuss the connection to random matrix theory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.8.2.1\. Spectral properties of SBM[#](#spectral-properties-of-sbm "Link to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SBM provides an alternative explanation for the efficacy of spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a toy version of the model. Specifically, we assume that:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of vertices \(n\) is even.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertices \(1,\ldots,n/2\) are in block \(C_1\) while vertices \(n/2+1,\ldots,n\)
    are in block \(C_2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intra-block connection probability is \(b_{1,1} = b_{2,2} = p\) and the
    inter-block connection probability is \(b_{1,2} = b_{2,1} = q < p\), with \(p,
    q \in (0,1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We allow self-loops, whose probability are the intra-block connection probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In that case, the matrix \(M\) is the block matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} M = \begin{pmatrix} p J & q J\\ qJ & pJ \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(J \in \mathbb{R}^{n/2 \times n/2}\) is the all-one matrix. We refer
    to this model as the symmetric stochastic blockmodel (SSBM).
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(M\) is symmetric. Hence it has a spectral decomposition. It is
    straightforward to compute. Let \(\mathbf{1}_m\) be the all-one vector of size
    \(m\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Spectral Decomposition of SSBM)** Consider the matrix \(M\) above.
    Let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{n}} \mathbf{1}_n \quad \text{and}
    \quad \mathbf{q}_2 = \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{n/2} \\ -
    \mathbf{1}_{n/2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{q}_3,\ldots,\mathbf{q}_n\) be an orthonormal basis of \((\mathrm{span}\{\mathbf{q}_1,
    \mathbf{q}_2\})^\perp\). Denote by \(Q\) the matrix whose columns are \(\mathbf{q}_1,\ldots,\mathbf{q}_n\).
    Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_1 = \frac{p + q}{2} n \quad \text{and} \quad \lambda_2 = \frac{p
    - q}{2} n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\lambda_3,\ldots,\lambda_n = 0\). Denote by \(\Lambda\) the diagonal matrix
    with diagonal entries \(\lambda_1,\ldots,\lambda_n\).
  prefs: []
  type: TYPE_NORMAL
- en: Then a spectral decomposition of \(M\) is given by
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = Q \Lambda Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, \(\mathbf{q}_i\) is an eigenvector of \(M\) with eigenvalue \(\lambda_i\).
    \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We start with \(\mathbf{q}_1\) and note that by the formula for the
    multiplication of block matrices and the definition of \(J\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} M \mathbf{q}_1 &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \mathbf{1}_n \\ &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}} \\ \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} p J \mathbf{1}_{\frac{n}{2}}
    + q J \mathbf{1}_{\frac{n}{2}}\\ q J \mathbf{1}_{\frac{n}{2}} + p J \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} (p + q) J \mathbf{1}_{\frac{n}{2}}\\
    (p + q) J \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix}
    (p + q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}\\ (p + q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{p + q}{2} \sqrt{n} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}}\\
    \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \lambda_1 \mathbf{q}_1. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} M \mathbf{q}_2 &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}} \\ - \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} p J \mathbf{1}_{\frac{n}{2}}
    - q J \mathbf{1}_{\frac{n}{2}}\\ q J \mathbf{1}_{\frac{n}{2}} - p J \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} (p - q) J \mathbf{1}_{\frac{n}{2}}\\
    (q - p) J \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix}
    (p - q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}\\ (q - p) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{p - q}{2} \sqrt{n} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}}\\
    - \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \lambda_2 \mathbf{q}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Matrix \(M\) has rank 2 since it only has two distinct columns (assuming \(p
    \neq q\)). By the *Spectral Theorem*, there is an orthonormal basis of eigenvectors.
    We have shown that \(\mathbf{q}_1\) and \(\mathbf{q}_2\) are eigenvectors with
    nonzero eigenvalues (again using that \(p \neq q\)). So the remaining eigenvectors
    must form a basis of the orthogonal complement of \(\mathrm{span}\{\mathbf{q}_1,
    \mathbf{q}_2\}\) and they must have eigenvalue \(0\) since they lie in the null
    space. In particular,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_3 = \lambda_4 = \ldots = \lambda_n = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Why is this relevant to graph cutting? We first compute the expected Laplacian
    matrix. For this we need to expected degree of each vertex. This is obtained as
    follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[\delta(1)] = \E\left[\sum_{j=1}^n \mathbf{1}_{\{i,j\} \in E}\right] =
    \sum_{j=1}^n \E[\mathbf{1}_{\{i,j\} \in E}] = \sum_{j=1}^{n/2} p + \sum_{j=n/2}^n
    q = (p + q) \frac{n}{2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we counted the self-loop (if present) once. The same holds for the other
    vertices. So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{L} := \E[L] = \E[D] - \E[A] = (p + q) \frac{n}{2} I_{n \times n}
    - M. \]
  prefs: []
  type: TYPE_NORMAL
- en: For any eigenvector \(\mathbf{q}_i\) of \(M\), we note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{L} \,\mathbf{q}_i = (p + q) \frac{n}{2} I_{n \times n} \mathbf{q}_i
    - M \mathbf{q}_i = \left\{(p + q) \frac{n}{2} - \lambda_i \right\}\mathbf{q}_i,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, \(\mathbf{q}_i\) is also an eigenvector of \(\overline{L}\). Its eigenvalues
    are therefore
  prefs: []
  type: TYPE_NORMAL
- en: \[ (p + q) \frac{n}{2} - \frac{p + q}{2} n = 0, \quad (p + q) \frac{n}{2} -
    \frac{p - q}{2} n = q n, \quad (p + q) \frac{n}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: When \(p > q\), \(q n\) is the second smallest eigenvalue of \(\overline{L}\)
    with corresponding eigenvector
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_2 = \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{n/2}
    \\ - \mathbf{1}_{n/2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The key observation:'
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvector corresponding to the second smallest eigenvalue of \(\overline{L}\)
    perfectly encodes the community structure by assigning \(1/\sqrt{n}\) to the vertices
    in block \(C_1\) and \(-1/\sqrt{n}\) to the vertices in block \(C_2\)!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, in reality, we do not get to observe \(\overline{L}\). Instead we compute
    the actual Laplacian \(L\), a random matrix whose expectation if \(\overline{L}\).
    But it turns out that, for large \(n\), the eigenvectors of \(L\) corresponding
    to its two smallest eigenvalues are close to \(\mathbf{q}_1\) and \(\mathbf{q}_2\).
    Hence we can recover the community structure approximately from \(L\). This is
    far from obvious since \(L\) and \(\overline{L}\) are very different matrices.
  prefs: []
  type: TYPE_NORMAL
- en: A formal proof of this claim is beyond this course. But we illustrate it numerically
    next.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We first construct the block assignment and matrix \(M\)
    in the case of SSBM.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The eigenvectors and eigenvalues of the Laplacian in this case are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The first eigenvalue is roughly \(0\) as expected with an eigenvector which
    is proportional to the all-one vector. The second eigenvalue is somewhat close
    to the expected \(q n = 0.2 \cdot 10 = 2\) with an eigenvector that has different
    signs on the two blocks. This is consistent with our prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues exhibit an interesting behavior that is common for random matrices.
    This is easier to see for larger \(n\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/c310cb9e0454e14839e37ace47a3a2e6be6493c406b477b7b6e5d1cc26e660e9.png](../Images/c13e5512c5c6416c42ab21f7a26e3278.png)'
  prefs: []
  type: TYPE_IMG
- en: The first two eigenvalues are close to \(0\) and \(0.2 \cdot 100 = 20\) as expected.
    The rest of the eigenvalues are centered around \( (0.2 + 0.8) 100 /2 = 50\),
    but they are quite spread out, with a density resembling a half-circle. This is
    related to [Wigner’s semicircular law](https://en.wikipedia.org/wiki/Wigner_semicircle_distribution)
    which plays a key role in random matrix theory.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.2\. Weyl’s inequality[#](#weyls-inequality "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We prove an inequality on the sensitivity of eigenvalues which is useful in
    certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: For a symmetric matrix \(C \in \mathbb{R}^{d \times d}\), we let \(\lambda_j(C)\),
    \(j=1, \ldots, d\), be the eigenvalues of \(C\) in non-increasing order with corresponding
    orthonormal eigenvectors \(\mathbf{v}_j\), \(j=1, \ldots, d\). The following lemma
    is one version of what is known as *Weyl’s Inequality*.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Weyl)** Let \(A \in \mathbb{R}^{d \times d}\) and \(B \in \mathbb{R}^{d
    \times d}\) be symmetric matrices. Then, for all \(j=1, \ldots, d\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max_{j \in [d]} \left|\lambda_j(B) - \lambda_j(A)\right| \leq \|B- A\|_2
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\|C\|_2\) is the induced \(2\)-norm of \(C\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the extremal characterization of the eigenvalues together
    with a dimension argument.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For a symmetric matrix \(C \in \mathbb{R}^{d \times d}\), define the
    subspaces'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{V}_k(C) = \mathrm{span}(\mathbf{v}_1, \ldots, \mathbf{v}_k) \quad\text{and}\quad
    \mathcal{W}_{d-k+1}(C) = \mathrm{span}(\mathbf{v}_k, \ldots, \mathbf{v}_d) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{v}_1,\ldots,\mathbf{v}_d\) form an orthonormal basis of eigenvectors
    of \(C\). Let \(H = B - A\). We prove only the upper bound. The other direction
    follows from interchanging the roles of \(A\) and \(B\). Because
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(\mathcal{V}_j(B)) + \mathrm{dim}(\mathcal{W}_{d-j+1}(A)) = j
    + (d-j+1) = d+1 \]
  prefs: []
  type: TYPE_NORMAL
- en: it it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}\left(\mathcal{V}_j(B) \cap \mathcal{W}_{d-j+1}(A)\right) \geq
    d+1 - d = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the \(\mathcal{V}_j(B) \cap \mathcal{W}_{d-j+1}(A)\) is non-empty. Let
    \(\mathbf{v}\) be a unit vector in that intersection.
  prefs: []
  type: TYPE_NORMAL
- en: By *Courant-Fischer*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_j(B) \leq \langle \mathbf{v}, (A+H) \mathbf{v}\rangle = \langle \mathbf{v},
    A \mathbf{v}\rangle + \langle \mathbf{v}, H \mathbf{v}\rangle \leq \lambda_j(A)
    + \langle \mathbf{v}, H \mathbf{v}\rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by *Cauchy-Schwarz*, since \(\|\mathbf{v}\|=1\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}, H \mathbf{v}\rangle \leq \|\mathbf{v}\| \|H\mathbf{v}\|
    \leq \|H\|_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: which proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.3\. Weighted case[#](#weighted-case "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concepts we have introduced can also be extended to weighted graphs, that
    is, graphs with weights on the edges. These weights might be a measure of the
    strength of the connection for instance. In this section, we briefly describe
    this extension, which is the basis for a [discrete calculus](https://en.wikipedia.org/wiki/Calculus_on_finite_weighted_graphs).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Weighted Graph or Digraph)** A weighted graph (or weighted
    digraph) is a triple \(G = (V, E, w)\) where \((V, E)\) is a graph (or directed
    graph) and \(w : E \to \mathbb{R}_+\) is a function that assigns positive real
    weights to the edges. For ease of notation, we write \(w_e = w_{ij} = w(i,j)\)
    for the weight of edge \(e = \{i,j\}\) (or \(e = (i,j)\) in the directed case).
    \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: As we did for graphs, we denote the vertices \(\{1,\ldots, n\}\) and the edges
    \(\{e_1,\ldots, e_{m}\}\), where \(n = |V|\) and \(m =|E|\). Properties of graphs
    can be generalized naturally. For instance, one defines the degree of a vertex
    \(i\) as, in the undirected case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \delta(i) = \sum_{j:\{i,j\} \in E} w_{ij}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in the directed case, the out-degree and in-degree are
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \delta^+(i) = \sum_{j: (i,j) \in E} w_{ij} \qquad \text{and} \qquad \delta^+(i)
    = \sum_{j: (j,i) \in E} w_{ij}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: In the undirected case, the adjacency matrix is generalized as follows. (A similar
    generalization holds for the directed case.)
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Adjacency Matrix for Weighted Graph)** Let \(G = (V, E, w)\)
    be a weighted graph with \(n = |V|\) vertices. The adjacency matrix \(A\) of \(G\)
    is the \(n\times n\) symmetric matrix defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A_{ij} = \begin{cases} w_{ij} & \text{if $\{i,j\} \in E$}\\
    0 & \text{o.w.} \end{cases} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: A similar generalization holds for the directed case.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Adjacency Matrix for Weighted Digraph)** Let \(G = (V, E,
    w)\) be a weighted digraph with \(n = |V|\) vertices. The adjacency matrix \(A\)
    of \(G\) is the \(n\times n\) matrix defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A_{ij} = \begin{cases} w_{ij} & \text{if $(i,j) \in E$}\\ 0
    & \text{o.w.} \end{cases} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Laplacian matrix for weighted graphs** In the case of a weighted graph, the
    Laplacian can then be defined as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Laplacian for Weighted Graph)** Let \(G = (V, E, w)\) be
    a weighted graph with \(n = |V|\) vertices and adjacency matrix \(A\). Let \(D
    = \mathrm{diag}(\delta(1), \ldots, \delta(n))\) be the weighted degree matrix.
    The Laplacian matrix associated to \(G\) is defined as \(L = D - A\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown (Try it!) that the Laplacian quadratic form satisfies in the
    weighted case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{x}, L \mathbf{x} \rangle = \sum_{\{i,j\} \in E} w_{ij} (x_i
    - x_j)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\mathbf{x} = (x_1,\ldots,x_n) \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a positive semidefinite matrix (Exercise: Why?), the weighted Laplacian
    has an orthonormal basis of eigenvectors with nonnegative eigenvalues that satisfy
    the variational characterization we derived above. In particular, if we denote
    the eigenvalues \(0 = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\), it follows from
    *Courant-Fischer* that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min\left\{ \sum_{\{u, v\} \in E} w_{uv} (x_u - x_v)^2 \,:\, \mathbf{x}
    = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{u=1}^n x_u = 0, \sum_{u = 1}^n x_u^2
    = 1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: If we generalize the cut ratio as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi(S) = \frac{\sum_{i \in S, j \in S^c} w_{ij}}{\min\{|S|, |S^c|\}} \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\emptyset \neq S \subset V\) and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi_G = \min\left\{ \phi(S)\,:\, \emptyset \neq S \subset V \right\} \]
  prefs: []
  type: TYPE_NORMAL
- en: it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 \leq 2 \phi_G \]
  prefs: []
  type: TYPE_NORMAL
- en: as in the unweighted case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized Laplacian** Other variants of the Laplacian matrix have also been
    studied. We introduced the normalized Laplacian next. Recall that in the weighted
    case, the degree is defined as \(\delta(i) = \sum_{j:\{i,j\} \in E} w_{i,j}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Normalized Laplacian)** The normalized Laplacian of \(G =
    (V,E,w)\) with adjacency matrix \(A\) and degree matrix \(D\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{-1/2} A D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Using our previous observations about multiplication by diagonal matrices, the
    entries of \(\mathcal{L}\) are
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathcal{L})_{i,j} = (I - (D^{-1/2} A D^{-1/2})_{i,j} = 1 - \frac{a_{i,j}}{\sqrt{\delta(i)
    \delta(j)}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We also note the following relation to the Laplacian matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = D^{-1/2} L D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We check that the normalized Laplacian is symmetric:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathcal{L}^T &= I^T - (D^{-1/2} A D^{-1/2})^T\\ &= I - (D^{-1/2})^T
    A^T (D^{-1/2})^T\\ &= I - D^{-1/2} A D^{-1/2}\\ &= \mathcal{L}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: It is also positive semidefinite. Indeed,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T \mathcal{L} \mathbf{x} = \mathbf{x}^T D^{-1/2} L D^{-1/2} \mathbf{x}
    = (D^{-1/2} \mathbf{x})^T L (D^{-1/2} \mathbf{x}) \geq 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the properties of the Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Hence by the *Spectral Theorem*, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \sum_{i=1}^n \eta_i \mathbf{z}_i \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{z}_i\)s are orthonormal eigenvectors of \(\mathcal{L}\)
    and the eigenvalues satisfy \(0 \leq \eta_1 \leq \eta_2 \leq \cdots \leq \eta_n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'One more observation: because the constant vector is eigenvector of \(L\) with
    eigenvalue \(0\), we get that \(D^{1/2} \mathbf{1}\) is an eigenvector of \(\mathcal{L}\)
    with eigenvalue \(0\). So \(\eta_1 = 0\) and we set'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{z}_1)_i = \left(\frac{D^{1/2} \mathbf{1}}{\|D^{1/2} \mathbf{1}\|_2}\right)_i
    = \sqrt{\frac{\delta(i)}{\sum_{i\in V} \delta(i)}}, \quad \forall i \in [n], \]
  prefs: []
  type: TYPE_NORMAL
- en: which makes \(\mathbf{z}_1\) into a unit norm vector.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship to the Laplacian matrix immediately implies (prove it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T \mathcal{L} \mathbf{x} = \sum_{\{i,j\} \in E} w_{ij} \left(\frac{x_i}{\sqrt{\delta(i)}}
    - \frac{x_j}{\sqrt{\delta(j)}}\right)^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\mathbf{x} = (x_1,\ldots,x_n)^T \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: Through the change of variables
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i = \frac{x_i}{\sqrt{\delta(i)}}, \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Courant-Fischer* gives this time (Why?)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_2 = \min\left\{ \sum_{\{u, v\} \in E} w_{uv} (y_u - y_v)^2 \,:\, \mathbf{y}
    = (y_1, \ldots, y_n)^T \in \mathbb{R}^n, \sum_{u=1}^n \delta(u) y_u = 0, \sum_{u
    = 1}^n \delta(u) y_u^2 = 1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: For a subset of vertices \(S \subseteq V\), let
  prefs: []
  type: TYPE_NORMAL
- en: \[ |S|_w = \sum_{i \in S} \delta(i), \]
  prefs: []
  type: TYPE_NORMAL
- en: which we refer to as the volume of \(S\). It is measure of the size of \(S\)
    weighted by the degrees.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider the normalized cut ratio, or bottleneck ratio,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi^N(S) = \frac{\sum_{i \in S, j \in S^c} w_{ij}}{\min\{|S|_w, |S^c|_w\}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\emptyset \neq S \subset V\) and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi^N_G = \min\left\{ \phi^N(S)\,:\, \emptyset \neq S \subset V \right\}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_2 \leq 2 \phi^N_G. \]
  prefs: []
  type: TYPE_NORMAL
- en: The normalized cut ratio is similar to the cut ratio, except that the notion
    of balance of the cut is measured in terms of volume. Note that this concept is
    also useful in the unweighted case.
  prefs: []
  type: TYPE_NORMAL
- en: We will an application of the normalized Laplacian later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.4\. Image segmentation[#](#image-segmentation "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We give a different, more involved application of the ideas developed in this
    topic to image segmentation. Let us quote Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, image segmentation is the process of partitioning a digital
    image into multiple segments (sets of pixels, also known as image objects). The
    goal of segmentation is to simplify and/or change the representation of an image
    into something that is more meaningful and easier to analyze. Image segmentation
    is typically used to locate objects and boundaries (lines, curves, etc.) in images.
    More precisely, image segmentation is the process of assigning a label to every
    pixel in an image such that pixels with the same label share certain characteristics.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Throughout, we will use the [`scikit-image`](https://scikit-image.org) library
    for processing images.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As an example, here is a picture of cell nuclei taken through optical microscopy
    as part of some medical experiment. It is taken from [here](https://www.kaggle.com/c/data-science-bowl-2018/data).
    Here we used the function [`skimage.io.imread`](https://scikit-image.org/docs/dev/api/skimage.io.html#skimage.io.imread)
    to load an image from file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/97466aec7bce35e54824c2c5e06887867f2842611eaf3732aeddf742f92cd10a.png](../Images/b66f83b52d9b11d0de7f40288586ce34.png)'
  prefs: []
  type: TYPE_IMG
- en: Suppose that, as part of this experiment, we have a large number of such images
    and need to keep track of the cell nuclei in some way (maybe count how many there
    are, or track them from frame to frame). A natural pre-processing step is to identify
    the cell nuclei in the image. We use image segmentation for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to the example below. Let us start with some further examples.
  prefs: []
  type: TYPE_NORMAL
- en: We will first work with the following [map of Wisconsin regions](https://www.dhs.wisconsin.gov/areaadmin/index.htm).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/01522a0eaaf3a16ba3949def4e39f361f25796dc87411fa8ec5eab3c2899f2ce.png](../Images/b3847f3c93be42a38b89f12fdc9dbb12.png)'
  prefs: []
  type: TYPE_IMG
- en: A color image such as this one is encoded as a \(3\)-dimensional array (or [tensor](https://en.wikipedia.org/wiki/Tensor)),
    meaning that it is an array with \(3\) indices (unlike matrices which have only
    two indices).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first two indices capture the position of a pixel. The third index capture
    the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model). Put differently,
    each pixel in the image has three numbers (between 0 and 255) attached to it that
    encodes its color.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, at position \((300,400)\) the RGB color is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/13b8e08064348a7fea064b045068e2d0414b2d1683ceddb1bc8773f2db0ede76.png](../Images/c1276f6e98c715b5fb7749fec4e9e8f4.png)'
  prefs: []
  type: TYPE_IMG
- en: To perform image segmentation using the spectral graph theory we have developed,
    we transform our image into a graph.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to coarsen the image by creating super-pixels, or regions
    of pixels that are close and have similar color. For this purpose, we will use
    [`skimage.segmentation.slic`](https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.slic),
    which in essence uses \(k\)-means clustering on the color space to identify blobs
    of pixels that are in close proximity and have similar colors. It takes as imput
    a number of super-pixels desired (`n_segments`), a compactness parameter (`compactness`)
    and a smoothing parameter (`sigma`). The output is a label assignment for each
    pixel in the form of a \(2\)-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the choice of the parameter `compactness` via [scikit-image](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic):'
  prefs: []
  type: TYPE_NORMAL
- en: Balances color proximity and space proximity. Higher values give more weight
    to space proximity, making superpixel shapes more square/cubic. This parameter
    depends strongly on image contrast and on the shapes of objects in the image.
    We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10,
    100, before refining around a chosen value.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The parameter `sigma` controls the level of [blurring](https://en.wikipedia.org/wiki/Gaussian_blur)
    applied to the image as a pre-processing step. In practice, experimentation is
    required to choose good parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A neat way to vizualize the super-pixels is to use the function [`skimage.color.label2rgb`](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb)
    which takes as input an image and an array of labels. In the mode `kind='avg'`,
    it outputs a new image where the color of each pixel is replaced with the average
    color of its label (that is, the average of the RGB color over all pixels with
    the same label). As they say, an image is worth a thousand words - let’s just
    see what it does.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/7959aeea4b885bae42571d1446beddae6b8272cb0a592785efe289c5b481d97d.png](../Images/e3129bc4814ed40c14212f0ea836da19.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that our goal is to turn our original image into a graph. After the first
    step of creating super-pixels, the second step is to form a graph whose nodes
    are the super-pixels. Edges are added between adjacent super-pixels and a weight
    is given to each edge which reflects the difference in mean color between the
    two.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use [`skimage.graph.rag_mean_color`](https://scikit-image.org/docs/stable/api/skimage.graph.html#skimage.graph.rag_mean_color).
    In mode `similarity`, it uses the following weight formula (quoting the documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: The weight between two adjacent regions is exp(-d^2/sigma) where d=|c1-c2|,
    where c1 and c2 are the mean colors of the two regions. It represents how similar
    two regions are.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The output, which is known as a region adjacency graph (RAG), is a `NetworkX`
    graph and can be manipulated using that package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/230bc75d3f1084bc07ca478f991bd5ced705add22482e96a084317bb211fb081.png](../Images/bb37436c8158e8a34ec74fa221b117a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '`scikit-image` also provides a more effective way of vizualizing a RAG, using
    the function [`skimage.future.graph.show_rag`](https://scikit-image.org/docs/dev/api/skimage.future.graph.html#skimage.future.graph.show_rag).
    Here the graph is super-imposed on the image and the edge weights are depicted
    by their color.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/8a74a1138feb384576ca2d04f1969c62068ac89d8007bd6c60571b02238c378d.png](../Images/849c34aaf5b7ff0e0edbd7a96bef3e4f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can apply the spectral clustering techniques we have developed in this chapter.
    Next we compute a spectral decomposition of the weighted Laplacian and plot the
    eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/35f7fdf29a2fda4096b00760443b1d7aedeceb12bdb718235b397a5193cff774.png](../Images/7cd4aa1ca84d37bfcc939bc25f473504.png)'
  prefs: []
  type: TYPE_IMG
- en: From the theory, this suggests that there are roughly 15 components in this
    graph. We project to \(15\) dimensions and apply \(k\)-means clustering to find
    segments. Rather than using our own implementation, we use [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    from the [`scikit-learn`](https://scikit-learn.org/stable/index.html) library.
    That implementation uses the [\(k\)-means\(++\)](https://en.wikipedia.org/wiki/K-means%2B%2B)
    initialization, which is particularly effective in practice. A label assignment
    for each node can be accessed using `labels_`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To vizualize the segmentation, we assign to each segment (i.e., collection
    of super-pixels) a random color. This can be done using [`skimage.color.label2rgb`](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb)
    again, this time in mode `kind=''overlay''`. First, we assign to each pixel from
    the original image its label under this clustering. Recall that `labels1` assigns
    to each pixel its super-pixel (represented by a node of the RAG), so that applying
    `assign_seg` element-wise to `labels1` results is assigning a cluster to each
    pixel. In code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/5aabdebf592677a0ae025884f681391b4f658489872b6a3d2b0ac134372946b4.png](../Images/d35fc1e0118ce41fd83d724b6ed4ed0b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the result is reasonable but far from perfect.
  prefs: []
  type: TYPE_NORMAL
- en: For ease of use, we encapsulate the main steps above in sub-routines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try a more complicated image. This one is taken from [here](https://www.reddit.com/r/aww/comments/169s6e/badgers_can_be_cute_too/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ffbfad2bbecf61e6ece848ef5ae22a7637c7fc18d200b285ae234c3468c213b2.png](../Images/1b8ad9014f9e0ecc4b6afbad4deca689.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that the choice of parameters requires significant fidgeting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/4d5e0ff3eed959013ca469b82626364e171eeee2de5a4e5e98111f328f4b67ac.png](../Images/e61b90b863a2bb5d77a156780ddc4898.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9d2a898681451451a2db82660a24f7d69757b43f55f4314ccd08161ec7f528ea.png](../Images/ab217ef799ccdb23d1ef49e871bc0379.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/439663d1e8381a740ff9db0be691c77db1a1eedbda596696d6f03b481d55ed43.png](../Images/0915e41ec9a5be32f78662924ffd0775.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, the results are far from perfect - but not unreasonable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we return to our medical example. We first reload the image and find
    super-pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e78b4d7d976169569b2d8223c75fc6a4bfddef251d88a030ebc48acd4dc02a22.png](../Images/aebbc424dbdb162039dbd74dd736c90a.png)'
  prefs: []
  type: TYPE_IMG
- en: We then form the weighted Laplacian and plot its eigenvalues. This time, about
    \(40\) dimensions seem appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9e8a00b5de0f0b92c8929fa508140d83f21bb7aba6c8accee6efd3b5b1681859.png](../Images/c6a0760eba98be16f4b3c9540e8f0519.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/3b2c73655508b73097ed97881a720ff130f1d5ac41e1a12c10c87237952d3ef3.png](../Images/afe3edc4d3fc31871e23e544fb4c2542.png)'
  prefs: []
  type: TYPE_IMG
- en: This method is quite finicky. The choice of parameters affects the results significantly.
    You should see for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: We mention that `scikit-image` has an implementation of a closely related method,
    Normalized Cut, [`skimage.graph.cut_normalized`](https://scikit-image.org/docs/dev/api/skimage.graph.html#skimage.graph.cut_normalized).
    Rather than performing \(k\)-means after projection, it recursively performs \(2\)-way
    cuts on the RAG and resulting subgraphs.
  prefs: []
  type: TYPE_NORMAL
- en: We try it next. The results are similar as you can see.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e09c9c323359a039a1f8bdac677f17df8c556b94f5811ba25723a203b5adf9d8.png](../Images/e021d21059fceddb156d0c352273b8a2.png)'
  prefs: []
  type: TYPE_IMG
- en: There are many other image segmentation methods. See for example [here](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#module-skimage.segmentation).
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_specgraph_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_specgraph_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_specgraph_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 5.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-specgraph-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-specgraph-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.1** \(V = \{1, 2, 3, 4\}\), \(E = \{\{1, 2\}, \{1, 3\}, \{2, 4\}, \{3,
    4\}\}\). This follows directly from the definition of a graph as a pair \(G =
    (V, E)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.3** One possible path is \(1 \sim 2 \sim 4\). Its length is 2, since
    it consists of two edges.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.5** \(\delta^-(2) = 1\), \(\delta^+(2) = 2\). The in-degree of a vertex
    \(v\) is the number of edges with destination \(v\), and the out-degree is the
    number of edges with source \(v\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.7** The graph \(G\) is not connected. There is no path between vertex
    1 and vertex 4, indicating that the graph is not connected.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.9** The number of connected components is 1\. There is a path between
    every pair of vertices, so the graph is connected, having only one connected component.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} 1 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 1
    & 0 & 1 \\ 0 & 0 & 1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The entry \(B_{ij}\) is 1 if vertex \(i\) is incident to edge \(j\), and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} -1 & 0 & 1 & 0 \\ 1 & -1 & 0 & -1 \\ 0 &
    1 & -1 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The entry \(B_{ij}\) is 1 if edge \(j\) leaves vertex \(i\), -1 if edge \(j\)
    enters vertex \(i\), and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.15** The Petersen graph is 3-regular, as stated in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.1** We can find the eigenvectors of \(A\) and normalize them to get
    an orthonormal basis. The characteristic polynomial of \(A\) is \((5-\lambda)^2
    - 9 = \lambda^2 - 10\lambda + 16 = (\lambda - 2)(\lambda - 8)\), so the eigenvalues
    are 2 and 8\. For \(\lambda = 2\), an eigenvector is \(\begin{pmatrix} -1 \\ 1
    \end{pmatrix}\), and for \(\lambda = 8\), an eigenvector is \(\begin{pmatrix}
    1 \\ 1 \end{pmatrix}\). Normalizing these vectors, we get the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} W = \begin{pmatrix} -1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} &
    1/\sqrt{2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.3** \(R_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A\mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} = \langle \mathbf{u}, A\mathbf{u} \rangle = \begin{pmatrix}
    \frac{\sqrt{3}}{2} & \frac{1}{2} \end{pmatrix} \begin{pmatrix} 3 & 1 \\ 1 & 2
    \end{pmatrix} \begin{pmatrix} \frac{\sqrt{3}}{2} \\ \frac{1}{2} \end{pmatrix}
    = \frac{7}{2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.5** To find the eigenvalues, we solve the characteristic equation: \(\det(A
    - \lambda I) = \begin{vmatrix} 2-\lambda & -1 \\ -1 & 2-\lambda \end{vmatrix}
    = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = (\lambda - 1)(\lambda - 3) =
    0\). So the eigenvalues are \(\lambda_1 = 3\) and \(\lambda_2 = 1\). To find the
    eigenvectors, we solve \((A - \lambda_i I)\mathbf{v}_i = 0\) for each eigenvalue:
    For \(\lambda_1 = 3\): \(\begin{pmatrix} -1 & -1 \\ -1 & -1 \end{pmatrix}\mathbf{v}_1
    = \mathbf{0}\), which gives \(\mathbf{v}_1 = c\begin{pmatrix} 1 \\ -1 \end{pmatrix}\).
    Normalizing, we get \(\mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1
    \end{pmatrix}\). For \(\lambda_2 = 1\): \(\begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix}\mathbf{v}_2
    = \mathbf{0}\), which gives \(\mathbf{v}_2 = c\begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
    Normalizing, we get \(\mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1
    \end{pmatrix}\). To verify the variational characterization for \(\lambda_1\):
    \(R_A(\mathbf{v}_1) = \frac{\langle \mathbf{v}_1, A\mathbf{v}_1 \rangle}{\langle
    \mathbf{v}_1, \mathbf{v}_1 \rangle} = \frac{1}{2}\begin{pmatrix} 1 & 1 \end{pmatrix}\begin{pmatrix}
    2 & -1 \\ -1 & 2 \end{pmatrix}\begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{1}{2}(2
    - 1 - 1 + 2) = 3 = \lambda_1\). For any unit vector \(\mathbf{u} = \begin{pmatrix}
    \cos\theta \\ \sin\theta \end{pmatrix}\), we have: \(R_A(\mathbf{u}) = \begin{pmatrix}
    \cos\theta & \sin\theta \end{pmatrix}\begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}\begin{pmatrix}
    \cos\theta \\ \sin\theta \end{pmatrix} = 2\cos^2\theta + 2\sin^2\theta - 2\cos\theta\sin\theta
    = 2 - 2\cos\theta\sin\theta \leq 2 + 2|\cos\theta\sin\theta| \leq 3\), with equality
    when \(\theta = \frac{\pi}{4}\), which corresponds to \(\mathbf{u} = \mathbf{v}_1\).
    Thus, \(\lambda_1 = \max_{\mathbf{u} \neq \mathbf{0}} R_A(\mathbf{u})\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.7** First, we find the eigenvalues corresponding to the given eigenvectors:
    \(A\mathbf{v}_1 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 3 \\ 3 \\ 0 \end{pmatrix} = 3\mathbf{v}_1\), so \(\lambda_1 =
    3\). \(A\mathbf{v}_2 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \frac{1}{\sqrt{2}} \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} = \frac{1}{\sqrt{2}}
    \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} = \mathbf{v}_2\), so \(\lambda_2 =
    1\). \(A\mathbf{v}_3 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 3 \end{pmatrix}
    = 3\mathbf{v}_3\), so \(\lambda_3 = 3\).'
  prefs: []
  type: TYPE_NORMAL
- en: So the eigenvalues are \(\lambda_1 = 3\), \(\lambda_2 = 1\), and \(\lambda_3
    = 3\). Note that \(\lambda_2\) is the second smallest eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s verify the variational characterization for \(\lambda_2\). Any vector
    \(\mathbf{u} \in V_2\) can be written as \(\mathbf{u} = c_1 \mathbf{v}_1 + c_2
    \mathbf{v}_2\) for some scalars \(c_1\) and \(c_2\). Then:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\langle \mathbf{u}, \mathbf{u} \rangle = \langle c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2,
    c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 \rangle = c_1^2 \langle \mathbf{v}_1, \mathbf{v}_1
    \rangle + 2c_1c_2 \langle \mathbf{v}_1, \mathbf{v}_2 \rangle + c_2^2 \langle \mathbf{v}_2,
    \mathbf{v}_2 \rangle = c_1^2 + c_2^2\), since \(\mathbf{v}_1\) and \(\mathbf{v}_2\)
    are orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: \(\langle \mathbf{u}, A\mathbf{u} \rangle = \langle c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2,
    A(c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2) \rangle = \langle c_1 \mathbf{v}_1 + c_2
    \mathbf{v}_2, 3c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 \rangle = 3c_1^2 + c_2^2\),
    since \(A\mathbf{v}_1 = 3\mathbf{v}_1\) and \(A\mathbf{v}_2 = \mathbf{v}_2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, \(R_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A\mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} = \frac{3c_1^2 + c_2^2}{c_1^2 + c_2^2} \geq 1\)
    for all \(\mathbf{u} \neq \mathbf{0}\) in \(V_2\), with equality when \(c_1 =
    0\). Thus: \(\lambda_2 = \min_{\mathbf{0} \neq \mathbf{u} \in V_2} R_A(\mathbf{u})
    = 1 = R_A(\mathbf{v}_2)\).'
  prefs: []
  type: TYPE_NORMAL
- en: So indeed, the second smallest eigenvalue \(\lambda_2\) satisfies the variational
    characterization \(\lambda_2 = \min_{\mathbf{0} \neq \mathbf{u} \in V_2} R_A(\mathbf{u})\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.1** The degree matrix \(D\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0
    & 2 & 0 \\ 0 & 0 & 0 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The Laplacian matrix \(L\) is \(L = D - A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{pmatrix} 1 & -1 & 0 & 0 \\ -1 & 3 & -1 & -1 \\ 0
    & -1 & 2 & -1 \\ 0 & -1 & -1 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.3**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{y}_1 = \begin{pmatrix} 2 & -1 & 0 & -1 & 0\\ -1 & 2
    & -1 & 0 & 0\\ 0 & -1 & 3 & -1 & -1\\ -1 & 0 & -1 & 2 & 0\\ 0 & 0 & -1 & 0 & 1
    \end{pmatrix} \cdot \frac{1}{\sqrt{5}}\begin{pmatrix} 1\\ 1\\ 1\\ 1\\ 1 \end{pmatrix}
    = \frac{1}{\sqrt{5}}\begin{pmatrix} 0\\ 0\\ 0\\ 0\\ 0 \end{pmatrix} = 0 \cdot
    \mathbf{y}_1. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.5** Let’s verify that \(\mathbf{y}_1\) and \(\mathbf{y}_2\) are eigenvectors
    of \(L\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{y}_1 = \begin{pmatrix} 1 & -1 & 0 & 0\\ -1 & 2 & -1
    & 0\\ 0 & -1 & 2 & -1\\ 0 & 0 & -1 & 1 \end{pmatrix} \cdot \frac{1}{2}\begin{pmatrix}
    1\\ 1\\ 1\\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix} 0\\ 0\\ 0\\ 0 \end{pmatrix}
    = 0 \cdot \mathbf{y}_1, \end{split}\]\[\begin{split} L\mathbf{y}_2 = \begin{pmatrix}
    1 & -1 & 0 & 0\\ -1 & 2 & -1 & 0\\ 0 & -1 & 2 & -1\\ 0 & 0 & -1 & 1 \end{pmatrix}
    \cdot \frac{1}{2}\begin{pmatrix} 1\\ \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}\\
    -1 \end{pmatrix} = (2 - \sqrt{2}) \cdot \frac{1}{2}\begin{pmatrix} 1\\ \frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}\\ -1 \end{pmatrix} = (2 - \sqrt{2}) \cdot \mathbf{y}_2. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: So, \(\mathbf{y}_1\) is an eigenvector with eigenvalue \(\mu_1 = 0\), and \(\mathbf{y}_2\)
    is an eigenvector with eigenvalue \(\mu_2 = 2 - \sqrt{2}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.7** The maximum degree of \(K_4\) is \(\bar{\delta} = 3\). Using the
    bounds \(\bar{\delta} + 1 \leq \mu_n \leq 2\bar{\delta}\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 4 = \bar{\delta} + 1 \leq \mu_4 \leq 2\bar{\delta} = 6. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.9** The diagonal entry \(L_{ii}\) is the degree of vertex \(i\), and
    the off-diagonal entry \(L_{ij}\) (for \(i \neq j\)) is \(-1\) if vertices \(i\)
    and \(j\) are adjacent, and 0 otherwise. Thus, the sum of the entries in row \(i\)
    is'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \deg(i) - \sum_{j: \{i,j\} \in E} 1 = 0. \]'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.11** For any vector \(\mathbf{x} \in \mathbb{R}^n\), we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L_G \mathbf{x} = \sum_{\{i,j\} \in E} (x_i - x_j)^2 \ge 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Since this holds for all \(\mathbf{x}\), \(L_G\) is positive semidefinite.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.13** In a complete graph, each vertex has degree \(n-1\). Thus, the
    Laplacian matrix is \(L_G = nI - J\), where \(I\) is the identity matrix and \(J\)
    is the all-ones matrix. The eigenvalues of \(J\) are \(n\) (with multiplicity
    1) and, because \(J\) has rank one, 0 (with multiplicity \(n-1\)). Therefore,
    the eigenvalues of \(L_G\) are 0 (with multiplicity 1) and \(n\) (with multiplicity
    \(n-1\)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.1** \(|E(S, S^c)| = 2\) (the edges between \(S\) and \(S^c\) are \(\{1,
    2\}\) and \(\{2, 3\}\)), and \(\min\{|S|, |S^c|\} = 1\). Therefore, \(\phi(S)
    = \frac{|E(S, S^c)|}{\min\{|S|, |S^c|\}} = 2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.3** In a connected graph with \(n\) vertices, the numerator of the cut
    ratio is at least 1, and the denominator is at most \(n/2\), which is achieved
    by cutting the graph into two equal parts. Therefore, the smallest possible value
    of the isoperimetric number is \(\frac{1}{n/2} = \frac{2}{n}\). For \(n = 6\),
    this is \(\frac{1}{3}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.5** The Cheeger Inequality also states that \(\frac{\phi_G^2}{2\bar{\delta}}
    \leq \mu_2\). Therefore, \(\phi_G \leq \sqrt{2\bar{\delta}\mu_2} = \sqrt{2 \cdot
    4 \cdot 0.5} = 2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.7** Let \(\mathbf{v}_1 = (1, 1, 1, 1)/2\), \(\mathbf{v}_2 = (-1, -1,
    1, 1)/2\), \(\mathbf{v}_3 = (1, -1, -1, 1)/2\), and \(\mathbf{v}_4 = (1, -1, 1,
    -1)/2\). These vectors form an orthonormal list. We can verify that these are
    eigenvectors of \(L\) and find their corresponding eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{v}_1 = \begin{pmatrix} 2 & -1 & 0 & -1 \\ -1 & 2 &
    -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix} \frac{1}{2}\begin{pmatrix}
    1 \\ 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}
    = 0 \cdot \mathbf{v}_1, \end{split}\]\[\begin{split} L\mathbf{v}_2 = \begin{pmatrix}
    2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix}
    \frac{1}{2}\begin{pmatrix} -1 \\ -1 \\ 1 \\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
    -2 \\ -2 \\ 2 \\ 2 \end{pmatrix} = 2 \cdot \mathbf{v}_2, \end{split}\]\[\begin{split}
    L\mathbf{v}_3 = \begin{pmatrix} 2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 &
    2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix} \frac{1}{2}\begin{pmatrix} 1 \\ -1 \\
    -1 \\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix} 2 \\ -2 \\ -2 \\ 2 \end{pmatrix}
    = 2 \cdot \mathbf{v}_3, \end{split}\]\[\begin{split} L\mathbf{v}_4 = \begin{pmatrix}
    2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix}
    \frac{1}{2}\begin{pmatrix} 1 \\ -1 \\ 1 \\ -1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
    4 \\ -4 \\ 4 \\ -4 \end{pmatrix} = 4 \cdot \mathbf{v}_4. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the corresponding eigenvalues are \(\mu_1 = 0\), \(\mu_2 = 2\), \(\mu_3
    = 2\), and \(\mu_4 = 4\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.9** Using the Fiedler vector \(\mathbf{v}_2 = (-1, -1, 1, 1)/2\), an
    order is \(\pi(1) = 1\), \(\pi(2) = 2\), \(\pi(3) = 3\), \(\pi(4) = 4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.11** To find the isoperimetric number, we need to consider all possible
    cuts and find the minimum cut ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider all possible cuts:'
  prefs: []
  type: TYPE_NORMAL
- en: \(S = \{1\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 1\), so \(\phi(S)
    = 2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 2\), so \(\phi(S)
    = 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2, 3\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 1\), so \(\phi(S)
    = 2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2, 3, 4\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 0\), so
    \(\phi(S)\) is undefined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum cut ratio is \(1\), achieved by the cut \(S = \{1, 2\}\). Therefore,
    the isoperimetric number of the graph is \(\phi_G = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing this to the results from E5.5.8 and E5.5.10:'
  prefs: []
  type: TYPE_NORMAL
- en: In E5.5.8, we found that the Fiedler vector is either \((-1, -1, 1, 1)/2\),
    which suggests a cut separating vertices \(\{1, 2\}\) from \(\{3, 4\}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In E5.5.10, using the ordering based on the Fiedler vector, we found that the
    cut with the smallest ratio is \(S_2 = \{1, 2\}\), with a cut ratio of \(1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the Fiedler vector and the spectral clustering algorithm based on it correctly
    identify the cut that achieves the isoperimetric number (Cheeger constant) of
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s compare the isoperimetric number to the bounds given by Cheeger’s
    inequality. From E5.5.7, we know that the second smallest eigenvalue of the Laplacian
    matrix is \(\mu_2 = 2\). The maximum degree of the graph is \(\bar{\delta} = 2\).
    Cheeger’s inequality states that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{\phi_G^2}{2\bar{\delta}} \leq \mu_2 \leq 2\phi_G\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging in the values, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{(\frac{1}{2})^2}{2 \cdot 2} \leq 2 \leq 2 \cdot 1\]
  prefs: []
  type: TYPE_NORMAL
- en: 'which simplifies to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{1}{16} \leq 2 \leq 2\]
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the isoperimetric number \(\phi_G = 1\) satisfies the bounds
    given by Cheeger’s inequality.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 2 & 0 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 & 0 \\
    0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The degree matrix \(D\) is a diagonal matrix where each entry \(D_{ii}\) is
    the degree of vertex \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.15**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Cutset} = \{\{1, 3\}, \{2, 3\}, \{2, 4\}\}, \quad |E(S, S^c)| = 3 \]\[
    \phi(S) = \frac{|E(S, S^c)|}{\min(|S|, |S^c|)} = \frac{3}{2} = 1.5 \]
  prefs: []
  type: TYPE_NORMAL
- en: The cutset consists of edges between \(S\) and \(S^c\). The cut ratio \(\phi(S)\)
    is the size of the cutset divided by the size of the smaller subset.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.17**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This code creates and displays the graph with the cut edges highlighted in red.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.1** The expected number of edges is \(\mathbb{E}[|E|] = \binom{n}{2}p
    = \binom{6}{2} \cdot 0.4 = 15 \cdot 0.4 = 6\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.3** The expected number of triangles is \(\mathbb{E}[|T_3|] = \binom{n}{3}p^3
    = \binom{10}{3} \cdot 0.3^3 = 120 \cdot 0.027 = 3.24\). The expected triangle
    density is \(\mathbb{E}[|T_3|/\binom{n}{3}] = p^3 = 0.3^3 = 0.027\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.5** The block assignment matrix \(Z\) is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Z = \begin{pmatrix} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 0 &
    1 \\ 0 & 1 \\ 0 & 1 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.7** The degree of a vertex in \(G(4, 0.5)\) follows a binomial distribution
    \(\mathrm{Bin}(3, 0.5)\). The probabilities are:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 0) = (0.5)^3 = 0.125\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 1) = 3 \cdot (0.5)^3 = 0.375\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 2) = 3 \cdot (0.5)^3 = 0.375\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 3) = (0.5)^3 = 0.125\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E5.6.9** The variance is \(\mathrm{Var}[|E|] = \binom{3}{2} \cdot p \cdot
    (1 - p) = 3 \cdot 0.5 \cdot 0.5 = 0.75\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.11** Since vertex 2 is in block \(C_1\) and vertex 4 is in block \(C_2\),
    the probability of an edge between them is \(b_{1,2} = 1/4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.13** The expected degree of each vertex is \((p+q)n/2 = (1)(8)/2 = 4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.15** Let \(A_{i,j}\) be the indicator random variable for the presence
    of edge \(\{i,j\}\). Then the number of edges is \(X = \sum_{i<j} A_{i,j}\). Since
    the edges are independent,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{Var}[X] &= \mathrm{Var}\left[\sum_{i<j} A_{i,j}\right]
    \\ &= \sum_{i<j} \mathrm{Var}[A_{i,j}] \\ &= \sum_{i<j} m_{i,j}(1-m_{i,j}) \\
    &= \frac{1}{2}\left(1-\frac{1}{2}\right) + \frac{1}{4}\left(1-\frac{1}{4}\right)
    + \frac{1}{2}\left(1-\frac{1}{2}\right) \\ &= \frac{11}{16}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define undirected and directed graphs, and identify their key components such
    as vertices, edges, paths, and cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize special types of graphs, including cliques, trees, forests, and directed
    acyclic graphs (DAGs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the connectivity of a graph and identify its connected components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the adjacency matrix, incidence matrix, and Laplacian matrix representations
    of a graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove key properties of the Laplacian matrix, such as symmetry and positive
    semidefiniteness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the concepts of graphs and their matrix representations to weighted graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement graph representations and algorithms using the NetworkX package in
    Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply graph theory concepts to model and analyze real-world networks and relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outline the proof of the Spectral Theorem using a sequence of orthogonal transformations
    to diagonalize a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Rayleigh quotient and explain its relationship to eigenvectors and
    eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the variational characterizations of the largest, smallest, and second
    smallest eigenvalues of a symmetric matrix using the Rayleigh quotient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Courant-Fischer Theorem and interpret its local and global formulas
    for the eigenvalues of a symmetric matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Courant-Fischer Theorem to characterize the third smallest eigenvalue
    of a symmetric matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the key properties of the Laplacian matrix, including symmetry, positive
    semidefiniteness, and the constant eigenvector associated with the zero eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the relationship between graph connectivity and the second smallest eigenvalue
    (algebraic connectivity) of the Laplacian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive bounds on the largest eigenvalue of the Laplacian matrix using the maximum
    degree of the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the variational characterization of the second smallest eigenvalue
    of the Laplacian matrix as a constrained optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how the eigenvectors of the Laplacian matrix can be used for graph drawing
    and revealing the underlying geometry of the graph, using the variational characterization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the Laplacian matrix and its eigenvalues and eigenvectors for simple
    graphs, and interpret the results in terms of graph connectivity and geometry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the cut ratio and the isoperimetric number (Cheeger constant) for a
    given graph cut.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Cheeger Inequality and explain its significance in relating the isoperimetric
    number to the second smallest Laplacian eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the main steps of the graph-cutting algorithm based on the Fiedler
    vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the performance guarantees of the Fiedler vector-based graph-cutting
    algorithm and compare them to the optimal cut.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply spectral clustering techniques to identify communities within a graph,
    and evaluate the quality of the resulting partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the minimum bisection problem as a discrete optimization problem and
    relax it into a continuous optimization problem related to the Laplacian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the inhomogeneous Erdős-Rényi (ER) random graph model and explain how
    it generalizes the standard ER model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate inhomogeneous ER graphs and analyze their properties, such as edge
    density and the probability of connectivity, using Python and NetworkX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the expected number of edges and triangles in an ER random graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the stochastic blockmodel (SBM) and its role in creating random graphs
    with planted partitions, and explain how it relates to the concept of homophily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct SBMs with specified block assignments and edge probabilities, and
    visualize the resulting graphs using Python and NetworkX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the expected adjacency matrix of an SBM given the block assignment matrix
    and connection probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply spectral clustering algorithms to SBMs and evaluate their performance
    in recovering the ground truth community structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the simplified symmetric stochastic blockmodel (SSBM) and derive the
    spectral decomposition of its expected adjacency matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between the eigenvectors of the expected Laplacian
    matrix and the community structure in the SSBM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigate the behavior of the eigenvalues of the Laplacian matrix for large
    random graphs and discuss the connection to random matrix theory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_specgraph_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_specgraph_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_specgraph_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 5.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 5.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_5_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-specgraph-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-specgraph-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.1** \(V = \{1, 2, 3, 4\}\), \(E = \{\{1, 2\}, \{1, 3\}, \{2, 4\}, \{3,
    4\}\}\). This follows directly from the definition of a graph as a pair \(G =
    (V, E)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.3** One possible path is \(1 \sim 2 \sim 4\). Its length is 2, since
    it consists of two edges.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.5** \(\delta^-(2) = 1\), \(\delta^+(2) = 2\). The in-degree of a vertex
    \(v\) is the number of edges with destination \(v\), and the out-degree is the
    number of edges with source \(v\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.7** The graph \(G\) is not connected. There is no path between vertex
    1 and vertex 4, indicating that the graph is not connected.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.9** The number of connected components is 1\. There is a path between
    every pair of vertices, so the graph is connected, having only one connected component.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} 1 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 1
    & 0 & 1 \\ 0 & 0 & 1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The entry \(B_{ij}\) is 1 if vertex \(i\) is incident to edge \(j\), and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} -1 & 0 & 1 & 0 \\ 1 & -1 & 0 & -1 \\ 0 &
    1 & -1 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The entry \(B_{ij}\) is 1 if edge \(j\) leaves vertex \(i\), -1 if edge \(j\)
    enters vertex \(i\), and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.2.15** The Petersen graph is 3-regular, as stated in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.1** We can find the eigenvectors of \(A\) and normalize them to get
    an orthonormal basis. The characteristic polynomial of \(A\) is \((5-\lambda)^2
    - 9 = \lambda^2 - 10\lambda + 16 = (\lambda - 2)(\lambda - 8)\), so the eigenvalues
    are 2 and 8\. For \(\lambda = 2\), an eigenvector is \(\begin{pmatrix} -1 \\ 1
    \end{pmatrix}\), and for \(\lambda = 8\), an eigenvector is \(\begin{pmatrix}
    1 \\ 1 \end{pmatrix}\). Normalizing these vectors, we get the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} W = \begin{pmatrix} -1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} &
    1/\sqrt{2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.3** \(R_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A\mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} = \langle \mathbf{u}, A\mathbf{u} \rangle = \begin{pmatrix}
    \frac{\sqrt{3}}{2} & \frac{1}{2} \end{pmatrix} \begin{pmatrix} 3 & 1 \\ 1 & 2
    \end{pmatrix} \begin{pmatrix} \frac{\sqrt{3}}{2} \\ \frac{1}{2} \end{pmatrix}
    = \frac{7}{2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.5** To find the eigenvalues, we solve the characteristic equation: \(\det(A
    - \lambda I) = \begin{vmatrix} 2-\lambda & -1 \\ -1 & 2-\lambda \end{vmatrix}
    = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = (\lambda - 1)(\lambda - 3) =
    0\). So the eigenvalues are \(\lambda_1 = 3\) and \(\lambda_2 = 1\). To find the
    eigenvectors, we solve \((A - \lambda_i I)\mathbf{v}_i = 0\) for each eigenvalue:
    For \(\lambda_1 = 3\): \(\begin{pmatrix} -1 & -1 \\ -1 & -1 \end{pmatrix}\mathbf{v}_1
    = \mathbf{0}\), which gives \(\mathbf{v}_1 = c\begin{pmatrix} 1 \\ -1 \end{pmatrix}\).
    Normalizing, we get \(\mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1
    \end{pmatrix}\). For \(\lambda_2 = 1\): \(\begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix}\mathbf{v}_2
    = \mathbf{0}\), which gives \(\mathbf{v}_2 = c\begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
    Normalizing, we get \(\mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1
    \end{pmatrix}\). To verify the variational characterization for \(\lambda_1\):
    \(R_A(\mathbf{v}_1) = \frac{\langle \mathbf{v}_1, A\mathbf{v}_1 \rangle}{\langle
    \mathbf{v}_1, \mathbf{v}_1 \rangle} = \frac{1}{2}\begin{pmatrix} 1 & 1 \end{pmatrix}\begin{pmatrix}
    2 & -1 \\ -1 & 2 \end{pmatrix}\begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{1}{2}(2
    - 1 - 1 + 2) = 3 = \lambda_1\). For any unit vector \(\mathbf{u} = \begin{pmatrix}
    \cos\theta \\ \sin\theta \end{pmatrix}\), we have: \(R_A(\mathbf{u}) = \begin{pmatrix}
    \cos\theta & \sin\theta \end{pmatrix}\begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}\begin{pmatrix}
    \cos\theta \\ \sin\theta \end{pmatrix} = 2\cos^2\theta + 2\sin^2\theta - 2\cos\theta\sin\theta
    = 2 - 2\cos\theta\sin\theta \leq 2 + 2|\cos\theta\sin\theta| \leq 3\), with equality
    when \(\theta = \frac{\pi}{4}\), which corresponds to \(\mathbf{u} = \mathbf{v}_1\).
    Thus, \(\lambda_1 = \max_{\mathbf{u} \neq \mathbf{0}} R_A(\mathbf{u})\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.3.7** First, we find the eigenvalues corresponding to the given eigenvectors:
    \(A\mathbf{v}_1 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 3 \\ 3 \\ 0 \end{pmatrix} = 3\mathbf{v}_1\), so \(\lambda_1 =
    3\). \(A\mathbf{v}_2 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \frac{1}{\sqrt{2}} \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} = \frac{1}{\sqrt{2}}
    \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} = \mathbf{v}_2\), so \(\lambda_2 =
    1\). \(A\mathbf{v}_3 = \begin{pmatrix} 1 & 2 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 3 \end{pmatrix}
    \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 3 \end{pmatrix}
    = 3\mathbf{v}_3\), so \(\lambda_3 = 3\).'
  prefs: []
  type: TYPE_NORMAL
- en: So the eigenvalues are \(\lambda_1 = 3\), \(\lambda_2 = 1\), and \(\lambda_3
    = 3\). Note that \(\lambda_2\) is the second smallest eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s verify the variational characterization for \(\lambda_2\). Any vector
    \(\mathbf{u} \in V_2\) can be written as \(\mathbf{u} = c_1 \mathbf{v}_1 + c_2
    \mathbf{v}_2\) for some scalars \(c_1\) and \(c_2\). Then:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\langle \mathbf{u}, \mathbf{u} \rangle = \langle c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2,
    c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 \rangle = c_1^2 \langle \mathbf{v}_1, \mathbf{v}_1
    \rangle + 2c_1c_2 \langle \mathbf{v}_1, \mathbf{v}_2 \rangle + c_2^2 \langle \mathbf{v}_2,
    \mathbf{v}_2 \rangle = c_1^2 + c_2^2\), since \(\mathbf{v}_1\) and \(\mathbf{v}_2\)
    are orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: \(\langle \mathbf{u}, A\mathbf{u} \rangle = \langle c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2,
    A(c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2) \rangle = \langle c_1 \mathbf{v}_1 + c_2
    \mathbf{v}_2, 3c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 \rangle = 3c_1^2 + c_2^2\),
    since \(A\mathbf{v}_1 = 3\mathbf{v}_1\) and \(A\mathbf{v}_2 = \mathbf{v}_2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, \(R_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A\mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} = \frac{3c_1^2 + c_2^2}{c_1^2 + c_2^2} \geq 1\)
    for all \(\mathbf{u} \neq \mathbf{0}\) in \(V_2\), with equality when \(c_1 =
    0\). Thus: \(\lambda_2 = \min_{\mathbf{0} \neq \mathbf{u} \in V_2} R_A(\mathbf{u})
    = 1 = R_A(\mathbf{v}_2)\).'
  prefs: []
  type: TYPE_NORMAL
- en: So indeed, the second smallest eigenvalue \(\lambda_2\) satisfies the variational
    characterization \(\lambda_2 = \min_{\mathbf{0} \neq \mathbf{u} \in V_2} R_A(\mathbf{u})\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.1** The degree matrix \(D\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0
    & 2 & 0 \\ 0 & 0 & 0 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The Laplacian matrix \(L\) is \(L = D - A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{pmatrix} 1 & -1 & 0 & 0 \\ -1 & 3 & -1 & -1 \\ 0
    & -1 & 2 & -1 \\ 0 & -1 & -1 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.3**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{y}_1 = \begin{pmatrix} 2 & -1 & 0 & -1 & 0\\ -1 & 2
    & -1 & 0 & 0\\ 0 & -1 & 3 & -1 & -1\\ -1 & 0 & -1 & 2 & 0\\ 0 & 0 & -1 & 0 & 1
    \end{pmatrix} \cdot \frac{1}{\sqrt{5}}\begin{pmatrix} 1\\ 1\\ 1\\ 1\\ 1 \end{pmatrix}
    = \frac{1}{\sqrt{5}}\begin{pmatrix} 0\\ 0\\ 0\\ 0\\ 0 \end{pmatrix} = 0 \cdot
    \mathbf{y}_1. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.5** Let’s verify that \(\mathbf{y}_1\) and \(\mathbf{y}_2\) are eigenvectors
    of \(L\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{y}_1 = \begin{pmatrix} 1 & -1 & 0 & 0\\ -1 & 2 & -1
    & 0\\ 0 & -1 & 2 & -1\\ 0 & 0 & -1 & 1 \end{pmatrix} \cdot \frac{1}{2}\begin{pmatrix}
    1\\ 1\\ 1\\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix} 0\\ 0\\ 0\\ 0 \end{pmatrix}
    = 0 \cdot \mathbf{y}_1, \end{split}\]\[\begin{split} L\mathbf{y}_2 = \begin{pmatrix}
    1 & -1 & 0 & 0\\ -1 & 2 & -1 & 0\\ 0 & -1 & 2 & -1\\ 0 & 0 & -1 & 1 \end{pmatrix}
    \cdot \frac{1}{2}\begin{pmatrix} 1\\ \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}\\
    -1 \end{pmatrix} = (2 - \sqrt{2}) \cdot \frac{1}{2}\begin{pmatrix} 1\\ \frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}\\ -1 \end{pmatrix} = (2 - \sqrt{2}) \cdot \mathbf{y}_2. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: So, \(\mathbf{y}_1\) is an eigenvector with eigenvalue \(\mu_1 = 0\), and \(\mathbf{y}_2\)
    is an eigenvector with eigenvalue \(\mu_2 = 2 - \sqrt{2}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.7** The maximum degree of \(K_4\) is \(\bar{\delta} = 3\). Using the
    bounds \(\bar{\delta} + 1 \leq \mu_n \leq 2\bar{\delta}\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 4 = \bar{\delta} + 1 \leq \mu_4 \leq 2\bar{\delta} = 6. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.9** The diagonal entry \(L_{ii}\) is the degree of vertex \(i\), and
    the off-diagonal entry \(L_{ij}\) (for \(i \neq j\)) is \(-1\) if vertices \(i\)
    and \(j\) are adjacent, and 0 otherwise. Thus, the sum of the entries in row \(i\)
    is'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \deg(i) - \sum_{j: \{i,j\} \in E} 1 = 0. \]'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.11** For any vector \(\mathbf{x} \in \mathbb{R}^n\), we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T L_G \mathbf{x} = \sum_{\{i,j\} \in E} (x_i - x_j)^2 \ge 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Since this holds for all \(\mathbf{x}\), \(L_G\) is positive semidefinite.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.4.13** In a complete graph, each vertex has degree \(n-1\). Thus, the
    Laplacian matrix is \(L_G = nI - J\), where \(I\) is the identity matrix and \(J\)
    is the all-ones matrix. The eigenvalues of \(J\) are \(n\) (with multiplicity
    1) and, because \(J\) has rank one, 0 (with multiplicity \(n-1\)). Therefore,
    the eigenvalues of \(L_G\) are 0 (with multiplicity 1) and \(n\) (with multiplicity
    \(n-1\)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.1** \(|E(S, S^c)| = 2\) (the edges between \(S\) and \(S^c\) are \(\{1,
    2\}\) and \(\{2, 3\}\)), and \(\min\{|S|, |S^c|\} = 1\). Therefore, \(\phi(S)
    = \frac{|E(S, S^c)|}{\min\{|S|, |S^c|\}} = 2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.3** In a connected graph with \(n\) vertices, the numerator of the cut
    ratio is at least 1, and the denominator is at most \(n/2\), which is achieved
    by cutting the graph into two equal parts. Therefore, the smallest possible value
    of the isoperimetric number is \(\frac{1}{n/2} = \frac{2}{n}\). For \(n = 6\),
    this is \(\frac{1}{3}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.5** The Cheeger Inequality also states that \(\frac{\phi_G^2}{2\bar{\delta}}
    \leq \mu_2\). Therefore, \(\phi_G \leq \sqrt{2\bar{\delta}\mu_2} = \sqrt{2 \cdot
    4 \cdot 0.5} = 2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.7** Let \(\mathbf{v}_1 = (1, 1, 1, 1)/2\), \(\mathbf{v}_2 = (-1, -1,
    1, 1)/2\), \(\mathbf{v}_3 = (1, -1, -1, 1)/2\), and \(\mathbf{v}_4 = (1, -1, 1,
    -1)/2\). These vectors form an orthonormal list. We can verify that these are
    eigenvectors of \(L\) and find their corresponding eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L\mathbf{v}_1 = \begin{pmatrix} 2 & -1 & 0 & -1 \\ -1 & 2 &
    -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix} \frac{1}{2}\begin{pmatrix}
    1 \\ 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}
    = 0 \cdot \mathbf{v}_1, \end{split}\]\[\begin{split} L\mathbf{v}_2 = \begin{pmatrix}
    2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix}
    \frac{1}{2}\begin{pmatrix} -1 \\ -1 \\ 1 \\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
    -2 \\ -2 \\ 2 \\ 2 \end{pmatrix} = 2 \cdot \mathbf{v}_2, \end{split}\]\[\begin{split}
    L\mathbf{v}_3 = \begin{pmatrix} 2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 &
    2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix} \frac{1}{2}\begin{pmatrix} 1 \\ -1 \\
    -1 \\ 1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix} 2 \\ -2 \\ -2 \\ 2 \end{pmatrix}
    = 2 \cdot \mathbf{v}_3, \end{split}\]\[\begin{split} L\mathbf{v}_4 = \begin{pmatrix}
    2 & -1 & 0 & -1 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ -1 & 0 & -1 & 2 \end{pmatrix}
    \frac{1}{2}\begin{pmatrix} 1 \\ -1 \\ 1 \\ -1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
    4 \\ -4 \\ 4 \\ -4 \end{pmatrix} = 4 \cdot \mathbf{v}_4. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the corresponding eigenvalues are \(\mu_1 = 0\), \(\mu_2 = 2\), \(\mu_3
    = 2\), and \(\mu_4 = 4\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.9** Using the Fiedler vector \(\mathbf{v}_2 = (-1, -1, 1, 1)/2\), an
    order is \(\pi(1) = 1\), \(\pi(2) = 2\), \(\pi(3) = 3\), \(\pi(4) = 4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.11** To find the isoperimetric number, we need to consider all possible
    cuts and find the minimum cut ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider all possible cuts:'
  prefs: []
  type: TYPE_NORMAL
- en: \(S = \{1\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 1\), so \(\phi(S)
    = 2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 2\), so \(\phi(S)
    = 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2, 3\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 1\), so \(\phi(S)
    = 2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(S = \{1, 2, 3, 4\}\), \(|E(S, S^c)| = 2\), \(\min\{|S|, |S^c|\} = 0\), so
    \(\phi(S)\) is undefined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum cut ratio is \(1\), achieved by the cut \(S = \{1, 2\}\). Therefore,
    the isoperimetric number of the graph is \(\phi_G = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing this to the results from E5.5.8 and E5.5.10:'
  prefs: []
  type: TYPE_NORMAL
- en: In E5.5.8, we found that the Fiedler vector is either \((-1, -1, 1, 1)/2\),
    which suggests a cut separating vertices \(\{1, 2\}\) from \(\{3, 4\}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In E5.5.10, using the ordering based on the Fiedler vector, we found that the
    cut with the smallest ratio is \(S_2 = \{1, 2\}\), with a cut ratio of \(1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the Fiedler vector and the spectral clustering algorithm based on it correctly
    identify the cut that achieves the isoperimetric number (Cheeger constant) of
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s compare the isoperimetric number to the bounds given by Cheeger’s
    inequality. From E5.5.7, we know that the second smallest eigenvalue of the Laplacian
    matrix is \(\mu_2 = 2\). The maximum degree of the graph is \(\bar{\delta} = 2\).
    Cheeger’s inequality states that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{\phi_G^2}{2\bar{\delta}} \leq \mu_2 \leq 2\phi_G\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging in the values, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{(\frac{1}{2})^2}{2 \cdot 2} \leq 2 \leq 2 \cdot 1\]
  prefs: []
  type: TYPE_NORMAL
- en: 'which simplifies to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\frac{1}{16} \leq 2 \leq 2\]
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the isoperimetric number \(\phi_G = 1\) satisfies the bounds
    given by Cheeger’s inequality.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 2 & 0 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 & 0 \\
    0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The degree matrix \(D\) is a diagonal matrix where each entry \(D_{ii}\) is
    the degree of vertex \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.15**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Cutset} = \{\{1, 3\}, \{2, 3\}, \{2, 4\}\}, \quad |E(S, S^c)| = 3 \]\[
    \phi(S) = \frac{|E(S, S^c)|}{\min(|S|, |S^c|)} = \frac{3}{2} = 1.5 \]
  prefs: []
  type: TYPE_NORMAL
- en: The cutset consists of edges between \(S\) and \(S^c\). The cut ratio \(\phi(S)\)
    is the size of the cutset divided by the size of the smaller subset.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.5.17**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This code creates and displays the graph with the cut edges highlighted in red.
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.1** The expected number of edges is \(\mathbb{E}[|E|] = \binom{n}{2}p
    = \binom{6}{2} \cdot 0.4 = 15 \cdot 0.4 = 6\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.3** The expected number of triangles is \(\mathbb{E}[|T_3|] = \binom{n}{3}p^3
    = \binom{10}{3} \cdot 0.3^3 = 120 \cdot 0.027 = 3.24\). The expected triangle
    density is \(\mathbb{E}[|T_3|/\binom{n}{3}] = p^3 = 0.3^3 = 0.027\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.5** The block assignment matrix \(Z\) is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Z = \begin{pmatrix} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 0 &
    1 \\ 0 & 1 \\ 0 & 1 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.7** The degree of a vertex in \(G(4, 0.5)\) follows a binomial distribution
    \(\mathrm{Bin}(3, 0.5)\). The probabilities are:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 0) = (0.5)^3 = 0.125\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 1) = 3 \cdot (0.5)^3 = 0.375\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 2) = 3 \cdot (0.5)^3 = 0.375\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{P}(d = 3) = (0.5)^3 = 0.125\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E5.6.9** The variance is \(\mathrm{Var}[|E|] = \binom{3}{2} \cdot p \cdot
    (1 - p) = 3 \cdot 0.5 \cdot 0.5 = 0.75\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.11** Since vertex 2 is in block \(C_1\) and vertex 4 is in block \(C_2\),
    the probability of an edge between them is \(b_{1,2} = 1/4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.13** The expected degree of each vertex is \((p+q)n/2 = (1)(8)/2 = 4\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E5.6.15** Let \(A_{i,j}\) be the indicator random variable for the presence
    of edge \(\{i,j\}\). Then the number of edges is \(X = \sum_{i<j} A_{i,j}\). Since
    the edges are independent,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{Var}[X] &= \mathrm{Var}\left[\sum_{i<j} A_{i,j}\right]
    \\ &= \sum_{i<j} \mathrm{Var}[A_{i,j}] \\ &= \sum_{i<j} m_{i,j}(1-m_{i,j}) \\
    &= \frac{1}{2}\left(1-\frac{1}{2}\right) + \frac{1}{4}\left(1-\frac{1}{4}\right)
    + \frac{1}{2}\left(1-\frac{1}{2}\right) \\ &= \frac{11}{16}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define undirected and directed graphs, and identify their key components such
    as vertices, edges, paths, and cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize special types of graphs, including cliques, trees, forests, and directed
    acyclic graphs (DAGs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the connectivity of a graph and identify its connected components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the adjacency matrix, incidence matrix, and Laplacian matrix representations
    of a graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove key properties of the Laplacian matrix, such as symmetry and positive
    semidefiniteness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the concepts of graphs and their matrix representations to weighted graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement graph representations and algorithms using the NetworkX package in
    Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply graph theory concepts to model and analyze real-world networks and relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outline the proof of the Spectral Theorem using a sequence of orthogonal transformations
    to diagonalize a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Rayleigh quotient and explain its relationship to eigenvectors and
    eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the variational characterizations of the largest, smallest, and second
    smallest eigenvalues of a symmetric matrix using the Rayleigh quotient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Courant-Fischer Theorem and interpret its local and global formulas
    for the eigenvalues of a symmetric matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Courant-Fischer Theorem to characterize the third smallest eigenvalue
    of a symmetric matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the key properties of the Laplacian matrix, including symmetry, positive
    semidefiniteness, and the constant eigenvector associated with the zero eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the relationship between graph connectivity and the second smallest eigenvalue
    (algebraic connectivity) of the Laplacian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive bounds on the largest eigenvalue of the Laplacian matrix using the maximum
    degree of the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the variational characterization of the second smallest eigenvalue
    of the Laplacian matrix as a constrained optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how the eigenvectors of the Laplacian matrix can be used for graph drawing
    and revealing the underlying geometry of the graph, using the variational characterization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the Laplacian matrix and its eigenvalues and eigenvectors for simple
    graphs, and interpret the results in terms of graph connectivity and geometry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the cut ratio and the isoperimetric number (Cheeger constant) for a
    given graph cut.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Cheeger Inequality and explain its significance in relating the isoperimetric
    number to the second smallest Laplacian eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the main steps of the graph-cutting algorithm based on the Fiedler
    vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the performance guarantees of the Fiedler vector-based graph-cutting
    algorithm and compare them to the optimal cut.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply spectral clustering techniques to identify communities within a graph,
    and evaluate the quality of the resulting partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the minimum bisection problem as a discrete optimization problem and
    relax it into a continuous optimization problem related to the Laplacian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the inhomogeneous Erdős-Rényi (ER) random graph model and explain how
    it generalizes the standard ER model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate inhomogeneous ER graphs and analyze their properties, such as edge
    density and the probability of connectivity, using Python and NetworkX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the expected number of edges and triangles in an ER random graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the stochastic blockmodel (SBM) and its role in creating random graphs
    with planted partitions, and explain how it relates to the concept of homophily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct SBMs with specified block assignments and edge probabilities, and
    visualize the resulting graphs using Python and NetworkX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the expected adjacency matrix of an SBM given the block assignment matrix
    and connection probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply spectral clustering algorithms to SBMs and evaluate their performance
    in recovering the ground truth community structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the simplified symmetric stochastic blockmodel (SSBM) and derive the
    spectral decomposition of its expected adjacency matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between the eigenvectors of the expected Laplacian
    matrix and the community structure in the SSBM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigate the behavior of the eigenvalues of the Laplacian matrix for large
    random graphs and discuss the connection to random matrix theory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.8.2.1\. Spectral properties of SBM[#](#spectral-properties-of-sbm "Link to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SBM provides an alternative explanation for the efficacy of spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a toy version of the model. Specifically, we assume that:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of vertices \(n\) is even.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertices \(1,\ldots,n/2\) are in block \(C_1\) while vertices \(n/2+1,\ldots,n\)
    are in block \(C_2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intra-block connection probability is \(b_{1,1} = b_{2,2} = p\) and the
    inter-block connection probability is \(b_{1,2} = b_{2,1} = q < p\), with \(p,
    q \in (0,1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We allow self-loops, whose probability are the intra-block connection probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In that case, the matrix \(M\) is the block matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} M = \begin{pmatrix} p J & q J\\ qJ & pJ \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(J \in \mathbb{R}^{n/2 \times n/2}\) is the all-one matrix. We refer
    to this model as the symmetric stochastic blockmodel (SSBM).
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(M\) is symmetric. Hence it has a spectral decomposition. It is
    straightforward to compute. Let \(\mathbf{1}_m\) be the all-one vector of size
    \(m\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Spectral Decomposition of SSBM)** Consider the matrix \(M\) above.
    Let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{n}} \mathbf{1}_n \quad \text{and}
    \quad \mathbf{q}_2 = \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{n/2} \\ -
    \mathbf{1}_{n/2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{q}_3,\ldots,\mathbf{q}_n\) be an orthonormal basis of \((\mathrm{span}\{\mathbf{q}_1,
    \mathbf{q}_2\})^\perp\). Denote by \(Q\) the matrix whose columns are \(\mathbf{q}_1,\ldots,\mathbf{q}_n\).
    Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_1 = \frac{p + q}{2} n \quad \text{and} \quad \lambda_2 = \frac{p
    - q}{2} n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\lambda_3,\ldots,\lambda_n = 0\). Denote by \(\Lambda\) the diagonal matrix
    with diagonal entries \(\lambda_1,\ldots,\lambda_n\).
  prefs: []
  type: TYPE_NORMAL
- en: Then a spectral decomposition of \(M\) is given by
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = Q \Lambda Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, \(\mathbf{q}_i\) is an eigenvector of \(M\) with eigenvalue \(\lambda_i\).
    \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We start with \(\mathbf{q}_1\) and note that by the formula for the
    multiplication of block matrices and the definition of \(J\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} M \mathbf{q}_1 &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \mathbf{1}_n \\ &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}} \\ \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} p J \mathbf{1}_{\frac{n}{2}}
    + q J \mathbf{1}_{\frac{n}{2}}\\ q J \mathbf{1}_{\frac{n}{2}} + p J \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} (p + q) J \mathbf{1}_{\frac{n}{2}}\\
    (p + q) J \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix}
    (p + q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}\\ (p + q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{p + q}{2} \sqrt{n} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}}\\
    \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \lambda_1 \mathbf{q}_1. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} M \mathbf{q}_2 &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}} \\ - \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} p J \mathbf{1}_{\frac{n}{2}}
    - q J \mathbf{1}_{\frac{n}{2}}\\ q J \mathbf{1}_{\frac{n}{2}} - p J \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} (p - q) J \mathbf{1}_{\frac{n}{2}}\\
    (q - p) J \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix}
    (p - q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}\\ (q - p) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{p - q}{2} \sqrt{n} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}}\\
    - \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \lambda_2 \mathbf{q}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Matrix \(M\) has rank 2 since it only has two distinct columns (assuming \(p
    \neq q\)). By the *Spectral Theorem*, there is an orthonormal basis of eigenvectors.
    We have shown that \(\mathbf{q}_1\) and \(\mathbf{q}_2\) are eigenvectors with
    nonzero eigenvalues (again using that \(p \neq q\)). So the remaining eigenvectors
    must form a basis of the orthogonal complement of \(\mathrm{span}\{\mathbf{q}_1,
    \mathbf{q}_2\}\) and they must have eigenvalue \(0\) since they lie in the null
    space. In particular,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_3 = \lambda_4 = \ldots = \lambda_n = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Why is this relevant to graph cutting? We first compute the expected Laplacian
    matrix. For this we need to expected degree of each vertex. This is obtained as
    follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[\delta(1)] = \E\left[\sum_{j=1}^n \mathbf{1}_{\{i,j\} \in E}\right] =
    \sum_{j=1}^n \E[\mathbf{1}_{\{i,j\} \in E}] = \sum_{j=1}^{n/2} p + \sum_{j=n/2}^n
    q = (p + q) \frac{n}{2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we counted the self-loop (if present) once. The same holds for the other
    vertices. So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{L} := \E[L] = \E[D] - \E[A] = (p + q) \frac{n}{2} I_{n \times n}
    - M. \]
  prefs: []
  type: TYPE_NORMAL
- en: For any eigenvector \(\mathbf{q}_i\) of \(M\), we note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{L} \,\mathbf{q}_i = (p + q) \frac{n}{2} I_{n \times n} \mathbf{q}_i
    - M \mathbf{q}_i = \left\{(p + q) \frac{n}{2} - \lambda_i \right\}\mathbf{q}_i,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, \(\mathbf{q}_i\) is also an eigenvector of \(\overline{L}\). Its eigenvalues
    are therefore
  prefs: []
  type: TYPE_NORMAL
- en: \[ (p + q) \frac{n}{2} - \frac{p + q}{2} n = 0, \quad (p + q) \frac{n}{2} -
    \frac{p - q}{2} n = q n, \quad (p + q) \frac{n}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: When \(p > q\), \(q n\) is the second smallest eigenvalue of \(\overline{L}\)
    with corresponding eigenvector
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_2 = \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{n/2}
    \\ - \mathbf{1}_{n/2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The key observation:'
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvector corresponding to the second smallest eigenvalue of \(\overline{L}\)
    perfectly encodes the community structure by assigning \(1/\sqrt{n}\) to the vertices
    in block \(C_1\) and \(-1/\sqrt{n}\) to the vertices in block \(C_2\)!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, in reality, we do not get to observe \(\overline{L}\). Instead we compute
    the actual Laplacian \(L\), a random matrix whose expectation if \(\overline{L}\).
    But it turns out that, for large \(n\), the eigenvectors of \(L\) corresponding
    to its two smallest eigenvalues are close to \(\mathbf{q}_1\) and \(\mathbf{q}_2\).
    Hence we can recover the community structure approximately from \(L\). This is
    far from obvious since \(L\) and \(\overline{L}\) are very different matrices.
  prefs: []
  type: TYPE_NORMAL
- en: A formal proof of this claim is beyond this course. But we illustrate it numerically
    next.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We first construct the block assignment and matrix \(M\)
    in the case of SSBM.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The eigenvectors and eigenvalues of the Laplacian in this case are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The first eigenvalue is roughly \(0\) as expected with an eigenvector which
    is proportional to the all-one vector. The second eigenvalue is somewhat close
    to the expected \(q n = 0.2 \cdot 10 = 2\) with an eigenvector that has different
    signs on the two blocks. This is consistent with our prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues exhibit an interesting behavior that is common for random matrices.
    This is easier to see for larger \(n\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/c310cb9e0454e14839e37ace47a3a2e6be6493c406b477b7b6e5d1cc26e660e9.png](../Images/c13e5512c5c6416c42ab21f7a26e3278.png)'
  prefs: []
  type: TYPE_IMG
- en: The first two eigenvalues are close to \(0\) and \(0.2 \cdot 100 = 20\) as expected.
    The rest of the eigenvalues are centered around \( (0.2 + 0.8) 100 /2 = 50\),
    but they are quite spread out, with a density resembling a half-circle. This is
    related to [Wigner’s semicircular law](https://en.wikipedia.org/wiki/Wigner_semicircle_distribution)
    which plays a key role in random matrix theory.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.2\. Weyl’s inequality[#](#weyls-inequality "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We prove an inequality on the sensitivity of eigenvalues which is useful in
    certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: For a symmetric matrix \(C \in \mathbb{R}^{d \times d}\), we let \(\lambda_j(C)\),
    \(j=1, \ldots, d\), be the eigenvalues of \(C\) in non-increasing order with corresponding
    orthonormal eigenvectors \(\mathbf{v}_j\), \(j=1, \ldots, d\). The following lemma
    is one version of what is known as *Weyl’s Inequality*.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Weyl)** Let \(A \in \mathbb{R}^{d \times d}\) and \(B \in \mathbb{R}^{d
    \times d}\) be symmetric matrices. Then, for all \(j=1, \ldots, d\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max_{j \in [d]} \left|\lambda_j(B) - \lambda_j(A)\right| \leq \|B- A\|_2
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\|C\|_2\) is the induced \(2\)-norm of \(C\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the extremal characterization of the eigenvalues together
    with a dimension argument.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For a symmetric matrix \(C \in \mathbb{R}^{d \times d}\), define the
    subspaces'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{V}_k(C) = \mathrm{span}(\mathbf{v}_1, \ldots, \mathbf{v}_k) \quad\text{and}\quad
    \mathcal{W}_{d-k+1}(C) = \mathrm{span}(\mathbf{v}_k, \ldots, \mathbf{v}_d) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{v}_1,\ldots,\mathbf{v}_d\) form an orthonormal basis of eigenvectors
    of \(C\). Let \(H = B - A\). We prove only the upper bound. The other direction
    follows from interchanging the roles of \(A\) and \(B\). Because
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(\mathcal{V}_j(B)) + \mathrm{dim}(\mathcal{W}_{d-j+1}(A)) = j
    + (d-j+1) = d+1 \]
  prefs: []
  type: TYPE_NORMAL
- en: it it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}\left(\mathcal{V}_j(B) \cap \mathcal{W}_{d-j+1}(A)\right) \geq
    d+1 - d = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the \(\mathcal{V}_j(B) \cap \mathcal{W}_{d-j+1}(A)\) is non-empty. Let
    \(\mathbf{v}\) be a unit vector in that intersection.
  prefs: []
  type: TYPE_NORMAL
- en: By *Courant-Fischer*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_j(B) \leq \langle \mathbf{v}, (A+H) \mathbf{v}\rangle = \langle \mathbf{v},
    A \mathbf{v}\rangle + \langle \mathbf{v}, H \mathbf{v}\rangle \leq \lambda_j(A)
    + \langle \mathbf{v}, H \mathbf{v}\rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by *Cauchy-Schwarz*, since \(\|\mathbf{v}\|=1\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}, H \mathbf{v}\rangle \leq \|\mathbf{v}\| \|H\mathbf{v}\|
    \leq \|H\|_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: which proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.3\. Weighted case[#](#weighted-case "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concepts we have introduced can also be extended to weighted graphs, that
    is, graphs with weights on the edges. These weights might be a measure of the
    strength of the connection for instance. In this section, we briefly describe
    this extension, which is the basis for a [discrete calculus](https://en.wikipedia.org/wiki/Calculus_on_finite_weighted_graphs).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Weighted Graph or Digraph)** A weighted graph (or weighted
    digraph) is a triple \(G = (V, E, w)\) where \((V, E)\) is a graph (or directed
    graph) and \(w : E \to \mathbb{R}_+\) is a function that assigns positive real
    weights to the edges. For ease of notation, we write \(w_e = w_{ij} = w(i,j)\)
    for the weight of edge \(e = \{i,j\}\) (or \(e = (i,j)\) in the directed case).
    \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: As we did for graphs, we denote the vertices \(\{1,\ldots, n\}\) and the edges
    \(\{e_1,\ldots, e_{m}\}\), where \(n = |V|\) and \(m =|E|\). Properties of graphs
    can be generalized naturally. For instance, one defines the degree of a vertex
    \(i\) as, in the undirected case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \delta(i) = \sum_{j:\{i,j\} \in E} w_{ij}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in the directed case, the out-degree and in-degree are
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \delta^+(i) = \sum_{j: (i,j) \in E} w_{ij} \qquad \text{and} \qquad \delta^+(i)
    = \sum_{j: (j,i) \in E} w_{ij}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: In the undirected case, the adjacency matrix is generalized as follows. (A similar
    generalization holds for the directed case.)
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Adjacency Matrix for Weighted Graph)** Let \(G = (V, E, w)\)
    be a weighted graph with \(n = |V|\) vertices. The adjacency matrix \(A\) of \(G\)
    is the \(n\times n\) symmetric matrix defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A_{ij} = \begin{cases} w_{ij} & \text{if $\{i,j\} \in E$}\\
    0 & \text{o.w.} \end{cases} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: A similar generalization holds for the directed case.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Adjacency Matrix for Weighted Digraph)** Let \(G = (V, E,
    w)\) be a weighted digraph with \(n = |V|\) vertices. The adjacency matrix \(A\)
    of \(G\) is the \(n\times n\) matrix defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A_{ij} = \begin{cases} w_{ij} & \text{if $(i,j) \in E$}\\ 0
    & \text{o.w.} \end{cases} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Laplacian matrix for weighted graphs** In the case of a weighted graph, the
    Laplacian can then be defined as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Laplacian for Weighted Graph)** Let \(G = (V, E, w)\) be
    a weighted graph with \(n = |V|\) vertices and adjacency matrix \(A\). Let \(D
    = \mathrm{diag}(\delta(1), \ldots, \delta(n))\) be the weighted degree matrix.
    The Laplacian matrix associated to \(G\) is defined as \(L = D - A\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown (Try it!) that the Laplacian quadratic form satisfies in the
    weighted case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{x}, L \mathbf{x} \rangle = \sum_{\{i,j\} \in E} w_{ij} (x_i
    - x_j)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\mathbf{x} = (x_1,\ldots,x_n) \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a positive semidefinite matrix (Exercise: Why?), the weighted Laplacian
    has an orthonormal basis of eigenvectors with nonnegative eigenvalues that satisfy
    the variational characterization we derived above. In particular, if we denote
    the eigenvalues \(0 = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\), it follows from
    *Courant-Fischer* that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min\left\{ \sum_{\{u, v\} \in E} w_{uv} (x_u - x_v)^2 \,:\, \mathbf{x}
    = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{u=1}^n x_u = 0, \sum_{u = 1}^n x_u^2
    = 1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: If we generalize the cut ratio as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi(S) = \frac{\sum_{i \in S, j \in S^c} w_{ij}}{\min\{|S|, |S^c|\}} \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\emptyset \neq S \subset V\) and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi_G = \min\left\{ \phi(S)\,:\, \emptyset \neq S \subset V \right\} \]
  prefs: []
  type: TYPE_NORMAL
- en: it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 \leq 2 \phi_G \]
  prefs: []
  type: TYPE_NORMAL
- en: as in the unweighted case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized Laplacian** Other variants of the Laplacian matrix have also been
    studied. We introduced the normalized Laplacian next. Recall that in the weighted
    case, the degree is defined as \(\delta(i) = \sum_{j:\{i,j\} \in E} w_{i,j}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Normalized Laplacian)** The normalized Laplacian of \(G =
    (V,E,w)\) with adjacency matrix \(A\) and degree matrix \(D\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{-1/2} A D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Using our previous observations about multiplication by diagonal matrices, the
    entries of \(\mathcal{L}\) are
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathcal{L})_{i,j} = (I - (D^{-1/2} A D^{-1/2})_{i,j} = 1 - \frac{a_{i,j}}{\sqrt{\delta(i)
    \delta(j)}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We also note the following relation to the Laplacian matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = D^{-1/2} L D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We check that the normalized Laplacian is symmetric:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathcal{L}^T &= I^T - (D^{-1/2} A D^{-1/2})^T\\ &= I - (D^{-1/2})^T
    A^T (D^{-1/2})^T\\ &= I - D^{-1/2} A D^{-1/2}\\ &= \mathcal{L}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: It is also positive semidefinite. Indeed,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T \mathcal{L} \mathbf{x} = \mathbf{x}^T D^{-1/2} L D^{-1/2} \mathbf{x}
    = (D^{-1/2} \mathbf{x})^T L (D^{-1/2} \mathbf{x}) \geq 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the properties of the Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Hence by the *Spectral Theorem*, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \sum_{i=1}^n \eta_i \mathbf{z}_i \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{z}_i\)s are orthonormal eigenvectors of \(\mathcal{L}\)
    and the eigenvalues satisfy \(0 \leq \eta_1 \leq \eta_2 \leq \cdots \leq \eta_n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'One more observation: because the constant vector is eigenvector of \(L\) with
    eigenvalue \(0\), we get that \(D^{1/2} \mathbf{1}\) is an eigenvector of \(\mathcal{L}\)
    with eigenvalue \(0\). So \(\eta_1 = 0\) and we set'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{z}_1)_i = \left(\frac{D^{1/2} \mathbf{1}}{\|D^{1/2} \mathbf{1}\|_2}\right)_i
    = \sqrt{\frac{\delta(i)}{\sum_{i\in V} \delta(i)}}, \quad \forall i \in [n], \]
  prefs: []
  type: TYPE_NORMAL
- en: which makes \(\mathbf{z}_1\) into a unit norm vector.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship to the Laplacian matrix immediately implies (prove it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T \mathcal{L} \mathbf{x} = \sum_{\{i,j\} \in E} w_{ij} \left(\frac{x_i}{\sqrt{\delta(i)}}
    - \frac{x_j}{\sqrt{\delta(j)}}\right)^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\mathbf{x} = (x_1,\ldots,x_n)^T \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: Through the change of variables
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i = \frac{x_i}{\sqrt{\delta(i)}}, \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Courant-Fischer* gives this time (Why?)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_2 = \min\left\{ \sum_{\{u, v\} \in E} w_{uv} (y_u - y_v)^2 \,:\, \mathbf{y}
    = (y_1, \ldots, y_n)^T \in \mathbb{R}^n, \sum_{u=1}^n \delta(u) y_u = 0, \sum_{u
    = 1}^n \delta(u) y_u^2 = 1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: For a subset of vertices \(S \subseteq V\), let
  prefs: []
  type: TYPE_NORMAL
- en: \[ |S|_w = \sum_{i \in S} \delta(i), \]
  prefs: []
  type: TYPE_NORMAL
- en: which we refer to as the volume of \(S\). It is measure of the size of \(S\)
    weighted by the degrees.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider the normalized cut ratio, or bottleneck ratio,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi^N(S) = \frac{\sum_{i \in S, j \in S^c} w_{ij}}{\min\{|S|_w, |S^c|_w\}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\emptyset \neq S \subset V\) and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi^N_G = \min\left\{ \phi^N(S)\,:\, \emptyset \neq S \subset V \right\}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_2 \leq 2 \phi^N_G. \]
  prefs: []
  type: TYPE_NORMAL
- en: The normalized cut ratio is similar to the cut ratio, except that the notion
    of balance of the cut is measured in terms of volume. Note that this concept is
    also useful in the unweighted case.
  prefs: []
  type: TYPE_NORMAL
- en: We will an application of the normalized Laplacian later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.4\. Image segmentation[#](#image-segmentation "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We give a different, more involved application of the ideas developed in this
    topic to image segmentation. Let us quote Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, image segmentation is the process of partitioning a digital
    image into multiple segments (sets of pixels, also known as image objects). The
    goal of segmentation is to simplify and/or change the representation of an image
    into something that is more meaningful and easier to analyze. Image segmentation
    is typically used to locate objects and boundaries (lines, curves, etc.) in images.
    More precisely, image segmentation is the process of assigning a label to every
    pixel in an image such that pixels with the same label share certain characteristics.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Throughout, we will use the [`scikit-image`](https://scikit-image.org) library
    for processing images.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: As an example, here is a picture of cell nuclei taken through optical microscopy
    as part of some medical experiment. It is taken from [here](https://www.kaggle.com/c/data-science-bowl-2018/data).
    Here we used the function [`skimage.io.imread`](https://scikit-image.org/docs/dev/api/skimage.io.html#skimage.io.imread)
    to load an image from file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/97466aec7bce35e54824c2c5e06887867f2842611eaf3732aeddf742f92cd10a.png](../Images/b66f83b52d9b11d0de7f40288586ce34.png)'
  prefs: []
  type: TYPE_IMG
- en: Suppose that, as part of this experiment, we have a large number of such images
    and need to keep track of the cell nuclei in some way (maybe count how many there
    are, or track them from frame to frame). A natural pre-processing step is to identify
    the cell nuclei in the image. We use image segmentation for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to the example below. Let us start with some further examples.
  prefs: []
  type: TYPE_NORMAL
- en: We will first work with the following [map of Wisconsin regions](https://www.dhs.wisconsin.gov/areaadmin/index.htm).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/01522a0eaaf3a16ba3949def4e39f361f25796dc87411fa8ec5eab3c2899f2ce.png](../Images/b3847f3c93be42a38b89f12fdc9dbb12.png)'
  prefs: []
  type: TYPE_IMG
- en: A color image such as this one is encoded as a \(3\)-dimensional array (or [tensor](https://en.wikipedia.org/wiki/Tensor)),
    meaning that it is an array with \(3\) indices (unlike matrices which have only
    two indices).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The first two indices capture the position of a pixel. The third index capture
    the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model). Put differently,
    each pixel in the image has three numbers (between 0 and 255) attached to it that
    encodes its color.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, at position \((300,400)\) the RGB color is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/13b8e08064348a7fea064b045068e2d0414b2d1683ceddb1bc8773f2db0ede76.png](../Images/c1276f6e98c715b5fb7749fec4e9e8f4.png)'
  prefs: []
  type: TYPE_IMG
- en: To perform image segmentation using the spectral graph theory we have developed,
    we transform our image into a graph.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to coarsen the image by creating super-pixels, or regions
    of pixels that are close and have similar color. For this purpose, we will use
    [`skimage.segmentation.slic`](https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.slic),
    which in essence uses \(k\)-means clustering on the color space to identify blobs
    of pixels that are in close proximity and have similar colors. It takes as imput
    a number of super-pixels desired (`n_segments`), a compactness parameter (`compactness`)
    and a smoothing parameter (`sigma`). The output is a label assignment for each
    pixel in the form of a \(2\)-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the choice of the parameter `compactness` via [scikit-image](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic):'
  prefs: []
  type: TYPE_NORMAL
- en: Balances color proximity and space proximity. Higher values give more weight
    to space proximity, making superpixel shapes more square/cubic. This parameter
    depends strongly on image contrast and on the shapes of objects in the image.
    We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10,
    100, before refining around a chosen value.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The parameter `sigma` controls the level of [blurring](https://en.wikipedia.org/wiki/Gaussian_blur)
    applied to the image as a pre-processing step. In practice, experimentation is
    required to choose good parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: A neat way to vizualize the super-pixels is to use the function [`skimage.color.label2rgb`](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb)
    which takes as input an image and an array of labels. In the mode `kind='avg'`,
    it outputs a new image where the color of each pixel is replaced with the average
    color of its label (that is, the average of the RGB color over all pixels with
    the same label). As they say, an image is worth a thousand words - let’s just
    see what it does.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/7959aeea4b885bae42571d1446beddae6b8272cb0a592785efe289c5b481d97d.png](../Images/e3129bc4814ed40c14212f0ea836da19.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that our goal is to turn our original image into a graph. After the first
    step of creating super-pixels, the second step is to form a graph whose nodes
    are the super-pixels. Edges are added between adjacent super-pixels and a weight
    is given to each edge which reflects the difference in mean color between the
    two.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use [`skimage.graph.rag_mean_color`](https://scikit-image.org/docs/stable/api/skimage.graph.html#skimage.graph.rag_mean_color).
    In mode `similarity`, it uses the following weight formula (quoting the documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: The weight between two adjacent regions is exp(-d^2/sigma) where d=|c1-c2|,
    where c1 and c2 are the mean colors of the two regions. It represents how similar
    two regions are.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The output, which is known as a region adjacency graph (RAG), is a `NetworkX`
    graph and can be manipulated using that package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/230bc75d3f1084bc07ca478f991bd5ced705add22482e96a084317bb211fb081.png](../Images/bb37436c8158e8a34ec74fa221b117a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '`scikit-image` also provides a more effective way of vizualizing a RAG, using
    the function [`skimage.future.graph.show_rag`](https://scikit-image.org/docs/dev/api/skimage.future.graph.html#skimage.future.graph.show_rag).
    Here the graph is super-imposed on the image and the edge weights are depicted
    by their color.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/8a74a1138feb384576ca2d04f1969c62068ac89d8007bd6c60571b02238c378d.png](../Images/849c34aaf5b7ff0e0edbd7a96bef3e4f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can apply the spectral clustering techniques we have developed in this chapter.
    Next we compute a spectral decomposition of the weighted Laplacian and plot the
    eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/35f7fdf29a2fda4096b00760443b1d7aedeceb12bdb718235b397a5193cff774.png](../Images/7cd4aa1ca84d37bfcc939bc25f473504.png)'
  prefs: []
  type: TYPE_IMG
- en: From the theory, this suggests that there are roughly 15 components in this
    graph. We project to \(15\) dimensions and apply \(k\)-means clustering to find
    segments. Rather than using our own implementation, we use [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    from the [`scikit-learn`](https://scikit-learn.org/stable/index.html) library.
    That implementation uses the [\(k\)-means\(++\)](https://en.wikipedia.org/wiki/K-means%2B%2B)
    initialization, which is particularly effective in practice. A label assignment
    for each node can be accessed using `labels_`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'To vizualize the segmentation, we assign to each segment (i.e., collection
    of super-pixels) a random color. This can be done using [`skimage.color.label2rgb`](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb)
    again, this time in mode `kind=''overlay''`. First, we assign to each pixel from
    the original image its label under this clustering. Recall that `labels1` assigns
    to each pixel its super-pixel (represented by a node of the RAG), so that applying
    `assign_seg` element-wise to `labels1` results is assigning a cluster to each
    pixel. In code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/5aabdebf592677a0ae025884f681391b4f658489872b6a3d2b0ac134372946b4.png](../Images/d35fc1e0118ce41fd83d724b6ed4ed0b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the result is reasonable but far from perfect.
  prefs: []
  type: TYPE_NORMAL
- en: For ease of use, we encapsulate the main steps above in sub-routines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try a more complicated image. This one is taken from [here](https://www.reddit.com/r/aww/comments/169s6e/badgers_can_be_cute_too/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ffbfad2bbecf61e6ece848ef5ae22a7637c7fc18d200b285ae234c3468c213b2.png](../Images/1b8ad9014f9e0ecc4b6afbad4deca689.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that the choice of parameters requires significant fidgeting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/4d5e0ff3eed959013ca469b82626364e171eeee2de5a4e5e98111f328f4b67ac.png](../Images/e61b90b863a2bb5d77a156780ddc4898.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9d2a898681451451a2db82660a24f7d69757b43f55f4314ccd08161ec7f528ea.png](../Images/ab217ef799ccdb23d1ef49e871bc0379.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/439663d1e8381a740ff9db0be691c77db1a1eedbda596696d6f03b481d55ed43.png](../Images/0915e41ec9a5be32f78662924ffd0775.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, the results are far from perfect - but not unreasonable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we return to our medical example. We first reload the image and find
    super-pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e78b4d7d976169569b2d8223c75fc6a4bfddef251d88a030ebc48acd4dc02a22.png](../Images/aebbc424dbdb162039dbd74dd736c90a.png)'
  prefs: []
  type: TYPE_IMG
- en: We then form the weighted Laplacian and plot its eigenvalues. This time, about
    \(40\) dimensions seem appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9e8a00b5de0f0b92c8929fa508140d83f21bb7aba6c8accee6efd3b5b1681859.png](../Images/c6a0760eba98be16f4b3c9540e8f0519.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/3b2c73655508b73097ed97881a720ff130f1d5ac41e1a12c10c87237952d3ef3.png](../Images/afe3edc4d3fc31871e23e544fb4c2542.png)'
  prefs: []
  type: TYPE_IMG
- en: This method is quite finicky. The choice of parameters affects the results significantly.
    You should see for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: We mention that `scikit-image` has an implementation of a closely related method,
    Normalized Cut, [`skimage.graph.cut_normalized`](https://scikit-image.org/docs/dev/api/skimage.graph.html#skimage.graph.cut_normalized).
    Rather than performing \(k\)-means after projection, it recursively performs \(2\)-way
    cuts on the RAG and resulting subgraphs.
  prefs: []
  type: TYPE_NORMAL
- en: We try it next. The results are similar as you can see.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e09c9c323359a039a1f8bdac677f17df8c556b94f5811ba25723a203b5adf9d8.png](../Images/e021d21059fceddb156d0c352273b8a2.png)'
  prefs: []
  type: TYPE_IMG
- en: There are many other image segmentation methods. See for example [here](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#module-skimage.segmentation).
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.1\. Spectral properties of SBM[#](#spectral-properties-of-sbm "Link to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SBM provides an alternative explanation for the efficacy of spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a toy version of the model. Specifically, we assume that:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of vertices \(n\) is even.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertices \(1,\ldots,n/2\) are in block \(C_1\) while vertices \(n/2+1,\ldots,n\)
    are in block \(C_2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intra-block connection probability is \(b_{1,1} = b_{2,2} = p\) and the
    inter-block connection probability is \(b_{1,2} = b_{2,1} = q < p\), with \(p,
    q \in (0,1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We allow self-loops, whose probability are the intra-block connection probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In that case, the matrix \(M\) is the block matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} M = \begin{pmatrix} p J & q J\\ qJ & pJ \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(J \in \mathbb{R}^{n/2 \times n/2}\) is the all-one matrix. We refer
    to this model as the symmetric stochastic blockmodel (SSBM).
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(M\) is symmetric. Hence it has a spectral decomposition. It is
    straightforward to compute. Let \(\mathbf{1}_m\) be the all-one vector of size
    \(m\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Spectral Decomposition of SSBM)** Consider the matrix \(M\) above.
    Let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{n}} \mathbf{1}_n \quad \text{and}
    \quad \mathbf{q}_2 = \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{n/2} \\ -
    \mathbf{1}_{n/2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{q}_3,\ldots,\mathbf{q}_n\) be an orthonormal basis of \((\mathrm{span}\{\mathbf{q}_1,
    \mathbf{q}_2\})^\perp\). Denote by \(Q\) the matrix whose columns are \(\mathbf{q}_1,\ldots,\mathbf{q}_n\).
    Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_1 = \frac{p + q}{2} n \quad \text{and} \quad \lambda_2 = \frac{p
    - q}{2} n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\lambda_3,\ldots,\lambda_n = 0\). Denote by \(\Lambda\) the diagonal matrix
    with diagonal entries \(\lambda_1,\ldots,\lambda_n\).
  prefs: []
  type: TYPE_NORMAL
- en: Then a spectral decomposition of \(M\) is given by
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = Q \Lambda Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, \(\mathbf{q}_i\) is an eigenvector of \(M\) with eigenvalue \(\lambda_i\).
    \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We start with \(\mathbf{q}_1\) and note that by the formula for the
    multiplication of block matrices and the definition of \(J\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} M \mathbf{q}_1 &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \mathbf{1}_n \\ &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}} \\ \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} p J \mathbf{1}_{\frac{n}{2}}
    + q J \mathbf{1}_{\frac{n}{2}}\\ q J \mathbf{1}_{\frac{n}{2}} + p J \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} (p + q) J \mathbf{1}_{\frac{n}{2}}\\
    (p + q) J \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix}
    (p + q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}\\ (p + q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{p + q}{2} \sqrt{n} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}}\\
    \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \lambda_1 \mathbf{q}_1. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} M \mathbf{q}_2 &= \begin{pmatrix} p J & q J\\ q J & p J \end{pmatrix}
    \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}} \\ - \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} p J \mathbf{1}_{\frac{n}{2}}
    - q J \mathbf{1}_{\frac{n}{2}}\\ q J \mathbf{1}_{\frac{n}{2}} - p J \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix} (p - q) J \mathbf{1}_{\frac{n}{2}}\\
    (q - p) J \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \frac{1}{\sqrt{n}} \begin{pmatrix}
    (p - q) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}\\ (q - p) \frac{n}{2} \mathbf{1}_{\frac{n}{2}}
    \end{pmatrix} \\ &= \frac{p - q}{2} \sqrt{n} \begin{pmatrix} \mathbf{1}_{\frac{n}{2}}\\
    - \mathbf{1}_{\frac{n}{2}} \end{pmatrix} \\ &= \lambda_2 \mathbf{q}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Matrix \(M\) has rank 2 since it only has two distinct columns (assuming \(p
    \neq q\)). By the *Spectral Theorem*, there is an orthonormal basis of eigenvectors.
    We have shown that \(\mathbf{q}_1\) and \(\mathbf{q}_2\) are eigenvectors with
    nonzero eigenvalues (again using that \(p \neq q\)). So the remaining eigenvectors
    must form a basis of the orthogonal complement of \(\mathrm{span}\{\mathbf{q}_1,
    \mathbf{q}_2\}\) and they must have eigenvalue \(0\) since they lie in the null
    space. In particular,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_3 = \lambda_4 = \ldots = \lambda_n = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Why is this relevant to graph cutting? We first compute the expected Laplacian
    matrix. For this we need to expected degree of each vertex. This is obtained as
    follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[\delta(1)] = \E\left[\sum_{j=1}^n \mathbf{1}_{\{i,j\} \in E}\right] =
    \sum_{j=1}^n \E[\mathbf{1}_{\{i,j\} \in E}] = \sum_{j=1}^{n/2} p + \sum_{j=n/2}^n
    q = (p + q) \frac{n}{2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we counted the self-loop (if present) once. The same holds for the other
    vertices. So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{L} := \E[L] = \E[D] - \E[A] = (p + q) \frac{n}{2} I_{n \times n}
    - M. \]
  prefs: []
  type: TYPE_NORMAL
- en: For any eigenvector \(\mathbf{q}_i\) of \(M\), we note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{L} \,\mathbf{q}_i = (p + q) \frac{n}{2} I_{n \times n} \mathbf{q}_i
    - M \mathbf{q}_i = \left\{(p + q) \frac{n}{2} - \lambda_i \right\}\mathbf{q}_i,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, \(\mathbf{q}_i\) is also an eigenvector of \(\overline{L}\). Its eigenvalues
    are therefore
  prefs: []
  type: TYPE_NORMAL
- en: \[ (p + q) \frac{n}{2} - \frac{p + q}{2} n = 0, \quad (p + q) \frac{n}{2} -
    \frac{p - q}{2} n = q n, \quad (p + q) \frac{n}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: When \(p > q\), \(q n\) is the second smallest eigenvalue of \(\overline{L}\)
    with corresponding eigenvector
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_2 = \frac{1}{\sqrt{n}} \begin{pmatrix} \mathbf{1}_{n/2}
    \\ - \mathbf{1}_{n/2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The key observation:'
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvector corresponding to the second smallest eigenvalue of \(\overline{L}\)
    perfectly encodes the community structure by assigning \(1/\sqrt{n}\) to the vertices
    in block \(C_1\) and \(-1/\sqrt{n}\) to the vertices in block \(C_2\)!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, in reality, we do not get to observe \(\overline{L}\). Instead we compute
    the actual Laplacian \(L\), a random matrix whose expectation if \(\overline{L}\).
    But it turns out that, for large \(n\), the eigenvectors of \(L\) corresponding
    to its two smallest eigenvalues are close to \(\mathbf{q}_1\) and \(\mathbf{q}_2\).
    Hence we can recover the community structure approximately from \(L\). This is
    far from obvious since \(L\) and \(\overline{L}\) are very different matrices.
  prefs: []
  type: TYPE_NORMAL
- en: A formal proof of this claim is beyond this course. But we illustrate it numerically
    next.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We first construct the block assignment and matrix \(M\)
    in the case of SSBM.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'The eigenvectors and eigenvalues of the Laplacian in this case are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: The first eigenvalue is roughly \(0\) as expected with an eigenvector which
    is proportional to the all-one vector. The second eigenvalue is somewhat close
    to the expected \(q n = 0.2 \cdot 10 = 2\) with an eigenvector that has different
    signs on the two blocks. This is consistent with our prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues exhibit an interesting behavior that is common for random matrices.
    This is easier to see for larger \(n\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/c310cb9e0454e14839e37ace47a3a2e6be6493c406b477b7b6e5d1cc26e660e9.png](../Images/c13e5512c5c6416c42ab21f7a26e3278.png)'
  prefs: []
  type: TYPE_IMG
- en: The first two eigenvalues are close to \(0\) and \(0.2 \cdot 100 = 20\) as expected.
    The rest of the eigenvalues are centered around \( (0.2 + 0.8) 100 /2 = 50\),
    but they are quite spread out, with a density resembling a half-circle. This is
    related to [Wigner’s semicircular law](https://en.wikipedia.org/wiki/Wigner_semicircle_distribution)
    which plays a key role in random matrix theory.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.2\. Weyl’s inequality[#](#weyls-inequality "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We prove an inequality on the sensitivity of eigenvalues which is useful in
    certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: For a symmetric matrix \(C \in \mathbb{R}^{d \times d}\), we let \(\lambda_j(C)\),
    \(j=1, \ldots, d\), be the eigenvalues of \(C\) in non-increasing order with corresponding
    orthonormal eigenvectors \(\mathbf{v}_j\), \(j=1, \ldots, d\). The following lemma
    is one version of what is known as *Weyl’s Inequality*.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Weyl)** Let \(A \in \mathbb{R}^{d \times d}\) and \(B \in \mathbb{R}^{d
    \times d}\) be symmetric matrices. Then, for all \(j=1, \ldots, d\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max_{j \in [d]} \left|\lambda_j(B) - \lambda_j(A)\right| \leq \|B- A\|_2
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\|C\|_2\) is the induced \(2\)-norm of \(C\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the extremal characterization of the eigenvalues together
    with a dimension argument.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For a symmetric matrix \(C \in \mathbb{R}^{d \times d}\), define the
    subspaces'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{V}_k(C) = \mathrm{span}(\mathbf{v}_1, \ldots, \mathbf{v}_k) \quad\text{and}\quad
    \mathcal{W}_{d-k+1}(C) = \mathrm{span}(\mathbf{v}_k, \ldots, \mathbf{v}_d) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{v}_1,\ldots,\mathbf{v}_d\) form an orthonormal basis of eigenvectors
    of \(C\). Let \(H = B - A\). We prove only the upper bound. The other direction
    follows from interchanging the roles of \(A\) and \(B\). Because
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(\mathcal{V}_j(B)) + \mathrm{dim}(\mathcal{W}_{d-j+1}(A)) = j
    + (d-j+1) = d+1 \]
  prefs: []
  type: TYPE_NORMAL
- en: it it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}\left(\mathcal{V}_j(B) \cap \mathcal{W}_{d-j+1}(A)\right) \geq
    d+1 - d = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the \(\mathcal{V}_j(B) \cap \mathcal{W}_{d-j+1}(A)\) is non-empty. Let
    \(\mathbf{v}\) be a unit vector in that intersection.
  prefs: []
  type: TYPE_NORMAL
- en: By *Courant-Fischer*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_j(B) \leq \langle \mathbf{v}, (A+H) \mathbf{v}\rangle = \langle \mathbf{v},
    A \mathbf{v}\rangle + \langle \mathbf{v}, H \mathbf{v}\rangle \leq \lambda_j(A)
    + \langle \mathbf{v}, H \mathbf{v}\rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by *Cauchy-Schwarz*, since \(\|\mathbf{v}\|=1\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}, H \mathbf{v}\rangle \leq \|\mathbf{v}\| \|H\mathbf{v}\|
    \leq \|H\|_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: which proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.3\. Weighted case[#](#weighted-case "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concepts we have introduced can also be extended to weighted graphs, that
    is, graphs with weights on the edges. These weights might be a measure of the
    strength of the connection for instance. In this section, we briefly describe
    this extension, which is the basis for a [discrete calculus](https://en.wikipedia.org/wiki/Calculus_on_finite_weighted_graphs).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Weighted Graph or Digraph)** A weighted graph (or weighted
    digraph) is a triple \(G = (V, E, w)\) where \((V, E)\) is a graph (or directed
    graph) and \(w : E \to \mathbb{R}_+\) is a function that assigns positive real
    weights to the edges. For ease of notation, we write \(w_e = w_{ij} = w(i,j)\)
    for the weight of edge \(e = \{i,j\}\) (or \(e = (i,j)\) in the directed case).
    \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: As we did for graphs, we denote the vertices \(\{1,\ldots, n\}\) and the edges
    \(\{e_1,\ldots, e_{m}\}\), where \(n = |V|\) and \(m =|E|\). Properties of graphs
    can be generalized naturally. For instance, one defines the degree of a vertex
    \(i\) as, in the undirected case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \delta(i) = \sum_{j:\{i,j\} \in E} w_{ij}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in the directed case, the out-degree and in-degree are
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \delta^+(i) = \sum_{j: (i,j) \in E} w_{ij} \qquad \text{and} \qquad \delta^+(i)
    = \sum_{j: (j,i) \in E} w_{ij}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: In the undirected case, the adjacency matrix is generalized as follows. (A similar
    generalization holds for the directed case.)
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Adjacency Matrix for Weighted Graph)** Let \(G = (V, E, w)\)
    be a weighted graph with \(n = |V|\) vertices. The adjacency matrix \(A\) of \(G\)
    is the \(n\times n\) symmetric matrix defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A_{ij} = \begin{cases} w_{ij} & \text{if $\{i,j\} \in E$}\\
    0 & \text{o.w.} \end{cases} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: A similar generalization holds for the directed case.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Adjacency Matrix for Weighted Digraph)** Let \(G = (V, E,
    w)\) be a weighted digraph with \(n = |V|\) vertices. The adjacency matrix \(A\)
    of \(G\) is the \(n\times n\) matrix defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A_{ij} = \begin{cases} w_{ij} & \text{if $(i,j) \in E$}\\ 0
    & \text{o.w.} \end{cases} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Laplacian matrix for weighted graphs** In the case of a weighted graph, the
    Laplacian can then be defined as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Laplacian for Weighted Graph)** Let \(G = (V, E, w)\) be
    a weighted graph with \(n = |V|\) vertices and adjacency matrix \(A\). Let \(D
    = \mathrm{diag}(\delta(1), \ldots, \delta(n))\) be the weighted degree matrix.
    The Laplacian matrix associated to \(G\) is defined as \(L = D - A\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown (Try it!) that the Laplacian quadratic form satisfies in the
    weighted case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{x}, L \mathbf{x} \rangle = \sum_{\{i,j\} \in E} w_{ij} (x_i
    - x_j)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\mathbf{x} = (x_1,\ldots,x_n) \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a positive semidefinite matrix (Exercise: Why?), the weighted Laplacian
    has an orthonormal basis of eigenvectors with nonnegative eigenvalues that satisfy
    the variational characterization we derived above. In particular, if we denote
    the eigenvalues \(0 = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\), it follows from
    *Courant-Fischer* that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 = \min\left\{ \sum_{\{u, v\} \in E} w_{uv} (x_u - x_v)^2 \,:\, \mathbf{x}
    = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{u=1}^n x_u = 0, \sum_{u = 1}^n x_u^2
    = 1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: If we generalize the cut ratio as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi(S) = \frac{\sum_{i \in S, j \in S^c} w_{ij}}{\min\{|S|, |S^c|\}} \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\emptyset \neq S \subset V\) and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi_G = \min\left\{ \phi(S)\,:\, \emptyset \neq S \subset V \right\} \]
  prefs: []
  type: TYPE_NORMAL
- en: it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_2 \leq 2 \phi_G \]
  prefs: []
  type: TYPE_NORMAL
- en: as in the unweighted case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized Laplacian** Other variants of the Laplacian matrix have also been
    studied. We introduced the normalized Laplacian next. Recall that in the weighted
    case, the degree is defined as \(\delta(i) = \sum_{j:\{i,j\} \in E} w_{i,j}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Normalized Laplacian)** The normalized Laplacian of \(G =
    (V,E,w)\) with adjacency matrix \(A\) and degree matrix \(D\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{-1/2} A D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Using our previous observations about multiplication by diagonal matrices, the
    entries of \(\mathcal{L}\) are
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathcal{L})_{i,j} = (I - (D^{-1/2} A D^{-1/2})_{i,j} = 1 - \frac{a_{i,j}}{\sqrt{\delta(i)
    \delta(j)}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We also note the following relation to the Laplacian matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = D^{-1/2} L D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We check that the normalized Laplacian is symmetric:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathcal{L}^T &= I^T - (D^{-1/2} A D^{-1/2})^T\\ &= I - (D^{-1/2})^T
    A^T (D^{-1/2})^T\\ &= I - D^{-1/2} A D^{-1/2}\\ &= \mathcal{L}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: It is also positive semidefinite. Indeed,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T \mathcal{L} \mathbf{x} = \mathbf{x}^T D^{-1/2} L D^{-1/2} \mathbf{x}
    = (D^{-1/2} \mathbf{x})^T L (D^{-1/2} \mathbf{x}) \geq 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the properties of the Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Hence by the *Spectral Theorem*, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \sum_{i=1}^n \eta_i \mathbf{z}_i \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{z}_i\)s are orthonormal eigenvectors of \(\mathcal{L}\)
    and the eigenvalues satisfy \(0 \leq \eta_1 \leq \eta_2 \leq \cdots \leq \eta_n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'One more observation: because the constant vector is eigenvector of \(L\) with
    eigenvalue \(0\), we get that \(D^{1/2} \mathbf{1}\) is an eigenvector of \(\mathcal{L}\)
    with eigenvalue \(0\). So \(\eta_1 = 0\) and we set'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{z}_1)_i = \left(\frac{D^{1/2} \mathbf{1}}{\|D^{1/2} \mathbf{1}\|_2}\right)_i
    = \sqrt{\frac{\delta(i)}{\sum_{i\in V} \delta(i)}}, \quad \forall i \in [n], \]
  prefs: []
  type: TYPE_NORMAL
- en: which makes \(\mathbf{z}_1\) into a unit norm vector.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship to the Laplacian matrix immediately implies (prove it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^T \mathcal{L} \mathbf{x} = \sum_{\{i,j\} \in E} w_{ij} \left(\frac{x_i}{\sqrt{\delta(i)}}
    - \frac{x_j}{\sqrt{\delta(j)}}\right)^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\mathbf{x} = (x_1,\ldots,x_n)^T \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: Through the change of variables
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i = \frac{x_i}{\sqrt{\delta(i)}}, \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Courant-Fischer* gives this time (Why?)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_2 = \min\left\{ \sum_{\{u, v\} \in E} w_{uv} (y_u - y_v)^2 \,:\, \mathbf{y}
    = (y_1, \ldots, y_n)^T \in \mathbb{R}^n, \sum_{u=1}^n \delta(u) y_u = 0, \sum_{u
    = 1}^n \delta(u) y_u^2 = 1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: For a subset of vertices \(S \subseteq V\), let
  prefs: []
  type: TYPE_NORMAL
- en: \[ |S|_w = \sum_{i \in S} \delta(i), \]
  prefs: []
  type: TYPE_NORMAL
- en: which we refer to as the volume of \(S\). It is measure of the size of \(S\)
    weighted by the degrees.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider the normalized cut ratio, or bottleneck ratio,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi^N(S) = \frac{\sum_{i \in S, j \in S^c} w_{ij}}{\min\{|S|_w, |S^c|_w\}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\emptyset \neq S \subset V\) and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi^N_G = \min\left\{ \phi^N(S)\,:\, \emptyset \neq S \subset V \right\}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: it can be shown (Try it!) that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_2 \leq 2 \phi^N_G. \]
  prefs: []
  type: TYPE_NORMAL
- en: The normalized cut ratio is similar to the cut ratio, except that the notion
    of balance of the cut is measured in terms of volume. Note that this concept is
    also useful in the unweighted case.
  prefs: []
  type: TYPE_NORMAL
- en: We will an application of the normalized Laplacian later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2.4\. Image segmentation[#](#image-segmentation "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We give a different, more involved application of the ideas developed in this
    topic to image segmentation. Let us quote Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, image segmentation is the process of partitioning a digital
    image into multiple segments (sets of pixels, also known as image objects). The
    goal of segmentation is to simplify and/or change the representation of an image
    into something that is more meaningful and easier to analyze. Image segmentation
    is typically used to locate objects and boundaries (lines, curves, etc.) in images.
    More precisely, image segmentation is the process of assigning a label to every
    pixel in an image such that pixels with the same label share certain characteristics.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Throughout, we will use the [`scikit-image`](https://scikit-image.org) library
    for processing images.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: As an example, here is a picture of cell nuclei taken through optical microscopy
    as part of some medical experiment. It is taken from [here](https://www.kaggle.com/c/data-science-bowl-2018/data).
    Here we used the function [`skimage.io.imread`](https://scikit-image.org/docs/dev/api/skimage.io.html#skimage.io.imread)
    to load an image from file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/97466aec7bce35e54824c2c5e06887867f2842611eaf3732aeddf742f92cd10a.png](../Images/b66f83b52d9b11d0de7f40288586ce34.png)'
  prefs: []
  type: TYPE_IMG
- en: Suppose that, as part of this experiment, we have a large number of such images
    and need to keep track of the cell nuclei in some way (maybe count how many there
    are, or track them from frame to frame). A natural pre-processing step is to identify
    the cell nuclei in the image. We use image segmentation for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to the example below. Let us start with some further examples.
  prefs: []
  type: TYPE_NORMAL
- en: We will first work with the following [map of Wisconsin regions](https://www.dhs.wisconsin.gov/areaadmin/index.htm).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/01522a0eaaf3a16ba3949def4e39f361f25796dc87411fa8ec5eab3c2899f2ce.png](../Images/b3847f3c93be42a38b89f12fdc9dbb12.png)'
  prefs: []
  type: TYPE_IMG
- en: A color image such as this one is encoded as a \(3\)-dimensional array (or [tensor](https://en.wikipedia.org/wiki/Tensor)),
    meaning that it is an array with \(3\) indices (unlike matrices which have only
    two indices).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: The first two indices capture the position of a pixel. The third index capture
    the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model). Put differently,
    each pixel in the image has three numbers (between 0 and 255) attached to it that
    encodes its color.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, at position \((300,400)\) the RGB color is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/13b8e08064348a7fea064b045068e2d0414b2d1683ceddb1bc8773f2db0ede76.png](../Images/c1276f6e98c715b5fb7749fec4e9e8f4.png)'
  prefs: []
  type: TYPE_IMG
- en: To perform image segmentation using the spectral graph theory we have developed,
    we transform our image into a graph.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to coarsen the image by creating super-pixels, or regions
    of pixels that are close and have similar color. For this purpose, we will use
    [`skimage.segmentation.slic`](https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.slic),
    which in essence uses \(k\)-means clustering on the color space to identify blobs
    of pixels that are in close proximity and have similar colors. It takes as imput
    a number of super-pixels desired (`n_segments`), a compactness parameter (`compactness`)
    and a smoothing parameter (`sigma`). The output is a label assignment for each
    pixel in the form of a \(2\)-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the choice of the parameter `compactness` via [scikit-image](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic):'
  prefs: []
  type: TYPE_NORMAL
- en: Balances color proximity and space proximity. Higher values give more weight
    to space proximity, making superpixel shapes more square/cubic. This parameter
    depends strongly on image contrast and on the shapes of objects in the image.
    We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10,
    100, before refining around a chosen value.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The parameter `sigma` controls the level of [blurring](https://en.wikipedia.org/wiki/Gaussian_blur)
    applied to the image as a pre-processing step. In practice, experimentation is
    required to choose good parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: A neat way to vizualize the super-pixels is to use the function [`skimage.color.label2rgb`](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb)
    which takes as input an image and an array of labels. In the mode `kind='avg'`,
    it outputs a new image where the color of each pixel is replaced with the average
    color of its label (that is, the average of the RGB color over all pixels with
    the same label). As they say, an image is worth a thousand words - let’s just
    see what it does.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/7959aeea4b885bae42571d1446beddae6b8272cb0a592785efe289c5b481d97d.png](../Images/e3129bc4814ed40c14212f0ea836da19.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that our goal is to turn our original image into a graph. After the first
    step of creating super-pixels, the second step is to form a graph whose nodes
    are the super-pixels. Edges are added between adjacent super-pixels and a weight
    is given to each edge which reflects the difference in mean color between the
    two.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use [`skimage.graph.rag_mean_color`](https://scikit-image.org/docs/stable/api/skimage.graph.html#skimage.graph.rag_mean_color).
    In mode `similarity`, it uses the following weight formula (quoting the documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: The weight between two adjacent regions is exp(-d^2/sigma) where d=|c1-c2|,
    where c1 and c2 are the mean colors of the two regions. It represents how similar
    two regions are.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The output, which is known as a region adjacency graph (RAG), is a `NetworkX`
    graph and can be manipulated using that package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/230bc75d3f1084bc07ca478f991bd5ced705add22482e96a084317bb211fb081.png](../Images/bb37436c8158e8a34ec74fa221b117a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '`scikit-image` also provides a more effective way of vizualizing a RAG, using
    the function [`skimage.future.graph.show_rag`](https://scikit-image.org/docs/dev/api/skimage.future.graph.html#skimage.future.graph.show_rag).
    Here the graph is super-imposed on the image and the edge weights are depicted
    by their color.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/8a74a1138feb384576ca2d04f1969c62068ac89d8007bd6c60571b02238c378d.png](../Images/849c34aaf5b7ff0e0edbd7a96bef3e4f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can apply the spectral clustering techniques we have developed in this chapter.
    Next we compute a spectral decomposition of the weighted Laplacian and plot the
    eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/35f7fdf29a2fda4096b00760443b1d7aedeceb12bdb718235b397a5193cff774.png](../Images/7cd4aa1ca84d37bfcc939bc25f473504.png)'
  prefs: []
  type: TYPE_IMG
- en: From the theory, this suggests that there are roughly 15 components in this
    graph. We project to \(15\) dimensions and apply \(k\)-means clustering to find
    segments. Rather than using our own implementation, we use [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    from the [`scikit-learn`](https://scikit-learn.org/stable/index.html) library.
    That implementation uses the [\(k\)-means\(++\)](https://en.wikipedia.org/wiki/K-means%2B%2B)
    initialization, which is particularly effective in practice. A label assignment
    for each node can be accessed using `labels_`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'To vizualize the segmentation, we assign to each segment (i.e., collection
    of super-pixels) a random color. This can be done using [`skimage.color.label2rgb`](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb)
    again, this time in mode `kind=''overlay''`. First, we assign to each pixel from
    the original image its label under this clustering. Recall that `labels1` assigns
    to each pixel its super-pixel (represented by a node of the RAG), so that applying
    `assign_seg` element-wise to `labels1` results is assigning a cluster to each
    pixel. In code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/5aabdebf592677a0ae025884f681391b4f658489872b6a3d2b0ac134372946b4.png](../Images/d35fc1e0118ce41fd83d724b6ed4ed0b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the result is reasonable but far from perfect.
  prefs: []
  type: TYPE_NORMAL
- en: For ease of use, we encapsulate the main steps above in sub-routines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try a more complicated image. This one is taken from [here](https://www.reddit.com/r/aww/comments/169s6e/badgers_can_be_cute_too/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ffbfad2bbecf61e6ece848ef5ae22a7637c7fc18d200b285ae234c3468c213b2.png](../Images/1b8ad9014f9e0ecc4b6afbad4deca689.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that the choice of parameters requires significant fidgeting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/4d5e0ff3eed959013ca469b82626364e171eeee2de5a4e5e98111f328f4b67ac.png](../Images/e61b90b863a2bb5d77a156780ddc4898.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9d2a898681451451a2db82660a24f7d69757b43f55f4314ccd08161ec7f528ea.png](../Images/ab217ef799ccdb23d1ef49e871bc0379.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/439663d1e8381a740ff9db0be691c77db1a1eedbda596696d6f03b481d55ed43.png](../Images/0915e41ec9a5be32f78662924ffd0775.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, the results are far from perfect - but not unreasonable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we return to our medical example. We first reload the image and find
    super-pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e78b4d7d976169569b2d8223c75fc6a4bfddef251d88a030ebc48acd4dc02a22.png](../Images/aebbc424dbdb162039dbd74dd736c90a.png)'
  prefs: []
  type: TYPE_IMG
- en: We then form the weighted Laplacian and plot its eigenvalues. This time, about
    \(40\) dimensions seem appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9e8a00b5de0f0b92c8929fa508140d83f21bb7aba6c8accee6efd3b5b1681859.png](../Images/c6a0760eba98be16f4b3c9540e8f0519.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/3b2c73655508b73097ed97881a720ff130f1d5ac41e1a12c10c87237952d3ef3.png](../Images/afe3edc4d3fc31871e23e544fb4c2542.png)'
  prefs: []
  type: TYPE_IMG
- en: This method is quite finicky. The choice of parameters affects the results significantly.
    You should see for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: We mention that `scikit-image` has an implementation of a closely related method,
    Normalized Cut, [`skimage.graph.cut_normalized`](https://scikit-image.org/docs/dev/api/skimage.graph.html#skimage.graph.cut_normalized).
    Rather than performing \(k\)-means after projection, it recursively performs \(2\)-way
    cuts on the RAG and resulting subgraphs.
  prefs: []
  type: TYPE_NORMAL
- en: We try it next. The results are similar as you can see.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e09c9c323359a039a1f8bdac677f17df8c556b94f5811ba25723a203b5adf9d8.png](../Images/e021d21059fceddb156d0c352273b8a2.png)'
  prefs: []
  type: TYPE_IMG
- en: There are many other image segmentation methods. See for example [here](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#module-skimage.segmentation).
  prefs: []
  type: TYPE_NORMAL
