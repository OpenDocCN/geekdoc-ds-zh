<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>1.6. Online supplementary materials#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>1.6. Online supplementary materials#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap01_intro/supp/roch-mmids-intro-supp.html">https://mmids-textbook.github.io/chap01_intro/supp/roch-mmids-intro-supp.html</a></blockquote>

<section id="quizzes-solutions-code-etc">
<h2><span class="section-number">1.6.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">1.6.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_intro_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">1.6.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_2.html">Section 1.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_3.html">Section 1.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_4.html">Section 1.4</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">1.6.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-intro-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-intro-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">1.6.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E1.2.1: The Euclidean norm <span class="math notranslate nohighlight">\(\|\mathbf{x}\|_2\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2} = \sqrt{6^2 + 8^2} = \sqrt{36 + 64} = \sqrt{100} = 10.
\]</div>
<p>Answer and justification for E1.2.3: The transpose <span class="math notranslate nohighlight">\(A^T\)</span> is obtained by switching rows and columns</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix}1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6\end{pmatrix}
\end{split}\]</div>
<p>Answer and justification for E1.2.5: A matrix <span class="math notranslate nohighlight">\(A\)</span> is symmetric if <span class="math notranslate nohighlight">\(A = A^T\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix}2 &amp; 0 \\ 0 &amp; 3\end{pmatrix} = A.
\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(A\)</span> is symmetric.</p>
<p>Answer and justification for E1.2.7: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = \lim_{h \to 0} \frac{f(x+h, y) - f(x, y)}{h} = \lim_{h \to 0} \frac{(x+h)^2 + (x+h)y + y^2 - (x^2 + xy + y^2)}{h} = \lim_{h \to 0} \frac{2xh + h^2 + hy}{h} = 2x + y\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = \lim_{h \to 0} \frac{f(x, y+h) - f(x, y)}{h} = \lim_{h \to 0} \frac{x^2 + x(y+h) + (y+h)^2 - (x^2 + xy + y^2)}{h} = \lim_{h \to 0} \frac{xh + 2yh + h^2}{h} = x + 2y\)</span>.</p>
<p>Answer and justification E1.2.9: By <em>Taylor’s Theorem</em>, <span class="math notranslate nohighlight">\(f(x) = f(a) + (x - a)f'(a) + \frac{1}{2}(x - a)^2f''(\xi)\)</span> for some <span class="math notranslate nohighlight">\(\xi\)</span> between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(x\)</span>. Here, <span class="math notranslate nohighlight">\(f(1) = 1^3 - 3 \cdot 1^2 + 2 \cdot 1 = 0\)</span>, <span class="math notranslate nohighlight">\(f'(x) = 3x^2 - 6x + 2\)</span>, so <span class="math notranslate nohighlight">\(f'(1) = 3 - 6 + 2 = -1\)</span>, and <span class="math notranslate nohighlight">\(f''(x) = 6x - 6\)</span>. Therefore,
<span class="math notranslate nohighlight">\(f(x) = 0 + (x - 1)(-1) + \frac{1}{2}(x - 1)^2(6\xi - 6)\)</span> for some <span class="math notranslate nohighlight">\(\xi\)</span> between <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Answer and justification for E1.2.11: First, compute <span class="math notranslate nohighlight">\(\E[X^2]\)</span></p>
<div class="math notranslate nohighlight">
\[
\E[X^2] = \sum_{x} x^2 \cdot P(X = x) = 1^2 \cdot 0.4 + 2^2 \cdot 0.6 = 0.4 + 4 \cdot 0.6 = 0.4 + 2.4 = 2.8.
\]</div>
<p>Then, use the formula <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = \E[X^2] - (\E[X])^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[X] = 2.8 - (1.6)^2 = 2.8 - 2.56 = 0.24.
\]</div>
<p>Answer and justification for E1.2.13: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(\P[|X - \mathbb{E}[X]| \geq \alpha] \leq \frac{\mathrm{Var}[X]}{\alpha^2}\)</span> for any <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>. Here, <span class="math notranslate nohighlight">\(\alpha = 4\)</span>, so
<span class="math notranslate nohighlight">\(\P[|X - 3| \geq 4] \leq \frac{4}{4^2} = \frac{1}{4}\)</span>.</p>
<p>Answer and justification for E1.2.15: The covariance matrix of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}\)</span>. Since <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are independent, their covariance is zero. The variance of each is 1 since they are standard normal.</p>
<p>Answer and justification for E1.2.17: <span class="math notranslate nohighlight">\(\mathbb{E}[AX] = A\mathbb{E}[X] = A\mu_X = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 5 \\ 11 \end{pmatrix}\)</span>, <span class="math notranslate nohighlight">\(\text{Cov}[AX] = A\text{Cov}[X]A^T = A\Sigma_XA^T = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 4 \end{pmatrix} \begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix} = \begin{pmatrix} 11 &amp; 19 \\ 19 &amp; 35 \end{pmatrix}\)</span></p>
<p>Answer and justification for E1.2.19: For any non-zero vector <span class="math notranslate nohighlight">\(z = \begin{pmatrix} z_1 \\ z_2 \end{pmatrix}\)</span>, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z^TAz &amp;= \begin{pmatrix} z_1 &amp; z_2 \end{pmatrix} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \begin{pmatrix} z_1 \\ z_2 \end{pmatrix} \\
&amp;= \begin{pmatrix} z_1 &amp; z_2 \end{pmatrix} \begin{pmatrix} 2z_1 - z_2 \\ -z_1 + 2z_2 \end{pmatrix} \\
&amp;= 2z_1^2 - 2z_1z_2 + 2z_2^2 \\
&amp;= z_1^2 + (z_1 - z_2)^2 + z_2^2 &gt; 0
\end{align*}\]</div>
<p>since <span class="math notranslate nohighlight">\((z_1 - z_2)^2 \geq 0\)</span>, and either <span class="math notranslate nohighlight">\(z_1^2 &gt; 0\)</span> or <span class="math notranslate nohighlight">\(z_2^2 &gt; 0\)</span> (since <span class="math notranslate nohighlight">\(z \neq 0\)</span>). Therefore, <span class="math notranslate nohighlight">\(A\)</span> is positive definite.</p>
<p>Answer and justification for E1.3.1: <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^* = \frac{1}{2}(\mathbf{x}_1 + \mathbf{x}_4) = (\frac{3}{2}, 1)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2^* = \frac{1}{2}(\mathbf{x}_2 + \mathbf{x}_3) = (-\frac{1}{2}, 0)\)</span>.</p>
<p>Answer and justification for E1.3.3: <span class="math notranslate nohighlight">\(C_1 = \{1, 5\}\)</span>, <span class="math notranslate nohighlight">\(C_2 = \{3\}\)</span>, <span class="math notranslate nohighlight">\(C_3 = \{2, 4\}\)</span>.</p>
<p>Answer and justification for E1.3.5: Expanding the squared norms and cancelling terms yields the equivalent inequality.</p>
<p>Answer and justification for E1.3.7: Expand <span class="math notranslate nohighlight">\(\|A + B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (A_{ij} + B_{ij})^2\)</span> and simplify.</p>
<p>Answer and justification for E1.3.9:  <span class="math notranslate nohighlight">\(q(x) = 3(x-1)^2 + 2\)</span>, minimum value is 2 at <span class="math notranslate nohighlight">\(x = 1\)</span>.</p>
<p>Answer and justification for E1.3.11: <span class="math notranslate nohighlight">\(\|A\|_F = \sqrt{14}\)</span>.</p>
<p>Answer and justification for E1.4.1: Since <span class="math notranslate nohighlight">\(X_i\)</span> is uniformly distributed on <span class="math notranslate nohighlight">\([-1/2, 1/2]\)</span>, its probability density function is <span class="math notranslate nohighlight">\(f_{X_i}(x) = 1\)</span> for <span class="math notranslate nohighlight">\(x \in [-1/2, 1/2]\)</span> and 0 otherwise. Therefore,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X_i] = \int_{-1/2}^{1/2} x f_{X_i}(x) dx = \int_{-1/2}^{1/2} x dx = 0\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\text{Var}[X_i] = \mathbb{E}[X_i^2] - (\mathbb{E}[X_i])^2 = \int_{-1/2}^{1/2} x^2 dx = \frac{1}{12}.\]</div>
<p>Answer and justification for E1.4.3: We have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\|X\|^2] = \mathbb{E}\left[\sum_{i=1}^d X_i^2\right] = \sum_{i=1}^d \mathbb{E}[X_i^2] = \sum_{i=1}^d 1 = d.\]</div>
<p>Answer and justification for E1.4.5: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(P[|X - 1| \geq 3] \leq \frac{\mathrm{Var}[X]}{3^2} = \frac{4}{9}\)</span>.</p>
<p>Answer and justification for E1.4.7: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(\P[|X| \geq 2\sqrt{d}] \leq \frac{\mathrm{Var}[X]}{(2\sqrt{d})^2} = \frac{1}{4d}\)</span>. For <span class="math notranslate nohighlight">\(d = 1\)</span>, <span class="math notranslate nohighlight">\(\P[|X| \geq 2] \leq \frac{1}{4}\)</span>. For <span class="math notranslate nohighlight">\(d = 10\)</span>, <span class="math notranslate nohighlight">\(P[|X| \geq 2\sqrt{10}] \leq \frac{1}{40}\)</span>. For <span class="math notranslate nohighlight">\(d = 100\)</span>, <span class="math notranslate nohighlight">\(\P[|X| \geq 20] \leq \frac{1}{400}\)</span>.</p>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">1.6.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Compute norms of vectors and matrices, and inner products between vectors.</p></li>
<li><p>Utilize properties of norms and inner products, including the Cauchy-Schwarz inequality and the triangle inequality.</p></li>
<li><p>Perform basic matrix operations, including addition, scalar multiplication, multiplication, and transposition.</p></li>
<li><p>Compute matrix-vector and matrix-matrix products and interpret their geometric and algebraic significance.</p></li>
<li><p>Explain the concept of a descent direction and its relationship to optimization.</p></li>
<li><p>Use Taylor’s Theorem to approximate functions and analyze the error terms in these approximations.</p></li>
<li><p>Define and compute partial derivatives and gradients for functions of multiple variables.</p></li>
<li><p>State and apply Chebyshev’s inequality to quantify the concentration of a random variable around its mean.</p></li>
<li><p>Compute the expectation, variance, and covariance of random variables and vectors.</p></li>
<li><p>Define and give examples of positive semidefinite matrices.</p></li>
<li><p>Explain the properties and significance of the covariance matrix of a random vector.</p></li>
<li><p>Describe and generate samples from a spherical Gaussian distribution.</p></li>
<li><p>Write Python code to compute norms, inner products, and perform matrix operations.</p></li>
<li><p>Use Python to simulate random variables, calculate their statistical properties, and visualize distributions and the law of large numbers.</p></li>
<li><p>Formulate the k-means clustering problem as an optimization problem.</p></li>
<li><p>Describe the k-means algorithm and its iterative steps for finding cluster centers and assignments.</p></li>
<li><p>Derive the optimal cluster representatives (centroids) for a fixed partition.</p></li>
<li><p>Derive the optimal partition (cluster assignments) for fixed representatives.</p></li>
<li><p>Analyze the convergence properties of the k-means algorithm and understand its limitations in finding global optima.</p></li>
<li><p>Apply the k-means algorithm to real-world datasets, such as the penguins dataset, and interpret the results.</p></li>
<li><p>Express the k-means objective function in matrix form and relate it to matrix factorization.</p></li>
<li><p>Understand the importance of data preprocessing steps like standardization for k-means.</p></li>
<li><p>Recognize the limitations of k-means, such as its sensitivity to initialization and the need to specify the number of clusters in advance.</p></li>
<li><p>Discuss the challenges of determining the optimal number of clusters in k-means clustering.</p></li>
<li><p>Understand the challenges of clustering high-dimensional data using the k-means algorithm and recognize how the increasing dimensionality can cause the noise to overwhelm the signal, making it difficult to distinguish between clusters.</p></li>
<li><p>Comprehend the concept of concentration of measure in high-dimensional spaces, specifically the counterintuitive fact that most of the volume of a high-dimensional cube is concentrated in its corners, making it appear like a “spiky ball.”</p></li>
<li><p>Apply Chebyshev’s inequality to derive the probability that a randomly selected point from a high-dimensional cube falls within the inscribed ball, and understand how this probability decreases as the dimensionality increases.</p></li>
<li><p>Analyze the behavior of the norm of a standard Normal vector in high-dimensional spaces and prove, using Chebyshev’s inequality, that the norm is highly likely to be close to the square root of the dimension, despite the joint probability density function being maximized at the origin.</p></li>
<li><p>Use Chebyshev’s inequality to derive probabilistic bounds in high-dimensional settings.</p></li>
<li><p>Simulate high-dimensional data to empirically verify theoretical results.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
</section>
<section id="additional-sections">
<h2><span class="section-number">1.6.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="a-more-rigorous-analysis-of-clustering-in-high-dimension">
<h3><span class="section-number">1.6.2.1. </span>A more rigorous analysis of clustering in high dimension<a class="headerlink" href="#a-more-rigorous-analysis-of-clustering-in-high-dimension" title="Link to this heading">#</a></h3>
<p>In this optional section, we give one formal statement of the phenomenon described in the previous subsection.</p>
<p><strong>THEOREM</strong> Let <span class="math notranslate nohighlight">\(\mathbf{X}_1, \mathbf{X}_2, \mathbf{Y}_1\)</span> be independent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with mean <span class="math notranslate nohighlight">\(-w_d \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>, where <span class="math notranslate nohighlight">\(\{w_d\}\)</span> is a monotone sequence in <span class="math notranslate nohighlight">\(d\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{Y}_2\)</span> be an indepedent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean <span class="math notranslate nohighlight">\(w_d \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. Then, letting <span class="math notranslate nohighlight">\(\Delta_d = \|\mathbf{Y}_1 - \mathbf{Y}_2\|^2 - \|\mathbf{X}_1 - \mathbf{X}_2\|^2\)</span>, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\mathbb{E}[\Delta_d]}{\sqrt{\mathrm{Var}[\Delta_d]}} \to
\begin{cases}
0, &amp; \text{if $w_d \ll d^{1/4}$}\\
+\infty, &amp; \text{if $w_d \gg d^{1/4}$}
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_d \ll d^{1/4}\)</span> means <span class="math notranslate nohighlight">\(w_d/d^{1/4} \to 0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The ratio is the statement is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio">signal-to-noise ratio</a>.</p>
<p>To prove the claim, we will need the following property.</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(W\)</span> be a real-valued random variable symmetric about zero, that is, such that <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(-W\)</span> are identically distributed. Then for all odd <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[W^k] = 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By the symmetry,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[W^k] = \mathbb{E}[(-W)^k] = \mathbb{E}[(-1)^k W^k] = - \mathbb{E}[W^k].
\]</div>
<p>The only way to satisfy this equation is to have <span class="math notranslate nohighlight">\(\mathbb{E}[W^k] = 0\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Returning to the proof of the claim:</p>
<p><em>Proof idea:</em> <em>(Theorem)</em> The only coordinate contributing to <span class="math notranslate nohighlight">\(\mathbb{E}[\Delta_d]\)</span> is the first one by linearity of expectation, while all coordinates contribute to <span class="math notranslate nohighlight">\(\mathrm{Var}[\Delta_d]\)</span>. More specifically, a calculation shows that the former is <span class="math notranslate nohighlight">\(c_0 w^2\)</span> while the latter is <span class="math notranslate nohighlight">\(c_1 w^2 + c_2 d\)</span>, where <span class="math notranslate nohighlight">\(c_0, c_1, c_2\)</span> are constants.</p>
<p><em>Proof:</em> <em>(Claim)</em> Write <span class="math notranslate nohighlight">\(w := w_d\)</span> and <span class="math notranslate nohighlight">\(\Delta := \Delta_d\)</span> to simplify the notation. There are two steps:</p>
<p><em>(1) Expectation of <span class="math notranslate nohighlight">\(\Delta\)</span>:</em> By defintion, the random variables <span class="math notranslate nohighlight">\(X_{1,i} - X_{2,i}\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots, d\)</span>, and <span class="math notranslate nohighlight">\(Y_{1,i} - Y_{2,i}\)</span>, <span class="math notranslate nohighlight">\(i = 2,\ldots, d\)</span>, are identically distributed. So, by linearity of expectation,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}[\Delta]
&amp;= \sum_{i=1}^d \mathbb{E}[(Y_{1,i} - Y_{2,i})^2] - \sum_{i=1}^d \mathbb{E}[(X_{1,i} - X_{2,i})^2]\\
&amp;= \mathbb{E}[(Y_{1,1} - Y_{2,1})^2] - \mathbb{E}[(X_{1,1} - X_{2,1})^2].
\end{align*}\]</div>
<p>Further, we can write <span class="math notranslate nohighlight">\(Y_{1,1} - Y_{1,2} \sim (Z_1 -w) - (Z_2+w)\)</span> where <span class="math notranslate nohighlight">\(Z_1, Z_2 \sim N(0,1)\)</span> are independent, where here <span class="math notranslate nohighlight">\(\sim\)</span> indicates equality in distribution. Hence, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}[(Y_{1,1} - Y_{2,1})^2]
&amp;= \mathbb{E}[(Z_1 - Z_2 - 2w)^2]\\
&amp;= \mathbb{E}[(Z_1 - Z_2)^2] 
- 4w \,\mathbb{E}[Z_1 - Z_2]
+ 4 w^2.
\end{align*}\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(X_{1,1} - X_{1,2} \sim Z_1 - Z_2\)</span> so <span class="math notranslate nohighlight">\(\mathbb{E}[(X_{1,1} - X_{2,1})^2] = \mathbb{E}[(Z_1 - Z_2)^2]\)</span>. Since <span class="math notranslate nohighlight">\(\mathbb{E}[Z_1 - Z_2] = 0\)</span>, we finally get <span class="math notranslate nohighlight">\(\mathbb{E}[\Delta] = 4 w^2\)</span>.</p>
<p><em>(2) Variance of <span class="math notranslate nohighlight">\(\Delta\)</span>:</em> Using the observations from (1) and the independence of the coordinates we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[\Delta]
&amp;= \sum_{i=1}^d \mathrm{Var}[(Y_{1,i} - Y_{2,i})^2] + \sum_{i=1}^d \mathrm{Var}[(X_{1,i} - X_{2,i})^2]\\
&amp;= \mathrm{Var}[(Z_1 - Z_2 - 2w)^2] 
+ (2d-1) \,\mathrm{Var}[(Z_1 - Z_2)^2].
\end{align*}\]</div>
<p>By the <em>Variance of a Sum</em>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[(Z_1 - Z_2 - 2w)^2]
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2 - 4w(Z_1 - Z_2) + 4w^2]\\ 
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2 - 4w(Z_1 - Z_2)]\\
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2] 
+ 16 w^2 \mathrm{Var}[Z_1 - Z_2]\\
&amp;\quad - 8w \,\mathrm{Cov}[(Z_1 - Z_2)^2, Z_1 - Z_2].
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(Z_1\)</span> and <span class="math notranslate nohighlight">\(Z_2\)</span> are independent, <span class="math notranslate nohighlight">\(\mathrm{Var}[Z_1 - Z_2]
= \mathrm{Var}[Z_1] + \mathrm{Var}[Z_2] = 2\)</span>. Moreover, the random variable <span class="math notranslate nohighlight">\((Z_1 - Z_2)\)</span> is symmetric, so</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[(Z_1 - Z_2)^2, Z_1 - Z_2]
&amp;= \mathbb{E}[(Z_1 - Z_2)^3]\\ 
&amp; \quad - \mathbb{E}[(Z_1 - Z_2)^2] \,\mathbb{E}[Z_1 - Z_2]\\
&amp;= 0.
\end{align*}\]</div>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[\Delta]
= 32 w^2 
+ 2d \,\mathrm{Var}[(Z_1 - Z_2)^2]
\]</div>
<p><em>Putting everything together:</em></p>
<div class="math notranslate nohighlight">
\[
\frac{\mathbb{E}[\Delta]}{\sqrt{\mathrm{Var}[\Delta]}}
=
\frac{4 w^2}{\sqrt{32 w^2 
+ 2d \,\mathrm{Var}[(Z_1 - Z_2)^2]}}.
\]</div>
<p>Taking <span class="math notranslate nohighlight">\(d \to +\infty\)</span> gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
</section>
&#13;

<h2><span class="section-number">1.6.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">1.6.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_intro_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">1.6.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_2.html">Section 1.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_3.html">Section 1.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_4.html">Section 1.4</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">1.6.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-intro-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-intro-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">1.6.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E1.2.1: The Euclidean norm <span class="math notranslate nohighlight">\(\|\mathbf{x}\|_2\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2} = \sqrt{6^2 + 8^2} = \sqrt{36 + 64} = \sqrt{100} = 10.
\]</div>
<p>Answer and justification for E1.2.3: The transpose <span class="math notranslate nohighlight">\(A^T\)</span> is obtained by switching rows and columns</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix}1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6\end{pmatrix}
\end{split}\]</div>
<p>Answer and justification for E1.2.5: A matrix <span class="math notranslate nohighlight">\(A\)</span> is symmetric if <span class="math notranslate nohighlight">\(A = A^T\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix}2 &amp; 0 \\ 0 &amp; 3\end{pmatrix} = A.
\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(A\)</span> is symmetric.</p>
<p>Answer and justification for E1.2.7: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = \lim_{h \to 0} \frac{f(x+h, y) - f(x, y)}{h} = \lim_{h \to 0} \frac{(x+h)^2 + (x+h)y + y^2 - (x^2 + xy + y^2)}{h} = \lim_{h \to 0} \frac{2xh + h^2 + hy}{h} = 2x + y\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = \lim_{h \to 0} \frac{f(x, y+h) - f(x, y)}{h} = \lim_{h \to 0} \frac{x^2 + x(y+h) + (y+h)^2 - (x^2 + xy + y^2)}{h} = \lim_{h \to 0} \frac{xh + 2yh + h^2}{h} = x + 2y\)</span>.</p>
<p>Answer and justification E1.2.9: By <em>Taylor’s Theorem</em>, <span class="math notranslate nohighlight">\(f(x) = f(a) + (x - a)f'(a) + \frac{1}{2}(x - a)^2f''(\xi)\)</span> for some <span class="math notranslate nohighlight">\(\xi\)</span> between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(x\)</span>. Here, <span class="math notranslate nohighlight">\(f(1) = 1^3 - 3 \cdot 1^2 + 2 \cdot 1 = 0\)</span>, <span class="math notranslate nohighlight">\(f'(x) = 3x^2 - 6x + 2\)</span>, so <span class="math notranslate nohighlight">\(f'(1) = 3 - 6 + 2 = -1\)</span>, and <span class="math notranslate nohighlight">\(f''(x) = 6x - 6\)</span>. Therefore,
<span class="math notranslate nohighlight">\(f(x) = 0 + (x - 1)(-1) + \frac{1}{2}(x - 1)^2(6\xi - 6)\)</span> for some <span class="math notranslate nohighlight">\(\xi\)</span> between <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Answer and justification for E1.2.11: First, compute <span class="math notranslate nohighlight">\(\E[X^2]\)</span></p>
<div class="math notranslate nohighlight">
\[
\E[X^2] = \sum_{x} x^2 \cdot P(X = x) = 1^2 \cdot 0.4 + 2^2 \cdot 0.6 = 0.4 + 4 \cdot 0.6 = 0.4 + 2.4 = 2.8.
\]</div>
<p>Then, use the formula <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = \E[X^2] - (\E[X])^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[X] = 2.8 - (1.6)^2 = 2.8 - 2.56 = 0.24.
\]</div>
<p>Answer and justification for E1.2.13: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(\P[|X - \mathbb{E}[X]| \geq \alpha] \leq \frac{\mathrm{Var}[X]}{\alpha^2}\)</span> for any <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>. Here, <span class="math notranslate nohighlight">\(\alpha = 4\)</span>, so
<span class="math notranslate nohighlight">\(\P[|X - 3| \geq 4] \leq \frac{4}{4^2} = \frac{1}{4}\)</span>.</p>
<p>Answer and justification for E1.2.15: The covariance matrix of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}\)</span>. Since <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are independent, their covariance is zero. The variance of each is 1 since they are standard normal.</p>
<p>Answer and justification for E1.2.17: <span class="math notranslate nohighlight">\(\mathbb{E}[AX] = A\mathbb{E}[X] = A\mu_X = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 5 \\ 11 \end{pmatrix}\)</span>, <span class="math notranslate nohighlight">\(\text{Cov}[AX] = A\text{Cov}[X]A^T = A\Sigma_XA^T = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 4 \end{pmatrix} \begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix} = \begin{pmatrix} 11 &amp; 19 \\ 19 &amp; 35 \end{pmatrix}\)</span></p>
<p>Answer and justification for E1.2.19: For any non-zero vector <span class="math notranslate nohighlight">\(z = \begin{pmatrix} z_1 \\ z_2 \end{pmatrix}\)</span>, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z^TAz &amp;= \begin{pmatrix} z_1 &amp; z_2 \end{pmatrix} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \begin{pmatrix} z_1 \\ z_2 \end{pmatrix} \\
&amp;= \begin{pmatrix} z_1 &amp; z_2 \end{pmatrix} \begin{pmatrix} 2z_1 - z_2 \\ -z_1 + 2z_2 \end{pmatrix} \\
&amp;= 2z_1^2 - 2z_1z_2 + 2z_2^2 \\
&amp;= z_1^2 + (z_1 - z_2)^2 + z_2^2 &gt; 0
\end{align*}\]</div>
<p>since <span class="math notranslate nohighlight">\((z_1 - z_2)^2 \geq 0\)</span>, and either <span class="math notranslate nohighlight">\(z_1^2 &gt; 0\)</span> or <span class="math notranslate nohighlight">\(z_2^2 &gt; 0\)</span> (since <span class="math notranslate nohighlight">\(z \neq 0\)</span>). Therefore, <span class="math notranslate nohighlight">\(A\)</span> is positive definite.</p>
<p>Answer and justification for E1.3.1: <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^* = \frac{1}{2}(\mathbf{x}_1 + \mathbf{x}_4) = (\frac{3}{2}, 1)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2^* = \frac{1}{2}(\mathbf{x}_2 + \mathbf{x}_3) = (-\frac{1}{2}, 0)\)</span>.</p>
<p>Answer and justification for E1.3.3: <span class="math notranslate nohighlight">\(C_1 = \{1, 5\}\)</span>, <span class="math notranslate nohighlight">\(C_2 = \{3\}\)</span>, <span class="math notranslate nohighlight">\(C_3 = \{2, 4\}\)</span>.</p>
<p>Answer and justification for E1.3.5: Expanding the squared norms and cancelling terms yields the equivalent inequality.</p>
<p>Answer and justification for E1.3.7: Expand <span class="math notranslate nohighlight">\(\|A + B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (A_{ij} + B_{ij})^2\)</span> and simplify.</p>
<p>Answer and justification for E1.3.9:  <span class="math notranslate nohighlight">\(q(x) = 3(x-1)^2 + 2\)</span>, minimum value is 2 at <span class="math notranslate nohighlight">\(x = 1\)</span>.</p>
<p>Answer and justification for E1.3.11: <span class="math notranslate nohighlight">\(\|A\|_F = \sqrt{14}\)</span>.</p>
<p>Answer and justification for E1.4.1: Since <span class="math notranslate nohighlight">\(X_i\)</span> is uniformly distributed on <span class="math notranslate nohighlight">\([-1/2, 1/2]\)</span>, its probability density function is <span class="math notranslate nohighlight">\(f_{X_i}(x) = 1\)</span> for <span class="math notranslate nohighlight">\(x \in [-1/2, 1/2]\)</span> and 0 otherwise. Therefore,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X_i] = \int_{-1/2}^{1/2} x f_{X_i}(x) dx = \int_{-1/2}^{1/2} x dx = 0\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\text{Var}[X_i] = \mathbb{E}[X_i^2] - (\mathbb{E}[X_i])^2 = \int_{-1/2}^{1/2} x^2 dx = \frac{1}{12}.\]</div>
<p>Answer and justification for E1.4.3: We have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\|X\|^2] = \mathbb{E}\left[\sum_{i=1}^d X_i^2\right] = \sum_{i=1}^d \mathbb{E}[X_i^2] = \sum_{i=1}^d 1 = d.\]</div>
<p>Answer and justification for E1.4.5: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(P[|X - 1| \geq 3] \leq \frac{\mathrm{Var}[X]}{3^2} = \frac{4}{9}\)</span>.</p>
<p>Answer and justification for E1.4.7: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(\P[|X| \geq 2\sqrt{d}] \leq \frac{\mathrm{Var}[X]}{(2\sqrt{d})^2} = \frac{1}{4d}\)</span>. For <span class="math notranslate nohighlight">\(d = 1\)</span>, <span class="math notranslate nohighlight">\(\P[|X| \geq 2] \leq \frac{1}{4}\)</span>. For <span class="math notranslate nohighlight">\(d = 10\)</span>, <span class="math notranslate nohighlight">\(P[|X| \geq 2\sqrt{10}] \leq \frac{1}{40}\)</span>. For <span class="math notranslate nohighlight">\(d = 100\)</span>, <span class="math notranslate nohighlight">\(\P[|X| \geq 20] \leq \frac{1}{400}\)</span>.</p>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">1.6.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Compute norms of vectors and matrices, and inner products between vectors.</p></li>
<li><p>Utilize properties of norms and inner products, including the Cauchy-Schwarz inequality and the triangle inequality.</p></li>
<li><p>Perform basic matrix operations, including addition, scalar multiplication, multiplication, and transposition.</p></li>
<li><p>Compute matrix-vector and matrix-matrix products and interpret their geometric and algebraic significance.</p></li>
<li><p>Explain the concept of a descent direction and its relationship to optimization.</p></li>
<li><p>Use Taylor’s Theorem to approximate functions and analyze the error terms in these approximations.</p></li>
<li><p>Define and compute partial derivatives and gradients for functions of multiple variables.</p></li>
<li><p>State and apply Chebyshev’s inequality to quantify the concentration of a random variable around its mean.</p></li>
<li><p>Compute the expectation, variance, and covariance of random variables and vectors.</p></li>
<li><p>Define and give examples of positive semidefinite matrices.</p></li>
<li><p>Explain the properties and significance of the covariance matrix of a random vector.</p></li>
<li><p>Describe and generate samples from a spherical Gaussian distribution.</p></li>
<li><p>Write Python code to compute norms, inner products, and perform matrix operations.</p></li>
<li><p>Use Python to simulate random variables, calculate their statistical properties, and visualize distributions and the law of large numbers.</p></li>
<li><p>Formulate the k-means clustering problem as an optimization problem.</p></li>
<li><p>Describe the k-means algorithm and its iterative steps for finding cluster centers and assignments.</p></li>
<li><p>Derive the optimal cluster representatives (centroids) for a fixed partition.</p></li>
<li><p>Derive the optimal partition (cluster assignments) for fixed representatives.</p></li>
<li><p>Analyze the convergence properties of the k-means algorithm and understand its limitations in finding global optima.</p></li>
<li><p>Apply the k-means algorithm to real-world datasets, such as the penguins dataset, and interpret the results.</p></li>
<li><p>Express the k-means objective function in matrix form and relate it to matrix factorization.</p></li>
<li><p>Understand the importance of data preprocessing steps like standardization for k-means.</p></li>
<li><p>Recognize the limitations of k-means, such as its sensitivity to initialization and the need to specify the number of clusters in advance.</p></li>
<li><p>Discuss the challenges of determining the optimal number of clusters in k-means clustering.</p></li>
<li><p>Understand the challenges of clustering high-dimensional data using the k-means algorithm and recognize how the increasing dimensionality can cause the noise to overwhelm the signal, making it difficult to distinguish between clusters.</p></li>
<li><p>Comprehend the concept of concentration of measure in high-dimensional spaces, specifically the counterintuitive fact that most of the volume of a high-dimensional cube is concentrated in its corners, making it appear like a “spiky ball.”</p></li>
<li><p>Apply Chebyshev’s inequality to derive the probability that a randomly selected point from a high-dimensional cube falls within the inscribed ball, and understand how this probability decreases as the dimensionality increases.</p></li>
<li><p>Analyze the behavior of the norm of a standard Normal vector in high-dimensional spaces and prove, using Chebyshev’s inequality, that the norm is highly likely to be close to the square root of the dimension, despite the joint probability density function being maximized at the origin.</p></li>
<li><p>Use Chebyshev’s inequality to derive probabilistic bounds in high-dimensional settings.</p></li>
<li><p>Simulate high-dimensional data to empirically verify theoretical results.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
&#13;

<h3><span class="section-number">1.6.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_intro_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_intro_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
&#13;

<h3><span class="section-number">1.6.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_2.html">Section 1.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_3.html">Section 1.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_1_4.html">Section 1.4</a></p></li>
</ul>
&#13;

<h3><span class="section-number">1.6.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-intro-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-intro-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
&#13;

<h3><span class="section-number">1.6.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E1.2.1: The Euclidean norm <span class="math notranslate nohighlight">\(\|\mathbf{x}\|_2\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2} = \sqrt{6^2 + 8^2} = \sqrt{36 + 64} = \sqrt{100} = 10.
\]</div>
<p>Answer and justification for E1.2.3: The transpose <span class="math notranslate nohighlight">\(A^T\)</span> is obtained by switching rows and columns</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix}1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6\end{pmatrix}
\end{split}\]</div>
<p>Answer and justification for E1.2.5: A matrix <span class="math notranslate nohighlight">\(A\)</span> is symmetric if <span class="math notranslate nohighlight">\(A = A^T\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix}2 &amp; 0 \\ 0 &amp; 3\end{pmatrix} = A.
\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(A\)</span> is symmetric.</p>
<p>Answer and justification for E1.2.7: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = \lim_{h \to 0} \frac{f(x+h, y) - f(x, y)}{h} = \lim_{h \to 0} \frac{(x+h)^2 + (x+h)y + y^2 - (x^2 + xy + y^2)}{h} = \lim_{h \to 0} \frac{2xh + h^2 + hy}{h} = 2x + y\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = \lim_{h \to 0} \frac{f(x, y+h) - f(x, y)}{h} = \lim_{h \to 0} \frac{x^2 + x(y+h) + (y+h)^2 - (x^2 + xy + y^2)}{h} = \lim_{h \to 0} \frac{xh + 2yh + h^2}{h} = x + 2y\)</span>.</p>
<p>Answer and justification E1.2.9: By <em>Taylor’s Theorem</em>, <span class="math notranslate nohighlight">\(f(x) = f(a) + (x - a)f'(a) + \frac{1}{2}(x - a)^2f''(\xi)\)</span> for some <span class="math notranslate nohighlight">\(\xi\)</span> between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(x\)</span>. Here, <span class="math notranslate nohighlight">\(f(1) = 1^3 - 3 \cdot 1^2 + 2 \cdot 1 = 0\)</span>, <span class="math notranslate nohighlight">\(f'(x) = 3x^2 - 6x + 2\)</span>, so <span class="math notranslate nohighlight">\(f'(1) = 3 - 6 + 2 = -1\)</span>, and <span class="math notranslate nohighlight">\(f''(x) = 6x - 6\)</span>. Therefore,
<span class="math notranslate nohighlight">\(f(x) = 0 + (x - 1)(-1) + \frac{1}{2}(x - 1)^2(6\xi - 6)\)</span> for some <span class="math notranslate nohighlight">\(\xi\)</span> between <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Answer and justification for E1.2.11: First, compute <span class="math notranslate nohighlight">\(\E[X^2]\)</span></p>
<div class="math notranslate nohighlight">
\[
\E[X^2] = \sum_{x} x^2 \cdot P(X = x) = 1^2 \cdot 0.4 + 2^2 \cdot 0.6 = 0.4 + 4 \cdot 0.6 = 0.4 + 2.4 = 2.8.
\]</div>
<p>Then, use the formula <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = \E[X^2] - (\E[X])^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[X] = 2.8 - (1.6)^2 = 2.8 - 2.56 = 0.24.
\]</div>
<p>Answer and justification for E1.2.13: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(\P[|X - \mathbb{E}[X]| \geq \alpha] \leq \frac{\mathrm{Var}[X]}{\alpha^2}\)</span> for any <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>. Here, <span class="math notranslate nohighlight">\(\alpha = 4\)</span>, so
<span class="math notranslate nohighlight">\(\P[|X - 3| \geq 4] \leq \frac{4}{4^2} = \frac{1}{4}\)</span>.</p>
<p>Answer and justification for E1.2.15: The covariance matrix of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}\)</span>. Since <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are independent, their covariance is zero. The variance of each is 1 since they are standard normal.</p>
<p>Answer and justification for E1.2.17: <span class="math notranslate nohighlight">\(\mathbb{E}[AX] = A\mathbb{E}[X] = A\mu_X = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 5 \\ 11 \end{pmatrix}\)</span>, <span class="math notranslate nohighlight">\(\text{Cov}[AX] = A\text{Cov}[X]A^T = A\Sigma_XA^T = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix} \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 4 \end{pmatrix} \begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix} = \begin{pmatrix} 11 &amp; 19 \\ 19 &amp; 35 \end{pmatrix}\)</span></p>
<p>Answer and justification for E1.2.19: For any non-zero vector <span class="math notranslate nohighlight">\(z = \begin{pmatrix} z_1 \\ z_2 \end{pmatrix}\)</span>, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z^TAz &amp;= \begin{pmatrix} z_1 &amp; z_2 \end{pmatrix} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \begin{pmatrix} z_1 \\ z_2 \end{pmatrix} \\
&amp;= \begin{pmatrix} z_1 &amp; z_2 \end{pmatrix} \begin{pmatrix} 2z_1 - z_2 \\ -z_1 + 2z_2 \end{pmatrix} \\
&amp;= 2z_1^2 - 2z_1z_2 + 2z_2^2 \\
&amp;= z_1^2 + (z_1 - z_2)^2 + z_2^2 &gt; 0
\end{align*}\]</div>
<p>since <span class="math notranslate nohighlight">\((z_1 - z_2)^2 \geq 0\)</span>, and either <span class="math notranslate nohighlight">\(z_1^2 &gt; 0\)</span> or <span class="math notranslate nohighlight">\(z_2^2 &gt; 0\)</span> (since <span class="math notranslate nohighlight">\(z \neq 0\)</span>). Therefore, <span class="math notranslate nohighlight">\(A\)</span> is positive definite.</p>
<p>Answer and justification for E1.3.1: <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^* = \frac{1}{2}(\mathbf{x}_1 + \mathbf{x}_4) = (\frac{3}{2}, 1)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2^* = \frac{1}{2}(\mathbf{x}_2 + \mathbf{x}_3) = (-\frac{1}{2}, 0)\)</span>.</p>
<p>Answer and justification for E1.3.3: <span class="math notranslate nohighlight">\(C_1 = \{1, 5\}\)</span>, <span class="math notranslate nohighlight">\(C_2 = \{3\}\)</span>, <span class="math notranslate nohighlight">\(C_3 = \{2, 4\}\)</span>.</p>
<p>Answer and justification for E1.3.5: Expanding the squared norms and cancelling terms yields the equivalent inequality.</p>
<p>Answer and justification for E1.3.7: Expand <span class="math notranslate nohighlight">\(\|A + B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (A_{ij} + B_{ij})^2\)</span> and simplify.</p>
<p>Answer and justification for E1.3.9:  <span class="math notranslate nohighlight">\(q(x) = 3(x-1)^2 + 2\)</span>, minimum value is 2 at <span class="math notranslate nohighlight">\(x = 1\)</span>.</p>
<p>Answer and justification for E1.3.11: <span class="math notranslate nohighlight">\(\|A\|_F = \sqrt{14}\)</span>.</p>
<p>Answer and justification for E1.4.1: Since <span class="math notranslate nohighlight">\(X_i\)</span> is uniformly distributed on <span class="math notranslate nohighlight">\([-1/2, 1/2]\)</span>, its probability density function is <span class="math notranslate nohighlight">\(f_{X_i}(x) = 1\)</span> for <span class="math notranslate nohighlight">\(x \in [-1/2, 1/2]\)</span> and 0 otherwise. Therefore,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X_i] = \int_{-1/2}^{1/2} x f_{X_i}(x) dx = \int_{-1/2}^{1/2} x dx = 0\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\text{Var}[X_i] = \mathbb{E}[X_i^2] - (\mathbb{E}[X_i])^2 = \int_{-1/2}^{1/2} x^2 dx = \frac{1}{12}.\]</div>
<p>Answer and justification for E1.4.3: We have</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\|X\|^2] = \mathbb{E}\left[\sum_{i=1}^d X_i^2\right] = \sum_{i=1}^d \mathbb{E}[X_i^2] = \sum_{i=1}^d 1 = d.\]</div>
<p>Answer and justification for E1.4.5: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(P[|X - 1| \geq 3] \leq \frac{\mathrm{Var}[X]}{3^2} = \frac{4}{9}\)</span>.</p>
<p>Answer and justification for E1.4.7: By <em>Chebyshev’s inequality</em>, <span class="math notranslate nohighlight">\(\P[|X| \geq 2\sqrt{d}] \leq \frac{\mathrm{Var}[X]}{(2\sqrt{d})^2} = \frac{1}{4d}\)</span>. For <span class="math notranslate nohighlight">\(d = 1\)</span>, <span class="math notranslate nohighlight">\(\P[|X| \geq 2] \leq \frac{1}{4}\)</span>. For <span class="math notranslate nohighlight">\(d = 10\)</span>, <span class="math notranslate nohighlight">\(P[|X| \geq 2\sqrt{10}] \leq \frac{1}{40}\)</span>. For <span class="math notranslate nohighlight">\(d = 100\)</span>, <span class="math notranslate nohighlight">\(\P[|X| \geq 20] \leq \frac{1}{400}\)</span>.</p>
&#13;

<h3><span class="section-number">1.6.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Compute norms of vectors and matrices, and inner products between vectors.</p></li>
<li><p>Utilize properties of norms and inner products, including the Cauchy-Schwarz inequality and the triangle inequality.</p></li>
<li><p>Perform basic matrix operations, including addition, scalar multiplication, multiplication, and transposition.</p></li>
<li><p>Compute matrix-vector and matrix-matrix products and interpret their geometric and algebraic significance.</p></li>
<li><p>Explain the concept of a descent direction and its relationship to optimization.</p></li>
<li><p>Use Taylor’s Theorem to approximate functions and analyze the error terms in these approximations.</p></li>
<li><p>Define and compute partial derivatives and gradients for functions of multiple variables.</p></li>
<li><p>State and apply Chebyshev’s inequality to quantify the concentration of a random variable around its mean.</p></li>
<li><p>Compute the expectation, variance, and covariance of random variables and vectors.</p></li>
<li><p>Define and give examples of positive semidefinite matrices.</p></li>
<li><p>Explain the properties and significance of the covariance matrix of a random vector.</p></li>
<li><p>Describe and generate samples from a spherical Gaussian distribution.</p></li>
<li><p>Write Python code to compute norms, inner products, and perform matrix operations.</p></li>
<li><p>Use Python to simulate random variables, calculate their statistical properties, and visualize distributions and the law of large numbers.</p></li>
<li><p>Formulate the k-means clustering problem as an optimization problem.</p></li>
<li><p>Describe the k-means algorithm and its iterative steps for finding cluster centers and assignments.</p></li>
<li><p>Derive the optimal cluster representatives (centroids) for a fixed partition.</p></li>
<li><p>Derive the optimal partition (cluster assignments) for fixed representatives.</p></li>
<li><p>Analyze the convergence properties of the k-means algorithm and understand its limitations in finding global optima.</p></li>
<li><p>Apply the k-means algorithm to real-world datasets, such as the penguins dataset, and interpret the results.</p></li>
<li><p>Express the k-means objective function in matrix form and relate it to matrix factorization.</p></li>
<li><p>Understand the importance of data preprocessing steps like standardization for k-means.</p></li>
<li><p>Recognize the limitations of k-means, such as its sensitivity to initialization and the need to specify the number of clusters in advance.</p></li>
<li><p>Discuss the challenges of determining the optimal number of clusters in k-means clustering.</p></li>
<li><p>Understand the challenges of clustering high-dimensional data using the k-means algorithm and recognize how the increasing dimensionality can cause the noise to overwhelm the signal, making it difficult to distinguish between clusters.</p></li>
<li><p>Comprehend the concept of concentration of measure in high-dimensional spaces, specifically the counterintuitive fact that most of the volume of a high-dimensional cube is concentrated in its corners, making it appear like a “spiky ball.”</p></li>
<li><p>Apply Chebyshev’s inequality to derive the probability that a randomly selected point from a high-dimensional cube falls within the inscribed ball, and understand how this probability decreases as the dimensionality increases.</p></li>
<li><p>Analyze the behavior of the norm of a standard Normal vector in high-dimensional spaces and prove, using Chebyshev’s inequality, that the norm is highly likely to be close to the square root of the dimension, despite the joint probability density function being maximized at the origin.</p></li>
<li><p>Use Chebyshev’s inequality to derive probabilistic bounds in high-dimensional settings.</p></li>
<li><p>Simulate high-dimensional data to empirically verify theoretical results.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
&#13;

<h2><span class="section-number">1.6.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="a-more-rigorous-analysis-of-clustering-in-high-dimension">
<h3><span class="section-number">1.6.2.1. </span>A more rigorous analysis of clustering in high dimension<a class="headerlink" href="#a-more-rigorous-analysis-of-clustering-in-high-dimension" title="Link to this heading">#</a></h3>
<p>In this optional section, we give one formal statement of the phenomenon described in the previous subsection.</p>
<p><strong>THEOREM</strong> Let <span class="math notranslate nohighlight">\(\mathbf{X}_1, \mathbf{X}_2, \mathbf{Y}_1\)</span> be independent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with mean <span class="math notranslate nohighlight">\(-w_d \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>, where <span class="math notranslate nohighlight">\(\{w_d\}\)</span> is a monotone sequence in <span class="math notranslate nohighlight">\(d\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{Y}_2\)</span> be an indepedent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean <span class="math notranslate nohighlight">\(w_d \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. Then, letting <span class="math notranslate nohighlight">\(\Delta_d = \|\mathbf{Y}_1 - \mathbf{Y}_2\|^2 - \|\mathbf{X}_1 - \mathbf{X}_2\|^2\)</span>, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\mathbb{E}[\Delta_d]}{\sqrt{\mathrm{Var}[\Delta_d]}} \to
\begin{cases}
0, &amp; \text{if $w_d \ll d^{1/4}$}\\
+\infty, &amp; \text{if $w_d \gg d^{1/4}$}
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_d \ll d^{1/4}\)</span> means <span class="math notranslate nohighlight">\(w_d/d^{1/4} \to 0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The ratio is the statement is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio">signal-to-noise ratio</a>.</p>
<p>To prove the claim, we will need the following property.</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(W\)</span> be a real-valued random variable symmetric about zero, that is, such that <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(-W\)</span> are identically distributed. Then for all odd <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[W^k] = 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By the symmetry,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[W^k] = \mathbb{E}[(-W)^k] = \mathbb{E}[(-1)^k W^k] = - \mathbb{E}[W^k].
\]</div>
<p>The only way to satisfy this equation is to have <span class="math notranslate nohighlight">\(\mathbb{E}[W^k] = 0\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Returning to the proof of the claim:</p>
<p><em>Proof idea:</em> <em>(Theorem)</em> The only coordinate contributing to <span class="math notranslate nohighlight">\(\mathbb{E}[\Delta_d]\)</span> is the first one by linearity of expectation, while all coordinates contribute to <span class="math notranslate nohighlight">\(\mathrm{Var}[\Delta_d]\)</span>. More specifically, a calculation shows that the former is <span class="math notranslate nohighlight">\(c_0 w^2\)</span> while the latter is <span class="math notranslate nohighlight">\(c_1 w^2 + c_2 d\)</span>, where <span class="math notranslate nohighlight">\(c_0, c_1, c_2\)</span> are constants.</p>
<p><em>Proof:</em> <em>(Claim)</em> Write <span class="math notranslate nohighlight">\(w := w_d\)</span> and <span class="math notranslate nohighlight">\(\Delta := \Delta_d\)</span> to simplify the notation. There are two steps:</p>
<p><em>(1) Expectation of <span class="math notranslate nohighlight">\(\Delta\)</span>:</em> By defintion, the random variables <span class="math notranslate nohighlight">\(X_{1,i} - X_{2,i}\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots, d\)</span>, and <span class="math notranslate nohighlight">\(Y_{1,i} - Y_{2,i}\)</span>, <span class="math notranslate nohighlight">\(i = 2,\ldots, d\)</span>, are identically distributed. So, by linearity of expectation,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}[\Delta]
&amp;= \sum_{i=1}^d \mathbb{E}[(Y_{1,i} - Y_{2,i})^2] - \sum_{i=1}^d \mathbb{E}[(X_{1,i} - X_{2,i})^2]\\
&amp;= \mathbb{E}[(Y_{1,1} - Y_{2,1})^2] - \mathbb{E}[(X_{1,1} - X_{2,1})^2].
\end{align*}\]</div>
<p>Further, we can write <span class="math notranslate nohighlight">\(Y_{1,1} - Y_{1,2} \sim (Z_1 -w) - (Z_2+w)\)</span> where <span class="math notranslate nohighlight">\(Z_1, Z_2 \sim N(0,1)\)</span> are independent, where here <span class="math notranslate nohighlight">\(\sim\)</span> indicates equality in distribution. Hence, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}[(Y_{1,1} - Y_{2,1})^2]
&amp;= \mathbb{E}[(Z_1 - Z_2 - 2w)^2]\\
&amp;= \mathbb{E}[(Z_1 - Z_2)^2] 
- 4w \,\mathbb{E}[Z_1 - Z_2]
+ 4 w^2.
\end{align*}\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(X_{1,1} - X_{1,2} \sim Z_1 - Z_2\)</span> so <span class="math notranslate nohighlight">\(\mathbb{E}[(X_{1,1} - X_{2,1})^2] = \mathbb{E}[(Z_1 - Z_2)^2]\)</span>. Since <span class="math notranslate nohighlight">\(\mathbb{E}[Z_1 - Z_2] = 0\)</span>, we finally get <span class="math notranslate nohighlight">\(\mathbb{E}[\Delta] = 4 w^2\)</span>.</p>
<p><em>(2) Variance of <span class="math notranslate nohighlight">\(\Delta\)</span>:</em> Using the observations from (1) and the independence of the coordinates we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[\Delta]
&amp;= \sum_{i=1}^d \mathrm{Var}[(Y_{1,i} - Y_{2,i})^2] + \sum_{i=1}^d \mathrm{Var}[(X_{1,i} - X_{2,i})^2]\\
&amp;= \mathrm{Var}[(Z_1 - Z_2 - 2w)^2] 
+ (2d-1) \,\mathrm{Var}[(Z_1 - Z_2)^2].
\end{align*}\]</div>
<p>By the <em>Variance of a Sum</em>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[(Z_1 - Z_2 - 2w)^2]
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2 - 4w(Z_1 - Z_2) + 4w^2]\\ 
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2 - 4w(Z_1 - Z_2)]\\
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2] 
+ 16 w^2 \mathrm{Var}[Z_1 - Z_2]\\
&amp;\quad - 8w \,\mathrm{Cov}[(Z_1 - Z_2)^2, Z_1 - Z_2].
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(Z_1\)</span> and <span class="math notranslate nohighlight">\(Z_2\)</span> are independent, <span class="math notranslate nohighlight">\(\mathrm{Var}[Z_1 - Z_2]
= \mathrm{Var}[Z_1] + \mathrm{Var}[Z_2] = 2\)</span>. Moreover, the random variable <span class="math notranslate nohighlight">\((Z_1 - Z_2)\)</span> is symmetric, so</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[(Z_1 - Z_2)^2, Z_1 - Z_2]
&amp;= \mathbb{E}[(Z_1 - Z_2)^3]\\ 
&amp; \quad - \mathbb{E}[(Z_1 - Z_2)^2] \,\mathbb{E}[Z_1 - Z_2]\\
&amp;= 0.
\end{align*}\]</div>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[\Delta]
= 32 w^2 
+ 2d \,\mathrm{Var}[(Z_1 - Z_2)^2]
\]</div>
<p><em>Putting everything together:</em></p>
<div class="math notranslate nohighlight">
\[
\frac{\mathbb{E}[\Delta]}{\sqrt{\mathrm{Var}[\Delta]}}
=
\frac{4 w^2}{\sqrt{32 w^2 
+ 2d \,\mathrm{Var}[(Z_1 - Z_2)^2]}}.
\]</div>
<p>Taking <span class="math notranslate nohighlight">\(d \to +\infty\)</span> gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
&#13;

<h3><span class="section-number">1.6.2.1. </span>A more rigorous analysis of clustering in high dimension<a class="headerlink" href="#a-more-rigorous-analysis-of-clustering-in-high-dimension" title="Link to this heading">#</a></h3>
<p>In this optional section, we give one formal statement of the phenomenon described in the previous subsection.</p>
<p><strong>THEOREM</strong> Let <span class="math notranslate nohighlight">\(\mathbf{X}_1, \mathbf{X}_2, \mathbf{Y}_1\)</span> be independent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with mean <span class="math notranslate nohighlight">\(-w_d \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>, where <span class="math notranslate nohighlight">\(\{w_d\}\)</span> is a monotone sequence in <span class="math notranslate nohighlight">\(d\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{Y}_2\)</span> be an indepedent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean <span class="math notranslate nohighlight">\(w_d \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. Then, letting <span class="math notranslate nohighlight">\(\Delta_d = \|\mathbf{Y}_1 - \mathbf{Y}_2\|^2 - \|\mathbf{X}_1 - \mathbf{X}_2\|^2\)</span>, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\mathbb{E}[\Delta_d]}{\sqrt{\mathrm{Var}[\Delta_d]}} \to
\begin{cases}
0, &amp; \text{if $w_d \ll d^{1/4}$}\\
+\infty, &amp; \text{if $w_d \gg d^{1/4}$}
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_d \ll d^{1/4}\)</span> means <span class="math notranslate nohighlight">\(w_d/d^{1/4} \to 0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The ratio is the statement is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio">signal-to-noise ratio</a>.</p>
<p>To prove the claim, we will need the following property.</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(W\)</span> be a real-valued random variable symmetric about zero, that is, such that <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(-W\)</span> are identically distributed. Then for all odd <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[W^k] = 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By the symmetry,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[W^k] = \mathbb{E}[(-W)^k] = \mathbb{E}[(-1)^k W^k] = - \mathbb{E}[W^k].
\]</div>
<p>The only way to satisfy this equation is to have <span class="math notranslate nohighlight">\(\mathbb{E}[W^k] = 0\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Returning to the proof of the claim:</p>
<p><em>Proof idea:</em> <em>(Theorem)</em> The only coordinate contributing to <span class="math notranslate nohighlight">\(\mathbb{E}[\Delta_d]\)</span> is the first one by linearity of expectation, while all coordinates contribute to <span class="math notranslate nohighlight">\(\mathrm{Var}[\Delta_d]\)</span>. More specifically, a calculation shows that the former is <span class="math notranslate nohighlight">\(c_0 w^2\)</span> while the latter is <span class="math notranslate nohighlight">\(c_1 w^2 + c_2 d\)</span>, where <span class="math notranslate nohighlight">\(c_0, c_1, c_2\)</span> are constants.</p>
<p><em>Proof:</em> <em>(Claim)</em> Write <span class="math notranslate nohighlight">\(w := w_d\)</span> and <span class="math notranslate nohighlight">\(\Delta := \Delta_d\)</span> to simplify the notation. There are two steps:</p>
<p><em>(1) Expectation of <span class="math notranslate nohighlight">\(\Delta\)</span>:</em> By defintion, the random variables <span class="math notranslate nohighlight">\(X_{1,i} - X_{2,i}\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots, d\)</span>, and <span class="math notranslate nohighlight">\(Y_{1,i} - Y_{2,i}\)</span>, <span class="math notranslate nohighlight">\(i = 2,\ldots, d\)</span>, are identically distributed. So, by linearity of expectation,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}[\Delta]
&amp;= \sum_{i=1}^d \mathbb{E}[(Y_{1,i} - Y_{2,i})^2] - \sum_{i=1}^d \mathbb{E}[(X_{1,i} - X_{2,i})^2]\\
&amp;= \mathbb{E}[(Y_{1,1} - Y_{2,1})^2] - \mathbb{E}[(X_{1,1} - X_{2,1})^2].
\end{align*}\]</div>
<p>Further, we can write <span class="math notranslate nohighlight">\(Y_{1,1} - Y_{1,2} \sim (Z_1 -w) - (Z_2+w)\)</span> where <span class="math notranslate nohighlight">\(Z_1, Z_2 \sim N(0,1)\)</span> are independent, where here <span class="math notranslate nohighlight">\(\sim\)</span> indicates equality in distribution. Hence, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}[(Y_{1,1} - Y_{2,1})^2]
&amp;= \mathbb{E}[(Z_1 - Z_2 - 2w)^2]\\
&amp;= \mathbb{E}[(Z_1 - Z_2)^2] 
- 4w \,\mathbb{E}[Z_1 - Z_2]
+ 4 w^2.
\end{align*}\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(X_{1,1} - X_{1,2} \sim Z_1 - Z_2\)</span> so <span class="math notranslate nohighlight">\(\mathbb{E}[(X_{1,1} - X_{2,1})^2] = \mathbb{E}[(Z_1 - Z_2)^2]\)</span>. Since <span class="math notranslate nohighlight">\(\mathbb{E}[Z_1 - Z_2] = 0\)</span>, we finally get <span class="math notranslate nohighlight">\(\mathbb{E}[\Delta] = 4 w^2\)</span>.</p>
<p><em>(2) Variance of <span class="math notranslate nohighlight">\(\Delta\)</span>:</em> Using the observations from (1) and the independence of the coordinates we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[\Delta]
&amp;= \sum_{i=1}^d \mathrm{Var}[(Y_{1,i} - Y_{2,i})^2] + \sum_{i=1}^d \mathrm{Var}[(X_{1,i} - X_{2,i})^2]\\
&amp;= \mathrm{Var}[(Z_1 - Z_2 - 2w)^2] 
+ (2d-1) \,\mathrm{Var}[(Z_1 - Z_2)^2].
\end{align*}\]</div>
<p>By the <em>Variance of a Sum</em>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[(Z_1 - Z_2 - 2w)^2]
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2 - 4w(Z_1 - Z_2) + 4w^2]\\ 
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2 - 4w(Z_1 - Z_2)]\\
&amp;= \mathrm{Var}[(Z_1 - Z_2)^2] 
+ 16 w^2 \mathrm{Var}[Z_1 - Z_2]\\
&amp;\quad - 8w \,\mathrm{Cov}[(Z_1 - Z_2)^2, Z_1 - Z_2].
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(Z_1\)</span> and <span class="math notranslate nohighlight">\(Z_2\)</span> are independent, <span class="math notranslate nohighlight">\(\mathrm{Var}[Z_1 - Z_2]
= \mathrm{Var}[Z_1] + \mathrm{Var}[Z_2] = 2\)</span>. Moreover, the random variable <span class="math notranslate nohighlight">\((Z_1 - Z_2)\)</span> is symmetric, so</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Cov}[(Z_1 - Z_2)^2, Z_1 - Z_2]
&amp;= \mathbb{E}[(Z_1 - Z_2)^3]\\ 
&amp; \quad - \mathbb{E}[(Z_1 - Z_2)^2] \,\mathbb{E}[Z_1 - Z_2]\\
&amp;= 0.
\end{align*}\]</div>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}[\Delta]
= 32 w^2 
+ 2d \,\mathrm{Var}[(Z_1 - Z_2)^2]
\]</div>
<p><em>Putting everything together:</em></p>
<div class="math notranslate nohighlight">
\[
\frac{\mathbb{E}[\Delta]}{\sqrt{\mathrm{Var}[\Delta]}}
=
\frac{4 w^2}{\sqrt{32 w^2 
+ 2d \,\mathrm{Var}[(Z_1 - Z_2)^2]}}.
\]</div>
<p>Taking <span class="math notranslate nohighlight">\(d \to +\infty\)</span> gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
    
</body>
</html>