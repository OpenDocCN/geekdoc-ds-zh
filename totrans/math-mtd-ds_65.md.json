["```py\ndef sigmoid(z): \n    return 1/(1+np.exp(-z))\n\ndef pred_fn(x, A): \n    return sigmoid(A @ x)\n\ndef loss_fn(x, A, b): \n    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))\n\ndef grad_fn(x, A, b):\n    return -A.T @ (b - pred_fn(x, A))/len(b)\n\ndef desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n    gradient = grad_fn(curr_x, A, b)\n    return curr_x - beta*gradient\n\ndef sgd_for_logreg(rng, loss_fn, grad_fn, A, b, \n                   init_x, beta=1e-3, niters=int(1e5), batch=40):\n\n    curr_x = init_x\n    nsamples = len(b)\n    for _ in range(niters):\n        I = rng.integers(nsamples, size=batch)\n        curr_x = desc_update_for_logreg(\n            grad_fn, A[I,:], b[I], curr_x, beta)\n\n    return curr_x \n```", "```py\ndata = pd.read_csv('SAHeart.csv')\ndata.head() \n```", "```py\nfeature = data[['tobacco', 'ldl', 'age']].to_numpy()\nprint(feature) \n```", "```py\n[[1.200e+01 5.730e+00 5.200e+01]\n [1.000e-02 4.410e+00 6.300e+01]\n [8.000e-02 3.480e+00 4.600e+01]\n ...\n [3.000e+00 1.590e+00 5.500e+01]\n [5.400e+00 1.161e+01 4.000e+01]\n [0.000e+00 4.820e+00 4.600e+01]] \n```", "```py\nlabel = data['chd'].to_numpy()\nA = np.concatenate((np.ones((len(label),1)),feature),axis=1)\nb = label \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\ninit_x = np.zeros(A.shape[1])\nbest_x = sgd_for_logreg(rng, loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e6))\nprint(best_x) \n```", "```py\n[-4.06558071  0.07990955  0.18813635  0.04693118] \n```", "```py\ndef logis_acc(x, A, b):\n    return np.sum((pred_fn(x, A) > 0.5) == b)/len(b) \n```", "```py\nlogis_acc(best_x, A, b) \n```", "```py\n0.7207792207792207 \n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() \n                      else (\"mps\" if torch.backends.mps.is_available() \n                            else \"cpu\"))\nprint(\"Using device:\", device) \n```", "```py\nUsing device: mps \n```", "```py\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\nseed = 42\ntorch.manual_seed(seed)\n\nif device.type == 'cuda': # device-specific seeding and settings\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelif device.type == 'mps':\n    torch.mps.manual_seed(seed)  # MPS-specific seeding\n\ng = torch.Generator()\ng.manual_seed(seed)\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                               download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                              download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n```", "```py\nif device.type == 'cuda': # device-specific seeding and settings\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelif device.type == 'mps':\n    torch.mps.manual_seed(seed)  # MPS-specific seeding \n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28 * 28, 10)\n).to(device) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=1e-3) \n```", "```py\ndef train(dataloader, model, loss_fn, optimizer, device):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)    \n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ndef training_loop(train_loader, model, loss_fn, optimizer, device, epochs=3):\n    for epoch in range(epochs):\n        train(train_loader, model, loss_fn, optimizer, device)\n        print(f\"Epoch {epoch+1}/{epochs}\") \n```", "```py\ntraining_loop(train_loader, model, loss_fn, optimizer, device, epochs=10) \n```", "```py\ndef test(dataloader, model, loss_fn, device):\n    size = len(dataloader.dataset)\n    correct = 0    \n    model.eval()\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n\n    print(f\"Test error: {(100*(correct  /  size)):>0.1f}% accuracy\") \n```", "```py\ntest(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 78.7% accuracy \n```", "```py\nimport torch.nn.functional as F\n\ndef predict_softmax(dataloader, model, device):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            probabilities = F.softmax(pred, dim=1)\n            predictions.append(probabilities.cpu())\n\n    return torch.cat(predictions, dim=0)\n\npredictions = predict_softmax(test_loader, model, device).numpy() \n```", "```py\nprint(predictions[0]) \n```", "```py\n[4.4307188e-04 3.8354204e-04 2.0886613e-03 8.8066678e-04 3.6079765e-03\n 1.7791630e-01 1.4651606e-03 2.2466542e-01 4.8245404e-02 5.4030383e-01] \n```", "```py\npredictions[0].argmax(0) \n```", "```py\n9 \n```", "```py\nimages, labels = next(iter(test_loader))\nimages = images.squeeze().numpy()\nlabels = labels.numpy()\n\nprint(f\"{labels[0]}: '{mmids.FashionMNIST_get_class_name(labels[0])}'\") \n```", "```py\n9: 'Ankle boot' \n```", "```py\ndef sigmoid(z): \n    return 1/(1+np.exp(-z))\n\ndef pred_fn(x, A): \n    return sigmoid(A @ x)\n\ndef loss_fn(x, A, b): \n    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))\n\ndef grad_fn(x, A, b):\n    return -A.T @ (b - pred_fn(x, A))/len(b)\n\ndef desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n    gradient = grad_fn(curr_x, A, b)\n    return curr_x - beta*gradient\n\ndef sgd_for_logreg(rng, loss_fn, grad_fn, A, b, \n                   init_x, beta=1e-3, niters=int(1e5), batch=40):\n\n    curr_x = init_x\n    nsamples = len(b)\n    for _ in range(niters):\n        I = rng.integers(nsamples, size=batch)\n        curr_x = desc_update_for_logreg(\n            grad_fn, A[I,:], b[I], curr_x, beta)\n\n    return curr_x \n```", "```py\ndata = pd.read_csv('SAHeart.csv')\ndata.head() \n```", "```py\nfeature = data[['tobacco', 'ldl', 'age']].to_numpy()\nprint(feature) \n```", "```py\n[[1.200e+01 5.730e+00 5.200e+01]\n [1.000e-02 4.410e+00 6.300e+01]\n [8.000e-02 3.480e+00 4.600e+01]\n ...\n [3.000e+00 1.590e+00 5.500e+01]\n [5.400e+00 1.161e+01 4.000e+01]\n [0.000e+00 4.820e+00 4.600e+01]] \n```", "```py\nlabel = data['chd'].to_numpy()\nA = np.concatenate((np.ones((len(label),1)),feature),axis=1)\nb = label \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\ninit_x = np.zeros(A.shape[1])\nbest_x = sgd_for_logreg(rng, loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e6))\nprint(best_x) \n```", "```py\n[-4.06558071  0.07990955  0.18813635  0.04693118] \n```", "```py\ndef logis_acc(x, A, b):\n    return np.sum((pred_fn(x, A) > 0.5) == b)/len(b) \n```", "```py\nlogis_acc(best_x, A, b) \n```", "```py\n0.7207792207792207 \n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() \n                      else (\"mps\" if torch.backends.mps.is_available() \n                            else \"cpu\"))\nprint(\"Using device:\", device) \n```", "```py\nUsing device: mps \n```", "```py\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\nseed = 42\ntorch.manual_seed(seed)\n\nif device.type == 'cuda': # device-specific seeding and settings\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelif device.type == 'mps':\n    torch.mps.manual_seed(seed)  # MPS-specific seeding\n\ng = torch.Generator()\ng.manual_seed(seed)\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                               download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                              download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n```", "```py\nif device.type == 'cuda': # device-specific seeding and settings\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelif device.type == 'mps':\n    torch.mps.manual_seed(seed)  # MPS-specific seeding \n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28 * 28, 10)\n).to(device) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=1e-3) \n```", "```py\ndef train(dataloader, model, loss_fn, optimizer, device):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)    \n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ndef training_loop(train_loader, model, loss_fn, optimizer, device, epochs=3):\n    for epoch in range(epochs):\n        train(train_loader, model, loss_fn, optimizer, device)\n        print(f\"Epoch {epoch+1}/{epochs}\") \n```", "```py\ntraining_loop(train_loader, model, loss_fn, optimizer, device, epochs=10) \n```", "```py\ndef test(dataloader, model, loss_fn, device):\n    size = len(dataloader.dataset)\n    correct = 0    \n    model.eval()\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n\n    print(f\"Test error: {(100*(correct  /  size)):>0.1f}% accuracy\") \n```", "```py\ntest(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 78.7% accuracy \n```", "```py\nimport torch.nn.functional as F\n\ndef predict_softmax(dataloader, model, device):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            probabilities = F.softmax(pred, dim=1)\n            predictions.append(probabilities.cpu())\n\n    return torch.cat(predictions, dim=0)\n\npredictions = predict_softmax(test_loader, model, device).numpy() \n```", "```py\nprint(predictions[0]) \n```", "```py\n[4.4307188e-04 3.8354204e-04 2.0886613e-03 8.8066678e-04 3.6079765e-03\n 1.7791630e-01 1.4651606e-03 2.2466542e-01 4.8245404e-02 5.4030383e-01] \n```", "```py\npredictions[0].argmax(0) \n```", "```py\n9 \n```", "```py\nimages, labels = next(iter(test_loader))\nimages = images.squeeze().numpy()\nlabels = labels.numpy()\n\nprint(f\"{labels[0]}: '{mmids.FashionMNIST_get_class_name(labels[0])}'\") \n```", "```py\n9: 'Ankle boot' \n```"]