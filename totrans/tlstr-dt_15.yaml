- en: 7  APIs, scraping, and parsing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7  API、爬虫与解析
- en: 原文：[https://tellingstorieswithdata.com/07-gather.html](https://tellingstorieswithdata.com/07-gather.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://tellingstorieswithdata.com/07-gather.html](https://tellingstorieswithdata.com/07-gather.html)
- en: '[Acquisition](./06-farm.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[数据获取](./06-farm.html)'
- en: '[7  APIs, scraping, and parsing](./07-gather.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7  API、爬虫与解析](./07-gather.html)'
- en: '*Chapman and Hall/CRC published this book in July 2023\. You can purchase that
    [here](https://www.routledge.com/Telling-Stories-with-Data-With-Applications-in-R/Alexander/p/book/9781032134772).
    This online version has some updates to what was printed.*  ***Prerequisites**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chapman and Hall/CRC于2023年7月出版了本书。您可以[在此处](https://www.routledge.com/Telling-Stories-with-Data-With-Applications-in-R/Alexander/p/book/9781032134772)购买。此在线版本对印刷版内容进行了一些更新。*  ***先决条件**'
- en: 'Read *Turning History into Data: Data Collection, Measurement, and Inference
    in HPE*, ([Cirone and Spirling 2021](99-references.html#ref-cirone))'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读《将历史转化为数据：HPE中的数据收集、测量与推断》，([Cirone and Spirling 2021](99-references.html#ref-cirone))
- en: This paper discusses some of the challenges of creating datasets.
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文讨论了创建数据集时面临的一些挑战。
- en: Read *Two Regimes of Prison Data Collection*, ([Johnson 2021](99-references.html#ref-Johnson2021Two))
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读《监狱数据收集的两种模式》，([Johnson 2021](99-references.html#ref-Johnson2021Two))
- en: This paper compares data about prisons from the United States government with
    data from incarcerated people and the community.
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文比较了美国政府提供的监狱数据与被监禁人员及社区提供的数据。
- en: Read *Atlas of AI*, ([Crawford 2021](99-references.html#ref-crawford))
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读《人工智能图集》，([Crawford 2021](99-references.html#ref-crawford))
- en: Focus on Chapter 3 “Data”, which discusses the importance of understanding the
    sources of data.
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重点关注第3章“数据”，该章讨论了理解数据来源的重要性。
- en: '**Key concepts and skills**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**核心概念与技能**'
- en: Sometimes data are available but they are not necessarily put together for the
    purposes of being a dataset. We must gather the data.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时数据是可用的，但它们不一定是为了构成一个数据集而被整合在一起的。我们必须去收集这些数据。
- en: It can be cumbersome and annoying to clean and prepare the datasets that come
    from these unstructured sources but the resulting structured, tidy data are often
    especially exciting and useful.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理和准备这些来自非结构化来源的数据集可能繁琐且令人烦恼，但最终得到的结构化、整洁的数据通常特别令人兴奋且有用。
- en: We can gather data from a variety of sources. This includes APIs, both directly,
    which may involve semi-structured data, and indirectly through `R` packages. We
    can also gather data through reasonable and ethical web scraping. Finally, we
    may wish to gather data from PDFs.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以从多种来源收集数据。这包括直接通过API（可能涉及半结构化数据）以及间接通过`R`包。我们也可以通过合理且符合伦理的网络爬虫来收集数据。最后，我们可能希望从PDF文件中收集数据。
- en: '**Software and packages**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**软件与包**'
- en: Base `R` ([R Core Team 2024](99-references.html#ref-citeR))
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础`R` ([R Core Team 2024](99-references.html#ref-citeR))
- en: '`babynames` ([Wickham 2021](99-references.html#ref-citebabynames))'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`babynames` ([Wickham 2021](99-references.html#ref-citebabynames))'
- en: '`gh` ([Bryan and Wickham 2021](99-references.html#ref-gh))'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gh` ([Bryan and Wickham 2021](99-references.html#ref-gh))'
- en: '`here` ([Müller 2020](99-references.html#ref-here))'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`here` ([Müller 2020](99-references.html#ref-here))'
- en: '`httr` ([Wickham 2023](99-references.html#ref-citehttr))'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`httr` ([Wickham 2023](99-references.html#ref-citehttr))'
- en: '`janitor` ([Firke 2023](99-references.html#ref-janitor))'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`janitor` ([Firke 2023](99-references.html#ref-janitor))'
- en: '`jsonlite` ([Ooms 2014](99-references.html#ref-jsonlite))'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jsonlite` ([Ooms 2014](99-references.html#ref-jsonlite))'
- en: '`lubridate` ([Grolemund and Wickham 2011](99-references.html#ref-GrolemundWickham2011))'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lubridate` ([Grolemund and Wickham 2011](99-references.html#ref-GrolemundWickham2011))'
- en: '`pdftools` ([Ooms 2022a](99-references.html#ref-pdftools))'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pdftools` ([Ooms 2022a](99-references.html#ref-pdftools))'
- en: '`purrr` ([Wickham and Henry 2022](99-references.html#ref-citepurrr))'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`purrr` ([Wickham and Henry 2022](99-references.html#ref-citepurrr))'
- en: '`rvest` ([Wickham 2022](99-references.html#ref-citervest))'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rvest` ([Wickham 2022](99-references.html#ref-citervest))'
- en: '`spotifyr` ([Thompson et al. 2022](99-references.html#ref-spotifyr)) (this
    package is not on CRAN so install it with: `install.packages("devtools")` then
    `devtools::install_github("charlie86/spotifyr")`)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spotifyr` ([Thompson et al. 2022](99-references.html#ref-spotifyr))（此包不在CRAN上，请通过以下方式安装：`install.packages("devtools")`，然后
    `devtools::install_github("charlie86/spotifyr")`）'
- en: '`tesseract` ([Ooms 2022b](99-references.html#ref-citetesseract))'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tesseract` ([Ooms 2022b](99-references.html#ref-citetesseract))'
- en: '`tidyverse` ([Wickham et al. 2019](99-references.html#ref-tidyverse))'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidyverse` ([Wickham et al. 2019](99-references.html#ref-tidyverse))'
- en: '`tinytable` ([Arel-Bundock 2024](99-references.html#ref-tinytable))'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tinytable` ([Arel-Bundock 2024](99-references.html#ref-tinytable))'
- en: '`usethis` ([Wickham, Bryan, and Barrett 2022](99-references.html#ref-usethis))'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`usethis` ([Wickham, Bryan, and Barrett 2022](99-references.html#ref-usethis))'
- en: '`xml2` ([Wickham, Hester, and Ooms 2021](99-references.html#ref-xml2))'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xml2` ([Wickham, Hester, and Ooms 2021](99-references.html#ref-xml2))'
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*## 7.1 Introduction'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*## 7.1 引言'
- en: In this chapter we consider data that we must gather ourselves. This means that
    although the observations exist, we must parse, pull, clean, and prepare them
    to get the dataset that we will consider. In contrast to farmed data, discussed
    in [Chapter 6](06-farm.html), often these observations are not being made available
    for the purpose of analysis. This means that we need to be especially concerned
    with documentation, inclusion and exclusion decisions, missing data, and ethical
    behavior.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们讨论的是必须由我们自己收集的数据。这意味着尽管观测数据已经存在，但我们必须对其进行解析、提取、清理和准备，才能得到我们将要使用的数据集。与[第6章](06-farm.html)讨论的“耕作数据”不同，这些观测数据通常并非为分析目的而提供。这意味着我们需要特别关注文档记录、纳入与排除决策、缺失数据以及伦理行为。
- en: As an example of such a dataset, consider Cummins ([2022](99-references.html#ref-Cummins2022))
    who create a dataset using individual-level probate records from England between
    1892 and 1992\. They find that about one-third of the inheritance of “elites”
    is concealed. Similarly, Taflaga and Kerby ([2019](99-references.html#ref-Taflaga2019))
    construct a systematic dataset of job responsibilities based on Australian Ministerial
    telephone directories. They find substantial differences by gender. Neither wills
    nor telephone directories were created for the purpose of being included in a
    dataset. But with a respectful approach they enable insights that we could not
    get by other means. We term this “data gathering”—the data exist but we need to
    get them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类数据集为例，Cummins（[2022](99-references.html#ref-Cummins2022)）利用1892年至1992年间英格兰的个人遗嘱认证记录创建了一个数据集。他们发现“精英”阶层约三分之一的遗产被隐藏。同样，Taflaga和Kerby（[2019](99-references.html#ref-Taflaga2019)）基于澳大利亚部长电话簿构建了一个系统性的工作职责数据集。他们发现了显著的性别差异。无论是遗嘱还是电话簿，其创建初衷都不是为了被纳入数据集。但通过一种尊重数据的方式，它们能提供我们无法通过其他途径获得的洞见。我们将此称为“数据收集”——数据已然存在，但我们需要去获取它们。
- en: Decisions need to be made at the start of a project about the values we want
    the project to have. For instance, Saulnier et al. ([2022](99-references.html#ref-huggingfaceethics))
    value transparency, reproducibility, fairness, being self-critical, and giving
    credit. How might that affect the project? Valuing “giving credit” might mean
    being especially zealous about attribution and licensing. In the case of gathered
    data we should give special thought to this as the original, unedited data may
    not be ours.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 项目启动时，需要就我们希望项目秉持的价值观做出决策。例如，Saulnier等人（[2022](99-references.html#ref-huggingfaceethics）重视透明度、可复现性、公平性、自我批判精神以及给予认可。这可能会如何影响项目？重视“给予认可”可能意味着要特别注重归属和许可。对于收集的数据，我们应特别考虑这一点，因为原始的、未经编辑的数据可能并不属于我们。
- en: The results of a data science workflow cannot be better than their underlying
    data ([Bailey 2008](99-references.html#ref-bailey2008design)). Even the most-sophisticated
    statistical analysis will struggle to adjust for poorly-gathered data. This means
    when working in a team, data gathering should be overseen and at least partially
    conducted by senior members of the team. And when working by yourself, try to
    give special consideration and care to this stage.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学工作流的结果不可能优于其底层数据（[Bailey 2008](99-references.html#ref-bailey2008design)）。即使是最复杂的统计分析，也难以对收集不当的数据进行调整。这意味着在团队合作时，数据收集工作应由团队资深成员监督并至少部分参与执行。而在独立工作时，则应特别关注并谨慎对待此阶段。
- en: In this chapter we go through a variety of approaches for gathering data. We
    begin with the use of APIs and semi-structured data, such as JSON and XML. Using
    an API is typically a situation in which the data provider has specified the conditions
    under which they are comfortable providing access. An API allows us to write code
    to gather data. This is valuable because it can be efficient and scales well.
    Developing comfort with gathering data through APIs enables access to exciting
    datasets. For instance, Wong ([2020](99-references.html#ref-facebookapitrump))
    use the Facebook Political Ad API to gather 218,100 of the Trump 2020 campaign
    ads to better understand the campaign.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨多种收集数据的方法。我们从使用API和半结构化数据（如JSON和XML）开始。使用API通常是数据提供方已明确其愿意提供访问的条件的情况。API允许我们编写代码来收集数据。这很有价值，因为它高效且扩展性好。熟练掌握通过API收集数据，便能访问许多令人兴奋的数据集。例如，Wong（[2020](99-references.html#ref-facebookapitrump)）利用Facebook政治广告API收集了218,100条特朗普2020年竞选广告，以更好地理解该竞选活动。
- en: We then turn to web scraping, which we may want to use when there are data available
    on a website. As these data have typically not been put together for the purposes
    of being a dataset, it is especially important to have deliberate and definite
    values for the project. Scraping is a critical part of data gathering because
    there are many data sources where the priorities of the data provider mean they
    have not implemented an API. For instance, considerable use of web scraping was
    critical for creating COVID-19 dashboards in the early days of the pandemic ([Eisenstein
    2022](99-references.html#ref-scrapecoviddata)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们转向网络爬虫技术，当网站上有可用数据时我们可能需要使用它。由于这些数据通常并非为构建数据集而整理，因此为项目设定明确且清晰的目标值尤为重要。爬虫是数据收集的关键环节，因为许多数据源因提供方的优先级考量而未实现API。例如，在疫情初期创建COVID-19仪表板时，大量使用网络爬虫技术至关重要（[Eisenstein
    2022](99-references.html#ref-scrapecoviddata)）。
- en: Finally, we consider gathering data from PDFs. This enables the construction
    of interesting datasets, especially those contained in government reports and
    old books. Indeed, while freedom of information legislation exists in many countries
    and require the government to make data available, these all too often result
    in spreadsheets being shared as PDFs, even when they were a CSV to begin with.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨从PDF文件中收集数据。这种方法能够构建有趣的数据集，尤其适用于政府报告和古籍中包含的数据。事实上，尽管许多国家制定了信息自由法规，要求政府公开数据，但结果往往是电子表格以PDF格式共享，即使它们最初是CSV格式。
- en: Gathering data can require more of us than using farmed data, but it allows
    us to explore datasets and answer questions that we could not otherwise. Some
    of the most exciting work in the world uses gathered data, but it is especially
    important that we approach it with respect.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数据可能比使用现成数据要求更高，但它使我们能够探索数据集并回答其他方法无法解决的问题。世界上一些最激动人心的研究都运用了收集的数据，但我们必须以尊重的态度对待这项工作，这一点尤为重要。
- en: 7.2 APIs
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 API
- en: In everyday language, and for our purposes, an Application Programming Interface
    (API) is a situation in which someone has set up specific files on their computer
    such that we can follow their instructions to get them. For instance, when we
    use a gif on Slack, one way it could work in the background is that Slack asks
    Giphy’s server for the appropriate gif, Giphy’s server gives that gif to Slack,
    and then Slack inserts it into the chat. The way in which Slack and Giphy interact
    is determined by Giphy’s API. More strictly, an API is an application that runs
    on a server that we access using the HTTP protocol.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常用语及我们的研究场景中，应用程序编程接口（API）指这样一种情形：他人在其计算机上设置了特定文件，使我们能够按照其说明获取这些文件。例如，当我们在Slack中使用GIF动图时，其后台的一种可能运作方式是：Slack向Giphy服务器请求对应的GIF动图，Giphy服务器将该动图提供给Slack，随后Slack将其插入聊天界面。Slack与Giphy的交互方式由Giphy的API决定。更严格地说，API是一种运行在服务器上的应用程序，我们通过HTTP协议进行访问。
- en: 'Here we focus on using APIs for gathering data. In that context an API is a
    website that is set up for another computer to be able to access it, rather than
    a person. For instance, we could go to [Google Maps](https://www.google.com/maps).
    And we could then scroll and click and drag to center the map on, say, Canberra,
    Australia. Or we could paste [this link](https://www.google.com/maps/@-35.2812958,149.1248113,16z)
    into the browser. By pasting that link, rather than navigating, we have mimicked
    how we will use an API: provide a URL and be given something back. In this case
    the result should be a map like [Figure 7.1](#fig-focuson2020).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此重点关注使用API收集数据。在此背景下，API是一种专为其他计算机而非人类访问而设置的网站。例如，我们可以访问[谷歌地图](https://www.google.com/maps)，然后通过滚动、点击和拖拽将地图中心定位到澳大利亚堪培拉等地。或者，我们可以将[此链接](https://www.google.com/maps/@-35.2812958,149.1248113,16z)粘贴到浏览器中。通过粘贴链接而非手动导航，我们模拟了使用API的方式：提供一个URL并获取返回内容。本例中，结果应是一张类似[图7.1](#fig-focuson2020)的地图。
- en: '![](../Images/afd3bfc181560c91543bc1e12a859c71.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afd3bfc181560c91543bc1e12a859c71.png)'
- en: 'Figure 7.1: Example of an API response from Google Maps, as of 12 February
    2023'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：截至2023年2月12日，谷歌地图API响应示例
- en: The advantage of using an API is that the data provider usually specifies the
    data that they are willing to provide, and the terms under which they will provide
    it. These terms may include aspects such as rate limits (i.e. how often we can
    ask for data), and what we can do with the data, for instance, we might not be
    allowed to use it for commercial purposes, or to republish it. As the API is being
    provided specifically for us to use it, it is less likely to be subject to unexpected
    changes or legal issues. Because of this it is clear that when an API is available,
    we should try to use it rather than web scraping.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用API的优势在于，数据提供者通常会明确他们愿意提供的数据以及提供这些数据的条款。这些条款可能包括速率限制（即我们可以请求数据的频率）以及我们可以对数据做什么，例如，可能不允许用于商业目的或重新发布。由于API是专门为我们使用而提供的，因此不太可能受到意外更改或法律问题的影响。因此，很明显，当有API可用时，我们应该尝试使用它而不是进行网络抓取。
- en: We will now go through a few case studies of using APIs. In the first we deal
    directly with an API using `httr`. And then we access data from Spotify using
    `spotifyr`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将通过几个使用API的案例研究。在第一个案例中，我们直接使用`httr`处理一个API。然后，我们使用`spotifyr`从Spotify获取数据。
- en: 7.2.1 arXiv, NASA, and Dataverse
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 arXiv、NASA和Dataverse
- en: After installing and loading `httr` we use `GET()` to obtain data from an API
    directly. This will try to get some specific data and the main argument is “url”.
    This is similar to the Google Maps example in [Figure 7.1](#fig-focuson2020) where
    the specific information that we were interested in was a map.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 安装并加载`httr`后，我们使用`GET()`直接从API获取数据。这将尝试获取一些特定数据，其主要参数是“url”。这类似于[图7.1](#fig-focuson2020)中的Google地图示例，其中我们感兴趣的具体信息是一张地图。
- en: 7.2.1.1 arXiv
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1.1 arXiv
- en: In this case study we will use an [API provided by arXiv](https://arxiv.org/help/api/).
    arXiv is an online repository for academic papers before they go through peer
    review. These papers are typically referred to as “pre-prints”. We use `GET()`
    to ask arXiv to obtain some information about a pre-print by providing a URL.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们将使用[arXiv提供的API](https://arxiv.org/help/api/)。arXiv是一个在线存储库，用于存放经过同行评审之前的学术论文。这些论文通常被称为“预印本”。我们通过提供URL，使用`GET()`请求arXiv获取关于某个预印本的一些信息。
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*We can use `status_code()` to check our response. For instance, 200 means
    a success, while 400 means we received an error from the server. Assuming we received
    something back from the server, we can use `content()` to display it. In this
    case we have received XML formatted data. XML is a markup language where entries
    are identified by tags, which can be nested within other tags. After installing
    and loading `xml2` we can read XML using `read_xml()`. XML is a semi-formatted
    structure, and it can be useful to start by having a look at it using `html_structure()`.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们可以使用`status_code()`来检查我们的响应。例如，200表示成功，而400表示我们从服务器收到了错误。假设我们从服务器收到了某些内容，我们可以使用`content()`来显示它。在这种情况下，我们收到了XML格式的数据。XML是一种标记语言，其中的条目由标签标识，这些标签可以嵌套在其他标签内。安装并加载`xml2`后，我们可以使用`read_xml()`读取XML。XML是一种半格式化结构，使用`html_structure()`初步查看它可能会很有用。'
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*We might like to create a dataset based on extracting various aspects of this
    XML tree. For instance, we might look at “entry”, which is the eighth item, and
    in particular obtain the “title” and the “URL”, which are the fourth and ninth
    items, respectively, within “entry”.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们可能希望基于提取此XML树的各种方面来创建一个数据集。例如，我们可以查看“entry”，即第八项，并特别获取“title”和“URL”，它们分别是“entry”内的第四和第九项。'
- en: '[PRE3]**  ***#### 7.2.1.2 NASA Astronomy Picture of the Day'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]**  ***#### 7.2.1.2 NASA每日天文图'
- en: To consider another example, each day, NASA provides the Astronomy Picture of
    the Day (APOD) through its [APOD API](https://api.nasa.gov). We can use `GET()`
    to obtain the URL for the photo on particular dates and then display it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 再考虑另一个例子，美国国家航空航天局（NASA）每天通过其[APOD API](https://api.nasa.gov)提供“每日天文图”。我们可以使用`GET()`获取特定日期的照片URL，然后显示它。
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Examining the returned data using `content()`, we can see that we are provided
    with various fields, such as date, title, explanation, and a URL.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用`content()`检查返回的数据，我们可以看到我们获得了各种字段，如日期、标题、解释和一个URL。'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE6]'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE8]'
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE10]'
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*[PRE12]****  ***We can provide that URL to `include_graphics()` from `knitr`
    to display it ([Figure 7.2](#fig-nasaone)).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE12]****  ***我们可以将该URL提供给`knitr`的`include_graphics()`来显示它（[图7.2](#fig-nasaone)）。'
- en: '![](../Images/ec8efbfe286dd94d07ab7813ed0e21cf.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec8efbfe286dd94d07ab7813ed0e21cf.png)'
- en: '(a) Tranquility Base Panorama (Image Credit: Neil Armstrong, Apollo 11, NASA)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 宁静基地全景图（图片来源：尼尔·阿姆斯特朗，阿波罗 11 号，NASA）
- en: 'Figure 7.2: Images obtained from the NASA APOD API****  ***#### 7.2.1.3 Dataverse'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：从 NASA APOD API 获取的图像****  ***#### 7.2.1.3 Dataverse
- en: Finally, another common API response in semi-structured form is JSON. JSON is
    a human-readable way to store data that can be parsed by machines. In contrast
    to, say, a CSV, where we are used to rows and columns, JSON uses key-value pairs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一种常见的半结构化 API 响应格式是 JSON。JSON 是一种人类可读的、机器可解析的数据存储方式。与我们已经习惯的行列结构的 CSV 不同，JSON
    使用键值对。
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*We can parse JSON with `jsonlite`. To consider a specific example, we use
    a “Dataverse” which is a web application that makes it easier to share datasets.
    We can use an API to query a demonstration dataverse. For instance, we might be
    interested in datasets related to politics.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们可以使用 `jsonlite` 解析 JSON。考虑一个具体例子，我们使用“Dataverse”——一个便于共享数据集的 Web 应用程序。我们可以使用
    API 查询一个演示用的 Dataverse。例如，我们可能对与政治相关的数据集感兴趣。'
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*[PRE15]*  *We could look at the dataset using `View(politics_datasets)`, which
    would allow us to expand the tree based on what we are interested in. We can even
    get the code that we need to focus on different aspects by hovering on the item
    and then clicking the icon with the green arrow ([Figure 7.3](#fig-jsonfirst)).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE15]*  *我们可以使用 `View(politics_datasets)` 查看数据集，这允许我们根据兴趣展开树状结构。我们甚至可以通过悬停在项目上，然后点击带有绿色箭头的图标来获取关注不同方面所需的代码（[图
    7.3](#fig-jsonfirst)）。'
- en: '![](../Images/c6093076a61736f04b6362ce66c2c468.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6093076a61736f04b6362ce66c2c468.png)'
- en: 'Figure 7.3: Example of hovering over a JSON element, “items”, where the icon
    with a green arrow can be clicked on to get the code that would focus on that
    element'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：悬停在 JSON 元素“items”上的示例，可以点击带有绿色箭头的图标来获取聚焦于该元素的代码
- en: This tells us how to obtain the dataset of interest.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们如何获取感兴趣的数据集。
- en: '[PRE16]********  ***### 7.2.2 Spotify'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE16]********  ***### 7.2.2 Spotify'
- en: Sometimes there is an `R` package built around an API and allows us to interact
    with it in ways that are similar what we have seen before. For instance, `spotifyr`
    is a wrapper around the Spotify API. When using APIs, even when they are wrapped
    in an `R` package, in this case `spotifyr`, it is important to read the terms
    under which access is provided.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会有一个围绕 API 构建的 `R` 包，允许我们以类似之前见过的方式与之交互。例如，`spotifyr` 就是 Spotify API 的一个封装包。在使用
    API 时，即使它们被封装在 `R` 包中（本例中是 `spotifyr`），阅读访问条款也非常重要。
- en: To access the Spotify API, we need a [Spotify Developer Account](https://developer.spotify.com/dashboard/).
    This is free but will require logging in with a Spotify account and then accepting
    the Developer Terms ([Figure 7.4](#fig-spotifyaccept)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 Spotify API，我们需要一个 [Spotify 开发者账户](https://developer.spotify.com/dashboard/)。这是免费的，但需要使用
    Spotify 账户登录并接受开发者条款（[图 7.4](#fig-spotifyaccept)）。
- en: '![](../Images/728ebffe0411c5b89542c6a03f62393d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/728ebffe0411c5b89542c6a03f62393d.png)'
- en: 'Figure 7.4: Spotify Developer Account Terms agreement page'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：Spotify 开发者账户条款协议页面
- en: Continuing with the registration process, in our case, we “do not know” what
    we are building and so Spotify requires us to use a non-commercial agreement which
    is fine. To use the Spotify API we need a “Client ID” and a “Client Secret”. These
    are things that we want to keep to ourselves because otherwise anyone with the
    details could use our developer account as though they were us. One way to keep
    these details secret with minimum hassle is to keep them in our “System Environment”.
    In this way, when we push to GitHub they should not be included. To do this we
    will load and use `usethis` to modify our System Environment. In particular, there
    is a file called “.Renviron” which we will open and then add our “Client ID” and
    “Client Secret”.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 继续注册流程，在我们的案例中，我们“不知道”要构建什么，因此 Spotify 要求我们使用非商业协议，这没有问题。要使用 Spotify API，我们需要一个“客户端
    ID”和一个“客户端密钥”。这些信息我们需要保密，否则任何知道这些信息的人都可以像我们一样使用我们的开发者账户。一种省事且能保密这些信息的方法是将它们保存在我们的“系统环境变量”中。这样，当我们推送到
    GitHub 时，它们就不会被包含进去。为此，我们将加载并使用 `usethis` 来修改我们的系统环境变量。具体来说，有一个名为“.Renviron”的文件，我们将打开它并添加我们的“客户端
    ID”和“客户端密钥”。
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*When we run `edit_r_environ()`, a “.Renviron” file will open and we can add
    our “Spotify Client ID” and “Client Secret”. Use the same names, because `spotifyr`
    will look in our environment for keys with those specific names. Being careful
    to use single quotes is important here even though we normally use double quotes
    in this book.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*当我们运行 `edit_r_environ()` 时，会打开一个“.Renviron”文件，我们可以在此添加我们的“Spotify客户端ID”和“客户端密钥”。请使用相同的名称，因为
    `spotifyr` 会在我们的环境中查找具有这些特定名称的密钥。这里小心使用单引号很重要，尽管我们在本书中通常使用双引号。'
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Save the “.Renviron” file, and then restart R: “Session” \(\rightarrow\) “Restart
    R”. We can now use our “Spotify Client ID” and “Client Secret” as needed. And
    functions that require those details as arguments will work without them being
    explicitly specified again.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*保存“.Renviron”文件，然后重启R：“会话” → “重启R”。现在我们可以根据需要来使用我们的“Spotify客户端ID”和“客户端密钥”。那些需要这些信息作为参数的函数将无需再次显式指定即可工作。'
- en: To try this out we install and load `spotifyr`. We will get and save some information
    about Radiohead, the English rock band, using `get_artist_audio_features()`. One
    of the required arguments is `authorization`, but as that is set, by default,
    to look at the “.Renviron” file, we do not need to specify it here.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试这一点，我们安装并加载 `spotifyr`。我们将使用 `get_artist_audio_features()` 获取并保存关于英国摇滚乐队电台司令的一些信息。其中一个必需的参数是
    `authorization`，但由于该参数默认设置为查看“.Renviron”文件，我们在此无需指定它。
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE20]'
- en: '*There is a variety of information available based on songs. We might be interested
    to see whether their songs are getting longer over time ([Figure 7.5](#fig-readioovertime)).
    Following the guidance in [Chapter 5](05-graphs_tables_maps.html) this is a nice
    opportunity to additionally use a boxplot to communicate summary statistics by
    album at the same time.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于歌曲有各种各样的信息可用。我们可能感兴趣的是看看他们的歌曲是否随着时间的推移变得更长（[图 7.5](#fig-readioovertime)）。遵循[第5章](05-graphs_tables_maps.html)的指导，这是一个很好的机会，可以同时使用箱线图来传达按专辑汇总的统计数据。'
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*![](../Images/0afb82a03ce84939f1677b823cb68b8f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](../Images/0afb82a03ce84939f1677b823cb68b8f.png)'
- en: 'Figure 7.5: Length of each Radiohead song, over time, as gathered from Spotify*  *One
    interesting variable provided by Spotify about each song is “valence”. The Spotify
    [documentation](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features)
    describes this as a measure between zero and one that signals “the musical positiveness”
    of the track with higher values being more positive. We might be interested to
    compare valence over time between a few artists, for instance, Radiohead, the
    American rock band The National, and the American singer Taylor Swift.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：从Spotify收集的电台司令每首歌曲的长度随时间变化图*  *Spotify为每首歌曲提供的一个有趣变量是“效价”。Spotify[文档](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features)将其描述为一个介于0到1之间的度量值，表示曲目的“音乐积极性”，数值越高表示越积极。我们可能感兴趣的是比较几位艺术家（例如电台司令、美国摇滚乐队国民乐队和美国歌手泰勒·斯威夫特）的效价随时间的变化。
- en: First, we need to gather the data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要收集数据。
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*Then we can bring them together and make the graph ([Figure 7.6](#fig-swiftyvsnationalvsradiohead)).
    This appears to show that while Taylor Swift and Radiohead have largely maintained
    their level of valence over time, The National has decreased theirs.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*接着，我们可以将它们整合在一起并绘制图表（[图 7.6](#fig-swiftyvsnationalvsradiohead)）。这似乎表明，虽然泰勒·斯威夫特和电台司令的“效价”水平总体上随时间推移保持稳定，但国民乐队的效价水平有所下降。'
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*![](../Images/81547c5eba6fd1fa6f2121d817444d4c.png)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](../Images/81547c5eba6fd1fa6f2121d817444d4c.png)'
- en: 'Figure 7.6: Comparing valence, over time, for Radiohead, Taylor Swift, and
    The National*  *How amazing that we live in a world where all that information
    is available with very little effort or cost! And having gathered the data, there
    much that could be done. For instance, Pavlik ([2019](99-references.html#ref-kaylinpavlik))
    uses an expanded dataset to classify musical genres and The Economist ([2022](99-references.html#ref-theeconomistonspotify))
    looks at how language is associated with music streaming on Spotify. Our ability
    to gather such data enables us to answer questions that had to be considered experimentally
    in the past. For instance, Salganik, Dodds, and Watts ([2006](99-references.html#ref-salganik2006experimental))
    had to use experimental data to analyze the social aspect of what makes a hit
    song, rather than the observational data we can now access.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：比较 Radiohead、Taylor Swift 和 The National 随时间变化的效价*  *我们生活在一个只需极少努力或成本就能获取所有这些信息的世界，多么令人惊叹！收集到数据后，还有很多事情可以做。例如，Pavlik
    ([2019](99-references.html#ref-kaylinpavlik)) 使用扩展的数据集对音乐流派进行分类，而《经济学人》([2022](99-references.html#ref-theeconomistonspotify))
    则研究了语言如何与 Spotify 上的音乐流媒体相关联。我们收集此类数据的能力使我们能够回答过去必须通过实验才能考虑的问题。例如，Salganik、Dodds
    和 Watts ([2006](99-references.html#ref-salganik2006experimental)) 不得不使用实验数据来分析一首热门歌曲的社会因素，而不是我们现在可以获取的观察数据。
- en: That said, it is worth thinking about what valence is purporting to measure.
    Little information is available in the Spotify documentation how it was created.
    It is doubtful that one number can completely represent how positive is a song.
    And what about the songs from these artists that are not on Spotify, or even publicly
    released? This is a nice example of how measurement and sampling pervade all aspects
    of telling stories with data.**********  ***## 7.3 Web scraping
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，值得思考的是，效价声称要衡量的究竟是什么。Spotify 文档中关于其创建方式的信息很少。一个数字能否完全代表一首歌曲的积极程度是值得怀疑的。那么，这些艺术家未在
    Spotify 上架、甚至未公开发行的歌曲呢？这是一个很好的例子，说明了测量和抽样如何渗透到用数据讲述故事的方方面面。**********  ***## 7.3
    网络爬取
- en: 7.3.1 Principles
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 原则
- en: Web scraping is a way to get data from websites. Rather than going to a website
    using a browser and then saving a copy of it, we write code that does it for us.
    This opens considerable data to us, but on the other hand, it is not typically
    data that are being made available for these purposes. This means that it is especially
    important to be respectful. While generally not illegal, the specifics about the
    legality of web scraping depend on jurisdictions and what we are doing, and so
    it is also important to be mindful. Even if our use is not commercially competitive,
    of particular concern is the conflict between the need for our work to be reproducible
    with the need to respect terms of service that may disallow data republishing
    ([Luscombe, Dick, and Walby 2021](99-references.html#ref-luscombe2021algorithmic)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬取是一种从网站获取数据的方式。我们不是通过浏览器访问网站然后保存副本，而是编写代码来为我们完成这项工作。这为我们打开了大量数据，但另一方面，这些数据通常并非为此目的而提供。这意味着保持尊重尤为重要。虽然通常不违法，但网络爬取的法律细节取决于司法管辖区和我们的具体行为，因此保持谨慎也很重要。即使我们的使用不具有商业竞争性，一个特别值得关注的问题是：我们工作的可重复性需求与尊重可能禁止数据重新发布的服务条款之间的冲突
    ([Luscombe, Dick, and Walby 2021](99-references.html#ref-luscombe2021algorithmic))。
- en: Privacy often trumps reproducibility. There is also a considerable difference
    between data being publicly available on a website and being scraped, cleaned,
    and prepared into a dataset which is then publicly released. For instance, Kirkegaard
    and Bjerrekær ([2016](99-references.html#ref-kirkegaard2016okcupid)) scraped publicly
    available OKCupid profiles and then made the resulting dataset easily available
    ([Hackett 2016](99-references.html#ref-hackett2016researchers)). Zimmer ([2018](99-references.html#ref-zimmer2018addressing))
    details some of the important considerations that were overlooked including “minimizing
    harm”, “informed consent”, and ensuring those in the dataset maintain “privacy
    and confidentiality”. While it is correct to say that OKCupid made data public,
    they did so in a certain context, and when their data was scraped that context
    was changed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私常常优先于可复现性。数据在网站上公开可用与被抓取、清理、准备成数据集然后公开发布之间也存在相当大的差异。例如，Kirkegaard 和 Bjerrekær（[2016](99-references.html#ref-kirkegaard2016okcupid)）抓取了公开可用的
    OKCupid 个人资料，然后使生成的数据集易于获取（[Hackett 2016](99-references.html#ref-hackett2016researchers)）。Zimmer（[2018](99-references.html#ref-zimmer2018addressing)）详细阐述了一些被忽视的重要考量，包括“最小化伤害”、“知情同意”，以及确保数据集中的个人保持“隐私和保密性”。虽然可以说
    OKCupid 公开了数据，但他们是在特定背景下这样做的，而当他们的数据被抓取时，这个背景就改变了。
- en: '*Oh, you think we have good data on that!* *Police violence is particularly
    concerning because of the need for trust between the police and society. Without
    good data it is difficult to hold police departments accountable, or know whether
    there is an issue, but getting data is difficult ([Thomson-DeVeaux, Bronner, and
    Sharma 2021](99-references.html#ref-bronnerpolicenordata)). The fundamental problem
    is that there is no way to easily simplify an encounter that results in violence
    into a dataset. Two popular datasets draw on web scraping:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*哦，你以为我们在这方面有很好的数据吗！* *警察暴力尤其令人担忧，因为警察与社会之间需要信任。没有良好的数据，就很难追究警察部门的责任，或者知道是否存在问题，但获取数据很困难（[Thomson-DeVeaux,
    Bronner, and Sharma 2021](99-references.html#ref-bronnerpolicenordata)）。根本问题在于，无法轻易将导致暴力的遭遇简化为数据集。两个流行的数据集依赖于网络抓取：*'
- en: “Mapping Police Violence”; and
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “警察暴力地图”；以及
- en: “Fatal Force Database”.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “致命武力数据库”。
- en: Bor et al. ([2018](99-references.html#ref-Bor2018)) use “Mapping Police Violence”
    to examine police killings of Black Americans, especially when unarmed, and find
    a substantial effect on the mental health of Black Americans. Responses to the
    paper, such as Nix and Lozada ([2020](99-references.html#ref-Nix2020)), have special
    concern with the coding of the dataset, and after re-coding draw different conclusions.
    An example of a coding difference is the unanswerable question, because it depends
    on context and usage, of whether to code an individual who was killed with a toy
    firearm as “armed” or “unarmed”. We may want a separate category, but some simplification
    is necessary for the construction of a quantitative dataset. *The Washington Post*
    writes many articles using the “Fatal Force Database” ([The Washington Post 2023](99-references.html#ref-washpostfatalforce)).
    Jenkins et al. ([2022](99-references.html#ref-washpostfatalforcemethods)) describes
    their methodology and the challenges of standardization. Comer and Ingram ([2022](99-references.html#ref-Comer2022))
    compare the datasets and find similarities, but document ways in which the datasets
    are different.*  *Web scraping is an invaluable source of data. But they are typically
    datasets that can be created as a by-product of someone trying to achieve another
    aim. And web scraping imposes a cost on the website host, and so we should reduce
    this to the extent possible. For instance, a retailer may have a website with
    their products and their prices. That has not been created deliberately as a source
    of data, but we can scrape it to create a dataset. The following principles may
    be useful to guide web scraping.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Bor等人（[2018](99-references.html#ref-Bor2018)）使用“Mapping Police Violence”来研究警察杀害美国黑人的情况，尤其是手无寸铁的情况，并发现这对美国黑人的心理健康有显著影响。对该论文的回应，如Nix和Lozada（[2020](99-references.html#ref-Nix2020)），特别关注数据集的编码问题，并在重新编码后得出了不同的结论。编码差异的一个例子是一个无法回答的问题，因为它取决于上下文和使用情况，即是否将被玩具枪杀害的人编码为“武装”或“非武装”。我们可能需要一个单独的类别，但为了构建定量数据集，某些简化是必要的。*《华盛顿邮报》使用“Fatal
    Force Database”（[The Washington Post 2023](99-references.html#ref-washpostfatalforce)）撰写了许多文章。Jenkins等人（[2022](99-references.html#ref-washpostfatalforcemethods)）描述了他们的方法论和标准化的挑战。Comer和Ingram（[2022](99-references.html#ref-Comer2022)）比较了这些数据集并发现了相似之处，但也记录了数据集之间的差异。*  *网络爬虫是宝贵的数据来源。但它们通常是某人试图实现另一个目标时产生的副产品。网络爬虫会给网站主机带来成本，因此我们应尽可能减少这种影响。例如，零售商可能有一个展示产品和价格的网站，这并非特意作为数据源创建，但我们可以通过爬取来创建数据集。以下原则可能有助于指导网络爬虫。
- en: Avoid it. Try to use an API wherever possible.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽量避免。尽可能使用API。
- en: Abide by their desires. Some websites have a “robots.txt” file that contains
    information about what they are comfortable with scrapers doing. In general, if
    it exists, a “robots.txt” file can be accessed by appending “robots.txt” to the
    base URL. For instance, the “robots.txt” file for https://www.google.com, can
    be accessed at https://www.google.com/robots.txt. Note if there are folders listed
    against “Disallow:”. These are the folders that the website would not like to
    be scraped. And also note any instances of “Crawl-delay:”. This is the number
    of seconds the website would like you to wait between visits.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遵守网站的意愿。一些网站有一个“robots.txt”文件，其中包含了他们允许爬虫操作的信息。通常，如果存在这样的文件，可以通过在基础URL后添加“robots.txt”来访问。例如，https://www.google.com
    的“robots.txt”文件可以在 https://www.google.com/robots.txt 访问。注意“Disallow:”下列出的文件夹，这些是网站不希望被爬取的文件夹。同时注意“Crawl-delay:”的实例，这是网站希望你每次访问之间等待的秒数。
- en: Reduce the impact.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少影响。
- en: Slow down the scraper, for instance, rather than having it visit the website
    every second, slow it down using `sys.sleep()`. If you only need a few hundred
    files, then why not just have it visit the website a few times a minute, running
    in the background overnight?
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低爬虫速度，例如，不要让它每秒访问网站，而是使用 `sys.sleep()` 来减慢速度。如果你只需要几百个文件，何不让它每分钟只访问网站几次，在夜间后台运行呢？
- en: Consider the timing of when you run the scraper. For instance, if you are scraping
    a retailer then maybe set the script to run from 10pm through to the morning,
    when fewer customers are likely using the site. Similarly, if it is a government
    website and they have a regular monthly release, then it might be polite to avoid
    that day.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑运行爬虫的时机。例如，如果你正在爬取零售商网站，或许可以将脚本设置为从晚上10点运行到次日早晨，因为此时使用网站的客户可能较少。同样地，如果是政府网站，并且他们每月定期发布数据，那么避开那一天可能更为礼貌。
- en: Take only what is needed. For instance, you do not need to scrape the entirety
    of Wikipedia if all you need is the names of the ten largest cities in Croatia.
    This reduces the impact on the website, and allows us to more easily justify our
    actions.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只取所需。例如，如果您只需要克罗地亚十大城市的名称，则无需爬取整个维基百科。这减少了对网站的影响，并使我们更容易为自己的行为辩护。
- en: Only scrape once. This means you should save everything as you go so that you
    do not have to re-collect data when the scraper inevitably fails at some point.
    For instance, you will typically spend considerable time getting a scraper working
    on one page, but typically the page structure will change at some point and the
    scraper will need to be updated. Once you have the data, you should save that
    original, unedited data separately to the modified data. If you need data over
    time then you will need to go back, but this is different than needlessly re-scraping
    a page.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只爬取一次。这意味着您应该在操作过程中保存所有内容，以便在爬虫不可避免地出现故障时无需重新收集数据。例如，您通常会花费大量时间让爬虫在一个页面上正常工作，但通常页面结构会在某个时刻发生变化，届时需要更新爬虫。一旦获得数据，您应该将原始的、未经编辑的数据与修改后的数据分开保存。如果您需要随时间变化的数据，那么您需要返回重新爬取，但这与不必要地重复爬取页面是不同的。
- en: Do not republish the pages that were scraped (this contrasts with datasets that
    you create from it).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要重新发布被爬取的页面（这与您从中创建的数据集形成对比）。
- en: Take ownership and ask permission if possible. At a minimum all scripts should
    have contact details in them. Depending on the circumstances, it may be worthwhile
    asking for permission before you scrape.*  *### 7.3.2 HTML/CSS essentials
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽可能承担责任并请求许可。至少所有脚本中应包含联系方式。根据情况，在爬取前请求许可可能是值得的。*  *### 7.3.2 HTML/CSS 基础
- en: 'Web scraping is possible by taking advantage of the underlying structure of
    a webpage. We use patterns in the HTML/CSS to get the data that we want. To look
    at the underlying HTML/CSS we can either:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 网页爬取是通过利用网页的底层结构实现的。我们利用 HTML/CSS 中的模式来获取所需的数据。要查看底层的 HTML/CSS，我们可以：
- en: open a browser, right-click, and choose something like “Inspect”; or
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器，右键单击，并选择类似“检查”的选项；或者
- en: save the website and then open it with a text editor rather than a browser.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存网站，然后用文本编辑器而不是浏览器打开它。
- en: 'HTML/CSS is a markup language based on matching tags. If we want text to be
    bold, then we would use something like:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: HTML/CSS 是一种基于匹配标签的标记语言。如果我们希望文本加粗，我们会使用类似这样的代码：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Similarly, if we want a list, then we start and end the list as well as indicating
    each item.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果我们想要一个列表，那么我们会开始和结束列表，并标明每个项目。
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When scraping we will search for these tags.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬取时，我们将搜索这些标签。
- en: To get started, we can pretend that we obtained some HTML from a website, and
    that we want to get the name from it. We can see that the name is in bold, so
    we want to focus on that feature and extract it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以假设我们从某个网站获取了一些 HTML，并且我们想从中提取名称。我们可以看到名称是加粗的，因此我们希望专注于该特征并将其提取出来。
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*`rvest` is part of the `tidyverse` so it does not have to be installed, but
    it is not part of the core, so it does need to be loaded. After that, use `read_html()`
    to read in the data.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*`rvest` 是 `tidyverse` 的一部分，因此无需单独安装，但它不属于核心组件，所以需要手动加载。之后，使用 `read_html()`
    来读取数据。'
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*[PRE28]*  *The language used by `rvest` to look for tags is “node”, so we
    focus on bold nodes. By default `html_elements()` returns the tags as well. We
    extract the text with `html_text()`.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE28]*  *`rvest` 用于查找标签的语言是“节点”，因此我们专注于加粗节点。默认情况下，`html_elements()` 会同时返回标签。我们使用
    `html_text()` 来提取文本。'
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE30]'
- en: '[PRE31]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*[PRE32]**  **Web scraping is an exciting source of data, and we will now go
    through some examples. But in contrast to these examples, information is not usually
    all on one page. Web scraping quickly becomes a difficult art form that requires
    practice. For instance, we distinguish between an index scrape and a contents
    scrape. The former is scraping to build the list of URLs that have the content
    you want, while the latter is to get the content from those URLs. An example is
    provided by Luscombe, Duncan, and Walby ([2022](99-references.html#ref-luscombe2022jumpstarting)).
    If you end up doing much web scraping, then `polite` ([Perepolkin 2022](99-references.html#ref-polite))
    may be helpful to better optimize your workflow. And using GitHub Actions to allow
    for larger and slower scrapes over time.****  ***### 7.3.3 Book information'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE32]**  **网络抓取是一种令人兴奋的数据来源，我们现在将通过一些示例进行说明。但与这些示例不同，信息通常并不全部集中在一个页面上。网络抓取很快会变成一门需要练习的困难艺术。例如，我们区分索引抓取和内容抓取。前者是抓取以构建包含所需内容的
    URL 列表，而后者是从这些 URL 中获取内容。Luscombe、Duncan 和 Walby ([2022](99-references.html#ref-luscombe2022jumpstarting))
    提供了一个示例。如果你最终需要进行大量网络抓取，那么 `polite` ([Perepolkin 2022](99-references.html#ref-polite))
    可能有助于更好地优化你的工作流程。并且可以使用 GitHub Actions 来支持随时间推移进行更大、更慢的抓取。****  ***### 7.3.3 书籍信息'
- en: 'In this case study we will scrape a list of books available [here](https://rohansbooks.com).
    We will then clean the data and look at the distribution of the first letters
    of author surnames. It is slightly more complicated than the example above, but
    the underlying workflow is the same: download the website, look for the nodes
    of interest, extract the information, and clean it.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们将抓取[此处](https://rohansbooks.com)提供的书籍列表。然后我们将清理数据并查看作者姓氏首字母的分布。这比上面的示例稍微复杂一些，但基本工作流程是相同的：下载网站，寻找感兴趣的节点，提取信息，并进行清理。
- en: We use `rvest` to download a website, and to then navigate the HTML to find
    the aspects that we are interested in. And we use the `tidyverse` to clean the
    dataset. We first need to go to the website and then save a local copy.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `rvest` 来下载网站，然后导航 HTML 以找到我们感兴趣的方面。我们使用 `tidyverse` 来清理数据集。我们首先需要访问网站，然后保存一个本地副本。
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*We need to navigate the HTML to get the aspects that we want. And then try
    to get the data into a tibble as quickly as possible because this will allow us
    to more easily use `dplyr` verbs and other functions from the `tidyverse`.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们需要导航 HTML 以获取我们想要的方面。然后尝试尽快将数据放入 tibble 中，因为这将使我们更容易使用 `dplyr` 动词和 `tidyverse`
    中的其他函数。'
- en: See [Online Appendix A](20-r_essentials.html) if this is unfamiliar to you.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对此不熟悉，请参阅[在线附录 A](20-r_essentials.html)。
- en: '[PRE34]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*[PRE35]'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE35]'
- en: '*[PRE36]*  *To get the data into a tibble we first need to use HTML tags to
    identify the data that we are interested in. If we look at the website then we
    know we need to focus on list items ([Figure 7.7 (a)](#fig-rohansbooks-display)).
    And we can look at the source, focusing particularly on looking for a list ([Figure 7.7
    (b)](#fig-rohansbooks-html)).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE36]*  *为了将数据放入 tibble 中，我们首先需要使用 HTML 标签来识别我们感兴趣的数据。如果我们查看网站，就会知道需要关注列表项（[图
    7.7 (a)](#fig-rohansbooks-display)）。我们可以查看源代码，特别关注寻找列表（[图 7.7 (b)](#fig-rohansbooks-html)）。'
- en: '![](../Images/f65ffef9474575c08d02f1f82ddd6d4f.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f65ffef9474575c08d02f1f82ddd6d4f.png)'
- en: (a) Books website as displayed
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 网站显示的书籍页面
- en: '![](../Images/9aeae5c6cf4ecd66247cb1879f7ca1db.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9aeae5c6cf4ecd66247cb1879f7ca1db.png)'
- en: (b) HTML for the top of the books website and the list of books
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 书籍网站顶部及书籍列表的 HTML
- en: 'Figure 7.7: Screen captures from the books website as at 16 June 2022'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：截至 2022 年 6 月 16 日的书籍网站截图
- en: The tag for a list item is “li”, so we can use that to focus on the list.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表项的标签是“li”，因此我们可以用它来定位列表。
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*[PRE38]*  *We now need to clean the data. First we want to separate the title
    and the author using `separate()` and then clean up the author and title columns.
    We can take advantage of the fact that the year is present and separate based
    on that.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE38]*  *我们现在需要清理数据。首先，我们想使用 `separate()` 将标题和作者分开，然后清理作者和标题列。我们可以利用年份存在这一事实，并基于此进行分离。'
- en: '[PRE39]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*[PRE40]*  *Finally, we could make, say, a table of the distribution of the
    first letter of the names ([Table 7.1](#tbl-lettersofbooks)).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE40]*  *最后，我们可以制作一个名字首字母分布表（[表 7.1](#tbl-lettersofbooks)）。'
- en: '[PRE41]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '*Table 7.1: Distribution of first letter of author names in a collection of
    books'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 7.1：书籍收藏中作者姓氏首字母的分布'
- en: '| First letter | Number of times |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 首字母 | 出现次数 |'
- en: '| --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| A | 1 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| A | 1 |'
- en: '| C | 1 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| C | 1 |'
- en: '| D | 1 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| D | 1 |'
- en: '| G | 1 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| G | 1 |'
- en: '| H | 1 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| H | 1 |'
- en: '| I | 1 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| I | 1 |'
- en: '| L | 1 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| L | 1 |'
- en: '| M | 1 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| M | 1 |'
- en: '| P | 3 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| P | 3 |'
- en: '| R | 1 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| R | 1 |'
- en: '| V | 2 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| V | 2 |'
- en: '| W | 4 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| W | 4 |'
- en: '| Y | 1 |******  ***### 7.3.4 Prime Ministers of the United Kingdom'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '| Y | 1 |******  ***### 7.3.4 英国首相'
- en: In this case study we are interested in how long prime ministers of the United
    Kingdom lived, based on the year they were born. We will scrape data from Wikipedia
    using `rvest`, clean it, and then make a graph. From time to time a website will
    change. This makes many scrapes largely bespoke, even if we can borrow some code
    from earlier projects. It is normal to feel frustrated at times. It helps to begin
    with an end in mind.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们关注的是英国首相根据其出生年份计算的寿命。我们将使用 `rvest` 从维基百科抓取数据，进行清理，然后制作图表。网站时常会发生变化。这使得许多抓取工作在很大程度上是定制化的，即使我们可以借鉴早期项目的一些代码。有时感到沮丧是正常的。在开始时心中有一个最终目标会有所帮助。
- en: 'To that end, we can start by generating some simulated data. Ideally, we want
    a table that has a row for each prime minister, a column for their name, and a
    column each for the birth and death years. If they are still alive, then that
    death year can be empty. We know that birth and death years should be somewhere
    between 1700 and 1990, and that death year should be larger than birth year. Finally,
    we also know that the years should be integers, and the names should be characters.
    We want something that looks roughly like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以从生成一些模拟数据开始。理想情况下，我们希望得到一个表格，其中每一行代表一位首相，一列是他们的姓名，另外两列分别是出生年份和死亡年份。如果他们仍然在世，那么死亡年份可以为空。我们知道出生和死亡年份应该在
    1700 到 1990 年之间，并且死亡年份应大于出生年份。最后，我们还知道年份应该是整数，姓名应该是字符类型。我们希望得到大致如下所示的内容：
- en: '[PRE42]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '*[PRE43]*  *One of the advantages of generating a simulated dataset is that
    if we are working in groups then one person can start making the graph, using
    the simulated dataset, while the other person gathers the data. In terms of a
    graph, we are aiming for something like [Figure 7.8](#fig-pmsgraphexample).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE43]*  *生成模拟数据集的一个优点是，如果我们分组工作，那么一个人可以使用模拟数据集开始制作图表，而另一个人则收集数据。就图表而言，我们的目标是类似于[图 7.8](#fig-pmsgraphexample)
    的内容。'
- en: '![](../Images/0af0cc21150e3b575aa9173b733dd030.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0af0cc21150e3b575aa9173b733dd030.png)'
- en: 'Figure 7.8: Sketch of planned graph showing how long United Kingdom prime ministers
    lived'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：展示英国首相寿命的计划图表草图
- en: We are starting with a question that is of interest, which is how long each
    prime minister of the United Kingdom lived. As such, we need to identify a source
    of data. While there are plenty of data sources that have the births and deaths
    of each prime minister, we want one that we can trust, and as we are going to
    be scraping, we want one that has some structure to it. The [Wikipedia page about
    prime ministers of the United Kingdom](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom)
    fits both these criteria. As it is a popular page the information is likely to
    be correct, and the data are available in a table.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个感兴趣的问题开始，即每位英国首相的寿命。因此，我们需要确定一个数据来源。虽然有很多数据源提供了每位首相的出生和死亡信息，但我们想要一个可信的来源，并且由于我们要进行抓取，我们希望它具有一定的结构。[关于英国首相的维基百科页面](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom)符合这两个标准。作为一个热门页面，其信息很可能正确，并且数据以表格形式提供。
- en: We load `rvest` and then download the page using `read_html()`. Saving it locally
    provides us with a copy that we need for reproducibility in case the website changes,
    and means that we do not have to keep visiting the website. But it is not ours,
    and so this is typically not something that should be publicly redistributed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载 `rvest`，然后使用 `read_html()` 下载页面。将其本地保存为我们提供了一份副本，以便在网站发生变化时保证可复现性，并且意味着我们不必持续访问该网站。但这并非我们的数据，因此通常不应公开重新分发。
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '*As with the earlier case study, we are looking for patterns in the HTML that
    we can use to help us get closer to the data that we want. This is an iterative
    process and involves trial and error. Even simple examples will take time.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*与之前的案例研究一样，我们正在寻找 HTML 中的模式，以帮助我们更接近所需的数据。这是一个迭代过程，涉及反复试验。即使是简单的例子也需要时间。'
- en: One tool that may help is the [SelectorGadget](https://rvest.tidyverse.org/articles/articles/selectorgadget.html).
    This allows us to pick and choose the elements that we want, and then gives us
    the input for `html_element()` ([Figure 7.9](#fig-selectorgadget)). By default,
    SelectorGadget uses CSS selectors. These are not the only way to specify the location
    of the information you want, and using an alternative, such as XPath, can be a
    useful option to consider.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能有帮助的工具是[选择器小工具](https://rvest.tidyverse.org/articles/articles/selectorgadget.html)。它允许我们挑选所需的元素，然后为我们提供
    `html_element()` 的输入（[图 7.9](#fig-selectorgadget)）。默认情况下，选择器小工具使用CSS选择器。这不是指定所需信息位置的唯一方法，考虑使用替代方案（如XPath）可能是一个有用的选择。
- en: '![](../Images/b96206416f85fa0a02df6208f651ba1b.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b96206416f85fa0a02df6208f651ba1b.png)'
- en: 'Figure 7.9: Using the Selector Gadget to identify the tag, as at 12 February
    2023'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：使用选择器小工具识别标签，截至2023年2月12日
- en: '[PRE45]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '*[PRE46]'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE46]'
- en: '*[PRE47]*  *In this case there are many columns that we do not need, and some
    duplicated rows.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE47]*  *在这种情况下，有许多我们不需要的列，以及一些重复的行。'
- en: '[PRE48]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '*[PRE49]*  *Now that we have the parsed data, we need to clean it to match
    what we wanted. We want a names column, as well as columns for birth year and
    death year. We use `separate()` to take advantage of the fact that it looks like
    the names and dates are distinguished by brackets. The argument in `str_extract()`
    is a regular expression. It looks for four digits in a row, followed by a dash,
    followed by four more digits in a row. We use a slightly different regular expression
    for those prime ministers who are still alive.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE49]*  *现在我们有了解析后的数据，需要对其进行清理以匹配我们的需求。我们需要一个姓名列，以及出生年份和去世年份列。我们利用姓名和日期似乎由括号区分的这一事实，使用
    `separate()` 函数。`str_extract()` 中的参数是一个正则表达式。它查找连续四个数字，后跟一个破折号，再后跟另外四个连续数字。对于那些仍在世的部长，我们使用一个略有不同的正则表达式。'
- en: '[PRE50]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '*[PRE51]*  *Finally, we need to clean up the columns.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE51]*  *最后，我们需要清理这些列。'
- en: '[PRE52]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '*[PRE53]*  *Our dataset looks similar to the one that we said we wanted at
    the start ([Table 7.2](#tbl-canadianpmscleanddata)).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE53]*  *我们的数据集看起来与开始时我们想要的那个相似（[表 7.2](#tbl-canadianpmscleanddata)）。'
- en: '[PRE54]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '*Table 7.2: UK Prime Ministers, by how old they were when they died'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 7.2：按去世年龄排列的英国首相'
- en: '| Prime Minister | Birth year | Death year | Age at death |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 首相 | 出生年份 | 去世年份 | 去世年龄 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Robert Walpole | 1676 | 1745 | 69 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 罗伯特·沃波尔 | 1676 | 1745 | 69 |'
- en: '| Spencer Compton | 1673 | 1743 | 70 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 斯宾塞·康普顿 | 1673 | 1743 | 70 |'
- en: '| Henry Pelham | 1694 | 1754 | 60 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 亨利·佩勒姆 | 1694 | 1754 | 60 |'
- en: '| Thomas Pelham-Holles | 1693 | 1768 | 75 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 托马斯·佩勒姆-霍利斯 | 1693 | 1768 | 75 |'
- en: '| William Cavendish | 1720 | 1764 | 44 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 威廉·卡文迪什 | 1720 | 1764 | 44 |'
- en: '| John Stuart | 1713 | 1792 | 79 |*  *At this point we would like to make a
    graph that illustrates how long each prime minister lived ([Figure 7.10](#fig-pmslives)).
    If they are still alive then we would like them to run to the end, but we would
    like to color them differently.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '| 约翰·斯图亚特 | 1713 | 1792 | 79 |*  *此时，我们希望制作一个图表来说明每位首相的寿命（[图 7.10](#fig-pmslives)）。如果他们仍然在世，我们希望他们的数据线延伸到图表末尾，但希望用不同的颜色来区分。'
- en: '[PRE55]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '*![](../Images/1e641b6b14a8ccac6488fe42707a87b3.png)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](../Images/1e641b6b14a8ccac6488fe42707a87b3.png)'
- en: 'Figure 7.10: How long each prime minister of the United Kingdom lived*********  ***###
    7.3.5 Iteration'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：每位英国首相的寿命*********  ***### 7.3.5 迭代
- en: Considering text as data is exciting and allows us to explore many different
    research questions. We will draw on it in [Chapter 17](16-text.html). Many guides
    assume that we already have a nicely formatted text dataset, but that is rarely
    actually the case. In this case study we will download files from a few different
    pages. While we have already seen two examples of web scraping, those were focused
    on just one page, whereas we often need many. Here we will focus on this iteration.
    We will use `download.file()` to do the download, and use `purrr` to apply this
    function across multiple sites. You do not need to install or load that package
    because it is part of the core `tidyverse` so it is loaded when you load the `tidyverse`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本视为数据令人兴奋，并允许我们探索许多不同的研究问题。我们将在[第17章](16-text.html)中借鉴这一点。许多指南假设我们已经拥有一个格式良好的文本数据集，但这在实际中很少见。在本案例研究中，我们将从几个不同的页面下载文件。虽然我们已经看到了两个网络抓取的例子，但那些都只专注于一个页面，而我们通常需要多个页面。这里我们将专注于这种迭代。我们将使用
    `download.file()` 进行下载，并使用 `purrr` 将此函数应用于多个站点。您不需要安装或加载该包，因为它是核心 `tidyverse`
    的一部分，所以在加载 `tidyverse` 时会自动加载它。
- en: The Reserve Bank of Australia (RBA) is Australia’s central bank. It has responsibility
    for setting the cash rate, which is the interest rate used for loans between banks.
    This interest rate is an especially important one and has a large impact on the
    other interest rates in the economy. Four times a year—February, May, August,
    and November—the RBA publishes a statement on monetary policy, and these are available
    as PDFs. In this example we will download two statements published in 2023.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚储备银行（RBA）是澳大利亚的中央银行。它负责设定现金利率，即银行间贷款所使用的利率。这一利率尤为重要，对经济中的其他利率有重大影响。每年二月、五月、八月和十一月，RBA会发布四次货币政策声明，这些声明以PDF格式提供。在本例中，我们将下载2023年发布的两份声明。
- en: First we set up a tibble that has the information that we need. We will take
    advantage of commonalities in the structure of the URLs. We need to specify both
    a URL and a local file name for each state.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们建立一个包含所需信息的tibble。我们将利用URL结构中的共性。我们需要为每个状态指定URL和本地文件名。
- en: '[PRE56]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '*[PRE57]'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE57]'
- en: '*[PRE58]*  *We want to apply the function `download.files()` to these two statements.
    To do this we write a function that will download the file, let us know that it
    was downloaded, wait a polite amount of time, and then go get the next file.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE58]*  *我们希望将函数 `download.files()` 应用于这两个语句。为此，我们编写一个函数来下载文件，通知我们文件已下载，等待一段适当的时间，然后继续获取下一个文件。'
- en: '[PRE59]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '*We now apply that function to our tibble of URLs and save names using the
    function `walk2()`.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在，我们使用函数 `walk2()` 将该函数应用于我们的URL tibble并保存名称。'
- en: '[PRE60]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '*The result is that we have downloaded these PDFs and saved them to our computer.
    An alternative to writing these functions ourselves would be to use `heapsofpapers`
    ([Alexander and Mahfouz 2021](99-references.html#ref-heapsofpapers)). This includes
    various helpful options for downloading lists of files, especially PDF, CSV, and
    txt files. For instance, Collins and Alexander ([2022](99-references.html#ref-collinsalexander))
    use this to obtain thousands of PDFs and estimate the extent to which COVID-19
    research was reproducible. In the next section we will build on this to discuss
    getting information from PDFs.**************  ****## 7.4 PDFs'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*结果是我们已经下载了这些PDF文件并将其保存到计算机上。除了自己编写这些函数外，另一种选择是使用 `heapsofpapers` ([Alexander
    and Mahfouz 2021](99-references.html#ref-heapsofpapers))。该工具包含多种有用的选项，用于下载文件列表，特别是PDF、CSV和txt文件。例如，Collins和Alexander
    ([2022](99-references.html#ref-collinsalexander)) 使用该工具获取了数千份PDF文件，并评估了COVID-19研究的可重复性程度。在下一节中，我们将在此基础上讨论如何从PDF中获取信息。**************  ****##
    7.4 PDF文件'
- en: PDF files were developed in the 1990s by the technology company Adobe. They
    are useful for documents because they are meant to display in a consistent way
    independent of the environment that created them or the environment in which they
    are being viewed. A PDF viewed on an iPhone should look the same as on an Android
    phone, as on a Linux desktop. One feature of PDFs is that they can include a variety
    of objects, for instance, text, photos, figures, etc. However, this variety can
    limit the capacity of PDFs to be used directly as data. The data first needs to
    be extracted from the PDF.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: PDF文件由科技公司Adobe在20世纪90年代开发。它们对于文档非常有用，因为其设计目的是在不同的创建环境或查看环境中都能以一致的方式显示。在iPhone上查看的PDF应该与在Android手机或Linux桌面上查看的看起来相同。PDF的一个特点是它可以包含多种对象，例如文本、照片、图表等。然而，这种多样性可能会限制PDF直接用作数据的能力。数据首先需要从PDF中提取出来。
- en: It is often possible to copy and paste the data from the PDF. This is more likely
    when the PDF only contains text or regular tables. In particular, if the PDF has
    been created by an application such as Microsoft Word, or another document- or
    form-creation system, then often the text data can be extracted in this way because
    they are actually stored as text within the PDF. We begin with that case. But
    it is not as easy if the text has been stored as an image which is then part of
    the PDF. This may be the case for PDFs produced through scans or photos of physical
    documents, and some older document preparation software. We go through that case
    later.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通常可以从PDF中复制和粘贴数据。当PDF仅包含文本或常规表格时，这种情况更常见。特别是，如果PDF是由Microsoft Word或其他文档或表单创建系统等应用程序创建的，那么通常可以以这种方式提取文本数据，因为它们实际上是以文本形式存储在PDF中的。我们从这种情况开始。但如果文本以图像形式存储，然后作为PDF的一部分，那么提取就不那么容易了。这可能是通过扫描或拍摄实体文档产生的PDF，以及一些较旧的文档准备软件的情况。我们稍后会讨论这种情况。
- en: 'In contrast to an API, a PDF is usually produced for human rather than computer
    consumption. The nice thing about PDFs is that they are static and constant. And
    it is great that data are available. But the trade-off is that:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与 API 不同，PDF 通常是为人类而非计算机消费而制作的。PDF 的好处在于它们是静态且恒定的。数据可用性高是件好事。但代价是：
- en: It is not overly useful to do larger-scale data.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行大规模数据处理并不十分有用。
- en: We do not know how the PDF was put together so we do not know whether we can
    trust it.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不知道 PDF 是如何组合的，因此无法确定是否可以信任它。
- en: We cannot manipulate the data to get results that we are interested in.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们无法通过操纵数据来获得我们感兴趣的结果。
- en: 'There are two important aspects to keep in mind when extracting data from a
    PDF:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PDF 中提取数据时，有两个重要方面需要牢记：
- en: Begin with an end in mind. Plan and sketch what we want from a final dataset/graph/paper
    to limit time wastage.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以终为始。规划和勾勒我们最终想要的数据集/图表/论文，以减少时间浪费。
- en: Start simple, then iterate. The quickest way to make something that needs to
    be complicated is often to first build a simple version and then add to it. Start
    with just trying to get one page of the PDF working or even just one line. Then
    iterate from there.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从简单开始，然后迭代。构建复杂事物的最快方法通常是先构建一个简单版本，然后逐步添加。开始时，只需尝试让 PDF 的一页甚至一行正常工作。然后在此基础上进行迭代。
- en: We will go through several examples and then go through a case study where we
    will gather data on United States Total Fertility Rate, by state.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过几个示例，然后进行一个案例研究，收集美国各州的总生育率数据。
- en: 7.4.1 *Jane Eyre*
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 *《简·爱》*
- en: '[Figure 7.11](#fig-firstpdfexample) is a PDF that consists of just the first
    sentence from Charlotte Brontë’s novel *Jane Eyre* taken from Project Gutenberg
    ([Brontë 1847](99-references.html#ref-janeeyre)). You can get it [here](https://github.com/RohanAlexander/telling_stories/blob/aa6e2d76c80eba7bd31ca68161f0065344449ed8/inputs/pdfs/first_example.pdf).
    If we assume that it was saved as “first_example.pdf”, then after installing and
    loading `pdftools` to get the text from this one-page PDF into R.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.11](#fig-firstpdfexample) 是一个 PDF 文件，仅包含来自古登堡计划的夏洛蒂·勃朗特小说*《简·爱》*的第一句话（[勃朗特
    1847](99-references.html#ref-janeeyre)）。你可以在此处获取它。如果我们假设它被保存为“first_example.pdf”，那么在安装并加载
    `pdftools` 后，就可以将这个单页 PDF 的文本导入 R。'
- en: '![](../Images/9d5cd624bcb2fd578afc10be9828f45e.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d5cd624bcb2fd578afc10be9828f45e.png)'
- en: 'Figure 7.11: First sentence of Jane Eyre'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：《简·爱》的第一句话
- en: '[PRE61]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '*[PRE62]'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE62]'
- en: '[PRE63]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We can see that the PDF has been correctly read in, as a character vector.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 PDF 已正确读入，作为一个字符向量。
- en: We will now try a slightly more complicated example that consists of the first
    few paragraphs of *Jane Eyre* ([Figure 7.12](#fig-secondpdfexample)). Now we have
    the chapter heading as well.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将尝试一个稍微复杂一些的例子，包含*《简·爱》*的前几段（[图 7.12](#fig-secondpdfexample)）。现在我们还包含了章节标题。
- en: '![](../Images/15421ea8d7a8b7df160791a2f901ad72.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15421ea8d7a8b7df160791a2f901ad72.png)'
- en: 'Figure 7.12: First few paragraphs of Jane Eyre'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：《简·爱》的前几段
- en: We use the same function as before.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与之前相同的函数。
- en: '[PRE64]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '*[PRE65]'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE65]'
- en: '[PRE66]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Again, we have a character vector. The end of each line is signaled by “\n”,
    but other than that it looks pretty good. Finally, we consider the first two pages.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们有一个字符向量。每行的结尾由“\n”表示，除此之外看起来相当不错。最后，我们考虑前两页。
- en: '[PRE67]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '*[PRE68]'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE68]'
- en: '[PRE69]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Notice that the first page is the first element of the character vector, and
    the second page is the second element. As we are most familiar with rectangular
    data, we will try to get it into that format as quickly as possible. And then
    we can use functions from the `tidyverse` to deal with it.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一页是字符向量的第一个元素，第二页是第二个元素。由于我们对矩形数据最为熟悉，我们将尽快将其转换为该格式。然后，我们就可以使用 `tidyverse`
    中的函数来处理它。
- en: First we want to convert the character vector into a tibble. At this point we
    may like to add page numbers as well.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们希望将字符向量转换为 tibble。此时，我们可能还想添加页码。
- en: '[PRE70]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '*We then want to separate the lines so that each line is an observation. We
    can do that by looking for “\n” remembering that we need to escape the backslash
    as it is a special character.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*然后，我们希望将行分开，使每一行成为一个观测值。我们可以通过查找“\n”来实现，记住我们需要转义反斜杠，因为它是一个特殊字符。'
- en: '[PRE71]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '*[PRE72]*****  ***### 7.4.2 Total Fertility Rate in the United States'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE72]*****  ***### 7.4.2 美国的总生育率'
- en: The United States Department of Health and Human Services Vital Statistics Report
    provides information about the Total Fertility Rate (TFR) for each state. The
    average number of births per woman if women experience the current age-specific
    fertility rates throughout their reproductive years. The data are available in
    PDFs. We can use the approaches above to get the data into a dataset.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 美国卫生与公众服务部生命统计报告提供了各州的总和生育率信息。总和生育率是指如果女性在整个育龄期内都经历当前特定年龄的生育率，平均每位女性生育的子女数。数据以
    PDF 格式提供。我们可以使用上述方法将数据提取到数据集中。
- en: 'The table that we are interested in is on page 40 of a PDF that is available
    [here](https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf) or [here](https://github.com/RohanAlexander/telling_stories/blob/main/inputs/pdfs/dhs/year_2000.pdf).
    The column of interest is labelled: “Total fertility rate” ([Figure 7.13](#fig-dhsexample)).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的表格位于一个 PDF 文件的第 40 页，该文件可在此处[此处](https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf)或[此处](https://github.com/RohanAlexander/telling_stories/blob/main/inputs/pdfs/dhs/year_2000.pdf)获取。感兴趣的列标记为："总和生育率"（[图
    7.13](#fig-dhsexample)）。
- en: '![](../Images/d99b741f301988e450ed5f485e2f10ff.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d99b741f301988e450ed5f485e2f10ff.png)'
- en: 'Figure 7.13: Example Vital Statistics Report, from 2000'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：2000 年生命统计报告示例
- en: The first step when getting data out of a PDF is to sketch out what we eventually
    want. A PDF typically contains a considerable amount of information, and so we
    should be clear about what is needed. This helps keep you focused, and prevents
    scope creep, but it is also helpful when thinking about data checks. We literally
    write down on paper what we have in mind. In this case, what is needed is a table
    with a column for state, year, and total fertility rate (TFR) ([Figure 7.14](#fig-tfrdesired)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PDF 中提取数据的第一步是勾勒出我们最终想要的结果。PDF 通常包含大量信息，因此我们应该明确需要什么。这有助于保持专注，防止范围蔓延，并且在考虑数据检查时也很有帮助。我们实际上是在纸上写下我们的设想。在本例中，我们需要一个包含州、年份和总和生育率列的表格（[图
    7.14](#fig-tfrdesired)）。
- en: '![](../Images/800a04066c07d5149af902f221770cfb.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/800a04066c07d5149af902f221770cfb.png)'
- en: 'Figure 7.14: Planned dataset of TFR for each US state'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14：计划的美国各州总和生育率数据集
- en: We are interested in a particular column in a particular table for this PDF.
    Unfortunately, there is nothing magical about what is coming. This first step
    requires finding the PDF online, working out the link for each, and searching
    for the page and column name that is of interest. We have built a CSV with the
    details that we need and can read that in.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对此 PDF 文件中特定表格的特定列感兴趣。遗憾的是，接下来的步骤并无神奇之处。第一步需要在线找到 PDF 文件，确定每个文件的链接，并搜索感兴趣的页面和列名。我们已经构建了一个包含所需详细信息的
    CSV 文件，可以将其读入。
- en: '[PRE73]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '*Table 7.3: Year and associated data for TFR tables'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 7.3：总和生育率表的年份及相关数据*'
- en: '| Year | Page | Table | Column | URL |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 页码 | 表格 | 列名 | URL |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2000 | 40 | 10 | Total fertility rate | https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 40 | 10 | 总和生育率 | https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf
    |'
- en: We first download and save the PDF using `download.file()`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 `download.file()` 下载并保存 PDF 文件。
- en: '[PRE74]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '*We then read the PDF in as a character vector using `pdf_text()` from `pdftools`.
    And then convert it to a tibble, so that we can use familiar verbs on it.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*然后，我们使用 `pdftools` 包中的 `pdf_text()` 函数将 PDF 作为字符向量读入。接着将其转换为 tibble，以便我们可以使用熟悉的动词对其进行操作。'
- en: '[PRE75]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '*[PRE76]'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE76]*'
- en: '*[PRE77]*  *Grab the page that is of interest (remembering that each page is
    an element of the character vector, hence a row in the tibble).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE77]*  *提取感兴趣的页面（请记住，每个页面都是字符向量的一个元素，因此也是 tibble 中的一行）。*'
- en: '[PRE78]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '*[PRE79]*  *We want to separate the rows and use `separate_rows()` from `tidyr`,
    which is part of the core tidyverse.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE79]*  *我们希望拆分这些行，并使用 `tidyr` 包（核心 tidyverse 的一部分）中的 `separate_rows()`
    函数。*'
- en: '[PRE80]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '*[PRE81]*  *We are searching for patterns that we can use. Let us look at the
    first ten lines of content (ignoring aspects such as headings and page numbers
    at the top of the page).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE81]*  *我们正在寻找可以使用的模式。让我们查看内容的前十行（忽略页面顶部的标题和页码等元素）。*'
- en: '[PRE82]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '*[PRE83]*  *And now at just one line.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE83]*  *现在只需一行代码。*'
- en: '[PRE84]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '*[PRE85]*  *It does not get much better than this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE85]*  *这已经非常理想了：*'
- en: We have dots separating the states from the data.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用点号将州名与数据分隔开。
- en: We have a space between each of the columns.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 各列之间有一个空格。
- en: We can now separate this into columns. First, we want to match on when there
    are at least two dots (remembering that the dot is a special character and so
    needs to be escaped).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将其分成列。首先，我们希望匹配至少有两个点的情况（记住点是一个特殊字符，因此需要转义）。
- en: '[PRE86]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '*[PRE87]*  *We then separate the data based on spaces. There is an inconsistent
    number of spaces, so we first squish any example of more than one space into just
    one with `str_squish()` from `stringr`.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE87]*  *然后我们基于空格分隔数据。空格数量不一致，因此我们首先使用 `stringr` 包中的 `str_squish()` 函数将多个连续空格压缩为一个。'
- en: '[PRE88]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '*[PRE89]*  *This is all looking fairly great. The only thing left is to clean
    up.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE89]*  *这一切看起来相当不错。剩下的唯一事情就是清理一下。'
- en: '[PRE90]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '*And run some checks, for instance that we have all the states.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*并运行一些检查，例如确保我们包含了所有州。'
- en: '[PRE91]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '*[PRE92]*  *And we are done ([Table 7.4](#tbl-tfrforthewin)). We can see that
    there is quite a wide distribution of TFR by US state ([Figure 7.15](#fig-smalldhsexample)).
    Utah has the highest and Vermont the lowest.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE92]*  *这样我们就完成了（[表 7.4](#tbl-tfrforthewin)）。我们可以看到美国各州的总和生育率分布相当广泛（[图
    7.15](#fig-smalldhsexample)）。犹他州最高，佛蒙特州最低。'
- en: '[PRE93]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '*Table 7.4: First ten rows of a dataset of TFR by United States state, 2000-2019'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 7.4：2000-2019年美国各州总和生育率数据集的前十行'
- en: '| State | TFR |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 州 | 总和生育率 |'
- en: '| --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Total | 2,130 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 2,130 |'
- en: '| Alabama | 2,021 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 阿拉巴马州 | 2,021 |'
- en: '| Alaska | 2,437 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 阿拉斯加州 | 2,437 |'
- en: '| Arizona | 2,652 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 亚利桑那州 | 2,652 |'
- en: '| Arkansas | 2,140 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 阿肯色州 | 2,140 |'
- en: '| California | 2,186 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 加利福尼亚州 | 2,186 |'
- en: '| Colorado | 2,356 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 科罗拉多州 | 2,356 |'
- en: '| Connecticut | 1,932 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 康涅狄格州 | 1,932 |'
- en: '| Delaware | 2,014 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 特拉华州 | 2,014 |'
- en: '| District of Columbia | 1,976 |*  *[PRE94]'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '| 哥伦比亚特区 | 1,976 |*  *[PRE94]'
- en: '*![](../Images/e43a07d60868afd17f856a21c14ea53c.png)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](../Images/e43a07d60868afd17f856a21c14ea53c.png)'
- en: 'Figure 7.15: Distribution of TFR by US state in 2000*  *Healy ([2022](99-references.html#ref-kieransparsing))
    provides another example of using this approach in a different context.**************  ***###
    7.4.3 Optical Character Recognition'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15：2000年美国各州总和生育率分布*  *Healy ([2022](99-references.html#ref-kieransparsing))
    在另一个上下文中提供了使用此方法的另一个例子。**************  ***### 7.4.3 光学字符识别
- en: All of the above is predicated on having a PDF that is already “digitized”.
    But what if it is made of images, such as the result of a scan. Such PDFs often
    contain unstructured data, meaning that the data are not tagged nor organized
    in a regular way. Optical Character Recognition (OCR) is a process that transforms
    an image of text into actual text. Although there may not be much difference to
    a human reading a PDF before and after OCR, the PDF becomes machine-readable which
    allows us to use scripts ([Cheriet et al. 2007](99-references.html#ref-Cheriet2007)).
    OCR has been used to parse images of characters since the 1950s, initially using
    manual approaches. While manual approaches remain the gold standard, for reasons
    of cost effectiveness, this has been largely replaced with statistical models.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 以上所有操作都基于一个前提，即我们拥有一个已经“数字化”的PDF文件。但如果它是由图像组成的呢，比如扫描的结果。这类PDF通常包含非结构化数据，意味着数据既没有标签，也没有以规则的方式组织。光学字符识别（OCR）是一个将文本图像转换为实际文本的过程。尽管对人类读者来说，OCR前后的PDF可能没有太大区别，但PDF会因此变得机器可读，从而允许我们使用脚本（[Cheriet
    et al. 2007](99-references.html#ref-Cheriet2007)）。自20世纪50年代以来，OCR就被用于解析字符图像，最初采用手动方法。虽然手动方法仍然是黄金标准，但由于成本效益的原因，它已基本被统计模型所取代。
- en: In this example we use `tesseract` to OCR a document. This is a `R` wrapper
    around the Tesseract open-source OCR engine. Tesseract was initially developed
    at HP in the 1980s, and is now mostly developed by Google. After we install and
    load `tesseract` we can use `ocr()`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用 `tesseract` 对文档进行OCR。这是对Tesseract开源OCR引擎的一个 `R` 语言封装。Tesseract最初由惠普公司在20世纪80年代开发，现在主要由谷歌开发。安装并加载
    `tesseract` 后，我们可以使用 `ocr()` 函数。
- en: Let us see an example with a scan from the first page of *Jane Eyre* ([Figure 7.16](#fig-janescan)).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个使用《*简·爱*》第一页扫描件的例子（[图 7.16](#fig-janescan)）。
- en: '![](../Images/df597ee5e0c63d66038720a9fc9c9c9d.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df597ee5e0c63d66038720a9fc9c9c9d.png)'
- en: 'Figure 7.16: Scan of first page of Jane Eyre'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16：《简·爱》第一页扫描件
- en: '[PRE95]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '*[PRE96]'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE96]'
- en: In general the result is not too bad. OCR is a useful tool but is not perfect
    and the resulting data may require extra attention in terms of cleaning. For instance,
    in the OCR results of [Figure 7.16](#fig-janescan) we see irregularities that
    would need to be fixed. Various options, such as focusing on the particular data
    of interest and increasing the contrast can help. Other popular OCR engines include
    Amazon Textract, Google Vision API, and ABBYY.*******  ***## 7.5 Exercises
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果不算太差。OCR 是一个有用的工具，但并不完美，生成的数据在清洗方面可能需要额外关注。例如，在[图 7.16](#fig-janescan)
    的 OCR 结果中，我们看到了一些需要修正的不规则之处。各种选项，例如专注于特定的目标数据和增加对比度，可能会有所帮助。其他流行的 OCR 引擎包括 Amazon
    Textract、Google Vision API 和 ABBYY。*******  ***## 7.5 练习
- en: Practice
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: '*(Plan)* Consider the following scenario: *A group of five undergraduates—Matt,
    Ash, Jacki, Rol, and Mike—each read some number of pages from a book each day
    for 100 days. Two of the undergraduates are a couple and so their number of pages
    is positively correlated, however all the others are independent.* Please sketch
    what a dataset could look like, and then sketch a graph that you could build to
    show all observations.'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*（规划）* 考虑以下场景：*五名本科生——Matt、Ash、Jacki、Rol 和 Mike——在 100 天里，每人每天阅读一定页数的书籍。其中两名本科生是情侣，因此他们的阅读页数呈正相关，但其他所有人的阅读量都是独立的。*
    请草拟数据集可能的样子，然后草拟一张可以展示所有观测值的图表。'
- en: '*(Simulate)* Please further consider the scenario described and simulate the
    situation (note the relationship between some variables). Then write five tests
    based on the simulated data.'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*（模拟）* 请进一步考虑所描述的场景并模拟该情况（注意某些变量之间的关系）。然后基于模拟数据编写五个测试。'
- en: '*(Acquire)* Please obtain some actual data, similar to the scenario, and add
    a script updating the simulated tests to these actual data.'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*（获取）* 请获取一些与实际场景类似的真实数据，并添加一个脚本，将模拟测试更新为这些真实数据。'
- en: '*(Explore)* Build a graph and table using the real data.'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*（探索）* 使用真实数据构建图表和表格。'
- en: '*(Communicate)* Please write some text to accompany the graph and table. Separate
    the code appropriately into `R` files and a Quarto doc. Submit a link to a high-quality
    GitHub repo.'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*（沟通）* 请撰写一些文字来配合图表和表格。将代码适当地分离到 `R` 文件和 Quarto 文档中。提交一个高质量 GitHub 仓库的链接。'
- en: Quiz
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测验
- en: What is an API, in the context of data gathering (pick one)?
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据收集的语境中，什么是 API（单选）？
- en: A standardized set of functions to process data locally.
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组用于本地处理数据的标准化函数。
- en: A markup language for structuring data.
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种用于结构化数据的标记语言。
- en: A protocol for web browsers to render HTML content.
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种供网络浏览器渲染 HTML 内容的协议。
- en: An interface provided by a server that allows someone else to request data using
    code.
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务器提供的一种接口，允许他人使用代码请求数据。
- en: When using APIs for data gathering, which of the following can be used for authentication
    (pick one)?
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 API 收集数据时，以下哪项可用于身份验证（单选）？
- en: Providing an API key or token in the request.
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在请求中提供 API 密钥或令牌。
- en: Using cookies stored in the browser.
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用浏览器中存储的 Cookie。
- en: Disabling SSL verification.
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用 SSL 验证。
- en: Modifying the hosts file on the client machine.
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改客户端机器上的 hosts 文件。
- en: Consider the following code, which uses `gh` to access the GitHub API. When
    was the repo for `heapsofpapers` created (pick one)?
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑以下使用 `gh` 访问 GitHub API 的代码。`heapsofpapers` 的代码仓库是何时创建的（单选）？
- en: '2021-02-23'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2021-02-23'
- en: '2021-03-06'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2021-03-06'
- en: '2021-05-25'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2021-05-25'
- en: '2021-04-27'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2021-04-27'
- en: '[PRE97]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '*4.  Please consider the UN’s [Data API](https://population.un.org/dataportal/about/dataapi)
    and the introductory note on how to use it by Schmertmann ([2022](99-references.html#ref-schmertmannunapi)).
    Argentina’s location code is 32\. Modify the following code to determine what
    Argentina’s single-year fertility rate was for 20-year-olds in 1995 (pick one)?'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*4.  请参考联合国的[数据 API](https://population.un.org/dataportal/about/dataapi) 以及
    Schmertmann ([2022](99-references.html#ref-schmertmannunapi)) 关于如何使用它的介绍性说明。阿根廷的位置代码是
    32。修改以下代码，以确定阿根廷 1995 年 20 岁人群的单年生育率是多少（单选）？'
- en: '147.679'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '147.679'
- en: '172.988'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '172.988'
- en: '204.124'
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '204.124'
- en: '128.665'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '128.665'
- en: '[PRE98]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '*5.  What is the main argument to `GET()` from `httr` (pick one)?'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '*5.  `httr` 包中 `GET()` 函数的主要参数是什么（单选）？'
- en: “url”
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “网址”
- en: “website”
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “网站”
- en: “domain”
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “域名”
- en: “location”
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “位置”
- en: In web scraping, what is the purpose of respecting `robots.txt` (pick one)?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网络抓取中，尊重 `robots.txt` 的目的是什么（单选）？
- en: To ensure the scraped data are accurate.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保抓取的数据准确无误。
- en: To avoid violating the website’s terms of service by following the site’s crawling
    guidelines.
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过遵循网站的爬取指南，以避免违反网站的服务条款。
- en: To speed up the scraping process.
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了加快抓取过程。
- en: To obtain authentication credentials.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获取身份验证凭据。
- en: What features of a website do we typically take advantage of when we parse the
    code (pick one)?
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在解析代码时，我们通常利用网站的哪些特性（单选）？
- en: HTML/CSS mark-up.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: HTML/CSS 标记。
- en: Cookies.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cookies。
- en: Facebook beacons.
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Facebook 信标。
- en: Code comments.
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码注释。
- en: What are some principles to follow when scraping (select all that apply)?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抓取时应遵循哪些原则（多选）？
- en: Avoid it if possible
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能，避免这样做
- en: Follow the site’s guidance
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遵循网站的指引
- en: Slow down
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 放慢速度
- en: Use a scalpel not an axe.
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用手术刀而非斧头。
- en: Which of the following is NOT a recommended principle when performing web scraping
    (pick one)?
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪项**不是**执行网络抓取时的推荐原则（单选）？
- en: Abide by the website’s terms of service.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遵守网站的服务条款。
- en: Reduce the impact on the website’s server by slowing down requests.
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减慢请求速度来减少对网站服务器的影响。
- en: Scrape all data regardless of necessity.
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抓取所有数据，无论是否必要。
- en: Avoid republishing scraped pages.
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免重新发布抓取的页面。
- en: 'Which of the following, used as part of a regular expression, would match a
    full stop (hint: see the “strings” cheat sheet) (pick one)?'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪项作为正则表达式的一部分，可以匹配句号（提示：参见“字符串”速查表）（单选）？
- en: “.”
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “.”
- en: “\.”
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “\.”
- en: “\\\.”
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “\\\.”
- en: What are three checks that we might like to use for demographic data, such as
    the number of births in a country in a particular year?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于人口统计数据（例如某国特定年份的出生人数），我们可能希望使用哪三项检查？
- en: Which of these are functions from the `purrr` package (select all that apply)?
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些是 `purrr` 包中的函数（多选）？
- en: '`map()`'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`map()`'
- en: '`walk()`'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`walk()`'
- en: '`run()`'
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`run()`'
- en: '`safely()`'
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`safely()`'
- en: What is the HTML tag for an item in a list (pick one)?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列表中项目的 HTML 标签是什么（单选）？
- en: '`li`'
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`li`'
- en: '`body`'
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`body`'
- en: '`b`'
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`b`'
- en: '`em`'
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`em`'
- en: Which function should we use if we have the text “rohan_alexander” in a column
    called “names” and want to split it into first name and surname based on the underscore
    (pick one)?
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们有一个名为“names”的列，其中包含文本“rohan_alexander”，并希望基于下划线将其拆分为名和姓，应该使用哪个函数（单选）？
- en: '`spacing()`'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spacing()`'
- en: '`slice()`'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`slice()`'
- en: '`separate()`'
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`separate()`'
- en: '`text_to_columns()`'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`text_to_columns()`'
- en: What is Optical Character Recognition (OCR) (pick one)?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是光学字符识别（OCR）（单选）？
- en: A process of converting handwritten notes into typed text.
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将手写笔记转换为打字文本的过程。
- en: A method for translating images of text into machine-readable text.
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种将文本图像转换为机器可读文本的方法。
- en: A technique for parsing structured data from APIs.
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种从 API 解析结构化数据的技术。
- en: A way to optimize code for faster execution.
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种优化代码以加快执行速度的方法。
- en: Which function in `R` can be used to pause execution for a specified amount
    of time, useful for respecting rate limits during web scraping (pick one)?
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`R` 中哪个函数可用于暂停执行指定时间，有助于在网络抓取时遵守速率限制（单选）？'
- en: '`sleep()`'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sleep()`'
- en: '`pause()`'
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pause()`'
- en: '`sys.sleep()`'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sys.sleep()`'
- en: '`wait()`'
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`wait()`'
- en: Which of the following is a challenge when extracting data from PDFs (pick one)?
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪项是从 PDF 中提取数据时面临的挑战（单选）？
- en: PDFs cannot be read by any programming language.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: PDF 文件无法被任何编程语言直接读取。
- en: PDFs are designed for consistent human reading, not for data extraction.
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: PDF 是为保持人类阅读一致性而设计的，而非用于数据提取。
- en: PDFs always contain unstructured data that cannot be processed.
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: PDF 总是包含无法处理的无结构数据。
- en: PDFs are encrypted and cannot be accessed without a password.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: PDF 已加密，没有密码无法访问。
- en: When performing OCR on a scanned document, what is a common issue that might
    affect the accuracy of text recognition (pick one)?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对扫描文档执行 OCR 时，可能影响文本识别准确性的常见问题是什么（单选）？
- en: The file size of the image.
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像的文件大小。
- en: The programming language used.
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所使用的编程语言。
- en: The quality and resolution of the scanned image.
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描图像的质量和分辨率。
- en: The number of pages in the document.
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档的页数。
- en: From Cirone and Spirling ([2021](99-references.html#ref-cirone)), which of the
    following is NOT a common threat to inference when working with historical data
    (pick one)?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 Cirone 和 Spirling ([2021](99-references.html#ref-cirone))，在处理历史数据时，以下哪项**不是**常见的推断威胁（单选）？
- en: Selection bias.
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择偏差。
- en: Confirmation bias.
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认偏差。
- en: Time decay.
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间衰减。
- en: Over-representation of marginalized groups.
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边缘化群体的过度代表。
- en: From Cirone and Spirling ([2021](99-references.html#ref-cirone)), what is the
    drunkard’s search problem in historical political economy (and more generally)
    (pick one)?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 Cirone 和 Spirling ([2021](99-references.html#ref-cirone))，历史政治经济学（以及更广泛意义上）中的“醉汉寻物”问题是什么（单选）？
- en: Selecting data that are easiest to access without considering representativeness.
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最容易获取的数据而不考虑代表性。
- en: Searching for data only from elite sources.
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅从精英来源搜索数据。
- en: Over-relying on digital archives for research.
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究中过度依赖数字档案。
- en: Misinterpreting historical texts due to modern biases.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因现代偏见而误解历史文本。
- en: From Cirone and Spirling ([2021](99-references.html#ref-cirone)), what role
    do DAGs have (pick one)?
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Cirone和Spirling ([2021](99-references.html#ref-cirone))，DAGs扮演什么角色（选择一项）？
- en: They improve the accuracy of OCR for historical data.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们提高了历史数据OCR的准确性。
- en: They generate machine-readable text from historical sources.
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们从历史资料中生成机器可读文本。
- en: They help researchers visualize and address causal relationships.
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们帮助研究者可视化并处理因果关系。
- en: They serve as metadata to organize historical archives.
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们作为元数据来组织历史档案。
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), what was the focus
    of early prison data collection by the U.S. Census Bureau (pick one)?
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Johnson ([2021](99-references.html#ref-Johnson2021Two))，美国人口普查局早期监狱数据收集的重点是什么（选择一项）？
- en: Documenting health conditions.
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录健康状况。
- en: Investigating racial differences in sentencing.
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调查量刑中的种族差异。
- en: Recording socioeconomic background and employment.
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录社会经济背景和就业情况。
- en: Counting the number of incarcerated people and their demographics.
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计被监禁人数及其人口统计数据。
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), how does community-sourced
    prison data differ from state-sourced prison data (pick one)?
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Johnson ([2021](99-references.html#ref-Johnson2021Two))，社区来源的监狱数据与政府来源的监狱数据有何不同（选择一项）？
- en: Community data are collected by government officials.
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社区数据由政府官员收集。
- en: Community data emphasizes lived experiences and prison conditions.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社区数据强调生活经历和监狱条件。
- en: State data are less reliable than community data.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 政府数据不如社区数据可靠。
- en: State data are more reliable than community data.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 政府数据比社区数据更可靠。
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), which of the following
    is a limitation of state-sourced data (pick one)?
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Johnson ([2021](99-references.html#ref-Johnson2021Two))，以下哪项是政府来源数据的局限性（选择一项）？
- en: State-sourced data are less reliable than academic studies.
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 政府来源的数据不如学术研究可靠。
- en: It under-represents the prison population.
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它低估了监狱人口。
- en: It may reproduce the biases and assumptions of earlier data collections.
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可能复制了早期数据收集中的偏见和假设。
- en: It focuses on nonviolent offenders only.
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它只关注非暴力罪犯。
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), what question
    should be asked when looking at prison data collection (pick one)?
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Johnson ([2021](99-references.html#ref-Johnson2021Two))，查看监狱数据收集时应提出什么问题（选择一项）？
- en: “Who established the data infrastructure and why?”.
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “谁建立了数据基础设施以及为什么？”
- en: “How do the economic factors affect prison management?”.
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “经济因素如何影响监狱管理？”
- en: “Is the data being used to create public policy?”.**  **### Class activities
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “这些数据是否被用于制定公共政策？”.**  **### 课堂活动
- en: Use the [starter folder](https://github.com/RohanAlexander/starter_folder) and
    create a new repo. Obtain the NASA APOD for today using the API and then add it
    to the Quarto document in the repo.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[起始文件夹](https://github.com/RohanAlexander/starter_folder)并创建一个新仓库。通过API获取今日的NASA每日天文图，然后将其添加到仓库中的Quarto文档里。
- en: Use the Spotify API to determine which Beyonce album has the highest average
    “danceability”?
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spotify API确定碧昂斯的哪张专辑平均“可舞性”最高？
- en: Please make a graph to answer the question of whether Camila Cabello’s departure
    from Fifth Harmony in December 2016 affected the valence of the songs in their
    studio albums.[¹](#fn1) Some helpful cleaning code is below.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请制作一个图表来回答卡米拉·卡贝洛在2016年12月离开五美组合是否影响了她们录音室专辑中歌曲的效价。[¹](#fn1)下面是一些有用的清理代码。
- en: '[PRE99]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '**   Build an equivalent to [Figure 7.10](#fig-pmslives), but for Canada.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '**   构建一个与[图7.10](#fig-pmslives)等效的图表，但针对加拿大。'
- en: '*Paper review:* Please read Kish ([1959](99-references.html#ref-Kish1959))
    and write a review of at least one page, drawing on an example that you are familiar
    with.*  *### Task'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*论文评述：* 请阅读Kish ([1959](99-references.html#ref-Kish1959))，并撰写至少一页的评述，结合你熟悉的一个例子。*  *###
    任务'
- en: 'Please redo the web scraping example, but for one of: [Australia](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Australia),
    [Canada](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada), [India](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_India),
    or [New Zealand](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_New_Zealand).'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 请重做网页抓取示例，但针对以下国家之一：[澳大利亚](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Australia)、[加拿大](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada)、[印度](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_India)或[新西兰](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_New_Zealand)。
- en: Plan, gather, and clean the data, and then use it to create a similar table
    to the one created above. Write a few paragraphs about your findings. Then write
    a few paragraphs about the data source, what you gathered, and how you went about
    it. What took longer than you expected? When did it become fun? What would you
    do differently next time you do this? Your submission should be at least two pages,
    but likely more.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 规划、收集并清理数据，然后利用这些数据创建一个与上述类似的表格。撰写几段文字阐述你的发现。接着，用几段文字说明数据来源、你收集的内容以及具体操作过程。哪些环节花费的时间超出预期？何时开始感到有趣？下次再做类似工作时，你会采取哪些不同的做法？你的提交内容应至少两页，但很可能更多。
- en: Use Quarto, and include an appropriate title, author, date, link to a GitHub
    repo, and citations. Submit a PDF.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Quarto，并包含适当的标题、作者、日期、GitHub 仓库链接以及引用。提交 PDF 格式的文件。
- en: 'Alexander, Rohan, and A Mahfouz. 2021\. *heapsofpapers: Easily Download Heaps
    of PDF and CSV Files*. [https://CRAN.R-project.org/package=heapsofpapers](https://CRAN.R-project.org/package=heapsofpapers).Arel-Bundock,
    Vincent. 2024\. *tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,”
    “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats*. [https://vincentarelbundock.github.io/tinytable/](https://vincentarelbundock.github.io/tinytable/).Bailey,
    Rosemary. 2008\. *Design of Comparative Experiments*. Cambridge: Cambridge University
    Press. [https://doi.org/10.1017/CBO9780511611483](https://doi.org/10.1017/CBO9780511611483).Bor,
    Jacob, Atheendar Venkataramani, David Williams, and Alexander Tsai. 2018\. “Police
    Killings and Their Spillover Effects on the Mental Health of Black Americans:
    A Population-Based, Quasi-Experimental Study.” *The Lancet* 392 (10144): 302–10\.
    [https://doi.org/10.1016/s0140-6736(18)31130-9](https://doi.org/10.1016/s0140-6736(18)31130-9).Brontë,
    Charlotte. 1847\. *Jane Eyre*. [https://www.gutenberg.org/files/1260/1260-h/1260-h.htm](https://www.gutenberg.org/files/1260/1260-h/1260-h.htm).Bryan,
    Jenny, and Hadley Wickham. 2021\. *gh: GitHub API*. [https://CRAN.R-project.org/package=gh](https://CRAN.R-project.org/package=gh).Cheriet,
    Mohamed, Nawwaf Kharma, Cheng-Lin Liu, and Ching Suen. 2007\. *Character Recognition
    Systems: A Guide for Students and Practitioner*. Wiley.Cirone, Alexandra, and
    Arthur Spirling. 2021\. “Turning History into Data: Data Collection, Measurement,
    and Inference in HPE.” *Journal of Historical Political Economy* 1 (1): 127–54\.
    [https://doi.org/10.1561/115.00000005](https://doi.org/10.1561/115.00000005).Collins,
    Annie, and Rohan Alexander. 2022\. “Reproducibility of COVID-19 Pre-Prints.” *Scientometrics*
    127: 4655–73\. [https://doi.org/10.1007/s11192-022-04418-2](https://doi.org/10.1007/s11192-022-04418-2).Comer,
    Benjamin P., and Jason R. Ingram. 2022\. “Comparing Fatal Encounters, Mapping
    Police Violence, and Washington Post Fatal Police Shooting Data from 2015-2019:
    A Research Note.” *Criminal Justice Review*, January, 073401682110710\. [https://doi.org/10.1177/07340168211071014](https://doi.org/10.1177/07340168211071014).Crawford,
    Kate. 2021\. *Atlas of AI*. 1st ed. New Haven: Yale University Press.Cummins,
    Neil. 2022\. “The Hidden Wealth of English Dynasties, 1892–2016.” *The Economic
    History Review* 75 (3): 667–702\. [https://doi.org/10.1111/ehr.13120](https://doi.org/10.1111/ehr.13120).Eisenstein,
    Michael. 2022\. “Need Web Data? Here’s How to Harvest Them.” *Nature* 607: 200–201\.
    [https://doi.org/10.1038/d41586-022-01830-9](https://doi.org/10.1038/d41586-022-01830-9).Firke,
    Sam. 2023\. *janitor: Simple Tools for Examining and Cleaning Dirty Data*. [https://CRAN.R-project.org/package=janitor](https://CRAN.R-project.org/package=janitor).Grolemund,
    Garrett, and Hadley Wickham. 2011\. “Dates and Times Made Easy with lubridate.”
    *Journal of Statistical Software* 40 (3): 1–25\. [https://doi.org/10.18637/jss.v040.i03](https://doi.org/10.18637/jss.v040.i03).Hackett,
    Robert. 2016\. “Researchers Caused an Uproar By Publishing Data From 70,000 OkCupid
    Users.” *Fortune*, May. [https://fortune.com/2016/05/18/okcupid-data-research/](https://fortune.com/2016/05/18/okcupid-data-research/).Healy,
    Kieran. 2022\. “Unhappy in Its Own Way,” July. [https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/](https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/).Jenkins,
    Jennifer, Steven Rich, Andrew Ba Tran, Paige Moody, Julie Tate, and Ted Mellnik.
    2022\. “How the Washington Post Examines Police Shootings in the United States.”
    [https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/](https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/).Johnson,
    Kaneesha. 2021\. “Two Regimes of Prison Data Collection.” *Harvard Data Science
    Review* 3 (3). [https://doi.org/10.1162/99608f92.72825001](https://doi.org/10.1162/99608f92.72825001).Kirkegaard,
    Emil, and Julius Bjerrekær. 2016\. “The OKCupid Dataset: A Very Large Public Dataset
    of Dating Site Users.” *Open Differential Psychology*, 1–10\. [https://doi.org/10.26775/ODP.2016.11.03](https://doi.org/10.26775/ODP.2016.11.03).Kish,
    Leslie. 1959\. “Some Statistical Problems in Research Design.” *American Sociological
    Review* 24 (3): 328–38\. [https://doi.org/10.2307/2089381](https://doi.org/10.2307/2089381).Luscombe,
    Alex, Kevin Dick, and Kevin Walby. 2021\. “Algorithmic Thinking in the Public
    Interest: Navigating Technical, Legal, and Ethical Hurdles to Web Scraping in
    the Social Sciences.” *Quality & Quantity* 56 (3): 1–22\. [https://doi.org/10.1007/s11135-021-01164-0](https://doi.org/10.1007/s11135-021-01164-0).Luscombe,
    Alex, Jamie Duncan, and Kevin Walby. 2022\. “Jumpstarting the Justice Disciplines:
    A Computational-Qualitative Approach to Collecting and Analyzing Text and Image
    Data in Criminology and Criminal Justice Studies.” *Journal of Criminal Justice
    Education* 33 (2): 151–71\. [https://doi.org/10.1080/10511253.2022.2027477](https://doi.org/10.1080/10511253.2022.2027477).Müller,
    Kirill. 2020\. *here: A Simpler Way to Find Your Files*. [https://CRAN.R-project.org/package=here](https://CRAN.R-project.org/package=here).Nix,
    Justin, and M. James Lozada. 2020\. “Police Killings of Unarmed Black Americans:
    A Reassessment of Community Mental Health Spillover Effects,” January. [https://doi.org/10.31235/osf.io/ajz2q](https://doi.org/10.31235/osf.io/ajz2q).Ooms,
    Jeroen. 2014\. “The jsonlite Package: A Practical and Consistent Mapping Between
    JSON Data and R Objects.” *arXiv:1403.2805 [Stat.CO]*. [https://arxiv.org/abs/1403.2805](https://arxiv.org/abs/1403.2805).———.
    2022a. *pdftools: Text Extraction, Rendering and Converting of PDF Documents*.
    [https://CRAN.R-project.org/package=pdftools](https://CRAN.R-project.org/package=pdftools).———.
    2022b. *tesseract: Open Source OCR Engine*. [https://CRAN.R-project.org/package=tesseract](https://CRAN.R-project.org/package=tesseract).Pavlik,
    Kaylin. 2019\. “Understanding + Classifying Genres Using Spotify Audio Features.”
    [https://www.kaylinpavlik.com/classifying-songs-genres/](https://www.kaylinpavlik.com/classifying-songs-genres/).Perepolkin,
    Dmytro. 2022\. *polite: Be Nice on the Web*. [https://CRAN.R-project.org/package=polite](https://CRAN.R-project.org/package=polite).R
    Core Team. 2024\. *R: A Language and Environment for Statistical Computing*. Vienna,
    Austria: R Foundation for Statistical Computing. [https://www.R-project.org/](https://www.R-project.org/).Salganik,
    Matthew, Peter Sheridan Dodds, and Duncan Watts. 2006\. “Experimental Study of
    Inequality and Unpredictability in an Artificial Cultural Market.” *Science* 311
    (5762): 854–56\. [https://doi.org/10.1126/science.1121066](https://doi.org/10.1126/science.1121066).Saulnier,
    Lucile, Siddharth Karamcheti, Hugo Laurençon, Léo Tronchon, Thomas Wang, Victor
    Sanh, Amanpreet Singh, et al. 2022\. “Putting Ethical Principles at the Core of
    the Research Lifecycle.” [https://huggingface.co/blog/ethical-charter-multimodal](https://huggingface.co/blog/ethical-charter-multimodal).Schmertmann,
    Carl. 2022\. “UN API Test,” July. [https://bonecave.schmert.net/un-api-example.html](https://bonecave.schmert.net/un-api-example.html).Taflaga,
    Marija, and Matthew Kerby. 2019\. “Who Does What Work in a Ministerial Office:
    Politically Appointed Staff and the Descriptive Representation of Women in Australian
    Political Offices, 19792010.” *Political Studies* 68 (2): 463–85\. [https://doi.org/10.1177/0032321719853459](https://doi.org/10.1177/0032321719853459).The
    Economist. 2022\. “What Spotify Data Show about the Decline of English,” January.
    [https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english](https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english).The
    Washington Post. 2023\. “Fatal Force Database.” [https://github.com/washingtonpost/data-police-shootings](https://github.com/washingtonpost/data-police-shootings).Thompson,
    Charlie, Daniel Antal, Josiah Parry, Donal Phipps, and Tom Wolff. 2022\. *spotifyr:
    R Wrapper for the “Spotify” Web API*. [https://CRAN.R-project.org/package=spotifyr](https://CRAN.R-project.org/package=spotifyr).Thomson-DeVeaux,
    Amelia, Laura Bronner, and Damini Sharma. 2021\. “Cities Spend Millions On Police
    Misconduct Every Year. Here’s Why It’s So Difficult to Hold Departments Accountable.”
    *FiveThirtyEight*, February. [https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/](https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/).Wickham,
    Hadley. 2021\. *babynames: US Baby Names 1880-2017*. [https://CRAN.R-project.org/package=babynames](https://CRAN.R-project.org/package=babynames).———.
    2022\. *rvest: Easily Harvest (Scrape) Web Pages*. [https://CRAN.R-project.org/package=rvest](https://CRAN.R-project.org/package=rvest).———.
    2023\. *httr: Tools for Working with URLs and HTTP*. [https://CRAN.R-project.org/package=httr](https://CRAN.R-project.org/package=httr).Wickham,
    Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain
    François, Garrett Grolemund, et al. 2019\. “Welcome to the Tidyverse.” *Journal
    of Open Source Software* 4 (43): 1686\. [https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686).Wickham,
    Hadley, Jennifer Bryan, and Malcolm Barrett. 2022\. *usethis: Automate Package
    and Project Setup*. [https://CRAN.R-project.org/package=usethis](https://CRAN.R-project.org/package=usethis).Wickham,
    Hadley, and Lionel Henry. 2022\. *purrr: Functional Programming Tools*. [https://CRAN.R-project.org/package=purrr](https://CRAN.R-project.org/package=purrr).Wickham,
    Hadley, Jim Hester, and Jeroen Ooms. 2021\. *xml2: Parse XML*. [https://CRAN.R-project.org/package=xml2](https://CRAN.R-project.org/package=xml2).Wong,
    Julia Carrie. 2020\. “One Year Inside Trump’s Monumental Facebook Campaign.” *The
    Guardian*, January. [https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election](https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election).Zimmer,
    Michael. 2018\. “Addressing Conceptual Gaps in Big Data Research Ethics: An Application
    of Contextual Integrity.” *Social Media + Society* 4 (2): 1–11\. [https://doi.org/10.1177/2056305118768300](https://doi.org/10.1177/2056305118768300).***
    **** * *'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 'Alexander, Rohan, 和 A Mahfouz。2021年。*heapsofpapers: 轻松下载大量PDF和CSV文件*。[https://CRAN.R-project.org/package=heapsofpapers](https://CRAN.R-project.org/package=heapsofpapers)。Arel-Bundock,
    Vincent。2024年。*tinytable: 适用于“HTML”、“LaTeX”、“Markdown”、“Word”、“PNG”、“PDF”和“Typst”格式的简单可配置表格*。[https://vincentarelbundock.github.io/tinytable/](https://vincentarelbundock.github.io/tinytable/)。Bailey,
    Rosemary。2008年。*比较实验设计*。剑桥：剑桥大学出版社。[https://doi.org/10.1017/CBO9780511611483](https://doi.org/10.1017/CBO9780511611483)。Bor,
    Jacob, Atheendar Venkataramani, David Williams, 和 Alexander Tsai。2018年。“警察枪击事件及其对非裔美国人心理健康的溢出效应：一项基于人群的准实验研究。”*《柳叶刀》*
    392 (10144): 302–10。[https://doi.org/10.1016/s0140-6736(18)31130-9](https://doi.org/10.1016/s0140-6736(18)31130-9)。Brontë,
    Charlotte。1847年。*《简·爱》*。[https://www.gutenberg.org/files/1260/1260-h/1260-h.htm](https://www.gutenberg.org/files/1260/1260-h/1260-h.htm)。Bryan,
    Jenny, 和 Hadley Wickham。2021年。*gh: GitHub API*。[https://CRAN.R-project.org/package=gh](https://CRAN.R-project.org/package=gh)。Cheriet,
    Mohamed, Nawwaf Kharma, Cheng-Lin Liu, 和 Ching Suen。2007年。*字符识别系统：学生与实践者指南*。Wiley出版社。Cirone,
    Alexandra, 和 Arthur Spirling。2021年。“将历史转化为数据：HPE中的数据收集、测量与推断。”*《历史政治经济学杂志》* 1
    (1): 127–54。[https://doi.org/10.1561/115.00000005](https://doi.org/10.1561/115.00000005)。Collins,
    Annie, 和 Rohan Alexander。2022年。“COVID-19预印本的可重复性。”*《科学计量学》* 127: 4655–73。[https://doi.org/10.1007/s11192-022-04418-2](https://doi.org/10.1007/s11192-022-04418-2)。Comer,
    Benjamin P., 和 Jason R. Ingram。2022年。“比较2015-2019年间的‘致命遭遇’、‘警察暴力地图’和《华盛顿邮报》致命警察枪击数据：一项研究笔记。”*《刑事司法评论》*，一月，073401682110710。[https://doi.org/10.1177/07340168211071014](https://doi.org/10.1177/07340168211071014)。Crawford,
    Kate。2021年。*《人工智能图集》*。第一版。纽黑文：耶鲁大学出版社。Cummins, Neil。2022年。“英格兰王朝的隐性财富，1892–2016。”*《经济史评论》*
    75 (3): 667–702。[https://doi.org/10.1111/ehr.13120](https://doi.org/10.1111/ehr.13120)。Eisenstein,
    Michael。2022年。“需要网络数据？这是如何获取它们的方法。”*《自然》* 607: 200–201。[https://doi.org/10.1038/d41586-022-01830-9](https://doi.org/10.1038/d41586-022-01830-9)。Firke,
    Sam。2023年。*janitor: 用于检查和清理脏数据的简单工具*。[https://CRAN.R-project.org/package=janitor](https://CRAN.R-project.org/package=janitor)。Grolemund,
    Garrett, 和 Hadley Wickham。2011年。“使用lubridate轻松处理日期和时间。”*《统计软件杂志》* 40 (3): 1–25。[https://doi.org/10.18637/jss.v040.i03](https://doi.org/10.18637/jss.v040.i03)。Hackett,
    Robert。2016年。“研究人员因发布7万名OkCupid用户数据引发轩然大波。”*《财富》*，五月。[https://fortune.com/2016/05/18/okcupid-data-research/](https://fortune.com/2016/05/18/okcupid-data-research/)。Healy,
    Kieran。2022年。“各有各的不幸，”七月。[https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/](https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/)。Jenkins,
    Jennifer, Steven Rich, Andrew Ba Tran, Paige Moody, Julie Tate, 和 Ted Mellnik。2022年。“《华盛顿邮报》如何审查美国的警察枪击事件。”[https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/](https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/)。Johnson,
    Kaneesha。2021年。“监狱数据收集的两种制度。”*《哈佛数据科学评论》* 3 (3)。[https://doi.org/10.1162/99608f92.72825001](https://doi.org/10.1162/99608f92.72825001)。Kirkegaard,
    Emil, 和 Julius Bjerrekær。2016年。“OkCupid数据集：一个非常庞大的约会网站用户公共数据集。”*《开放差异心理学》*，1–10。[https://doi.org/10.26775/ODP.2016.11.03](https://doi.org/10.26775/ODP.2016.11.03)。Kish,
    Leslie。1959年。“研究设计中的一些统计问题。”*《美国社会学评论》* 24 (3): 328–38。[https://doi.org/10.2307/2089381](https://doi.org/10.2307/2089381)。Luscombe,
    Alex, Kevin Dick, 和 Kevin Walby。2021年。“公共利益中的算法思维：克服社会科学中网络爬取的技术、法律和伦理障碍。”*《质量与数量》*
    56 (3): 1–22。[https://doi.org/10.1007/s11135-021-01164-0](https://doi.org/10.1007/s11135-021-01164-0)。Luscombe,
    Alex, Jamie Duncan, 和 Kevin Walby。2022年。“启动司法学科：犯罪学与刑事司法研究中收集和分析文本与图像数据的计算-定性方法。”*《刑事司法教育杂志》*
    33 (2): 151–71。[https://doi.org/10.1080/10511253.2022.2027477](https://doi.org/10.1080/10511253.2022.2027477)。Müller,
    Kirill。2020年。*here: 一种更简单的文件定位方式*。[https://CRAN.R-project.org/package=here](https://CRAN.R-project.org/package=here)。Nix,
    Justin, 和 M. James Lozada。2020年。“警察枪杀手无寸铁的非裔美国人：对社区心理健康溢出效应的重新评估，”一月。[https://doi.org/10.31235/osf.io/ajz2q](https://doi.org/10.31235/osf.io/ajz2q)。Ooms,
    Jeroen。2014年。“jsonlite包：JSON数据与R对象之间实用且一致的映射。”*arXiv:1403.2805 [Stat.CO]*。[https://arxiv.org/abs/1403.2805](https://arxiv.org/abs/1403.2805)。———.
    2022a。*pdftools: PDF文档的文本提取、渲染和转换*。[https://CRAN.R-project.org/package=pdftools](https://CRAN.R-project.org/package=pdftools)。———.
    2022b。*tesseract: 开源OCR引擎*。[https://CRAN.R-project.org/package=tesseract](https://CRAN.R-project.org/package=tesseract)。Pavlik,
    Kaylin。2019年。“使用Spotify音频特征理解与分类音乐流派。”[https://www.kaylinpavlik.com/classifying-songs-genres/](https://www.kaylinpavlik.com/classifying-songs-genres/)。Perepolkin,
    Dmytro。2022年。*polite: 在网络上保持礼貌*。[https://CRAN.R-project.org/package=polite](https://CRAN.R-project.org/package=polite)。R
    Core Team。2024年。*R: 用于统计计算的语言和环境*。奥地利维也纳：R统计计算基金会。[https://www.R-project.org/](https://www.R-project.org/)。Salganik,
    Matthew, Peter Sheridan Dodds, 和 Duncan Watts。2006年。“人工文化市场中不平等与不可预测性的实验研究。”*《科学》*
    311 (5762): 854–56。[https://doi.org/10.1126/science.1121066](https://doi.org/10.1126/science.1121066)。Saulnier,
    Lucile, Siddharth Karamcheti, Hugo Laurençon, Léo Tronchon, Thomas Wang, Victor
    Sanh, Amanpreet Singh, 等人。2022年。“将伦理原则置于研究生命周期的核心。”[https://huggingface.co/blog/ethical-charter-multimodal](https://huggingface.co/blog/ethical-charter-multimodal)。Schmertmann,
    Carl。2022年。“联合国API测试，”七月。[https://bonecave.schmert.net/un-api-example.html](https://bonecave.schmert.net/un-api-example.html)。Taflaga,
    Marija, 和 Matthew Kerby。2019年。“部长办公室中谁做什么工作：澳大利亚政治办公室中政治任命人员与女性的描述性代表，1979-2010。”*《政治研究》*
    68 (2): 463–85。[https://doi.org/10.1177/0032321719853459](https://doi.org/10.1177/0032321719853459)。《经济学人》。2022年。“Spotify数据显示英语的衰落，”一月。[https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english](https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english)。《华盛顿邮报》。2023年。“致命武力数据库。”[https://github.com/washingtonpost/data-police-shootings](https://github.com/washingtonpost/data-police-shootings)。Thompson,
    Charlie, Daniel Antal, Josiah Parry, Donal Phipps, 和 Tom Wolff。2022年。*spotifyr:
    “Spotify” Web API的R语言封装*。[https://CRAN.R-project.org/package=spotifyr](https://CRAN.R-project.org/package=spotifyr)。Thomson-DeVeaux,
    Amelia, Laura Bronner, 和 Damini Sharma。2021年。“城市每年在警察不当行为上花费数百万美元。这就是为什么追究部门责任如此困难。”*《FiveThirtyEight》*，二月。[https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/](https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/)。Wickham,
    Hadley。2021年。*babynames: 美国婴儿名字 1880-2017*。[https://CRAN.R-project.org/package=babynames](https://CRAN.R-project.org/package=babynames)。———.
    2022年。*rvest: 轻松抓取网页*。[https://CRAN.R-project.org/package=rvest](https://CRAN.R-project.org/package=rvest)。———.
    2023年。*httr: 处理URL和HTTP的工具*。[https://CRAN.R-project.org/package=httr](https://CRAN.R-project.org/package=httr)。Wickham,
    Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain
    François, Garrett Grolemund, 等人。2019年。“欢迎来到Tidyverse。”*《开源软件杂志》* 4 (43): 1686。[https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686)。Wickham,
    Hadley, Jennifer Bryan, 和 Malcolm Barrett。2022年。*usethis: 自动化包和项目设置*。[https://CRAN.R-project.org/package=usethis](https://CRAN.R-project.org/package=usethis)。Wickham,
    Hadley, 和 Lionel Henry。2022年。*purrr: 函数式编程工具*。[https://CRAN.R-project.org/package=purrr](https://CRAN.R-project.org/package=purrr)。Wickham,
    Hadley, Jim Hester, 和 Jeroen Ooms。2021年。*xml2: 解析XML*。[https://CRAN.R-project.org/package=xml2](https://CRAN.R-project.org/package=xml2)。Wong,
    Julia Carrie。2020年。“在特朗普庞大的Facebook竞选活动内部一年。”*《卫报》*，一月。[https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election](https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election)。Zimmer,
    Michael。2018年。“弥合大数据研究伦理中的概念鸿沟：情境完整性的应用。”*《社交媒体+社会》* 4 (2): 1–11。[https://doi.org/10.1177/2056305118768300](https://doi.org/10.1177/2056305118768300)。'
- en: Students who finish quickly should similarly look at Jessica’s departure from
    Girls’ Generation in September 2014, and then attempt to compare the two situations.[↩︎](#fnref1)***************
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提前完成的学生同样应该查看杰西卡于2014年9月离开少女时代的情况，然后尝试比较这两种情形。[↩︎](#fnref1)
