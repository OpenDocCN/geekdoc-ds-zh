- en: 7  APIs, scraping, and parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://tellingstorieswithdata.com/07-gather.html](https://tellingstorieswithdata.com/07-gather.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Acquisition](./06-farm.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7  APIs, scraping, and parsing](./07-gather.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Chapman and Hall/CRC published this book in July 2023\. You can purchase that
    [here](https://www.routledge.com/Telling-Stories-with-Data-With-Applications-in-R/Alexander/p/book/9781032134772).
    This online version has some updates to what was printed.*  ***Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read *Turning History into Data: Data Collection, Measurement, and Inference
    in HPE*, ([Cirone and Spirling 2021](99-references.html#ref-cirone))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper discusses some of the challenges of creating datasets.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *Two Regimes of Prison Data Collection*, ([Johnson 2021](99-references.html#ref-Johnson2021Two))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper compares data about prisons from the United States government with
    data from incarcerated people and the community.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *Atlas of AI*, ([Crawford 2021](99-references.html#ref-crawford))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on Chapter 3 “Data”, which discusses the importance of understanding the
    sources of data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key concepts and skills**'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes data are available but they are not necessarily put together for the
    purposes of being a dataset. We must gather the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be cumbersome and annoying to clean and prepare the datasets that come
    from these unstructured sources but the resulting structured, tidy data are often
    especially exciting and useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can gather data from a variety of sources. This includes APIs, both directly,
    which may involve semi-structured data, and indirectly through `R` packages. We
    can also gather data through reasonable and ethical web scraping. Finally, we
    may wish to gather data from PDFs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software and packages**'
  prefs: []
  type: TYPE_NORMAL
- en: Base `R` ([R Core Team 2024](99-references.html#ref-citeR))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`babynames` ([Wickham 2021](99-references.html#ref-citebabynames))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gh` ([Bryan and Wickham 2021](99-references.html#ref-gh))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`here` ([Müller 2020](99-references.html#ref-here))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`httr` ([Wickham 2023](99-references.html#ref-citehttr))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`janitor` ([Firke 2023](99-references.html#ref-janitor))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jsonlite` ([Ooms 2014](99-references.html#ref-jsonlite))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lubridate` ([Grolemund and Wickham 2011](99-references.html#ref-GrolemundWickham2011))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pdftools` ([Ooms 2022a](99-references.html#ref-pdftools))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`purrr` ([Wickham and Henry 2022](99-references.html#ref-citepurrr))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rvest` ([Wickham 2022](99-references.html#ref-citervest))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spotifyr` ([Thompson et al. 2022](99-references.html#ref-spotifyr)) (this
    package is not on CRAN so install it with: `install.packages("devtools")` then
    `devtools::install_github("charlie86/spotifyr")`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tesseract` ([Ooms 2022b](99-references.html#ref-citetesseract))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tidyverse` ([Wickham et al. 2019](99-references.html#ref-tidyverse))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tinytable` ([Arel-Bundock 2024](99-references.html#ref-tinytable))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`usethis` ([Wickham, Bryan, and Barrett 2022](99-references.html#ref-usethis))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xml2` ([Wickham, Hester, and Ooms 2021](99-references.html#ref-xml2))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*## 7.1 Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we consider data that we must gather ourselves. This means that
    although the observations exist, we must parse, pull, clean, and prepare them
    to get the dataset that we will consider. In contrast to farmed data, discussed
    in [Chapter 6](06-farm.html), often these observations are not being made available
    for the purpose of analysis. This means that we need to be especially concerned
    with documentation, inclusion and exclusion decisions, missing data, and ethical
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of such a dataset, consider Cummins ([2022](99-references.html#ref-Cummins2022))
    who create a dataset using individual-level probate records from England between
    1892 and 1992\. They find that about one-third of the inheritance of “elites”
    is concealed. Similarly, Taflaga and Kerby ([2019](99-references.html#ref-Taflaga2019))
    construct a systematic dataset of job responsibilities based on Australian Ministerial
    telephone directories. They find substantial differences by gender. Neither wills
    nor telephone directories were created for the purpose of being included in a
    dataset. But with a respectful approach they enable insights that we could not
    get by other means. We term this “data gathering”—the data exist but we need to
    get them.
  prefs: []
  type: TYPE_NORMAL
- en: Decisions need to be made at the start of a project about the values we want
    the project to have. For instance, Saulnier et al. ([2022](99-references.html#ref-huggingfaceethics))
    value transparency, reproducibility, fairness, being self-critical, and giving
    credit. How might that affect the project? Valuing “giving credit” might mean
    being especially zealous about attribution and licensing. In the case of gathered
    data we should give special thought to this as the original, unedited data may
    not be ours.
  prefs: []
  type: TYPE_NORMAL
- en: The results of a data science workflow cannot be better than their underlying
    data ([Bailey 2008](99-references.html#ref-bailey2008design)). Even the most-sophisticated
    statistical analysis will struggle to adjust for poorly-gathered data. This means
    when working in a team, data gathering should be overseen and at least partially
    conducted by senior members of the team. And when working by yourself, try to
    give special consideration and care to this stage.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we go through a variety of approaches for gathering data. We
    begin with the use of APIs and semi-structured data, such as JSON and XML. Using
    an API is typically a situation in which the data provider has specified the conditions
    under which they are comfortable providing access. An API allows us to write code
    to gather data. This is valuable because it can be efficient and scales well.
    Developing comfort with gathering data through APIs enables access to exciting
    datasets. For instance, Wong ([2020](99-references.html#ref-facebookapitrump))
    use the Facebook Political Ad API to gather 218,100 of the Trump 2020 campaign
    ads to better understand the campaign.
  prefs: []
  type: TYPE_NORMAL
- en: We then turn to web scraping, which we may want to use when there are data available
    on a website. As these data have typically not been put together for the purposes
    of being a dataset, it is especially important to have deliberate and definite
    values for the project. Scraping is a critical part of data gathering because
    there are many data sources where the priorities of the data provider mean they
    have not implemented an API. For instance, considerable use of web scraping was
    critical for creating COVID-19 dashboards in the early days of the pandemic ([Eisenstein
    2022](99-references.html#ref-scrapecoviddata)).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we consider gathering data from PDFs. This enables the construction
    of interesting datasets, especially those contained in government reports and
    old books. Indeed, while freedom of information legislation exists in many countries
    and require the government to make data available, these all too often result
    in spreadsheets being shared as PDFs, even when they were a CSV to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering data can require more of us than using farmed data, but it allows
    us to explore datasets and answer questions that we could not otherwise. Some
    of the most exciting work in the world uses gathered data, but it is especially
    important that we approach it with respect.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In everyday language, and for our purposes, an Application Programming Interface
    (API) is a situation in which someone has set up specific files on their computer
    such that we can follow their instructions to get them. For instance, when we
    use a gif on Slack, one way it could work in the background is that Slack asks
    Giphy’s server for the appropriate gif, Giphy’s server gives that gif to Slack,
    and then Slack inserts it into the chat. The way in which Slack and Giphy interact
    is determined by Giphy’s API. More strictly, an API is an application that runs
    on a server that we access using the HTTP protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we focus on using APIs for gathering data. In that context an API is a
    website that is set up for another computer to be able to access it, rather than
    a person. For instance, we could go to [Google Maps](https://www.google.com/maps).
    And we could then scroll and click and drag to center the map on, say, Canberra,
    Australia. Or we could paste [this link](https://www.google.com/maps/@-35.2812958,149.1248113,16z)
    into the browser. By pasting that link, rather than navigating, we have mimicked
    how we will use an API: provide a URL and be given something back. In this case
    the result should be a map like [Figure 7.1](#fig-focuson2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afd3bfc181560c91543bc1e12a859c71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Example of an API response from Google Maps, as of 12 February
    2023'
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using an API is that the data provider usually specifies the
    data that they are willing to provide, and the terms under which they will provide
    it. These terms may include aspects such as rate limits (i.e. how often we can
    ask for data), and what we can do with the data, for instance, we might not be
    allowed to use it for commercial purposes, or to republish it. As the API is being
    provided specifically for us to use it, it is less likely to be subject to unexpected
    changes or legal issues. Because of this it is clear that when an API is available,
    we should try to use it rather than web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: We will now go through a few case studies of using APIs. In the first we deal
    directly with an API using `httr`. And then we access data from Spotify using
    `spotifyr`.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 arXiv, NASA, and Dataverse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After installing and loading `httr` we use `GET()` to obtain data from an API
    directly. This will try to get some specific data and the main argument is “url”.
    This is similar to the Google Maps example in [Figure 7.1](#fig-focuson2020) where
    the specific information that we were interested in was a map.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1.1 arXiv
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this case study we will use an [API provided by arXiv](https://arxiv.org/help/api/).
    arXiv is an online repository for academic papers before they go through peer
    review. These papers are typically referred to as “pre-prints”. We use `GET()`
    to ask arXiv to obtain some information about a pre-print by providing a URL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*We can use `status_code()` to check our response. For instance, 200 means
    a success, while 400 means we received an error from the server. Assuming we received
    something back from the server, we can use `content()` to display it. In this
    case we have received XML formatted data. XML is a markup language where entries
    are identified by tags, which can be nested within other tags. After installing
    and loading `xml2` we can read XML using `read_xml()`. XML is a semi-formatted
    structure, and it can be useful to start by having a look at it using `html_structure()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*We might like to create a dataset based on extracting various aspects of this
    XML tree. For instance, we might look at “entry”, which is the eighth item, and
    in particular obtain the “title” and the “URL”, which are the fourth and ninth
    items, respectively, within “entry”.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]**  ***#### 7.2.1.2 NASA Astronomy Picture of the Day'
  prefs: []
  type: TYPE_NORMAL
- en: To consider another example, each day, NASA provides the Astronomy Picture of
    the Day (APOD) through its [APOD API](https://api.nasa.gov). We can use `GET()`
    to obtain the URL for the photo on particular dates and then display it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Examining the returned data using `content()`, we can see that we are provided
    with various fields, such as date, title, explanation, and a URL.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE6]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE8]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE10]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE12]****  ***We can provide that URL to `include_graphics()` from `knitr`
    to display it ([Figure 7.2](#fig-nasaone)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec8efbfe286dd94d07ab7813ed0e21cf.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Tranquility Base Panorama (Image Credit: Neil Armstrong, Apollo 11, NASA)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.2: Images obtained from the NASA APOD API****  ***#### 7.2.1.3 Dataverse'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, another common API response in semi-structured form is JSON. JSON is
    a human-readable way to store data that can be parsed by machines. In contrast
    to, say, a CSV, where we are used to rows and columns, JSON uses key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*We can parse JSON with `jsonlite`. To consider a specific example, we use
    a “Dataverse” which is a web application that makes it easier to share datasets.
    We can use an API to query a demonstration dataverse. For instance, we might be
    interested in datasets related to politics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE15]*  *We could look at the dataset using `View(politics_datasets)`, which
    would allow us to expand the tree based on what we are interested in. We can even
    get the code that we need to focus on different aspects by hovering on the item
    and then clicking the icon with the green arrow ([Figure 7.3](#fig-jsonfirst)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6093076a61736f04b6362ce66c2c468.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Example of hovering over a JSON element, “items”, where the icon
    with a green arrow can be clicked on to get the code that would focus on that
    element'
  prefs: []
  type: TYPE_NORMAL
- en: This tells us how to obtain the dataset of interest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]********  ***### 7.2.2 Spotify'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes there is an `R` package built around an API and allows us to interact
    with it in ways that are similar what we have seen before. For instance, `spotifyr`
    is a wrapper around the Spotify API. When using APIs, even when they are wrapped
    in an `R` package, in this case `spotifyr`, it is important to read the terms
    under which access is provided.
  prefs: []
  type: TYPE_NORMAL
- en: To access the Spotify API, we need a [Spotify Developer Account](https://developer.spotify.com/dashboard/).
    This is free but will require logging in with a Spotify account and then accepting
    the Developer Terms ([Figure 7.4](#fig-spotifyaccept)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/728ebffe0411c5b89542c6a03f62393d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Spotify Developer Account Terms agreement page'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with the registration process, in our case, we “do not know” what
    we are building and so Spotify requires us to use a non-commercial agreement which
    is fine. To use the Spotify API we need a “Client ID” and a “Client Secret”. These
    are things that we want to keep to ourselves because otherwise anyone with the
    details could use our developer account as though they were us. One way to keep
    these details secret with minimum hassle is to keep them in our “System Environment”.
    In this way, when we push to GitHub they should not be included. To do this we
    will load and use `usethis` to modify our System Environment. In particular, there
    is a file called “.Renviron” which we will open and then add our “Client ID” and
    “Client Secret”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*When we run `edit_r_environ()`, a “.Renviron” file will open and we can add
    our “Spotify Client ID” and “Client Secret”. Use the same names, because `spotifyr`
    will look in our environment for keys with those specific names. Being careful
    to use single quotes is important here even though we normally use double quotes
    in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*Save the “.Renviron” file, and then restart R: “Session” \(\rightarrow\) “Restart
    R”. We can now use our “Spotify Client ID” and “Client Secret” as needed. And
    functions that require those details as arguments will work without them being
    explicitly specified again.'
  prefs: []
  type: TYPE_NORMAL
- en: To try this out we install and load `spotifyr`. We will get and save some information
    about Radiohead, the English rock band, using `get_artist_audio_features()`. One
    of the required arguments is `authorization`, but as that is set, by default,
    to look at the “.Renviron” file, we do not need to specify it here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE20]'
  prefs: []
  type: TYPE_NORMAL
- en: '*There is a variety of information available based on songs. We might be interested
    to see whether their songs are getting longer over time ([Figure 7.5](#fig-readioovertime)).
    Following the guidance in [Chapter 5](05-graphs_tables_maps.html) this is a nice
    opportunity to additionally use a boxplot to communicate summary statistics by
    album at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*![](../Images/0afb82a03ce84939f1677b823cb68b8f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.5: Length of each Radiohead song, over time, as gathered from Spotify*  *One
    interesting variable provided by Spotify about each song is “valence”. The Spotify
    [documentation](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features)
    describes this as a measure between zero and one that signals “the musical positiveness”
    of the track with higher values being more positive. We might be interested to
    compare valence over time between a few artists, for instance, Radiohead, the
    American rock band The National, and the American singer Taylor Swift.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to gather the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*Then we can bring them together and make the graph ([Figure 7.6](#fig-swiftyvsnationalvsradiohead)).
    This appears to show that while Taylor Swift and Radiohead have largely maintained
    their level of valence over time, The National has decreased theirs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*![](../Images/81547c5eba6fd1fa6f2121d817444d4c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.6: Comparing valence, over time, for Radiohead, Taylor Swift, and
    The National*  *How amazing that we live in a world where all that information
    is available with very little effort or cost! And having gathered the data, there
    much that could be done. For instance, Pavlik ([2019](99-references.html#ref-kaylinpavlik))
    uses an expanded dataset to classify musical genres and The Economist ([2022](99-references.html#ref-theeconomistonspotify))
    looks at how language is associated with music streaming on Spotify. Our ability
    to gather such data enables us to answer questions that had to be considered experimentally
    in the past. For instance, Salganik, Dodds, and Watts ([2006](99-references.html#ref-salganik2006experimental))
    had to use experimental data to analyze the social aspect of what makes a hit
    song, rather than the observational data we can now access.'
  prefs: []
  type: TYPE_NORMAL
- en: That said, it is worth thinking about what valence is purporting to measure.
    Little information is available in the Spotify documentation how it was created.
    It is doubtful that one number can completely represent how positive is a song.
    And what about the songs from these artists that are not on Spotify, or even publicly
    released? This is a nice example of how measurement and sampling pervade all aspects
    of telling stories with data.**********  ***## 7.3 Web scraping
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Principles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Web scraping is a way to get data from websites. Rather than going to a website
    using a browser and then saving a copy of it, we write code that does it for us.
    This opens considerable data to us, but on the other hand, it is not typically
    data that are being made available for these purposes. This means that it is especially
    important to be respectful. While generally not illegal, the specifics about the
    legality of web scraping depend on jurisdictions and what we are doing, and so
    it is also important to be mindful. Even if our use is not commercially competitive,
    of particular concern is the conflict between the need for our work to be reproducible
    with the need to respect terms of service that may disallow data republishing
    ([Luscombe, Dick, and Walby 2021](99-references.html#ref-luscombe2021algorithmic)).
  prefs: []
  type: TYPE_NORMAL
- en: Privacy often trumps reproducibility. There is also a considerable difference
    between data being publicly available on a website and being scraped, cleaned,
    and prepared into a dataset which is then publicly released. For instance, Kirkegaard
    and Bjerrekær ([2016](99-references.html#ref-kirkegaard2016okcupid)) scraped publicly
    available OKCupid profiles and then made the resulting dataset easily available
    ([Hackett 2016](99-references.html#ref-hackett2016researchers)). Zimmer ([2018](99-references.html#ref-zimmer2018addressing))
    details some of the important considerations that were overlooked including “minimizing
    harm”, “informed consent”, and ensuring those in the dataset maintain “privacy
    and confidentiality”. While it is correct to say that OKCupid made data public,
    they did so in a certain context, and when their data was scraped that context
    was changed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Oh, you think we have good data on that!* *Police violence is particularly
    concerning because of the need for trust between the police and society. Without
    good data it is difficult to hold police departments accountable, or know whether
    there is an issue, but getting data is difficult ([Thomson-DeVeaux, Bronner, and
    Sharma 2021](99-references.html#ref-bronnerpolicenordata)). The fundamental problem
    is that there is no way to easily simplify an encounter that results in violence
    into a dataset. Two popular datasets draw on web scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: “Mapping Police Violence”; and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “Fatal Force Database”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bor et al. ([2018](99-references.html#ref-Bor2018)) use “Mapping Police Violence”
    to examine police killings of Black Americans, especially when unarmed, and find
    a substantial effect on the mental health of Black Americans. Responses to the
    paper, such as Nix and Lozada ([2020](99-references.html#ref-Nix2020)), have special
    concern with the coding of the dataset, and after re-coding draw different conclusions.
    An example of a coding difference is the unanswerable question, because it depends
    on context and usage, of whether to code an individual who was killed with a toy
    firearm as “armed” or “unarmed”. We may want a separate category, but some simplification
    is necessary for the construction of a quantitative dataset. *The Washington Post*
    writes many articles using the “Fatal Force Database” ([The Washington Post 2023](99-references.html#ref-washpostfatalforce)).
    Jenkins et al. ([2022](99-references.html#ref-washpostfatalforcemethods)) describes
    their methodology and the challenges of standardization. Comer and Ingram ([2022](99-references.html#ref-Comer2022))
    compare the datasets and find similarities, but document ways in which the datasets
    are different.*  *Web scraping is an invaluable source of data. But they are typically
    datasets that can be created as a by-product of someone trying to achieve another
    aim. And web scraping imposes a cost on the website host, and so we should reduce
    this to the extent possible. For instance, a retailer may have a website with
    their products and their prices. That has not been created deliberately as a source
    of data, but we can scrape it to create a dataset. The following principles may
    be useful to guide web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid it. Try to use an API wherever possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abide by their desires. Some websites have a “robots.txt” file that contains
    information about what they are comfortable with scrapers doing. In general, if
    it exists, a “robots.txt” file can be accessed by appending “robots.txt” to the
    base URL. For instance, the “robots.txt” file for https://www.google.com, can
    be accessed at https://www.google.com/robots.txt. Note if there are folders listed
    against “Disallow:”. These are the folders that the website would not like to
    be scraped. And also note any instances of “Crawl-delay:”. This is the number
    of seconds the website would like you to wait between visits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the impact.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slow down the scraper, for instance, rather than having it visit the website
    every second, slow it down using `sys.sleep()`. If you only need a few hundred
    files, then why not just have it visit the website a few times a minute, running
    in the background overnight?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the timing of when you run the scraper. For instance, if you are scraping
    a retailer then maybe set the script to run from 10pm through to the morning,
    when fewer customers are likely using the site. Similarly, if it is a government
    website and they have a regular monthly release, then it might be polite to avoid
    that day.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Take only what is needed. For instance, you do not need to scrape the entirety
    of Wikipedia if all you need is the names of the ten largest cities in Croatia.
    This reduces the impact on the website, and allows us to more easily justify our
    actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only scrape once. This means you should save everything as you go so that you
    do not have to re-collect data when the scraper inevitably fails at some point.
    For instance, you will typically spend considerable time getting a scraper working
    on one page, but typically the page structure will change at some point and the
    scraper will need to be updated. Once you have the data, you should save that
    original, unedited data separately to the modified data. If you need data over
    time then you will need to go back, but this is different than needlessly re-scraping
    a page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do not republish the pages that were scraped (this contrasts with datasets that
    you create from it).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take ownership and ask permission if possible. At a minimum all scripts should
    have contact details in them. Depending on the circumstances, it may be worthwhile
    asking for permission before you scrape.*  *### 7.3.2 HTML/CSS essentials
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Web scraping is possible by taking advantage of the underlying structure of
    a webpage. We use patterns in the HTML/CSS to get the data that we want. To look
    at the underlying HTML/CSS we can either:'
  prefs: []
  type: TYPE_NORMAL
- en: open a browser, right-click, and choose something like “Inspect”; or
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: save the website and then open it with a text editor rather than a browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'HTML/CSS is a markup language based on matching tags. If we want text to be
    bold, then we would use something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, if we want a list, then we start and end the list as well as indicating
    each item.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When scraping we will search for these tags.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, we can pretend that we obtained some HTML from a website, and
    that we want to get the name from it. We can see that the name is in bold, so
    we want to focus on that feature and extract it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*`rvest` is part of the `tidyverse` so it does not have to be installed, but
    it is not part of the core, so it does need to be loaded. After that, use `read_html()`
    to read in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE28]*  *The language used by `rvest` to look for tags is “node”, so we
    focus on bold nodes. By default `html_elements()` returns the tags as well. We
    extract the text with `html_text()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE30]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE32]**  **Web scraping is an exciting source of data, and we will now go
    through some examples. But in contrast to these examples, information is not usually
    all on one page. Web scraping quickly becomes a difficult art form that requires
    practice. For instance, we distinguish between an index scrape and a contents
    scrape. The former is scraping to build the list of URLs that have the content
    you want, while the latter is to get the content from those URLs. An example is
    provided by Luscombe, Duncan, and Walby ([2022](99-references.html#ref-luscombe2022jumpstarting)).
    If you end up doing much web scraping, then `polite` ([Perepolkin 2022](99-references.html#ref-polite))
    may be helpful to better optimize your workflow. And using GitHub Actions to allow
    for larger and slower scrapes over time.****  ***### 7.3.3 Book information'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case study we will scrape a list of books available [here](https://rohansbooks.com).
    We will then clean the data and look at the distribution of the first letters
    of author surnames. It is slightly more complicated than the example above, but
    the underlying workflow is the same: download the website, look for the nodes
    of interest, extract the information, and clean it.'
  prefs: []
  type: TYPE_NORMAL
- en: We use `rvest` to download a website, and to then navigate the HTML to find
    the aspects that we are interested in. And we use the `tidyverse` to clean the
    dataset. We first need to go to the website and then save a local copy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*We need to navigate the HTML to get the aspects that we want. And then try
    to get the data into a tibble as quickly as possible because this will allow us
    to more easily use `dplyr` verbs and other functions from the `tidyverse`.'
  prefs: []
  type: TYPE_NORMAL
- en: See [Online Appendix A](20-r_essentials.html) if this is unfamiliar to you.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE35]'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE36]*  *To get the data into a tibble we first need to use HTML tags to
    identify the data that we are interested in. If we look at the website then we
    know we need to focus on list items ([Figure 7.7 (a)](#fig-rohansbooks-display)).
    And we can look at the source, focusing particularly on looking for a list ([Figure 7.7
    (b)](#fig-rohansbooks-html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f65ffef9474575c08d02f1f82ddd6d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Books website as displayed
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9aeae5c6cf4ecd66247cb1879f7ca1db.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HTML for the top of the books website and the list of books
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.7: Screen captures from the books website as at 16 June 2022'
  prefs: []
  type: TYPE_NORMAL
- en: The tag for a list item is “li”, so we can use that to focus on the list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE38]*  *We now need to clean the data. First we want to separate the title
    and the author using `separate()` and then clean up the author and title columns.
    We can take advantage of the fact that the year is present and separate based
    on that.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE40]*  *Finally, we could make, say, a table of the distribution of the
    first letter of the names ([Table 7.1](#tbl-lettersofbooks)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 7.1: Distribution of first letter of author names in a collection of
    books'
  prefs: []
  type: TYPE_NORMAL
- en: '| First letter | Number of times |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| G | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| H | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| I | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| L | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| M | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| P | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| R | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| V | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| W | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 1 |******  ***### 7.3.4 Prime Ministers of the United Kingdom'
  prefs: []
  type: TYPE_NORMAL
- en: In this case study we are interested in how long prime ministers of the United
    Kingdom lived, based on the year they were born. We will scrape data from Wikipedia
    using `rvest`, clean it, and then make a graph. From time to time a website will
    change. This makes many scrapes largely bespoke, even if we can borrow some code
    from earlier projects. It is normal to feel frustrated at times. It helps to begin
    with an end in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'To that end, we can start by generating some simulated data. Ideally, we want
    a table that has a row for each prime minister, a column for their name, and a
    column each for the birth and death years. If they are still alive, then that
    death year can be empty. We know that birth and death years should be somewhere
    between 1700 and 1990, and that death year should be larger than birth year. Finally,
    we also know that the years should be integers, and the names should be characters.
    We want something that looks roughly like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE43]*  *One of the advantages of generating a simulated dataset is that
    if we are working in groups then one person can start making the graph, using
    the simulated dataset, while the other person gathers the data. In terms of a
    graph, we are aiming for something like [Figure 7.8](#fig-pmsgraphexample).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0af0cc21150e3b575aa9173b733dd030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Sketch of planned graph showing how long United Kingdom prime ministers
    lived'
  prefs: []
  type: TYPE_NORMAL
- en: We are starting with a question that is of interest, which is how long each
    prime minister of the United Kingdom lived. As such, we need to identify a source
    of data. While there are plenty of data sources that have the births and deaths
    of each prime minister, we want one that we can trust, and as we are going to
    be scraping, we want one that has some structure to it. The [Wikipedia page about
    prime ministers of the United Kingdom](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom)
    fits both these criteria. As it is a popular page the information is likely to
    be correct, and the data are available in a table.
  prefs: []
  type: TYPE_NORMAL
- en: We load `rvest` and then download the page using `read_html()`. Saving it locally
    provides us with a copy that we need for reproducibility in case the website changes,
    and means that we do not have to keep visiting the website. But it is not ours,
    and so this is typically not something that should be publicly redistributed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '*As with the earlier case study, we are looking for patterns in the HTML that
    we can use to help us get closer to the data that we want. This is an iterative
    process and involves trial and error. Even simple examples will take time.'
  prefs: []
  type: TYPE_NORMAL
- en: One tool that may help is the [SelectorGadget](https://rvest.tidyverse.org/articles/articles/selectorgadget.html).
    This allows us to pick and choose the elements that we want, and then gives us
    the input for `html_element()` ([Figure 7.9](#fig-selectorgadget)). By default,
    SelectorGadget uses CSS selectors. These are not the only way to specify the location
    of the information you want, and using an alternative, such as XPath, can be a
    useful option to consider.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b96206416f85fa0a02df6208f651ba1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Using the Selector Gadget to identify the tag, as at 12 February
    2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE46]'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE47]*  *In this case there are many columns that we do not need, and some
    duplicated rows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE49]*  *Now that we have the parsed data, we need to clean it to match
    what we wanted. We want a names column, as well as columns for birth year and
    death year. We use `separate()` to take advantage of the fact that it looks like
    the names and dates are distinguished by brackets. The argument in `str_extract()`
    is a regular expression. It looks for four digits in a row, followed by a dash,
    followed by four more digits in a row. We use a slightly different regular expression
    for those prime ministers who are still alive.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE51]*  *Finally, we need to clean up the columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE53]*  *Our dataset looks similar to the one that we said we wanted at
    the start ([Table 7.2](#tbl-canadianpmscleanddata)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 7.2: UK Prime Ministers, by how old they were when they died'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prime Minister | Birth year | Death year | Age at death |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Robert Walpole | 1676 | 1745 | 69 |'
  prefs: []
  type: TYPE_TB
- en: '| Spencer Compton | 1673 | 1743 | 70 |'
  prefs: []
  type: TYPE_TB
- en: '| Henry Pelham | 1694 | 1754 | 60 |'
  prefs: []
  type: TYPE_TB
- en: '| Thomas Pelham-Holles | 1693 | 1768 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| William Cavendish | 1720 | 1764 | 44 |'
  prefs: []
  type: TYPE_TB
- en: '| John Stuart | 1713 | 1792 | 79 |*  *At this point we would like to make a
    graph that illustrates how long each prime minister lived ([Figure 7.10](#fig-pmslives)).
    If they are still alive then we would like them to run to the end, but we would
    like to color them differently.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '*![](../Images/1e641b6b14a8ccac6488fe42707a87b3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.10: How long each prime minister of the United Kingdom lived*********  ***###
    7.3.5 Iteration'
  prefs: []
  type: TYPE_NORMAL
- en: Considering text as data is exciting and allows us to explore many different
    research questions. We will draw on it in [Chapter 17](16-text.html). Many guides
    assume that we already have a nicely formatted text dataset, but that is rarely
    actually the case. In this case study we will download files from a few different
    pages. While we have already seen two examples of web scraping, those were focused
    on just one page, whereas we often need many. Here we will focus on this iteration.
    We will use `download.file()` to do the download, and use `purrr` to apply this
    function across multiple sites. You do not need to install or load that package
    because it is part of the core `tidyverse` so it is loaded when you load the `tidyverse`.
  prefs: []
  type: TYPE_NORMAL
- en: The Reserve Bank of Australia (RBA) is Australia’s central bank. It has responsibility
    for setting the cash rate, which is the interest rate used for loans between banks.
    This interest rate is an especially important one and has a large impact on the
    other interest rates in the economy. Four times a year—February, May, August,
    and November—the RBA publishes a statement on monetary policy, and these are available
    as PDFs. In this example we will download two statements published in 2023.
  prefs: []
  type: TYPE_NORMAL
- en: First we set up a tibble that has the information that we need. We will take
    advantage of commonalities in the structure of the URLs. We need to specify both
    a URL and a local file name for each state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE57]'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE58]*  *We want to apply the function `download.files()` to these two statements.
    To do this we write a function that will download the file, let us know that it
    was downloaded, wait a polite amount of time, and then go get the next file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '*We now apply that function to our tibble of URLs and save names using the
    function `walk2()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '*The result is that we have downloaded these PDFs and saved them to our computer.
    An alternative to writing these functions ourselves would be to use `heapsofpapers`
    ([Alexander and Mahfouz 2021](99-references.html#ref-heapsofpapers)). This includes
    various helpful options for downloading lists of files, especially PDF, CSV, and
    txt files. For instance, Collins and Alexander ([2022](99-references.html#ref-collinsalexander))
    use this to obtain thousands of PDFs and estimate the extent to which COVID-19
    research was reproducible. In the next section we will build on this to discuss
    getting information from PDFs.**************  ****## 7.4 PDFs'
  prefs: []
  type: TYPE_NORMAL
- en: PDF files were developed in the 1990s by the technology company Adobe. They
    are useful for documents because they are meant to display in a consistent way
    independent of the environment that created them or the environment in which they
    are being viewed. A PDF viewed on an iPhone should look the same as on an Android
    phone, as on a Linux desktop. One feature of PDFs is that they can include a variety
    of objects, for instance, text, photos, figures, etc. However, this variety can
    limit the capacity of PDFs to be used directly as data. The data first needs to
    be extracted from the PDF.
  prefs: []
  type: TYPE_NORMAL
- en: It is often possible to copy and paste the data from the PDF. This is more likely
    when the PDF only contains text or regular tables. In particular, if the PDF has
    been created by an application such as Microsoft Word, or another document- or
    form-creation system, then often the text data can be extracted in this way because
    they are actually stored as text within the PDF. We begin with that case. But
    it is not as easy if the text has been stored as an image which is then part of
    the PDF. This may be the case for PDFs produced through scans or photos of physical
    documents, and some older document preparation software. We go through that case
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to an API, a PDF is usually produced for human rather than computer
    consumption. The nice thing about PDFs is that they are static and constant. And
    it is great that data are available. But the trade-off is that:'
  prefs: []
  type: TYPE_NORMAL
- en: It is not overly useful to do larger-scale data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not know how the PDF was put together so we do not know whether we can
    trust it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cannot manipulate the data to get results that we are interested in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two important aspects to keep in mind when extracting data from a
    PDF:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin with an end in mind. Plan and sketch what we want from a final dataset/graph/paper
    to limit time wastage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start simple, then iterate. The quickest way to make something that needs to
    be complicated is often to first build a simple version and then add to it. Start
    with just trying to get one page of the PDF working or even just one line. Then
    iterate from there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will go through several examples and then go through a case study where we
    will gather data on United States Total Fertility Rate, by state.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 *Jane Eyre*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 7.11](#fig-firstpdfexample) is a PDF that consists of just the first
    sentence from Charlotte Brontë’s novel *Jane Eyre* taken from Project Gutenberg
    ([Brontë 1847](99-references.html#ref-janeeyre)). You can get it [here](https://github.com/RohanAlexander/telling_stories/blob/aa6e2d76c80eba7bd31ca68161f0065344449ed8/inputs/pdfs/first_example.pdf).
    If we assume that it was saved as “first_example.pdf”, then after installing and
    loading `pdftools` to get the text from this one-page PDF into R.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d5cd624bcb2fd578afc10be9828f45e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: First sentence of Jane Eyre'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE62]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the PDF has been correctly read in, as a character vector.
  prefs: []
  type: TYPE_NORMAL
- en: We will now try a slightly more complicated example that consists of the first
    few paragraphs of *Jane Eyre* ([Figure 7.12](#fig-secondpdfexample)). Now we have
    the chapter heading as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15421ea8d7a8b7df160791a2f901ad72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: First few paragraphs of Jane Eyre'
  prefs: []
  type: TYPE_NORMAL
- en: We use the same function as before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE65]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Again, we have a character vector. The end of each line is signaled by “\n”,
    but other than that it looks pretty good. Finally, we consider the first two pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE68]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the first page is the first element of the character vector, and
    the second page is the second element. As we are most familiar with rectangular
    data, we will try to get it into that format as quickly as possible. And then
    we can use functions from the `tidyverse` to deal with it.
  prefs: []
  type: TYPE_NORMAL
- en: First we want to convert the character vector into a tibble. At this point we
    may like to add page numbers as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '*We then want to separate the lines so that each line is an observation. We
    can do that by looking for “\n” remembering that we need to escape the backslash
    as it is a special character.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE72]*****  ***### 7.4.2 Total Fertility Rate in the United States'
  prefs: []
  type: TYPE_NORMAL
- en: The United States Department of Health and Human Services Vital Statistics Report
    provides information about the Total Fertility Rate (TFR) for each state. The
    average number of births per woman if women experience the current age-specific
    fertility rates throughout their reproductive years. The data are available in
    PDFs. We can use the approaches above to get the data into a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table that we are interested in is on page 40 of a PDF that is available
    [here](https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf) or [here](https://github.com/RohanAlexander/telling_stories/blob/main/inputs/pdfs/dhs/year_2000.pdf).
    The column of interest is labelled: “Total fertility rate” ([Figure 7.13](#fig-dhsexample)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d99b741f301988e450ed5f485e2f10ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Example Vital Statistics Report, from 2000'
  prefs: []
  type: TYPE_NORMAL
- en: The first step when getting data out of a PDF is to sketch out what we eventually
    want. A PDF typically contains a considerable amount of information, and so we
    should be clear about what is needed. This helps keep you focused, and prevents
    scope creep, but it is also helpful when thinking about data checks. We literally
    write down on paper what we have in mind. In this case, what is needed is a table
    with a column for state, year, and total fertility rate (TFR) ([Figure 7.14](#fig-tfrdesired)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/800a04066c07d5149af902f221770cfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Planned dataset of TFR for each US state'
  prefs: []
  type: TYPE_NORMAL
- en: We are interested in a particular column in a particular table for this PDF.
    Unfortunately, there is nothing magical about what is coming. This first step
    requires finding the PDF online, working out the link for each, and searching
    for the page and column name that is of interest. We have built a CSV with the
    details that we need and can read that in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 7.3: Year and associated data for TFR tables'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Page | Table | Column | URL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2000 | 40 | 10 | Total fertility rate | https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf
    |'
  prefs: []
  type: TYPE_TB
- en: We first download and save the PDF using `download.file()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '*We then read the PDF in as a character vector using `pdf_text()` from `pdftools`.
    And then convert it to a tibble, so that we can use familiar verbs on it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE76]'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE77]*  *Grab the page that is of interest (remembering that each page is
    an element of the character vector, hence a row in the tibble).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE79]*  *We want to separate the rows and use `separate_rows()` from `tidyr`,
    which is part of the core tidyverse.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE81]*  *We are searching for patterns that we can use. Let us look at the
    first ten lines of content (ignoring aspects such as headings and page numbers
    at the top of the page).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE83]*  *And now at just one line.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE85]*  *It does not get much better than this:'
  prefs: []
  type: TYPE_NORMAL
- en: We have dots separating the states from the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have a space between each of the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can now separate this into columns. First, we want to match on when there
    are at least two dots (remembering that the dot is a special character and so
    needs to be escaped).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE87]*  *We then separate the data based on spaces. There is an inconsistent
    number of spaces, so we first squish any example of more than one space into just
    one with `str_squish()` from `stringr`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE89]*  *This is all looking fairly great. The only thing left is to clean
    up.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '*And run some checks, for instance that we have all the states.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE92]*  *And we are done ([Table 7.4](#tbl-tfrforthewin)). We can see that
    there is quite a wide distribution of TFR by US state ([Figure 7.15](#fig-smalldhsexample)).
    Utah has the highest and Vermont the lowest.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 7.4: First ten rows of a dataset of TFR by United States state, 2000-2019'
  prefs: []
  type: TYPE_NORMAL
- en: '| State | TFR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 2,130 |'
  prefs: []
  type: TYPE_TB
- en: '| Alabama | 2,021 |'
  prefs: []
  type: TYPE_TB
- en: '| Alaska | 2,437 |'
  prefs: []
  type: TYPE_TB
- en: '| Arizona | 2,652 |'
  prefs: []
  type: TYPE_TB
- en: '| Arkansas | 2,140 |'
  prefs: []
  type: TYPE_TB
- en: '| California | 2,186 |'
  prefs: []
  type: TYPE_TB
- en: '| Colorado | 2,356 |'
  prefs: []
  type: TYPE_TB
- en: '| Connecticut | 1,932 |'
  prefs: []
  type: TYPE_TB
- en: '| Delaware | 2,014 |'
  prefs: []
  type: TYPE_TB
- en: '| District of Columbia | 1,976 |*  *[PRE94]'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](../Images/e43a07d60868afd17f856a21c14ea53c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.15: Distribution of TFR by US state in 2000*  *Healy ([2022](99-references.html#ref-kieransparsing))
    provides another example of using this approach in a different context.**************  ***###
    7.4.3 Optical Character Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: All of the above is predicated on having a PDF that is already “digitized”.
    But what if it is made of images, such as the result of a scan. Such PDFs often
    contain unstructured data, meaning that the data are not tagged nor organized
    in a regular way. Optical Character Recognition (OCR) is a process that transforms
    an image of text into actual text. Although there may not be much difference to
    a human reading a PDF before and after OCR, the PDF becomes machine-readable which
    allows us to use scripts ([Cheriet et al. 2007](99-references.html#ref-Cheriet2007)).
    OCR has been used to parse images of characters since the 1950s, initially using
    manual approaches. While manual approaches remain the gold standard, for reasons
    of cost effectiveness, this has been largely replaced with statistical models.
  prefs: []
  type: TYPE_NORMAL
- en: In this example we use `tesseract` to OCR a document. This is a `R` wrapper
    around the Tesseract open-source OCR engine. Tesseract was initially developed
    at HP in the 1980s, and is now mostly developed by Google. After we install and
    load `tesseract` we can use `ocr()`.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see an example with a scan from the first page of *Jane Eyre* ([Figure 7.16](#fig-janescan)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df597ee5e0c63d66038720a9fc9c9c9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Scan of first page of Jane Eyre'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE96]'
  prefs: []
  type: TYPE_NORMAL
- en: In general the result is not too bad. OCR is a useful tool but is not perfect
    and the resulting data may require extra attention in terms of cleaning. For instance,
    in the OCR results of [Figure 7.16](#fig-janescan) we see irregularities that
    would need to be fixed. Various options, such as focusing on the particular data
    of interest and increasing the contrast can help. Other popular OCR engines include
    Amazon Textract, Google Vision API, and ABBYY.*******  ***## 7.5 Exercises
  prefs: []
  type: TYPE_NORMAL
- en: Practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(Plan)* Consider the following scenario: *A group of five undergraduates—Matt,
    Ash, Jacki, Rol, and Mike—each read some number of pages from a book each day
    for 100 days. Two of the undergraduates are a couple and so their number of pages
    is positively correlated, however all the others are independent.* Please sketch
    what a dataset could look like, and then sketch a graph that you could build to
    show all observations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Simulate)* Please further consider the scenario described and simulate the
    situation (note the relationship between some variables). Then write five tests
    based on the simulated data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Acquire)* Please obtain some actual data, similar to the scenario, and add
    a script updating the simulated tests to these actual data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Explore)* Build a graph and table using the real data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Communicate)* Please write some text to accompany the graph and table. Separate
    the code appropriately into `R` files and a Quarto doc. Submit a link to a high-quality
    GitHub repo.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is an API, in the context of data gathering (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A standardized set of functions to process data locally.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A markup language for structuring data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A protocol for web browsers to render HTML content.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: An interface provided by a server that allows someone else to request data using
    code.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When using APIs for data gathering, which of the following can be used for authentication
    (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing an API key or token in the request.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using cookies stored in the browser.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Disabling SSL verification.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Modifying the hosts file on the client machine.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the following code, which uses `gh` to access the GitHub API. When
    was the repo for `heapsofpapers` created (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '2021-02-23'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '2021-03-06'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '2021-05-25'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '2021-04-27'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '*4.  Please consider the UN’s [Data API](https://population.un.org/dataportal/about/dataapi)
    and the introductory note on how to use it by Schmertmann ([2022](99-references.html#ref-schmertmannunapi)).
    Argentina’s location code is 32\. Modify the following code to determine what
    Argentina’s single-year fertility rate was for 20-year-olds in 1995 (pick one)?'
  prefs: []
  type: TYPE_NORMAL
- en: '147.679'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '172.988'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '204.124'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '128.665'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '*5.  What is the main argument to `GET()` from `httr` (pick one)?'
  prefs: []
  type: TYPE_NORMAL
- en: “url”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: “website”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: “domain”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: “location”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In web scraping, what is the purpose of respecting `robots.txt` (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To ensure the scraped data are accurate.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To avoid violating the website’s terms of service by following the site’s crawling
    guidelines.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To speed up the scraping process.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To obtain authentication credentials.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What features of a website do we typically take advantage of when we parse the
    code (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: HTML/CSS mark-up.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Cookies.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Facebook beacons.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Code comments.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some principles to follow when scraping (select all that apply)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid it if possible
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the site’s guidance
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Slow down
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a scalpel not an axe.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following is NOT a recommended principle when performing web scraping
    (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abide by the website’s terms of service.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the impact on the website’s server by slowing down requests.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Scrape all data regardless of necessity.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid republishing scraped pages.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Which of the following, used as part of a regular expression, would match a
    full stop (hint: see the “strings” cheat sheet) (pick one)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “.”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: “\.”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: “\\\.”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What are three checks that we might like to use for demographic data, such as
    the number of births in a country in a particular year?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of these are functions from the `purrr` package (select all that apply)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`map()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`walk()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`run()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`safely()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the HTML tag for an item in a list (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`li`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`body`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`b`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`em`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which function should we use if we have the text “rohan_alexander” in a column
    called “names” and want to split it into first name and surname based on the underscore
    (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spacing()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`slice()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`separate()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`text_to_columns()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Optical Character Recognition (OCR) (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A process of converting handwritten notes into typed text.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A method for translating images of text into machine-readable text.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A technique for parsing structured data from APIs.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A way to optimize code for faster execution.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which function in `R` can be used to pause execution for a specified amount
    of time, useful for respecting rate limits during web scraping (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sleep()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pause()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sys.sleep()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`wait()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following is a challenge when extracting data from PDFs (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PDFs cannot be read by any programming language.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: PDFs are designed for consistent human reading, not for data extraction.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: PDFs always contain unstructured data that cannot be processed.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: PDFs are encrypted and cannot be accessed without a password.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When performing OCR on a scanned document, what is a common issue that might
    affect the accuracy of text recognition (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The file size of the image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The programming language used.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The quality and resolution of the scanned image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of pages in the document.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From Cirone and Spirling ([2021](99-references.html#ref-cirone)), which of the
    following is NOT a common threat to inference when working with historical data
    (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selection bias.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Confirmation bias.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Time decay.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Over-representation of marginalized groups.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From Cirone and Spirling ([2021](99-references.html#ref-cirone)), what is the
    drunkard’s search problem in historical political economy (and more generally)
    (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting data that are easiest to access without considering representativeness.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Searching for data only from elite sources.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Over-relying on digital archives for research.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Misinterpreting historical texts due to modern biases.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From Cirone and Spirling ([2021](99-references.html#ref-cirone)), what role
    do DAGs have (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They improve the accuracy of OCR for historical data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They generate machine-readable text from historical sources.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They help researchers visualize and address causal relationships.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They serve as metadata to organize historical archives.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), what was the focus
    of early prison data collection by the U.S. Census Bureau (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Documenting health conditions.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Investigating racial differences in sentencing.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Recording socioeconomic background and employment.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Counting the number of incarcerated people and their demographics.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), how does community-sourced
    prison data differ from state-sourced prison data (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Community data are collected by government officials.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Community data emphasizes lived experiences and prison conditions.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: State data are less reliable than community data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: State data are more reliable than community data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), which of the following
    is a limitation of state-sourced data (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: State-sourced data are less reliable than academic studies.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It under-represents the prison population.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It may reproduce the biases and assumptions of earlier data collections.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It focuses on nonviolent offenders only.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From Johnson ([2021](99-references.html#ref-Johnson2021Two)), what question
    should be asked when looking at prison data collection (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “Who established the data infrastructure and why?”.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: “How do the economic factors affect prison management?”.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: “Is the data being used to create public policy?”.**  **### Class activities
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the [starter folder](https://github.com/RohanAlexander/starter_folder) and
    create a new repo. Obtain the NASA APOD for today using the API and then add it
    to the Quarto document in the repo.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the Spotify API to determine which Beyonce album has the highest average
    “danceability”?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please make a graph to answer the question of whether Camila Cabello’s departure
    from Fifth Harmony in December 2016 affected the valence of the songs in their
    studio albums.[¹](#fn1) Some helpful cleaning code is below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '**   Build an equivalent to [Figure 7.10](#fig-pmslives), but for Canada.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Paper review:* Please read Kish ([1959](99-references.html#ref-Kish1959))
    and write a review of at least one page, drawing on an example that you are familiar
    with.*  *### Task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please redo the web scraping example, but for one of: [Australia](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Australia),
    [Canada](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada), [India](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_India),
    or [New Zealand](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_New_Zealand).'
  prefs: []
  type: TYPE_NORMAL
- en: Plan, gather, and clean the data, and then use it to create a similar table
    to the one created above. Write a few paragraphs about your findings. Then write
    a few paragraphs about the data source, what you gathered, and how you went about
    it. What took longer than you expected? When did it become fun? What would you
    do differently next time you do this? Your submission should be at least two pages,
    but likely more.
  prefs: []
  type: TYPE_NORMAL
- en: Use Quarto, and include an appropriate title, author, date, link to a GitHub
    repo, and citations. Submit a PDF.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alexander, Rohan, and A Mahfouz. 2021\. *heapsofpapers: Easily Download Heaps
    of PDF and CSV Files*. [https://CRAN.R-project.org/package=heapsofpapers](https://CRAN.R-project.org/package=heapsofpapers).Arel-Bundock,
    Vincent. 2024\. *tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,”
    “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats*. [https://vincentarelbundock.github.io/tinytable/](https://vincentarelbundock.github.io/tinytable/).Bailey,
    Rosemary. 2008\. *Design of Comparative Experiments*. Cambridge: Cambridge University
    Press. [https://doi.org/10.1017/CBO9780511611483](https://doi.org/10.1017/CBO9780511611483).Bor,
    Jacob, Atheendar Venkataramani, David Williams, and Alexander Tsai. 2018\. “Police
    Killings and Their Spillover Effects on the Mental Health of Black Americans:
    A Population-Based, Quasi-Experimental Study.” *The Lancet* 392 (10144): 302–10\.
    [https://doi.org/10.1016/s0140-6736(18)31130-9](https://doi.org/10.1016/s0140-6736(18)31130-9).Brontë,
    Charlotte. 1847\. *Jane Eyre*. [https://www.gutenberg.org/files/1260/1260-h/1260-h.htm](https://www.gutenberg.org/files/1260/1260-h/1260-h.htm).Bryan,
    Jenny, and Hadley Wickham. 2021\. *gh: GitHub API*. [https://CRAN.R-project.org/package=gh](https://CRAN.R-project.org/package=gh).Cheriet,
    Mohamed, Nawwaf Kharma, Cheng-Lin Liu, and Ching Suen. 2007\. *Character Recognition
    Systems: A Guide for Students and Practitioner*. Wiley.Cirone, Alexandra, and
    Arthur Spirling. 2021\. “Turning History into Data: Data Collection, Measurement,
    and Inference in HPE.” *Journal of Historical Political Economy* 1 (1): 127–54\.
    [https://doi.org/10.1561/115.00000005](https://doi.org/10.1561/115.00000005).Collins,
    Annie, and Rohan Alexander. 2022\. “Reproducibility of COVID-19 Pre-Prints.” *Scientometrics*
    127: 4655–73\. [https://doi.org/10.1007/s11192-022-04418-2](https://doi.org/10.1007/s11192-022-04418-2).Comer,
    Benjamin P., and Jason R. Ingram. 2022\. “Comparing Fatal Encounters, Mapping
    Police Violence, and Washington Post Fatal Police Shooting Data from 2015-2019:
    A Research Note.” *Criminal Justice Review*, January, 073401682110710\. [https://doi.org/10.1177/07340168211071014](https://doi.org/10.1177/07340168211071014).Crawford,
    Kate. 2021\. *Atlas of AI*. 1st ed. New Haven: Yale University Press.Cummins,
    Neil. 2022\. “The Hidden Wealth of English Dynasties, 1892–2016.” *The Economic
    History Review* 75 (3): 667–702\. [https://doi.org/10.1111/ehr.13120](https://doi.org/10.1111/ehr.13120).Eisenstein,
    Michael. 2022\. “Need Web Data? Here’s How to Harvest Them.” *Nature* 607: 200–201\.
    [https://doi.org/10.1038/d41586-022-01830-9](https://doi.org/10.1038/d41586-022-01830-9).Firke,
    Sam. 2023\. *janitor: Simple Tools for Examining and Cleaning Dirty Data*. [https://CRAN.R-project.org/package=janitor](https://CRAN.R-project.org/package=janitor).Grolemund,
    Garrett, and Hadley Wickham. 2011\. “Dates and Times Made Easy with lubridate.”
    *Journal of Statistical Software* 40 (3): 1–25\. [https://doi.org/10.18637/jss.v040.i03](https://doi.org/10.18637/jss.v040.i03).Hackett,
    Robert. 2016\. “Researchers Caused an Uproar By Publishing Data From 70,000 OkCupid
    Users.” *Fortune*, May. [https://fortune.com/2016/05/18/okcupid-data-research/](https://fortune.com/2016/05/18/okcupid-data-research/).Healy,
    Kieran. 2022\. “Unhappy in Its Own Way,” July. [https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/](https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/).Jenkins,
    Jennifer, Steven Rich, Andrew Ba Tran, Paige Moody, Julie Tate, and Ted Mellnik.
    2022\. “How the Washington Post Examines Police Shootings in the United States.”
    [https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/](https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/).Johnson,
    Kaneesha. 2021\. “Two Regimes of Prison Data Collection.” *Harvard Data Science
    Review* 3 (3). [https://doi.org/10.1162/99608f92.72825001](https://doi.org/10.1162/99608f92.72825001).Kirkegaard,
    Emil, and Julius Bjerrekær. 2016\. “The OKCupid Dataset: A Very Large Public Dataset
    of Dating Site Users.” *Open Differential Psychology*, 1–10\. [https://doi.org/10.26775/ODP.2016.11.03](https://doi.org/10.26775/ODP.2016.11.03).Kish,
    Leslie. 1959\. “Some Statistical Problems in Research Design.” *American Sociological
    Review* 24 (3): 328–38\. [https://doi.org/10.2307/2089381](https://doi.org/10.2307/2089381).Luscombe,
    Alex, Kevin Dick, and Kevin Walby. 2021\. “Algorithmic Thinking in the Public
    Interest: Navigating Technical, Legal, and Ethical Hurdles to Web Scraping in
    the Social Sciences.” *Quality & Quantity* 56 (3): 1–22\. [https://doi.org/10.1007/s11135-021-01164-0](https://doi.org/10.1007/s11135-021-01164-0).Luscombe,
    Alex, Jamie Duncan, and Kevin Walby. 2022\. “Jumpstarting the Justice Disciplines:
    A Computational-Qualitative Approach to Collecting and Analyzing Text and Image
    Data in Criminology and Criminal Justice Studies.” *Journal of Criminal Justice
    Education* 33 (2): 151–71\. [https://doi.org/10.1080/10511253.2022.2027477](https://doi.org/10.1080/10511253.2022.2027477).Müller,
    Kirill. 2020\. *here: A Simpler Way to Find Your Files*. [https://CRAN.R-project.org/package=here](https://CRAN.R-project.org/package=here).Nix,
    Justin, and M. James Lozada. 2020\. “Police Killings of Unarmed Black Americans:
    A Reassessment of Community Mental Health Spillover Effects,” January. [https://doi.org/10.31235/osf.io/ajz2q](https://doi.org/10.31235/osf.io/ajz2q).Ooms,
    Jeroen. 2014\. “The jsonlite Package: A Practical and Consistent Mapping Between
    JSON Data and R Objects.” *arXiv:1403.2805 [Stat.CO]*. [https://arxiv.org/abs/1403.2805](https://arxiv.org/abs/1403.2805).———.
    2022a. *pdftools: Text Extraction, Rendering and Converting of PDF Documents*.
    [https://CRAN.R-project.org/package=pdftools](https://CRAN.R-project.org/package=pdftools).———.
    2022b. *tesseract: Open Source OCR Engine*. [https://CRAN.R-project.org/package=tesseract](https://CRAN.R-project.org/package=tesseract).Pavlik,
    Kaylin. 2019\. “Understanding + Classifying Genres Using Spotify Audio Features.”
    [https://www.kaylinpavlik.com/classifying-songs-genres/](https://www.kaylinpavlik.com/classifying-songs-genres/).Perepolkin,
    Dmytro. 2022\. *polite: Be Nice on the Web*. [https://CRAN.R-project.org/package=polite](https://CRAN.R-project.org/package=polite).R
    Core Team. 2024\. *R: A Language and Environment for Statistical Computing*. Vienna,
    Austria: R Foundation for Statistical Computing. [https://www.R-project.org/](https://www.R-project.org/).Salganik,
    Matthew, Peter Sheridan Dodds, and Duncan Watts. 2006\. “Experimental Study of
    Inequality and Unpredictability in an Artificial Cultural Market.” *Science* 311
    (5762): 854–56\. [https://doi.org/10.1126/science.1121066](https://doi.org/10.1126/science.1121066).Saulnier,
    Lucile, Siddharth Karamcheti, Hugo Laurençon, Léo Tronchon, Thomas Wang, Victor
    Sanh, Amanpreet Singh, et al. 2022\. “Putting Ethical Principles at the Core of
    the Research Lifecycle.” [https://huggingface.co/blog/ethical-charter-multimodal](https://huggingface.co/blog/ethical-charter-multimodal).Schmertmann,
    Carl. 2022\. “UN API Test,” July. [https://bonecave.schmert.net/un-api-example.html](https://bonecave.schmert.net/un-api-example.html).Taflaga,
    Marija, and Matthew Kerby. 2019\. “Who Does What Work in a Ministerial Office:
    Politically Appointed Staff and the Descriptive Representation of Women in Australian
    Political Offices, 19792010.” *Political Studies* 68 (2): 463–85\. [https://doi.org/10.1177/0032321719853459](https://doi.org/10.1177/0032321719853459).The
    Economist. 2022\. “What Spotify Data Show about the Decline of English,” January.
    [https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english](https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english).The
    Washington Post. 2023\. “Fatal Force Database.” [https://github.com/washingtonpost/data-police-shootings](https://github.com/washingtonpost/data-police-shootings).Thompson,
    Charlie, Daniel Antal, Josiah Parry, Donal Phipps, and Tom Wolff. 2022\. *spotifyr:
    R Wrapper for the “Spotify” Web API*. [https://CRAN.R-project.org/package=spotifyr](https://CRAN.R-project.org/package=spotifyr).Thomson-DeVeaux,
    Amelia, Laura Bronner, and Damini Sharma. 2021\. “Cities Spend Millions On Police
    Misconduct Every Year. Here’s Why It’s So Difficult to Hold Departments Accountable.”
    *FiveThirtyEight*, February. [https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/](https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/).Wickham,
    Hadley. 2021\. *babynames: US Baby Names 1880-2017*. [https://CRAN.R-project.org/package=babynames](https://CRAN.R-project.org/package=babynames).———.
    2022\. *rvest: Easily Harvest (Scrape) Web Pages*. [https://CRAN.R-project.org/package=rvest](https://CRAN.R-project.org/package=rvest).———.
    2023\. *httr: Tools for Working with URLs and HTTP*. [https://CRAN.R-project.org/package=httr](https://CRAN.R-project.org/package=httr).Wickham,
    Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain
    François, Garrett Grolemund, et al. 2019\. “Welcome to the Tidyverse.” *Journal
    of Open Source Software* 4 (43): 1686\. [https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686).Wickham,
    Hadley, Jennifer Bryan, and Malcolm Barrett. 2022\. *usethis: Automate Package
    and Project Setup*. [https://CRAN.R-project.org/package=usethis](https://CRAN.R-project.org/package=usethis).Wickham,
    Hadley, and Lionel Henry. 2022\. *purrr: Functional Programming Tools*. [https://CRAN.R-project.org/package=purrr](https://CRAN.R-project.org/package=purrr).Wickham,
    Hadley, Jim Hester, and Jeroen Ooms. 2021\. *xml2: Parse XML*. [https://CRAN.R-project.org/package=xml2](https://CRAN.R-project.org/package=xml2).Wong,
    Julia Carrie. 2020\. “One Year Inside Trump’s Monumental Facebook Campaign.” *The
    Guardian*, January. [https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election](https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election).Zimmer,
    Michael. 2018\. “Addressing Conceptual Gaps in Big Data Research Ethics: An Application
    of Contextual Integrity.” *Social Media + Society* 4 (2): 1–11\. [https://doi.org/10.1177/2056305118768300](https://doi.org/10.1177/2056305118768300).***
    **** * *'
  prefs: []
  type: TYPE_NORMAL
- en: Students who finish quickly should similarly look at Jessica’s departure from
    Girls’ Generation in September 2014, and then attempt to compare the two situations.[↩︎](#fnref1)***************
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
