- en: Chapter 5
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章
- en: Grids, Blocks, and Threads
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格、块和线程
- en: What it all Means
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这意味着什么
- en: NVIDIA chose a rather interesting model for its scheduling, a variant of SIMD
    it calls SPMD (single program, multiple data). This is based on the underlying
    hardware implementation in many respects. At the heart of parallel programming
    is the idea of a thread, a single flow of execution through the program in the
    same way a piece of cotton flows through a garment. In the same way threads of
    cotton are woven into cloth, threads used together make up a parallel program.
    The CUDA programming model groups threads into special groups it calls warps,
    blocks, and grids, which we will look at in turn.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 为其调度选择了一种相当有趣的模型，它称之为 SPMD（单程序，多数据），这是 SIMD 的一种变体。这在很多方面都基于底层硬件的实现。并行编程的核心思想是线程，即程序中执行流的单一路径，就像一根棉线穿过一件衣物一样。就像棉线被编织成布一样，多个线程一起构成了一个并行程序。CUDA
    编程模型将线程分组为它所称的 warp、块和网格，我们将逐一探讨。
- en: Threads
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程
- en: A thread is the fundamental building block of a parallel program. Most C programmers
    are familiar with the concept if they have done any multicore programming. Even
    if you have never launched a thread in any code, you will be familiar with executing
    at least one thread, the single thread of execution through any serial piece of
    code.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 线程是并行程序的基本构建块。大多数 C 程序员如果做过多核编程，应该对这个概念很熟悉。即使你从未在任何代码中启动过线程，你也应该熟悉至少执行过一个线程，即通过任何串行代码执行的单线程。
- en: With the advent of dual, quad, hex core processors, and beyond, more emphasis
    is explicitly placed on the programmer to make use of such hardware. Most programs
    written in the past few decades, with the exception of perhaps the past decade,
    were single-thread programs because the primary hardware on which they would execute
    was a single-core CPU. Sure, you had clusters and supercomputers that sought to
    exploit a high level of parallelism by duplicating the hardware and having thousands
    of commodity servers instead of a handful of massively powerful macines. However,
    these were mostly restricted to universities and large institutions, not generally
    available to the masses.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着双核、四核、六核处理器的出现，程序员在利用这些硬件方面的责任显著增加。过去几十年写的大多数程序，除了也许过去十年间的程序，都是单线程程序，因为它们执行的主要硬件是单核
    CPU。当然，你有集群和超级计算机，它们通过复制硬件并拥有成千上万的普通服务器来试图利用高并行度，而不是拥有少数几台极其强大的机器。然而，这些大多只限于大学和大型机构，而非普及到大众。
- en: Thinking in terms of lots of threads is hard. It’s much easier to think in terms
    of one task at a time. Serial programming languages like C/C++ were born from
    a time when serial processing speed doubled every few years. There was little
    need to do the hard parallel programming. That stopped almost a decade ago, and
    now, like it or not, to improve program speed requires us to think in terms of
    parallel design.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以大量线程为思维方式很困难。一次处理一个任务要容易得多。像 C/C++ 这样的串行编程语言诞生于串行处理速度每几年翻倍的时代。当时几乎没有必要做困难的并行编程。十年前这一趋势停止了，现在，无论你是否愿意，要提高程序速度，我们必须从并行设计的角度进行思考。
- en: Problem decomposition
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题分解
- en: Parallelism in the CPU domain tends to be driven by the desire to run more than
    one (single-threaded) program on a single CPU. This is the task-level parallelism
    that we covered earlier. Programs, which are data intensive, like video encoding,
    for example, use the data parallelism model and split the task in *N* parts where
    *N* is the number of CPU cores available. You might, for example, have each CPU
    core calculate one “frame” of data where there are no interdependencies between
    frames. You may also choose to split each frame into *N* segments and allocate
    each one of the segments to an individual core.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 领域，并行性通常是为了在单个 CPU 上运行多个（单线程）程序。这就是我们之前讲解的任务级并行性。例如，像视频编码这样的数据密集型程序使用数据并行模型，将任务拆分成
    *N* 部分，其中 *N* 是可用的 CPU 核心数。例如，你可以让每个 CPU 核心计算一个“帧”的数据，而各个帧之间没有相互依赖。你也可以选择将每一帧拆分成
    *N* 个段，并将每个段分配给一个独立的核心。
- en: In the GPU domain, you see exactly these choices when attempting to speed up
    rendering of 3D worlds in computer games by using more than one GPU. You can send
    complete, alternate frames to each GPU ([Figure 5.1](#F0010)). Alternatively,
    you can ask one GPU to render the different parts of the screen.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU领域，当尝试通过使用多个GPU加速计算机游戏中3D世界的渲染时，您会看到这些选择。您可以将完整的交替帧发送到每个GPU（[图5.1](#F0010)）。或者，您可以让一个GPU渲染屏幕的不同部分。
- en: '![image](../images/F000053f05-01-9780124159334.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-01-9780124159334.jpg)'
- en: Figure 5.1 Alternate frame rendering (AFR) vs. Split Frame Rendering (SFR).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 交替帧渲染（AFR）与分帧渲染（SFR）。
- en: However, there is a trade off here. If the dataset is self-contained, you can
    use less memory and transfer less data by only providing the GPU (or CPU) with
    the subset of the data you need to calculate. In the SFR GPU example used here,
    there may be no need for GPU3, which is rendering the floor to know the content
    of data from GPU0, which is probably rendering the sky. However, there may be
    shadows from a flying object, or the lighting level of the floor may need to vary
    based on the time of day. In such instances, it might be more beneficial to go
    with the alternate frame rendering approach because of this shared data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个权衡。如果数据集是自包含的，您可以通过仅向GPU（或CPU）提供需要计算的数据子集，来使用更少的内存并传输更少的数据。在这里使用的SFR
    GPU示例中，GPU3负责渲染地面，可能不需要知道GPU0的数据内容，而GPU0可能正在渲染天空。然而，可能会有飞行物体的阴影，或者地面的光照强度可能需要根据一天中的时间变化。在这种情况下，由于共享数据，选择交替帧渲染方法可能会更有利。
- en: We refer to SFR type splits as coarse-grained parallelism. Large chunks of data
    are split in some way between *N* powerful devices and then reconstructed later
    as the processed data. When designing applications for a parallel environment,
    choices at this level seriously impact the performance of your programs. The best
    choice here is very much linked to the actual hardware you will be using, as you
    will see with the various applications we develop throughout this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将SFR类型的拆分称为粗粒度并行性。大块数据以某种方式在*N*个强大设备之间拆分，然后作为处理后的数据重新构建。在为并行环境设计应用程序时，这一层次的选择会严重影响程序的性能。这里的最佳选择与您将使用的实际硬件紧密相关，正如我们在本书中开发的各种应用程序所展示的那样。
- en: With a small number of powerful devices, such as in CPUs, the issue is often
    how to split the workload evenly. This is often easier to reason with because
    you are typically talking about only a small number of devices. With huge numbers
    of smaller devices, as with GPUs, they average out peaks in workload much better,
    but suffer from issues around synchronization and coordination.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于少数几个强大的设备，例如CPU，问题通常是如何平均分配工作负载。这通常更容易推理，因为您通常只涉及少量设备。对于大量小型设备（如GPU），它们能更好地平衡工作负载的峰值，但会遇到同步和协调方面的问题。
- en: In the same way as you have macro (large-scale) and micro (small-scale) economics,
    you have coarse and fine-grained parallelism. However, you only really find fine-grained
    parallelism at the programmer level on devices that support huge numbers of threads,
    such as GPUs. CPUs, by contrast, also support threads, but with a large overhead
    and thus are considered to be useful for more coarse-grained parallelism problems.
    CPUs, unlike GPUs, follow the MIMD (Multiple Instruction Multiple Data) model
    in that they support multiple independent instruction streams. This is a more
    flexible approach, but incurs additional overhead in terms of fetching multiple
    independent instruction streams as opposed to amortizing the single instruction
    stream over multiple processors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 就像宏观（大规模）和微观（小规模）经济学一样，您也有粗粒度和细粒度并行性。然而，您通常只有在支持大量线程的设备（如GPU）上，才能在程序员层面找到细粒度并行性。相比之下，CPU也支持线程，但由于开销较大，因此被认为更适合解决粗粒度并行性问题。与GPU不同，CPU遵循MIMD（多指令多数据）模型，支持多个独立的指令流。这是一种更灵活的方法，但在获取多个独立的指令流时会产生额外的开销，而不是将单个指令流分摊到多个处理器上。
- en: To put this in context, let’s consider a digital photo where you apply an image
    correction function to increase the brightness. On a GPU you might choose to assign
    one thread for every pixel in the image. On a quad-core CPU, you would likely
    assign one-quarter of the image to each CPU core.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个问题更具上下文，我们来考虑一张数字照片，您应用图像修正功能以增加亮度。在GPU上，您可能选择为每个像素分配一个线程。在四核CPU上，您可能会将图像的四分之一分配给每个CPU核心。
- en: How CPUs and GPUs are different
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU 和 GPU 的不同之处
- en: GPUs and CPUs are architecturally very different devices. CPUs are designed
    for running a small number of potentially quite complex tasks. GPUs are designed
    for running a large number of quite simple tasks. The CPU design is aimed at systems
    that execute a number of discrete and unconnected tasks. The GPU design is aimed
    at problems that can be broken down into thousands of tiny fragments and worked
    on individually. Thus, CPUs are very suitable for running operating systems and
    application software where there are a vast variety of tasks a computer may be
    performing at any given time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 和 CPU 在架构上是非常不同的设备。CPU 设计用于运行少量的、可能非常复杂的任务。GPU 设计用于运行大量的、相对简单的任务。CPU 设计针对的是执行若干个离散且互不相关的任务的系统。而
    GPU 设计则是针对那些可以被分解成数千个小片段并单独处理的问题。因此，CPU 非常适合运行操作系统和应用软件，因为计算机在任何给定时刻可能需要执行多种多样的任务。
- en: CPUs and GPUs consequently support threads in very different ways. The CPU has
    a small number of registers per core that must be used to execute any given task.
    To achieve this, they rapidly context switch between tasks. Context switching
    on CPUs is expensive in terms of time, in that the entire register set must be
    saved to RAM and the next one restored from RAM. GPUs, by comparison, also use
    the same concept of context switching, but instead of having a single set of registers,
    they have multiple banks of registers. Consequently, a context switch simply involves
    setting a bank selector to switch in and out the current set of registers, which
    is several orders of magnitude faster than having to save to RAM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CPU 和 GPU 在支持线程的方式上有很大的不同。每个 CPU 核心有少量寄存器，必须用来执行给定的任务。为了实现这一点，CPU 会快速进行任务之间的上下文切换。CPU
    上下文切换的时间成本较高，因为整个寄存器集必须保存到 RAM 中，并且下一个寄存器集必须从 RAM 恢复。相比之下，GPU 也使用上下文切换的概念，但它们不仅仅拥有一组寄存器，而是有多个寄存器组。因此，上下文切换仅涉及设置一个寄存器组选择器来切换当前的寄存器集，这比将数据保存到
    RAM 中快几个数量级。
- en: Both CPUs and GPUs must deal with stall conditions. These are generally caused
    by I/O operations and memory fetches. The CPU does this by context switching.
    Providing there are enough tasks and the runtime of a thread is not too small,
    this works reasonably well. If there are not enough processes to keep the CPU
    busy, it will idle. If there are too many small tasks, each blocking after a short
    period, the CPU will spend most of its time context switching and very little
    time doing useful work. CPU scheduling policies are often based on time slicing,
    dividing the time equally among the threads. As the number of threads increases,
    the percentage of time spent context switching becomes increasingly large and
    the efficiency starts to rapidly drop off.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 和 GPU 都必须处理停顿情况。这些通常是由 I/O 操作和内存提取引起的。CPU 通过上下文切换来处理这一问题。只要任务足够多，且线程的运行时间不太短，这种方式通常效果不错。如果没有足够的进程来保持
    CPU 忙碌，CPU 就会空闲。如果任务过多且每个任务在短时间内阻塞，CPU 大部分时间都会用于上下文切换，而很少有时间进行有效的工作。CPU 调度策略通常基于时间分片，将时间均匀分配给各个线程。随着线程数的增加，花费在上下文切换上的时间比例也会越来越大，效率会迅速下降。
- en: GPUs are designed to handle stall conditions and expect this to happen with
    high frequency. The GPU model is a data-parallel one and thus it needs thousands
    of threads to work efficiently. It uses this pool of available work to ensure
    it always has something useful to work on. Thus, when it hits a memory fetch operation
    or has to wait on the result of a calculation, the streaming processors simply
    switch to another instruction stream and return to the stalled instruction stream
    sometime later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 被设计用来处理停顿情况，并且预期这种情况会频繁发生。GPU 模型是数据并行模型，因此它需要数千个线程才能高效运行。它使用这池可用的工作来确保总是有有用的任务可以执行。因此，当它遇到内存提取操作或必须等待计算结果时，流处理器只需切换到另一个指令流，稍后再返回到停顿的指令流。
- en: One of the major differences between CPUs and GPUs is the sheer number of processors
    on each device. CPUs are typically dual- or quad-core devices. That is to say
    they have a number of execution cores available to run programs on. The current
    Fermi GPUs have 16 SMs, which can be thought of a lot like CPU cores. CPUs often
    run single-thread programs, meaning they calculate just a single data point per
    core, per iteration. GPUs run in parallel by default. Thus, instead of calculating
    just a single data point per SM, GPUs calculate 32 per SM. This gives a 4 times
    advantage in terms of number of cores (SMs) over a typical quad core CPU, but
    also a 32 times advantage in terms of data throughput. Of course, CPU programs
    can also use all the available cores and extensions like MMX, SSE, and AVX. The
    question is how many CPU applications actually use these types of extensions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和GPU之间的主要区别之一是每个设备上的处理器数量。CPU通常是双核或四核设备。也就是说，它们有多个执行核心来运行程序。目前的Fermi GPU拥有16个SM，这可以看作是类似CPU核心的存在。CPU通常运行单线程程序，这意味着它们每个核心每次迭代只计算一个数据点。GPU默认并行运行。因此，GPU每个SM计算的不是一个数据点，而是32个数据点。这使得在核心数量（SM数量）上，GPU比典型的四核CPU有4倍的优势，在数据吞吐量上有32倍的优势。当然，CPU程序也可以使用所有可用的核心以及像MMX、SSE和AVX这样的扩展。问题是，究竟有多少CPU应用程序实际使用了这些类型的扩展。
- en: GPUs also provide something quite unique—high-speed memory next to the SM, so-called
    shared memory. In many respects this implements the design philosophy of the Connection
    Machine and the Cell processor, in that it provides local workspace for the device
    outside of the standard register file. Thus, the programmer can leave data in
    this memory, safe in the knowledge the hardware will not evict it behind his or
    her back. It is also the primary mechanism communication between threads.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPU还提供了一些独特的功能——高速内存位于SM旁边，即所谓的共享内存。在许多方面，这实现了连接机器和Cell处理器的设计理念，因为它为设备提供了一个标准寄存器文件之外的本地工作区。因此，程序员可以将数据保存在这块内存中，放心地知道硬件不会在背后将其驱逐出去。它也是线程间通信的主要机制。
- en: Task execution model
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务执行模型
- en: There are two major differences in the task execution model. The first is that
    groups of *N* SPs execute in a lock-step basis ([Figure 5.3](#F0020)), running
    the *same* program but on different data. The second is that, because of this
    huge register file, switching threads has effectively *zero* overhead. Thus, the
    GPU can support a very large number of threads and is designed in this way.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 任务执行模型有两个主要区别。第一个是*N*个SP组以锁步方式执行（[图5.3](#F0020)），运行相同的程序但处理不同的数据。第二个是，由于这个巨大的寄存器文件，线程切换的开销几乎为*零*。因此，GPU可以支持大量线程，并且是以这种方式设计的。
- en: '![image](../images/F000053f05-02-9780124159334.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-02-9780124159334.jpg)'
- en: Figure 5.2 Coarse-grained parallelism.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 粗粒度并行。
- en: '![image](../images/F000053f05-03-9780124159334.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-03-9780124159334.jpg)'
- en: Figure 5.3 Lock-step instruction dispatch.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 锁步指令调度。
- en: Now what exactly do we mean by lock-step basis? Each instruction in the instruction
    queue is dispatched to every SP within an SM. Remember each SM can be thought
    of as single processor with *N* cores (SPs) embedded within it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，锁步方式究竟是什么意思呢？指令队列中的每条指令都会被分发到SM内的每个SP。记住，每个SM可以看作是一个单独的处理器，其中嵌入了*N*个核心（SP）。
- en: A conventional CPU will fetch a separate instruction stream for each CPU core.
    The GPU SPMD model used here allows an instruction fetch for *N* logical execution
    units, meaning you have 1/*N* the instructions memory bandwidth requirements of
    a conventional processor. This is a very similar approach to the vector or SIMD
    processors found in many high-end supercomputers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的CPU会为每个CPU核心获取一个单独的指令流。这里使用的GPU SPMD模型允许为*N*个逻辑执行单元获取指令流，这意味着你拥有的是传统处理器的1/*N*的指令内存带宽需求。这与许多高端超级计算机中找到的矢量或SIMD处理器采用的方法非常相似。
- en: However, this is not without its costs. As you will see later, if the program
    does not follow a nice neat execution flow where all *N* threads follow the same
    control path, for each branch, you will require additional execution cycles.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并非没有代价。正如你稍后将看到的那样，如果程序没有遵循一个整洁的执行流，其中所有*N*线程遵循相同的控制路径，那么对于每个分支，你将需要额外的执行周期。
- en: Threading on GPUs
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU上的线程调度
- en: So coming back to threads, let’s look at a section of code and see what this
    means from a programming perspective.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回到线程，来看一段代码，看看从编程角度来看这意味着什么。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This piece of code is very simple. It stores the result of a multiplication
    of `b` and `c` value for a given index in the result variable `a` for that same
    index. The `for` loop iterates 128 times (indexes 0 to 127). In CUDA you could
    translate this to 128 threads, each of which executes the line
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常简单。它将给定索引处`b`和`c`值的乘积结果存储到同一索引位置的结果变量`a`中。`for`循环迭代128次（索引从0到127）。在CUDA中，你可以将其转换为128个线程，每个线程执行以下代码行。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is possible because there is no dependency between one iteration of the
    loop and the next. Thus, to transform this into a parallel program is actually
    quite easy. This is called loop parallelization and is very much the basis for
    one of the more popular parallel language extensions, OpenMP.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可能的，因为循环的每次迭代之间没有依赖关系。因此，将其转换为并行程序实际上非常简单。这就是所谓的循环并行化，也是许多流行并行语言扩展（如OpenMP）的基础。
- en: On a quad-core CPU you could also translate this to four blocks, where CPU core
    1 handles indexes 0–31, core 2 indexes 32–63, core 3 indexes 64–95, and core 4
    indexes 96–127\. Some compilers will either automatically translate such blocks
    or translate them where the programmer marks that this loop can be parallelized.
    The Intel compiler is particularly good at this. Such compilers can be used to
    create embedded SSE instructions to vectorize a loop in this way, in addition
    to spawning multiple threads. This gives two levels of parallelism and is not
    too different from the GPU model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在四核CPU上，你也可以将其转换为四个块，其中CPU核心1处理索引0–31，核心2处理索引32–63，核心3处理索引64–95，核心4处理索引96–127。某些编译器会自动转换这些块，或者在程序员标记该循环可以并行化时进行转换。Intel编译器在这方面尤其出色。此类编译器可以用于创建嵌入式SSE指令，以此方式向量化循环，并生成多个线程。这提供了两级并行性，与GPU模型没有太大不同。
- en: In CUDA, you translate this loop by creating a kernel function, which is a function
    that executes on the GPU *only* and cannot be executed directly on the CPU. In
    the CUDA programming model the CPU handles the serial code execution, which is
    where it excels. When you come to a computationally intense section of code the
    CPU hands it over to the GPU to make use of the huge computational power it has.
    Some of you might remember the days when CPUs would use a floating-point coprocessor.
    Applications that used a large amount of floating-point math ran many times faster
    on machines fitted with such coprocessors. Exactly the same is true for GPUs.
    They are used to accelerate computationally intensive sections of a program.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，你通过创建一个内核函数来翻译这个循环。内核函数是在GPU上执行的*唯一*函数，不能直接在CPU上执行。在CUDA编程模型中，CPU处理串行代码的执行，这是它擅长的地方。当你遇到计算密集型的代码段时，CPU会将其交给GPU，以利用其强大的计算能力。有些人可能还记得早期CPU使用浮点协处理器的时代。使用大量浮点运算的应用程序在配备了此类协处理器的机器上运行得更快。GPU的情况也是如此。它们被用来加速程序中的计算密集型部分。
- en: 'The GPU kernel function, conceptually, looks identical to the loop body, but
    with the loop structure removed. Thus, you have the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，GPU内核函数看起来与循环体相同，但移除了循环结构。因此，你得到如下代码：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice you have lost the loop and the loop control variable, `i`. You also have
    a `__global__` prefix added to the C function that tells the compiler to generate
    GPU code and not CPU code when compiling this function, and to make that GPU code
    globally visible from within the CPU.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你已经丢失了循环和循环控制变量`i`。你还会在C函数前面加上`__global__`前缀，告诉编译器在编译此函数时生成GPU代码而非CPU代码，并使得该GPU代码在CPU中全局可见。
- en: The CPU and GPU have separate memory spaces, meaning you cannot access CPU parameters
    in the GPU code and vice versa. There are some special ways of doing exactly this,
    which we’ll cover later in the book, but for now we will deal with them as separate
    memory spaces. As a consequence, the global arrays `a`, `b`, and `c` at the CPU
    level are no longer visible on the GPU level. You have to declare memory space
    on the GPU, copy over the arrays from the CPU, and pass the kernel function pointers
    to the GPU memory space to both read and write from. When you are done, you copy
    that memory back into the CPU. We’ll look at this a little later.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和GPU有各自独立的内存空间，这意味着你不能在GPU代码中访问CPU参数，反之亦然。虽然有一些特殊的方式可以做到这一点，我们将在书中的后面部分讲解，但现在我们将它们当作独立的内存空间来处理。因此，CPU级别的全局数组`a`、`b`和`c`在GPU级别上不再可见。你必须在GPU上声明内存空间，将数组从CPU复制到GPU，并将内核函数指针传递给GPU内存空间以进行读写。完成后，你需要将内存数据复制回CPU。稍后我们将进一步讨论这个问题。
- en: The next problem you have is that `i` is no longer defined; instead, the value
    of `i` is defined for you by the thread you are currently running. You will be
    launching 128 instances of this function, and initially this will be in the form
    of 128 threads. CUDA provides a special parameter, different for each thread,
    which defines the thread ID or number. You can use this to directly index into
    the array. This is very similar to MPI, where you get the process rank for each
    process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你面临的问题是，`i`不再被定义；相反，`i`的值是由你当前运行的线程定义的。你将启动128个该函数的实例，最初这些实例将以128个线程的形式启动。CUDA提供了一个特殊的参数，每个线程不同，定义了线程的ID或编号。你可以使用它来直接索引数组。这与MPI非常相似，在MPI中，你可以获得每个进程的进程等级。
- en: 'The thread information is provided in a structure. As it’s a structure element,
    we will store it in a variable, `thread_idx` for now to avoid having to reference
    the structure every time. Thus, the code becomes:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 线程信息以结构的形式提供。由于它是一个结构元素，我们暂时将其存储在变量`thread_idx`中，以避免每次都引用结构。因此，代码变成了：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note, some people prefer `idx` or `tid` as the name for the thread index since
    these are somewhat shorter to type.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有些人更喜欢使用`idx`或`tid`作为线程索引的名称，因为这些名称比较简短，打字方便。
- en: What is happening, now, is that for thread 0, the `thread_idx` calculation returns
    0\. For thread 1, it returns 1, and so on, up to thread 127, which uses index
    127\. Each thread does exactly two reads from memory, one multiply and one store
    operation, and then terminates. Notice how the code executed by each thread is
    identical, but the data changes. This is at the heart of the CUDA and SPMD model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在发生的事情是，对于线程0，`thread_idx`计算返回0，对于线程1，返回1，依此类推，直到线程127，使用索引127。每个线程都会执行两个内存读取操作，一个乘法和一个存储操作，然后终止。请注意，每个线程执行的代码是相同的，但数据有所不同。这正是CUDA和SPMD模型的核心。
- en: In OpenMP and MPI, you have similar blocks of code. They extract, for a given
    iteration of the loop, the thread ID or thread rank allocated to that thread.
    This is then used to index into the dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenMP和MPI中，你有类似的代码块。它们为循环的每次迭代提取分配给该线程的线程ID或线程等级。然后使用这些ID来索引数据集。
- en: A peek at hardware
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件一窥
- en: Now remember you only actually have *N* cores on each SM, so how can you run
    128 threads? Well, like the CPU, each thread group is placed into the SM and the
    *N* SPs start running the code. The first thing you do after extracting the thread
    index is fetch a parameter from the `b` and `c` array. Unfortunately, this doesn’t
    happen immediately. In fact, some 400–600 GPU clocks can go by before the memory
    subsystem comes back with the requested data. During this time the set of *N*
    threads gets suspended.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在记住，你每个SM上实际上只有*N*个核心，那么如何能运行128个线程呢？就像CPU一样，每个线程组被放置到SM中，然后*N*个SP开始运行代码。在提取线程索引后，首先要做的事情是从`b`和`c`数组中获取参数。不幸的是，这并不会立即发生。实际上，在内存子系统返回请求的数据之前，可能会经过400到600个GPU时钟周期。在这段时间里，*N*个线程会被挂起。
- en: Threads are, in practice, actually grouped into 32 thread groups, and when all
    32 threads are waiting on something such as memory access, they are suspended.
    The technical term for these groups of threads is a warp (32 threads) and a half
    warp (16 threads), something we’ll return to later.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 线程实际上是被分组为32个线程组的，当所有32个线程都在等待某些操作（例如内存访问）时，它们会被挂起。这些线程组的技术术语是warp（32个线程）和半warp（16个线程），我们稍后会再讨论这个问题。
- en: Thus, the 128 threads translate into four groups of 32 threads. The first set
    all run together to extract the thread ID and then calculate the address in the
    arrays and issue a memory fetch request (see [Figure 5.4](#F0025)). The next instruction,
    a multiply, requires both operands to have been provided, so the thread is suspended.
    When all 32 threads in that block of 32 threads are suspended, the hardware switches
    to another warp.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，128个线程被划分为四组，每组32个线程。第一组线程一起运行，用来提取线程ID，然后计算数组中的地址并发出内存访问请求（见[图5.4](#F0025)）。接下来的指令是乘法运算，需要两个操作数都已经准备好，因此该线程被挂起。当该组32个线程中的所有线程都被挂起时，硬件会切换到另一个warp。
- en: '![image](../images/F000053f05-04-9780124159334.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-04-9780124159334.jpg)'
- en: Figure 5.4 Cycle 0.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 周期0。
- en: In [Figure 5.5](#F0030), you can see that when warp 0 is suspended pending its
    memory access completing, warp 1 becomes the executing warp. The GPU continues
    in this manner until all warps have moved to the suspended state (see [Figure
    5.6](#F0035)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5.5](#F0030)中可以看到，当warp 0由于等待内存访问完成而被挂起时，warp 1成为执行中的warp。GPU会继续以这种方式工作，直到所有warp都进入挂起状态（见[图5.6](#F0035)）。
- en: '![image](../images/F000053f05-05-9780124159334.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-05-9780124159334.jpg)'
- en: Figure 5.5 Cycle 1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 周期 1。
- en: '![image](../images/F000053f05-06-9780124159334.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-06-9780124159334.jpg)'
- en: Figure 5.6 Cycle 8.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 周期 8。
- en: Prior to issuing the memory fetch, fetches from consecutive threads are usually
    coalesced or grouped together. This reduces the overall latency (time to respond
    to the request), as there is an overhead associated in the hardware with managing
    each request. As a result of the coalescing, the memory fetch returns with the
    data for a whole group of threads, usually enough to enable an entire warp.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在发出内存获取请求之前，来自连续线程的获取通常会被合并或分组在一起。这减少了整体的延迟（响应请求的时间），因为在硬件上管理每个请求会有一定的开销。通过合并，内存获取请求会返回包含一组线程数据，通常足以支持一个完整的warp。
- en: These threads are then placed in the ready state and become available for the
    GPU to switch in the next time it hits a blocking operation, such as another memory
    fetch from another set of threads.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些线程随后会进入就绪状态，并在GPU遇到下一个阻塞操作时（如来自另一组线程的内存获取）可以切换进来。
- en: Having executed all the warps (groups of 32 threads) the GPU becomes idle waiting
    for any one of the pending memory accesses to complete. At some point later, you’ll
    get a sequence of memory blocks being returned from the memory subsystem. It is
    likely, but not guaranteed, that these will come back in the order in which they
    were requested.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完所有warps（32线程的组）后，GPU变为空闲状态，等待任何一个待处理的内存访问完成。稍后，你会得到一系列内存块从内存子系统返回。这些内存块很可能会按请求的顺序返回，但这并不是保证的。
- en: Let’s assume that addresses 0–31 were returned at the same time. Warp 0 moves
    to the ready queue, and since there is no warp currently executing, warp 0 automatically
    moves to the executing state (see [Figure 5.7](#F0040)). Gradually all the pending
    memory requests will complete, resulting in all of the warp blocks moving back
    to the ready queue.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 假设地址 0–31 是同时返回的。warp 0 移动到就绪队列，由于当前没有正在执行的warp，warp 0 会自动进入执行状态（见[图 5.7](#F0040)）。逐渐地，所有待处理的内存请求会完成，导致所有warp块都返回到就绪队列。
- en: '![image](../images/F000053f05-07-9780124159334.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-07-9780124159334.jpg)'
- en: Figure 5.7 Cycle 9.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 周期 9。
- en: Once warp 0 has executed, its final instruction is a write to the destination
    array `a`. As there are no dependent instructions on this operation, warp 0 is
    then complete and is retired. The other warps move through this same cycle and
    eventually they have all issued a store request. Each warp is then retired, and
    the kernel completes, returning control to the CPU.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦warp 0执行完毕，它的最后一条指令是写入目标数组`a`。由于该操作没有依赖的指令，warp 0就完成并被注销。其他warps会进入同样的周期，最终它们都会发出存储请求。每个warp随后都被注销，内核完成，控制权返回到CPU。
- en: CUDA kernels
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CUDA内核
- en: 'Now let’s look a little more at how exactly you invoke a kernel. CUDA defines
    an extension to the C language used to invoke a kernel. Remember, a kernel is
    just a name for a function that executes on the GPU. To invoke a kernel you use
    the following syntax:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地看看如何调用内核。CUDA定义了一种扩展的C语言语法来调用内核。记住，内核只是一个在GPU上执行的函数的名称。要调用一个内核，你需要使用以下语法：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There are some other parameters you can pass, and we’ll come back to this,
    but for now you have two important parameters to look at: `num_blocks` and `num_threads`.
    These can be either variables or literal values. I’d recommend the use of variables
    because you’ll use them later when tuning performance.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以传递一些其他参数，我们稍后会讨论，但现在你需要关注两个重要的参数：`num_blocks` 和 `num_threads`。这些可以是变量或字面值。我推荐使用变量，因为你在调优性能时会用到它们。
- en: The `num_blocks` parameter is something you have not yet covered and is covered
    in detail in the next section. For now all you need to do is ensure you have at
    least one block of threads.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_blocks`参数是你尚未涉及的内容，接下来的章节会详细讲解。现在你只需要确保至少有一个线程块。'
- en: The `num_threads` parameter is simply the number of threads you wish to launch
    into the kernel. For this simple example, this directly translates to the number
    of iterations of the loop. However, be aware that the hardware limits you to 512
    threads per block on the early hardware and 1024 on the later hardware. In this
    example, it is not an issue, but for any real program it is almost certainly an
    issue. You’ll see in the following section how to overcome this.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_threads` 参数仅仅是你希望在内核中启动的线程数量。对于这个简单的例子，这直接对应于循环的迭代次数。然而，需要注意的是，硬件限制了每个块上的线程数量，早期硬件限制为
    512 个线程，而后期硬件则为 1024 个线程。在这个例子中，这不是问题，但对于任何实际的程序来说，这几乎肯定是一个问题。你将在接下来的章节中看到如何解决这个问题。'
- en: The next part of the kernel call is the parameters passed. Parameters can be
    passed via registers or constant memory, the choice of which is based on the compilers.
    If using registers, you will use one register for every thread per parameter passed.
    Thus, for 128 threads with three parameters, you use 3 × 128 = 384 registers.
    This may sound like a lot, but remember that you have at least 8192 registers
    in each SM and potentially more on later hardware revisions. So with 128 threads,
    you have a total of 64 registers (8192 registers ÷ 128 threads) available to you,
    *if* you run just one block of threads on an SM.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 内核调用的下一部分是传递的参数。参数可以通过寄存器或常量内存传递，选择哪种方式取决于编译器。如果使用寄存器，你将为每个传递的参数每个线程使用一个寄存器。因此，对于
    128 个线程和三个参数，你将使用 3 × 128 = 384 个寄存器。这可能听起来很多，但请记住，每个 SM 至少有 8192 个寄存器，并且在后期硬件版本中可能更多。所以对于
    128 个线程，你有总共 64 个寄存器（8192 个寄存器 ÷ 128 个线程）可用，*如果*你在一个 SM 上只运行一个线程块。
- en: However, running one block of 128 threads per SM is a very bad idea, even if
    you can use 64 registers per thread. As soon as you access memory, the SM would
    effectively idle. Only in the very limited case of heavy arithmetic intensity
    utilizing the 64 registers should you even consider this sort of approach. In
    practice, multiple blocks are run on each SM to avoid any idle states.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个 SM 上运行一个包含 128 个线程的线程块是一个非常糟糕的主意，即使你可以为每个线程使用 64 个寄存器。只要你访问内存，SM 就会有效地处于空闲状态。只有在非常有限的情况下，重度算术密集型操作利用了
    64 个寄存器时，才应该考虑这种方法。实际上，每个 SM 上会运行多个块，以避免任何空闲状态。
- en: Blocks
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块
- en: Now 512 threads are not really going to get you very far on a GPU. This may
    sound like a huge number to many programmers from the CPU domain, but on a GPU
    you usually need thousands or tens of thousands of concurrent threads to really
    achieve the throughput available on the device.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，512 个线程在 GPU 上并不会让你走得很远。对于许多来自 CPU 领域的程序员来说，这可能听起来是一个庞大的数字，但在 GPU 上，通常你需要成千上万或者甚至是数万个并发线程，才能真正实现设备上可用的吞吐量。
- en: 'We touched on this previously in the last section on threads, with the `num_blocks`
    parameter for the kernel invocation. This is the first parameter within the `<<<`
    and `>>>` symbols:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节讨论过这个问题，涉及到内核调用中的 `num_blocks` 参数。这是 `<<<` 和 `>>>` 符号中的第一个参数：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you change this from one to two, you double the number of threads you are
    asking the GPU to invoke on the hardware. Thus, the same call,
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将其从 1 改为 2，你就把要求 GPU 调用硬件的线程数量翻倍。因此，相同的调用，
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: will call the GPU function named `some_kernel_func` 2 × 128 times, each with
    a different thread. This, however, complicates the calculation of the `thread_idx`
    parameter, effectively the array index position. This previous, simple kernel
    needs a slight amendment to account for this.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将会调用名为 `some_kernel_func` 的 GPU 函数 2 × 128 次，每次使用不同的线程。这样做会使得 `thread_idx` 参数的计算变得复杂，实际上就是数组的索引位置。这个之前简单的内核需要稍作修改，以考虑到这一点。
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To calculate the `thread_idx` parameter, you must now take into account the
    number of blocks. For the first block, `blockIdx.x` will contain zero, so effectively
    the `thread_idx` parameter is equal to the `threadIdx.x` parameter you used earlier.
    However, for block two, `blockIdx.x` will hold the value 1\. The parameter `blockDim.x`
    holds the value 128, which is, in effect, the number of threads you requested
    per block in this example. Thus, you have a 1 × 128 thread base addresses, before
    adding in the thread offset from the `threadIdx.x` parameter.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 `thread_idx` 参数，你现在必须考虑块的数量。对于第一个块，`blockIdx.x` 将包含零，因此，实际上 `thread_idx`
    参数等于你之前使用的 `threadIdx.x` 参数。然而，对于第二个块，`blockIdx.x` 将保存值 1。参数 `blockDim.x` 保存值
    128，这实际上是你在这个例子中为每个块请求的线程数。因此，在加入来自 `threadIdx.x` 参数的线程偏移量之前，你将有一个 1 × 128 的线程基地址。
- en: Have you noticed the small error we have introduced in adding in another block?
    You will now launch 256 threads in total and index the array from 0 to 255\. If
    you don’t also change the size of the array, from 128 elements to 256 elements,
    you will access and write beyond the end of the array. This array out-of-bounds
    error will not be caught by the compiler and the code may actually run, depending
    on what is located after the destination array, `a`. Be careful when invoking
    a kernel that you do not access out of bounds elements.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否注意到在添加另一个块时引入的小错误？现在，你将启动256个线程，并且将数组索引从0到255。如果你没有改变数组的大小，从128个元素扩展到256个元素，你将访问并写入数组的末尾之外。这个数组越界错误不会被编译器捕获，代码可能会运行，具体取决于目标数组`a`之后的内容。调用内核时要小心，确保不访问越界的元素。
- en: 'For this example, we will stick with the 128-byte array size and change the
    kernel to invoke two blocks of 64 threads each:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将保持128字节的数组大小，并将内核更改为调用两个64个线程的块：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Thus, you get what is shown in [Figure 5.8](#F0045).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你得到的是[图5.8](#F0045)所示的效果。
- en: '![image](../images/F000053f05-08-9780124159334.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-08-9780124159334.jpg)'
- en: Figure 5.8 Block mapping to address.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 块映射到地址。
- en: Notice how, despite now having two blocks, the `thread_idx` parameter still
    equates to the array index, exactly as before. So what is the point of using blocks?
    In this trivial example, absolutely nothing. However, in any real-world problem,
    you have far more than 512 elements to deal with. In fact, if you look at the
    limit on the number of blocks, you find you have 65,536 blocks you can use.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管现在有了两个块，`thread_idx`参数仍然等同于数组索引，和之前完全一样。那么，使用块的意义何在呢？在这个简单的例子中，根本没有任何意义。然而，在任何实际问题中，你需要处理的元素数量远远超过512个。事实上，如果你查看块的数量限制，你会发现可以使用65,536个块。
- en: At 65,536 blocks, with 512 threads per block, you can schedule 33,554,432 (around
    33.5 million) threads in total. At 512 threads, you can have up to three blocks
    per SM. Actually, this limit is based on the total number of threads per SM, which
    is 1536 in the latest Fermi hardware, and as little as 768 in the original G80
    hardware.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在65,536个块的情况下，每个块有512个线程，你总共可以调度33,554,432个线程（大约3350万个）。在每个块512个线程的情况下，每个SM最多可以有三个块。实际上，这个限制是基于每个SM的线程总数，在最新的Fermi硬件上为1536个，而在最初的G80硬件上仅为768个。
- en: If you schedule the maximum of 1024 threads per block on the Fermi hardware,
    65,536 blocks would translate into around 64 million threads. Unfortunately, at
    1024 threads, you only get one thread block per SM. Consequently, you’d need some
    65,536 SMs in a single GPU before you could not allocate at least one block per
    SM. Currently, the maximum number of SMs found on any card is 30\. Thus, there
    is some provision for the number of SMs to grow before you have more SMs than
    the number of blocks the hardware can support. This is one of the beauties of
    CUDA—the fact it can scale to thousands of execution units. The limit of the parallelism
    is only really the limit of the amount of parallelism that can be found in the
    application.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Fermi硬件上调度每个块最大1024个线程，那么65,536个块将转换为大约6400万个线程。不幸的是，在1024个线程的情况下，你每个SM只能获得一个线程块。因此，你需要大约65,536个SM才能在每个SM上至少分配一个块。目前，任何卡上最大数量的SM是30个。因此，SM的数量可以增长，直到超出硬件支持的块数量为止，这也是CUDA的一个优点——它能够扩展到成千上万个执行单元。并行性的限制实际上仅仅是应用程序中可以找到的并行性数量的限制。
- en: With 64 million threads, assuming one thread per array element, you can process
    up to 64 million elements. Assuming each element is a single-precision floating-point
    number, requiring 4 bytes of data, you’d need around 256 million bytes, or 256
    MB, of data storage space. Almost all GPU cards support at least this amount of
    memory space, so working with threads and blocks alone you can achieve quite a
    large amount of parallelism and data coverage.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在6400万个线程的情况下，假设每个线程对应一个数组元素，你可以处理最多6400万个元素。假设每个元素是一个单精度浮点数，需要4个字节的数据，那么你大约需要256百万字节，或者256MB的数据存储空间。几乎所有GPU卡都支持至少这种大小的内存空间，因此，仅通过线程和块，你就可以实现相当大的并行度和数据覆盖。
- en: For anyone worried about large datasets, where large problems can run into gigabytes,
    terabytes, or petabytes of data, there is a solution. For this, you generally
    either process more than one element per thread or use another dimension of blocks,
    which we’ll cover in the next section.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些担心大数据集的用户，尤其是当问题涉及到数GB、TB或PB的数据时，有一个解决方案。为此，通常你要么在每个线程中处理多个元素，要么使用另一个维度的块，接下来的章节将介绍这一点。
- en: Block arrangement
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块排列
- en: 'To ensure that we understand the block arrangement, we’re going to write a
    short kernel program to print the block, thread, warp, and thread index to the
    screen. Now, unless you have at least version 3.2 of the SDK, the `printf` statement
    is not supported in kernels. So we’ll ship the data back to the CPU and print
    it to the console window. The kernel program is thus as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们理解块的排列，我们将编写一个简短的内核程序，打印块、线程、warp和线程索引到屏幕上。现在，除非你至少使用了3.2版本的SDK，否则内核中的`printf`语句是不被支持的。因此，我们将数据传回CPU并打印到控制台窗口。内核程序如下：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now on the CPU you have to run a section of code, as follows, to allocate memory
    for the arrays on the GPU and then transfer the arrays back from the GPU and display
    them on the CPU.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在CPU上，你需要运行以下一段代码，为GPU上的数组分配内存，然后将数组从GPU传回并在CPU上显示它们。
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`}`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`   /∗ Free the arrays on the GPU as now we’re done with them ∗/`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`   /∗ 释放GPU上的数组，因为我们已经不再使用它们 ∗/`'
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In this example, what you see is that each block is located immediately after
    the one before it. As you have only a single dimension to the array, laying out
    the thread blocks in a similar way is an easy way to conceptualize a problem.
    The output of the previous program is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你可以看到每个块的位置紧接在前一个块之后。由于数组只有一个维度，将线程块按相似方式排列是一种简单的思维方式。前面程序的输出如下：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Calculated Thread: 127 - Block: 1 - Warp 1 - Thread 63`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`计算的线程: 127 - 块: 1 - Warp 1 - 线程 63`'
- en: As you can see, the calculated thread, or the thread ID, goes from 0 to 127\.
    Within that you allocate two blocks of 64 threads each. The thread indexes within
    each of these blocks go from 0 to 63\. You also see that each block generates
    two warps.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，计算得到的线程ID，从0到127。在其中，你分配了两个64线程的块。每个块中的线程索引从0到63。你还会看到，每个块生成两个warps。
- en: Grids
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格
- en: A grid is simply a set of blocks where you have an *X* and a *Y* axis, in effect
    a 2D mapping. The final *Y* mapping gives you *Y* × *X* × *T* possibilities for
    a thread index. Let’s look at this using an example, but limiting the *Y* axis
    to a single row to start off with.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 网格实际上就是一组块，其中你有一个*X*轴和一个*Y*轴，本质上是一个二维映射。最终的*Y*映射给你提供了*Y* × *X* × *T*种线程索引的可能性。让我们通过一个例子来看一下，但首先将*Y*轴限制为单行。
- en: If you were to look at a typical HD image, you have a 1920 × 1080 resolution.
    The number of threads in a block should *always* be a multiple of the warp size,
    which is currently defined as 32\. As you can only schedule a full warp on the
    hardware, if you don’t do this, then the remaining part of the warp goes unused
    and you have to introduce a condition to ensure you don’t process elements off
    the end of the *X* axis. This, as you’ll see later, slows everything down.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下典型的高清图像，它的分辨率是1920 × 1080。块中的线程数应该*始终*是warp大小的倍数，当前warp大小定义为32。因为硬件上只能调度完整的warp，如果你不这样做，warp中剩余的部分将无法使用，并且你必须引入一个条件，以确保你不会处理超出*X*轴末端的元素。如你将看到的，这会使得整个过程变慢。
- en: To avoid poor memory coalescing, you should always try to arrange the memory
    and thread usage so they map. This will be covered in more detail in the next
    chapter on memory. Failure to do so will result in something in the order of a
    five times drop in performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免不良的内存合并，你应该始终尝试安排内存和线程的使用，使其能够映射。这将在下一个关于内存的章节中详细讲解。如果不这么做，性能可能会下降约五倍。
- en: To avoid tiny blocks, as they don’t make full use of the hardware, we’ll pick
    192 threads per block. In most cases, this is the *minimum* number of threads
    you should think about using. This gives you exactly 10 blocks across each row
    of the image, which is an easy number to work with ([Figure 5.9](#F0050)). Using
    a thread size that is a multiple of the *X* axis and the warp size makes life
    a lot easier.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免产生过小的块，因为它们无法充分利用硬件，我们将每个块选择为192个线程。在大多数情况下，这是你应该考虑使用的*最小*线程数。这样，每行图像正好有10个块，这是一个容易处理的数字（见[图
    5.9](#F0050)）。使用线程数是*X*轴和warp大小的倍数会让工作变得更简单。
- en: '![image](../images/F000053f05-09-9780124159334.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-09-9780124159334.jpg)'
- en: Figure 5.9 Block allocation to rows.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 块分配到行。
- en: Along the top on the *X* axis, you have the thread index. The row index forms
    the *Y* axis. The height of the row is exactly one pixel. As you have 1080 rows
    of 10 blocks, you have in total 1080 × 10 = 10,800 blocks. As each block has 192
    threads, you are scheduling just over two million threads, one for each pixel.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部的 *X* 轴上，你有线程索引。行索引形成 *Y* 轴。行的高度正好是一个像素。由于你有 1080 行，每行 10 个块，因此你总共有 1080
    × 10 = 10,800 个块。由于每个块有 192 个线程，你将调度超过两百万个线程，每个像素对应一个线程。
- en: This particular layout is useful where you have one operation on a single pixel
    or data point, *or* where you have some operation on a number of data points in
    the *same* row. On the Fermi hardware, at eight blocks per SM, you’d need a total
    of 1350 SMs (10,800 total blocks ÷ 8 scheduled blocks) to run out of parallelism
    at the application level. On the Fermi hardware currently available, you have
    only 16 SMs (GTX580), so each SM would be given 675 blocks to process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定的布局在你对单个像素或数据点进行操作时非常有用，*或者*在你对同一行中的多个数据点进行操作时也同样适用。在 Fermi 硬件上，每个 SM 有八个块，你需要总共
    1350 个 SM（10,800 个总块 ÷ 8 个已调度块）来消耗应用层面的并行性。在目前可用的 Fermi 硬件上，只有 16 个 SM（GTX580），因此每个
    SM 将处理 675 个块。
- en: This is all very well, but what if your data is not row based? As with arrays,
    you are not limited to a single dimension. You can have a 2D thread block arrangement.
    A lot of image algorithms, for example, use 8 × 8 blocks of pixels. We’re using
    pixels here to show this arrangement, as it’s easy for most people to conceptualize.
    Your data need not be pixel based. You typically represent pixels as a red, green,
    and blue component. You could equally have *x*, *y*, and *z* spatial coordinates
    as a single data point, or a simple 2D or 3D matrix holding the data points.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，但如果你的数据不是基于行的呢？与数组一样，你并不局限于单一维度。你可以使用 2D 线程块排列。例如，许多图像算法使用 8 × 8 的像素块。我们这里使用像素来展示这种排列，因为大多数人更容易理解。你的数据不一定是基于像素的。你通常将像素表示为红、绿、蓝三色分量。你也可以将
    *x*、*y* 和 *z* 空间坐标视为单个数据点，或者使用简单的 2D 或 3D 矩阵来存储数据点。
- en: Stride and offset
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步幅和偏移
- en: As with arrays in C, thread blocks can be thought of as 2D structures. However,
    for 2D thread blocks, we need to introduce some new concepts. Just like in array
    indexing, to index into a *Y* element of 2D array, you need to know the width
    of the array, the number of *X* elements. Consider the array in [Figure 5.10](#F0055).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与 C 中的数组一样，线程块可以被视为 2D 结构。然而，对于 2D 线程块，我们需要引入一些新概念。就像数组索引一样，要索引到 2D 数组的 *Y*
    元素，你需要知道数组的宽度，即 *X* 元素的数量。请参考 [图 5.10](#F0055) 中的数组。
- en: '![image](../images/F000053f05-10-9780124159334.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-10-9780124159334.jpg)'
- en: Figure 5.10 Array mapping to elements.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 数组映射到元素。
- en: The width of the array is referred to as the stride of the memory access. The
    offset is the column value being accessed, starting at the left, which is always
    element 0\. Thus, you have array element 5 being accessed with the index [1][5]
    or via the address calculation (row × (sizeof(array_element) × width))) + ((sizeof(array_element)
    × offset)). This is the calculation the compiler effectively uses, in an optimized
    form, when you do multidimensional array indexing in C code.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 数组的宽度被称为内存访问的步幅。偏移量是正在访问的列值，从左边开始，总是元素 0。因此，你有数组元素 5 被通过索引 [1][5] 或通过地址计算（行
    ×（sizeof(array_element) × 宽度)）+（（sizeof(array_element) × 偏移量））访问。这是编译器在 C 代码中进行多维数组索引时，实际上使用的优化计算方法。
- en: Now, how is this relevant to threads and blocks in CUDA? CUDA is designed to
    allow for data decomposition into parallel threads and blocks. It allows you to
    define 1D, 2D, or 3D indexes (*Y* × *X* × *T*) when referring to the parallel
    structure of the program. This maps directly onto the way a typical area of memory
    is set out, allowing the data you are processing to be allocated to individual
    SMs. The process of keeping data close to the processor hugely increases performance,
    both on the GPU and CPU.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这与 CUDA 中的线程和块有什么关系呢？CUDA 设计允许将数据分解为并行的线程和块。它允许你在引用程序的并行结构时定义 1D、2D 或 3D
    索引（*Y* × *X* × *T*）。这直接映射到典型内存区域的排列方式，使得你正在处理的数据能够分配到各个 SM 中。将数据保持在接近处理器的位置，显著提高了
    GPU 和 CPU 的性能。
- en: However, there is one caveat you must be aware of when laying out such arrays.
    The width value of the array must always be a multiple of the warp size. If it
    is not, pad the array to the next largest multiple of the warp size. Padding to
    the next multiple of the warp size should introduce only a very modest increase
    in the size of the dataset. Be aware, however, you’ll need to deal with the padded
    boundary, or halo cells, differently than the rest of the cells. You can do this
    using divergence in the execution flow (e.g., using an `if` statement) or you
    can simply calculate the padded cells and discard the result. We’ll cover divergence
    and the problems it causes later in the book.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在布置这样的数组时，你必须注意一个警告。数组的宽度值必须始终是warp大小的倍数。如果不是，则需要将数组填充到下一个最接近warp大小的倍数。填充到warp大小的下一个倍数应该只会使数据集的大小略微增加。然而，请注意，你需要以不同的方式处理填充边界或光晕单元格，而不是处理其余的单元格。你可以通过执行流中的分歧来处理这个问题（例如，使用`if`语句），或者你可以简单地计算填充的单元格并丢弃结果。我们将在本书后面讨论分歧及其引起的问题。
- en: '*X* and *Y* thread indexes'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '*X*和*Y*线程索引'
- en: 'Having a 2D array in terms of blocks means you get two thread indexes, as you
    will be accessing the data in a 2D way:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 具有2D数组形式的块意味着你将获得两个线程索引，因为你将以2D方式访问数据：
- en: '[PRE31]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice the use of `blockDim.x` and `blockDim.y`, which the CUDA runtime completes
    for you, specifying the dimension on the *X* and *Y* axis. So let’s modify the
    existing program to work on a 32 × 16 array. As you want to schedule four blocks,
    you can schedule them as stripes across the array, or as squares within the array,
    as shown in [Figure 5.11](#F0060).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`blockDim.x`和`blockDim.y`的使用，CUDA运行时会自动为你完成这些工作，指定*X*轴和*Y*轴的维度。因此，我们来修改现有程序，使其适用于32
    × 16的数组。由于你想调度四个块，可以将它们作为条带安排在数组中，或者作为数组中的方块，正如[图5.11](#F0060)所示。
- en: '![image](../images/F000053f05-11-9780124159334.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-11-9780124159334.jpg)'
- en: Figure 5.11 Alternative thread block layouts.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 替代线程块布局。
- en: You could also rotate the striped version 90 degrees and have a column per thread
    block. Never do this, as it will result in completely noncoalesced memory accesses
    that will drop the performance of your application by an order of magnitude or
    more. Be careful when parallelizing loops so that the access pattern always runs
    sequentially through memory in rows and never columns. This applies equally to
    CPU and GPU code.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将条带版本旋转90度，并让每个线程块对应一列。不要这样做，因为这样会导致完全未合并的内存访问，性能可能下降一个数量级甚至更多。并且在并行化循环时要小心，确保访问模式始终按行顺序通过内存访问，而不是按列顺序。无论是CPU代码还是GPU代码，这都适用。
- en: Now why might you choose the square layout over the rectangle layout? Well,
    two reasons. The first is that threads within *the same block* can communicate
    using shared memory, a very quick way to cooperate with one another. The second
    consideration is you get marginally quicker memory access with single 128-byte
    transaction instead of two, 64-byte transactions, due to accessing within a warp
    being coalesced and 128 bytes being the size of a cache line in the Fermi hardware.
    In the square layout notice you have threads 0 to 15 mapped to one block and the
    next memory location belongs to another block. As a consequence you get two transactions
    instead of one, as with the rectangular layout. However, if the array was slightly
    larger, say 64 × 16, then you would not see this issue, as you’d have 32 threads
    accessing contiguous memory, and thus a single 128-byte fetch from memory issued.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么你可能会选择方形布局而不是矩形布局呢？有两个原因。第一个是，*同一块中的线程*可以通过共享内存进行通信，这是一种非常快速的协作方式。第二个考虑因素是，由于warp内的访问是合并的，且128字节是Fermi硬件中缓存行的大小，因此你通过单次128字节事务获取内存，比通过两次64字节事务获得内存要稍微快一些。在方形布局中，注意到你有0到15号线程映射到一个块，而下一个内存位置属于另一个块。因此，你会得到两次事务，而不是矩形布局中的一次。然而，如果数组稍大一些，比如64
    × 16，那么你就不会看到这个问题，因为你将有32个线程访问连续内存，从而发出一次128字节的内存获取。
- en: 'Use the following to modify the program to use either of the two layouts:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码修改程序以使用这两种布局之一：
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`dim3 blocks_rect(1,4);`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim3 blocks_rect(1,4);`'
- en: '[PRE34]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In either arrangement you have the same total number of threads (32 × 4 = 128,
    16 × 8 = 128). It’s simply the layout of the threads that is different.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种排列方式，你都有相同的线程总数（32 × 4 = 128，16 × 8 = 128）。不同之处仅在于线程的布局。
- en: The `dim3` type is simply a special CUDA type that you have to use to create
    a 2D layout of threads. In the rectangle example, you’re saying you want 32 threads
    along the *X* axis by 4 threads along the *Y* axis, within a single block. You’re
    then saying you want the blocks to be laid out as one block wide by four blocks
    high.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim3` 类型是一个特殊的 CUDA 类型，你必须使用它来创建线程的二维布局。在矩形示例中，你表示你希望在 *X* 轴上有 32 个线程，在 *Y*
    轴上有 4 个线程，且都在单个块内。然后，你表示你希望块的布局是宽度为 1 个块，高度为 4 个块。'
- en: You’ll need to invoke the kernel with
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用以下方式调用内核：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: or
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you no longer want just a single thread ID, but an *X* and *Y* position,
    you’ll need to update the kernel to reflect this. However, you also need to linearize
    the thread ID because there are situations where you may want an absolute thread
    index. For this we need to introduce a couple of new concepts, shown in [Figure
    5.12](#F0065).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你不再仅需要一个线程 ID，而是需要 *X* 和 *Y* 位置，因此你需要更新内核来反映这一点。然而，你还需要线性化线程 ID，因为有时你可能需要一个绝对线程索引。为此，我们需要引入几个新概念，如[图
    5.12](#F0065)所示。
- en: '![image](../images/F000053f05-12-9780124159334.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-12-9780124159334.jpg)'
- en: Figure 5.12 Grid, block, and thread dimensions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 网格、块和线程维度。
- en: 'You can see a number of new parameters, which are:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到一些新的参数，它们是：
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`blockDim.y–The size in threads of the Y dimension of a single block.`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`blockDim.y–单个块的 Y 维度线程大小。`'
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You can work out the absolute thread index by working out the *Y* position and
    multiplying this by number of threads in a row. You then simply add in the *X*
    offset from the start of the row. Thus, the thread index calculation is
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过计算 *Y* 位置并将其乘以每行的线程数来得出绝对线程索引。然后，你只需将 *X* 偏移量加到行的起始位置。因此，线程索引计算为：
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So you need to modify the kernel to additionally return the *X* and *Y* positions
    plus some other useful bits of information, as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要修改内核以返回额外的 *X* 和 *Y* 位置，以及其他一些有用的信息，如下所示：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`unsigned int ∗ const grid_dimx,`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`unsigned int ∗ const grid_dimx,`'
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We’ll call the kernel twice to demonstrate how you can arrange array blocks
    and threads.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调用两次内核，以演示如何安排数组块和线程。
- en: As you’re now passing an additional dataset to compute, you need an additional
    `cudaMalloc`, `cudaFree`, and `cudaMemcpy` to copy the data from the device. As
    you’re using two dimensions, you’ll also need to modify the array size to allocate
    and transfer the correct size of data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你现在要传递额外的数据集进行计算，你需要额外的 `cudaMalloc`、`cudaFree` 和 `cudaMemcpy` 来将数据从设备复制过来。由于你使用的是二维数据，你还需要修改数组大小以分配和传输正确大小的数据。
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`  const dim3 blocks_rect(1,4);`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '` const dim3 blocks_rect(1,4);`'
- en: '[PRE48]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`      {`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '` {`'
- en: '[PRE54]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`    printf("Press any key to continue\n");`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '` printf("按任意键继续\n");`'
- en: '[PRE59]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The output is too large to list here. If you run the program in the downloadable
    source code section, you’ll see you iterate through the threads and blocks as
    illustrated in [Figure 5.12](#F0065).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果太大，无法在此列出。如果你在可下载的源代码部分运行程序，你会看到你在[图 5.12](#F0065)中迭代线程和块的过程。
- en: Warps
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Warp
- en: We touched a little on warp scheduling when talking about threads. Warps are
    the basic unit of execution on the GPU. The GPU is effectively a collection of
    SIMD vector processors. Each group of threads, or warps, is executed together.
    This means, in the ideal case, only one fetch from memory for the current instruction
    and a broadcast of that instruction to the entire set of SPs in the warp. This
    is much more efficient than the CPU model, which fetches independent execution
    streams to support task-level parallelism. In the CPU model, for every core you
    have running an independent task, you can conceptually divide the memory bandwidth,
    and thus the effective instruction throughput, by the number of cores. In practice,
    on CPUs, the multilevel, on-chip caches hide a lot of this providing the program
    fits within the cache.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论线程时稍微提到过 warp 调度。Warp 是 GPU 上执行的基本单元。GPU 实际上是多个 SIMD 向量处理器的集合。每一组线程或 warp
    会一起执行。这意味着，在理想情况下，当前指令只需要从内存中获取一次，并将该指令广播到 warp 中的所有 SP（流处理器）。这比 CPU 模型更高效，因为
    CPU 模型获取独立的执行流来支持任务级并行。在 CPU 模型中，对于每个运行独立任务的核心，你可以通过核心数来概念性地划分内存带宽，从而影响有效指令吞吐量。在实际情况中，CPU
    的多级片上缓存会隐藏很多这一部分的细节，前提是程序适合缓存。
- en: You find vector-type instructions on conventional CPUs, in the form of SSE,
    MMX, and AVX instructions. These execute the same single instruction on multiple
    data operands. Thus, you can say, for *N* values, increment all values by one.
    With SSE, you get 128-bit registers, so you can operate on four parameters at
    any given time. AVX extends this to 256 bits. This is quite powerful, but until
    recently, unless you were using the Intel compiler, there was little native support
    for this type of optimization. AVX is now supported by the current GNU gcc compiler.
    Microsoft Visual Studio 2010 supports it through the use of a “/arch:AVX” compiler
    switch. Given this lack of support until relatively recently, vector-type instructions
    are not as widely used as they could be, although this is likely to change significantly
    now that support is no longer restricted to the Intel compiler.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在传统 CPU 上找到矢量类型指令，如 SSE、MMX 和 AVX 指令。这些指令会对多个数据操作数执行相同的单一指令。因此，你可以说，对于 *N*
    个值，将所有值都加 1。使用 SSE，你可以获得 128 位寄存器，因此可以同时处理四个参数。AVX 将此扩展到 256 位。这非常强大，但直到最近，除非你使用英特尔编译器，否则几乎没有原生支持这种优化类型。AVX
    现在已被当前的 GNU gcc 编译器支持。Microsoft Visual Studio 2010 通过使用“/arch:AVX”编译器开关来支持它。由于直到最近才有支持，矢量类型指令并没有像它们应该的那样广泛使用，但随着不再仅限于英特尔编译器，情况可能会显著改变。
- en: 'With GPU programming, you have no choice: It’s vector architecture and expects
    you to write code that runs on thousands of threads. You can actually write a
    single-thread GPU program with a simple `if` statement checking if the thread
    ID is zero, but this will get you terrible performance compared with the CPU.
    It can, however, be useful just to get an initial serial CPU implementation working.
    This approach allows you to check things, such as whether memory copying to/from
    the GPU is working correctly, before introducing parallelism into the application.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 编程中，你别无选择：它是矢量架构，并且期望你编写能够在成千上万个线程上运行的代码。你实际上可以写一个单线程 GPU 程序，使用简单的 `if`
    语句检查线程 ID 是否为零，但与 CPU 相比，这样做会让你得到糟糕的性能。然而，这种方法可以在将并行化引入应用程序之前，确保一些初步的串行 CPU 实现能够正常工作。例如，你可以在此时检查是否能够正确地将内存复制到/从
    GPU。
- en: Warps on the GPU are currently 32 elements, although nVidia reserves the right
    to change this in the future. Therefore, they provide an intrinsic variable, `warpSize`,
    for you to use to obtain the warp size on the current hardware. As with any magic
    number, you should not hard code an assumed warp size of 32\. Many SSE-optimized
    programs were hard coded to assume an SSE size of 128 bits. When AVX was released,
    simply recompiling the code was not sufficient. Don’t make the same mistake and
    hard code such details into your programs.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，GPU 上的 warp 大小为 32 个元素，尽管 nVidia 保留未来更改这一大小的权利。因此，它们提供了一个内建变量 `warpSize`，供你使用以获取当前硬件的
    warp 大小。像任何“魔法数字”一样，你不应该硬编码假设的 32 大小。许多 SSE 优化的程序曾经硬编码假定 SSE 大小为 128 位。当 AVX 发布时，简单地重新编译代码并不足够。不要犯同样的错误，把这种细节硬编码到你的程序中。
- en: So why should you be interested in the size of a warp? The reasons are many,
    so we’ll look briefly at each.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么你应该关心 warp 的大小呢？原因有很多，下面我们将简要讨论每一个。
- en: Branching
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分支
- en: The first reason to be interested in the size of a warp is because of branching.
    Because a warp is a single unit of execution, branching (e.g., `if`, `else`, `for`,
    `while`, `do`, `switch`, etc.) causes a divergence in the flow of execution. On
    a CPU there is complex hardware to do branch prediction, predicting from past
    execution which path a given piece of code will take. The instruction flow is
    then prefetched and pumped into the CPU instruction pipeline ahead of time. Assuming
    the prediction is correct, the CPU avoids a “stall event.” Stall events are very
    bad, as the CPU then has to undo any speculative instruction execution, fetch
    instructions from the other branch path, and refill the pipeline.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣于 warp 大小的第一个原因是分支。由于 warp 是一个单一的执行单元，分支（例如`if`、`else`、`for`、`while`、`do`、`switch`等）会导致执行流的分歧。在
    CPU 上，有复杂的硬件来进行分支预测，预测过去的执行情况，从而确定某段代码将走哪条路径。指令流会提前预取并注入到 CPU 指令流水线中。如果预测正确，CPU
    就能避免“停顿事件”。停顿事件非常糟糕，因为此时 CPU 需要撤销任何猜测执行的指令，从另一条分支路径中提取指令，并重新填充流水线。
- en: 'The GPU is a much simpler device and has none of this complexity. It simply
    executes one path of the branch and then the other. Those threads that take the
    branch are executed and those that do not are marked as inactive. Once the taken
    branch is resolved, the other side of the branch is executed, until the threads
    converge once more. Take the following code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: GPU是一个更简单的设备，并没有这种复杂性。它只是执行分支的一个路径，然后执行另一个路径。那些选择了分支的线程会被执行，而那些没有选择分支的线程则被标记为非活动状态。一旦选中的分支被解决，分支的另一边就会执行，直到线程再次汇聚。看一下下面的代码：
- en: '[PRE61]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: As soon as you evaluate `some_condition`, you will have divergence in at least
    one block or there is no point in having the test in the program. Let’s say all
    the even thread numbers take the true path and all the odd threads take the false
    path. The warp scoreboard then looks as shown in [Figure 5.13](#F0070).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你评估了`some_condition`，至少在一个块中会出现分歧，否则程序中就没有必要进行测试。假设所有偶数编号的线程走真路径，所有奇数编号的线程走假路径。然后，warp的计分板就像[图5.13](#F0070)所示。
- en: '![image](../images/F000053f05-13-9780124159334.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-13-9780124159334.jpg)'
- en: Figure 5.13 Predicate thread/branch selection.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 谓词线程/分支选择。
- en: For simplicity, I’ve drawn only 16 of the 32 threads, and you’ll see why in
    a minute. All those threads marked + take the true or positive path and all those
    marked − take the false or negative path.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我只画了32个线程中的16个，稍后你会明白为什么。所有标记为+的线程走真（正）路径，所有标记为−的线程走假（负）路径。
- en: As the hardware can only fetch a single instruction stream per warp, half of
    the threads stall and half progress down one path. This is really bad news as
    you now have only 50% utilization of the hardware. This is a bit like having a
    dual-core CPU and only using one core. Many lazy programmers get away with it,
    but the performance is terrible compared to what it could be.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件每次只能获取一个单独的指令流，因此一半的线程会停滞，另一半会沿着一条路径继续执行。这是非常不利的消息，因为此时硬件的利用率仅为50%。这有点像是你拥有一颗双核CPU，却只使用一个核心。许多懒惰的程序员能够忍受这种情况，但与可能的性能相比，这种性能非常糟糕。
- en: Now as it happens, there is a trick here that can avoid this issue. The actual
    scheduler in terms of instruction execution is half-warp based, not warp based.
    This means if you can arrange the divergence to fall on a half warp (16-thread)
    boundary, you can actually execute both sides of the branch condition, the `if-else`
    construct in the example program. You can achieve 100% utilization of the device
    in this way.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，恰好有一个技巧可以避免这个问题。实际的指令执行调度器是基于半warp的，而不是基于warp的。这意味着如果你能够将分歧安排在半warp（16线程）边界上，你就可以执行分支条件的两侧，示例程序中的`if-else`结构。通过这种方式，你可以实现设备的100%利用率。
- en: 'If you have two types of processing of the data, interleaving the data on a
    16-word boundary can result in quite good performance. The code would simply branch
    on the thread ID, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有两种类型的数据处理方式，在16字边界上交错数据可以带来不错的性能。代码只需根据线程ID进行分支，如下所示：
- en: '[PRE62]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The modulus operator in C (`%`) returns the remainder of the integer division
    of the operand. In effect, you count from 0 to 31 and then loop back to 0 again.
    Ideally, the function `action_a()` has each of its 16 threads access a single
    float or integer value. This causes a single 64-byte memory fetch. The following
    half warp does the same and thus you issue a single 128-byte memory fetch, which
    it just so happens is the size of the cache line and therefore the optimal memory
    fetch size for a warp.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: C中的取模运算符（`%`）返回操作数整数除法的余数。实际上，你从0计数到31，然后再回到0。理想情况下，`action_a()`函数让它的每个16个线程访问一个单一的浮点数或整数值。这样就会进行一次单一的64字节内存获取。接下来的半warp也做同样的事情，因此你会发出一次单一的128字节内存获取，而恰好128字节是缓存行的大小，因此是warp的最优内存获取大小。
- en: GPU utilization
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU利用率
- en: So why else might you be interested in warps? To avoid underutilizing the GPU.
    The CUDA model uses huge numbers of threads to hide memory latency (the time it
    takes for a memory request to come back). Typically, latency to the global memory
    (DRAM) is around 400–600 cycles. During this time the GPU is busy doing other
    tasks, rather than idly waiting for the memory fetch to complete.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么你还会对warp感兴趣呢？为了避免低效利用GPU。CUDA模型使用大量线程来隐藏内存延迟（内存请求返回的时间）。通常，访问全局内存（DRAM）的延迟约为400到600个周期。在此期间，GPU忙于执行其他任务，而不是闲待内存获取完成。
- en: When you allocate a kernel to a GPU, the maximum number of threads you can put
    onto an SM is currently 768 to 2048, depending on the compute level. This is implementation
    dependent, so it may change with future hardware revisions. Take a quick look
    at utilization with different numbers of threads in [Table 5.1](#T0010).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将内核分配到GPU时，当前你可以分配到SM上的最大线程数为768到2048，具体取决于计算级别。这是与实现相关的，因此随着未来硬件版本的更新可能会有所变化。快速查看不同线程数量下的利用率，详见[表5.1](#T0010)。
- en: Table 5.1 Utilization %
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 利用率百分比
- en: '![Image](../images/T000053tabT0010.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000053tabT0010.jpg)'
- en: Compute 1.0 and 1.2 devices are the G80/G92 series devices. Compute 1.3 devices
    are the GT200 series. Compute 2.0/2.1 devices are the Fermi range. Compute 3.0
    is Kepler.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 计算1.0和1.2设备是G80/G92系列设备，计算1.3设备是GT200系列，计算2.0/2.1设备是Fermi系列，计算3.0是Kepler系列。
- en: Notice that the only consistent value that gets you 100% utilization across
    all levels of the hardware is 256 threads. Thus, for maximum compatibility, you
    should aim for either 192 or 256 threads. The dataset should, however, match the
    thread layout to achieve certain optimizations. You should, therefore, also consider
    the 192-thread layout where you have a three-point data layout.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，唯一能在所有硬件级别上实现100%利用率的线程数量是256个。因此，为了最大兼容性，你应该选择192个或256个线程。然而，数据集应与线程布局匹配，以实现特定的优化。因此，你还应考虑使用192线程布局，尤其是在你拥有三点数据布局的情况下。
- en: Another alternative to having a fixed number of threads is to simply look up
    the compute level from the device and select a the smallest number of threads,
    that gives the highest device utilization.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种替代固定线程数量的方法是直接从设备中查找计算级别，并选择能够实现最高设备利用率的最小线程数量。
- en: Now you might want to also consider the number of blocks that can be scheduled
    into a given SM. This really only makes a difference when you have synchronization
    points in the kernel. These are points where every thread must wait on every other
    thread to reach the same point, for example, when you’re doing a staged read and
    all threads must do the read. Due to the nature of the execution, some warps may
    make good progress and some may make poor progress to the synchronization point.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能还需要考虑可以调度到给定SM中的块数量。当内核中有同步点时，这个问题才会显得重要。这些同步点是每个线程必须等待其他线程达到相同位置的地方。例如，当你进行分阶段读取时，所有线程都必须完成读取。由于执行的性质，一些warps可能进展顺利，而一些可能在同步点前进展缓慢。
- en: The time, or latency, to execute a given block is undefined. This is not good
    from a load balancing point of view. You want lots of threads available to be
    run. With 256 threads, 32 threads per warp give you 8 warps on compute 2.x hardware.
    You can schedule up to 24 warps (32 × 24 = 768 threads) at any one time into a
    given SM for compute 1.x devices and 48 (32 × 48 = 1536 threads) for compute 2.x
    devices. A block cannot be retired from an SM until it’s completed its *entire*
    execution. With compute 2.0x devices or higher that support 1024 threads per block,
    you can be waiting for that single warp to complete while all other warps are
    idle, effectively making the SM also idle.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 执行给定块所需的时间或延迟是未定义的。从负载平衡的角度来看，这并不好。你希望有足够的线程可供运行。对于256个线程，每个warp有32个线程，在计算2.x硬件上，你可以有8个warps。对于计算1.x设备，你可以在任一时刻调度最多24个warps（32
    × 24 = 768个线程），而对于计算2.x设备，可以调度48个warps（32 × 48 = 1536个线程）。块必须完成*整个*执行过程，才能从SM中移除。对于支持每块1024个线程的计算2.0x或更高设备，你可能需要等待那个单独的warp完成，而其他所有warps都处于空闲状态，实际上使SM也处于空闲状态。
- en: Thus, the larger the thread block, the more potential you have to wait for a
    slow warp to catch up, because the GPU can’t continue until all threads have passed
    the checkpoint. Therefore, you might have chosen a smaller number of threads,
    say 128 threads in the past, to reduce this potential waiting time. However, this
    hurts the performance on Fermi-level hardware as the device utilization drops
    to two-thirds. As you can see from [Table 5.1](#T0010), on compute 2.0 devices
    (Fermi), you need to have at least 192 threads per block to make good use of the
    SM.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，线程块越大，你就越有可能等待一个慢速warp追赶上来，因为GPU不能继续执行，直到所有线程都通过检查点。因此，你可能曾选择较小数量的线程，例如128个线程，以减少这种潜在的等待时间。然而，这会降低Fermi级硬件的性能，因为设备利用率会降至三分之二。如[表5.1](#T0010)所示，在计算2.0设备（Fermi）上，你需要每块至少192个线程才能充分利用SM。
- en: However, you should not get too tied up concerning the number of warps, as they
    are really just a measure of the overall number of threads present on the SMs.
    [Table 5.3](#T0020) shows the total number of threads running, and it’s this total
    number that is really the interesting part, along with the percentage utilization
    shown in [Table 5.1](#T0010).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你不应该过于关注warp的数量，因为它们实际上只是表示SM上线程总数的一个指标。[表5.3](#T0020)显示了运行的总线程数，实际上，真正值得关注的是这个总数，以及[表5.1](#T0010)中显示的利用率百分比。
- en: Table 5.2 Blocks per SM
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 每个SM的块数
- en: '![Image](../images/T000053tabT0015.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000053tabT0015.jpg)'
- en: Table 5.3 Total Threads per SM
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.3 每个SM的总线程数
- en: '![Image](../images/T000053tabT0020.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000053tabT0020.jpg)'
- en: Notice with 128 or less threads per block, as you move from the compute 1.3
    hardware (the GT200 series) to the compute 2.x hardware (Fermi), you see no difference
    in the total number of threads running. This is because there are limits to the
    number of blocks an SM can schedule. The number of
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当每个块的线程数为128或更少时，从计算1.3硬件（GT200系列）到计算2.x硬件（Fermi），你会发现运行的总线程数没有差异。这是因为每个SM可以调度的块数是有限制的。线程数的多少
- en: threads an SM could support was increased, but not the number of blocks. Thus,
    to achieve better scaling you need to ensure you have at least 192 threads and
    preferably considerably more.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一个SM可以支持的线程数增加了，但块数并没有增加。因此，为了实现更好的扩展性，你需要确保至少有192个线程，最好是更多。
- en: Block Scheduling
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块调度
- en: Suppose you have 1024 blocks to schedule, and eight SMs to schedule these onto.
    With the Fermi hardware, each SM can accept up to 8 blocks, but only if there
    is a low thread count per block. With a reasonable thread count, you typically
    see 6 to 8 blocks per SM.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有1024个块需要调度，并且有八个SM来调度这些块。在Fermi硬件下，每个SM最多可以接受8个块，但前提是每个块的线程数较低。在合理的线程数下，通常每个SM看到6到8个块。
- en: Now 1024 blocks divided between six SMs is 170 complete blocks each, plus 4
    blocks left over. We’ll look at the leftover blocks in a minute, because it causes
    an interesting problem.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，1024个块分配给六个SM，每个SM得到170个完整的块，还有4个块剩余。稍后我们会讨论这些剩余的块，因为它们会引发一个有趣的问题。
- en: With the 1020 blocks that can be allocated to the SMs, how should they be allocated?
    The hardware could allocate 6 blocks to the first SM, 6 to the second, and so
    on. Alternatively, it could distribute 1 block to each SM in turn, so SM 0 gets
    block 0, SM 1 gets block 1, SM 2 gets block 2, etc. NVIDIA doesn’t specify what
    method it uses, but it’s fairly likely to be the latter to achieve a reasonable
    load balance across the SMs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可以分配给SM的1020个块，应该如何分配？硬件可以将6个块分配给第一个SM，6个块分配给第二个SM，以此类推。或者，它可以轮流将1个块分配给每个SM，这样SM
    0得到块0，SM 1得到块1，SM 2得到块2，等等。NVIDIA没有具体说明使用哪种方法，但很可能是后者，以便在SM之间实现合理的负载平衡。
- en: If you have 19 blocks and four SMs, allocating blocks to an SM until it’s full
    is not a good idea. The first three SMs would get 6 blocks each, and the last
    SM, a single block. The last SM would likely finish quickly and sit idle waiting
    for the other SMs. The utilization of the available hardware is poor.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有19个块和四个SM，将块分配给SM直到其满载并不是一个好主意。前三个SM会各得到6个块，最后一个SM会得到1个块。最后一个SM可能很快完成并空闲，等待其他SM。这会导致可用硬件的利用率较低。
- en: If you allocate blocks to alternate SMs on a rotating basis, each SM gets 4
    blocks (4 SMs × 4 blocks = 16 total) and three SMs get an additional block each.
    Assuming each block takes the same time to execute you have reduced the execution
    time by 17%, simply by balancing the blocks among the SMs, rather than overloading
    some SMs while underloading others.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按轮换方式将块分配给交替的SM，每个SM将获得4个块（4个SM × 4个块 = 总共16个块），并且有三个SM各获得一个额外的块。假设每个块的执行时间相同，通过在SM之间平衡块，而不是使某些SM过载而其他SM空闲，你已经将执行时间减少了17%。
- en: Now in practice you will usually have thousands or tens of thousands of blocks
    to get through in a typical application. Having done the initial allocation of
    blocks to an SM, the block dispatcher is then idle until one block finishes on
    any of the SMs. At this point the block is retired and the resources used by that
    block become free. As all the blocks are the same size, *any* block in the list
    of waiting blocks can be scheduled. The order of execution of blocks is deliberately
    undefined and there should be no implicit assumption that blocks will execute
    in any order when programming a solution to a problem.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你通常需要处理成千上万的块。在典型的应用中，完成初步的块分配到 SM 后，块调度器会一直空闲，直到任何一个 SM 上的块执行完毕。此时，该块被退役，使用的资源变得空闲。由于所有的块大小相同，*任何*等待块列表中的块都可以被调度。块的执行顺序是故意未定义的，编写问题解决方案时不应隐含假设块会按某种顺序执行。
- en: This can have serious problems if there is some associative operation being
    performed, such as floating-point addition, which is not in practice associative.
    The order of execution of adds through an array in floating-point math will affect
    the result. This is due to the rounding errors and the way in which floating-point
    math works. The result is correct in all cases. It’s not a parallel execution
    problem, but an ordering problem. You see exactly the same issue with single-thread
    CPU code. If you add a set of random numbers from bottom to top, or top to bottom,
    in a floating-point array on a CPU or GPU, you will get different answers. Perhaps
    worse still is that on a GPU, due to the undefined block scheduling, multiple
    runs on the same data can result in different but correct answers. There are methods
    to deal with this and it is something we cover later in the book. So for now,
    just be aware that because the result is different than before, it doesn’t necessarily
    make the result incorrect.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行了某些关联操作，比如浮点加法，而浮点加法在实践中并不是关联的，这可能会带来严重的问题。在浮点数学中，通过数组执行加法的顺序将影响结果。这是由于舍入误差以及浮点数学的工作方式。结果在所有情况下都是正确的。这不是一个并行执行问题，而是一个排序问题。在单线程
    CPU 代码中，你也能看到完全相同的问题。如果你在 CPU 或 GPU 上将一组随机数从下到上或从上到下地加到一个浮点数组中，你会得到不同的答案。更糟糕的是，在
    GPU 上，由于块调度未定义，对相同数据的多次运行可能会得到不同但正确的答案。有一些方法可以解决这个问题，我们将在本书后面讨论。所以现在，只需要意识到，由于结果与之前不同，并不一定意味着结果是错误的。
- en: Coming back to the problem of having leftover blocks, you will have this scenario
    anytime the number of blocks is not a multiple of the number of SMs. Typically
    you see CUDA devices ship with an odd number of SMs, due to it being difficult
    to make large, complex processors. As the physical amount of silicon used in creating
    a processor increases, the likelihood there is a failure in some section increases
    considerably. NVIDIA, like many processor manufacturers, simply disables faulty
    SMs and ships devices as lower-specification units. This increases yields and
    provides some economic value to otherwise faulty devices. However, for the programmer,
    this means the total number of SMs is not always even a multiple of two. The Fermi
    480 series cards, and also the Tesla S2050/S2070/C2050/C2070 series, have a 16
    SM device with 1 SM disabled, thus making 15 SMs. This was resolved in the 580
    series, but this problem is likely to be repeated as we see future GPU generations
    released.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 回到剩余块的问题，每当块的数量不是 SM 数量的倍数时，你就会遇到这种情况。通常情况下，你会看到 CUDA 设备配备了奇数数量的 SM，因为制造大型复杂处理器是困难的。随着用于制造处理器的硅材料量增加，某些部分发生故障的可能性也会大幅增加。像许多处理器制造商一样，NVIDIA
    会简单地禁用故障的 SM，并将设备作为低规格单元发货。这提高了产量，并为本应有缺陷的设备提供了经济价值。然而，对于程序员来说，这意味着总的 SM 数量不一定是偶数倍数。Fermi
    480 系列卡，以及 Tesla S2050/S2070/C2050/C2070 系列，都配备了一个有 1 个 SM 被禁用的 16 SM 设备，从而变成了
    15 个 SM。这在 580 系列中得到了修复，但随着未来 GPU 世代的发布，这个问题可能会再次出现。
- en: Having a few leftover blocks is really only an issue if you have a very long
    kernel and need to wait for each kernel to complete. You might see this, for example,
    in a finite time step simulation. If you had 16 blocks, assuming a Fermi 480 series
    card, 15 blocks would be allocated to each of the SMs. The remaining block will
    be scheduled only after one of the other 15 blocks has completed. If each kernel
    took 10 minutes to execute, it’s likely all the blocks would finish at approximately
    the same time. The GPU would then schedule one additional block and the complete
    kernel invocation would wait for an additional 10 minutes for this single block
    to execute. At the same time, the other 14 available SMs would be idle. The solution
    to this problem is to provide better granularity to break down the small number
    of blocks into a much larger number.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一些剩余的块，只有在你有一个非常长的内核并且需要等待每个内核完成时，这才是一个问题。例如，在有限时间步长的仿真中，你可能会看到这种情况。如果你有
    16 个块，假设使用的是 Fermi 480 系列显卡，15 个块会被分配到每个 SM 上。剩余的块只有在其他 15 个块完成后才会调度。如果每个内核执行需要
    10 分钟，那么很可能所有块会在差不多同一时间完成。GPU 会再调度一个额外的块，整个内核调用将等待额外的 10 分钟，直到这个单独的块执行完成。与此同时，其他
    14 个 SM 将处于空闲状态。解决这个问题的方法是提供更好的粒度，将少量块拆分成更多的块。
- en: In a server environment you may not have just 15 SMs, but actually multiple
    nodes each having multiple GPUs. If their only task is this kernel, then they
    will likely sit idle toward the end of the kernel invocation. In this instance
    it might prove better to redesign the kernel in some way to ensure the number
    of blocks is an exact multiple of the number of SMs on each node.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器环境中，你可能不仅仅有 15 个 SM，而是实际拥有多个节点，每个节点有多个 GPU。如果它们的唯一任务是这个内核，那么它们很可能会在内核调用结束时处于空闲状态。在这种情况下，重新设计内核可能会更好，确保块的数量是每个节点上
    SM 数量的精确倍数。
- en: From a load balancing perspective, this problem is clearly not good. As a consequence,
    in the later CUDA runtime, you have support for overlapping kernels and running
    multiple, separate kernels on the same CUDA device. Using this method, you can
    maintain the throughput if you have more than one source of jobs to schedule onto
    the cluster of GPUs. As the CUDA devices start to idle, they instead pick up another
    kernel from a stream of available kernels.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从负载均衡的角度来看，这个问题显然不好。因此，在后来的 CUDA 运行时，你可以支持内核重叠，并在同一 CUDA 设备上运行多个独立的内核。使用这种方法，如果你有多个作业源需要调度到
    GPU 集群上，你可以保持吞吐量。当 CUDA 设备开始空闲时，它们会从一系列可用内核中选择另一个内核。
- en: A Practical Example—Histograms
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际例子—直方图
- en: Histograms are commonly found in programming problems. They work by counting
    the distribution of data over a number of “bins.” Where the data point contains
    a value that is associated with a given bin, the value in that bin is incremented.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图在编程问题中很常见。它们通过统计数据在多个“箱子”中的分布来工作。当数据点包含一个与某个箱子相关的值时，该箱子中的值就会递增。
- en: In the simplest example, you have 256 bins and data that range from 0 to 255\.
    You iterate through an array of bytes. If the value of the element in the array
    is 0, you increment bin 0\. If the value of the element is 10, you increment bin
    10, etc.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的例子中，你有 256 个箱子，数据范围从 0 到 255。你遍历一个字节数组。如果数组中元素的值是 0，你就增加箱子 0 的值。如果元素的值是
    10，你就增加箱子 10 的值，依此类推。
- en: 'The algorithm from a serial perspective is quite simple:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 从串行的角度来看，算法非常简单：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Here you extract the value from the array, indexed by `i`. You then increment
    the appropriate bin using the `++` operator.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你从数组中提取值，通过 `i` 进行索引。然后，你使用 `++` 运算符递增相应的箱子值。
- en: The serial implementation suffers from a problem when you convert it to a parallel
    problem. If you execute this with 256 threads, you get more than one thread *simultaneously*
    incrementing the value in the *same* bin.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 串行实现会在转换为并行问题时遇到问题。如果你使用 256 个线程来执行这段代码，你会得到多个线程*同时*递增*同一个*箱子中的值。
- en: If you look at how the C language gets converted to an assembler, you see it
    can take a series of assembler instructions to execute this code. These would
    break down into
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察 C 语言如何被转换为汇编代码，你会发现执行这段代码可能需要一系列汇编指令。这些指令会分解成
- en: 1. Read the value from the array into a register.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 从数组中读取值到寄存器。
- en: 2. Work out the base address and offset to the correct bin element.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 计算基地址和偏移量以定位到正确的箱子元素。
- en: 3. Fetch the existing bin value.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 获取现有的箱值。
- en: 4. Increment the bin value by one.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 将箱值增加 1。
- en: 5. Write the new bin value back to the bin in memory.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 将新的箱子值写回内存中的箱子。
- en: The problem is steps three, four, and five are not atomic. An atomic operation
    is one that cannot be interrupted prior to completion. If you execute this pseudocode
    in a lockstep manner, as CUDA does with its thread model, you hit a problem. Two
    or more threads fetch the same value at step three. They all increment it and
    write it back. The last thread to do the write wins. The value should have been
    incremented *N* times, but it’s incremented only once. All threads read the same
    value to apply the increment to, thus you lose *N* increments to the value.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于第三步、第四步和第五步不是原子的。原子操作是指在完成之前不能被中断的操作。如果你像CUDA的线程模型那样以锁步方式执行这段伪代码，就会遇到问题。两个或更多线程在第三步时获取相同的值。它们都递增该值并写回。最后一个执行写入的线程获胜。值本应递增*N*次，但实际上只递增了一次。所有线程读取相同的值来应用递增，因此你丢失了*N*次递增。
- en: The problem here is that you have a data dependency you do not see on the serial
    execution version. Each increment of the bin value must complete before the read
    and increment by the next thread. You have a shared resource between threads.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是你在串行执行版本中看不到的数据依赖性。每次增量的箱子值必须在下一个线程读取并增量之前完成。线程之间存在共享资源。
- en: This is not an uncommon problem and CUDA provides a primitive for this called
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个罕见的问题，CUDA为此提供了一种原语，称为
- en: '[PRE64]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This operation guarantees the addition operation is serialized among all threads.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作保证了在所有线程之间的加法操作是串行化的。
- en: 'Having now solved this problem, you come to the real choice here—how to structure
    the tasks you have to cover into threads, blocks, and grids. There are two approaches:
    the task decomposition model or the data decomposition model. Both generally need
    to be considered.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你解决了这个问题，你来到了真正的选择——如何将你必须处理的任务结构化为线程、块和网格。这里有两种方法：任务分解模型或数据分解模型。通常两者都需要考虑。
- en: With the task decomposition model, you simply allocate one thread to every element
    in input array and have it do an atomic add. This is the simplest solution to
    program, but has some major disadvantages. You must remember that this is actually
    a shared resource. If you have 256 bins and an array of 1024 elements, assuming
    an equal distribution, you have 4 elements contending for each bin. With large
    arrays (there is no point in processing small arrays with CUDA) this problem becomes
    the dominant factor determining the total execution time.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任务分解模型，你只需为输入数组中的每个元素分配一个线程，并让它执行原子加法。这是最简单的编程解决方案，但也有一些主要的缺点。你必须记住，这实际上是一个共享资源。如果你有256个箱子和一个包含1024个元素的数组，假设均匀分布，那么每个箱子会有4个元素在争夺资源。对于大型数组（使用CUDA处理小数组没有意义），这个问题成为决定总执行时间的主导因素。
- en: If you assume an equal distribution of values in the histogram, which is often
    not the case, the number of elements contending for any single bin is simply the
    array size in elements divided by the number of bins. With a 512 MB array (524,288
    elements) you would have 131,072 elements contending for each bin. In the worst
    case, all elements write to the same bin, so you have, in effect, a serial program
    due to the serialization of the atomic memory writes.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果假设直方图中的值均匀分布（通常不是这种情况），那么争夺每个箱子的元素数量就是数组元素大小除以箱子数量。对于一个512 MB的数组（524,288个元素），每个箱子将有131,072个元素在争夺。在最坏的情况下，所有元素都写入同一个箱子，因此，由于原子内存写入的串行化，实际上你会得到一个串行程序。
- en: In either example, the execution time is limited by the hardware’s ability to
    handle this contention and the read/write memory bandwidth.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种例子中，执行时间受限于硬件处理这种争用的能力和读/写内存带宽。
- en: Let’s see how this works in reality. Here is the GPU program to do this.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在现实中这是如何运作的。这里是实现这一操作的GPU程序。
- en: '[PRE65]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '`  /∗ Fetch the data value ∗/`'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`  /∗ 获取数据值 ∗/`'
- en: '[PRE69]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: With a GTX460 card, we measured 1025 MB/s with this approach. What is interesting
    is that it does not scale with the number of elements in the array. You get a
    consistently poor performance, regardless of the array size. Note that the GPU
    used for this test, a 1 GB GTX460, has a memory bandwidth of 115 GB/s, so this
    shows just how terrible a performance you can achieve by implementing the naive
    solution.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GTX460显卡，我们测得了1025 MB/s的性能。值得注意的是，这个方法并没有随着数组元素的增加而扩展。无论数组的大小如何，你都会得到持续较差的性能。请注意，参与此测试的GPU——1
    GB的GTX460——具有115 GB/s的内存带宽，所以这显示了通过实现天真解决方案可以达到的糟糕性能。
- en: This figure, although bad, simply tells you that you are limited by some factor
    and it’s your job as a programmer to figure out which factor and eliminate it.
    The most likely factor affecting performance in this type of program is memory
    bandwidth. You are fetching *N* values from the input array and compressing those
    down to *N* writes to a small, 1 K (256 elements × 4 bytes per integer counter)
    memory section.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表，虽然结果不佳，却仅仅告诉你，程序性能受某个因素的限制，而你的任务作为程序员就是找出这个因素并消除它。在这种类型的程序中，影响性能的最可能因素是内存带宽。你正在从输入数组中提取
    *N* 个值，并将这些值压缩成 *N* 次写入一个小的 1K（256 个元素 × 每个整数计数器 4 字节）内存区域。
- en: If you look at the memory reads first, you will see each thread reads one *byte*
    element of the array. Reads are combined together (coalesced) at the half-warp
    level (16 threads). The minimum transfer size is 32 bytes, so you’re wasting read
    memory bandwidth by about 50%, which is pretty poor. The optimal memory fetch
    for a half warp is the maximum supported size, which is 128 bytes. For this, each
    thread has to fetch 4 bytes of memory. You can do this by having each thread process
    four histogram entries instead of one.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你先看内存读取操作，你会发现每个线程读取数组中的一个 *字节* 元素。读取操作在半 warp 层级（16 个线程）合并。最小的传输大小是 32 字节，因此你浪费了大约
    50% 的读取内存带宽，表现得相当差。半 warp 的最佳内存读取是最大支持的大小，即 128 字节。为此，每个线程需要读取 4 字节的内存。你可以通过让每个线程处理四个直方图条目而不是一个条目来实现这一点。
- en: 'We can issue a 4-byte read, by reading a single integer, and then extracting
    the component parts of that integer as shown in [Figure 5.14](#F0075). This should
    provide better read coalescing and therefore better performance. The modified
    kernel is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过读取一个整数来发起一个 4 字节的读取操作，然后像[图 5.14](#F0075)所示那样提取该整数的组成部分。这应该能够提供更好的读取合并，因此提升性能。修改后的内核如下：
- en: '![image](../images/F000053f05-14-9780124159334.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000053f05-14-9780124159334.jpg)'
- en: Figure 5.14 Word-to-byte mapping.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 单词到字节的映射。
- en: '[PRE71]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: When running the kernel we notice we have achieved for all our effort zero speedup.
    This is, in fact, quite common when trying to optimize programs. It’s a pretty
    strong indicator you did not understand the cause of the bottleneck.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行内核时，我们注意到所有的努力并没有带来任何速度提升。事实上，在优化程序时，这种情况是相当常见的。这是一个非常强烈的指示，表明你没有理解瓶颈的根本原因。
- en: One issue to note here is that in compute 2.x, hardware does not suffer with
    only being able to coalesce data from a half warp and can do full-warp coalescing.
    Thus, on the test device, a GTX460 (compute 2.1 hardware), the 32 single byte
    fetches issued by a single warp were coalesced into a 32-byte read.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个问题是，在 compute 2.x 中，硬件并不会因为只能从半 warp 中合并数据而受到影响，能够进行全 warp 合并。因此，在测试设备
    GTX460（compute 2.1 硬件）上，由单个 warp 发出的 32 个单字节读取请求被合并成了一个 32 字节的读取操作。
- en: The obvious candidate is the atomic write operation, rather than the usual memory
    bandwidth culprit. For this you need to look at the alternative approach given
    by the data decomposition model. Here you look at the data flow side of the equation,
    looking for data reuse and optimizing the data size into that which works effectively
    with shared resources, such as a cache or shared memory.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 显然的候选者是原子写操作，而不是通常的内存带宽问题。对此，你需要查看数据分解模型所提供的替代方法。在这里，你需要从数据流的角度出发，寻找数据重用的机会，并优化数据大小，使其能够与共享资源（如缓存或共享内存）有效配合。
- en: You can see that the contention for the 256 bins is a problem. With multiple
    blocks writing to memory from multiple SMs, the hardware needs to sync the value
    of the bin array across the caches in all processors. To do this it needs to fetch
    the current value from memory, increment it, and then write it back. There is
    some potential for this to be held permanently in the L2 cache, which is shared
    between the SMs in the Fermi generation of hardware. With compute 1.x hardware,
    you are reading and writing to the global memory, so this approach is an order
    of magnitude slower.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，256 个桶的争用是一个问题。当多个块从多个 SM 向内存写入数据时，硬件需要在所有处理器的缓存之间同步桶数组的值。为此，它需要从内存中读取当前值，增加它，然后再写回内存。这样有可能将该值永久保存在
    L2 缓存中，而 L2 缓存在 Fermi 代硬件中是各 SM 共享的。对于 compute 1.x 硬件来说，你正在进行全局内存的读写操作，因此这种方法会慢上一个数量级。
- en: Even if you can use the L2 cache on the Fermi hardware, you are still having
    to go out of the SM to sync with all the other SMs. On top of this the write pattern
    you are generating is a scattered pattern, dependent very much on the nature of
    the input data for the histogram. This means no or very little coalescing, which
    again badly hurts performance.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 即便你可以使用Fermi硬件上的L2缓存，你仍然需要离开SM与其他所有SM进行同步。此外，你生成的写入模式是散乱的，这在很大程度上依赖于直方图输入数据的特性。这意味着几乎没有合并写入操作，这会严重影响性能。
- en: An alternative approach is to build the histogram within each SM and then write
    out the histogram to the main memory at the end. This is the approach you must
    always try to achieve, whether for CPU or GPU programming. The more you make use
    of resources close to the processor (SM in this case), the faster the program
    runs.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是在每个SM内部构建直方图，然后在结束时将直方图写入主内存。这是你必须始终尝试实现的方法，无论是CPU编程还是GPU编程。你越是利用靠近处理器的资源（在这种情况下是SM），程序运行得就越快。
- en: 'We mentioned earlier that we can use shared memory, a special form of memory
    that is on chip and thus very fast. You can create a 256-bin histogram in the
    shared memory and then do the atomic add at the end to the global memory. Assuming
    you process only one histogram per block, you do not decrease the number of global
    memory reads or writes, but you do coalesce all the writes to memory. The kernel
    for this approach is as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，我们可以使用共享内存，这是一种特殊形式的内存，它位于芯片上，因此非常快速。你可以在共享内存中创建一个256桶的直方图，然后在最后执行原子加法操作，将结果写入全局内存。假设每个块只处理一个直方图，你不会减少全局内存的读取或写入次数，但你确实会将所有的写入操作合并到一起。此方法的内核如下：
- en: '[PRE79]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The kernel must do an additional clear operation on the shared memory, as you
    otherwise have random data left there from other kernels. Notice also you need
    to wait (`__syncthreads`) until all the threads in a block have managed to clear
    their memory cell in the shared memory before you start allowing threads to update
    any of the shared memory cells. You need to do the same sync operation at the
    end, to ensure every thread has completed before you write the result back to
    the global memory.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 该内核必须对共享内存执行额外的清空操作，否则会有来自其他内核的随机数据残留在那里。另外，需要等到所有线程在块内完成共享内存单元的清空操作后，才能允许线程更新任何共享内存单元，因此你需要在开始更新前执行`__syncthreads`。最后，你还需要在结束时执行相同的同步操作，以确保每个线程都完成后，再将结果写回全局内存。
- en: You should see that, suddenly, you get a huge *six* times jump in performance,
    simply by virtue of arranging the writes in order so they can be coalesced. You
    can now achieve 6800 MB/s processing speed. Note, however, you can only do this
    with compute 1.2 or higher devices as only these support shared memory atomic
    operations.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，通过简单地按顺序安排写入操作以便合并写入，你的性能突然提升了六倍。现在，你可以实现6800 MB/s的处理速度。然而，请注意，只有计算能力为1.2或更高的设备才能执行此操作，因为只有这些设备支持共享内存的原子操作。
- en: Now that you have the ordering correct, you need to look at reducing the global
    memory traffic. You have to read every value from the source data, and you only
    read each value once. You are already using the optimal transfer size for read
    accesses, so let’s look at the data being written. If you process *N* histograms
    per block instead of one histogram per block you reduce the write bandwidth by
    a factor of *N*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然你的写入顺序已经正确，接下来需要考虑减少全局内存流量。你必须从源数据中读取每个值，并且每个值只读取一次。你已经使用了最优的读取传输大小，那么接下来我们来看看写入的数据。如果你每个块处理
    *N* 个直方图，而不是每个块处理一个直方图，你将写入带宽减少了 *N* 倍。
- en: '[Table 5.4](#T0025) shows the value achieved on the 512 MB histogram based
    on processing different values of *N* with a Fermi 460 card (which contains seven
    SMs). You can see a peak of 7886 MB/s at an *N* value of 64\. The kernel is as
    follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.4](#T0025) 显示了基于不同的 *N* 值在Fermi 460卡（包含七个SM）上处理512 MB直方图时所得到的结果。你可以看到在
    *N* 值为64时，峰值达到7886 MB/s。内核如下：'
- en: Table 5.4 Histogram Results
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.4 直方图结果
- en: '![Image](../images/T000053tabT0025.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000053tabT0025.jpg)'
- en: '[PRE87]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Let’s examine this a little, because it’s important to understand what you are
    doing here. You have a loop `i` that runs for *N* iterations. This is the number
    of times you will process 256 bytes of data into the shared memory histogram.
    There are 256 threads invoked for the kernel, one for each bin. As such, the only
    loop you need is a loop over the number of histograms to process. When you’ve
    done one iteration, you move 256 bytes on in memory to process the next histogram
    (`tid_offset += 256`).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微分析一下这个问题，因为理解你在这里做的事情很重要。你有一个循环`i`，它运行*N*次。这是你将256字节数据处理到共享内存直方图的次数。内核会调用256个线程，每个线程对应一个桶。因此，你所需要的唯一循环是对要处理的直方图数量进行循环。当你完成一次迭代后，你将在内存中移动256字节，处理下一个直方图（`tid_offset
    += 256`）。
- en: Notice also that as you’re using atomic operations throughout, you need sync
    points only at the start and end of the kernel. Adding unnecessary synchronization
    points typically slows down the program, but can lead to a more uniform access
    pattern in memory.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 另外需要注意的是，当你使用原子操作时，只有在内核的开始和结束时才需要同步点。添加不必要的同步点通常会减慢程序的运行速度，但可以导致内存中更均匀的访问模式。
- en: Now what is interesting here is that, after you start to process 32 or more
    histograms per block, you see no effective increase in throughput. The global
    memory bandwidth is dropping by a factor of two every time you increase that value
    of *N*. If global memory bandwidth is indeed the problem, you should see a linear
    speed up here for every factor of *N* you add. So what is going on?
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有趣的是，当你开始每个块处理32个或更多直方图时，吞吐量并没有有效提升。每次增加*N*值时，全球内存带宽都会下降一倍。如果全球内存带宽确实是问题所在，你应该看到每增加一个*N*因子，速度是线性提升的。那么，发生了什么？
- en: The main problem is the atomic operations. Every thread must content for access
    to the shared data area, along with other threads. The data pattern has a huge
    influence on the execution time, which is not a good design.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的问题是原子操作。每个线程必须与其他线程竞争访问共享数据区域。数据模式对执行时间有巨大影响，这不是一个好的设计。
- en: We’ll return to this issue later when we look at how you can write such algorithms
    without having to use atomic operations.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们回到这个问题时，我们会看看如何编写这样的算法，而不必使用原子操作。
- en: Conclusion
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We covered a lot in this chapter and you should now be familiar with how CUDA
    breaks tasks into grids, blocks, and threads. We covered the scheduling of blocks
    and warps on the hardware and the need to ensure you always have enough threads
    on the hardware.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章中涵盖了很多内容，你现在应该熟悉CUDA如何将任务分解为网格、块和线程。我们讲解了块和warp在硬件上的调度以及确保硬件上总是有足够线程的必要性。
- en: The threading model used in CUDA is fundamental to understanding how to program
    GPUs efficiently. You should understand how CPUs and GPUs are fundamentally different
    beasts to program, but at the same time how they are related to one another.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA中使用的线程模型是理解如何高效编程GPU的基础。你应该理解CPU和GPU在编程上根本不同，但同时又是相互关联的。
- en: You have seen how arrangement of threads relative to the data you are going
    to process is important and impacts performance. You have also seen, in particular
    with applications that need to share data, it is not always an easy task to parallelize
    a particular problem. You should note that often taking time to consider the correct
    approach is somewhat more important than diving in with the first solution that
    seems to fit.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到，线程如何相对于你要处理的数据进行排列非常重要，并且影响性能。你也看到了，特别是对于需要共享数据的应用程序来说，将某个问题并行化并非总是容易的任务。你应该注意，往往花时间考虑正确的解决方案，比直接用第一个看似合适的方案更为重要。
- en: We also covered the use of atomics and some of the problems of serialization
    these cause. We touched on the problems branching can cause and you should have
    in the back of your mind the need to ensure all threads follow the same control
    path. We look at atomics and branching in more detail later in the book.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了原子操作的使用以及它们可能引起的序列化问题。我们提到了分支可能引发的问题，你应该牢记，确保所有线程遵循相同的控制路径是必要的。稍后在书中，我们会更详细地探讨原子操作和分支。
- en: You have had some exposure to the extended C syntax used within CUDA and should
    feel comfortable in writing a CUDA program with a clear understanding of what
    will happen.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经接触过CUDA中使用的扩展C语法，并且应该能够写出CUDA程序，清楚地理解会发生什么。
- en: By reading this chapter you have gained a great deal of knowledge and hopefully
    should no longer feel that CUDA or parallel programming is a bit like a black
    art.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读本章，你已经获得了大量知识，希望你不再觉得 CUDA 或并行编程像是某种神秘的黑魔法。
- en: Questions
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: 1. Identify the best and worst data pattern for the histogram algorithm developed
    in this chapter. Is there a common usage case that is problematic? How might you
    overcome this?
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 确定本章开发的直方图算法的最佳和最差数据模式。是否有常见的使用场景存在问题？你如何克服这个问题？
- en: 2. Without running the algorithm, what do you think is the likely impact of
    running this code on older hardware based on the G80 design?
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 在不运行算法的情况下，你认为在基于 G80 设计的旧硬件上运行这段代码可能产生的影响是什么？
- en: 3. When processing an array in memory on a CPU, is it best to transverse in
    row-column order or column-row order? Does this change when you move the code
    to a GPU?
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 在 CPU 上处理内存中的数组时，按行列顺序遍历更好，还是按列行顺序遍历更好？当你将代码移植到 GPU 上时，这种情况是否会发生变化？
- en: 4. Consider a section of code that uses four blocks of 256 threads and the same
    code that uses one block of 1024 threads. Which is likely to complete first and
    why? Each block uses four `syncthreads()` calls at various points through the
    code. The blocks require no interblock cooperation.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 考虑一段代码，使用四个块，每个块有 256 个线程，和相同代码但使用一个块，包含 1024 个线程。哪个更可能先完成，为什么？每个块在代码的不同位置都会使用四个
    `syncthreads()` 调用。这些块不需要块间协作。
- en: 5. What are the advantages and disadvantages of an SIMD-based implementation
    that we find in GPUs versus the MIMD implementation we find in multicore CPUs?
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 在 GPU 中找到的基于 SIMD 的实现与在多核 CPU 中找到的 MIMD 实现相比，有哪些优缺点？
- en: Answers
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 答案
- en: 1. The best case is uniform distribution of data. This is because this loads
    the buckets equally and you therefore get an equal distribution of atomic operations
    on the available shared memory banks.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳情况是数据的均匀分布。这是因为这样可以均匀地加载桶，因此你可以在可用的共享内存银行上得到原子操作的均等分布。
- en: The worst case is identical data values. This causes all threads to continuously
    hit the same shared memory bucket, causing serialization of the entire program
    through both the atomic operations and bank conflicts in the shared memory.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 最坏的情况是数据值相同。这会导致所有线程不断地访问相同的共享内存桶，从而通过原子操作和共享内存中的银行冲突串行化整个程序。
- en: Unfortunately, one very common usage is with sorted data. This provides a variation
    on the worst-case usage. Here one bank after another gets continuously hit with
    atomic writes, effectively serializing the problem.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，一个非常常见的使用场景是排序数据。这提供了最坏情况使用的变种。在这种情况下，一个银行一个接一个地被连续访问原子写操作，从而有效地串行化了问题。
- en: One solution is to step through the dataset such that each iteration writes
    to a new bucket. This requires knowledge of the data distribution. For example,
    consider the case of 256 data points modeling a linear function using 32 buckets.
    Let’s assume data points 0 to 31 fall into the first bucket and this is replicated
    for every bucket. By processing one value for each bucket, you can distribute
    writes to the buckets and avoid contention. In this example, you would read data
    points 0, 32, 64, 96, 1, 33, 65, 97, 2, 34, 66, 98, etc.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是通过逐步遍历数据集，使得每次迭代都写入一个新的桶。这需要了解数据分布。例如，考虑使用 32 个桶对 256 个数据点建模线性函数的情况。假设数据点
    0 到 31 落入第一个桶，并且这种情况在每个桶中都被复制。通过每个桶处理一个值，你可以将写操作分配到各个桶，从而避免冲突。在这个示例中，你会按以下顺序读取数据点：0、32、64、96、1、33、65、97、2、34、66、98
    等等。
- en: 2. The G80 devices (compute 1.0, compute 1.1) don’t support shared memory atomics,
    so the code will not compile. Assuming you modified it to use global memory atomics,
    we saw a seven-fold decrease in performance in the example provided earlier in
    the chapter.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 2. G80 设备（计算 1.0，计算 1.1）不支持共享内存原子操作，因此代码将无法编译。假设你修改它以使用全局内存原子操作，我们在本章前面提供的示例中看到性能下降了七倍。
- en: 3. The row-column ordering is best because the CPU will likely use a prefetch
    technique, ensuring the subsequent data to be accessed will be in the cache. At
    the very least, an entire cache line will be fetched from memory. Thus, when the
    CPU comes to the second iteration of the row-based access, `a[0]` will have fetched
    `a[1]` into the cache.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 按行列顺序遍历更好，因为 CPU 很可能会使用预取技术，确保即将访问的数据会被加载到缓存中。至少，整个缓存行将被从内存中提取。因此，当 CPU 进行第二次行访问时，`a[0]`
    将会把 `a[1]` 加载到缓存中。
- en: The column transversal will result in much slower code because the fetch of
    a single cache line on the CPU is unlikely to fetch data used in the subsequent
    loop iteration unless the row size is very small. On the GPU each thread fetches
    one or more elements of the row, so the loop transversal, at a high level, is
    usually by column, with an entire row being made up of individual threads. As
    with the CPU the entire cache line will be fetched on compute 2.x hardware. However,
    unlike the CPU, this cache line will likely be immediately consumed by the multiple
    threads.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 列遍历将导致代码变得更慢，因为在CPU上获取单个缓存行不太可能获取在后续循环迭代中使用的数据，除非行大小非常小。在GPU上，每个线程获取行中的一个或多个元素，因此，循环遍历通常是按列进行的，整个行由多个独立的线程组成。与CPU一样，整个缓存行将在计算2.x硬件上被获取。然而，与CPU不同，这个缓存行可能会立即被多个线程消耗。
- en: 4. During a `syncthreads()` operation, the entire block stalls until every one
    of the threads meets the `syncthreads()` checkpoint. At this point they all become
    available for scheduling again. Having a very large number of threads per block
    can mean the SM runs out of other available warps to schedule while waiting for
    the threads in a single block to meet the checkpoint. The execution flow as to
    which thread gets to execute when is undefined. This means some threads can make
    much better progress than others to the `syncthreads()` checkpoint. This is the
    result of a design decision in favor of throughput over latency at the hardware
    level. A very high thread count per block is generally only useful where the threads
    in the block need to communicate with one another, without having to do interblock
    communication via the global memory.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 在`syncthreads()`操作期间，整个block会暂停，直到每个线程都达到`syncthreads()`检查点。此时，它们都重新变得可调度。每个block内的线程数量非常大时，可能意味着SM在等待一个block中的线程达到检查点时，没有其他可用的warp可以调度。哪个线程何时执行是未定义的。这意味着一些线程可以比其他线程更快地向`syncthreads()`检查点推进。这是硬件层面上的设计决策，优先考虑吞吐量而非延迟。每个block的线程数量非常高通常仅在block中的线程需要相互通信时有用，而不需要通过全局内存进行跨block通信。
- en: 5. The SIMD model amortizes the instruction fetch time over many execution units
    where the instruction stream is identical. However, where the instruction stream
    diverges, execution must be serialized. The MIMD model is designed for divergent
    execution flow and doesn’t need to stall threads when the flow diverges. However,
    the multiple fetch and decoding units require more silicon and higher instruction
    bandwidth requirements to maintain multiple independent execution paths.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 5. SIMD模型将指令获取时间分摊到多个执行单元上，其中指令流是相同的。然而，当指令流发生分歧时，执行必须序列化。MIMD模型专为分歧执行流设计，在执行流分歧时不需要暂停线程。然而，多个获取和解码单元需要更多的硅片和更高的指令带宽要求，以维持多个独立的执行路径。
- en: A mixture of SIMD and MIMD is often the best way of dealing with both control
    flow and identical operations of large datasets. You see this in CPUs in terms
    of SSE/MMX/AVX support. You see this in GPUs in terms of warps and blocks allowing
    for divergence at a higher granularity.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 混合使用SIMD和MIMD通常是处理大数据集中的控制流和相同操作的最佳方式。你可以在CPU上看到这种方式，比如SSE/MMX/AVX的支持。在GPU上，可以通过warp和block的方式看到，这些方式允许在更高粒度上进行分歧。
