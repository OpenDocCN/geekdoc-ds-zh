<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>3.8. Online supplementary materials#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>3.8. Online supplementary materials#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap03_opt/supp/roch-mmids-opt-supp.html">https://mmids-textbook.github.io/chap03_opt/supp/roch-mmids-opt-supp.html</a></blockquote>

<section id="quizzes-solutions-code-etc">
<h2><span class="section-number">3.8.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">3.8.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_opt_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">3.8.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_2.html">Section 3.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_3.html">Section 3.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_4.html">Section 3.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_5.html">Section 3.5</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_6.html">Section 3.6</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">3.8.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">3.8.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E3.2.1:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(x_1, x_2) &amp;= \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}\right) \\
&amp;= (6x_1 - 2x_2 - 5, -2x_1 + 8x_2 + 2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, -1)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(1, -1) = (6(1) - 2(-1) - 5, -2(1) + 8(-1) + 2) = (3, -8).
\]</div>
<p>Answer and justification for E3.2.3:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1} &amp;= \cos(x_1) \cos(x_2), \\
\frac{\partial f}{\partial x_2} &amp;= -\sin(x_1) \sin(x_2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((\frac{\pi}{4}, \frac{\pi}{3})\)</span>, we have:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= \cos(\frac{\pi}{4}) \cos(\frac{\pi}{3}) = \frac{\sqrt{2}}{2} \cdot \frac{1}{2} = \frac{\sqrt{2}}{4}, \\
\frac{\partial f}{\partial x_2}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= -\sin(\frac{\pi}{4}) \sin(\frac{\pi}{3}) = -\frac{\sqrt{2}}{2} \cdot \frac{\sqrt{3}}{2} = -\frac{\sqrt{6}}{4}.
\end{align*}\]</div>
<p>Answer and justification for E3.2.5: The Hessian matrix of <span class="math notranslate nohighlight">\(f(x_1, x_2)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
6x_1 &amp; 6x_2 \\
6x_2 &amp; 6x_1 - 12x_2
\end{pmatrix}.
\end{split}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, 2)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(1, 2) = \begin{pmatrix}
6 &amp; 12 \\
12 &amp; -18
\end{pmatrix}.
\end{split}\]</div>
<p>We can see that <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial x_1 \partial x_2}(1, 2) = \frac{\partial^2 f}{\partial x_2 \partial x_1}(1, 2) = 12\)</span>, confirming the Symmetry of the Hessian Theorem.</p>
<p>Answer and justification for E3.2.7:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial^2 f}{\partial x_1^2} &amp;= 2 \sin(x_2), \\
\frac{\partial^2 f}{\partial x_1 \partial x_2} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2^2} &amp;= -x_1^2 \sin(x_2).
\end{align*}\]</div>
<p>Answer and justification for E3.2.9: The Hessian matrix is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2, x_3) = \begin{pmatrix}
2 &amp; -2 &amp; 4 \\
-2 &amp; 4 &amp; -6 \\
4 &amp; -6 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.11: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = 3x^2y^2 - 2y^3\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = 2x^3y - 6xy^2 + 4y^3\)</span>, obtained by differentiating <span class="math notranslate nohighlight">\(f\)</span> with respect to each variable while holding the other constant.</p>
<p>Answer and justification for E3.2.13: The Hessian matrix is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_g(x, y) = \begin{pmatrix}
-\sin(x) \cos(y) &amp; -\cos(x) \sin(y) \\
-\cos(x) \sin(y) &amp; -\sin(x) \cos(y)
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.15: <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial x^2} = 6x\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial y^2} = -6x\)</span>. Adding these gives <span class="math notranslate nohighlight">\(6x - 6x = 0\)</span>, so <span class="math notranslate nohighlight">\(q\)</span> satisfies Laplace’s equation.</p>
<p>Answer and justification for E3.2.17: By the chain rule, the rate of change of temperature experienced by the particle is</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(t)) = \nabla u(\mathbf{c}(t))^T \mathbf{c}'(t).
\]</div>
<p>We have <span class="math notranslate nohighlight">\(\nabla u(x, y) = (-2xe^{-x^2 - y^2}, -2ye^{-x^2 - y^2})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}'(t) = (2t, 3t^2)\)</span>. Evaluating at <span class="math notranslate nohighlight">\(t = 1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(1)) = (-2e^{-2}, -2e^{-2})^T (2, 3) = -10e^{-2}.
\]</div>
<p>Answer and justification for E3.2.19: <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = 2t \cos t - t^2 \sin t\)</span>. Justification: <span class="math notranslate nohighlight">\(\nabla f = (y, x)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{g}'(t) = (2t, -\sin t)\)</span>. Then, <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = \cos t \cdot 2t + t^2 \cdot (-\sin t)\)</span>.</p>
<p>Answer and justification for E3.3.1: <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (2x_1, 4x_2)\)</span>. Setting this equal to zero yields <span class="math notranslate nohighlight">\(2x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(4x_2 = 0\)</span>, which implies <span class="math notranslate nohighlight">\(x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(x_2 = 0\)</span>. Thus, the only point where <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = 0\)</span> is <span class="math notranslate nohighlight">\((0, 0)\)</span>.</p>
<p>Answer and justification for E3.3.3: The second directional derivative is given by <span class="math notranslate nohighlight">\(\frac{\partial^2 f(\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \mathbf{v}\)</span>. We have <span class="math notranslate nohighlight">\(\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix}\)</span>. Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial^2 f(1, 1)}{\partial \mathbf{v}^2} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})^T \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix} (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) = \frac{1}{2}(2 + 2 + 2 + 2) = 4.
\end{split}\]</div>
<p>Answer and justification for E3.3.5: The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla_{x_1, x_2} L(x_1, x_2, \lambda) &amp;= 0, \\
h(x_1, x_2) &amp;= 0.
\end{align*}\]</div>
<p>Computing the gradients:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial L}{\partial x_1} &amp;= 2x_1 + \lambda = 0, \\
\frac{\partial L}{\partial x_2} &amp;= 2x_2 + \lambda = 0, \\
x_1 + x_2 &amp;= 1.
\end{align*}\]</div>
<p>From the first two equations, we have <span class="math notranslate nohighlight">\(x_1 = x_2 = -\frac{\lambda}{2}\)</span>. Substituting into the third equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - \frac{\lambda}{2} = 1 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = x_2 = \frac{1}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, \frac{1}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.7: The Lagrangian is <span class="math notranslate nohighlight">\(L(x_1, x_2, x_3, \lambda) = x_1^2 + x_2^2 + x_3^2 + \lambda(x_1 + 2x_2 + 3x_3 - 6)\)</span>. The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2x_1 + \lambda &amp;= 0, \\
2x_2 + 2\lambda &amp;= 0, \\
2x_3 + 3\lambda &amp;= 0, \\
x_1 + 2x_2 + 3x_3 &amp;= 6.
\end{align*}\]</div>
<p>From the first three equations, we have <span class="math notranslate nohighlight">\(x_1 = -\frac{\lambda}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = -\lambda\)</span>, <span class="math notranslate nohighlight">\(x_3 = -\frac{3\lambda}{2}\)</span>. Substituting into the fourth equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - 2\lambda - \frac{9\lambda}{2} = 6 \implies -6\lambda = 6 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = \frac{1}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = 1\)</span>, <span class="math notranslate nohighlight">\(x_3 = \frac{3}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, 1, \frac{3}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.9: The gradient of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (3x_1^2 - 3x_2^2, -6x_1x_2)\)</span>. At the point <span class="math notranslate nohighlight">\((1, 0)\)</span>, the gradient is <span class="math notranslate nohighlight">\(\nabla f(1, 0) = (3, 0)\)</span>. The directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((1, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(\mathbf{v} = (1, 1)\)</span> is <span class="math notranslate nohighlight">\(\nabla f(1, 0)^T \mathbf{v} = (3, 0)^T (1, 1) = 3\)</span>. Since this is positive, <span class="math notranslate nohighlight">\(v\)</span> is not a descent direction.</p>
<p>Answer and justification for E3.3.11: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2) = \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}.
\end{split}\]</div>
<p>Therefore, the second directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((0, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(v = (1, 0)\)</span> is <span class="math notranslate nohighlight">\(\mathbf{v}^T \mathbf{H}_f(0, 0) \mathbf{v} = (1, 0) \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = -2\)</span>.</p>
<p>Answer and justification for E3.3.13: The Hessian matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(1,1) = \begin{pmatrix}
6 &amp; -3 \\
-3 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Justification: Compute the second partial derivatives:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f}{\partial x^2} = 6x, \quad \frac{\partial^2 f}{\partial y^2} = 6y, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x} = -3.
\]</div>
<p>At <span class="math notranslate nohighlight">\((1,1)\)</span>, these values are <span class="math notranslate nohighlight">\(6\)</span>, <span class="math notranslate nohighlight">\(6\)</span>, and <span class="math notranslate nohighlight">\(-3\)</span>, respectively.</p>
<p>Answer and justification for E3.4.1: The convex combination is:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, 3) + \alpha(4, 5) = 0.7(2, 3) + 0.3(4, 5) = (0.7 \cdot 2 + 0.3 \cdot 4, 0.7 \cdot 3 + 0.3 \cdot 5) = (2.6, 3.6).
\]</div>
<p>Answer and justification for E3.4.3: <span class="math notranslate nohighlight">\(S_1\)</span> and <span class="math notranslate nohighlight">\(S_2\)</span> are halfspaces, which are convex sets. By the lemma in the text, the intersection of convex sets is also convex. Therefore, <span class="math notranslate nohighlight">\(S_1 \cap S_2\)</span> is a convex set.</p>
<p>Answer and justification for E3.4.5: The function <span class="math notranslate nohighlight">\(f\)</span> is a quadratic function with <span class="math notranslate nohighlight">\(P = 2\)</span>, <span class="math notranslate nohighlight">\(q = 2\)</span>, and <span class="math notranslate nohighlight">\(r = 1\)</span>. Since <span class="math notranslate nohighlight">\(P &gt; 0\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is strictly convex. The unique global minimizer is found by setting the gradient to zero:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(x) = 2x + 2 = 0 \implies x^* = -1.
\]</div>
<p>Answer and justification for E3.4.7: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla^2 f(x, y) = \begin{pmatrix}
2 &amp; 0 \\
0 &amp; 4
\end{pmatrix}.
\end{split}\]</div>
<p>For any <span class="math notranslate nohighlight">\((x, y) \in \mathbb{R}^2\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 f(x, y) \succeq 2I_{2 \times 2},
\]</div>
<p>so <span class="math notranslate nohighlight">\(f\)</span> is strongly convex with <span class="math notranslate nohighlight">\(m = 2\)</span>.</p>
<p>Answer and justification for E3.4.9: <span class="math notranslate nohighlight">\(f\)</span> is not convex. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2 - 4\)</span>, which is negative for <span class="math notranslate nohighlight">\(x \in (-\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}})\)</span>. Since the second derivative is not always nonnegative, <span class="math notranslate nohighlight">\(f\)</span> is not convex.</p>
<p>Answer and justification for E3.4.11: <span class="math notranslate nohighlight">\(f\)</span> is strongly convex. We have <span class="math notranslate nohighlight">\(f''(x) = 2 &gt; 0\)</span>, so <span class="math notranslate nohighlight">\(f\)</span> is 2-strongly convex.</p>
<p>Answer and justification for E3.4.13: <span class="math notranslate nohighlight">\(D\)</span> is convex. To show this, let <span class="math notranslate nohighlight">\((x_1, y_1), (x_2, y_2) \in D\)</span> and <span class="math notranslate nohighlight">\(\alpha \in (0, 1)\)</span>. We need to show that <span class="math notranslate nohighlight">\((1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) \in D\)</span>. Compute:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) = ((1 - \alpha)x_1 + \alpha x_2, (1 - \alpha)y_1 + \alpha y_2).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
((1 - \alpha)x_1 + \alpha x_2)^2 + ((1 - \alpha)y_1 + \alpha y_2)^2 &lt; (1 - \alpha)(x_1^2 + y_1^2) + \alpha(x_2^2 + y_2^2) &lt; 4.
\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(D\)</span> is convex.</p>
<p>Answer and justification for E3.4.15: <span class="math notranslate nohighlight">\(D\)</span> is not convex. For example, let <span class="math notranslate nohighlight">\((x_1, y_1) = (2, \sqrt{3})\)</span> and <span class="math notranslate nohighlight">\((x_2, y_2) = (2, -\sqrt{3})\)</span>, both of which are in <span class="math notranslate nohighlight">\(D\)</span>. For <span class="math notranslate nohighlight">\(\alpha = \frac{1}{2}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, \sqrt{3}) + \alpha(2, -\sqrt{3}) = \left(2, 0\right).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
\left(2\right)^2 - \left(0\right)^2 = 4 &gt; 1.
\]</div>
<p>Answer and justification for E3.5.1: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 8y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 8)\)</span>. The direction of steepest descent is <span class="math notranslate nohighlight">\(-\nabla f(1, 1) = (-2, -8)\)</span>.</p>
<p>Answer and justification for E3.5.3: <span class="math notranslate nohighlight">\(\nabla f(x) = 3x^2 - 12x + 9\)</span>. At <span class="math notranslate nohighlight">\(x_0 = 0\)</span>, <span class="math notranslate nohighlight">\(\nabla f(0) = 9\)</span>. The first iteration gives <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 0 - 0.1 \cdot 9 = -0.9\)</span>. At <span class="math notranslate nohighlight">\(x_1 = -0.9\)</span>, <span class="math notranslate nohighlight">\(\nabla f(-0.9) = 3 \cdot (-0.9)^2 - 12 \cdot (-0.9) + 9 = 2.43 + 10.8 + 9 = 22.23\)</span>. The second iteration gives <span class="math notranslate nohighlight">\(x_2 = x_1 - \alpha \nabla f(x_1) = -0.9 - 0.1 \cdot 22.23 = -3.123\)</span>.</p>
<p>Answer and justification for E3.5.5: We have <span class="math notranslate nohighlight">\(\nabla f(x) = 2x\)</span>, so <span class="math notranslate nohighlight">\(\nabla f(2) = 4\)</span>. Thus, the gradient descent update is <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 2 - 0.1 \cdot 4 = 1.6\)</span>.</p>
<p>Answer and justification for E3.5.7: <span class="math notranslate nohighlight">\(f''(x) = 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Therefore, <span class="math notranslate nohighlight">\(f''(x) \geq 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which implies that <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(4\)</span>-strongly convex.</p>
<p>Answer and justification for E3.5.9: No. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2\)</span>, which can be arbitrarily large as <span class="math notranslate nohighlight">\(x\)</span> increases. Thus, there is no constant <span class="math notranslate nohighlight">\(L\)</span> such that <span class="math notranslate nohighlight">\(-L \le f''(x) \le L\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>.</p>
<p>Answer and justification for E3.5.11: We have <span class="math notranslate nohighlight">\(f''(x) = 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Thus, we can take <span class="math notranslate nohighlight">\(m = 2\)</span>, and we have <span class="math notranslate nohighlight">\(f''(x) \ge 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which is the condition for <span class="math notranslate nohighlight">\(m\)</span>-strong convexity.</p>
<p>Answer and justification for E3.5.13: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 2y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 2)\)</span>. The first update is <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span>. The gradient at <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span> is <span class="math notranslate nohighlight">\((1.6, 1.6)\)</span>. The second update is <span class="math notranslate nohighlight">\((0.64, 0.64)\)</span>.</p>
<p>Answer and justification for E3.6.1: The log-odds is given by <span class="math notranslate nohighlight">\(\log \frac{p}{1-p} = \log \frac{0.25}{0.75} = \log \frac{1}{3} = -\log 3\)</span>.</p>
<p>Answer and justification for E3.6.3:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\alpha}^T \mathbf{x} = (-0.2 \cdot 1) + (0.4 \cdot 3) = -0.2 + 1.2 = 1.0
\]</div>
<div class="math notranslate nohighlight">
\[
p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(1.0) = \frac{1}{1 + e^{-1}} \approx 0.731
\]</div>
<p>Answer and justification for E3.6.5: By the quotient rule,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sigma'(z) &amp;= \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\
&amp;= \sigma(z) (1 - \sigma(z)).
\end{align*}\]</div>
<p>Answer and justification for E3.6.7: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}) \approx 0.05\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}) \approx -0.73\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}) \approx 0.73\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \ell(\mathbf{x}; A, b) &amp;= -\frac{1}{3} \sum_{i=1}^3 (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \boldsymbol{\alpha}_i \\
&amp;\approx -\frac{1}{3} \{(0.05)(1, 2) + (-0.73)(-1, 1) + 0.73(0, -1)\}.
\end{align*}\]</div>
<p>Answer and justification for E3.6.9: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}^0) = 1 - \sigma(0) = 0.5\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}^0) = -0.5\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}^0) = 0.5\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{x}^1 &amp;= \mathbf{x}^0 - \beta \nabla \ell(\mathbf{x}^0; A, \mathbf{b}) \\
&amp;= (0, 0) + \frac{0.1}{3} \{0.5(1, 2) + (-0.5)(-1, 1) + 0.5(0, -1)\} \\
&amp;= (0.1, 0.05).
\end{align*}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">3.8.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define and calculate partial derivatives and gradients for functions of several variables.</p></li>
<li><p>Compute second-order partial derivatives and construct the Hessian matrix for twice continuously differentiable functions.</p></li>
<li><p>Apply the Chain Rule to compute derivatives of composite functions of several variables.</p></li>
<li><p>State and prove the multivariable version of the Mean Value Theorem using the Chain Rule.</p></li>
<li><p>Calculate gradients and Hessians for affine and quadratic functions of several variables.</p></li>
<li><p>Define global and local minimizers for unconstrained optimization problems.</p></li>
<li><p>Derive the first-order necessary conditions for a local minimizer using the gradient.</p></li>
<li><p>Explain the concept of a descent direction and its relation to the gradient.</p></li>
<li><p>Explain the concept of directional derivatives and compute directional derivatives using the gradient.</p></li>
<li><p>State the second-order necessary and sufficient conditions for a local minimizer using the Hessian matrix.</p></li>
<li><p>Compute the gradient and Hessian matrix for a given multivariable function.</p></li>
<li><p>Formulate optimization problems with equality constraints.</p></li>
<li><p>Apply the method of Lagrange multipliers to derive the first-order necessary conditions for a constrained local minimizer.</p></li>
<li><p>Verify the second-order sufficient conditions for a constrained local minimizer using the Hessian matrix and Lagrange multipliers.</p></li>
<li><p>Analyze the regularity condition in the context of constrained optimization problems.</p></li>
<li><p>Solve a constrained optimization problem by finding points that satisfy the first-order necessary conditions and checking the second-order sufficient conditions.</p></li>
<li><p>Define convex sets and convex functions, and provide examples of each.</p></li>
<li><p>Identify operations that preserve convexity of sets and functions.</p></li>
<li><p>Characterize convex functions using the first-order convexity condition based on the gradient.</p></li>
<li><p>Determine the convexity of a function using the second-order convexity condition based on the Hessian matrix.</p></li>
<li><p>Explain the relationship between convexity and optimization, particularly how local minimizers of convex functions are also global minimizers.</p></li>
<li><p>State and prove the first-order optimality condition for convex functions on R^d and on convex sets.</p></li>
<li><p>Define strong convexity and its implications for the existence and uniqueness of global minimizers.</p></li>
<li><p>Analyze the convexity and strong convexity of quadratic functions and least-squares objectives.</p></li>
<li><p>Apply the concept of convexity to solve optimization problems, such as finding the projection of a point onto a convex set.</p></li>
<li><p>Define gradient descent and explain its motivation as a numerical optimization method.</p></li>
<li><p>Prove that the negative gradient is the steepest descent direction for a continuously differentiable function.</p></li>
<li><p>Analyze the convergence of gradient descent for smooth functions, proving that it produces a sequence of points with decreasing objective values and vanishing gradients.</p></li>
<li><p>Derive the convergence rate of gradient descent for smooth functions in terms of the number of iterations.</p></li>
<li><p>Define strong convexity for twice continuously differentiable functions and relate it to the function value and gradient.</p></li>
<li><p>Prove faster convergence rates for gradient descent when applied to smooth and strongly convex functions, showing exponential convergence to the global minimum.</p></li>
<li><p>Implement gradient descent in Python and apply it to simple examples to illustrate the theoretical convergence results.</p></li>
<li><p>Explain the role of the sigmoid function in transforming a linear function of features into a probability.</p></li>
<li><p>Derive the gradient and Hessian of the logistic regression objective function (cross-entropy loss).</p></li>
<li><p>Prove that the logistic regression objective function is convex and smooth.</p></li>
<li><p>Implement gradient descent to minimize the logistic regression objective function in Python.</p></li>
<li><p>Apply logistic regression to real-world datasets.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
</section>
<section id="additional-sections">
<h2><span class="section-number">3.8.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="logistic-regression-illustration-of-convergence-result">
<h3><span class="section-number">3.8.2.1. </span>Logistic regression: illustration of convergence result<a class="headerlink" href="#logistic-regression-illustration-of-convergence-result" title="Link to this heading">#</a></h3>
<p>We return to our proof of convergence for smooth functions using a special case of logistic regression. We first define the functions <span class="math notranslate nohighlight">\(\hat{f}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial}{\partial x}\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="p">)</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">b</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We illustrate GD on a random dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the upper and lower bounds in the <em>Quadratic Bound for Smooth Functions</em> around <span class="math notranslate nohighlight">\(x = x_0\)</span>. It turns out we can take <span class="math notranslate nohighlight">\(L=1\)</span> because all features are uniformly random between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Observe that minimizing the upper quadratic bound leads to a decrease in <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x0</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.3</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span><span class="n">x0</span><span class="o">+</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">lower</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'upper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'lower'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png" src="../Images/097a341c91c4cf50443a8e9f81f433b8.png" data-original-src="https://mmids-textbook.github.io/_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png"/>
</div>
</div>
</section>
<section id="logistic-regression-another-dataset">
<h3><span class="section-number">3.8.2.2. </span>Logistic regression: another dataset<a class="headerlink" href="#logistic-regression-another-dataset" title="Link to this heading">#</a></h3>
<p>Recall that to run gradient descent, we first implement a function computing a descent update. It takes as input a function <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> computing the gradient itself, as well as a current iterate and a step size. We now also feed a dataset as additional input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">curr_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">curr_x</span> <span class="o">-</span> <span class="n">beta</span><span class="o">*</span><span class="n">gradient</span>

<span class="k">def</span> <span class="nf">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)):</span>
    <span class="n">curr_x</span> <span class="o">=</span> <span class="n">init_x</span>
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">curr_x</span> <span class="o">=</span> <span class="n">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">curr_x</span>

<span class="k">def</span> <span class="nf">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">L</span>
</pre></div>
</div>
</div>
</div>
<p>We analyze with a simple dataset from UC Berkeley’s <a class="reference external" href="http://www.ds100.org">DS100</a> course. The file <code class="docutils literal notranslate"><span class="pre">lebron.csv</span></code> is available <a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets">here</a>. Quoting a previous version of the course’s textbook:</p>
<blockquote>
<div><p>In basketball, players score by shooting a ball through a hoop. One such player, LeBron James, is widely considered one of the best basketball players ever for his incredible ability to score. LeBron plays in the National Basketball Association (NBA), the United States’s premier basketball league. We’ve collected a dataset of all of LeBron’s attempts in the 2017 NBA Playoff Games using the NBA statistics website (<a class="reference external" href="https://stats.nba.com/">https://stats.nba.com/</a>).</p>
</div></blockquote>
<p>We first load the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'lebron.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>game_date</th>
      <th>minute</th>
      <th>opponent</th>
      <th>action_type</th>
      <th>shot_type</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20170415</td>
      <td>10</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20170415</td>
      <td>11</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20170415</td>
      <td>14</td>
      <td>IND</td>
      <td>Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20170415</td>
      <td>15</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20170415</td>
      <td>18</td>
      <td>IND</td>
      <td>Alley Oop Dunk Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>game_date</th>
      <th>minute</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3.840000e+02</td>
      <td>384.00000</td>
      <td>384.000000</td>
      <td>384.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.017052e+07</td>
      <td>24.40625</td>
      <td>10.695312</td>
      <td>0.565104</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.948501e+01</td>
      <td>13.67304</td>
      <td>10.547586</td>
      <td>0.496390</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2.017042e+07</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.017050e+07</td>
      <td>13.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.017052e+07</td>
      <td>25.00000</td>
      <td>6.500000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.017060e+07</td>
      <td>35.00000</td>
      <td>23.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.017061e+07</td>
      <td>48.00000</td>
      <td>31.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The two columns we will be interested in are <code class="docutils literal notranslate"><span class="pre">shot_distance</span></code> (LeBron’s distance from the basket when the shot was attempted (ft)) and <code class="docutils literal notranslate"><span class="pre">shot_made</span></code> (0 if the shot missed, 1 if the shot went in). As the summary table above indicates, the average distance was <code class="docutils literal notranslate"><span class="pre">10.6953</span></code> and the frequency of shots made was <code class="docutils literal notranslate"><span class="pre">0.565104</span></code>. We extract those two columns and display them on a scatter plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">feature</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'shot_distance'</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'shot_made'</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png" src="../Images/3d7f7643b1fbf4e975e00b74d1d83929.png" data-original-src="https://mmids-textbook.github.io/_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png"/>
</div>
</div>
<p>As you can see, this kind of data is hard to vizualize because of the superposition of points with the same <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>-values. One trick is to jiggle the <span class="math notranslate nohighlight">\(y\)</span>’s a little bit by adding Gaussian noise. We do this next and plot again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">label_jitter</span> <span class="o">=</span> <span class="n">label</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png" src="../Images/8ed222a6929c72dd22f117b3334ede4e.png" data-original-src="https://mmids-textbook.github.io/_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png"/>
</div>
</div>
<p>We apply GD to logistic regression. We first construct the data matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. To allow an affine function of the features, we add a column of <span class="math notranslate nohighlight">\(1\)</span>’s as we have done before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)),</span><span class="n">feature</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
<p>We run GD starting from <span class="math notranslate nohighlight">\((0,0)\)</span> with a step size computed from the smoothness of the objective as above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">stepsize</span> <span class="o">=</span> <span class="n">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stepsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.017671625306319678
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">best_x</span> <span class="o">=</span> <span class="n">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">stepsize</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.90959003 -0.05890828]
</pre></div>
</div>
</div>
</div>
<p>Finally we plot the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">feature_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)),</span><span class="n">grid</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">predict_grid</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">feature_grid</span> <span class="o">@</span> <span class="n">best_x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span><span class="n">predict_grid</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png" src="../Images/b5fe636fb5ab3cb9e6e07cac190911dd.png" data-original-src="https://mmids-textbook.github.io/_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png"/>
</div>
</div>
</section>
</section>
&#13;

<h2><span class="section-number">3.8.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">3.8.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_opt_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">3.8.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_2.html">Section 3.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_3.html">Section 3.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_4.html">Section 3.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_5.html">Section 3.5</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_6.html">Section 3.6</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">3.8.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">3.8.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E3.2.1:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(x_1, x_2) &amp;= \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}\right) \\
&amp;= (6x_1 - 2x_2 - 5, -2x_1 + 8x_2 + 2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, -1)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(1, -1) = (6(1) - 2(-1) - 5, -2(1) + 8(-1) + 2) = (3, -8).
\]</div>
<p>Answer and justification for E3.2.3:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1} &amp;= \cos(x_1) \cos(x_2), \\
\frac{\partial f}{\partial x_2} &amp;= -\sin(x_1) \sin(x_2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((\frac{\pi}{4}, \frac{\pi}{3})\)</span>, we have:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= \cos(\frac{\pi}{4}) \cos(\frac{\pi}{3}) = \frac{\sqrt{2}}{2} \cdot \frac{1}{2} = \frac{\sqrt{2}}{4}, \\
\frac{\partial f}{\partial x_2}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= -\sin(\frac{\pi}{4}) \sin(\frac{\pi}{3}) = -\frac{\sqrt{2}}{2} \cdot \frac{\sqrt{3}}{2} = -\frac{\sqrt{6}}{4}.
\end{align*}\]</div>
<p>Answer and justification for E3.2.5: The Hessian matrix of <span class="math notranslate nohighlight">\(f(x_1, x_2)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
6x_1 &amp; 6x_2 \\
6x_2 &amp; 6x_1 - 12x_2
\end{pmatrix}.
\end{split}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, 2)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(1, 2) = \begin{pmatrix}
6 &amp; 12 \\
12 &amp; -18
\end{pmatrix}.
\end{split}\]</div>
<p>We can see that <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial x_1 \partial x_2}(1, 2) = \frac{\partial^2 f}{\partial x_2 \partial x_1}(1, 2) = 12\)</span>, confirming the Symmetry of the Hessian Theorem.</p>
<p>Answer and justification for E3.2.7:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial^2 f}{\partial x_1^2} &amp;= 2 \sin(x_2), \\
\frac{\partial^2 f}{\partial x_1 \partial x_2} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2^2} &amp;= -x_1^2 \sin(x_2).
\end{align*}\]</div>
<p>Answer and justification for E3.2.9: The Hessian matrix is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2, x_3) = \begin{pmatrix}
2 &amp; -2 &amp; 4 \\
-2 &amp; 4 &amp; -6 \\
4 &amp; -6 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.11: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = 3x^2y^2 - 2y^3\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = 2x^3y - 6xy^2 + 4y^3\)</span>, obtained by differentiating <span class="math notranslate nohighlight">\(f\)</span> with respect to each variable while holding the other constant.</p>
<p>Answer and justification for E3.2.13: The Hessian matrix is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_g(x, y) = \begin{pmatrix}
-\sin(x) \cos(y) &amp; -\cos(x) \sin(y) \\
-\cos(x) \sin(y) &amp; -\sin(x) \cos(y)
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.15: <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial x^2} = 6x\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial y^2} = -6x\)</span>. Adding these gives <span class="math notranslate nohighlight">\(6x - 6x = 0\)</span>, so <span class="math notranslate nohighlight">\(q\)</span> satisfies Laplace’s equation.</p>
<p>Answer and justification for E3.2.17: By the chain rule, the rate of change of temperature experienced by the particle is</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(t)) = \nabla u(\mathbf{c}(t))^T \mathbf{c}'(t).
\]</div>
<p>We have <span class="math notranslate nohighlight">\(\nabla u(x, y) = (-2xe^{-x^2 - y^2}, -2ye^{-x^2 - y^2})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}'(t) = (2t, 3t^2)\)</span>. Evaluating at <span class="math notranslate nohighlight">\(t = 1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(1)) = (-2e^{-2}, -2e^{-2})^T (2, 3) = -10e^{-2}.
\]</div>
<p>Answer and justification for E3.2.19: <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = 2t \cos t - t^2 \sin t\)</span>. Justification: <span class="math notranslate nohighlight">\(\nabla f = (y, x)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{g}'(t) = (2t, -\sin t)\)</span>. Then, <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = \cos t \cdot 2t + t^2 \cdot (-\sin t)\)</span>.</p>
<p>Answer and justification for E3.3.1: <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (2x_1, 4x_2)\)</span>. Setting this equal to zero yields <span class="math notranslate nohighlight">\(2x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(4x_2 = 0\)</span>, which implies <span class="math notranslate nohighlight">\(x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(x_2 = 0\)</span>. Thus, the only point where <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = 0\)</span> is <span class="math notranslate nohighlight">\((0, 0)\)</span>.</p>
<p>Answer and justification for E3.3.3: The second directional derivative is given by <span class="math notranslate nohighlight">\(\frac{\partial^2 f(\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \mathbf{v}\)</span>. We have <span class="math notranslate nohighlight">\(\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix}\)</span>. Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial^2 f(1, 1)}{\partial \mathbf{v}^2} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})^T \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix} (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) = \frac{1}{2}(2 + 2 + 2 + 2) = 4.
\end{split}\]</div>
<p>Answer and justification for E3.3.5: The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla_{x_1, x_2} L(x_1, x_2, \lambda) &amp;= 0, \\
h(x_1, x_2) &amp;= 0.
\end{align*}\]</div>
<p>Computing the gradients:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial L}{\partial x_1} &amp;= 2x_1 + \lambda = 0, \\
\frac{\partial L}{\partial x_2} &amp;= 2x_2 + \lambda = 0, \\
x_1 + x_2 &amp;= 1.
\end{align*}\]</div>
<p>From the first two equations, we have <span class="math notranslate nohighlight">\(x_1 = x_2 = -\frac{\lambda}{2}\)</span>. Substituting into the third equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - \frac{\lambda}{2} = 1 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = x_2 = \frac{1}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, \frac{1}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.7: The Lagrangian is <span class="math notranslate nohighlight">\(L(x_1, x_2, x_3, \lambda) = x_1^2 + x_2^2 + x_3^2 + \lambda(x_1 + 2x_2 + 3x_3 - 6)\)</span>. The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2x_1 + \lambda &amp;= 0, \\
2x_2 + 2\lambda &amp;= 0, \\
2x_3 + 3\lambda &amp;= 0, \\
x_1 + 2x_2 + 3x_3 &amp;= 6.
\end{align*}\]</div>
<p>From the first three equations, we have <span class="math notranslate nohighlight">\(x_1 = -\frac{\lambda}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = -\lambda\)</span>, <span class="math notranslate nohighlight">\(x_3 = -\frac{3\lambda}{2}\)</span>. Substituting into the fourth equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - 2\lambda - \frac{9\lambda}{2} = 6 \implies -6\lambda = 6 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = \frac{1}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = 1\)</span>, <span class="math notranslate nohighlight">\(x_3 = \frac{3}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, 1, \frac{3}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.9: The gradient of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (3x_1^2 - 3x_2^2, -6x_1x_2)\)</span>. At the point <span class="math notranslate nohighlight">\((1, 0)\)</span>, the gradient is <span class="math notranslate nohighlight">\(\nabla f(1, 0) = (3, 0)\)</span>. The directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((1, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(\mathbf{v} = (1, 1)\)</span> is <span class="math notranslate nohighlight">\(\nabla f(1, 0)^T \mathbf{v} = (3, 0)^T (1, 1) = 3\)</span>. Since this is positive, <span class="math notranslate nohighlight">\(v\)</span> is not a descent direction.</p>
<p>Answer and justification for E3.3.11: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2) = \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}.
\end{split}\]</div>
<p>Therefore, the second directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((0, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(v = (1, 0)\)</span> is <span class="math notranslate nohighlight">\(\mathbf{v}^T \mathbf{H}_f(0, 0) \mathbf{v} = (1, 0) \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = -2\)</span>.</p>
<p>Answer and justification for E3.3.13: The Hessian matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(1,1) = \begin{pmatrix}
6 &amp; -3 \\
-3 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Justification: Compute the second partial derivatives:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f}{\partial x^2} = 6x, \quad \frac{\partial^2 f}{\partial y^2} = 6y, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x} = -3.
\]</div>
<p>At <span class="math notranslate nohighlight">\((1,1)\)</span>, these values are <span class="math notranslate nohighlight">\(6\)</span>, <span class="math notranslate nohighlight">\(6\)</span>, and <span class="math notranslate nohighlight">\(-3\)</span>, respectively.</p>
<p>Answer and justification for E3.4.1: The convex combination is:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, 3) + \alpha(4, 5) = 0.7(2, 3) + 0.3(4, 5) = (0.7 \cdot 2 + 0.3 \cdot 4, 0.7 \cdot 3 + 0.3 \cdot 5) = (2.6, 3.6).
\]</div>
<p>Answer and justification for E3.4.3: <span class="math notranslate nohighlight">\(S_1\)</span> and <span class="math notranslate nohighlight">\(S_2\)</span> are halfspaces, which are convex sets. By the lemma in the text, the intersection of convex sets is also convex. Therefore, <span class="math notranslate nohighlight">\(S_1 \cap S_2\)</span> is a convex set.</p>
<p>Answer and justification for E3.4.5: The function <span class="math notranslate nohighlight">\(f\)</span> is a quadratic function with <span class="math notranslate nohighlight">\(P = 2\)</span>, <span class="math notranslate nohighlight">\(q = 2\)</span>, and <span class="math notranslate nohighlight">\(r = 1\)</span>. Since <span class="math notranslate nohighlight">\(P &gt; 0\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is strictly convex. The unique global minimizer is found by setting the gradient to zero:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(x) = 2x + 2 = 0 \implies x^* = -1.
\]</div>
<p>Answer and justification for E3.4.7: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla^2 f(x, y) = \begin{pmatrix}
2 &amp; 0 \\
0 &amp; 4
\end{pmatrix}.
\end{split}\]</div>
<p>For any <span class="math notranslate nohighlight">\((x, y) \in \mathbb{R}^2\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 f(x, y) \succeq 2I_{2 \times 2},
\]</div>
<p>so <span class="math notranslate nohighlight">\(f\)</span> is strongly convex with <span class="math notranslate nohighlight">\(m = 2\)</span>.</p>
<p>Answer and justification for E3.4.9: <span class="math notranslate nohighlight">\(f\)</span> is not convex. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2 - 4\)</span>, which is negative for <span class="math notranslate nohighlight">\(x \in (-\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}})\)</span>. Since the second derivative is not always nonnegative, <span class="math notranslate nohighlight">\(f\)</span> is not convex.</p>
<p>Answer and justification for E3.4.11: <span class="math notranslate nohighlight">\(f\)</span> is strongly convex. We have <span class="math notranslate nohighlight">\(f''(x) = 2 &gt; 0\)</span>, so <span class="math notranslate nohighlight">\(f\)</span> is 2-strongly convex.</p>
<p>Answer and justification for E3.4.13: <span class="math notranslate nohighlight">\(D\)</span> is convex. To show this, let <span class="math notranslate nohighlight">\((x_1, y_1), (x_2, y_2) \in D\)</span> and <span class="math notranslate nohighlight">\(\alpha \in (0, 1)\)</span>. We need to show that <span class="math notranslate nohighlight">\((1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) \in D\)</span>. Compute:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) = ((1 - \alpha)x_1 + \alpha x_2, (1 - \alpha)y_1 + \alpha y_2).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
((1 - \alpha)x_1 + \alpha x_2)^2 + ((1 - \alpha)y_1 + \alpha y_2)^2 &lt; (1 - \alpha)(x_1^2 + y_1^2) + \alpha(x_2^2 + y_2^2) &lt; 4.
\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(D\)</span> is convex.</p>
<p>Answer and justification for E3.4.15: <span class="math notranslate nohighlight">\(D\)</span> is not convex. For example, let <span class="math notranslate nohighlight">\((x_1, y_1) = (2, \sqrt{3})\)</span> and <span class="math notranslate nohighlight">\((x_2, y_2) = (2, -\sqrt{3})\)</span>, both of which are in <span class="math notranslate nohighlight">\(D\)</span>. For <span class="math notranslate nohighlight">\(\alpha = \frac{1}{2}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, \sqrt{3}) + \alpha(2, -\sqrt{3}) = \left(2, 0\right).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
\left(2\right)^2 - \left(0\right)^2 = 4 &gt; 1.
\]</div>
<p>Answer and justification for E3.5.1: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 8y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 8)\)</span>. The direction of steepest descent is <span class="math notranslate nohighlight">\(-\nabla f(1, 1) = (-2, -8)\)</span>.</p>
<p>Answer and justification for E3.5.3: <span class="math notranslate nohighlight">\(\nabla f(x) = 3x^2 - 12x + 9\)</span>. At <span class="math notranslate nohighlight">\(x_0 = 0\)</span>, <span class="math notranslate nohighlight">\(\nabla f(0) = 9\)</span>. The first iteration gives <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 0 - 0.1 \cdot 9 = -0.9\)</span>. At <span class="math notranslate nohighlight">\(x_1 = -0.9\)</span>, <span class="math notranslate nohighlight">\(\nabla f(-0.9) = 3 \cdot (-0.9)^2 - 12 \cdot (-0.9) + 9 = 2.43 + 10.8 + 9 = 22.23\)</span>. The second iteration gives <span class="math notranslate nohighlight">\(x_2 = x_1 - \alpha \nabla f(x_1) = -0.9 - 0.1 \cdot 22.23 = -3.123\)</span>.</p>
<p>Answer and justification for E3.5.5: We have <span class="math notranslate nohighlight">\(\nabla f(x) = 2x\)</span>, so <span class="math notranslate nohighlight">\(\nabla f(2) = 4\)</span>. Thus, the gradient descent update is <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 2 - 0.1 \cdot 4 = 1.6\)</span>.</p>
<p>Answer and justification for E3.5.7: <span class="math notranslate nohighlight">\(f''(x) = 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Therefore, <span class="math notranslate nohighlight">\(f''(x) \geq 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which implies that <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(4\)</span>-strongly convex.</p>
<p>Answer and justification for E3.5.9: No. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2\)</span>, which can be arbitrarily large as <span class="math notranslate nohighlight">\(x\)</span> increases. Thus, there is no constant <span class="math notranslate nohighlight">\(L\)</span> such that <span class="math notranslate nohighlight">\(-L \le f''(x) \le L\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>.</p>
<p>Answer and justification for E3.5.11: We have <span class="math notranslate nohighlight">\(f''(x) = 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Thus, we can take <span class="math notranslate nohighlight">\(m = 2\)</span>, and we have <span class="math notranslate nohighlight">\(f''(x) \ge 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which is the condition for <span class="math notranslate nohighlight">\(m\)</span>-strong convexity.</p>
<p>Answer and justification for E3.5.13: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 2y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 2)\)</span>. The first update is <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span>. The gradient at <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span> is <span class="math notranslate nohighlight">\((1.6, 1.6)\)</span>. The second update is <span class="math notranslate nohighlight">\((0.64, 0.64)\)</span>.</p>
<p>Answer and justification for E3.6.1: The log-odds is given by <span class="math notranslate nohighlight">\(\log \frac{p}{1-p} = \log \frac{0.25}{0.75} = \log \frac{1}{3} = -\log 3\)</span>.</p>
<p>Answer and justification for E3.6.3:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\alpha}^T \mathbf{x} = (-0.2 \cdot 1) + (0.4 \cdot 3) = -0.2 + 1.2 = 1.0
\]</div>
<div class="math notranslate nohighlight">
\[
p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(1.0) = \frac{1}{1 + e^{-1}} \approx 0.731
\]</div>
<p>Answer and justification for E3.6.5: By the quotient rule,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sigma'(z) &amp;= \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\
&amp;= \sigma(z) (1 - \sigma(z)).
\end{align*}\]</div>
<p>Answer and justification for E3.6.7: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}) \approx 0.05\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}) \approx -0.73\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}) \approx 0.73\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \ell(\mathbf{x}; A, b) &amp;= -\frac{1}{3} \sum_{i=1}^3 (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \boldsymbol{\alpha}_i \\
&amp;\approx -\frac{1}{3} \{(0.05)(1, 2) + (-0.73)(-1, 1) + 0.73(0, -1)\}.
\end{align*}\]</div>
<p>Answer and justification for E3.6.9: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}^0) = 1 - \sigma(0) = 0.5\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}^0) = -0.5\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}^0) = 0.5\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{x}^1 &amp;= \mathbf{x}^0 - \beta \nabla \ell(\mathbf{x}^0; A, \mathbf{b}) \\
&amp;= (0, 0) + \frac{0.1}{3} \{0.5(1, 2) + (-0.5)(-1, 1) + 0.5(0, -1)\} \\
&amp;= (0.1, 0.05).
\end{align*}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">3.8.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define and calculate partial derivatives and gradients for functions of several variables.</p></li>
<li><p>Compute second-order partial derivatives and construct the Hessian matrix for twice continuously differentiable functions.</p></li>
<li><p>Apply the Chain Rule to compute derivatives of composite functions of several variables.</p></li>
<li><p>State and prove the multivariable version of the Mean Value Theorem using the Chain Rule.</p></li>
<li><p>Calculate gradients and Hessians for affine and quadratic functions of several variables.</p></li>
<li><p>Define global and local minimizers for unconstrained optimization problems.</p></li>
<li><p>Derive the first-order necessary conditions for a local minimizer using the gradient.</p></li>
<li><p>Explain the concept of a descent direction and its relation to the gradient.</p></li>
<li><p>Explain the concept of directional derivatives and compute directional derivatives using the gradient.</p></li>
<li><p>State the second-order necessary and sufficient conditions for a local minimizer using the Hessian matrix.</p></li>
<li><p>Compute the gradient and Hessian matrix for a given multivariable function.</p></li>
<li><p>Formulate optimization problems with equality constraints.</p></li>
<li><p>Apply the method of Lagrange multipliers to derive the first-order necessary conditions for a constrained local minimizer.</p></li>
<li><p>Verify the second-order sufficient conditions for a constrained local minimizer using the Hessian matrix and Lagrange multipliers.</p></li>
<li><p>Analyze the regularity condition in the context of constrained optimization problems.</p></li>
<li><p>Solve a constrained optimization problem by finding points that satisfy the first-order necessary conditions and checking the second-order sufficient conditions.</p></li>
<li><p>Define convex sets and convex functions, and provide examples of each.</p></li>
<li><p>Identify operations that preserve convexity of sets and functions.</p></li>
<li><p>Characterize convex functions using the first-order convexity condition based on the gradient.</p></li>
<li><p>Determine the convexity of a function using the second-order convexity condition based on the Hessian matrix.</p></li>
<li><p>Explain the relationship between convexity and optimization, particularly how local minimizers of convex functions are also global minimizers.</p></li>
<li><p>State and prove the first-order optimality condition for convex functions on R^d and on convex sets.</p></li>
<li><p>Define strong convexity and its implications for the existence and uniqueness of global minimizers.</p></li>
<li><p>Analyze the convexity and strong convexity of quadratic functions and least-squares objectives.</p></li>
<li><p>Apply the concept of convexity to solve optimization problems, such as finding the projection of a point onto a convex set.</p></li>
<li><p>Define gradient descent and explain its motivation as a numerical optimization method.</p></li>
<li><p>Prove that the negative gradient is the steepest descent direction for a continuously differentiable function.</p></li>
<li><p>Analyze the convergence of gradient descent for smooth functions, proving that it produces a sequence of points with decreasing objective values and vanishing gradients.</p></li>
<li><p>Derive the convergence rate of gradient descent for smooth functions in terms of the number of iterations.</p></li>
<li><p>Define strong convexity for twice continuously differentiable functions and relate it to the function value and gradient.</p></li>
<li><p>Prove faster convergence rates for gradient descent when applied to smooth and strongly convex functions, showing exponential convergence to the global minimum.</p></li>
<li><p>Implement gradient descent in Python and apply it to simple examples to illustrate the theoretical convergence results.</p></li>
<li><p>Explain the role of the sigmoid function in transforming a linear function of features into a probability.</p></li>
<li><p>Derive the gradient and Hessian of the logistic regression objective function (cross-entropy loss).</p></li>
<li><p>Prove that the logistic regression objective function is convex and smooth.</p></li>
<li><p>Implement gradient descent to minimize the logistic regression objective function in Python.</p></li>
<li><p>Apply logistic regression to real-world datasets.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
&#13;

<h3><span class="section-number">3.8.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_opt_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
&#13;

<h3><span class="section-number">3.8.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_2.html">Section 3.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_3.html">Section 3.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_4.html">Section 3.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_5.html">Section 3.5</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_6.html">Section 3.6</a></p></li>
</ul>
&#13;

<h3><span class="section-number">3.8.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
&#13;

<h3><span class="section-number">3.8.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E3.2.1:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(x_1, x_2) &amp;= \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}\right) \\
&amp;= (6x_1 - 2x_2 - 5, -2x_1 + 8x_2 + 2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, -1)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(1, -1) = (6(1) - 2(-1) - 5, -2(1) + 8(-1) + 2) = (3, -8).
\]</div>
<p>Answer and justification for E3.2.3:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1} &amp;= \cos(x_1) \cos(x_2), \\
\frac{\partial f}{\partial x_2} &amp;= -\sin(x_1) \sin(x_2).
\end{align*}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((\frac{\pi}{4}, \frac{\pi}{3})\)</span>, we have:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f}{\partial x_1}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= \cos(\frac{\pi}{4}) \cos(\frac{\pi}{3}) = \frac{\sqrt{2}}{2} \cdot \frac{1}{2} = \frac{\sqrt{2}}{4}, \\
\frac{\partial f}{\partial x_2}(\frac{\pi}{4}, \frac{\pi}{3}) &amp;= -\sin(\frac{\pi}{4}) \sin(\frac{\pi}{3}) = -\frac{\sqrt{2}}{2} \cdot \frac{\sqrt{3}}{2} = -\frac{\sqrt{6}}{4}.
\end{align*}\]</div>
<p>Answer and justification for E3.2.5: The Hessian matrix of <span class="math notranslate nohighlight">\(f(x_1, x_2)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
6x_1 &amp; 6x_2 \\
6x_2 &amp; 6x_1 - 12x_2
\end{pmatrix}.
\end{split}\]</div>
<p>At the point <span class="math notranslate nohighlight">\((1, 2)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(1, 2) = \begin{pmatrix}
6 &amp; 12 \\
12 &amp; -18
\end{pmatrix}.
\end{split}\]</div>
<p>We can see that <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial x_1 \partial x_2}(1, 2) = \frac{\partial^2 f}{\partial x_2 \partial x_1}(1, 2) = 12\)</span>, confirming the Symmetry of the Hessian Theorem.</p>
<p>Answer and justification for E3.2.7:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial^2 f}{\partial x_1^2} &amp;= 2 \sin(x_2), \\
\frac{\partial^2 f}{\partial x_1 \partial x_2} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp;= 2x_1 \cos(x_2), \\
\frac{\partial^2 f}{\partial x_2^2} &amp;= -x_1^2 \sin(x_2).
\end{align*}\]</div>
<p>Answer and justification for E3.2.9: The Hessian matrix is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2, x_3) = \begin{pmatrix}
2 &amp; -2 &amp; 4 \\
-2 &amp; 4 &amp; -6 \\
4 &amp; -6 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.11: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = 3x^2y^2 - 2y^3\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = 2x^3y - 6xy^2 + 4y^3\)</span>, obtained by differentiating <span class="math notranslate nohighlight">\(f\)</span> with respect to each variable while holding the other constant.</p>
<p>Answer and justification for E3.2.13: The Hessian matrix is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_g(x, y) = \begin{pmatrix}
-\sin(x) \cos(y) &amp; -\cos(x) \sin(y) \\
-\cos(x) \sin(y) &amp; -\sin(x) \cos(y)
\end{pmatrix}.
\end{split}\]</div>
<p>Answer and justification for E3.2.15: <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial x^2} = 6x\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial^2 q}{\partial y^2} = -6x\)</span>. Adding these gives <span class="math notranslate nohighlight">\(6x - 6x = 0\)</span>, so <span class="math notranslate nohighlight">\(q\)</span> satisfies Laplace’s equation.</p>
<p>Answer and justification for E3.2.17: By the chain rule, the rate of change of temperature experienced by the particle is</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(t)) = \nabla u(\mathbf{c}(t))^T \mathbf{c}'(t).
\]</div>
<p>We have <span class="math notranslate nohighlight">\(\nabla u(x, y) = (-2xe^{-x^2 - y^2}, -2ye^{-x^2 - y^2})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}'(t) = (2t, 3t^2)\)</span>. Evaluating at <span class="math notranslate nohighlight">\(t = 1\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dt}u(\mathbf{c}(1)) = (-2e^{-2}, -2e^{-2})^T (2, 3) = -10e^{-2}.
\]</div>
<p>Answer and justification for E3.2.19: <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = 2t \cos t - t^2 \sin t\)</span>. Justification: <span class="math notranslate nohighlight">\(\nabla f = (y, x)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{g}'(t) = (2t, -\sin t)\)</span>. Then, <span class="math notranslate nohighlight">\(\frac{d}{dt} f(\mathbf{g}(t)) = \cos t \cdot 2t + t^2 \cdot (-\sin t)\)</span>.</p>
<p>Answer and justification for E3.3.1: <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (2x_1, 4x_2)\)</span>. Setting this equal to zero yields <span class="math notranslate nohighlight">\(2x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(4x_2 = 0\)</span>, which implies <span class="math notranslate nohighlight">\(x_1 = 0\)</span> and <span class="math notranslate nohighlight">\(x_2 = 0\)</span>. Thus, the only point where <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = 0\)</span> is <span class="math notranslate nohighlight">\((0, 0)\)</span>.</p>
<p>Answer and justification for E3.3.3: The second directional derivative is given by <span class="math notranslate nohighlight">\(\frac{\partial^2 f(\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \mathbf{v}\)</span>. We have <span class="math notranslate nohighlight">\(\mathbf{H}_f(x_1, x_2) = \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix}\)</span>. Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial^2 f(1, 1)}{\partial \mathbf{v}^2} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})^T \begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix} (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) = \frac{1}{2}(2 + 2 + 2 + 2) = 4.
\end{split}\]</div>
<p>Answer and justification for E3.3.5: The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla_{x_1, x_2} L(x_1, x_2, \lambda) &amp;= 0, \\
h(x_1, x_2) &amp;= 0.
\end{align*}\]</div>
<p>Computing the gradients:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial L}{\partial x_1} &amp;= 2x_1 + \lambda = 0, \\
\frac{\partial L}{\partial x_2} &amp;= 2x_2 + \lambda = 0, \\
x_1 + x_2 &amp;= 1.
\end{align*}\]</div>
<p>From the first two equations, we have <span class="math notranslate nohighlight">\(x_1 = x_2 = -\frac{\lambda}{2}\)</span>. Substituting into the third equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - \frac{\lambda}{2} = 1 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = x_2 = \frac{1}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, \frac{1}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.7: The Lagrangian is <span class="math notranslate nohighlight">\(L(x_1, x_2, x_3, \lambda) = x_1^2 + x_2^2 + x_3^2 + \lambda(x_1 + 2x_2 + 3x_3 - 6)\)</span>. The first-order necessary conditions are:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2x_1 + \lambda &amp;= 0, \\
2x_2 + 2\lambda &amp;= 0, \\
2x_3 + 3\lambda &amp;= 0, \\
x_1 + 2x_2 + 3x_3 &amp;= 6.
\end{align*}\]</div>
<p>From the first three equations, we have <span class="math notranslate nohighlight">\(x_1 = -\frac{\lambda}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = -\lambda\)</span>, <span class="math notranslate nohighlight">\(x_3 = -\frac{3\lambda}{2}\)</span>. Substituting into the fourth equation:</p>
<div class="math notranslate nohighlight">
\[
-\frac{\lambda}{2} - 2\lambda - \frac{9\lambda}{2} = 6 \implies -6\lambda = 6 \implies \lambda = -1.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(x_1 = \frac{1}{2}\)</span>, <span class="math notranslate nohighlight">\(x_2 = 1\)</span>, <span class="math notranslate nohighlight">\(x_3 = \frac{3}{2}\)</span>, and the only point satisfying the first-order necessary conditions is <span class="math notranslate nohighlight">\((\frac{1}{2}, 1, \frac{3}{2}, -1)\)</span>.</p>
<p>Answer and justification for E3.3.9: The gradient of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\nabla f(x_1, x_2) = (3x_1^2 - 3x_2^2, -6x_1x_2)\)</span>. At the point <span class="math notranslate nohighlight">\((1, 0)\)</span>, the gradient is <span class="math notranslate nohighlight">\(\nabla f(1, 0) = (3, 0)\)</span>. The directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((1, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(\mathbf{v} = (1, 1)\)</span> is <span class="math notranslate nohighlight">\(\nabla f(1, 0)^T \mathbf{v} = (3, 0)^T (1, 1) = 3\)</span>. Since this is positive, <span class="math notranslate nohighlight">\(v\)</span> is not a descent direction.</p>
<p>Answer and justification for E3.3.11: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(x_1, x_2) = \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}.
\end{split}\]</div>
<p>Therefore, the second directional derivative of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\((0, 0)\)</span> in the direction <span class="math notranslate nohighlight">\(v = (1, 0)\)</span> is <span class="math notranslate nohighlight">\(\mathbf{v}^T \mathbf{H}_f(0, 0) \mathbf{v} = (1, 0) \begin{bmatrix} -2 &amp; 0 \\ 0 &amp; -2 \end{bmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = -2\)</span>.</p>
<p>Answer and justification for E3.3.13: The Hessian matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(1,1) = \begin{pmatrix}
6 &amp; -3 \\
-3 &amp; 6
\end{pmatrix}.
\end{split}\]</div>
<p>Justification: Compute the second partial derivatives:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f}{\partial x^2} = 6x, \quad \frac{\partial^2 f}{\partial y^2} = 6y, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x} = -3.
\]</div>
<p>At <span class="math notranslate nohighlight">\((1,1)\)</span>, these values are <span class="math notranslate nohighlight">\(6\)</span>, <span class="math notranslate nohighlight">\(6\)</span>, and <span class="math notranslate nohighlight">\(-3\)</span>, respectively.</p>
<p>Answer and justification for E3.4.1: The convex combination is:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, 3) + \alpha(4, 5) = 0.7(2, 3) + 0.3(4, 5) = (0.7 \cdot 2 + 0.3 \cdot 4, 0.7 \cdot 3 + 0.3 \cdot 5) = (2.6, 3.6).
\]</div>
<p>Answer and justification for E3.4.3: <span class="math notranslate nohighlight">\(S_1\)</span> and <span class="math notranslate nohighlight">\(S_2\)</span> are halfspaces, which are convex sets. By the lemma in the text, the intersection of convex sets is also convex. Therefore, <span class="math notranslate nohighlight">\(S_1 \cap S_2\)</span> is a convex set.</p>
<p>Answer and justification for E3.4.5: The function <span class="math notranslate nohighlight">\(f\)</span> is a quadratic function with <span class="math notranslate nohighlight">\(P = 2\)</span>, <span class="math notranslate nohighlight">\(q = 2\)</span>, and <span class="math notranslate nohighlight">\(r = 1\)</span>. Since <span class="math notranslate nohighlight">\(P &gt; 0\)</span>, <span class="math notranslate nohighlight">\(f\)</span> is strictly convex. The unique global minimizer is found by setting the gradient to zero:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(x) = 2x + 2 = 0 \implies x^* = -1.
\]</div>
<p>Answer and justification for E3.4.7: The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla^2 f(x, y) = \begin{pmatrix}
2 &amp; 0 \\
0 &amp; 4
\end{pmatrix}.
\end{split}\]</div>
<p>For any <span class="math notranslate nohighlight">\((x, y) \in \mathbb{R}^2\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 f(x, y) \succeq 2I_{2 \times 2},
\]</div>
<p>so <span class="math notranslate nohighlight">\(f\)</span> is strongly convex with <span class="math notranslate nohighlight">\(m = 2\)</span>.</p>
<p>Answer and justification for E3.4.9: <span class="math notranslate nohighlight">\(f\)</span> is not convex. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2 - 4\)</span>, which is negative for <span class="math notranslate nohighlight">\(x \in (-\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}})\)</span>. Since the second derivative is not always nonnegative, <span class="math notranslate nohighlight">\(f\)</span> is not convex.</p>
<p>Answer and justification for E3.4.11: <span class="math notranslate nohighlight">\(f\)</span> is strongly convex. We have <span class="math notranslate nohighlight">\(f''(x) = 2 &gt; 0\)</span>, so <span class="math notranslate nohighlight">\(f\)</span> is 2-strongly convex.</p>
<p>Answer and justification for E3.4.13: <span class="math notranslate nohighlight">\(D\)</span> is convex. To show this, let <span class="math notranslate nohighlight">\((x_1, y_1), (x_2, y_2) \in D\)</span> and <span class="math notranslate nohighlight">\(\alpha \in (0, 1)\)</span>. We need to show that <span class="math notranslate nohighlight">\((1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) \in D\)</span>. Compute:</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) = ((1 - \alpha)x_1 + \alpha x_2, (1 - \alpha)y_1 + \alpha y_2).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
((1 - \alpha)x_1 + \alpha x_2)^2 + ((1 - \alpha)y_1 + \alpha y_2)^2 &lt; (1 - \alpha)(x_1^2 + y_1^2) + \alpha(x_2^2 + y_2^2) &lt; 4.
\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(D\)</span> is convex.</p>
<p>Answer and justification for E3.4.15: <span class="math notranslate nohighlight">\(D\)</span> is not convex. For example, let <span class="math notranslate nohighlight">\((x_1, y_1) = (2, \sqrt{3})\)</span> and <span class="math notranslate nohighlight">\((x_2, y_2) = (2, -\sqrt{3})\)</span>, both of which are in <span class="math notranslate nohighlight">\(D\)</span>. For <span class="math notranslate nohighlight">\(\alpha = \frac{1}{2}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
(1 - \alpha)(2, \sqrt{3}) + \alpha(2, -\sqrt{3}) = \left(2, 0\right).
\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[
\left(2\right)^2 - \left(0\right)^2 = 4 &gt; 1.
\]</div>
<p>Answer and justification for E3.5.1: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 8y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 8)\)</span>. The direction of steepest descent is <span class="math notranslate nohighlight">\(-\nabla f(1, 1) = (-2, -8)\)</span>.</p>
<p>Answer and justification for E3.5.3: <span class="math notranslate nohighlight">\(\nabla f(x) = 3x^2 - 12x + 9\)</span>. At <span class="math notranslate nohighlight">\(x_0 = 0\)</span>, <span class="math notranslate nohighlight">\(\nabla f(0) = 9\)</span>. The first iteration gives <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 0 - 0.1 \cdot 9 = -0.9\)</span>. At <span class="math notranslate nohighlight">\(x_1 = -0.9\)</span>, <span class="math notranslate nohighlight">\(\nabla f(-0.9) = 3 \cdot (-0.9)^2 - 12 \cdot (-0.9) + 9 = 2.43 + 10.8 + 9 = 22.23\)</span>. The second iteration gives <span class="math notranslate nohighlight">\(x_2 = x_1 - \alpha \nabla f(x_1) = -0.9 - 0.1 \cdot 22.23 = -3.123\)</span>.</p>
<p>Answer and justification for E3.5.5: We have <span class="math notranslate nohighlight">\(\nabla f(x) = 2x\)</span>, so <span class="math notranslate nohighlight">\(\nabla f(2) = 4\)</span>. Thus, the gradient descent update is <span class="math notranslate nohighlight">\(x_1 = x_0 - \alpha \nabla f(x_0) = 2 - 0.1 \cdot 4 = 1.6\)</span>.</p>
<p>Answer and justification for E3.5.7: <span class="math notranslate nohighlight">\(f''(x) = 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Therefore, <span class="math notranslate nohighlight">\(f''(x) \geq 4\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which implies that <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(4\)</span>-strongly convex.</p>
<p>Answer and justification for E3.5.9: No. We have <span class="math notranslate nohighlight">\(f''(x) = 12x^2\)</span>, which can be arbitrarily large as <span class="math notranslate nohighlight">\(x\)</span> increases. Thus, there is no constant <span class="math notranslate nohighlight">\(L\)</span> such that <span class="math notranslate nohighlight">\(-L \le f''(x) \le L\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>.</p>
<p>Answer and justification for E3.5.11: We have <span class="math notranslate nohighlight">\(f''(x) = 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. Thus, we can take <span class="math notranslate nohighlight">\(m = 2\)</span>, and we have <span class="math notranslate nohighlight">\(f''(x) \ge 2\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>, which is the condition for <span class="math notranslate nohighlight">\(m\)</span>-strong convexity.</p>
<p>Answer and justification for E3.5.13: The gradient is <span class="math notranslate nohighlight">\(\nabla f(x, y) = (2x, 2y)\)</span>. At <span class="math notranslate nohighlight">\((1, 1)\)</span>, it is <span class="math notranslate nohighlight">\((2, 2)\)</span>. The first update is <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span>. The gradient at <span class="math notranslate nohighlight">\((0.8, 0.8)\)</span> is <span class="math notranslate nohighlight">\((1.6, 1.6)\)</span>. The second update is <span class="math notranslate nohighlight">\((0.64, 0.64)\)</span>.</p>
<p>Answer and justification for E3.6.1: The log-odds is given by <span class="math notranslate nohighlight">\(\log \frac{p}{1-p} = \log \frac{0.25}{0.75} = \log \frac{1}{3} = -\log 3\)</span>.</p>
<p>Answer and justification for E3.6.3:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\alpha}^T \mathbf{x} = (-0.2 \cdot 1) + (0.4 \cdot 3) = -0.2 + 1.2 = 1.0
\]</div>
<div class="math notranslate nohighlight">
\[
p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(1.0) = \frac{1}{1 + e^{-1}} \approx 0.731
\]</div>
<p>Answer and justification for E3.6.5: By the quotient rule,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sigma'(z) &amp;= \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\
&amp;= \sigma(z) (1 - \sigma(z)).
\end{align*}\]</div>
<p>Answer and justification for E3.6.7: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}) \approx 0.05\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}) \approx -0.73\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}) \approx 0.73\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \ell(\mathbf{x}; A, b) &amp;= -\frac{1}{3} \sum_{i=1}^3 (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \boldsymbol{\alpha}_i \\
&amp;\approx -\frac{1}{3} \{(0.05)(1, 2) + (-0.73)(-1, 1) + 0.73(0, -1)\}.
\end{align*}\]</div>
<p>Answer and justification for E3.6.9: We have <span class="math notranslate nohighlight">\(b_1 - \sigma(\boldsymbol{\alpha}_1^T \mathbf{x}^0) = 1 - \sigma(0) = 0.5\)</span>, <span class="math notranslate nohighlight">\(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x}^0) = -0.5\)</span>, <span class="math notranslate nohighlight">\(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}^0) = 0.5\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{x}^1 &amp;= \mathbf{x}^0 - \beta \nabla \ell(\mathbf{x}^0; A, \mathbf{b}) \\
&amp;= (0, 0) + \frac{0.1}{3} \{0.5(1, 2) + (-0.5)(-1, 1) + 0.5(0, -1)\} \\
&amp;= (0.1, 0.05).
\end{align*}\]</div>
&#13;

<h3><span class="section-number">3.8.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define and calculate partial derivatives and gradients for functions of several variables.</p></li>
<li><p>Compute second-order partial derivatives and construct the Hessian matrix for twice continuously differentiable functions.</p></li>
<li><p>Apply the Chain Rule to compute derivatives of composite functions of several variables.</p></li>
<li><p>State and prove the multivariable version of the Mean Value Theorem using the Chain Rule.</p></li>
<li><p>Calculate gradients and Hessians for affine and quadratic functions of several variables.</p></li>
<li><p>Define global and local minimizers for unconstrained optimization problems.</p></li>
<li><p>Derive the first-order necessary conditions for a local minimizer using the gradient.</p></li>
<li><p>Explain the concept of a descent direction and its relation to the gradient.</p></li>
<li><p>Explain the concept of directional derivatives and compute directional derivatives using the gradient.</p></li>
<li><p>State the second-order necessary and sufficient conditions for a local minimizer using the Hessian matrix.</p></li>
<li><p>Compute the gradient and Hessian matrix for a given multivariable function.</p></li>
<li><p>Formulate optimization problems with equality constraints.</p></li>
<li><p>Apply the method of Lagrange multipliers to derive the first-order necessary conditions for a constrained local minimizer.</p></li>
<li><p>Verify the second-order sufficient conditions for a constrained local minimizer using the Hessian matrix and Lagrange multipliers.</p></li>
<li><p>Analyze the regularity condition in the context of constrained optimization problems.</p></li>
<li><p>Solve a constrained optimization problem by finding points that satisfy the first-order necessary conditions and checking the second-order sufficient conditions.</p></li>
<li><p>Define convex sets and convex functions, and provide examples of each.</p></li>
<li><p>Identify operations that preserve convexity of sets and functions.</p></li>
<li><p>Characterize convex functions using the first-order convexity condition based on the gradient.</p></li>
<li><p>Determine the convexity of a function using the second-order convexity condition based on the Hessian matrix.</p></li>
<li><p>Explain the relationship between convexity and optimization, particularly how local minimizers of convex functions are also global minimizers.</p></li>
<li><p>State and prove the first-order optimality condition for convex functions on R^d and on convex sets.</p></li>
<li><p>Define strong convexity and its implications for the existence and uniqueness of global minimizers.</p></li>
<li><p>Analyze the convexity and strong convexity of quadratic functions and least-squares objectives.</p></li>
<li><p>Apply the concept of convexity to solve optimization problems, such as finding the projection of a point onto a convex set.</p></li>
<li><p>Define gradient descent and explain its motivation as a numerical optimization method.</p></li>
<li><p>Prove that the negative gradient is the steepest descent direction for a continuously differentiable function.</p></li>
<li><p>Analyze the convergence of gradient descent for smooth functions, proving that it produces a sequence of points with decreasing objective values and vanishing gradients.</p></li>
<li><p>Derive the convergence rate of gradient descent for smooth functions in terms of the number of iterations.</p></li>
<li><p>Define strong convexity for twice continuously differentiable functions and relate it to the function value and gradient.</p></li>
<li><p>Prove faster convergence rates for gradient descent when applied to smooth and strongly convex functions, showing exponential convergence to the global minimum.</p></li>
<li><p>Implement gradient descent in Python and apply it to simple examples to illustrate the theoretical convergence results.</p></li>
<li><p>Explain the role of the sigmoid function in transforming a linear function of features into a probability.</p></li>
<li><p>Derive the gradient and Hessian of the logistic regression objective function (cross-entropy loss).</p></li>
<li><p>Prove that the logistic regression objective function is convex and smooth.</p></li>
<li><p>Implement gradient descent to minimize the logistic regression objective function in Python.</p></li>
<li><p>Apply logistic regression to real-world datasets.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
&#13;

<h2><span class="section-number">3.8.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="logistic-regression-illustration-of-convergence-result">
<h3><span class="section-number">3.8.2.1. </span>Logistic regression: illustration of convergence result<a class="headerlink" href="#logistic-regression-illustration-of-convergence-result" title="Link to this heading">#</a></h3>
<p>We return to our proof of convergence for smooth functions using a special case of logistic regression. We first define the functions <span class="math notranslate nohighlight">\(\hat{f}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial}{\partial x}\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="p">)</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">b</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We illustrate GD on a random dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the upper and lower bounds in the <em>Quadratic Bound for Smooth Functions</em> around <span class="math notranslate nohighlight">\(x = x_0\)</span>. It turns out we can take <span class="math notranslate nohighlight">\(L=1\)</span> because all features are uniformly random between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Observe that minimizing the upper quadratic bound leads to a decrease in <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x0</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.3</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span><span class="n">x0</span><span class="o">+</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">lower</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'upper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'lower'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png" src="../Images/097a341c91c4cf50443a8e9f81f433b8.png" data-original-src="https://mmids-textbook.github.io/_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png"/>
</div>
</div>
</section>
<section id="logistic-regression-another-dataset">
<h3><span class="section-number">3.8.2.2. </span>Logistic regression: another dataset<a class="headerlink" href="#logistic-regression-another-dataset" title="Link to this heading">#</a></h3>
<p>Recall that to run gradient descent, we first implement a function computing a descent update. It takes as input a function <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> computing the gradient itself, as well as a current iterate and a step size. We now also feed a dataset as additional input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">curr_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">curr_x</span> <span class="o">-</span> <span class="n">beta</span><span class="o">*</span><span class="n">gradient</span>

<span class="k">def</span> <span class="nf">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)):</span>
    <span class="n">curr_x</span> <span class="o">=</span> <span class="n">init_x</span>
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">curr_x</span> <span class="o">=</span> <span class="n">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">curr_x</span>

<span class="k">def</span> <span class="nf">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">L</span>
</pre></div>
</div>
</div>
</div>
<p>We analyze with a simple dataset from UC Berkeley’s <a class="reference external" href="http://www.ds100.org">DS100</a> course. The file <code class="docutils literal notranslate"><span class="pre">lebron.csv</span></code> is available <a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets">here</a>. Quoting a previous version of the course’s textbook:</p>
<blockquote>
<div><p>In basketball, players score by shooting a ball through a hoop. One such player, LeBron James, is widely considered one of the best basketball players ever for his incredible ability to score. LeBron plays in the National Basketball Association (NBA), the United States’s premier basketball league. We’ve collected a dataset of all of LeBron’s attempts in the 2017 NBA Playoff Games using the NBA statistics website (<a class="reference external" href="https://stats.nba.com/">https://stats.nba.com/</a>).</p>
</div></blockquote>
<p>We first load the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'lebron.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>game_date</th>
      <th>minute</th>
      <th>opponent</th>
      <th>action_type</th>
      <th>shot_type</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20170415</td>
      <td>10</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20170415</td>
      <td>11</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20170415</td>
      <td>14</td>
      <td>IND</td>
      <td>Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20170415</td>
      <td>15</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20170415</td>
      <td>18</td>
      <td>IND</td>
      <td>Alley Oop Dunk Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>game_date</th>
      <th>minute</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3.840000e+02</td>
      <td>384.00000</td>
      <td>384.000000</td>
      <td>384.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.017052e+07</td>
      <td>24.40625</td>
      <td>10.695312</td>
      <td>0.565104</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.948501e+01</td>
      <td>13.67304</td>
      <td>10.547586</td>
      <td>0.496390</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2.017042e+07</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.017050e+07</td>
      <td>13.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.017052e+07</td>
      <td>25.00000</td>
      <td>6.500000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.017060e+07</td>
      <td>35.00000</td>
      <td>23.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.017061e+07</td>
      <td>48.00000</td>
      <td>31.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The two columns we will be interested in are <code class="docutils literal notranslate"><span class="pre">shot_distance</span></code> (LeBron’s distance from the basket when the shot was attempted (ft)) and <code class="docutils literal notranslate"><span class="pre">shot_made</span></code> (0 if the shot missed, 1 if the shot went in). As the summary table above indicates, the average distance was <code class="docutils literal notranslate"><span class="pre">10.6953</span></code> and the frequency of shots made was <code class="docutils literal notranslate"><span class="pre">0.565104</span></code>. We extract those two columns and display them on a scatter plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">feature</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'shot_distance'</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'shot_made'</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png" src="../Images/3d7f7643b1fbf4e975e00b74d1d83929.png" data-original-src="https://mmids-textbook.github.io/_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png"/>
</div>
</div>
<p>As you can see, this kind of data is hard to vizualize because of the superposition of points with the same <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>-values. One trick is to jiggle the <span class="math notranslate nohighlight">\(y\)</span>’s a little bit by adding Gaussian noise. We do this next and plot again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">label_jitter</span> <span class="o">=</span> <span class="n">label</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png" src="../Images/8ed222a6929c72dd22f117b3334ede4e.png" data-original-src="https://mmids-textbook.github.io/_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png"/>
</div>
</div>
<p>We apply GD to logistic regression. We first construct the data matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. To allow an affine function of the features, we add a column of <span class="math notranslate nohighlight">\(1\)</span>’s as we have done before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)),</span><span class="n">feature</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
<p>We run GD starting from <span class="math notranslate nohighlight">\((0,0)\)</span> with a step size computed from the smoothness of the objective as above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">stepsize</span> <span class="o">=</span> <span class="n">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stepsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.017671625306319678
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">best_x</span> <span class="o">=</span> <span class="n">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">stepsize</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.90959003 -0.05890828]
</pre></div>
</div>
</div>
</div>
<p>Finally we plot the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">feature_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)),</span><span class="n">grid</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">predict_grid</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">feature_grid</span> <span class="o">@</span> <span class="n">best_x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span><span class="n">predict_grid</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png" src="../Images/b5fe636fb5ab3cb9e6e07cac190911dd.png" data-original-src="https://mmids-textbook.github.io/_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png"/>
</div>
</div>
</section>
&#13;

<h3><span class="section-number">3.8.2.1. </span>Logistic regression: illustration of convergence result<a class="headerlink" href="#logistic-regression-illustration-of-convergence-result" title="Link to this heading">#</a></h3>
<p>We return to our proof of convergence for smooth functions using a special case of logistic regression. We first define the functions <span class="math notranslate nohighlight">\(\hat{f}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial}{\partial x}\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="p">)</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">b</span> <span class="o">-</span> <span class="n">fhat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We illustrate GD on a random dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the upper and lower bounds in the <em>Quadratic Bound for Smooth Functions</em> around <span class="math notranslate nohighlight">\(x = x_0\)</span>. It turns out we can take <span class="math notranslate nohighlight">\(L=1\)</span> because all features are uniformly random between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Observe that minimizing the upper quadratic bound leads to a decrease in <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x0</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.3</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span><span class="n">x0</span><span class="o">+</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">lower</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'upper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'lower'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png" src="../Images/097a341c91c4cf50443a8e9f81f433b8.png" data-original-src="https://mmids-textbook.github.io/_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png"/>
</div>
</div>
&#13;

<h3><span class="section-number">3.8.2.2. </span>Logistic regression: another dataset<a class="headerlink" href="#logistic-regression-another-dataset" title="Link to this heading">#</a></h3>
<p>Recall that to run gradient descent, we first implement a function computing a descent update. It takes as input a function <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> computing the gradient itself, as well as a current iterate and a step size. We now also feed a dataset as additional input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">curr_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">curr_x</span> <span class="o">-</span> <span class="n">beta</span><span class="o">*</span><span class="n">gradient</span>

<span class="k">def</span> <span class="nf">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)):</span>
    <span class="n">curr_x</span> <span class="o">=</span> <span class="n">init_x</span>
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">curr_x</span> <span class="o">=</span> <span class="n">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">curr_x</span>

<span class="k">def</span> <span class="nf">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">L</span>
</pre></div>
</div>
</div>
</div>
<p>We analyze with a simple dataset from UC Berkeley’s <a class="reference external" href="http://www.ds100.org">DS100</a> course. The file <code class="docutils literal notranslate"><span class="pre">lebron.csv</span></code> is available <a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets">here</a>. Quoting a previous version of the course’s textbook:</p>
<blockquote>
<div><p>In basketball, players score by shooting a ball through a hoop. One such player, LeBron James, is widely considered one of the best basketball players ever for his incredible ability to score. LeBron plays in the National Basketball Association (NBA), the United States’s premier basketball league. We’ve collected a dataset of all of LeBron’s attempts in the 2017 NBA Playoff Games using the NBA statistics website (<a class="reference external" href="https://stats.nba.com/">https://stats.nba.com/</a>).</p>
</div></blockquote>
<p>We first load the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'lebron.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>game_date</th>
      <th>minute</th>
      <th>opponent</th>
      <th>action_type</th>
      <th>shot_type</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20170415</td>
      <td>10</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20170415</td>
      <td>11</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20170415</td>
      <td>14</td>
      <td>IND</td>
      <td>Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20170415</td>
      <td>15</td>
      <td>IND</td>
      <td>Driving Layup Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20170415</td>
      <td>18</td>
      <td>IND</td>
      <td>Alley Oop Dunk Shot</td>
      <td>2PT Field Goal</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>game_date</th>
      <th>minute</th>
      <th>shot_distance</th>
      <th>shot_made</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3.840000e+02</td>
      <td>384.00000</td>
      <td>384.000000</td>
      <td>384.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.017052e+07</td>
      <td>24.40625</td>
      <td>10.695312</td>
      <td>0.565104</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.948501e+01</td>
      <td>13.67304</td>
      <td>10.547586</td>
      <td>0.496390</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2.017042e+07</td>
      <td>1.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.017050e+07</td>
      <td>13.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.017052e+07</td>
      <td>25.00000</td>
      <td>6.500000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.017060e+07</td>
      <td>35.00000</td>
      <td>23.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.017061e+07</td>
      <td>48.00000</td>
      <td>31.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The two columns we will be interested in are <code class="docutils literal notranslate"><span class="pre">shot_distance</span></code> (LeBron’s distance from the basket when the shot was attempted (ft)) and <code class="docutils literal notranslate"><span class="pre">shot_made</span></code> (0 if the shot missed, 1 if the shot went in). As the summary table above indicates, the average distance was <code class="docutils literal notranslate"><span class="pre">10.6953</span></code> and the frequency of shots made was <code class="docutils literal notranslate"><span class="pre">0.565104</span></code>. We extract those two columns and display them on a scatter plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">feature</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'shot_distance'</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'shot_made'</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png" src="../Images/3d7f7643b1fbf4e975e00b74d1d83929.png" data-original-src="https://mmids-textbook.github.io/_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png"/>
</div>
</div>
<p>As you can see, this kind of data is hard to vizualize because of the superposition of points with the same <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>-values. One trick is to jiggle the <span class="math notranslate nohighlight">\(y\)</span>’s a little bit by adding Gaussian noise. We do this next and plot again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">label_jitter</span> <span class="o">=</span> <span class="n">label</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png" src="../Images/8ed222a6929c72dd22f117b3334ede4e.png" data-original-src="https://mmids-textbook.github.io/_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png"/>
</div>
</div>
<p>We apply GD to logistic regression. We first construct the data matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. To allow an affine function of the features, we add a column of <span class="math notranslate nohighlight">\(1\)</span>’s as we have done before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)),</span><span class="n">feature</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
<p>We run GD starting from <span class="math notranslate nohighlight">\((0,0)\)</span> with a step size computed from the smoothness of the objective as above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">stepsize</span> <span class="o">=</span> <span class="n">stepsize_for_logreg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stepsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.017671625306319678
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">best_x</span> <span class="o">=</span> <span class="n">gd_for_logreg</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">stepsize</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.90959003 -0.05890828]
</pre></div>
</div>
</div>
</div>
<p>Finally we plot the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">feature_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)),</span><span class="n">grid</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">predict_grid</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">feature_grid</span> <span class="o">@</span> <span class="n">best_x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">label_jitter</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span><span class="n">predict_grid</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png" src="../Images/b5fe636fb5ab3cb9e6e07cac190911dd.png" data-original-src="https://mmids-textbook.github.io/_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png"/>
</div>
</div>
    
</body>
</html>