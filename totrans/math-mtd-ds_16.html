<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>2.7. Online supplementary materials#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>2.7. Online supplementary materials#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap02_ls/supp/roch-mmids-ls-supp.html">https://mmids-textbook.github.io/chap02_ls/supp/roch-mmids-ls-supp.html</a></blockquote>

<section id="quizzes-solutions-code-etc">
<h2><span class="section-number">2.7.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">2.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_ls_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">2.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_2.html">Section 2.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_3.html">Section 2.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_4.html">Section 2.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_5.html">Section 2.5</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">2.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-ls-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-ls-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">2.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E2.2.1: Yes, <span class="math notranslate nohighlight">\(U\)</span> is a linear subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. Let <span class="math notranslate nohighlight">\(u_1 = (x_1, y_1, z_1), u_2 = (x_2, y_2, z_2) \in U\)</span> and <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2y_1 - z_1 &amp;= 0 \\
x_2 + 2y_2 - z_2 &amp;= 0 \\
\alpha(x_1 + 2y_1 - z_1) + (x_2 + 2y_2 - z_2) &amp;= 0 \\
(\alpha x_1 + x_2) + 2(\alpha y_1 + y_2) - (\alpha z_1 + z_2) &amp;= 0
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\alpha u_1 + u_2 \in U\)</span>, proving <span class="math notranslate nohighlight">\(U\)</span> is a linear subspace.</p>
<p>Answer and justification for E2.2.3: One basis for <span class="math notranslate nohighlight">\(U\)</span> is <span class="math notranslate nohighlight">\(\{(1, 1, 0), (-1, 0, 1)\}\)</span>. Any vector <span class="math notranslate nohighlight">\((x, y, z) \in U\)</span> can be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(x, y, z) &amp;= (y-z, y, z) \\
&amp;= y(1, 1, 0) + z(-1, 0, 1)
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\{(1, 1, 0), (-1, 0, 1)\}\)</span> spans <span class="math notranslate nohighlight">\(U\)</span>. They are also linearly independent, as <span class="math notranslate nohighlight">\(\alpha(1, 1, 0) + \beta(-1, 0, 1) = \mathbf{0}\)</span> implies <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>. Thus, this is a basis for <span class="math notranslate nohighlight">\(U\)</span>.</p>
<p>Answer and justification for E2.2.5: Yes, <span class="math notranslate nohighlight">\(u_1\)</span> and <span class="math notranslate nohighlight">\(u_2\)</span> form an orthonormal list. We have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|u_1\| &amp;= \sqrt{(1/\sqrt{2})^2 + (1/\sqrt{2})^2} = 1 \\
\|u_2\| &amp;= \sqrt{(1/\sqrt{2})^2 + (-1/\sqrt{2})^2} = 1 \\
\langle u_1, u_2 \rangle &amp;= (1/\sqrt{2})(1/\sqrt{2}) + (1/\sqrt{2})(-1/\sqrt{2}) = 0
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(u_1\)</span> and <span class="math notranslate nohighlight">\(u_2\)</span> are unit vectors and are orthogonal to each other.</p>
<p>Answer and justification for E2.2.7: <span class="math notranslate nohighlight">\(A\)</span> is nonsingular. Its columns are <span class="math notranslate nohighlight">\((1, 3)\)</span> and <span class="math notranslate nohighlight">\((2, 4)\)</span>, which are linearly independent</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\alpha(1, 3) + \beta(2, 4) &amp;= (0, 0) \\
\alpha + 2\beta &amp;= 0 \\
3\alpha + 4\beta &amp;= 0
\end{align*}\]</div>
<p>This system has only the trivial solution <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>. So the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent, and since <span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix, this means it has rank 2 and is nonsingular.</p>
<p>Answer and justification for E2.2.9:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} = \alpha \mathbf{w}_1 + \beta \mathbf{w}_2 \implies (2, 3, 5) = \alpha (1, 0, 1) + \beta (0, 1, 1).
\]</div>
<p>Solving the system of equations</p>
<div class="math notranslate nohighlight">
\[
2 = \alpha, \quad 3 = \beta, \quad 5 = \alpha + \beta.
\]</div>
<p>Substitute <span class="math notranslate nohighlight">\(\alpha = 2\)</span> and <span class="math notranslate nohighlight">\(\beta = 3\)</span> into the third equation</p>
<div class="math notranslate nohighlight">
\[
5 = 2 + 3 \implies \alpha = 2, \beta = 3.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{v} = 2\mathbf{w}_1 + 3\mathbf{w}_2\)</span>.</p>
<p>Answer and justification for E2.2.11: To find the null space, we need to solve the homogeneous system <span class="math notranslate nohighlight">\(B\mathbf{x} = \mathbf{0}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\end{split}\]</div>
<p>This gives us two equations with three unknowns:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2x_2 + 3x_3 &amp;= 0 \tag{1}\\
4x_1 + 5x_2 + 6x_3 &amp;= 0 \tag{2}
\end{align*}\]</div>
<p>Multiplying equation (1) by 4:</p>
<div class="math notranslate nohighlight">
\[
4x_1 + 8x_2 + 12x_3 = 0 \tag{3}
\]</div>
<p>Subtracting equation (3) from equation (2):</p>
<div class="math notranslate nohighlight">
\[
(5-8)x_2 + (6-12)x_3 = 0
\]</div>
<p>This simplifies to:</p>
<div class="math notranslate nohighlight">
\[
-3x_2 - 6x_3 = 0
\]</div>
<p>Dividing by -3:
$<span class="math notranslate nohighlight">\(
x_2 = -2x_3
\)</span>$</p>
<p>Now, substituting this back into equation (1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2(-2x_3) + 3x_3 &amp;= 0 \\
x_1 - 4x_3 + 3x_3 &amp;= 0 \\
x_1 - x_3 &amp;= 0
\end{align*}\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[
x_1 = x_3
\]</div>
<p>Setting <span class="math notranslate nohighlight">\(x_3 = t\)</span> (a free parameter), we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} t \\ -2t \\ t \end{pmatrix} = t \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}
\end{split}\]</div>
<p>The null space of <span class="math notranslate nohighlight">\(B\)</span> is the span of the vector <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.2.13: To determine linear independence, we need to check if the only solution to <span class="math notranslate nohighlight">\(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \alpha_3\mathbf{u}_3 = \mathbf{0}\)</span> is <span class="math notranslate nohighlight">\(\alpha_1 = \alpha_2 = \alpha_3 = 0\)</span>.</p>
<p>Let’s write out this vector equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\alpha_1 \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} + \alpha_2 \begin{pmatrix} 2 \\ -1 \\ 0 \end{pmatrix} + \alpha_3 \begin{pmatrix} 1 \\ 8 \\ 6 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
\end{split}\]</div>
<p>This gives us a system of three equations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\alpha_1 + 2\alpha_2 + \alpha_3 &amp;= 0 \tag{1}\\
2\alpha_1 - \alpha_2 + 8\alpha_3 &amp;= 0 \tag{2}\\
3\alpha_1 + 0\alpha_2 + 6\alpha_3 &amp;= 0 \tag{3}
\end{align*}\]</div>
<p>From equation (3):</p>
<div class="math notranslate nohighlight">
\[
3\alpha_1 + 6\alpha_3 = 0
\]</div>
<p>Which gives us:</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 = -2\alpha_3 \tag{4}
\]</div>
<p>Substituting this into equation (1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-2\alpha_3 + 2\alpha_2 + \alpha_3 &amp;= 0\\
2\alpha_2 - \alpha_3 &amp;= 0\\
\alpha_2 &amp;= \frac{\alpha_3}{2} \tag{5}
\end{align*}\]</div>
<p>Now let’s check equation (2) by substituting equations (4) and (5):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2(-2\alpha_3) - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
-4\alpha_3 - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
\end{align*}\]</div>
<p>Simplifying:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-4\alpha_3 - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
\frac{-8\alpha_3 - \alpha_3 + 16\alpha_3}{2} &amp;= 0\\
\frac{7\alpha_3}{2} &amp;= 0
\end{align*}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\frac{7\alpha_3}{2} = 0\)</span> implies <span class="math notranslate nohighlight">\(\alpha_3 = 0\)</span>, and from equations (4) and (5), we get <span class="math notranslate nohighlight">\(\alpha_1 = 0\)</span> and <span class="math notranslate nohighlight">\(\alpha_2 = 0\)</span>.</p>
<p>Therefore, the only solution to the system is the trivial solution <span class="math notranslate nohighlight">\(\alpha_1 = \alpha_2 = \alpha_3 = 0\)</span>, which means the vectors <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>, <span class="math notranslate nohighlight">\(\mathbf{u}_2\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{u}_3\)</span> are linearly independent.</p>
<p>Consider the equation <span class="math notranslate nohighlight">\(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \alpha_3\mathbf{u}_3 = 0\)</span>. This leads to the system of equations</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 + 2\alpha_2 + \alpha_3 = 0
\]</div>
<div class="math notranslate nohighlight">
\[
2\alpha_1 - \alpha_2 + 8\alpha_3 = 0
\]</div>
<div class="math notranslate nohighlight">
\[
3\alpha_1  + 6\alpha_3 = 0
\]</div>
<p>Solving this system, we find that <span class="math notranslate nohighlight">\(\alpha_1 = -2\alpha_3\)</span> and <span class="math notranslate nohighlight">\(\alpha_2 = \alpha_3\)</span>. Choosing <span class="math notranslate nohighlight">\(\alpha_3 = 1\)</span>, we get a non-trivial solution <span class="math notranslate nohighlight">\(\alpha_1 = -2\)</span>, <span class="math notranslate nohighlight">\(\alpha_2 = 1\)</span>, <span class="math notranslate nohighlight">\(\alpha_3 = 1\)</span>. Therefore, the vectors are linearly dependent.</p>
<p>Answer and justification for E2.2.15: We can write this system as <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b}\)</span>, where
<span class="math notranslate nohighlight">\(A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{b} = \begin{bmatrix} 3 \\ 1 \end{bmatrix}\)</span>. Since <span class="math notranslate nohighlight">\(\det(A) = -3 \neq 0\)</span>, <span class="math notranslate nohighlight">\(A\)</span> is invertible. We find that <span class="math notranslate nohighlight">\(A^{-1} = \frac{1}{-3} \begin{bmatrix} -1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix}\)</span>.</p>
<p>Then, the solution is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{x} = A^{-1}\mathbf{b} = \frac{1}{-3} \begin{bmatrix} -1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix} \begin{bmatrix} 3 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{4}{3} \\ -\frac{1}{3} \end{bmatrix}.
\end{split}\]</div>
<p>Answer and justification for E2.3.1: While</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q^T Q = \begin{pmatrix} 1 &amp; 0\\ 0 &amp; 1 \end{pmatrix} = I_{2 \times 2},
\end{split}\]</div>
<p>the matrix <span class="math notranslate nohighlight">\(Q\)</span> is not square. Therefore, <span class="math notranslate nohighlight">\(Q\)</span> is not an orthogonal matrix.</p>
<p>Answer and justification for E2.3.3:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_{U} \mathbf{v} = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\|\mathbf{u}\|^2} \mathbf{u} = \frac{(2 \cdot 1 + 3 \cdot 1)}{(1^2 + 1^2)} \mathbf{u} = \frac{5}{2} (1, 1) = \left(\frac{5}{2}, \frac{5}{2}\right).
\]</div>
<p>Answer and justification for E2.3.5: The projection of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> onto <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{proj}_{\mathbf{u}} \mathbf{v} = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{\|\mathbf{u}\|^2} \mathbf{u} = \frac{(1)(1) + (1)(2) + (0)(1)}{1^2 + 1^2 + 0^2} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \frac{3}{2} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} \frac{3}{2} \\ \frac{3}{2} \\ 0 \end{bmatrix}.
\end{split}\]</div>
<p>Answer and justification for E2.3.7: Using the orthonormal basis <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> for <span class="math notranslate nohighlight">\(U\)</span> from the text, we have:
$<span class="math notranslate nohighlight">\(\mathrm{proj}_U \mathbf{v} = \langle \mathbf{v}, \mathbf{q}_1 \rangle \mathbf{q}_1 + \langle \mathbf{v}, \mathbf{q}_2 \rangle \mathbf{q}_2 = \frac{4}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}}(1, 0, 1) + \frac{5}{\sqrt{6}} \cdot \frac{1}{\sqrt{6}}(-1, 2, 1) = (\frac{7}{3}, \frac{10}{3}, \frac{13}{3}).\)</span>$</p>
<p>Answer and justification for E2.3.9: From E2.3.7 and E2.3.8, we have:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_U \mathbf{v} = (\frac{7}{3}, \frac{10}{3}, \frac{13}{3}), \quad \mathbf{v} - \mathrm{proj}_U \mathbf{v} = (-\frac{1}{3}, -\frac{1}{3}, \frac{2}{3}).
\]</div>
<p>Computing the squared norms:</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{v}\|^2 = 1^2 + 2^2 + 3^2 = 14, \quad \|\mathrm{proj}_U \mathbf{v}\|^2 = (\frac{7}{3})^2 + (\frac{10}{3})^2 + (\frac{13}{3})^2 = \frac{146}{3}, \quad \|\mathbf{v} - \mathrm{proj}_U \mathbf{v}\|^2 = (-\frac{1}{3})^2 + (-\frac{1}{3})^2 + (\frac{2}{3})^2 = \frac{2}{3}.
\]</div>
<p>Indeed, <span class="math notranslate nohighlight">\(\|\mathbf{v}\|^2 = 14 = \frac{146}{3} + \frac{2}{3} = \|\mathrm{proj}_U \mathbf{v}\|^2 + \|\mathbf{v} - \mathrm{proj}_U \mathbf{v}\|^2\)</span>, verifying the Pythagorean theorem.</p>
<p>Answer and justification for E2.3.11: Since <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> is not a unit vector, we first normalize it: <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{u}_1}{\|\mathbf{u}_1\|} = \frac{1}{3}\begin{pmatrix} 2 \\ 1 \\ -2 \end{pmatrix}\)</span>. Then, <span class="math notranslate nohighlight">\(\mathrm{proj}_{\mathbf{u}_1} \mathbf{u}_2 = \langle \mathbf{u}_2, \mathbf{q}_1 \rangle \mathbf{q}_1 = \frac{1}{9} \begin{pmatrix} 2 \\ 1 \\ -2 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.3.13: We want to find all vectors <span class="math notranslate nohighlight">\(\begin{pmatrix} x \\ y \\ z \end{pmatrix}\)</span> such that <span class="math notranslate nohighlight">\(\begin{pmatrix} x \\ y \\ z \end{pmatrix} \cdot \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} = 0\)</span>. This gives us the equation <span class="math notranslate nohighlight">\(x + y = 0\)</span>, or <span class="math notranslate nohighlight">\(x = -y\)</span>. Thus, any vector of the form <span class="math notranslate nohighlight">\(\begin{pmatrix} -y \\ y \\ z \end{pmatrix} = y\begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} + z\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\)</span> is in <span class="math notranslate nohighlight">\(W^\perp\)</span>. So, a basis for <span class="math notranslate nohighlight">\(W^\perp\)</span> is <span class="math notranslate nohighlight">\(\left\{\begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\right\}\)</span>.</p>
<p>Answer and justification for E2.3.15:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T A = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}^T \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}, \quad A^T \mathbf{b} = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}^T \begin{pmatrix} 3 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 2 \end{pmatrix}.
\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{x} = (A^T A)^{-1} A^T \mathbf{b} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}^{-1} \begin{pmatrix} 4 \\ 2 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.1: <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|} = (1, 0)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}_2 = \mathbf{a}_2 - \langle \mathbf{q}_1, \mathbf{a}_2 \rangle \mathbf{q}_1 = (0, 1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|} = (0, 1)\)</span>.</p>
<p>Answer and justification for E2.4.3: Let <span class="math notranslate nohighlight">\(\mathbf{w}_1 = (1, 1)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_2 = (1, 0)\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_1 &amp;= \frac{\mathbf{w}_1}{\|\mathbf{w}_1\|} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) \\
\mathbf{q}_2 &amp;= \frac{\mathbf{w}_2 - \langle \mathbf{w}_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\mathbf{w}_2 - \langle \mathbf{w}_2, q_1 \rangle q_1\|} \\
&amp;= \frac{(1, 0) - (\frac{1}{\sqrt{2}})(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})}{\sqrt{1 - (\frac{1}{\sqrt{2}})^2}} \\
&amp;= (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> is an orthonormal basis.</p>
<p>Answer and justification for E2.4.5: Using the orthonormal basis <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> from E2.4.4, we have <span class="math notranslate nohighlight">\(Q = [\mathbf{q}_1\ \mathbf{q}_2]\)</span>. To find <span class="math notranslate nohighlight">\(R\)</span>, we observe that <span class="math notranslate nohighlight">\(\mathbf{a}_1 = \sqrt{3}\mathbf{q}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{a}_2 = \frac{1}{\sqrt{3}}\mathbf{q}_1 + \frac{5}{\sqrt{21}}\mathbf{q}_2\)</span>. Therefore, <span class="math notranslate nohighlight">\(R = \begin{pmatrix} \sqrt{3} &amp; \frac{1}{\sqrt{3}} \\ 0 &amp; \frac{5}{\sqrt{21}} \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.7: <span class="math notranslate nohighlight">\(\mathbf{x} = (2, 1)\)</span>. From the second equation, <span class="math notranslate nohighlight">\(3x_2 = 3\)</span>, so <span class="math notranslate nohighlight">\(x_2 = 1\)</span>. Substituting into the first equation, <span class="math notranslate nohighlight">\(2x_1 - 1 = 4\)</span>, so <span class="math notranslate nohighlight">\(x_1 = 2\)</span>.</p>
<p>Answer and justification for E2.4.9: <span class="math notranslate nohighlight">\(H = I_{3 \times 3} - 2\mathbf{z}\mathbf{z}^T/\|\mathbf{z}\|^2 = \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.11: To verify that <span class="math notranslate nohighlight">\(H_1\)</span> is orthogonal, we check if <span class="math notranslate nohighlight">\(H_1^T H_1 = I_{2 \times 2}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_1^T H_1 = \begin{pmatrix} \frac{7}{5} &amp; -\frac{6}{5} \\ -\frac{6}{5} &amp; -\frac{1}{5} \end{pmatrix} \begin{pmatrix} \frac{7}{5} &amp; -\frac{6}{5} \\ -\frac{6}{5} &amp; -\frac{1}{5} \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = I_{2 \times 2}.
\end{split}\]</div>
<p>To verify that <span class="math notranslate nohighlight">\(H_1\)</span> is symmetric, we check if <span class="math notranslate nohighlight">\(H_1^T = H_1\)</span>, which is true by observation.</p>
<p>Answer and justification for E2.4.13: We have <span class="math notranslate nohighlight">\(H_1 A = R\)</span>, where <span class="math notranslate nohighlight">\(R = \begin{pmatrix} -\frac{\sqrt{10}}{5} &amp; -\frac{2}{\sqrt{10}} \\ 0 &amp; \frac{14}{5} \end{pmatrix}\)</span> is upper triangular. Therefore, <span class="math notranslate nohighlight">\(Q^T = H_1\)</span>, and <span class="math notranslate nohighlight">\(Q = H_1^T = \begin{pmatrix} \frac{7}{5} &amp; \frac{6}{5} \\ -\frac{6}{5} &amp; \frac{1}{5} \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.15: Let <span class="math notranslate nohighlight">\(\mathbf{y}_1 = (3, 4)^T\)</span> be the first column of <span class="math notranslate nohighlight">\(A\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(2)} - \mathbf{y}_1 = (5, -4)^T\)</span> and <span class="math notranslate nohighlight">\(H_1 = I_{2 \times 2} - 2\mathbf{z}_1\mathbf{z}_1^T / \|\mathbf{z}_1\|^2 = \begin{pmatrix} 3/5 &amp; 4/5 \\ 4/5 &amp; -3/5 \end{pmatrix}\)</span>. We can verify that <span class="math notranslate nohighlight">\(H_1 A = \begin{pmatrix} 5 &amp; 1 \\ 0 &amp; -2/5 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.17:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Q &amp;= \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \end{pmatrix}, \\
R &amp;= Q^T A = \begin{pmatrix} \sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2} \end{pmatrix}, \\
Q^T \mathbf{b} &amp;= \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 2 \\ 0 \end{pmatrix} = \begin{pmatrix} \sqrt{2} \\ \sqrt{2} \end{pmatrix}, \\
R \mathbf{x} &amp;= Q^T \mathbf{b}, \quad \begin{pmatrix} \sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2} \end{pmatrix} \mathbf{x} = \begin{pmatrix} \sqrt{2} \\ \sqrt{2} \end{pmatrix}, \\
\mathbf{x} &amp;= \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\end{align*}\]</div>
<p>The solution is <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.5.1:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{pmatrix} 
1 &amp; 1 \\
1 &amp; 2 \\
1 &amp; 3 \\
1 &amp; 4
\end{pmatrix}, \quad 
\mathbf{y} = \begin{pmatrix}
2 \\
4 \\
5 \\
7
\end{pmatrix}.
\end{split}\]</div>
<p>The normal equations are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T A \boldsymbol{\beta} = A^T \mathbf{y} \Rightarrow
\begin{pmatrix}
4 &amp; 10 \\
10 &amp; 30
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix} =
\begin{pmatrix}
18 \\
47
\end{pmatrix}.
\end{split}\]</div>
<p>Solving this system of equations yields <span class="math notranslate nohighlight">\(\beta_0 = \frac{1}{2}\)</span> and <span class="math notranslate nohighlight">\(\beta_1 = \frac{3}{2}\)</span>.</p>
<p>Answer and justification for E2.5.3:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \end{bmatrix} \quad \mathbf{y} = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix}
\end{split}\]</div>
<p>Answer and justification for E2.5.5:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = (A^T A)^{-1} A^T \mathbf{y} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}
\end{split}\]</div>
<p>We solve the linear system by inverting <span class="math notranslate nohighlight">\(A^T A\)</span> and multiplying by <span class="math notranslate nohighlight">\(A^T \mathbf{y}\)</span>.</p>
<p>Answer and justification for E2.5.7:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; -1 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<p>For a quadratic model, we need columns for <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(x^2\)</span>.</p>
<p>Answer and justification for E2.5.9: We solve the normal equations <span class="math notranslate nohighlight">\(A^T A \boldsymbol{\beta} = A^T \mathbf{y}\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A^T A &amp;= \begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \end{pmatrix} = \begin{pmatrix} 3 &amp; 6 \\ 6 &amp; 14 \end{pmatrix}, \\
A^T \mathbf{y} &amp;= \begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} = \begin{pmatrix} 6 \\ 14 \end{pmatrix}.
\end{align*}\]</div>
<p>Solving <span class="math notranslate nohighlight">\(\begin{pmatrix} 3 &amp; 6 \\ 6 &amp; 14 \end{pmatrix} \boldsymbol{\beta} = \begin{pmatrix} 6 \\ 14 \end{pmatrix}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\boldsymbol{\beta} &amp;= \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
\end{align*}\]</div>
<p>Answer and justification for E2.5.11: Compute the predicted <span class="math notranslate nohighlight">\(y\)</span> values:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{y}_1 &amp;= 2(1) + 1 = 3, \\
\hat{y}_2 &amp;= 2(2) + 1 = 5, \\
\hat{y}_3 &amp;= 2(3) + 1 = 7.
\end{align*}\]</div>
<p>Compute the residuals:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
r_1 &amp;= 3 - 3 = 0, \\
r_2 &amp;= 5 - 5 = 0, \\
r_3 &amp;= 7 - 7 = 0.
\end{align*}\]</div>
<p>RSS is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{RSS} &amp;= 0^2 + 0^2 + 0^2 = 0.
\end{align*}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">2.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define the concepts of linear subspaces, spans, and bases in vector spaces.</p></li>
<li><p>Check the conditions for a set of vectors to be linearly independent or form a basis of a vector space.</p></li>
<li><p>Define the inverse of a nonsingular matrix and prove its uniqueness.</p></li>
<li><p>Define the dimension of a linear subspace.</p></li>
<li><p>State the Pythagorean Theorem.</p></li>
<li><p>Verify whether a given list of vectors is orthonormal.</p></li>
<li><p>Derive the coefficients in an orthonormal basis expansion of a vector.</p></li>
<li><p>Apply the Gram-Schmidt process to transform a basis into an orthonormal basis.</p></li>
<li><p>Prove key theorems related to vector spaces and matrix inverses, such as the Pythagorean theorem for orthogonal vectors and the conditions for linear independence in matrix columns.</p></li>
<li><p>Illustrate examples of linear subspaces and their bases, using both theoretical definitions and numerical examples.</p></li>
<li><p>Define the orthogonal projection of a vector onto a linear subspace and characterize it geometrically.</p></li>
<li><p>Compute the orthogonal projection of a vector onto a linear subspace spanned by an orthonormal list of vectors.</p></li>
<li><p>Illustrate the process of finding the orthogonal projection of a vector onto a subspace using geometric intuition.</p></li>
<li><p>Prove the uniqueness of the orthogonal projection.</p></li>
<li><p>Express the orthogonal projection in matrix form.</p></li>
<li><p>Define the orthogonal complement of a linear subspace and find an orthonormal basis for it.</p></li>
<li><p>State and prove the Orthogonal Decomposition Lemma, and use it to decompose a vector into its orthogonal projection and a vector in the orthogonal complement.</p></li>
<li><p>Formulate the linear least squares problem as an optimization problem and derive the normal equations.</p></li>
<li><p>Apply the concepts of orthogonal projection and orthogonal complement to solve overdetermined systems using the least squares method.</p></li>
<li><p>Determine the uniqueness of the least squares solution based on the linear independence of the columns of the matrix.</p></li>
<li><p>Implement the Gram-Schmidt algorithm to obtain an orthonormal basis from a set of linearly independent vectors.</p></li>
<li><p>Express the Gram-Schmidt algorithm using a matrix factorization perspective, known as the QR decomposition.</p></li>
<li><p>Apply the QR decomposition to solve linear least squares problems as an alternative to the normal equations approach.</p></li>
<li><p>Define Householder reflections and explain their role in introducing zeros below the diagonal of a matrix.</p></li>
<li><p>Construct a QR decomposition using a sequence of Householder transformations.</p></li>
<li><p>Compare the numerical stability of the Gram-Schmidt algorithm and the Householder transformations approach for computing the QR decomposition.</p></li>
<li><p>Formulate the linear regression problem as a linear least squares optimization problem.</p></li>
<li><p>Extend the linear regression model to polynomial regression by incorporating higher-degree terms and interaction terms in the design matrix.</p></li>
<li><p>Recognize and explain the issue of overfitting in polynomial regression and its potential consequences.</p></li>
<li><p>Apply the least squares method to simulated data and real-world datasets, such as the Advertising dataset, to estimate regression coefficients.</p></li>
<li><p>Understand and implement the bootstrap method for linear regression to assess the variability of estimated coefficients and determine the statistical significance of the results.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
</section>
<section id="additional-sections">
<h2><span class="section-number">2.7.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="orthogonality-in-high-dimension">
<h3><span class="section-number">2.7.2.1. </span>Orthogonality in high dimension<a class="headerlink" href="#orthogonality-in-high-dimension" title="Link to this heading">#</a></h3>
<p>In high dimension, orthogonality – or more accurately near-orthogonality – is more common than one might expect. We illustrate this phenomenon here.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. Its joint PDF depends only on the its norm <span class="math notranslate nohighlight">\(\|\mathbf{X}\|\)</span>. So <span class="math notranslate nohighlight">\(\mathbf{Y} = \frac{\mathbf{X}}{\|\mathbf{X}\|}\)</span> is uniformly distributed over the <span class="math notranslate nohighlight">\((d-1)\)</span>-sphere <span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}\in \mathbb{R}^d:\|\mathbf{x}\|=1\}\)</span>, that is, the surface of the unit <span class="math notranslate nohighlight">\(d\)</span>-ball centered aroungd the origin. We write <span class="math notranslate nohighlight">\(\mathbf{Y} \sim \mathrm{U}[\mathcal{S}]\)</span>. The following theorem shows that if we take two independent samples <span class="math notranslate nohighlight">\(\mathbf{Y}_1, \mathbf{Y}_2 \sim \mathrm{U}[\mathcal{S}]\)</span> they are likely to be nearly orthogonal when <span class="math notranslate nohighlight">\(d\)</span> is large, that is, <span class="math notranslate nohighlight">\(|\langle\mathbf{Y}_1, \mathbf{Y}_2\rangle|\)</span> is likely to be small. By symmetry, there is no loss of generality in taking one of the two vectors to be the north pole <span class="math notranslate nohighlight">\(\mathbf{e}_d = (0,\ldots,0,1)\)</span>. A different way to state the theorem is that most of the mass of the <span class="math notranslate nohighlight">\((d-1)\)</span>-sphere is in a small band around the equator.</p>
<p><strong>Figure:</strong> Band around the equator (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:World_map_with_major_latitude_circles.jpg">Source</a>)</p>
<p><img alt="Band around the equator" src="../Images/88d34eed9002aa55ad262c1e2e281d2c.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/World_map_with_major_latitude_circles.jpg/640px-World_map_with_major_latitude_circles.jpg"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>THEOREM</strong> <strong>(Orthogonality in High Dimension)</strong> Let <span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}\in \mathbb{R}^d:\|\mathbf{x}\|=1\}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y} \sim \mathrm{U}[\mathcal{S}]\)</span>. Then for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[|\langle\mathbf{Y}, \mathbf{e}_d\rangle| \geq \varepsilon]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We write <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> in terms of a standard Normal. Its squared norm is a sum of independent random variables. After bringing it to the numerator, we can apply <em>Chebyshev</em>.</p>
<p><em>Proof:</em> Recall that <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is <span class="math notranslate nohighlight">\(\frac{\mathbf{X}}{\|\mathbf{X}\|}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. The probability we want to bound can be re-written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[|\langle\mathbf{Y}, \mathbf{e}_d\rangle| \geq \varepsilon]
&amp;=
\mathbb{P}\left[\left|\left\langle\frac{\mathbf{X}}{\|\mathbf{X}\|}, \mathbf{e}_d\right\rangle\right|^2 \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[\left|\frac{\langle\mathbf{X},\mathbf{e}_d\rangle}{\|\mathbf{X}\|}\right|^2 \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[\frac{X_d^2}{\sum_{j=1}^d X_j^2} \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[X_d^2 \geq \varepsilon^2 \sum_{j=1}^d X_j^2\right]\\
&amp;=
\mathbb{P}\left[\sum_{j=1}^{d-1} (-\varepsilon^2 X_j^2) + (1-\varepsilon^2) X_d^2 \geq 0\right].
\end{align*}\]</div>
<p>We are now looking at a sum of independent (but not identically distributed) random variables</p>
<div class="math notranslate nohighlight">
\[
Z = \sum_{j=1}^{d-1} (-\varepsilon^2 X_j^2) + (1-\varepsilon^2) X_d^2
\]</div>
<p>and we can appeal to our usual Chebyshev machinery. The expectation is, by linearity,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[Z]
= 
- \sum_{j=1}^{d-1} \varepsilon^2 \mathbb{E}[X_j^2] + (1-\varepsilon^2) \mathbb{E}[X_d^2]
= 
\{- (d-1) \,\varepsilon^2  + (1-\varepsilon^2)\}
\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(X_1,\ldots,X_d\)</span> are standard Normal variables and that, in particular, their mean is <span class="math notranslate nohighlight">\(0\)</span> and their variance is <span class="math notranslate nohighlight">\(1\)</span> so that <span class="math notranslate nohighlight">\(\mathbb{E}[X_1^2] = 1\)</span>.</p>
<p>The variance is, by independence of the <span class="math notranslate nohighlight">\(X_j\)</span>’s,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[Z]
&amp;= 
\sum_{j=1}^{d-1} \varepsilon^4 \mathrm{Var}[X_j^2] + (1-\varepsilon^2)^2 \mathrm{Var}[X_d^2]\\
&amp;= 
\{(d-1) \,\varepsilon^4  + (1-\varepsilon^2)^2\}\mathrm{Var}[X_1^2].
\end{align*}\]</div>
<p>So by Chebyshev</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}\left[Z \geq 0\right]
&amp;\leq 
\mathbb{P}\left[\left|Z 
- \mathbb{E}[Z]\right|\geq |\mathbb{E}[Z]|\right]\\
&amp;\leq \frac{\mathrm{Var}[Z]}{\mathbb{E}[Z]^2}\\
&amp;= \frac{\{(d-1) \,\varepsilon^4  + (1-\varepsilon^2)^2\} \mathrm{Var}[X_1^2]}{\{- (d-1) \,\varepsilon^2  + (1-\varepsilon^2)\}^2}\\
&amp;\to 0
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. To get the limit we observed that, for large <span class="math notranslate nohighlight">\(d\)</span>,
the deminator scales like <span class="math notranslate nohighlight">\(d^2\)</span> while the numerator scales only like <span class="math notranslate nohighlight">\(d\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
<section id="bootstrapping-for-linear-regression">
<h3><span class="section-number">2.7.2.2. </span>Bootstrapping for linear regression<a class="headerlink" href="#bootstrapping-for-linear-regression" title="Link to this heading">#</a></h3>
<p>We return to the linear case, but with the full set of predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'advertising.csv'</span><span class="p">)</span>
<span class="n">TV</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'TV'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">sales</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sales'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">TV</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">radio</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'radio'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">newspaper</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'newspaper'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newspaper</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'radio'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'newspaper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/51317ef101b3cee12a03056622483261592dcb9f2c317195483b55e872b586cd.png" src="../Images/d22fa5138c0a40fab66a6fe2d01db90a.png" data-original-src="https://mmids-textbook.github.io/_images/51317ef101b3cee12a03056622483261592dcb9f2c317195483b55e872b586cd.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<p>Newspaper advertising (the last coefficient) seems to have a much weaker effect on sales per dollar spent. Next, we briefly sketch one way to assess the <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a> of such a conclusion.</p>
<p>Our coefficients are estimated from a sample. There is intrinsic variability in our sampling procedure. We would like to understand how our estimated coefficients compare to the true coefficients. This is set up beautifully in [<a class="reference external" href="https://www.inferentialthinking.com/chapters/13/2/Bootstrap.html">Data8</a>, Section 13.2]:</p>
<blockquote>
<div><p>A data scientist is using the data in a random sample to estimate an unknown parameter. She uses the sample to calculate the value of a statistic that she will use as her estimate. Once she has calculated the observed value of her statistic, she could just present it as her estimate and go on her merry way. But she’s a data scientist. She knows that her random sample is just one of numerous possible random samples, and thus her estimate is just one of numerous plausible estimates. By how much could those estimates vary? To answer this, it appears as though she needs to draw another sample from the population, and compute a new estimate based on the new sample. But she doesn’t have the resources to go back to the population and draw another sample. It looks as though the data scientist is stuck. Fortunately, a brilliant idea called <em>the bootstrap</em> can help her out. Since it is not feasible to generate new samples from the population, the bootstrap generates new random samples by a method called <em>resampling</em>: the new samples are drawn at random <em>from the original sample</em>.</p>
</div></blockquote>
<p>Without going into full details (see [<a class="reference external" href="http://www.textbook.ds100.org/ch/17/inf_pred_gen_boot.html">DS100</a>, Section 17.3] for more), it works as follows. Let <span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span> be our data. We assume that our sample is representative of the population and we simulate our sampling procedure by resampling from the sample. That is, we take a random sample with replacement <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1} = \{(\mathbf{x}_i, y_i)\,:\,i \in I\}\)</span> where <span class="math notranslate nohighlight">\(I\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Multiset">multi-set</a> of elements from <span class="math notranslate nohighlight">\([n]\)</span> of size <span class="math notranslate nohighlight">\(n\)</span>. We recompute our estimated coefficients on <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1}\)</span>. Then we repeat independently for a desired number of replicates <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1}, \ldots, \mathcal{X}_{\mathrm{boot},r}\)</span>. Plotting a histogram of the resulting coefficients gives some idea of the variability of our estimates.</p>
<p>We implement the bootstrap for linear regression in Python next.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">replicates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="mi">10000</span><span class="p">)):</span>
    <span class="n">n</span><span class="p">,</span><span class="n">m</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">replicates</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">replicates</span><span class="p">):</span>
        <span class="n">resample</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
        <span class="n">Aboot</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">resample</span><span class="p">,:]</span>
        <span class="n">bboot</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">resample</span><span class="p">]</span>
        <span class="n">coeff_boot</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">Aboot</span><span class="p">,</span><span class="n">bboot</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coeff_boot</span>
</pre></div>
</div>
</div>
</div>
<p>First, let’s use a simple example with a known ground truth.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="n">x</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The estimated coefficients are the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-1.03381171  1.01808039]
</pre></div>
</div>
</div>
</div>
<p>Now we apply the bootstrap and plot histograms of the two coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'coeff1'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'coeff2'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c3cc7dc60c56be78f02bf1dccdc5ae5560950b1a92eae82cf8a24e842c7301dd.png" src="../Images/f4b22f1cd76691835cbf7152bc1a7575.png" data-original-src="https://mmids-textbook.github.io/_images/c3cc7dc60c56be78f02bf1dccdc5ae5560950b1a92eae82cf8a24e842c7301dd.png"/>
</div>
</div>
<p>We see in the histograms that the true coefficient values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> fall within the likely range.</p>
<p>We return to the <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset and apply the bootstrap. Plotting a histogram of the coefficients corresponding to newspaper advertising shows that <span class="math notranslate nohighlight">\(0\)</span> is a plausible value, while it is not for TV advertising.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">TV</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">3</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'TV'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'newspaper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9f419f9fef7226b110376f01b6e45d6805cdd3997bd2325fc8d914339f26c77b.png" src="../Images/575c5f6b502edd0aa5ae4daa062cf6dd.png" data-original-src="https://mmids-textbook.github.io/_images/9f419f9fef7226b110376f01b6e45d6805cdd3997bd2325fc8d914339f26c77b.png"/>
</div>
</div>
</section>
</section>
&#13;

<h2><span class="section-number">2.7.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">2.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_ls_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">2.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_2.html">Section 2.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_3.html">Section 2.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_4.html">Section 2.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_5.html">Section 2.5</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">2.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-ls-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-ls-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">2.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E2.2.1: Yes, <span class="math notranslate nohighlight">\(U\)</span> is a linear subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. Let <span class="math notranslate nohighlight">\(u_1 = (x_1, y_1, z_1), u_2 = (x_2, y_2, z_2) \in U\)</span> and <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2y_1 - z_1 &amp;= 0 \\
x_2 + 2y_2 - z_2 &amp;= 0 \\
\alpha(x_1 + 2y_1 - z_1) + (x_2 + 2y_2 - z_2) &amp;= 0 \\
(\alpha x_1 + x_2) + 2(\alpha y_1 + y_2) - (\alpha z_1 + z_2) &amp;= 0
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\alpha u_1 + u_2 \in U\)</span>, proving <span class="math notranslate nohighlight">\(U\)</span> is a linear subspace.</p>
<p>Answer and justification for E2.2.3: One basis for <span class="math notranslate nohighlight">\(U\)</span> is <span class="math notranslate nohighlight">\(\{(1, 1, 0), (-1, 0, 1)\}\)</span>. Any vector <span class="math notranslate nohighlight">\((x, y, z) \in U\)</span> can be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(x, y, z) &amp;= (y-z, y, z) \\
&amp;= y(1, 1, 0) + z(-1, 0, 1)
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\{(1, 1, 0), (-1, 0, 1)\}\)</span> spans <span class="math notranslate nohighlight">\(U\)</span>. They are also linearly independent, as <span class="math notranslate nohighlight">\(\alpha(1, 1, 0) + \beta(-1, 0, 1) = \mathbf{0}\)</span> implies <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>. Thus, this is a basis for <span class="math notranslate nohighlight">\(U\)</span>.</p>
<p>Answer and justification for E2.2.5: Yes, <span class="math notranslate nohighlight">\(u_1\)</span> and <span class="math notranslate nohighlight">\(u_2\)</span> form an orthonormal list. We have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|u_1\| &amp;= \sqrt{(1/\sqrt{2})^2 + (1/\sqrt{2})^2} = 1 \\
\|u_2\| &amp;= \sqrt{(1/\sqrt{2})^2 + (-1/\sqrt{2})^2} = 1 \\
\langle u_1, u_2 \rangle &amp;= (1/\sqrt{2})(1/\sqrt{2}) + (1/\sqrt{2})(-1/\sqrt{2}) = 0
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(u_1\)</span> and <span class="math notranslate nohighlight">\(u_2\)</span> are unit vectors and are orthogonal to each other.</p>
<p>Answer and justification for E2.2.7: <span class="math notranslate nohighlight">\(A\)</span> is nonsingular. Its columns are <span class="math notranslate nohighlight">\((1, 3)\)</span> and <span class="math notranslate nohighlight">\((2, 4)\)</span>, which are linearly independent</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\alpha(1, 3) + \beta(2, 4) &amp;= (0, 0) \\
\alpha + 2\beta &amp;= 0 \\
3\alpha + 4\beta &amp;= 0
\end{align*}\]</div>
<p>This system has only the trivial solution <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>. So the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent, and since <span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix, this means it has rank 2 and is nonsingular.</p>
<p>Answer and justification for E2.2.9:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} = \alpha \mathbf{w}_1 + \beta \mathbf{w}_2 \implies (2, 3, 5) = \alpha (1, 0, 1) + \beta (0, 1, 1).
\]</div>
<p>Solving the system of equations</p>
<div class="math notranslate nohighlight">
\[
2 = \alpha, \quad 3 = \beta, \quad 5 = \alpha + \beta.
\]</div>
<p>Substitute <span class="math notranslate nohighlight">\(\alpha = 2\)</span> and <span class="math notranslate nohighlight">\(\beta = 3\)</span> into the third equation</p>
<div class="math notranslate nohighlight">
\[
5 = 2 + 3 \implies \alpha = 2, \beta = 3.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{v} = 2\mathbf{w}_1 + 3\mathbf{w}_2\)</span>.</p>
<p>Answer and justification for E2.2.11: To find the null space, we need to solve the homogeneous system <span class="math notranslate nohighlight">\(B\mathbf{x} = \mathbf{0}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\end{split}\]</div>
<p>This gives us two equations with three unknowns:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2x_2 + 3x_3 &amp;= 0 \tag{1}\\
4x_1 + 5x_2 + 6x_3 &amp;= 0 \tag{2}
\end{align*}\]</div>
<p>Multiplying equation (1) by 4:</p>
<div class="math notranslate nohighlight">
\[
4x_1 + 8x_2 + 12x_3 = 0 \tag{3}
\]</div>
<p>Subtracting equation (3) from equation (2):</p>
<div class="math notranslate nohighlight">
\[
(5-8)x_2 + (6-12)x_3 = 0
\]</div>
<p>This simplifies to:</p>
<div class="math notranslate nohighlight">
\[
-3x_2 - 6x_3 = 0
\]</div>
<p>Dividing by -3:
$<span class="math notranslate nohighlight">\(
x_2 = -2x_3
\)</span>$</p>
<p>Now, substituting this back into equation (1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2(-2x_3) + 3x_3 &amp;= 0 \\
x_1 - 4x_3 + 3x_3 &amp;= 0 \\
x_1 - x_3 &amp;= 0
\end{align*}\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[
x_1 = x_3
\]</div>
<p>Setting <span class="math notranslate nohighlight">\(x_3 = t\)</span> (a free parameter), we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} t \\ -2t \\ t \end{pmatrix} = t \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}
\end{split}\]</div>
<p>The null space of <span class="math notranslate nohighlight">\(B\)</span> is the span of the vector <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.2.13: To determine linear independence, we need to check if the only solution to <span class="math notranslate nohighlight">\(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \alpha_3\mathbf{u}_3 = \mathbf{0}\)</span> is <span class="math notranslate nohighlight">\(\alpha_1 = \alpha_2 = \alpha_3 = 0\)</span>.</p>
<p>Let’s write out this vector equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\alpha_1 \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} + \alpha_2 \begin{pmatrix} 2 \\ -1 \\ 0 \end{pmatrix} + \alpha_3 \begin{pmatrix} 1 \\ 8 \\ 6 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
\end{split}\]</div>
<p>This gives us a system of three equations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\alpha_1 + 2\alpha_2 + \alpha_3 &amp;= 0 \tag{1}\\
2\alpha_1 - \alpha_2 + 8\alpha_3 &amp;= 0 \tag{2}\\
3\alpha_1 + 0\alpha_2 + 6\alpha_3 &amp;= 0 \tag{3}
\end{align*}\]</div>
<p>From equation (3):</p>
<div class="math notranslate nohighlight">
\[
3\alpha_1 + 6\alpha_3 = 0
\]</div>
<p>Which gives us:</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 = -2\alpha_3 \tag{4}
\]</div>
<p>Substituting this into equation (1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-2\alpha_3 + 2\alpha_2 + \alpha_3 &amp;= 0\\
2\alpha_2 - \alpha_3 &amp;= 0\\
\alpha_2 &amp;= \frac{\alpha_3}{2} \tag{5}
\end{align*}\]</div>
<p>Now let’s check equation (2) by substituting equations (4) and (5):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2(-2\alpha_3) - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
-4\alpha_3 - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
\end{align*}\]</div>
<p>Simplifying:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-4\alpha_3 - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
\frac{-8\alpha_3 - \alpha_3 + 16\alpha_3}{2} &amp;= 0\\
\frac{7\alpha_3}{2} &amp;= 0
\end{align*}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\frac{7\alpha_3}{2} = 0\)</span> implies <span class="math notranslate nohighlight">\(\alpha_3 = 0\)</span>, and from equations (4) and (5), we get <span class="math notranslate nohighlight">\(\alpha_1 = 0\)</span> and <span class="math notranslate nohighlight">\(\alpha_2 = 0\)</span>.</p>
<p>Therefore, the only solution to the system is the trivial solution <span class="math notranslate nohighlight">\(\alpha_1 = \alpha_2 = \alpha_3 = 0\)</span>, which means the vectors <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>, <span class="math notranslate nohighlight">\(\mathbf{u}_2\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{u}_3\)</span> are linearly independent.</p>
<p>Consider the equation <span class="math notranslate nohighlight">\(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \alpha_3\mathbf{u}_3 = 0\)</span>. This leads to the system of equations</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 + 2\alpha_2 + \alpha_3 = 0
\]</div>
<div class="math notranslate nohighlight">
\[
2\alpha_1 - \alpha_2 + 8\alpha_3 = 0
\]</div>
<div class="math notranslate nohighlight">
\[
3\alpha_1  + 6\alpha_3 = 0
\]</div>
<p>Solving this system, we find that <span class="math notranslate nohighlight">\(\alpha_1 = -2\alpha_3\)</span> and <span class="math notranslate nohighlight">\(\alpha_2 = \alpha_3\)</span>. Choosing <span class="math notranslate nohighlight">\(\alpha_3 = 1\)</span>, we get a non-trivial solution <span class="math notranslate nohighlight">\(\alpha_1 = -2\)</span>, <span class="math notranslate nohighlight">\(\alpha_2 = 1\)</span>, <span class="math notranslate nohighlight">\(\alpha_3 = 1\)</span>. Therefore, the vectors are linearly dependent.</p>
<p>Answer and justification for E2.2.15: We can write this system as <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b}\)</span>, where
<span class="math notranslate nohighlight">\(A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{b} = \begin{bmatrix} 3 \\ 1 \end{bmatrix}\)</span>. Since <span class="math notranslate nohighlight">\(\det(A) = -3 \neq 0\)</span>, <span class="math notranslate nohighlight">\(A\)</span> is invertible. We find that <span class="math notranslate nohighlight">\(A^{-1} = \frac{1}{-3} \begin{bmatrix} -1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix}\)</span>.</p>
<p>Then, the solution is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{x} = A^{-1}\mathbf{b} = \frac{1}{-3} \begin{bmatrix} -1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix} \begin{bmatrix} 3 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{4}{3} \\ -\frac{1}{3} \end{bmatrix}.
\end{split}\]</div>
<p>Answer and justification for E2.3.1: While</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q^T Q = \begin{pmatrix} 1 &amp; 0\\ 0 &amp; 1 \end{pmatrix} = I_{2 \times 2},
\end{split}\]</div>
<p>the matrix <span class="math notranslate nohighlight">\(Q\)</span> is not square. Therefore, <span class="math notranslate nohighlight">\(Q\)</span> is not an orthogonal matrix.</p>
<p>Answer and justification for E2.3.3:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_{U} \mathbf{v} = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\|\mathbf{u}\|^2} \mathbf{u} = \frac{(2 \cdot 1 + 3 \cdot 1)}{(1^2 + 1^2)} \mathbf{u} = \frac{5}{2} (1, 1) = \left(\frac{5}{2}, \frac{5}{2}\right).
\]</div>
<p>Answer and justification for E2.3.5: The projection of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> onto <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{proj}_{\mathbf{u}} \mathbf{v} = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{\|\mathbf{u}\|^2} \mathbf{u} = \frac{(1)(1) + (1)(2) + (0)(1)}{1^2 + 1^2 + 0^2} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \frac{3}{2} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} \frac{3}{2} \\ \frac{3}{2} \\ 0 \end{bmatrix}.
\end{split}\]</div>
<p>Answer and justification for E2.3.7: Using the orthonormal basis <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> for <span class="math notranslate nohighlight">\(U\)</span> from the text, we have:
$<span class="math notranslate nohighlight">\(\mathrm{proj}_U \mathbf{v} = \langle \mathbf{v}, \mathbf{q}_1 \rangle \mathbf{q}_1 + \langle \mathbf{v}, \mathbf{q}_2 \rangle \mathbf{q}_2 = \frac{4}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}}(1, 0, 1) + \frac{5}{\sqrt{6}} \cdot \frac{1}{\sqrt{6}}(-1, 2, 1) = (\frac{7}{3}, \frac{10}{3}, \frac{13}{3}).\)</span>$</p>
<p>Answer and justification for E2.3.9: From E2.3.7 and E2.3.8, we have:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_U \mathbf{v} = (\frac{7}{3}, \frac{10}{3}, \frac{13}{3}), \quad \mathbf{v} - \mathrm{proj}_U \mathbf{v} = (-\frac{1}{3}, -\frac{1}{3}, \frac{2}{3}).
\]</div>
<p>Computing the squared norms:</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{v}\|^2 = 1^2 + 2^2 + 3^2 = 14, \quad \|\mathrm{proj}_U \mathbf{v}\|^2 = (\frac{7}{3})^2 + (\frac{10}{3})^2 + (\frac{13}{3})^2 = \frac{146}{3}, \quad \|\mathbf{v} - \mathrm{proj}_U \mathbf{v}\|^2 = (-\frac{1}{3})^2 + (-\frac{1}{3})^2 + (\frac{2}{3})^2 = \frac{2}{3}.
\]</div>
<p>Indeed, <span class="math notranslate nohighlight">\(\|\mathbf{v}\|^2 = 14 = \frac{146}{3} + \frac{2}{3} = \|\mathrm{proj}_U \mathbf{v}\|^2 + \|\mathbf{v} - \mathrm{proj}_U \mathbf{v}\|^2\)</span>, verifying the Pythagorean theorem.</p>
<p>Answer and justification for E2.3.11: Since <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> is not a unit vector, we first normalize it: <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{u}_1}{\|\mathbf{u}_1\|} = \frac{1}{3}\begin{pmatrix} 2 \\ 1 \\ -2 \end{pmatrix}\)</span>. Then, <span class="math notranslate nohighlight">\(\mathrm{proj}_{\mathbf{u}_1} \mathbf{u}_2 = \langle \mathbf{u}_2, \mathbf{q}_1 \rangle \mathbf{q}_1 = \frac{1}{9} \begin{pmatrix} 2 \\ 1 \\ -2 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.3.13: We want to find all vectors <span class="math notranslate nohighlight">\(\begin{pmatrix} x \\ y \\ z \end{pmatrix}\)</span> such that <span class="math notranslate nohighlight">\(\begin{pmatrix} x \\ y \\ z \end{pmatrix} \cdot \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} = 0\)</span>. This gives us the equation <span class="math notranslate nohighlight">\(x + y = 0\)</span>, or <span class="math notranslate nohighlight">\(x = -y\)</span>. Thus, any vector of the form <span class="math notranslate nohighlight">\(\begin{pmatrix} -y \\ y \\ z \end{pmatrix} = y\begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} + z\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\)</span> is in <span class="math notranslate nohighlight">\(W^\perp\)</span>. So, a basis for <span class="math notranslate nohighlight">\(W^\perp\)</span> is <span class="math notranslate nohighlight">\(\left\{\begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\right\}\)</span>.</p>
<p>Answer and justification for E2.3.15:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T A = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}^T \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}, \quad A^T \mathbf{b} = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}^T \begin{pmatrix} 3 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 2 \end{pmatrix}.
\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{x} = (A^T A)^{-1} A^T \mathbf{b} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}^{-1} \begin{pmatrix} 4 \\ 2 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.1: <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|} = (1, 0)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}_2 = \mathbf{a}_2 - \langle \mathbf{q}_1, \mathbf{a}_2 \rangle \mathbf{q}_1 = (0, 1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|} = (0, 1)\)</span>.</p>
<p>Answer and justification for E2.4.3: Let <span class="math notranslate nohighlight">\(\mathbf{w}_1 = (1, 1)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_2 = (1, 0)\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_1 &amp;= \frac{\mathbf{w}_1}{\|\mathbf{w}_1\|} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) \\
\mathbf{q}_2 &amp;= \frac{\mathbf{w}_2 - \langle \mathbf{w}_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\mathbf{w}_2 - \langle \mathbf{w}_2, q_1 \rangle q_1\|} \\
&amp;= \frac{(1, 0) - (\frac{1}{\sqrt{2}})(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})}{\sqrt{1 - (\frac{1}{\sqrt{2}})^2}} \\
&amp;= (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> is an orthonormal basis.</p>
<p>Answer and justification for E2.4.5: Using the orthonormal basis <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> from E2.4.4, we have <span class="math notranslate nohighlight">\(Q = [\mathbf{q}_1\ \mathbf{q}_2]\)</span>. To find <span class="math notranslate nohighlight">\(R\)</span>, we observe that <span class="math notranslate nohighlight">\(\mathbf{a}_1 = \sqrt{3}\mathbf{q}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{a}_2 = \frac{1}{\sqrt{3}}\mathbf{q}_1 + \frac{5}{\sqrt{21}}\mathbf{q}_2\)</span>. Therefore, <span class="math notranslate nohighlight">\(R = \begin{pmatrix} \sqrt{3} &amp; \frac{1}{\sqrt{3}} \\ 0 &amp; \frac{5}{\sqrt{21}} \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.7: <span class="math notranslate nohighlight">\(\mathbf{x} = (2, 1)\)</span>. From the second equation, <span class="math notranslate nohighlight">\(3x_2 = 3\)</span>, so <span class="math notranslate nohighlight">\(x_2 = 1\)</span>. Substituting into the first equation, <span class="math notranslate nohighlight">\(2x_1 - 1 = 4\)</span>, so <span class="math notranslate nohighlight">\(x_1 = 2\)</span>.</p>
<p>Answer and justification for E2.4.9: <span class="math notranslate nohighlight">\(H = I_{3 \times 3} - 2\mathbf{z}\mathbf{z}^T/\|\mathbf{z}\|^2 = \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.11: To verify that <span class="math notranslate nohighlight">\(H_1\)</span> is orthogonal, we check if <span class="math notranslate nohighlight">\(H_1^T H_1 = I_{2 \times 2}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_1^T H_1 = \begin{pmatrix} \frac{7}{5} &amp; -\frac{6}{5} \\ -\frac{6}{5} &amp; -\frac{1}{5} \end{pmatrix} \begin{pmatrix} \frac{7}{5} &amp; -\frac{6}{5} \\ -\frac{6}{5} &amp; -\frac{1}{5} \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = I_{2 \times 2}.
\end{split}\]</div>
<p>To verify that <span class="math notranslate nohighlight">\(H_1\)</span> is symmetric, we check if <span class="math notranslate nohighlight">\(H_1^T = H_1\)</span>, which is true by observation.</p>
<p>Answer and justification for E2.4.13: We have <span class="math notranslate nohighlight">\(H_1 A = R\)</span>, where <span class="math notranslate nohighlight">\(R = \begin{pmatrix} -\frac{\sqrt{10}}{5} &amp; -\frac{2}{\sqrt{10}} \\ 0 &amp; \frac{14}{5} \end{pmatrix}\)</span> is upper triangular. Therefore, <span class="math notranslate nohighlight">\(Q^T = H_1\)</span>, and <span class="math notranslate nohighlight">\(Q = H_1^T = \begin{pmatrix} \frac{7}{5} &amp; \frac{6}{5} \\ -\frac{6}{5} &amp; \frac{1}{5} \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.15: Let <span class="math notranslate nohighlight">\(\mathbf{y}_1 = (3, 4)^T\)</span> be the first column of <span class="math notranslate nohighlight">\(A\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(2)} - \mathbf{y}_1 = (5, -4)^T\)</span> and <span class="math notranslate nohighlight">\(H_1 = I_{2 \times 2} - 2\mathbf{z}_1\mathbf{z}_1^T / \|\mathbf{z}_1\|^2 = \begin{pmatrix} 3/5 &amp; 4/5 \\ 4/5 &amp; -3/5 \end{pmatrix}\)</span>. We can verify that <span class="math notranslate nohighlight">\(H_1 A = \begin{pmatrix} 5 &amp; 1 \\ 0 &amp; -2/5 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.17:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Q &amp;= \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \end{pmatrix}, \\
R &amp;= Q^T A = \begin{pmatrix} \sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2} \end{pmatrix}, \\
Q^T \mathbf{b} &amp;= \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 2 \\ 0 \end{pmatrix} = \begin{pmatrix} \sqrt{2} \\ \sqrt{2} \end{pmatrix}, \\
R \mathbf{x} &amp;= Q^T \mathbf{b}, \quad \begin{pmatrix} \sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2} \end{pmatrix} \mathbf{x} = \begin{pmatrix} \sqrt{2} \\ \sqrt{2} \end{pmatrix}, \\
\mathbf{x} &amp;= \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\end{align*}\]</div>
<p>The solution is <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.5.1:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{pmatrix} 
1 &amp; 1 \\
1 &amp; 2 \\
1 &amp; 3 \\
1 &amp; 4
\end{pmatrix}, \quad 
\mathbf{y} = \begin{pmatrix}
2 \\
4 \\
5 \\
7
\end{pmatrix}.
\end{split}\]</div>
<p>The normal equations are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T A \boldsymbol{\beta} = A^T \mathbf{y} \Rightarrow
\begin{pmatrix}
4 &amp; 10 \\
10 &amp; 30
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix} =
\begin{pmatrix}
18 \\
47
\end{pmatrix}.
\end{split}\]</div>
<p>Solving this system of equations yields <span class="math notranslate nohighlight">\(\beta_0 = \frac{1}{2}\)</span> and <span class="math notranslate nohighlight">\(\beta_1 = \frac{3}{2}\)</span>.</p>
<p>Answer and justification for E2.5.3:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \end{bmatrix} \quad \mathbf{y} = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix}
\end{split}\]</div>
<p>Answer and justification for E2.5.5:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = (A^T A)^{-1} A^T \mathbf{y} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}
\end{split}\]</div>
<p>We solve the linear system by inverting <span class="math notranslate nohighlight">\(A^T A\)</span> and multiplying by <span class="math notranslate nohighlight">\(A^T \mathbf{y}\)</span>.</p>
<p>Answer and justification for E2.5.7:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; -1 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<p>For a quadratic model, we need columns for <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(x^2\)</span>.</p>
<p>Answer and justification for E2.5.9: We solve the normal equations <span class="math notranslate nohighlight">\(A^T A \boldsymbol{\beta} = A^T \mathbf{y}\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A^T A &amp;= \begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \end{pmatrix} = \begin{pmatrix} 3 &amp; 6 \\ 6 &amp; 14 \end{pmatrix}, \\
A^T \mathbf{y} &amp;= \begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} = \begin{pmatrix} 6 \\ 14 \end{pmatrix}.
\end{align*}\]</div>
<p>Solving <span class="math notranslate nohighlight">\(\begin{pmatrix} 3 &amp; 6 \\ 6 &amp; 14 \end{pmatrix} \boldsymbol{\beta} = \begin{pmatrix} 6 \\ 14 \end{pmatrix}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\boldsymbol{\beta} &amp;= \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
\end{align*}\]</div>
<p>Answer and justification for E2.5.11: Compute the predicted <span class="math notranslate nohighlight">\(y\)</span> values:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{y}_1 &amp;= 2(1) + 1 = 3, \\
\hat{y}_2 &amp;= 2(2) + 1 = 5, \\
\hat{y}_3 &amp;= 2(3) + 1 = 7.
\end{align*}\]</div>
<p>Compute the residuals:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
r_1 &amp;= 3 - 3 = 0, \\
r_2 &amp;= 5 - 5 = 0, \\
r_3 &amp;= 7 - 7 = 0.
\end{align*}\]</div>
<p>RSS is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{RSS} &amp;= 0^2 + 0^2 + 0^2 = 0.
\end{align*}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">2.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define the concepts of linear subspaces, spans, and bases in vector spaces.</p></li>
<li><p>Check the conditions for a set of vectors to be linearly independent or form a basis of a vector space.</p></li>
<li><p>Define the inverse of a nonsingular matrix and prove its uniqueness.</p></li>
<li><p>Define the dimension of a linear subspace.</p></li>
<li><p>State the Pythagorean Theorem.</p></li>
<li><p>Verify whether a given list of vectors is orthonormal.</p></li>
<li><p>Derive the coefficients in an orthonormal basis expansion of a vector.</p></li>
<li><p>Apply the Gram-Schmidt process to transform a basis into an orthonormal basis.</p></li>
<li><p>Prove key theorems related to vector spaces and matrix inverses, such as the Pythagorean theorem for orthogonal vectors and the conditions for linear independence in matrix columns.</p></li>
<li><p>Illustrate examples of linear subspaces and their bases, using both theoretical definitions and numerical examples.</p></li>
<li><p>Define the orthogonal projection of a vector onto a linear subspace and characterize it geometrically.</p></li>
<li><p>Compute the orthogonal projection of a vector onto a linear subspace spanned by an orthonormal list of vectors.</p></li>
<li><p>Illustrate the process of finding the orthogonal projection of a vector onto a subspace using geometric intuition.</p></li>
<li><p>Prove the uniqueness of the orthogonal projection.</p></li>
<li><p>Express the orthogonal projection in matrix form.</p></li>
<li><p>Define the orthogonal complement of a linear subspace and find an orthonormal basis for it.</p></li>
<li><p>State and prove the Orthogonal Decomposition Lemma, and use it to decompose a vector into its orthogonal projection and a vector in the orthogonal complement.</p></li>
<li><p>Formulate the linear least squares problem as an optimization problem and derive the normal equations.</p></li>
<li><p>Apply the concepts of orthogonal projection and orthogonal complement to solve overdetermined systems using the least squares method.</p></li>
<li><p>Determine the uniqueness of the least squares solution based on the linear independence of the columns of the matrix.</p></li>
<li><p>Implement the Gram-Schmidt algorithm to obtain an orthonormal basis from a set of linearly independent vectors.</p></li>
<li><p>Express the Gram-Schmidt algorithm using a matrix factorization perspective, known as the QR decomposition.</p></li>
<li><p>Apply the QR decomposition to solve linear least squares problems as an alternative to the normal equations approach.</p></li>
<li><p>Define Householder reflections and explain their role in introducing zeros below the diagonal of a matrix.</p></li>
<li><p>Construct a QR decomposition using a sequence of Householder transformations.</p></li>
<li><p>Compare the numerical stability of the Gram-Schmidt algorithm and the Householder transformations approach for computing the QR decomposition.</p></li>
<li><p>Formulate the linear regression problem as a linear least squares optimization problem.</p></li>
<li><p>Extend the linear regression model to polynomial regression by incorporating higher-degree terms and interaction terms in the design matrix.</p></li>
<li><p>Recognize and explain the issue of overfitting in polynomial regression and its potential consequences.</p></li>
<li><p>Apply the least squares method to simulated data and real-world datasets, such as the Advertising dataset, to estimate regression coefficients.</p></li>
<li><p>Understand and implement the bootstrap method for linear regression to assess the variability of estimated coefficients and determine the statistical significance of the results.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
&#13;

<h3><span class="section-number">2.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_ls_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_ls_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
&#13;

<h3><span class="section-number">2.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_2.html">Section 2.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_3.html">Section 2.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_4.html">Section 2.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_2_5.html">Section 2.5</a></p></li>
</ul>
&#13;

<h3><span class="section-number">2.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-ls-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-ls-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
&#13;

<h3><span class="section-number">2.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p>Answer and justification for E2.2.1: Yes, <span class="math notranslate nohighlight">\(U\)</span> is a linear subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. Let <span class="math notranslate nohighlight">\(u_1 = (x_1, y_1, z_1), u_2 = (x_2, y_2, z_2) \in U\)</span> and <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2y_1 - z_1 &amp;= 0 \\
x_2 + 2y_2 - z_2 &amp;= 0 \\
\alpha(x_1 + 2y_1 - z_1) + (x_2 + 2y_2 - z_2) &amp;= 0 \\
(\alpha x_1 + x_2) + 2(\alpha y_1 + y_2) - (\alpha z_1 + z_2) &amp;= 0
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\alpha u_1 + u_2 \in U\)</span>, proving <span class="math notranslate nohighlight">\(U\)</span> is a linear subspace.</p>
<p>Answer and justification for E2.2.3: One basis for <span class="math notranslate nohighlight">\(U\)</span> is <span class="math notranslate nohighlight">\(\{(1, 1, 0), (-1, 0, 1)\}\)</span>. Any vector <span class="math notranslate nohighlight">\((x, y, z) \in U\)</span> can be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(x, y, z) &amp;= (y-z, y, z) \\
&amp;= y(1, 1, 0) + z(-1, 0, 1)
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\{(1, 1, 0), (-1, 0, 1)\}\)</span> spans <span class="math notranslate nohighlight">\(U\)</span>. They are also linearly independent, as <span class="math notranslate nohighlight">\(\alpha(1, 1, 0) + \beta(-1, 0, 1) = \mathbf{0}\)</span> implies <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>. Thus, this is a basis for <span class="math notranslate nohighlight">\(U\)</span>.</p>
<p>Answer and justification for E2.2.5: Yes, <span class="math notranslate nohighlight">\(u_1\)</span> and <span class="math notranslate nohighlight">\(u_2\)</span> form an orthonormal list. We have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|u_1\| &amp;= \sqrt{(1/\sqrt{2})^2 + (1/\sqrt{2})^2} = 1 \\
\|u_2\| &amp;= \sqrt{(1/\sqrt{2})^2 + (-1/\sqrt{2})^2} = 1 \\
\langle u_1, u_2 \rangle &amp;= (1/\sqrt{2})(1/\sqrt{2}) + (1/\sqrt{2})(-1/\sqrt{2}) = 0
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(u_1\)</span> and <span class="math notranslate nohighlight">\(u_2\)</span> are unit vectors and are orthogonal to each other.</p>
<p>Answer and justification for E2.2.7: <span class="math notranslate nohighlight">\(A\)</span> is nonsingular. Its columns are <span class="math notranslate nohighlight">\((1, 3)\)</span> and <span class="math notranslate nohighlight">\((2, 4)\)</span>, which are linearly independent</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\alpha(1, 3) + \beta(2, 4) &amp;= (0, 0) \\
\alpha + 2\beta &amp;= 0 \\
3\alpha + 4\beta &amp;= 0
\end{align*}\]</div>
<p>This system has only the trivial solution <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>. So the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent, and since <span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix, this means it has rank 2 and is nonsingular.</p>
<p>Answer and justification for E2.2.9:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} = \alpha \mathbf{w}_1 + \beta \mathbf{w}_2 \implies (2, 3, 5) = \alpha (1, 0, 1) + \beta (0, 1, 1).
\]</div>
<p>Solving the system of equations</p>
<div class="math notranslate nohighlight">
\[
2 = \alpha, \quad 3 = \beta, \quad 5 = \alpha + \beta.
\]</div>
<p>Substitute <span class="math notranslate nohighlight">\(\alpha = 2\)</span> and <span class="math notranslate nohighlight">\(\beta = 3\)</span> into the third equation</p>
<div class="math notranslate nohighlight">
\[
5 = 2 + 3 \implies \alpha = 2, \beta = 3.
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{v} = 2\mathbf{w}_1 + 3\mathbf{w}_2\)</span>.</p>
<p>Answer and justification for E2.2.11: To find the null space, we need to solve the homogeneous system <span class="math notranslate nohighlight">\(B\mathbf{x} = \mathbf{0}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\end{split}\]</div>
<p>This gives us two equations with three unknowns:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2x_2 + 3x_3 &amp;= 0 \tag{1}\\
4x_1 + 5x_2 + 6x_3 &amp;= 0 \tag{2}
\end{align*}\]</div>
<p>Multiplying equation (1) by 4:</p>
<div class="math notranslate nohighlight">
\[
4x_1 + 8x_2 + 12x_3 = 0 \tag{3}
\]</div>
<p>Subtracting equation (3) from equation (2):</p>
<div class="math notranslate nohighlight">
\[
(5-8)x_2 + (6-12)x_3 = 0
\]</div>
<p>This simplifies to:</p>
<div class="math notranslate nohighlight">
\[
-3x_2 - 6x_3 = 0
\]</div>
<p>Dividing by -3:
$<span class="math notranslate nohighlight">\(
x_2 = -2x_3
\)</span>$</p>
<p>Now, substituting this back into equation (1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 + 2(-2x_3) + 3x_3 &amp;= 0 \\
x_1 - 4x_3 + 3x_3 &amp;= 0 \\
x_1 - x_3 &amp;= 0
\end{align*}\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[
x_1 = x_3
\]</div>
<p>Setting <span class="math notranslate nohighlight">\(x_3 = t\)</span> (a free parameter), we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} t \\ -2t \\ t \end{pmatrix} = t \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}
\end{split}\]</div>
<p>The null space of <span class="math notranslate nohighlight">\(B\)</span> is the span of the vector <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.2.13: To determine linear independence, we need to check if the only solution to <span class="math notranslate nohighlight">\(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \alpha_3\mathbf{u}_3 = \mathbf{0}\)</span> is <span class="math notranslate nohighlight">\(\alpha_1 = \alpha_2 = \alpha_3 = 0\)</span>.</p>
<p>Let’s write out this vector equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\alpha_1 \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} + \alpha_2 \begin{pmatrix} 2 \\ -1 \\ 0 \end{pmatrix} + \alpha_3 \begin{pmatrix} 1 \\ 8 \\ 6 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
\end{split}\]</div>
<p>This gives us a system of three equations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\alpha_1 + 2\alpha_2 + \alpha_3 &amp;= 0 \tag{1}\\
2\alpha_1 - \alpha_2 + 8\alpha_3 &amp;= 0 \tag{2}\\
3\alpha_1 + 0\alpha_2 + 6\alpha_3 &amp;= 0 \tag{3}
\end{align*}\]</div>
<p>From equation (3):</p>
<div class="math notranslate nohighlight">
\[
3\alpha_1 + 6\alpha_3 = 0
\]</div>
<p>Which gives us:</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 = -2\alpha_3 \tag{4}
\]</div>
<p>Substituting this into equation (1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-2\alpha_3 + 2\alpha_2 + \alpha_3 &amp;= 0\\
2\alpha_2 - \alpha_3 &amp;= 0\\
\alpha_2 &amp;= \frac{\alpha_3}{2} \tag{5}
\end{align*}\]</div>
<p>Now let’s check equation (2) by substituting equations (4) and (5):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2(-2\alpha_3) - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
-4\alpha_3 - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
\end{align*}\]</div>
<p>Simplifying:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-4\alpha_3 - \frac{\alpha_3}{2} + 8\alpha_3 &amp;= 0\\
\frac{-8\alpha_3 - \alpha_3 + 16\alpha_3}{2} &amp;= 0\\
\frac{7\alpha_3}{2} &amp;= 0
\end{align*}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\frac{7\alpha_3}{2} = 0\)</span> implies <span class="math notranslate nohighlight">\(\alpha_3 = 0\)</span>, and from equations (4) and (5), we get <span class="math notranslate nohighlight">\(\alpha_1 = 0\)</span> and <span class="math notranslate nohighlight">\(\alpha_2 = 0\)</span>.</p>
<p>Therefore, the only solution to the system is the trivial solution <span class="math notranslate nohighlight">\(\alpha_1 = \alpha_2 = \alpha_3 = 0\)</span>, which means the vectors <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>, <span class="math notranslate nohighlight">\(\mathbf{u}_2\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{u}_3\)</span> are linearly independent.</p>
<p>Consider the equation <span class="math notranslate nohighlight">\(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \alpha_3\mathbf{u}_3 = 0\)</span>. This leads to the system of equations</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 + 2\alpha_2 + \alpha_3 = 0
\]</div>
<div class="math notranslate nohighlight">
\[
2\alpha_1 - \alpha_2 + 8\alpha_3 = 0
\]</div>
<div class="math notranslate nohighlight">
\[
3\alpha_1  + 6\alpha_3 = 0
\]</div>
<p>Solving this system, we find that <span class="math notranslate nohighlight">\(\alpha_1 = -2\alpha_3\)</span> and <span class="math notranslate nohighlight">\(\alpha_2 = \alpha_3\)</span>. Choosing <span class="math notranslate nohighlight">\(\alpha_3 = 1\)</span>, we get a non-trivial solution <span class="math notranslate nohighlight">\(\alpha_1 = -2\)</span>, <span class="math notranslate nohighlight">\(\alpha_2 = 1\)</span>, <span class="math notranslate nohighlight">\(\alpha_3 = 1\)</span>. Therefore, the vectors are linearly dependent.</p>
<p>Answer and justification for E2.2.15: We can write this system as <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b}\)</span>, where
<span class="math notranslate nohighlight">\(A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{b} = \begin{bmatrix} 3 \\ 1 \end{bmatrix}\)</span>. Since <span class="math notranslate nohighlight">\(\det(A) = -3 \neq 0\)</span>, <span class="math notranslate nohighlight">\(A\)</span> is invertible. We find that <span class="math notranslate nohighlight">\(A^{-1} = \frac{1}{-3} \begin{bmatrix} -1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix}\)</span>.</p>
<p>Then, the solution is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{x} = A^{-1}\mathbf{b} = \frac{1}{-3} \begin{bmatrix} -1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix} \begin{bmatrix} 3 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{4}{3} \\ -\frac{1}{3} \end{bmatrix}.
\end{split}\]</div>
<p>Answer and justification for E2.3.1: While</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q^T Q = \begin{pmatrix} 1 &amp; 0\\ 0 &amp; 1 \end{pmatrix} = I_{2 \times 2},
\end{split}\]</div>
<p>the matrix <span class="math notranslate nohighlight">\(Q\)</span> is not square. Therefore, <span class="math notranslate nohighlight">\(Q\)</span> is not an orthogonal matrix.</p>
<p>Answer and justification for E2.3.3:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_{U} \mathbf{v} = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\|\mathbf{u}\|^2} \mathbf{u} = \frac{(2 \cdot 1 + 3 \cdot 1)}{(1^2 + 1^2)} \mathbf{u} = \frac{5}{2} (1, 1) = \left(\frac{5}{2}, \frac{5}{2}\right).
\]</div>
<p>Answer and justification for E2.3.5: The projection of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> onto <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{proj}_{\mathbf{u}} \mathbf{v} = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{\|\mathbf{u}\|^2} \mathbf{u} = \frac{(1)(1) + (1)(2) + (0)(1)}{1^2 + 1^2 + 0^2} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \frac{3}{2} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} \frac{3}{2} \\ \frac{3}{2} \\ 0 \end{bmatrix}.
\end{split}\]</div>
<p>Answer and justification for E2.3.7: Using the orthonormal basis <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> for <span class="math notranslate nohighlight">\(U\)</span> from the text, we have:
$<span class="math notranslate nohighlight">\(\mathrm{proj}_U \mathbf{v} = \langle \mathbf{v}, \mathbf{q}_1 \rangle \mathbf{q}_1 + \langle \mathbf{v}, \mathbf{q}_2 \rangle \mathbf{q}_2 = \frac{4}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}}(1, 0, 1) + \frac{5}{\sqrt{6}} \cdot \frac{1}{\sqrt{6}}(-1, 2, 1) = (\frac{7}{3}, \frac{10}{3}, \frac{13}{3}).\)</span>$</p>
<p>Answer and justification for E2.3.9: From E2.3.7 and E2.3.8, we have:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_U \mathbf{v} = (\frac{7}{3}, \frac{10}{3}, \frac{13}{3}), \quad \mathbf{v} - \mathrm{proj}_U \mathbf{v} = (-\frac{1}{3}, -\frac{1}{3}, \frac{2}{3}).
\]</div>
<p>Computing the squared norms:</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{v}\|^2 = 1^2 + 2^2 + 3^2 = 14, \quad \|\mathrm{proj}_U \mathbf{v}\|^2 = (\frac{7}{3})^2 + (\frac{10}{3})^2 + (\frac{13}{3})^2 = \frac{146}{3}, \quad \|\mathbf{v} - \mathrm{proj}_U \mathbf{v}\|^2 = (-\frac{1}{3})^2 + (-\frac{1}{3})^2 + (\frac{2}{3})^2 = \frac{2}{3}.
\]</div>
<p>Indeed, <span class="math notranslate nohighlight">\(\|\mathbf{v}\|^2 = 14 = \frac{146}{3} + \frac{2}{3} = \|\mathrm{proj}_U \mathbf{v}\|^2 + \|\mathbf{v} - \mathrm{proj}_U \mathbf{v}\|^2\)</span>, verifying the Pythagorean theorem.</p>
<p>Answer and justification for E2.3.11: Since <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> is not a unit vector, we first normalize it: <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{u}_1}{\|\mathbf{u}_1\|} = \frac{1}{3}\begin{pmatrix} 2 \\ 1 \\ -2 \end{pmatrix}\)</span>. Then, <span class="math notranslate nohighlight">\(\mathrm{proj}_{\mathbf{u}_1} \mathbf{u}_2 = \langle \mathbf{u}_2, \mathbf{q}_1 \rangle \mathbf{q}_1 = \frac{1}{9} \begin{pmatrix} 2 \\ 1 \\ -2 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.3.13: We want to find all vectors <span class="math notranslate nohighlight">\(\begin{pmatrix} x \\ y \\ z \end{pmatrix}\)</span> such that <span class="math notranslate nohighlight">\(\begin{pmatrix} x \\ y \\ z \end{pmatrix} \cdot \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} = 0\)</span>. This gives us the equation <span class="math notranslate nohighlight">\(x + y = 0\)</span>, or <span class="math notranslate nohighlight">\(x = -y\)</span>. Thus, any vector of the form <span class="math notranslate nohighlight">\(\begin{pmatrix} -y \\ y \\ z \end{pmatrix} = y\begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix} + z\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\)</span> is in <span class="math notranslate nohighlight">\(W^\perp\)</span>. So, a basis for <span class="math notranslate nohighlight">\(W^\perp\)</span> is <span class="math notranslate nohighlight">\(\left\{\begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\right\}\)</span>.</p>
<p>Answer and justification for E2.3.15:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T A = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}^T \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}, \quad A^T \mathbf{b} = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}^T \begin{pmatrix} 3 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 2 \end{pmatrix}.
\end{split}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathbf{x} = (A^T A)^{-1} A^T \mathbf{b} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}^{-1} \begin{pmatrix} 4 \\ 2 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.1: <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|} = (1, 0)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}_2 = \mathbf{a}_2 - \langle \mathbf{q}_1, \mathbf{a}_2 \rangle \mathbf{q}_1 = (0, 1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|} = (0, 1)\)</span>.</p>
<p>Answer and justification for E2.4.3: Let <span class="math notranslate nohighlight">\(\mathbf{w}_1 = (1, 1)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_2 = (1, 0)\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_1 &amp;= \frac{\mathbf{w}_1}{\|\mathbf{w}_1\|} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) \\
\mathbf{q}_2 &amp;= \frac{\mathbf{w}_2 - \langle \mathbf{w}_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\mathbf{w}_2 - \langle \mathbf{w}_2, q_1 \rangle q_1\|} \\
&amp;= \frac{(1, 0) - (\frac{1}{\sqrt{2}})(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})}{\sqrt{1 - (\frac{1}{\sqrt{2}})^2}} \\
&amp;= (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})
\end{align*}\]</div>
<p>So <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> is an orthonormal basis.</p>
<p>Answer and justification for E2.4.5: Using the orthonormal basis <span class="math notranslate nohighlight">\(\{\mathbf{q}_1, \mathbf{q}_2\}\)</span> from E2.4.4, we have <span class="math notranslate nohighlight">\(Q = [\mathbf{q}_1\ \mathbf{q}_2]\)</span>. To find <span class="math notranslate nohighlight">\(R\)</span>, we observe that <span class="math notranslate nohighlight">\(\mathbf{a}_1 = \sqrt{3}\mathbf{q}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{a}_2 = \frac{1}{\sqrt{3}}\mathbf{q}_1 + \frac{5}{\sqrt{21}}\mathbf{q}_2\)</span>. Therefore, <span class="math notranslate nohighlight">\(R = \begin{pmatrix} \sqrt{3} &amp; \frac{1}{\sqrt{3}} \\ 0 &amp; \frac{5}{\sqrt{21}} \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.7: <span class="math notranslate nohighlight">\(\mathbf{x} = (2, 1)\)</span>. From the second equation, <span class="math notranslate nohighlight">\(3x_2 = 3\)</span>, so <span class="math notranslate nohighlight">\(x_2 = 1\)</span>. Substituting into the first equation, <span class="math notranslate nohighlight">\(2x_1 - 1 = 4\)</span>, so <span class="math notranslate nohighlight">\(x_1 = 2\)</span>.</p>
<p>Answer and justification for E2.4.9: <span class="math notranslate nohighlight">\(H = I_{3 \times 3} - 2\mathbf{z}\mathbf{z}^T/\|\mathbf{z}\|^2 = \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.11: To verify that <span class="math notranslate nohighlight">\(H_1\)</span> is orthogonal, we check if <span class="math notranslate nohighlight">\(H_1^T H_1 = I_{2 \times 2}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_1^T H_1 = \begin{pmatrix} \frac{7}{5} &amp; -\frac{6}{5} \\ -\frac{6}{5} &amp; -\frac{1}{5} \end{pmatrix} \begin{pmatrix} \frac{7}{5} &amp; -\frac{6}{5} \\ -\frac{6}{5} &amp; -\frac{1}{5} \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = I_{2 \times 2}.
\end{split}\]</div>
<p>To verify that <span class="math notranslate nohighlight">\(H_1\)</span> is symmetric, we check if <span class="math notranslate nohighlight">\(H_1^T = H_1\)</span>, which is true by observation.</p>
<p>Answer and justification for E2.4.13: We have <span class="math notranslate nohighlight">\(H_1 A = R\)</span>, where <span class="math notranslate nohighlight">\(R = \begin{pmatrix} -\frac{\sqrt{10}}{5} &amp; -\frac{2}{\sqrt{10}} \\ 0 &amp; \frac{14}{5} \end{pmatrix}\)</span> is upper triangular. Therefore, <span class="math notranslate nohighlight">\(Q^T = H_1\)</span>, and <span class="math notranslate nohighlight">\(Q = H_1^T = \begin{pmatrix} \frac{7}{5} &amp; \frac{6}{5} \\ -\frac{6}{5} &amp; \frac{1}{5} \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.15: Let <span class="math notranslate nohighlight">\(\mathbf{y}_1 = (3, 4)^T\)</span> be the first column of <span class="math notranslate nohighlight">\(A\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(2)} - \mathbf{y}_1 = (5, -4)^T\)</span> and <span class="math notranslate nohighlight">\(H_1 = I_{2 \times 2} - 2\mathbf{z}_1\mathbf{z}_1^T / \|\mathbf{z}_1\|^2 = \begin{pmatrix} 3/5 &amp; 4/5 \\ 4/5 &amp; -3/5 \end{pmatrix}\)</span>. We can verify that <span class="math notranslate nohighlight">\(H_1 A = \begin{pmatrix} 5 &amp; 1 \\ 0 &amp; -2/5 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.4.17:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Q &amp;= \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \end{pmatrix}, \\
R &amp;= Q^T A = \begin{pmatrix} \sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2} \end{pmatrix}, \\
Q^T \mathbf{b} &amp;= \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 2 \\ 0 \end{pmatrix} = \begin{pmatrix} \sqrt{2} \\ \sqrt{2} \end{pmatrix}, \\
R \mathbf{x} &amp;= Q^T \mathbf{b}, \quad \begin{pmatrix} \sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2} \end{pmatrix} \mathbf{x} = \begin{pmatrix} \sqrt{2} \\ \sqrt{2} \end{pmatrix}, \\
\mathbf{x} &amp;= \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\end{align*}\]</div>
<p>The solution is <span class="math notranslate nohighlight">\(\mathbf{x} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Answer and justification for E2.5.1:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{pmatrix} 
1 &amp; 1 \\
1 &amp; 2 \\
1 &amp; 3 \\
1 &amp; 4
\end{pmatrix}, \quad 
\mathbf{y} = \begin{pmatrix}
2 \\
4 \\
5 \\
7
\end{pmatrix}.
\end{split}\]</div>
<p>The normal equations are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T A \boldsymbol{\beta} = A^T \mathbf{y} \Rightarrow
\begin{pmatrix}
4 &amp; 10 \\
10 &amp; 30
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix} =
\begin{pmatrix}
18 \\
47
\end{pmatrix}.
\end{split}\]</div>
<p>Solving this system of equations yields <span class="math notranslate nohighlight">\(\beta_0 = \frac{1}{2}\)</span> and <span class="math notranslate nohighlight">\(\beta_1 = \frac{3}{2}\)</span>.</p>
<p>Answer and justification for E2.5.3:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \end{bmatrix} \quad \mathbf{y} = \begin{bmatrix} 3 \\ 5 \\ 8 \end{bmatrix}
\end{split}\]</div>
<p>Answer and justification for E2.5.5:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = (A^T A)^{-1} A^T \mathbf{y} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}
\end{split}\]</div>
<p>We solve the linear system by inverting <span class="math notranslate nohighlight">\(A^T A\)</span> and multiplying by <span class="math notranslate nohighlight">\(A^T \mathbf{y}\)</span>.</p>
<p>Answer and justification for E2.5.7:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; -1 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<p>For a quadratic model, we need columns for <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(x^2\)</span>.</p>
<p>Answer and justification for E2.5.9: We solve the normal equations <span class="math notranslate nohighlight">\(A^T A \boldsymbol{\beta} = A^T \mathbf{y}\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A^T A &amp;= \begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \end{pmatrix} = \begin{pmatrix} 3 &amp; 6 \\ 6 &amp; 14 \end{pmatrix}, \\
A^T \mathbf{y} &amp;= \begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} = \begin{pmatrix} 6 \\ 14 \end{pmatrix}.
\end{align*}\]</div>
<p>Solving <span class="math notranslate nohighlight">\(\begin{pmatrix} 3 &amp; 6 \\ 6 &amp; 14 \end{pmatrix} \boldsymbol{\beta} = \begin{pmatrix} 6 \\ 14 \end{pmatrix}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\boldsymbol{\beta} &amp;= \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
\end{align*}\]</div>
<p>Answer and justification for E2.5.11: Compute the predicted <span class="math notranslate nohighlight">\(y\)</span> values:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{y}_1 &amp;= 2(1) + 1 = 3, \\
\hat{y}_2 &amp;= 2(2) + 1 = 5, \\
\hat{y}_3 &amp;= 2(3) + 1 = 7.
\end{align*}\]</div>
<p>Compute the residuals:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
r_1 &amp;= 3 - 3 = 0, \\
r_2 &amp;= 5 - 5 = 0, \\
r_3 &amp;= 7 - 7 = 0.
\end{align*}\]</div>
<p>RSS is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{RSS} &amp;= 0^2 + 0^2 + 0^2 = 0.
\end{align*}\]</div>
&#13;

<h3><span class="section-number">2.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define the concepts of linear subspaces, spans, and bases in vector spaces.</p></li>
<li><p>Check the conditions for a set of vectors to be linearly independent or form a basis of a vector space.</p></li>
<li><p>Define the inverse of a nonsingular matrix and prove its uniqueness.</p></li>
<li><p>Define the dimension of a linear subspace.</p></li>
<li><p>State the Pythagorean Theorem.</p></li>
<li><p>Verify whether a given list of vectors is orthonormal.</p></li>
<li><p>Derive the coefficients in an orthonormal basis expansion of a vector.</p></li>
<li><p>Apply the Gram-Schmidt process to transform a basis into an orthonormal basis.</p></li>
<li><p>Prove key theorems related to vector spaces and matrix inverses, such as the Pythagorean theorem for orthogonal vectors and the conditions for linear independence in matrix columns.</p></li>
<li><p>Illustrate examples of linear subspaces and their bases, using both theoretical definitions and numerical examples.</p></li>
<li><p>Define the orthogonal projection of a vector onto a linear subspace and characterize it geometrically.</p></li>
<li><p>Compute the orthogonal projection of a vector onto a linear subspace spanned by an orthonormal list of vectors.</p></li>
<li><p>Illustrate the process of finding the orthogonal projection of a vector onto a subspace using geometric intuition.</p></li>
<li><p>Prove the uniqueness of the orthogonal projection.</p></li>
<li><p>Express the orthogonal projection in matrix form.</p></li>
<li><p>Define the orthogonal complement of a linear subspace and find an orthonormal basis for it.</p></li>
<li><p>State and prove the Orthogonal Decomposition Lemma, and use it to decompose a vector into its orthogonal projection and a vector in the orthogonal complement.</p></li>
<li><p>Formulate the linear least squares problem as an optimization problem and derive the normal equations.</p></li>
<li><p>Apply the concepts of orthogonal projection and orthogonal complement to solve overdetermined systems using the least squares method.</p></li>
<li><p>Determine the uniqueness of the least squares solution based on the linear independence of the columns of the matrix.</p></li>
<li><p>Implement the Gram-Schmidt algorithm to obtain an orthonormal basis from a set of linearly independent vectors.</p></li>
<li><p>Express the Gram-Schmidt algorithm using a matrix factorization perspective, known as the QR decomposition.</p></li>
<li><p>Apply the QR decomposition to solve linear least squares problems as an alternative to the normal equations approach.</p></li>
<li><p>Define Householder reflections and explain their role in introducing zeros below the diagonal of a matrix.</p></li>
<li><p>Construct a QR decomposition using a sequence of Householder transformations.</p></li>
<li><p>Compare the numerical stability of the Gram-Schmidt algorithm and the Householder transformations approach for computing the QR decomposition.</p></li>
<li><p>Formulate the linear regression problem as a linear least squares optimization problem.</p></li>
<li><p>Extend the linear regression model to polynomial regression by incorporating higher-degree terms and interaction terms in the design matrix.</p></li>
<li><p>Recognize and explain the issue of overfitting in polynomial regression and its potential consequences.</p></li>
<li><p>Apply the least squares method to simulated data and real-world datasets, such as the Advertising dataset, to estimate regression coefficients.</p></li>
<li><p>Understand and implement the bootstrap method for linear regression to assess the variability of estimated coefficients and determine the statistical significance of the results.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
&#13;

<h2><span class="section-number">2.7.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="orthogonality-in-high-dimension">
<h3><span class="section-number">2.7.2.1. </span>Orthogonality in high dimension<a class="headerlink" href="#orthogonality-in-high-dimension" title="Link to this heading">#</a></h3>
<p>In high dimension, orthogonality – or more accurately near-orthogonality – is more common than one might expect. We illustrate this phenomenon here.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. Its joint PDF depends only on the its norm <span class="math notranslate nohighlight">\(\|\mathbf{X}\|\)</span>. So <span class="math notranslate nohighlight">\(\mathbf{Y} = \frac{\mathbf{X}}{\|\mathbf{X}\|}\)</span> is uniformly distributed over the <span class="math notranslate nohighlight">\((d-1)\)</span>-sphere <span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}\in \mathbb{R}^d:\|\mathbf{x}\|=1\}\)</span>, that is, the surface of the unit <span class="math notranslate nohighlight">\(d\)</span>-ball centered aroungd the origin. We write <span class="math notranslate nohighlight">\(\mathbf{Y} \sim \mathrm{U}[\mathcal{S}]\)</span>. The following theorem shows that if we take two independent samples <span class="math notranslate nohighlight">\(\mathbf{Y}_1, \mathbf{Y}_2 \sim \mathrm{U}[\mathcal{S}]\)</span> they are likely to be nearly orthogonal when <span class="math notranslate nohighlight">\(d\)</span> is large, that is, <span class="math notranslate nohighlight">\(|\langle\mathbf{Y}_1, \mathbf{Y}_2\rangle|\)</span> is likely to be small. By symmetry, there is no loss of generality in taking one of the two vectors to be the north pole <span class="math notranslate nohighlight">\(\mathbf{e}_d = (0,\ldots,0,1)\)</span>. A different way to state the theorem is that most of the mass of the <span class="math notranslate nohighlight">\((d-1)\)</span>-sphere is in a small band around the equator.</p>
<p><strong>Figure:</strong> Band around the equator (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:World_map_with_major_latitude_circles.jpg">Source</a>)</p>
<p><img alt="Band around the equator" src="../Images/88d34eed9002aa55ad262c1e2e281d2c.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/World_map_with_major_latitude_circles.jpg/640px-World_map_with_major_latitude_circles.jpg"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>THEOREM</strong> <strong>(Orthogonality in High Dimension)</strong> Let <span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}\in \mathbb{R}^d:\|\mathbf{x}\|=1\}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y} \sim \mathrm{U}[\mathcal{S}]\)</span>. Then for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[|\langle\mathbf{Y}, \mathbf{e}_d\rangle| \geq \varepsilon]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We write <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> in terms of a standard Normal. Its squared norm is a sum of independent random variables. After bringing it to the numerator, we can apply <em>Chebyshev</em>.</p>
<p><em>Proof:</em> Recall that <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is <span class="math notranslate nohighlight">\(\frac{\mathbf{X}}{\|\mathbf{X}\|}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. The probability we want to bound can be re-written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[|\langle\mathbf{Y}, \mathbf{e}_d\rangle| \geq \varepsilon]
&amp;=
\mathbb{P}\left[\left|\left\langle\frac{\mathbf{X}}{\|\mathbf{X}\|}, \mathbf{e}_d\right\rangle\right|^2 \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[\left|\frac{\langle\mathbf{X},\mathbf{e}_d\rangle}{\|\mathbf{X}\|}\right|^2 \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[\frac{X_d^2}{\sum_{j=1}^d X_j^2} \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[X_d^2 \geq \varepsilon^2 \sum_{j=1}^d X_j^2\right]\\
&amp;=
\mathbb{P}\left[\sum_{j=1}^{d-1} (-\varepsilon^2 X_j^2) + (1-\varepsilon^2) X_d^2 \geq 0\right].
\end{align*}\]</div>
<p>We are now looking at a sum of independent (but not identically distributed) random variables</p>
<div class="math notranslate nohighlight">
\[
Z = \sum_{j=1}^{d-1} (-\varepsilon^2 X_j^2) + (1-\varepsilon^2) X_d^2
\]</div>
<p>and we can appeal to our usual Chebyshev machinery. The expectation is, by linearity,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[Z]
= 
- \sum_{j=1}^{d-1} \varepsilon^2 \mathbb{E}[X_j^2] + (1-\varepsilon^2) \mathbb{E}[X_d^2]
= 
\{- (d-1) \,\varepsilon^2  + (1-\varepsilon^2)\}
\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(X_1,\ldots,X_d\)</span> are standard Normal variables and that, in particular, their mean is <span class="math notranslate nohighlight">\(0\)</span> and their variance is <span class="math notranslate nohighlight">\(1\)</span> so that <span class="math notranslate nohighlight">\(\mathbb{E}[X_1^2] = 1\)</span>.</p>
<p>The variance is, by independence of the <span class="math notranslate nohighlight">\(X_j\)</span>’s,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[Z]
&amp;= 
\sum_{j=1}^{d-1} \varepsilon^4 \mathrm{Var}[X_j^2] + (1-\varepsilon^2)^2 \mathrm{Var}[X_d^2]\\
&amp;= 
\{(d-1) \,\varepsilon^4  + (1-\varepsilon^2)^2\}\mathrm{Var}[X_1^2].
\end{align*}\]</div>
<p>So by Chebyshev</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}\left[Z \geq 0\right]
&amp;\leq 
\mathbb{P}\left[\left|Z 
- \mathbb{E}[Z]\right|\geq |\mathbb{E}[Z]|\right]\\
&amp;\leq \frac{\mathrm{Var}[Z]}{\mathbb{E}[Z]^2}\\
&amp;= \frac{\{(d-1) \,\varepsilon^4  + (1-\varepsilon^2)^2\} \mathrm{Var}[X_1^2]}{\{- (d-1) \,\varepsilon^2  + (1-\varepsilon^2)\}^2}\\
&amp;\to 0
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. To get the limit we observed that, for large <span class="math notranslate nohighlight">\(d\)</span>,
the deminator scales like <span class="math notranslate nohighlight">\(d^2\)</span> while the numerator scales only like <span class="math notranslate nohighlight">\(d\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
<section id="bootstrapping-for-linear-regression">
<h3><span class="section-number">2.7.2.2. </span>Bootstrapping for linear regression<a class="headerlink" href="#bootstrapping-for-linear-regression" title="Link to this heading">#</a></h3>
<p>We return to the linear case, but with the full set of predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'advertising.csv'</span><span class="p">)</span>
<span class="n">TV</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'TV'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">sales</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sales'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">TV</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">radio</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'radio'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">newspaper</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'newspaper'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newspaper</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'radio'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'newspaper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/51317ef101b3cee12a03056622483261592dcb9f2c317195483b55e872b586cd.png" src="../Images/d22fa5138c0a40fab66a6fe2d01db90a.png" data-original-src="https://mmids-textbook.github.io/_images/51317ef101b3cee12a03056622483261592dcb9f2c317195483b55e872b586cd.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<p>Newspaper advertising (the last coefficient) seems to have a much weaker effect on sales per dollar spent. Next, we briefly sketch one way to assess the <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a> of such a conclusion.</p>
<p>Our coefficients are estimated from a sample. There is intrinsic variability in our sampling procedure. We would like to understand how our estimated coefficients compare to the true coefficients. This is set up beautifully in [<a class="reference external" href="https://www.inferentialthinking.com/chapters/13/2/Bootstrap.html">Data8</a>, Section 13.2]:</p>
<blockquote>
<div><p>A data scientist is using the data in a random sample to estimate an unknown parameter. She uses the sample to calculate the value of a statistic that she will use as her estimate. Once she has calculated the observed value of her statistic, she could just present it as her estimate and go on her merry way. But she’s a data scientist. She knows that her random sample is just one of numerous possible random samples, and thus her estimate is just one of numerous plausible estimates. By how much could those estimates vary? To answer this, it appears as though she needs to draw another sample from the population, and compute a new estimate based on the new sample. But she doesn’t have the resources to go back to the population and draw another sample. It looks as though the data scientist is stuck. Fortunately, a brilliant idea called <em>the bootstrap</em> can help her out. Since it is not feasible to generate new samples from the population, the bootstrap generates new random samples by a method called <em>resampling</em>: the new samples are drawn at random <em>from the original sample</em>.</p>
</div></blockquote>
<p>Without going into full details (see [<a class="reference external" href="http://www.textbook.ds100.org/ch/17/inf_pred_gen_boot.html">DS100</a>, Section 17.3] for more), it works as follows. Let <span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span> be our data. We assume that our sample is representative of the population and we simulate our sampling procedure by resampling from the sample. That is, we take a random sample with replacement <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1} = \{(\mathbf{x}_i, y_i)\,:\,i \in I\}\)</span> where <span class="math notranslate nohighlight">\(I\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Multiset">multi-set</a> of elements from <span class="math notranslate nohighlight">\([n]\)</span> of size <span class="math notranslate nohighlight">\(n\)</span>. We recompute our estimated coefficients on <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1}\)</span>. Then we repeat independently for a desired number of replicates <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1}, \ldots, \mathcal{X}_{\mathrm{boot},r}\)</span>. Plotting a histogram of the resulting coefficients gives some idea of the variability of our estimates.</p>
<p>We implement the bootstrap for linear regression in Python next.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">replicates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="mi">10000</span><span class="p">)):</span>
    <span class="n">n</span><span class="p">,</span><span class="n">m</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">replicates</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">replicates</span><span class="p">):</span>
        <span class="n">resample</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
        <span class="n">Aboot</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">resample</span><span class="p">,:]</span>
        <span class="n">bboot</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">resample</span><span class="p">]</span>
        <span class="n">coeff_boot</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">Aboot</span><span class="p">,</span><span class="n">bboot</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coeff_boot</span>
</pre></div>
</div>
</div>
</div>
<p>First, let’s use a simple example with a known ground truth.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="n">x</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The estimated coefficients are the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-1.03381171  1.01808039]
</pre></div>
</div>
</div>
</div>
<p>Now we apply the bootstrap and plot histograms of the two coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'coeff1'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'coeff2'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c3cc7dc60c56be78f02bf1dccdc5ae5560950b1a92eae82cf8a24e842c7301dd.png" src="../Images/f4b22f1cd76691835cbf7152bc1a7575.png" data-original-src="https://mmids-textbook.github.io/_images/c3cc7dc60c56be78f02bf1dccdc5ae5560950b1a92eae82cf8a24e842c7301dd.png"/>
</div>
</div>
<p>We see in the histograms that the true coefficient values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> fall within the likely range.</p>
<p>We return to the <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset and apply the bootstrap. Plotting a histogram of the coefficients corresponding to newspaper advertising shows that <span class="math notranslate nohighlight">\(0\)</span> is a plausible value, while it is not for TV advertising.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">TV</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">3</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'TV'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'newspaper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9f419f9fef7226b110376f01b6e45d6805cdd3997bd2325fc8d914339f26c77b.png" src="../Images/575c5f6b502edd0aa5ae4daa062cf6dd.png" data-original-src="https://mmids-textbook.github.io/_images/9f419f9fef7226b110376f01b6e45d6805cdd3997bd2325fc8d914339f26c77b.png"/>
</div>
</div>
</section>
&#13;

<h3><span class="section-number">2.7.2.1. </span>Orthogonality in high dimension<a class="headerlink" href="#orthogonality-in-high-dimension" title="Link to this heading">#</a></h3>
<p>In high dimension, orthogonality – or more accurately near-orthogonality – is more common than one might expect. We illustrate this phenomenon here.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. Its joint PDF depends only on the its norm <span class="math notranslate nohighlight">\(\|\mathbf{X}\|\)</span>. So <span class="math notranslate nohighlight">\(\mathbf{Y} = \frac{\mathbf{X}}{\|\mathbf{X}\|}\)</span> is uniformly distributed over the <span class="math notranslate nohighlight">\((d-1)\)</span>-sphere <span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}\in \mathbb{R}^d:\|\mathbf{x}\|=1\}\)</span>, that is, the surface of the unit <span class="math notranslate nohighlight">\(d\)</span>-ball centered aroungd the origin. We write <span class="math notranslate nohighlight">\(\mathbf{Y} \sim \mathrm{U}[\mathcal{S}]\)</span>. The following theorem shows that if we take two independent samples <span class="math notranslate nohighlight">\(\mathbf{Y}_1, \mathbf{Y}_2 \sim \mathrm{U}[\mathcal{S}]\)</span> they are likely to be nearly orthogonal when <span class="math notranslate nohighlight">\(d\)</span> is large, that is, <span class="math notranslate nohighlight">\(|\langle\mathbf{Y}_1, \mathbf{Y}_2\rangle|\)</span> is likely to be small. By symmetry, there is no loss of generality in taking one of the two vectors to be the north pole <span class="math notranslate nohighlight">\(\mathbf{e}_d = (0,\ldots,0,1)\)</span>. A different way to state the theorem is that most of the mass of the <span class="math notranslate nohighlight">\((d-1)\)</span>-sphere is in a small band around the equator.</p>
<p><strong>Figure:</strong> Band around the equator (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:World_map_with_major_latitude_circles.jpg">Source</a>)</p>
<p><img alt="Band around the equator" src="../Images/88d34eed9002aa55ad262c1e2e281d2c.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/World_map_with_major_latitude_circles.jpg/640px-World_map_with_major_latitude_circles.jpg"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>THEOREM</strong> <strong>(Orthogonality in High Dimension)</strong> Let <span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}\in \mathbb{R}^d:\|\mathbf{x}\|=1\}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y} \sim \mathrm{U}[\mathcal{S}]\)</span>. Then for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[|\langle\mathbf{Y}, \mathbf{e}_d\rangle| \geq \varepsilon]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We write <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> in terms of a standard Normal. Its squared norm is a sum of independent random variables. After bringing it to the numerator, we can apply <em>Chebyshev</em>.</p>
<p><em>Proof:</em> Recall that <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is <span class="math notranslate nohighlight">\(\frac{\mathbf{X}}{\|\mathbf{X}\|}\)</span>
where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a standard Normal <span class="math notranslate nohighlight">\(d\)</span>-vector. The probability we want to bound can be re-written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[|\langle\mathbf{Y}, \mathbf{e}_d\rangle| \geq \varepsilon]
&amp;=
\mathbb{P}\left[\left|\left\langle\frac{\mathbf{X}}{\|\mathbf{X}\|}, \mathbf{e}_d\right\rangle\right|^2 \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[\left|\frac{\langle\mathbf{X},\mathbf{e}_d\rangle}{\|\mathbf{X}\|}\right|^2 \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[\frac{X_d^2}{\sum_{j=1}^d X_j^2} \geq \varepsilon^2\right]\\
&amp;=
\mathbb{P}\left[X_d^2 \geq \varepsilon^2 \sum_{j=1}^d X_j^2\right]\\
&amp;=
\mathbb{P}\left[\sum_{j=1}^{d-1} (-\varepsilon^2 X_j^2) + (1-\varepsilon^2) X_d^2 \geq 0\right].
\end{align*}\]</div>
<p>We are now looking at a sum of independent (but not identically distributed) random variables</p>
<div class="math notranslate nohighlight">
\[
Z = \sum_{j=1}^{d-1} (-\varepsilon^2 X_j^2) + (1-\varepsilon^2) X_d^2
\]</div>
<p>and we can appeal to our usual Chebyshev machinery. The expectation is, by linearity,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[Z]
= 
- \sum_{j=1}^{d-1} \varepsilon^2 \mathbb{E}[X_j^2] + (1-\varepsilon^2) \mathbb{E}[X_d^2]
= 
\{- (d-1) \,\varepsilon^2  + (1-\varepsilon^2)\}
\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(X_1,\ldots,X_d\)</span> are standard Normal variables and that, in particular, their mean is <span class="math notranslate nohighlight">\(0\)</span> and their variance is <span class="math notranslate nohighlight">\(1\)</span> so that <span class="math notranslate nohighlight">\(\mathbb{E}[X_1^2] = 1\)</span>.</p>
<p>The variance is, by independence of the <span class="math notranslate nohighlight">\(X_j\)</span>’s,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{Var}[Z]
&amp;= 
\sum_{j=1}^{d-1} \varepsilon^4 \mathrm{Var}[X_j^2] + (1-\varepsilon^2)^2 \mathrm{Var}[X_d^2]\\
&amp;= 
\{(d-1) \,\varepsilon^4  + (1-\varepsilon^2)^2\}\mathrm{Var}[X_1^2].
\end{align*}\]</div>
<p>So by Chebyshev</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}\left[Z \geq 0\right]
&amp;\leq 
\mathbb{P}\left[\left|Z 
- \mathbb{E}[Z]\right|\geq |\mathbb{E}[Z]|\right]\\
&amp;\leq \frac{\mathrm{Var}[Z]}{\mathbb{E}[Z]^2}\\
&amp;= \frac{\{(d-1) \,\varepsilon^4  + (1-\varepsilon^2)^2\} \mathrm{Var}[X_1^2]}{\{- (d-1) \,\varepsilon^2  + (1-\varepsilon^2)\}^2}\\
&amp;\to 0
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. To get the limit we observed that, for large <span class="math notranslate nohighlight">\(d\)</span>,
the deminator scales like <span class="math notranslate nohighlight">\(d^2\)</span> while the numerator scales only like <span class="math notranslate nohighlight">\(d\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
&#13;

<h3><span class="section-number">2.7.2.2. </span>Bootstrapping for linear regression<a class="headerlink" href="#bootstrapping-for-linear-regression" title="Link to this heading">#</a></h3>
<p>We return to the linear case, but with the full set of predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'advertising.csv'</span><span class="p">)</span>
<span class="n">TV</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'TV'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">sales</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sales'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">TV</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">radio</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'radio'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">newspaper</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'newspaper'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">radio</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newspaper</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'radio'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'newspaper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/51317ef101b3cee12a03056622483261592dcb9f2c317195483b55e872b586cd.png" src="../Images/d22fa5138c0a40fab66a6fe2d01db90a.png" data-original-src="https://mmids-textbook.github.io/_images/51317ef101b3cee12a03056622483261592dcb9f2c317195483b55e872b586cd.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<p>Newspaper advertising (the last coefficient) seems to have a much weaker effect on sales per dollar spent. Next, we briefly sketch one way to assess the <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a> of such a conclusion.</p>
<p>Our coefficients are estimated from a sample. There is intrinsic variability in our sampling procedure. We would like to understand how our estimated coefficients compare to the true coefficients. This is set up beautifully in [<a class="reference external" href="https://www.inferentialthinking.com/chapters/13/2/Bootstrap.html">Data8</a>, Section 13.2]:</p>
<blockquote>
<div><p>A data scientist is using the data in a random sample to estimate an unknown parameter. She uses the sample to calculate the value of a statistic that she will use as her estimate. Once she has calculated the observed value of her statistic, she could just present it as her estimate and go on her merry way. But she’s a data scientist. She knows that her random sample is just one of numerous possible random samples, and thus her estimate is just one of numerous plausible estimates. By how much could those estimates vary? To answer this, it appears as though she needs to draw another sample from the population, and compute a new estimate based on the new sample. But she doesn’t have the resources to go back to the population and draw another sample. It looks as though the data scientist is stuck. Fortunately, a brilliant idea called <em>the bootstrap</em> can help her out. Since it is not feasible to generate new samples from the population, the bootstrap generates new random samples by a method called <em>resampling</em>: the new samples are drawn at random <em>from the original sample</em>.</p>
</div></blockquote>
<p>Without going into full details (see [<a class="reference external" href="http://www.textbook.ds100.org/ch/17/inf_pred_gen_boot.html">DS100</a>, Section 17.3] for more), it works as follows. Let <span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span> be our data. We assume that our sample is representative of the population and we simulate our sampling procedure by resampling from the sample. That is, we take a random sample with replacement <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1} = \{(\mathbf{x}_i, y_i)\,:\,i \in I\}\)</span> where <span class="math notranslate nohighlight">\(I\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Multiset">multi-set</a> of elements from <span class="math notranslate nohighlight">\([n]\)</span> of size <span class="math notranslate nohighlight">\(n\)</span>. We recompute our estimated coefficients on <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1}\)</span>. Then we repeat independently for a desired number of replicates <span class="math notranslate nohighlight">\(\mathcal{X}_{\mathrm{boot},1}, \ldots, \mathcal{X}_{\mathrm{boot},r}\)</span>. Plotting a histogram of the resulting coefficients gives some idea of the variability of our estimates.</p>
<p>We implement the bootstrap for linear regression in Python next.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">replicates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="mi">10000</span><span class="p">)):</span>
    <span class="n">n</span><span class="p">,</span><span class="n">m</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">replicates</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">replicates</span><span class="p">):</span>
        <span class="n">resample</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
        <span class="n">Aboot</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">resample</span><span class="p">,:]</span>
        <span class="n">bboot</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">resample</span><span class="p">]</span>
        <span class="n">coeff_boot</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">Aboot</span><span class="p">,</span><span class="n">bboot</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coeff_boot</span>
</pre></div>
</div>
</div>
</div>
<p>First, let’s use a simple example with a known ground truth.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="n">x</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The estimated coefficients are the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-1.03381171  1.01808039]
</pre></div>
</div>
</div>
</div>
<p>Now we apply the bootstrap and plot histograms of the two coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'coeff1'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'coeff2'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c3cc7dc60c56be78f02bf1dccdc5ae5560950b1a92eae82cf8a24e842c7301dd.png" src="../Images/f4b22f1cd76691835cbf7152bc1a7575.png" data-original-src="https://mmids-textbook.github.io/_images/c3cc7dc60c56be78f02bf1dccdc5ae5560950b1a92eae82cf8a24e842c7301dd.png"/>
</div>
</div>
<p>We see in the histograms that the true coefficient values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> fall within the likely range.</p>
<p>We return to the <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset and apply the bootstrap. Plotting a histogram of the coefficients corresponding to newspaper advertising shows that <span class="math notranslate nohighlight">\(0\)</span> is a plausible value, while it is not for TV advertising.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">TV</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">coeff_boot</span> <span class="o">=</span> <span class="n">linregboot</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span><span class="n">sales</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coeff_boot</span><span class="p">[</span><span class="mi">3</span><span class="p">,:],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'lightblue'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'TV'</span><span class="p">),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'newspaper'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9f419f9fef7226b110376f01b6e45d6805cdd3997bd2325fc8d914339f26c77b.png" src="../Images/575c5f6b502edd0aa5ae4daa062cf6dd.png" data-original-src="https://mmids-textbook.github.io/_images/9f419f9fef7226b110376f01b6e45d6805cdd3997bd2325fc8d914339f26c77b.png"/>
</div>
</div>
    
</body>
</html>