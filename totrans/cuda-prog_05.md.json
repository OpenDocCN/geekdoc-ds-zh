["```cpp\nvoid some_func(void)\n{\n  int i;\n```", "```cpp\n  for (i=0;i<128;i++)\n  {\n    a[i] = b[i] ∗ c[i];\n  }\n}\n```", "```cpp\na[i] = b[i] ∗ c[i];\n```", "```cpp\n__global__ void some_kernel_func(int ∗ const a, const int ∗ const b, const int ∗ const c)\n{\n  a[i] = b[i] ∗ c[i];\n}\n```", "```cpp\n__global__ void some_kernel_func(int ∗ const a, const int ∗ const b, const int ∗ const c)\n{\n  const unsigned int thread_idx = threadIdx.x;\n  a[thread_idx] = b[thread_idx] ∗ c[thread_idx];\n}\n```", "```cpp\nkernel_function<<<num_blocks, num_threads>>>(param1, param2, …)\n```", "```cpp\nkernel_function<<<num_blocks, num_threads>>>(param1, param2,…..)\n```", "```cpp\nsome_kernel_func<<< 2, 128 >>>(a, b, c);\n```", "```cpp\n__global__ void some_kernel_func(int ∗ const a, const int ∗ const b, const int ∗ const c)\n{\n  const unsigned int thread_idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  a[thread_idx] = b[thread_idx] ∗ c[thread_idx];\n}\n```", "```cpp\nsome_kernel_func<<< 2, 64 >>>(a, b, c);\n```", "```cpp\n__global__ void what_is_my_id(unsigned int ∗ const block,\n                              unsigned int ∗ const thread,\n                              unsigned int ∗ const warp,\n                              unsigned int ∗ const calc_thread)\n{\n  /∗ Thread id is block index ∗ block size + thread offset into the block ∗/\n  const unsigned int thread_idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n  block[thread_idx] = blockIdx.x;\n  thread[thread_idx] = threadIdx.x;\n```", "```cpp\n  /∗ Calculate warp using built in variable warpSize ∗/\n  warp[thread_idx] = threadIdx.x / warpSize;\n```", "```cpp\n  calc_thread[thread_idx] = thread_idx;\n}\n```", "```cpp\n#include <stdio.h>\n#include <stdlib.h>\n#include <conio.h>\n```", "```cpp\n__global__ void what_is_my_id(unsigned int ∗ const block,\n              unsigned int ∗ const thread,\n              unsigned int ∗ const warp,\n              unsigned int ∗ const calc_thread)\n{\n  /∗ Thread id is block index ∗ block size + thread offset into the block ∗/\n  const unsigned int thread_idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  block[thread_idx] = blockIdx.x;\n  thread[thread_idx] = threadIdx.x;\n```", "```cpp\n  /∗ Calculate warp using built in variable warpSize ∗/\n  warp[thread_idx] = threadIdx.x / warpSize;\n```", "```cpp\n  calc_thread[thread_idx] = thread_idx;\n```", "```cpp\n#define ARRAY_SIZE 128\n#define ARRAY_SIZE_IN_BYTES (sizeof(unsigned int) ∗ (ARRAY_SIZE))\n```", "```cpp\n/∗ Declare statically four arrays of ARRAY_SIZE each ∗/\n```", "```cpp\nunsigned int cpu_block[ARRAY_SIZE];\nunsigned int cpu_thread[ARRAY_SIZE];\nunsigned int cpu_warp[ARRAY_SIZE];\nunsigned int cpu_calc_thread[ARRAY_SIZE];\n```", "```cpp\nint main(void)\n{\n  /∗ Total thread count = 2 ∗ 64 = 128 ∗/\n  const unsigned int num_blocks = 2;\n  const unsigned int num_threads = 64;\n  char ch;\n```", "```cpp\n  /∗ Declare pointers for GPU based params ∗/\n  unsigned int ∗ gpu_block;\n  unsigned int ∗ gpu_thread;\n  unsigned int ∗ gpu_warp;\n  unsigned int ∗ gpu_calc_thread;\n```", "```cpp\n  /∗ Declare loop counter for use later ∗/\n  unsigned int i;\n```", "```cpp\n  /∗ Allocate four arrays on the GPU ∗/\n  cudaMalloc((void ∗∗)&gpu_block, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_thread, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_warp, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_calc_thread, ARRAY_SIZE_IN_BYTES);\n```", "```cpp\n  /∗ Execute our kernel ∗/\n  what_is_my_id<<<num_blocks, num_threads>>>(gpu_block, gpu_thread, gpu_warp,\n                                             gpu_calc_thread);\n```", "```cpp\n /∗ Copy back the gpu results to the CPU ∗/\n   cudaMemcpy(cpu_block, gpu_block, ARRAY_SIZE_IN_BYTES,\n             cudaMemcpyDeviceToHost);\n   cudaMemcpy(cpu_thread, gpu_thread, ARRAY_SIZE_IN_BYTES,\n             cudaMemcpyDeviceToHost);\n   cudaMemcpy(cpu_warp, gpu_warp, ARRAY_SIZE_IN_BYTES,\n             cudaMemcpyDeviceToHost);\n   cudaMemcpy(cpu_calc_thread, gpu_calc_thread, ARRAY_SIZE_IN_BYTES,\n             cudaMemcpyDeviceToHost);\n```", "```cpp\n   cudaFree(gpu_block);\n   cudaFree(gpu_thread);\n   cudaFree(gpu_warp);\n   cudaFree(gpu_calc_thread);\n```", "```cpp\n   /∗ Iterate through the arrays and print ∗/\n   for (i=0; i < ARRAY_SIZE; i++)\n   {\n     printf(\"Calculated Thread: %3u - Block: %2u - Warp %2u - Thread %3u\\n\",\n      cpu_calc_thread[i], cpu_block[i], cpu_warp[i], cpu_thread[i]);\n   }\n   ch = getch();\n}\n```", "```cpp\nCalculated Thread: 0 - Block: 0 - Warp 0 - Thread 0\nCalculated Thread: 1 - Block: 0 - Warp 0 - Thread 1\nCalculated Thread: 2 - Block: 0 - Warp 0 - Thread 2\nCalculated Thread: 3 - Block: 0 - Warp 0 - Thread 3\nCalculated Thread: 4 - Block: 0 - Warp 0 - Thread 4\n…\nCalculated Thread: 30 - Block: 0 - Warp 0 - Thread 30\nCalculated Thread: 31 - Block: 0 - Warp 0 - Thread 31\nCalculated Thread: 32 - Block: 0 - Warp 1 - Thread 32\nCalculated Thread: 33 - Block: 0 - Warp 1 - Thread 33\nCalculated Thread: 34 - Block: 0 - Warp 1 - Thread 34\n…\nCalculated Thread: 62 - Block: 0 - Warp 1 - Thread 62\nCalculated Thread: 63 - Block: 0 - Warp 1 - Thread 63\nCalculated Thread: 64 - Block: 1 - Warp 0 - Thread 0\nCalculated Thread: 65 - Block: 1 - Warp 0 - Thread 1\nCalculated Thread: 66 - Block: 1 - Warp 0 - Thread 2\nCalculated Thread: 67 - Block: 1 - Warp 0 - Thread 3\n…\nCalculated Thread: 94 - Block: 1 - Warp 0 - Thread 30\nCalculated Thread: 95 - Block: 1 - Warp 0 - Thread 31\nCalculated Thread: 96 - Block: 1 - Warp 1 - Thread 32\nCalculated Thread: 97 - Block: 1 - Warp 1 - Thread 33\nCalculated Thread: 98 - Block: 1 - Warp 1 - Thread 34\nCalculated Thread: 99 - Block: 1 - Warp 1 - Thread 35\nCalculated Thread: 100 - Block: 1 - Warp 1 - Thread 36\n…\nCalculated Thread: 126 - Block: 1 - Warp 1 - Thread 62\n```", "```cpp\nconst unsigned int idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\nconst unsigned int idy = (blockIdx.y ∗ blockDim.y) + threadIdx.y;\n```", "```cpp\nsome_array[idy][idx] += 1.0;\n```", "```cpp\ndim3 threads_rect(32,4);\n```", "```cpp\nor\ndim3 threads_square(16,8);\ndim3 blocks_square(2,2);\n```", "```cpp\nsome_kernel_func<<< blocks_rect, threads_rect >>>(a, b, c);\n```", "```cpp\nsome_kernel_func<<< blocks_square, threads_square >>>(a, b, c);\n```", "```cpp\ngridDim.x–The size in blocks of the X dimension of the grid.\ngridDim.y–The size in blocks of the Y dimension of the grid.\n```", "```cpp\nblockDim.x–The size in threads of the X dimension of a single block.\n```", "```cpp\ntheadIdx.x–The offset within a block of the X thread index.\ntheadIdx.y–The offset within a block of the Y thread index.\n```", "```cpp\nthread_idx = ((gridDim.x ∗ blockDim.x) ∗ idy) + idx;\n```", "```cpp\n__global__ void what_is_my_id_2d_A(\nunsigned int ∗ const block_x,\nunsigned int ∗ const block_y,\nunsigned int ∗ const thread,\nunsigned int ∗ const calc_thread,\nunsigned int ∗ const x_thread,\nunsigned int ∗ const y_thread,\n```", "```cpp\nunsigned int ∗ const block_dimx,\nunsigned int ∗ const grid_dimy,\nunsigned int ∗ const block_dimy)\n{\n  const unsigned int idx        = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n  const unsigned int idy        = (blockIdx.y ∗ blockDim.y) + threadIdx.y;\n  const unsigned int thread_idx = ((gridDim.x ∗ blockDim.x) ∗ idy) + idx;\n```", "```cpp\n  block_x[thread_idx]     = blockIdx.x;\n  block_y[thread_idx]     = blockIdx.y;\n  thread[thread_idx]      = threadIdx.x;\n  calc_thread[thread_idx] = thread_idx;\n  x_thread[thread_idx]    = idx;\n  y_thread[thread_idx]    = idy;\n  grid_dimx[thread_idx]   = gridDim.x;\n  block_dimx[thread_idx]  = blockDim.x;\n  grid_dimy[thread_idx]   = gridDim.y;\n  block_dimy[thread_idx]  = blockDim.y;\n}\n```", "```cpp\n#define ARRAY_SIZE_X 32\n#define ARRAY_SIZE_Y 16\n```", "```cpp\n#define ARRAY_SIZE_IN_BYTES ((ARRAY_SIZE_X) ∗ (ARRAY_SIZE_Y) ∗ (sizeof(unsigned int)))\n```", "```cpp\n/∗ Declare statically six arrays of ARRAY_SIZE each ∗/\nunsigned int cpu_block_x[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_block_y[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_thread[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_warp[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_calc_thread[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_xthread[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_ythread[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_grid_dimx[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_block_dimx[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_grid_dimy[ARRAY_SIZE_Y][ARRAY_SIZE_X];\nunsigned int cpu_block_dimy[ARRAY_SIZE_Y][ARRAY_SIZE_X];\n```", "```cpp\nint main(void)\n{\n  /∗ Total thread count = 32 ∗ 4 = 128 ∗/\n  const dim3 threads_rect(32, 4); /∗ 32 ∗ 4 ∗/\n```", "```cpp\n  /∗ Total thread count = 16 ∗ 8 = 128 ∗/\n  const dim3 threads_square(16, 8); /∗ 16 ∗ 8 ∗/\n  const dim3 blocks_square(2,2);\n```", "```cpp\n  /∗ Needed to wait for a character at exit ∗/\n  char ch;\n```", "```cpp\n  /∗ Declare pointers for GPU based params ∗/\n  unsigned int ∗ gpu_block_x;\n  unsigned int ∗ gpu_block_y;\n  unsigned int ∗ gpu_thread;\n  unsigned int ∗ gpu_warp;\n  unsigned int ∗ gpu_calc_thread;\n  unsigned int ∗ gpu_xthread;\n  unsigned int ∗ gpu_ythread;\n  unsigned int ∗ gpu_grid_dimx;\n  unsigned int ∗ gpu_block_dimx;\n  unsigned int ∗ gpu_grid_dimy;\n  unsigned int ∗ gpu_block_dimy;\n```", "```cpp\n  /∗ Allocate four arrays on the GPU ∗/\n  cudaMalloc((void ∗∗)&gpu_block_x, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_block_y, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_thread, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_calc_thread, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_xthread, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_ythread, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_grid_dimx, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_block_dimx, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_grid_dimy, ARRAY_SIZE_IN_BYTES);\n  cudaMalloc((void ∗∗)&gpu_block_dimy, ARRAY_SIZE_IN_BYTES);\n```", "```cpp\n  for (int kernel=0; kernel < 2; kernel++)\n  {\n    switch (kernel)\n    {\n      case 0:\n      {\n        /∗ Execute our kernel ∗/\n        what_is_my_id_2d_A<<<blocks_rect, threads_rect>>>(gpu_block_x, gpu_block_y, gpu_thread, gpu_calc_thread, gpu_xthread, gpu_ythread, gpu_grid_dimx, gpu_block_dimx, gpu_grid_dimy, gpu_block_dimy);\n      } break;\n```", "```cpp\n      case 1:\n```", "```cpp\n       /∗ Execute our kernel ∗/\n       what_is_my_id_2d_A<<<blocks_square, threads_square>>>(gpu_block_x, gpu_block_y, gpu_thread, gpu_calc_thread, gpu_xthread, gpu_ythread, gpu_grid_dimx, gpu_block_dimx, gpu_grid_dimy, gpu_block_dimy);\n      } break;\n```", "```cpp\n      default: exit(1); break;\n    }\n```", "```cpp\n    /∗ Copy back the gpu results to the CPU ∗/\n    cudaMemcpy(cpu_block_x, gpu_block_x, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_block_y, gpu_block_y, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_thread, gpu_thread, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_calc_thread, gpu_calc_thread, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_xthread, gpu_xthread, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_ythread, gpu_ythread, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_grid_dimx, gpu_grid_dimx, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_block_dimx,gpu_block_dimx, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_grid_dimy, gpu_grid_dimy, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n    cudaMemcpy(cpu_block_dimy, gpu_block_dimy, ARRAY_SIZE_IN_BYTES,\n               cudaMemcpyDeviceToHost);\n```", "```cpp\n    printf(\"\\nKernel %d\\n\", kernel);\n    /∗ Iterate through the arrays and print ∗/\n    for (int y=0; y < ARRAY_SIZE_Y; y++)\n    {\n        for (int x=0; x < ARRAY_SIZE_X; x++)\n        {\n            printf(\"CT: %2u BKX: %1u BKY: %1u TID: %2u YTID: %2u XTID: %2u GDX: %1u BDX: %1u GDY %1u BDY %1u\\n\", cpu_calc_thread[y][x], cpu_block_x[y][x], cpu_block_y[y][x], cpu_thread[y][x], cpu_ythread[y][x], cpu_xthread[y][x], cpu_grid_dimx[y][x], cpu_block_dimx[y][x], cpu_grid_dimy[y][x], cpu_block_dimy[y][x]);\n```", "```cpp\n          /∗ Wait for any key so we can see the console window ∗/\n          ch = getch();\n          }\n    }\n    /∗ Wait for any key so we can see the console window ∗/\n```", "```cpp\n    ch = getch();\n  }\n```", "```cpp\n  /∗ Free the arrays on the GPU as now we’re done with them ∗/\n  cudaFree(gpu_block_x);\n  cudaFree(gpu_block_y);\n  cudaFree(gpu_thread);\n  cudaFree(gpu_calc_thread);\n  cudaFree(gpu_xthread);\n  cudaFree(gpu_ythread);\n  cudaFree(gpu_grid_dimx);\n  cudaFree(gpu_block_dimx);\n  cudaFree(gpu_grid_dimy);\n  cudaFree(gpu_block_dimy);\n}\n```", "```cpp\n__global__ some_func(void)\n{\n  if (some_condition)\n  {\n    action_a();\n  }\n  else\n  {\n    action_b();\n  }\n}\n```", "```cpp\nif ((thread_idx % 32) < 16)\n{\n  action_a();\n}\nelse\n{\n  action_b();\n}\n```", "```cpp\nfor (unsigned int i=0; i< max; i++)\n{\n  bin[array[i]]++;\n}\n```", "```cpp\natomicAdd(&value);\n```", "```cpp\n/∗ Each thread writes to one block of 256 elements of global memory and contends for write access ∗/\n```", "```cpp\n__global__ void myhistogram256Kernel_01(\n  const unsigned char const ∗ d_hist_data,\n  unsigned int ∗ const d_bin_data)\n{\n  /∗ Work out our thread id ∗/\n  const unsigned int idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  const unsigned int idy = (blockIdx.y ∗ blockDim.y) + threadIdx.y;\n```", "```cpp\n  const unsigned int tid = idx + idy ∗ blockDim.x ∗ gridDim.x;\n```", "```cpp\n  const unsigned char value = d_hist_data[tid];\n```", "```cpp\n  atomicAdd(&(d_bin_data[value]),1);\n}\n```", "```cpp\n/∗ Each read is 4 bytes, not one, 32 x 4 = 128 byte reads ∗/\n__global__ void myhistogram256Kernel_02(\nconst unsigned int const ∗ d_hist_data,\nunsigned int ∗ const d_bin_data)\n{\n  /∗ Work out our thread id ∗/\n  const unsigned int idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  const unsigned int idy = (blockIdx.y ∗ blockDim.y) + threadIdx.y;\n```", "```cpp\n  const unsigned int tid = idx + idy ∗ blockDim.x ∗ gridDim.x;\n```", "```cpp\n  /∗ Fetch the data value as 32 bit ∗/\n  const unsigned int value_u32 = d_hist_data[tid];\n```", "```cpp\n  atomicAdd(&(d_bin_data[ ((value_u32 & 0x000000FF) ) ]),1);\n```", "```cpp\n  atomicAdd(&(d_bin_data[ ((value_u32 & 0x0000FF00) >> 8 ) ]),1);\n```", "```cpp\n  atomicAdd(&(d_bin_data[ ((value_u32 & 0x00FF0000) >> 16 ) ]),1);\n```", "```cpp\n  atomicAdd(&(d_bin_data[ ((value_u32 & 0xFF000000) >> 24 ) ]),1);\n}\n```", "```cpp\n__shared__ unsigned int d_bin_data_shared[256];\n```", "```cpp\n/∗ Each read is 4 bytes, not one, 32 x 4 = 128 byte reads ∗/\n__global__ void myhistogram256Kernel_03(\nconst unsigned int const ∗ d_hist_data,\nunsigned int ∗ const d_bin_data)\n{\n  /∗ Work out our thread id ∗/\n  const unsigned int idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n  const unsigned int idy = (blockIdx.y ∗ blockDim.y) + threadIdx.y;\n  const unsigned int tid = idx + idy ∗ blockDim.x ∗ gridDim.x;\n```", "```cpp\n  /∗ Clear shared memory ∗/\n  d_bin_data_shared[threadIdx.x] = 0;\n```", "```cpp\n  /∗ Fetch the data value as 32 bit ∗/\n  const unsigned int value_u32 = d_hist_data[tid];\n```", "```cpp\n  /∗ Wait for all threads to update shared memory ∗/\n  __syncthreads();\n```", "```cpp\n  atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0x000000FF) ) ]),1);\n  atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0x0000FF00) >> 8 ) ]),1);\n  atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0x00FF0000) >> 16 ) ]),1);\n  atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0xFF000000) >> 24 ) ]),1);\n```", "```cpp\n  /∗ Wait for all threads to update shared memory ∗/\n  __syncthreads();\n```", "```cpp\n  /∗ The write the accumulated data back to global memory in blocks, not scattered ∗/\n  atomicAdd(&(d_bin_data[threadIdx.x]), d_bin_data_shared[threadIdx.x]);\n}\n```", "```cpp\n/∗ Each read is 4 bytes, not one, 32 x 4 = 128 byte reads ∗/\n/∗ Accumulate into shared memory N times ∗/\n__global__ void myhistogram256Kernel_07(const unsigned int const ∗ d_hist_data,\n                                        unsigned int ∗ const d_bin_data,\n                                        unsigned int N)\n{\n  /∗ Work out our thread id ∗/\n  const unsigned int idx = (blockIdx.x ∗ (blockDim.x∗N) ) + threadIdx.x;\n  const unsigned int idy = (blockIdx.y ∗ blockDim.y ) + threadIdx.y;\n  const unsigned int tid = idx + idy ∗ (blockDim.x∗N) ∗ (gridDim.x);\n```", "```cpp\n  /∗ Clear shared memory ∗/\n  d_bin_data_shared[threadIdx.x] = 0;\n```", "```cpp\n  /∗ Wait for all threads to update shared memory ∗/\n  __syncthreads();\n```", "```cpp\n  for (unsigned int i=0, tid_offset=0; i< N; i++, tid_offset+=256)\n  {\n   const unsigned int value_u32 = d_hist_data[tid+tid_offset];\n```", "```cpp\n   atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0x000000FF) ) ]),1);\n   atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0x0000FF00) >> 8 ) ]),1);\n   atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0x00FF0000) >> 16 ) ]),1);\n   atomicAdd(&(d_bin_data_shared[ ((value_u32 & 0xFF000000) >> 24 ) ]),1);\n  }\n  /∗ Wait for all threads to update shared memory ∗/\n  __syncthreads();\n```", "```cpp\n  /∗ The write the accumulated data back to global memory in blocks, not scattered ∗/\n  atomicAdd(&(d_bin_data[threadIdx.x]), d_bin_data_shared[threadIdx.x]);\n}\n```"]