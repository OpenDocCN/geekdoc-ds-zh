<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.7. Online supplementary materials#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.7. Online supplementary materials#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap08_nn/supp/roch-mmids-nn-supp.html">https://mmids-textbook.github.io/chap08_nn/supp/roch-mmids-nn-supp.html</a></blockquote>

<section id="quizzes-solutions-code-etc">
<h2><span class="section-number">8.7.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">8.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">8.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html">Section 8.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html">Section 8.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html">Section 8.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html">Section 8.5</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">8.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">8.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E8.2.1</strong> The vectorization is obtained by stacking the columns of <span class="math notranslate nohighlight">\(A\)</span>: <span class="math notranslate nohighlight">\(\text{vec}(A) = (2, 0, 1, -1)\)</span>.</p>
<p><strong>E8.2.3</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 1 \cdot B &amp; 2 \cdot B \\ -1 \cdot B &amp; 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 &amp; -1 &amp; 6 &amp; -2 \\ 2 &amp; 1 &amp; 4 &amp; 2 \\ -3 &amp; 1 &amp; -6 &amp; 2 \\ -2 &amp; -1 &amp; -4 &amp; -2 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.5</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} &amp; 2 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} \\ 3 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} &amp; 4 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} \end{pmatrix} = \begin{pmatrix} 5 &amp; 6 &amp; 10 &amp; 12 \\ 7 &amp; 8 &amp; 14 &amp; 16 \\ 15 &amp; 18 &amp; 20 &amp; 24 \\ 21 &amp; 24 &amp; 28 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.7</strong> First, compute the Jacobian matrices of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x &amp; 2y \\ y &amp; x \end{pmatrix}, \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v &amp; u \\ 1 &amp; 1 \end{pmatrix}.
\end{split}\]</div>
<p>Then, by the Chain Rule,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1, 2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix} 2 &amp; 5 \\ 1 &amp; 1 \end{pmatrix} \begin{pmatrix} 2 &amp; 4 \\ 2 &amp; 1 \end{pmatrix} = \begin{pmatrix} 14 &amp; 13 \\ 4 &amp; 5 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.9</strong> From E8.2.5, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 5 &amp; 6 &amp; 10 &amp; 12 \\ 7 &amp; 8 &amp; 14 &amp; 16 \\ 15 &amp; 18 &amp; 20 &amp; 24 \\ 21 &amp; 24 &amp; 28 &amp; 32 \end{pmatrix}
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 &amp; 7 &amp; 15 &amp; 21 \\ 6 &amp; 8 &amp; 18 &amp; 24 \\ 10 &amp; 14 &amp; 20 &amp; 28 \\ 12 &amp; 16 &amp; 24 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix}, \quad B^T = \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix}
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} &amp; 3 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} \\ 2 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} &amp; 4 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} \end{pmatrix} = \begin{pmatrix} 5 &amp; 7 &amp; 15 &amp; 21 \\ 6 &amp; 8 &amp; 18 &amp; 24 \\ 10 &amp; 14 &amp; 20 &amp; 28 \\ 12 &amp; 16 &amp; 24 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p>We see that <span class="math notranslate nohighlight">\((A \otimes B)^T = A^T \otimes B^T\)</span>, as expected from the properties of the Kronecker product.</p>
<p><strong>E8.2.11</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix} = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}.
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.13</strong> First, compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> and the Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x &amp; 0 \\ 0 &amp; 2y \end{pmatrix}.
\end{split}\]</div>
<p>Then, by the Chain Rule,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix} 4 &amp; 1 \end{pmatrix} \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 4 \end{pmatrix} = \begin{pmatrix} 8 &amp; 4 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.15</strong> The Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) &amp; 0 &amp; 0 \\ 0 &amp; f'(y) &amp; 0 \\ 0 &amp; 0 &amp; f'(z) \end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f'(x) = \cos(x).\)</span> So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6}) = \begin{pmatrix} \cos(\frac{\pi}{2}) &amp; 0 &amp; 0 \\ 0 &amp; \cos(\frac{\pi}{4}) &amp; 0 \\ 0 &amp; 0 &amp; \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{\sqrt{2}}{2} &amp; 0 \\ 0 &amp; 0 &amp; \frac{\sqrt{3}}{2} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.3.1</strong> Each entry of <span class="math notranslate nohighlight">\(AB\)</span> is the dot product of a row of <span class="math notranslate nohighlight">\(A\)</span> and a column of <span class="math notranslate nohighlight">\(B\)</span>, which takes 2 multiplications and 1 addition. Since <span class="math notranslate nohighlight">\(AB\)</span> has 4 entries, the total number of operations is <span class="math notranslate nohighlight">\(4 \times 3 = 12\)</span>.</p>
<p><strong>E8.3.3</strong> We have</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2
\]</div>
<p>so the partial derivatives are <span class="math notranslate nohighlight">\(\frac{\partial \ell}{\partial \hat{y}_1} = 2 \hat{y}_1\)</span>
and <span class="math notranslate nohighlight">\(\frac{\partial \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\)</span> and</p>
<div class="math notranslate nohighlight">
\[
J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T.
\]</div>
<p><strong>E8.3.5</strong> From E8.3.4, we have <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0) = (-1, -1)\)</span>. Then, <span class="math notranslate nohighlight">\(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\)</span> and <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \ell(\mathbf{z}_2) = 5\)</span>. By the <em>Chain Rule</em>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{x})^T = J_f(\mathbf{x}) 
= J_{\ell}(\mathbf{z}_1) \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) 
= 2 \mathbf{z}_2^T \begin{pmatrix} -1 &amp; 0 \\ 1 &amp; 1 \end{pmatrix} \begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 0 \end{pmatrix} 
= (-10, -4) \begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 0 \end{pmatrix} 
= (6, -20).
\end{split}\]</div>
<p><strong>E8.3.7</strong> We have</p>
<div class="math notranslate nohighlight">
\[
g_1(\mathbf{z}_1, \mathbf{w}_1)
= w_4 z_{1,1} + w_5 z_{1,2}
\]</div>
<p>so, by computing all partial derivatives,</p>
<div class="math notranslate nohighlight">
\[
J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) 
= \begin{pmatrix} w_4 &amp; w_5 &amp; z_{1,1} &amp; z_{1,2} \end{pmatrix} 
= \begin{pmatrix} \mathbf{w}_1^T &amp; \mathbf{z}_1^T \end{pmatrix}
= \begin{pmatrix} W_1 &amp; I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}.
\]</div>
<p>Using the notation in the text, <span class="math notranslate nohighlight">\(A_1 = W_1\)</span>
and
<span class="math notranslate nohighlight">\(B_1 = \mathbf{z}_1^T = I_{1 \times 1} \otimes \mathbf{z}_1^T\)</span>.</p>
<p><strong>E8.3.9</strong> We have</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2
\]</div>
<p>so, by the <em>Chain Rule</em>, the partial derivatives are</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_0}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_4)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_1}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (w_4)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_2}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_3}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_4}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_5}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3).
\]</div>
<p>Moreover, by E8.3.7,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1)
= W_1 \mathbf{z}_1
= \begin{pmatrix} w_4 &amp; w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix}
= w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3).
\end{split}\]</div>
<p>By the fundamental recursion and the results in E8.3.3 and E8.3.8,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
J_f(\mathbf{w}) &amp;= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
= 2 z_2 \begin{pmatrix} A_1 B_0 &amp; B_1\end{pmatrix}\\
&amp;= 2 (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3).
\end{align*}\]</div>
<p><strong>E8.4.1</strong> The full gradient descent step is:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1, 2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}).
\]</div>
<p>The expected SGD step with a batch size of 2 is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] = \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
\]</div>
<p>which is equal to the full gradient descent step, as proven in the “Expected SGD Step” lemma.</p>
<p><strong>E8.4.3</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{KL}(\mathbf{p} \| \mathbf{q}) &amp;= \sum_{i=1}^3 p_i \log \frac{p_i}{q_i} \\
&amp;= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5 \log \frac{0.5}{0.5} \\
&amp;\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5 \cdot 0 \\
&amp;\approx 0.0525.
\end{align*}\]</div>
<p><strong>E8.4.5</strong> The SGD update is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w &amp;\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial \ell}{\partial w}(w, b; x_i, y_i), \\
b &amp;\leftarrow b - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i).
\end{align*}\]</div>
<p>Plugging in the values, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w &amp;\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 - 3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\
b &amp;\leftarrow 0 - \frac{0.1}{2} (2(2 \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3.
\end{align*}\]</div>
<p><strong>E8.4.7</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \ell(w; x, y) &amp;= -\frac{y}{\sigma(wx)} \sigma'(wx) x + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\
&amp;= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx) \\
&amp;= x(\sigma(wx) - y).
\end{align*}\]</div>
<p>We used the fact that <span class="math notranslate nohighlight">\(\sigma'(z) = \sigma(z)(1-\sigma(z))\)</span>.</p>
<p><strong>E8.4.9</strong> First, we compute <span class="math notranslate nohighlight">\(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0 \end{pmatrix} (1, 2) = (0, 0, 0)\)</span>. Then, <span class="math notranslate nohighlight">\(\hat{\mathbf{y}} = \boldsymbol{\gamma}(\mathbf{z}_1) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)</span>. From the text, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y}) \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.4.11</strong> First, we compute the individual gradients:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &amp;= (\boldsymbol{\gamma}(W \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix}, \\
\nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W) &amp;= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3}, \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} &amp; \frac{2}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \end{pmatrix}.
\end{align*}\]</div>
<p>Then, the full gradient is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix} + \begin{pmatrix} -\frac{8}{3} &amp; \frac{2}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6} &amp; \frac{2}{3} \\ \frac{5}{6} &amp; \frac{1}{6} \\ \frac{1}{3} &amp; -\frac{5}{6} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.4.13</strong> The cross-entropy loss is given by</p>
<div class="math notranslate nohighlight">
\[
-\log(0.3) \approx 1.204.
\]</div>
<p><strong>E8.5.1</strong> <span class="math notranslate nohighlight">\(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\)</span>
<span class="math notranslate nohighlight">\(\sigma(-1) = \frac{1}{1 + e^{1}} \approx 0.27\)</span>
<span class="math notranslate nohighlight">\(\sigma(2) = \frac{1}{1 + e^{-2}} \approx 0.88\)</span></p>
<p><strong>E8.5.3</strong> <span class="math notranslate nohighlight">\(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx (0.73, 0.27, 0.88)\)</span>
<span class="math notranslate nohighlight">\(\bsigma'(\mathbf{z}) = (\bsigma'(1), \bsigma'(-1), \bsigma'(2)) \approx (0.20, 0.20, 0.10)\)</span></p>
<p><strong>E8.5.5</strong> <span class="math notranslate nohighlight">\(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\)</span>, so <span class="math notranslate nohighlight">\(\sigma(W\mathbf{x}) \approx (0.27, 0.98)\)</span>,
<span class="math notranslate nohighlight">\(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x}) \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 &amp; 0 \\ 0 &amp; 0.02 \end{pmatrix}\)</span></p>
<p><strong>E8.5.7</strong> <span class="math notranslate nohighlight">\(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2}) = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)</span></p>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">8.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define the Jacobian matrix and use it to compute the differential of a vector-valued function.</p></li>
<li><p>State and apply the generalized Chain Rule to compute the Jacobian of a composition of functions.</p></li>
<li><p>Perform calculations with the Hadamard and Kronecker products of matrices.</p></li>
<li><p>Describe the purpose of automatic differentiation and its advantages over symbolic and numerical differentiation.</p></li>
<li><p>Implement automatic differentiation in PyTorch to compute gradients of vector-valued functions.</p></li>
<li><p>Derive the Chain Rule for multi-layer progressive functions and apply it to compute gradients.</p></li>
<li><p>Compare and contrast the forward and reverse modes of automatic differentiation in terms of computational complexity.</p></li>
<li><p>Define progressive functions and identify their key properties.</p></li>
<li><p>Derive the fundamental recursion for the Jacobian of a progressive function.</p></li>
<li><p>Implement the backpropagation algorithm to efficiently compute gradients of progressive functions.</p></li>
<li><p>Analyze the computational complexity of the backpropagation algorithm in terms of the number of matrix-vector products.</p></li>
<li><p>Describe the stochastic gradient descent (SGD) algorithm and explain how it differs from standard gradient descent.</p></li>
<li><p>Derive the update rule for stochastic gradient descent from the gradient of the loss function.</p></li>
<li><p>Prove that the expected SGD step is equal to the full gradient descent step.</p></li>
<li><p>Evaluate the performance of models trained using stochastic gradient descent on real-world datasets.</p></li>
<li><p>Define the multilayer perceptron (MLP) architecture and describe the role of affine maps and activation functions in each layer.</p></li>
<li><p>Compute the Jacobian of the sigmoid activation function using properties of diagonal matrices and Kronecker products.</p></li>
<li><p>Apply the chain rule to calculate the gradient of the loss function with respect to the weights in a small MLP example.</p></li>
<li><p>Generalize the gradient computation for an MLP with an arbitrary number of layers using a forward and backward pass.</p></li>
<li><p>Implement the training of a neural network using PyTorch.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
</section>
<section id="additional-sections">
<h2><span class="section-number">8.7.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="another-example-linear-regression">
<h3><span class="section-number">8.7.2.1. </span>Another example: linear regression<a class="headerlink" href="#another-example-linear-regression" title="Link to this heading">#</a></h3>
<p>We give another concrete example of progressive functions and of the application of backpropagration and stochastic gradient descent.</p>
<p><strong>Computing the gradient</strong> While we have motivated the framework introduced in the previous section from the point of view of classification, it also immediately applies to the regression setting. Both classification and regression are instances of supervised learning.</p>
<p>We first compute the gradient of a single sample. Here <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> again, but <span class="math notranslate nohighlight">\(y\)</span> is a real-valued outcome variable. We revisit the case of linear regression where the loss function is</p>
<div class="math notranslate nohighlight">
\[
\ell(z) = (z - y)^2 
\]</div>
<p>and the regression function only has input and output layers and no hidden layer (that is, <span class="math notranslate nohighlight">\(L=0\)</span>) with</p>
<div class="math notranslate nohighlight">
\[
h(\mathbf{w})
= \sum_{j=1}^d w_{j} x_{j} 
= \mathbf{x}^T\mathbf{w},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^{d}\)</span> are the parameters. Recall that we can include a constant term (one that does not depend on the input) by adding a <span class="math notranslate nohighlight">\(1\)</span> to the input. To keep the notation simple, we assume that this pre-processing has already been performed if desired.</p>
<p>Finally, the objective function for a single sample is</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w})
= \ell(h(\mathbf{w}))
= \left(\sum_{j=1}^d w_{j} x_{j}  - y\right)^2 
= \left(\mathbf{x}^T\mathbf{w}  - y\right)^2. 
\]</div>
<p>Using the notation from the previous subsection, the forward pass in this case is:</p>
<p><em>Initialization:</em></p>
<div class="math notranslate nohighlight">
\[\mathbf{z}_0 := \mathbf{x}.\]</div>
<p><em>Forward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0)
&amp;= \sum_{j=1}^d w_{0,j} z_{0,j}
= \mathbf{z}_0^T \mathbf{w}_0
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\begin{pmatrix}
A_0 &amp; B_0
\end{pmatrix}
:= J_{g_0}(\mathbf{z}_0,\mathbf{w}_0)
&amp;= ( w_{0,1},\ldots, w_{0,d},z_{0,1},\ldots,z_{0,d} )^T
= \begin{pmatrix}\mathbf{w}_0^T &amp; \mathbf{z}_0^T\end{pmatrix},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}_0 := \mathbf{w}\)</span>.</p>
<p><em>Loss:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z_2
&amp;:= \ell(z_1) = (z_1 - y)^2\\
p_2
&amp;:= \frac{\mathrm{d}}{\mathrm{d} z_1} {\ell}(z_1)
= 2 (z_1 - y).
\end{align*}\]</div>
<p>The backward pass is:</p>
<p><em>Backward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_0 := B_0^T p_1 
&amp;= 2 (z_1 - y) \, \mathbf{z}_0.
\end{align*}\]</div>
<p><em>Output:</em></p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= \mathbf{q}_0.
\]</div>
<p>As we noted before, there is in fact no need to compute <span class="math notranslate nohighlight">\(A_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{p}_0\)</span>.</p>
<p><strong>The <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset and the least-squares solution</strong> We return to the <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'advertising.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Unnamed: 0</th>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>9.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>12.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>200
</pre></div>
</div>
</div>
</div>
<p>We first compute the solution using the least-squares approach we detailed previously. We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack"><code class="docutils literal notranslate"><span class="pre">numpy.column_stack</span></code></a> to add a column of ones to the feature vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">TV</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'TV'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">radio</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'radio'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">newspaper</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'newspaper'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">sales</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sales'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">features</span><span class="p">))</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<p>The MSE is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">A</span> <span class="o">@</span> <span class="n">coeff</span> <span class="o">-</span> <span class="n">sales</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>2.7841263145109365
</pre></div>
</div>
</div>
</div>
<p><strong>Solving the problem using PyTorch</strong> We will be using PyTorch to implement the previous method. We first convert the data into PyTorch tensors. We then use <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.TensorDataset</span></code></a> to create the dataset. Finally, <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> provides the utilities to load the data in batches for training. We take mini-batches of size <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span> <span class="pre">=</span> <span class="pre">64</span></code> and we apply a random permutation of the samples on every pass (with the option <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">features_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sales_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">features_tensor</span><span class="p">,</span> <span class="n">sales_tensor</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we construct our model. It is simply an affine map from <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Note that there is no need to pre-process the inputs by adding <span class="math notranslate nohighlight">\(1\)</span>s. A constant term (or “bias variable”) is automatically added by PyTorch (unless one chooses the option <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">bias=False</span></code></a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 3 input features, 1 output value</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we are ready to run an optimization method of our choice on the loss function, which are specified next. There are many <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#algorithms">optimizers</a> available. (See this <a class="reference external" href="https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc">post</a> for a brief explanation of many common optimizers.) Here we use SGD as the optimizer. And the loss function is the MSE. A quick tutorial is <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Choosing the right number of passes (i.e. epochs) through the data requires some experimenting. Here <span class="math notranslate nohighlight">\(10^4\)</span> suffices. But in the interest of time, we will run it only for <span class="math notranslate nohighlight">\(100\)</span> epochs. As you will see from the results, this is not quite enough. On each pass, we compute the output of the current model, use <code class="docutils literal notranslate"><span class="pre">backward()</span></code> to obtain the gradient, and then perform a descent update with <code class="docutils literal notranslate"><span class="pre">step()</span></code>. We also have to reset the gradients first (otherwise they add up by default).</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The final parameters and loss are:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">weights</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Weights:"</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Bias:"</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Weights: [[0.05736413 0.11314777 0.08020781]]
Bias: [-0.02631279]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Evaluate the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean Squared Error on Training Set: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Mean Squared Error on Training Set: 7.885213494300842
</pre></div>
</div>
</div>
</div>
</section>
<section id="convolutional-neural-networks">
<h3><span class="section-number">8.7.2.2. </span>Convolutional neural networks<a class="headerlink" href="#convolutional-neural-networks" title="Link to this heading">#</a></h3>
<p>We return to the Fashion MNIST dataset. One can do even better than we did before using a neural network tailored for images, known as <a class="reference external" href="https://cs231n.github.io/convolutional-networks/">convolutional neural networks</a>. From <a class="reference external" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Wikipedia</a>:</p>
<blockquote>
<div><p>In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.</p>
</div></blockquote>
<p>More background can be found in this excellent <a class="reference external" href="http://cs231n.github.io/convolutional-networks/">module</a> from Stanford’s <a class="reference external" href="http://cs231n.github.io/">CS231n</a>. Our CNN will be a composition of <a class="reference external" href="http://cs231n.github.io/convolutional-networks/#conv">convolutional layers</a> and <a class="reference external" href="http://cs231n.github.io/convolutional-networks/#pool">pooling layers</a>.</p>
<p><strong>CHAT &amp; LEARN</strong> Convolutional neural networks (CNNs) are powerful for image classification. Ask your favorite AI chatbot to explain the basic concepts of CNNs, including convolutional layers and pooling layers. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                               <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                      <span class="k">else</span> <span class="p">(</span><span class="s2">"mps"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                            <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using device:"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Using device: mps
</pre></div>
</div>
</div>
</div>
<p>The new model is the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="c1"># First convolution, operating upon a 28x28 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Second convolution, operating upon a 14x14 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Third convolution, operating upon a 7x7 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Flatten the tensor</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>

    <span class="c1"># Fully connected layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We train and test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 1/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 2/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 3/3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 88.6% accuracy
</pre></div>
</div>
</div>
</div>
<p>Note the higher accuracy.</p>
<p>Finally, we try the original MNIST dataset. We use the same CNN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                      <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                     <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 1/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 2/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 3/3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 98.6% accuracy
</pre></div>
</div>
</div>
</div>
<p>Note the very high accuracy on this (easy - as it turns out) dataset.</p>
</section>
</section>
<section id="additional-proofs">
<h2><span class="section-number">8.7.3. </span>Additional proofs<a class="headerlink" href="#additional-proofs" title="Link to this heading">#</a></h2>
<p><strong>Proofs of Lagrange Multipliers Conditions</strong> We first prove the <em>Lagrange Multipliers: First-Order Necessary Conditions</em>. We follow the excellent textbook [<a class="reference external" href="http://www.athenasc.com/nonlinbook.html">Ber</a>, Section 4.1]. The proof uses the concept of the Jacobian.</p>
<p><em>Proof idea:</em> We reduce the problem to an unconstrained optimization problem by penalizing the constraint in the objective function. We then apply the unconstrained <em>First-Order Necessary Conditions</em>.</p>
<p><em>Proof:</em> <em>(Lagrange Multipliers: First-Order Necessary Conditions)</em> We reduce the problem to an unconstrained optimization problem by penalizing the constraint in the objective function. We also add a regularization term to ensure that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is the unique local minimizer in a neighborhood. Specifically, for each non-negative integer <span class="math notranslate nohighlight">\(k\)</span>, consider the objective function</p>
<div class="math notranslate nohighlight">
\[
F^k(\mathbf{x})
= f(\mathbf{x})
+ \frac{k}{2} \|\mathbf{h}(\mathbf{x})\|^2
+ \frac{\alpha}{2} \|\mathbf{x} - \mathbf{x}^*\|^2
\]</div>
<p>for some positive constant <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>. Note that as <span class="math notranslate nohighlight">\(k\)</span> gets larger, the penalty becomes more significant and, therefore, enforcing the constraint becomes more desirable. The proof proceeds in several steps.</p>
<p>We first consider a version minimizing <span class="math notranslate nohighlight">\(F^k\)</span> constrained to lie in a neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>. Because <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local minimizer of <span class="math notranslate nohighlight">\(f\)</span> subject to <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{x}) = \mathbf{0}\)</span>, there is <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(f(\mathbf{x}^*)\leq f(\mathbf{x})\)</span> for all feasible <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> within</p>
<div class="math notranslate nohighlight">
\[
\mathscr{X}
=
B_{\delta}(\mathbf{x}^*)
= 
\{\mathbf{x}:\|\mathbf{x} - \mathbf{x}^*\| \leq \delta\}.
\]</div>
<p><strong>LEMMA</strong> <strong>(Step I: Solving the Penalized Problem in a Neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>)</strong> For <span class="math notranslate nohighlight">\(k \geq 1\)</span>, let <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> be a global minimizer of the minimization problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathscr{X}} F^k(\mathbf{x}).
\]</div>
<p>a) The sequence <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>.</p>
<p>b) For <span class="math notranslate nohighlight">\(k\)</span> sufficiently large, <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> is a local minimizer of the objective function <span class="math notranslate nohighlight">\(F^k\)</span> <em>without any constraint</em>.</p>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The set <span class="math notranslate nohighlight">\(\mathscr{X}\)</span> is closed and bounded, and <span class="math notranslate nohighlight">\(F^k\)</span> is continuous. Hence, the sequence <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> is well-defined by the <em>Extreme Value Theorem</em>. Let <span class="math notranslate nohighlight">\(\bar{\mathbf{x}}\)</span> be any limit point of <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span>. We show that <span class="math notranslate nohighlight">\(\bar{\mathbf{x}} = \mathbf{x}^*\)</span>. That will imply a). It also implied b) since hen, for large enough <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> must be an interior point of <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(-\infty &lt; m \leq M &lt; + \infty\)</span> be the smallest and largest values of <span class="math notranslate nohighlight">\(f\)</span> on <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>, which exist by the <em>Extreme Value Theorem</em>. Then, for all <span class="math notranslate nohighlight">\(k\)</span>, by definition of <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> and the fact that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is feasible</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(*)
\qquad 
f(\mathbf{x}^k)
&amp;+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^k)\|^2
+ \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\\
&amp;\leq
f(\mathbf{x}^*)
+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^*)\|^2
+ \frac{\alpha}{2} \|\mathbf{x}^* - \mathbf{x}^*\|^2
= f(\mathbf{x}^*).
\end{align*}\]</div>
<p>Rearraning gives</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{h}(\mathbf{x}^k)\|^2
\leq \frac{2}{k} \left[f(\mathbf{x}^*) - f(\mathbf{x}^k) - \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\right]
\leq \frac{2}{k} \left[ f(\mathbf{x}^*) - m\right].
\]</div>
<p>So <span class="math notranslate nohighlight">\(\lim_{k \to \infty} \|\mathbf{h}(\mathbf{x}^k)\|^2 = 0\)</span>, which, by the continuity of <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> and of the Frobenius norm, implies that
<span class="math notranslate nohighlight">\(\|\mathbf{h}(\bar{\mathbf{x}})\|^2 = 0\)</span>, that is, <span class="math notranslate nohighlight">\(\mathbf{h}(\bar{\mathbf{x}}) = \mathbf{0}\)</span>. In other words, any limit point <span class="math notranslate nohighlight">\(\bar{\mathbf{x}}\)</span> is feasible.</p>
<p>In addition to being feasible, <span class="math notranslate nohighlight">\(\bar{\mathbf{x}} \in \mathscr{X}\)</span> because that constraint set is closed. So, by the choice of <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>, we have <span class="math notranslate nohighlight">\(f(\mathbf{x}^*)
\leq f(\bar{\mathbf{x}})\)</span>. Furthermore, by <span class="math notranslate nohighlight">\((*)\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}^*)
\leq
f(\bar{\mathbf{x}}) + \frac{\alpha}{2} \|\bar{\mathbf{x}} - \mathbf{x}^*\|^2
\leq
f(\mathbf{x}^*).
\]</div>
<p>This is only possible if <span class="math notranslate nohighlight">\(\|\bar{\mathbf{x}} - \mathbf{x}^*\|^2 = 0\)</span> or, put differently, <span class="math notranslate nohighlight">\(\bar{\mathbf{x}} = \mathbf{x}^*\)</span>. That proves the lemma. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>LEMMA</strong> <strong>(Step II: Applying the Unconstrained Necessary Conditions)</strong> Let <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> be the sequence in the previous lemma.</p>
<p>a) For sufficiently large <span class="math notranslate nohighlight">\(k\)</span>, the vectors <span class="math notranslate nohighlight">\(\nabla h_i(\mathbf{x}^k)\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,\ell\)</span>, are linearly independent.</p>
<p>b) Let <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x})\)</span> be the Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>, that is, the matrix whose rows are the (row) vectors <span class="math notranslate nohighlight">\(\nabla h_i(\mathbf{x})^T\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,\ell\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x}^*)
+ \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
= \mathbf{0}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\blambda^*
=
- (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \, \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*) \nabla f(\mathbf{x}^*).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By the previous lemma, for <span class="math notranslate nohighlight">\(k\)</span> large enough, <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> is an unconstrained local minimizer of <span class="math notranslate nohighlight">\(F^k\)</span>. So by the (unconstrained) <em>First-Order Necessary Conditions</em>, it holds that</p>
<div class="math notranslate nohighlight">
\[
\nabla F^k(\mathbf{x}^k)
=
\mathbf{0}.
\]</div>
<p>To compute the gradient of <span class="math notranslate nohighlight">\(F^k\)</span> we note that</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{h}(\mathbf{x})\|^2
= 
\sum_{i=1}^\ell
(h_i(\mathbf{x}))^2.
\]</div>
<p>The partial derivatives are</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2
=
\sum_{i=1}^\ell
\frac{\partial}{\partial x_j} (h_i(\mathbf{x}))^2
=
\sum_{i=1}^\ell
2  h_i(\mathbf{x}) \frac{\partial h_i(\mathbf{x})}{\partial x_j},
\]</div>
<p>by the <em>Chain Rule</em>. So, in vector form,</p>
<div class="math notranslate nohighlight">
\[
\nabla \|\mathbf{h}(\mathbf{x})\|^2
= 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \mathbf{h}(\mathbf{x}).
\]</div>
<p>The term <span class="math notranslate nohighlight">\(\|\mathbf{x} - \mathbf{x}^*\|^2\)</span> can be rewritten as the quadratic function</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x} - \mathbf{x}^*\|^2
= \frac{1}{2}\mathbf{x}^T (2 I_{d \times d}) \mathbf{x} - 2 (\mathbf{x}^*)^T \mathbf{x} + (\mathbf{x}^*)^T \mathbf{x}^*.
\]</div>
<p>Using a previous formula with <span class="math notranslate nohighlight">\(P = 2 I_{d \times d}\)</span> (which is symmetric), <span class="math notranslate nohighlight">\(\mathbf{q} = -2 \mathbf{x}^*\)</span> and <span class="math notranslate nohighlight">\(r = (\mathbf{x}^*)^T \mathbf{x}^*\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
\nabla \|\mathbf{x} - \mathbf{x}^*\|^2
= 2\mathbf{x} -2 \mathbf{x}^*.
\]</div>
<p>So, putting everything together,</p>
<div class="math notranslate nohighlight">
\[
(**)
\qquad \mathbf{0}
=
\nabla F^k(\mathbf{x}^k)
=
\nabla f(\mathbf{x}^k)
+
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T (k \mathbf{h}(\mathbf{x}^k))
+ \alpha(\mathbf{x}^k - \mathbf{x}^*).
\]</div>
<p>By the previous lemma, <span class="math notranslate nohighlight">\(\mathbf{x}^k \to \mathbf{x}^*\)</span>, <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}^k) \to \nabla f(\mathbf{x}^*)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)</span> as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>.</p>
<p>So it remains to derive the limit of <span class="math notranslate nohighlight">\(k \mathbf{h}(\mathbf{x}^k)\)</span>. By assumption, the columns of <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T\)</span> are linearly independent. That implies that for any unit vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^\ell\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} = \|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}\|^2 &gt; 0
\]</div>
<p>otherwise we would have <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} = \mathbf{0}\)</span>, contradicting the linear independence assumption. By the <em>Extreme Value Theorem</em>, there is <span class="math notranslate nohighlight">\(\beta &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} \geq \beta
\]</div>
<p>for all unit vectors <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^\ell\)</span>. Since <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)</span>, it follows from a previous lemma that, for <span class="math notranslate nohighlight">\(k\)</span> large enough and any unit vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^\ell\)</span>,</p>
<div class="math notranslate nohighlight">
\[
|\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}
- 
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T \mathbf{z}|
\leq 
\|
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
-
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
\|_F
\leq 
\frac{\beta}{2}.
\]</div>
<p>That implies</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T \mathbf{z} \geq \frac{\beta}{2},
\]</div>
<p>so by the same argument as above the columns of <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)</span> are linearly independent for <span class="math notranslate nohighlight">\(k\)</span> large enough and
<span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)</span> is invertible. That proves a).</p>
<p>Going back to <span class="math notranslate nohighlight">\((**)\)</span>, multiplying both sides by <span class="math notranslate nohighlight">\((\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T)^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\)</span>, and taking a limit <span class="math notranslate nohighlight">\(k \to +\infty\)</span>, we get after rearranging that</p>
<div class="math notranslate nohighlight">
\[
k \mathbf{h}(\mathbf{x}^k)
\to
- (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) ^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)  \nabla f(\mathbf{x}^*) = \blambda^*.
\]</div>
<p>Plugging back we get</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x}^*)
+ \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
= \mathbf{0}
\]</div>
<p>as claimed. That proves b). <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Combining the lemmas establishes the theorem. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Next, we prove the <em>Lagrange Multipliers: Second-Order Sufficient Conditions</em>. Again, we follow [<a class="reference external" href="http://www.athenasc.com/nonlinbook.html">Ber</a>, Section 4.2]. We will need the following lemma. The proof can be skipped.</p>
<p><em>Proof idea:</em> We consider a slightly modified problem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
&amp;\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}
\end{align*}\]</div>
<p>which has the same local minimizers. Applying the <em>Second-Order Sufficient Conditions</em> to the Lagrangian of the modified problem gives the result when <span class="math notranslate nohighlight">\(c\)</span> is chosen large enough.</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> be symmetric matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{n \times n}\)</span>. Assume that <span class="math notranslate nohighlight">\(Q\)</span> is positive semidefinite and that <span class="math notranslate nohighlight">\(P\)</span> is positive definite on the null space of <span class="math notranslate nohighlight">\(Q\)</span>, that is, <span class="math notranslate nohighlight">\(\mathbf{w}^T P \mathbf{w} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{w} \neq \mathbf{0}\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{w}^T Q \mathbf{w} = \mathbf{0}\)</span>. Then there is a scalar <span class="math notranslate nohighlight">\(\bar{c} \geq 0\)</span> such that <span class="math notranslate nohighlight">\(P + c Q\)</span> is positive definite for all <span class="math notranslate nohighlight">\(c &gt; \bar{c}\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> <em>(lemma)</em> We argue by contradiction. Suppose there is an non-negative, increasing, diverging sequence <span class="math notranslate nohighlight">\(\{c_k\}_{k=1}^{+\infty}\)</span> and a sequence of unit vectors <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k \leq 0
\]</div>
<p>for all <span class="math notranslate nohighlight">\(k\)</span>. Because the sequence is bounded, it has a limit point <span class="math notranslate nohighlight">\(\bar{\mathbf{x}}\)</span>. Assume without loss of generality that <span class="math notranslate nohighlight">\(\mathbf{x}^k \to \bar{\mathbf{x}}\)</span>, as <span class="math notranslate nohighlight">\(k \to \infty\)</span>. Since <span class="math notranslate nohighlight">\(c_k \to +\infty\)</span> and <span class="math notranslate nohighlight">\((\mathbf{x}^k)^T Q \mathbf{x}^k \geq 0\)</span> by assumption, we must have <span class="math notranslate nohighlight">\((\bar{\mathbf{x}})^T Q \bar{\mathbf{x}} = 0\)</span>, otherwise <span class="math notranslate nohighlight">\((\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k\)</span> would diverge. Hence, by the assumption in the statement, it must be that <span class="math notranslate nohighlight">\((\bar{\mathbf{x}})^T P \bar{\mathbf{x}} &gt; 0\)</span>. This contradicts the inequality in the display above. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em>Proof:</em> <em>(Lagrange Multipliers: Second-Order Sufficient Conditions)</em> We consider the modified problem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
&amp;\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}.
\end{align*}\]</div>
<p>It has the same local minimizers as the orginal problem as the additional term in the objective is zero for feasible vectors. That extra term will allow us to use the previous lemma. For notational convenience, we define</p>
<div class="math notranslate nohighlight">
\[
g_c(\mathbf{x})
= f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2.
\]</div>
<p>The Lagrangian of the modified problem is</p>
<div class="math notranslate nohighlight">
\[
L_c(\mathbf{x}, \blambda)
= g_c(\mathbf{x}) + \mathbf{h}(\mathbf{x})^T \blambda.
\]</div>
<p>We will apply the <em>Second-Order Sufficient Conditions</em> to problem of minimizing <span class="math notranslate nohighlight">\(L_c\)</span> over <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. We indicate the Hessian with respect to only the variables <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as <span class="math notranslate nohighlight">\(\nabla^2_{\mathbf{x},\mathbf{x}}\)</span>.</p>
<p>Recall from the proof of  the <em>Lagrange Multipliers: First-Order Necessary Conditions</em> that</p>
<div class="math notranslate nohighlight">
\[
\nabla \|\mathbf{h}(\mathbf{x})\|^2
= 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \mathbf{h}(\mathbf{x}).
\]</div>
<p>To compute the Hessian of that function, we note</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial}{\partial x_i}\left(
\frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2\right)
&amp;=
\frac{\partial}{\partial x_i}\left(
\sum_{k=1}^\ell
2  h_k(\mathbf{x}) \frac{\partial h_k(\mathbf{x})}{\partial x_j}\right)\\
&amp;=
2 \sum_{k=1}^\ell\left(
 \frac{\partial h_k(\mathbf{x})}{\partial x_i}  \frac{\partial h_k(\mathbf{x})}{\partial x_j}
+
h_k(\mathbf{x}) \frac{\partial^2 h_k(\mathbf{x})}{\partial x_i \partial x_j}
\right)\\
&amp;= 2 \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}) +  \sum_{k=1}^\ell h_k(\mathbf{x}) \, \mathbf{H}_{h_k}(\mathbf{x})\right)_{i,j}.
\end{align*}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
=
\nabla f(\mathbf{x}^*)
+ 
c \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{h}(\mathbf{x}^*)
+ 
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
= 
\mathbf{H}_{f}(\mathbf{x}^*)
+
c \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) +  \sum_{k=1}^\ell h_k(\mathbf{x}^*) \, \mathbf{H}_{h_k}(\mathbf{x}^*)\right) 
+
\sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*).
\]</div>
<p>By the assumptions of the theorem, this simplifies to</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
=
\mathbf{0}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
=\underbrace{\left\{
\mathbf{H}_{f}(\mathbf{x}^*)
+
\sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*)
\right\}}_{P}
+
c \underbrace{\left\{\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \right\}}_{Q},
\]</div>
<p>where further <span class="math notranslate nohighlight">\(\mathbf{v}^T P \mathbf{v} &gt; 0\)</span> for any <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{v}  = \mathbf{0}\)</span> (which itself implies <span class="math notranslate nohighlight">\(\mathbf{v}^T Q \mathbf{v} = \mathbf{0}\)</span>). Furthermore, <span class="math notranslate nohighlight">\(Q \succeq \mathbf{0}\)</span> since</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^T Q \mathbf{w}
= \mathbf{w}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)  \mathbf{w}
= \left\|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)  \mathbf{w}\right\|^2 \geq 0
\]</div>
<p>for any <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^d\)</span>. The previous lemma allows us to take <span class="math notranslate nohighlight">\(c\)</span> large enough that <span class="math notranslate nohighlight">\(\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) \succ \mathbf{0}\)</span>.</p>
<p>As a result, the unconstrained <em>Second-Order Sufficient Conditions</em> are satisfied at <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> for the problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^d} L_c(\mathbf{x}, \blambda^*).
\]</div>
<p>That is, there is <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
L_c(\mathbf{x}^*, \blambda^*)
&lt; L_c(\mathbf{x}, \blambda^*),
\qquad \forall \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}.
\]</div>
<p>Restricting this to the feasible vectors of the modified constrained problem (i.e., those such that <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{x}) = \mathbf{0}\)</span>) implies after simplification</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}^*)
&lt; f(\mathbf{x}),
\qquad \forall \mathbf{x} \in \{\mathbf{x} : \mathbf{h}(\mathbf{x}) = \mathbf{0}\} \cap (B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}).
\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a strict local minimizer of the modified constrained problem (and, in turn, of the original constrained problem). That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
&#13;

<h2><span class="section-number">8.7.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">8.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">8.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html">Section 8.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html">Section 8.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html">Section 8.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html">Section 8.5</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">8.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">8.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E8.2.1</strong> The vectorization is obtained by stacking the columns of <span class="math notranslate nohighlight">\(A\)</span>: <span class="math notranslate nohighlight">\(\text{vec}(A) = (2, 0, 1, -1)\)</span>.</p>
<p><strong>E8.2.3</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 1 \cdot B &amp; 2 \cdot B \\ -1 \cdot B &amp; 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 &amp; -1 &amp; 6 &amp; -2 \\ 2 &amp; 1 &amp; 4 &amp; 2 \\ -3 &amp; 1 &amp; -6 &amp; 2 \\ -2 &amp; -1 &amp; -4 &amp; -2 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.5</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} &amp; 2 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} \\ 3 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} &amp; 4 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} \end{pmatrix} = \begin{pmatrix} 5 &amp; 6 &amp; 10 &amp; 12 \\ 7 &amp; 8 &amp; 14 &amp; 16 \\ 15 &amp; 18 &amp; 20 &amp; 24 \\ 21 &amp; 24 &amp; 28 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.7</strong> First, compute the Jacobian matrices of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x &amp; 2y \\ y &amp; x \end{pmatrix}, \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v &amp; u \\ 1 &amp; 1 \end{pmatrix}.
\end{split}\]</div>
<p>Then, by the Chain Rule,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1, 2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix} 2 &amp; 5 \\ 1 &amp; 1 \end{pmatrix} \begin{pmatrix} 2 &amp; 4 \\ 2 &amp; 1 \end{pmatrix} = \begin{pmatrix} 14 &amp; 13 \\ 4 &amp; 5 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.9</strong> From E8.2.5, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 5 &amp; 6 &amp; 10 &amp; 12 \\ 7 &amp; 8 &amp; 14 &amp; 16 \\ 15 &amp; 18 &amp; 20 &amp; 24 \\ 21 &amp; 24 &amp; 28 &amp; 32 \end{pmatrix}
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 &amp; 7 &amp; 15 &amp; 21 \\ 6 &amp; 8 &amp; 18 &amp; 24 \\ 10 &amp; 14 &amp; 20 &amp; 28 \\ 12 &amp; 16 &amp; 24 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix}, \quad B^T = \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix}
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} &amp; 3 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} \\ 2 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} &amp; 4 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} \end{pmatrix} = \begin{pmatrix} 5 &amp; 7 &amp; 15 &amp; 21 \\ 6 &amp; 8 &amp; 18 &amp; 24 \\ 10 &amp; 14 &amp; 20 &amp; 28 \\ 12 &amp; 16 &amp; 24 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p>We see that <span class="math notranslate nohighlight">\((A \otimes B)^T = A^T \otimes B^T\)</span>, as expected from the properties of the Kronecker product.</p>
<p><strong>E8.2.11</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix} = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}.
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.13</strong> First, compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> and the Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x &amp; 0 \\ 0 &amp; 2y \end{pmatrix}.
\end{split}\]</div>
<p>Then, by the Chain Rule,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix} 4 &amp; 1 \end{pmatrix} \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 4 \end{pmatrix} = \begin{pmatrix} 8 &amp; 4 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.15</strong> The Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) &amp; 0 &amp; 0 \\ 0 &amp; f'(y) &amp; 0 \\ 0 &amp; 0 &amp; f'(z) \end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f'(x) = \cos(x).\)</span> So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6}) = \begin{pmatrix} \cos(\frac{\pi}{2}) &amp; 0 &amp; 0 \\ 0 &amp; \cos(\frac{\pi}{4}) &amp; 0 \\ 0 &amp; 0 &amp; \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{\sqrt{2}}{2} &amp; 0 \\ 0 &amp; 0 &amp; \frac{\sqrt{3}}{2} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.3.1</strong> Each entry of <span class="math notranslate nohighlight">\(AB\)</span> is the dot product of a row of <span class="math notranslate nohighlight">\(A\)</span> and a column of <span class="math notranslate nohighlight">\(B\)</span>, which takes 2 multiplications and 1 addition. Since <span class="math notranslate nohighlight">\(AB\)</span> has 4 entries, the total number of operations is <span class="math notranslate nohighlight">\(4 \times 3 = 12\)</span>.</p>
<p><strong>E8.3.3</strong> We have</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2
\]</div>
<p>so the partial derivatives are <span class="math notranslate nohighlight">\(\frac{\partial \ell}{\partial \hat{y}_1} = 2 \hat{y}_1\)</span>
and <span class="math notranslate nohighlight">\(\frac{\partial \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\)</span> and</p>
<div class="math notranslate nohighlight">
\[
J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T.
\]</div>
<p><strong>E8.3.5</strong> From E8.3.4, we have <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0) = (-1, -1)\)</span>. Then, <span class="math notranslate nohighlight">\(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\)</span> and <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \ell(\mathbf{z}_2) = 5\)</span>. By the <em>Chain Rule</em>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{x})^T = J_f(\mathbf{x}) 
= J_{\ell}(\mathbf{z}_1) \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) 
= 2 \mathbf{z}_2^T \begin{pmatrix} -1 &amp; 0 \\ 1 &amp; 1 \end{pmatrix} \begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 0 \end{pmatrix} 
= (-10, -4) \begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 0 \end{pmatrix} 
= (6, -20).
\end{split}\]</div>
<p><strong>E8.3.7</strong> We have</p>
<div class="math notranslate nohighlight">
\[
g_1(\mathbf{z}_1, \mathbf{w}_1)
= w_4 z_{1,1} + w_5 z_{1,2}
\]</div>
<p>so, by computing all partial derivatives,</p>
<div class="math notranslate nohighlight">
\[
J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) 
= \begin{pmatrix} w_4 &amp; w_5 &amp; z_{1,1} &amp; z_{1,2} \end{pmatrix} 
= \begin{pmatrix} \mathbf{w}_1^T &amp; \mathbf{z}_1^T \end{pmatrix}
= \begin{pmatrix} W_1 &amp; I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}.
\]</div>
<p>Using the notation in the text, <span class="math notranslate nohighlight">\(A_1 = W_1\)</span>
and
<span class="math notranslate nohighlight">\(B_1 = \mathbf{z}_1^T = I_{1 \times 1} \otimes \mathbf{z}_1^T\)</span>.</p>
<p><strong>E8.3.9</strong> We have</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2
\]</div>
<p>so, by the <em>Chain Rule</em>, the partial derivatives are</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_0}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_4)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_1}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (w_4)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_2}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_3}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_4}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_5}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3).
\]</div>
<p>Moreover, by E8.3.7,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1)
= W_1 \mathbf{z}_1
= \begin{pmatrix} w_4 &amp; w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix}
= w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3).
\end{split}\]</div>
<p>By the fundamental recursion and the results in E8.3.3 and E8.3.8,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
J_f(\mathbf{w}) &amp;= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
= 2 z_2 \begin{pmatrix} A_1 B_0 &amp; B_1\end{pmatrix}\\
&amp;= 2 (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3).
\end{align*}\]</div>
<p><strong>E8.4.1</strong> The full gradient descent step is:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1, 2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}).
\]</div>
<p>The expected SGD step with a batch size of 2 is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] = \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
\]</div>
<p>which is equal to the full gradient descent step, as proven in the “Expected SGD Step” lemma.</p>
<p><strong>E8.4.3</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{KL}(\mathbf{p} \| \mathbf{q}) &amp;= \sum_{i=1}^3 p_i \log \frac{p_i}{q_i} \\
&amp;= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5 \log \frac{0.5}{0.5} \\
&amp;\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5 \cdot 0 \\
&amp;\approx 0.0525.
\end{align*}\]</div>
<p><strong>E8.4.5</strong> The SGD update is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w &amp;\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial \ell}{\partial w}(w, b; x_i, y_i), \\
b &amp;\leftarrow b - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i).
\end{align*}\]</div>
<p>Plugging in the values, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w &amp;\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 - 3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\
b &amp;\leftarrow 0 - \frac{0.1}{2} (2(2 \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3.
\end{align*}\]</div>
<p><strong>E8.4.7</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \ell(w; x, y) &amp;= -\frac{y}{\sigma(wx)} \sigma'(wx) x + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\
&amp;= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx) \\
&amp;= x(\sigma(wx) - y).
\end{align*}\]</div>
<p>We used the fact that <span class="math notranslate nohighlight">\(\sigma'(z) = \sigma(z)(1-\sigma(z))\)</span>.</p>
<p><strong>E8.4.9</strong> First, we compute <span class="math notranslate nohighlight">\(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0 \end{pmatrix} (1, 2) = (0, 0, 0)\)</span>. Then, <span class="math notranslate nohighlight">\(\hat{\mathbf{y}} = \boldsymbol{\gamma}(\mathbf{z}_1) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)</span>. From the text, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y}) \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.4.11</strong> First, we compute the individual gradients:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &amp;= (\boldsymbol{\gamma}(W \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix}, \\
\nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W) &amp;= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3}, \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} &amp; \frac{2}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \end{pmatrix}.
\end{align*}\]</div>
<p>Then, the full gradient is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix} + \begin{pmatrix} -\frac{8}{3} &amp; \frac{2}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6} &amp; \frac{2}{3} \\ \frac{5}{6} &amp; \frac{1}{6} \\ \frac{1}{3} &amp; -\frac{5}{6} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.4.13</strong> The cross-entropy loss is given by</p>
<div class="math notranslate nohighlight">
\[
-\log(0.3) \approx 1.204.
\]</div>
<p><strong>E8.5.1</strong> <span class="math notranslate nohighlight">\(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\)</span>
<span class="math notranslate nohighlight">\(\sigma(-1) = \frac{1}{1 + e^{1}} \approx 0.27\)</span>
<span class="math notranslate nohighlight">\(\sigma(2) = \frac{1}{1 + e^{-2}} \approx 0.88\)</span></p>
<p><strong>E8.5.3</strong> <span class="math notranslate nohighlight">\(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx (0.73, 0.27, 0.88)\)</span>
<span class="math notranslate nohighlight">\(\bsigma'(\mathbf{z}) = (\bsigma'(1), \bsigma'(-1), \bsigma'(2)) \approx (0.20, 0.20, 0.10)\)</span></p>
<p><strong>E8.5.5</strong> <span class="math notranslate nohighlight">\(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\)</span>, so <span class="math notranslate nohighlight">\(\sigma(W\mathbf{x}) \approx (0.27, 0.98)\)</span>,
<span class="math notranslate nohighlight">\(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x}) \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 &amp; 0 \\ 0 &amp; 0.02 \end{pmatrix}\)</span></p>
<p><strong>E8.5.7</strong> <span class="math notranslate nohighlight">\(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2}) = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)</span></p>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">8.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define the Jacobian matrix and use it to compute the differential of a vector-valued function.</p></li>
<li><p>State and apply the generalized Chain Rule to compute the Jacobian of a composition of functions.</p></li>
<li><p>Perform calculations with the Hadamard and Kronecker products of matrices.</p></li>
<li><p>Describe the purpose of automatic differentiation and its advantages over symbolic and numerical differentiation.</p></li>
<li><p>Implement automatic differentiation in PyTorch to compute gradients of vector-valued functions.</p></li>
<li><p>Derive the Chain Rule for multi-layer progressive functions and apply it to compute gradients.</p></li>
<li><p>Compare and contrast the forward and reverse modes of automatic differentiation in terms of computational complexity.</p></li>
<li><p>Define progressive functions and identify their key properties.</p></li>
<li><p>Derive the fundamental recursion for the Jacobian of a progressive function.</p></li>
<li><p>Implement the backpropagation algorithm to efficiently compute gradients of progressive functions.</p></li>
<li><p>Analyze the computational complexity of the backpropagation algorithm in terms of the number of matrix-vector products.</p></li>
<li><p>Describe the stochastic gradient descent (SGD) algorithm and explain how it differs from standard gradient descent.</p></li>
<li><p>Derive the update rule for stochastic gradient descent from the gradient of the loss function.</p></li>
<li><p>Prove that the expected SGD step is equal to the full gradient descent step.</p></li>
<li><p>Evaluate the performance of models trained using stochastic gradient descent on real-world datasets.</p></li>
<li><p>Define the multilayer perceptron (MLP) architecture and describe the role of affine maps and activation functions in each layer.</p></li>
<li><p>Compute the Jacobian of the sigmoid activation function using properties of diagonal matrices and Kronecker products.</p></li>
<li><p>Apply the chain rule to calculate the gradient of the loss function with respect to the weights in a small MLP example.</p></li>
<li><p>Generalize the gradient computation for an MLP with an arbitrary number of layers using a forward and backward pass.</p></li>
<li><p>Implement the training of a neural network using PyTorch.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
&#13;

<h3><span class="section-number">8.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
&#13;

<h3><span class="section-number">8.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html">Section 8.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html">Section 8.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html">Section 8.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html">Section 8.5</a></p></li>
</ul>
&#13;

<h3><span class="section-number">8.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
&#13;

<h3><span class="section-number">8.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E8.2.1</strong> The vectorization is obtained by stacking the columns of <span class="math notranslate nohighlight">\(A\)</span>: <span class="math notranslate nohighlight">\(\text{vec}(A) = (2, 0, 1, -1)\)</span>.</p>
<p><strong>E8.2.3</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 1 \cdot B &amp; 2 \cdot B \\ -1 \cdot B &amp; 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 &amp; -1 &amp; 6 &amp; -2 \\ 2 &amp; 1 &amp; 4 &amp; 2 \\ -3 &amp; 1 &amp; -6 &amp; 2 \\ -2 &amp; -1 &amp; -4 &amp; -2 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.5</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} &amp; 2 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} \\ 3 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} &amp; 4 \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix} \end{pmatrix} = \begin{pmatrix} 5 &amp; 6 &amp; 10 &amp; 12 \\ 7 &amp; 8 &amp; 14 &amp; 16 \\ 15 &amp; 18 &amp; 20 &amp; 24 \\ 21 &amp; 24 &amp; 28 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.7</strong> First, compute the Jacobian matrices of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x &amp; 2y \\ y &amp; x \end{pmatrix}, \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v &amp; u \\ 1 &amp; 1 \end{pmatrix}.
\end{split}\]</div>
<p>Then, by the Chain Rule,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1, 2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix} 2 &amp; 5 \\ 1 &amp; 1 \end{pmatrix} \begin{pmatrix} 2 &amp; 4 \\ 2 &amp; 1 \end{pmatrix} = \begin{pmatrix} 14 &amp; 13 \\ 4 &amp; 5 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.9</strong> From E8.2.5, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = \begin{pmatrix} 5 &amp; 6 &amp; 10 &amp; 12 \\ 7 &amp; 8 &amp; 14 &amp; 16 \\ 15 &amp; 18 &amp; 20 &amp; 24 \\ 21 &amp; 24 &amp; 28 &amp; 32 \end{pmatrix}
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 &amp; 7 &amp; 15 &amp; 21 \\ 6 &amp; 8 &amp; 18 &amp; 24 \\ 10 &amp; 14 &amp; 20 &amp; 28 \\ 12 &amp; 16 &amp; 24 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T = \begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix}, \quad B^T = \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix}
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} &amp; 3 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} \\ 2 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} &amp; 4 \begin{pmatrix} 5 &amp; 7 \\ 6 &amp; 8 \end{pmatrix} \end{pmatrix} = \begin{pmatrix} 5 &amp; 7 &amp; 15 &amp; 21 \\ 6 &amp; 8 &amp; 18 &amp; 24 \\ 10 &amp; 14 &amp; 20 &amp; 28 \\ 12 &amp; 16 &amp; 24 &amp; 32 \end{pmatrix}.
\end{split}\]</div>
<p>We see that <span class="math notranslate nohighlight">\((A \otimes B)^T = A^T \otimes B^T\)</span>, as expected from the properties of the Kronecker product.</p>
<p><strong>E8.2.11</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix} = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}.
\end{split}\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.13</strong> First, compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> and the Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x &amp; 0 \\ 0 &amp; 2y \end{pmatrix}.
\end{split}\]</div>
<p>Then, by the Chain Rule,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix} 4 &amp; 1 \end{pmatrix} \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 4 \end{pmatrix} = \begin{pmatrix} 8 &amp; 4 \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.2.15</strong> The Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) &amp; 0 &amp; 0 \\ 0 &amp; f'(y) &amp; 0 \\ 0 &amp; 0 &amp; f'(z) \end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f'(x) = \cos(x).\)</span> So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6}) = \begin{pmatrix} \cos(\frac{\pi}{2}) &amp; 0 &amp; 0 \\ 0 &amp; \cos(\frac{\pi}{4}) &amp; 0 \\ 0 &amp; 0 &amp; \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{\sqrt{2}}{2} &amp; 0 \\ 0 &amp; 0 &amp; \frac{\sqrt{3}}{2} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.3.1</strong> Each entry of <span class="math notranslate nohighlight">\(AB\)</span> is the dot product of a row of <span class="math notranslate nohighlight">\(A\)</span> and a column of <span class="math notranslate nohighlight">\(B\)</span>, which takes 2 multiplications and 1 addition. Since <span class="math notranslate nohighlight">\(AB\)</span> has 4 entries, the total number of operations is <span class="math notranslate nohighlight">\(4 \times 3 = 12\)</span>.</p>
<p><strong>E8.3.3</strong> We have</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2
\]</div>
<p>so the partial derivatives are <span class="math notranslate nohighlight">\(\frac{\partial \ell}{\partial \hat{y}_1} = 2 \hat{y}_1\)</span>
and <span class="math notranslate nohighlight">\(\frac{\partial \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\)</span> and</p>
<div class="math notranslate nohighlight">
\[
J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T.
\]</div>
<p><strong>E8.3.5</strong> From E8.3.4, we have <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0) = (-1, -1)\)</span>. Then, <span class="math notranslate nohighlight">\(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\)</span> and <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \ell(\mathbf{z}_2) = 5\)</span>. By the <em>Chain Rule</em>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{x})^T = J_f(\mathbf{x}) 
= J_{\ell}(\mathbf{z}_1) \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) 
= 2 \mathbf{z}_2^T \begin{pmatrix} -1 &amp; 0 \\ 1 &amp; 1 \end{pmatrix} \begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 0 \end{pmatrix} 
= (-10, -4) \begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 0 \end{pmatrix} 
= (6, -20).
\end{split}\]</div>
<p><strong>E8.3.7</strong> We have</p>
<div class="math notranslate nohighlight">
\[
g_1(\mathbf{z}_1, \mathbf{w}_1)
= w_4 z_{1,1} + w_5 z_{1,2}
\]</div>
<p>so, by computing all partial derivatives,</p>
<div class="math notranslate nohighlight">
\[
J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) 
= \begin{pmatrix} w_4 &amp; w_5 &amp; z_{1,1} &amp; z_{1,2} \end{pmatrix} 
= \begin{pmatrix} \mathbf{w}_1^T &amp; \mathbf{z}_1^T \end{pmatrix}
= \begin{pmatrix} W_1 &amp; I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}.
\]</div>
<p>Using the notation in the text, <span class="math notranslate nohighlight">\(A_1 = W_1\)</span>
and
<span class="math notranslate nohighlight">\(B_1 = \mathbf{z}_1^T = I_{1 \times 1} \otimes \mathbf{z}_1^T\)</span>.</p>
<p><strong>E8.3.9</strong> We have</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2
\]</div>
<p>so, by the <em>Chain Rule</em>, the partial derivatives are</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_0}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_4)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_1}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (w_4)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_2}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_3}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_4}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial w_5}
= 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3).
\]</div>
<p>Moreover, by E8.3.7,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1)
= W_1 \mathbf{z}_1
= \begin{pmatrix} w_4 &amp; w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix}
= w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3).
\end{split}\]</div>
<p>By the fundamental recursion and the results in E8.3.3 and E8.3.8,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
J_f(\mathbf{w}) &amp;= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
= 2 z_2 \begin{pmatrix} A_1 B_0 &amp; B_1\end{pmatrix}\\
&amp;= 2 (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3).
\end{align*}\]</div>
<p><strong>E8.4.1</strong> The full gradient descent step is:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1, 2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}).
\]</div>
<p>The expected SGD step with a batch size of 2 is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] = \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
\]</div>
<p>which is equal to the full gradient descent step, as proven in the “Expected SGD Step” lemma.</p>
<p><strong>E8.4.3</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{KL}(\mathbf{p} \| \mathbf{q}) &amp;= \sum_{i=1}^3 p_i \log \frac{p_i}{q_i} \\
&amp;= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5 \log \frac{0.5}{0.5} \\
&amp;\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5 \cdot 0 \\
&amp;\approx 0.0525.
\end{align*}\]</div>
<p><strong>E8.4.5</strong> The SGD update is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w &amp;\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial \ell}{\partial w}(w, b; x_i, y_i), \\
b &amp;\leftarrow b - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i).
\end{align*}\]</div>
<p>Plugging in the values, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w &amp;\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 - 3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\
b &amp;\leftarrow 0 - \frac{0.1}{2} (2(2 \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3.
\end{align*}\]</div>
<p><strong>E8.4.7</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla \ell(w; x, y) &amp;= -\frac{y}{\sigma(wx)} \sigma'(wx) x + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\
&amp;= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx) \\
&amp;= x(\sigma(wx) - y).
\end{align*}\]</div>
<p>We used the fact that <span class="math notranslate nohighlight">\(\sigma'(z) = \sigma(z)(1-\sigma(z))\)</span>.</p>
<p><strong>E8.4.9</strong> First, we compute <span class="math notranslate nohighlight">\(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0 \end{pmatrix} (1, 2) = (0, 0, 0)\)</span>. Then, <span class="math notranslate nohighlight">\(\hat{\mathbf{y}} = \boldsymbol{\gamma}(\mathbf{z}_1) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)</span>. From the text, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y}) \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.4.11</strong> First, we compute the individual gradients:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &amp;= (\boldsymbol{\gamma}(W \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix}, \\
\nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W) &amp;= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3}, \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} &amp; \frac{2}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \end{pmatrix}.
\end{align*}\]</div>
<p>Then, the full gradient is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3} &amp; \frac{2}{3} \\ \frac{1}{3} &amp; \frac{2}{3} \\ -\frac{2}{3} &amp; -\frac{4}{3} \end{pmatrix} + \begin{pmatrix} -\frac{8}{3} &amp; \frac{2}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \\ \frac{4}{3} &amp; -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6} &amp; \frac{2}{3} \\ \frac{5}{6} &amp; \frac{1}{6} \\ \frac{1}{3} &amp; -\frac{5}{6} \end{pmatrix}.
\end{split}\]</div>
<p><strong>E8.4.13</strong> The cross-entropy loss is given by</p>
<div class="math notranslate nohighlight">
\[
-\log(0.3) \approx 1.204.
\]</div>
<p><strong>E8.5.1</strong> <span class="math notranslate nohighlight">\(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\)</span>
<span class="math notranslate nohighlight">\(\sigma(-1) = \frac{1}{1 + e^{1}} \approx 0.27\)</span>
<span class="math notranslate nohighlight">\(\sigma(2) = \frac{1}{1 + e^{-2}} \approx 0.88\)</span></p>
<p><strong>E8.5.3</strong> <span class="math notranslate nohighlight">\(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx (0.73, 0.27, 0.88)\)</span>
<span class="math notranslate nohighlight">\(\bsigma'(\mathbf{z}) = (\bsigma'(1), \bsigma'(-1), \bsigma'(2)) \approx (0.20, 0.20, 0.10)\)</span></p>
<p><strong>E8.5.5</strong> <span class="math notranslate nohighlight">\(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\)</span>, so <span class="math notranslate nohighlight">\(\sigma(W\mathbf{x}) \approx (0.27, 0.98)\)</span>,
<span class="math notranslate nohighlight">\(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x}) \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 &amp; 0 \\ 0 &amp; 0.02 \end{pmatrix}\)</span></p>
<p><strong>E8.5.7</strong> <span class="math notranslate nohighlight">\(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2}) = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)</span></p>
&#13;

<h3><span class="section-number">8.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define the Jacobian matrix and use it to compute the differential of a vector-valued function.</p></li>
<li><p>State and apply the generalized Chain Rule to compute the Jacobian of a composition of functions.</p></li>
<li><p>Perform calculations with the Hadamard and Kronecker products of matrices.</p></li>
<li><p>Describe the purpose of automatic differentiation and its advantages over symbolic and numerical differentiation.</p></li>
<li><p>Implement automatic differentiation in PyTorch to compute gradients of vector-valued functions.</p></li>
<li><p>Derive the Chain Rule for multi-layer progressive functions and apply it to compute gradients.</p></li>
<li><p>Compare and contrast the forward and reverse modes of automatic differentiation in terms of computational complexity.</p></li>
<li><p>Define progressive functions and identify their key properties.</p></li>
<li><p>Derive the fundamental recursion for the Jacobian of a progressive function.</p></li>
<li><p>Implement the backpropagation algorithm to efficiently compute gradients of progressive functions.</p></li>
<li><p>Analyze the computational complexity of the backpropagation algorithm in terms of the number of matrix-vector products.</p></li>
<li><p>Describe the stochastic gradient descent (SGD) algorithm and explain how it differs from standard gradient descent.</p></li>
<li><p>Derive the update rule for stochastic gradient descent from the gradient of the loss function.</p></li>
<li><p>Prove that the expected SGD step is equal to the full gradient descent step.</p></li>
<li><p>Evaluate the performance of models trained using stochastic gradient descent on real-world datasets.</p></li>
<li><p>Define the multilayer perceptron (MLP) architecture and describe the role of affine maps and activation functions in each layer.</p></li>
<li><p>Compute the Jacobian of the sigmoid activation function using properties of diagonal matrices and Kronecker products.</p></li>
<li><p>Apply the chain rule to calculate the gradient of the loss function with respect to the weights in a small MLP example.</p></li>
<li><p>Generalize the gradient computation for an MLP with an arbitrary number of layers using a forward and backward pass.</p></li>
<li><p>Implement the training of a neural network using PyTorch.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
&#13;

<h2><span class="section-number">8.7.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="another-example-linear-regression">
<h3><span class="section-number">8.7.2.1. </span>Another example: linear regression<a class="headerlink" href="#another-example-linear-regression" title="Link to this heading">#</a></h3>
<p>We give another concrete example of progressive functions and of the application of backpropagration and stochastic gradient descent.</p>
<p><strong>Computing the gradient</strong> While we have motivated the framework introduced in the previous section from the point of view of classification, it also immediately applies to the regression setting. Both classification and regression are instances of supervised learning.</p>
<p>We first compute the gradient of a single sample. Here <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> again, but <span class="math notranslate nohighlight">\(y\)</span> is a real-valued outcome variable. We revisit the case of linear regression where the loss function is</p>
<div class="math notranslate nohighlight">
\[
\ell(z) = (z - y)^2 
\]</div>
<p>and the regression function only has input and output layers and no hidden layer (that is, <span class="math notranslate nohighlight">\(L=0\)</span>) with</p>
<div class="math notranslate nohighlight">
\[
h(\mathbf{w})
= \sum_{j=1}^d w_{j} x_{j} 
= \mathbf{x}^T\mathbf{w},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^{d}\)</span> are the parameters. Recall that we can include a constant term (one that does not depend on the input) by adding a <span class="math notranslate nohighlight">\(1\)</span> to the input. To keep the notation simple, we assume that this pre-processing has already been performed if desired.</p>
<p>Finally, the objective function for a single sample is</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w})
= \ell(h(\mathbf{w}))
= \left(\sum_{j=1}^d w_{j} x_{j}  - y\right)^2 
= \left(\mathbf{x}^T\mathbf{w}  - y\right)^2. 
\]</div>
<p>Using the notation from the previous subsection, the forward pass in this case is:</p>
<p><em>Initialization:</em></p>
<div class="math notranslate nohighlight">
\[\mathbf{z}_0 := \mathbf{x}.\]</div>
<p><em>Forward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0)
&amp;= \sum_{j=1}^d w_{0,j} z_{0,j}
= \mathbf{z}_0^T \mathbf{w}_0
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\begin{pmatrix}
A_0 &amp; B_0
\end{pmatrix}
:= J_{g_0}(\mathbf{z}_0,\mathbf{w}_0)
&amp;= ( w_{0,1},\ldots, w_{0,d},z_{0,1},\ldots,z_{0,d} )^T
= \begin{pmatrix}\mathbf{w}_0^T &amp; \mathbf{z}_0^T\end{pmatrix},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}_0 := \mathbf{w}\)</span>.</p>
<p><em>Loss:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z_2
&amp;:= \ell(z_1) = (z_1 - y)^2\\
p_2
&amp;:= \frac{\mathrm{d}}{\mathrm{d} z_1} {\ell}(z_1)
= 2 (z_1 - y).
\end{align*}\]</div>
<p>The backward pass is:</p>
<p><em>Backward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_0 := B_0^T p_1 
&amp;= 2 (z_1 - y) \, \mathbf{z}_0.
\end{align*}\]</div>
<p><em>Output:</em></p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= \mathbf{q}_0.
\]</div>
<p>As we noted before, there is in fact no need to compute <span class="math notranslate nohighlight">\(A_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{p}_0\)</span>.</p>
<p><strong>The <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset and the least-squares solution</strong> We return to the <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'advertising.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Unnamed: 0</th>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>9.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>12.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>200
</pre></div>
</div>
</div>
</div>
<p>We first compute the solution using the least-squares approach we detailed previously. We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack"><code class="docutils literal notranslate"><span class="pre">numpy.column_stack</span></code></a> to add a column of ones to the feature vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">TV</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'TV'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">radio</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'radio'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">newspaper</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'newspaper'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">sales</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sales'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">features</span><span class="p">))</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<p>The MSE is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">A</span> <span class="o">@</span> <span class="n">coeff</span> <span class="o">-</span> <span class="n">sales</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>2.7841263145109365
</pre></div>
</div>
</div>
</div>
<p><strong>Solving the problem using PyTorch</strong> We will be using PyTorch to implement the previous method. We first convert the data into PyTorch tensors. We then use <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.TensorDataset</span></code></a> to create the dataset. Finally, <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> provides the utilities to load the data in batches for training. We take mini-batches of size <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span> <span class="pre">=</span> <span class="pre">64</span></code> and we apply a random permutation of the samples on every pass (with the option <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">features_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sales_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">features_tensor</span><span class="p">,</span> <span class="n">sales_tensor</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we construct our model. It is simply an affine map from <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Note that there is no need to pre-process the inputs by adding <span class="math notranslate nohighlight">\(1\)</span>s. A constant term (or “bias variable”) is automatically added by PyTorch (unless one chooses the option <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">bias=False</span></code></a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 3 input features, 1 output value</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we are ready to run an optimization method of our choice on the loss function, which are specified next. There are many <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#algorithms">optimizers</a> available. (See this <a class="reference external" href="https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc">post</a> for a brief explanation of many common optimizers.) Here we use SGD as the optimizer. And the loss function is the MSE. A quick tutorial is <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Choosing the right number of passes (i.e. epochs) through the data requires some experimenting. Here <span class="math notranslate nohighlight">\(10^4\)</span> suffices. But in the interest of time, we will run it only for <span class="math notranslate nohighlight">\(100\)</span> epochs. As you will see from the results, this is not quite enough. On each pass, we compute the output of the current model, use <code class="docutils literal notranslate"><span class="pre">backward()</span></code> to obtain the gradient, and then perform a descent update with <code class="docutils literal notranslate"><span class="pre">step()</span></code>. We also have to reset the gradients first (otherwise they add up by default).</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The final parameters and loss are:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">weights</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Weights:"</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Bias:"</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Weights: [[0.05736413 0.11314777 0.08020781]]
Bias: [-0.02631279]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Evaluate the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean Squared Error on Training Set: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Mean Squared Error on Training Set: 7.885213494300842
</pre></div>
</div>
</div>
</div>
</section>
<section id="convolutional-neural-networks">
<h3><span class="section-number">8.7.2.2. </span>Convolutional neural networks<a class="headerlink" href="#convolutional-neural-networks" title="Link to this heading">#</a></h3>
<p>We return to the Fashion MNIST dataset. One can do even better than we did before using a neural network tailored for images, known as <a class="reference external" href="https://cs231n.github.io/convolutional-networks/">convolutional neural networks</a>. From <a class="reference external" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Wikipedia</a>:</p>
<blockquote>
<div><p>In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.</p>
</div></blockquote>
<p>More background can be found in this excellent <a class="reference external" href="http://cs231n.github.io/convolutional-networks/">module</a> from Stanford’s <a class="reference external" href="http://cs231n.github.io/">CS231n</a>. Our CNN will be a composition of <a class="reference external" href="http://cs231n.github.io/convolutional-networks/#conv">convolutional layers</a> and <a class="reference external" href="http://cs231n.github.io/convolutional-networks/#pool">pooling layers</a>.</p>
<p><strong>CHAT &amp; LEARN</strong> Convolutional neural networks (CNNs) are powerful for image classification. Ask your favorite AI chatbot to explain the basic concepts of CNNs, including convolutional layers and pooling layers. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                               <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                      <span class="k">else</span> <span class="p">(</span><span class="s2">"mps"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                            <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using device:"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Using device: mps
</pre></div>
</div>
</div>
</div>
<p>The new model is the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="c1"># First convolution, operating upon a 28x28 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Second convolution, operating upon a 14x14 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Third convolution, operating upon a 7x7 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Flatten the tensor</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>

    <span class="c1"># Fully connected layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We train and test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 1/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 2/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 3/3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 88.6% accuracy
</pre></div>
</div>
</div>
</div>
<p>Note the higher accuracy.</p>
<p>Finally, we try the original MNIST dataset. We use the same CNN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                      <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                     <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 1/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 2/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 3/3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 98.6% accuracy
</pre></div>
</div>
</div>
</div>
<p>Note the very high accuracy on this (easy - as it turns out) dataset.</p>
</section>
&#13;

<h3><span class="section-number">8.7.2.1. </span>Another example: linear regression<a class="headerlink" href="#another-example-linear-regression" title="Link to this heading">#</a></h3>
<p>We give another concrete example of progressive functions and of the application of backpropagration and stochastic gradient descent.</p>
<p><strong>Computing the gradient</strong> While we have motivated the framework introduced in the previous section from the point of view of classification, it also immediately applies to the regression setting. Both classification and regression are instances of supervised learning.</p>
<p>We first compute the gradient of a single sample. Here <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> again, but <span class="math notranslate nohighlight">\(y\)</span> is a real-valued outcome variable. We revisit the case of linear regression where the loss function is</p>
<div class="math notranslate nohighlight">
\[
\ell(z) = (z - y)^2 
\]</div>
<p>and the regression function only has input and output layers and no hidden layer (that is, <span class="math notranslate nohighlight">\(L=0\)</span>) with</p>
<div class="math notranslate nohighlight">
\[
h(\mathbf{w})
= \sum_{j=1}^d w_{j} x_{j} 
= \mathbf{x}^T\mathbf{w},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^{d}\)</span> are the parameters. Recall that we can include a constant term (one that does not depend on the input) by adding a <span class="math notranslate nohighlight">\(1\)</span> to the input. To keep the notation simple, we assume that this pre-processing has already been performed if desired.</p>
<p>Finally, the objective function for a single sample is</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w})
= \ell(h(\mathbf{w}))
= \left(\sum_{j=1}^d w_{j} x_{j}  - y\right)^2 
= \left(\mathbf{x}^T\mathbf{w}  - y\right)^2. 
\]</div>
<p>Using the notation from the previous subsection, the forward pass in this case is:</p>
<p><em>Initialization:</em></p>
<div class="math notranslate nohighlight">
\[\mathbf{z}_0 := \mathbf{x}.\]</div>
<p><em>Forward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0)
&amp;= \sum_{j=1}^d w_{0,j} z_{0,j}
= \mathbf{z}_0^T \mathbf{w}_0
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\begin{pmatrix}
A_0 &amp; B_0
\end{pmatrix}
:= J_{g_0}(\mathbf{z}_0,\mathbf{w}_0)
&amp;= ( w_{0,1},\ldots, w_{0,d},z_{0,1},\ldots,z_{0,d} )^T
= \begin{pmatrix}\mathbf{w}_0^T &amp; \mathbf{z}_0^T\end{pmatrix},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}_0 := \mathbf{w}\)</span>.</p>
<p><em>Loss:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z_2
&amp;:= \ell(z_1) = (z_1 - y)^2\\
p_2
&amp;:= \frac{\mathrm{d}}{\mathrm{d} z_1} {\ell}(z_1)
= 2 (z_1 - y).
\end{align*}\]</div>
<p>The backward pass is:</p>
<p><em>Backward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_0 := B_0^T p_1 
&amp;= 2 (z_1 - y) \, \mathbf{z}_0.
\end{align*}\]</div>
<p><em>Output:</em></p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= \mathbf{q}_0.
\]</div>
<p>As we noted before, there is in fact no need to compute <span class="math notranslate nohighlight">\(A_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{p}_0\)</span>.</p>
<p><strong>The <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset and the least-squares solution</strong> We return to the <code class="docutils literal notranslate"><span class="pre">Advertising</span></code> dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'advertising.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Unnamed: 0</th>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>9.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>12.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>200
</pre></div>
</div>
</div>
</div>
<p>We first compute the solution using the least-squares approach we detailed previously. We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack"><code class="docutils literal notranslate"><span class="pre">numpy.column_stack</span></code></a> to add a column of ones to the feature vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">TV</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'TV'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">radio</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'radio'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">newspaper</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'newspaper'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">sales</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sales'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">TV</span><span class="p">,</span> <span class="n">radio</span><span class="p">,</span> <span class="n">newspaper</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">features</span><span class="p">))</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">sales</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coeff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]
</pre></div>
</div>
</div>
</div>
<p>The MSE is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">A</span> <span class="o">@</span> <span class="n">coeff</span> <span class="o">-</span> <span class="n">sales</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>2.7841263145109365
</pre></div>
</div>
</div>
</div>
<p><strong>Solving the problem using PyTorch</strong> We will be using PyTorch to implement the previous method. We first convert the data into PyTorch tensors. We then use <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.TensorDataset</span></code></a> to create the dataset. Finally, <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> provides the utilities to load the data in batches for training. We take mini-batches of size <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span> <span class="pre">=</span> <span class="pre">64</span></code> and we apply a random permutation of the samples on every pass (with the option <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">features_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sales_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">features_tensor</span><span class="p">,</span> <span class="n">sales_tensor</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we construct our model. It is simply an affine map from <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Note that there is no need to pre-process the inputs by adding <span class="math notranslate nohighlight">\(1\)</span>s. A constant term (or “bias variable”) is automatically added by PyTorch (unless one chooses the option <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">bias=False</span></code></a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 3 input features, 1 output value</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we are ready to run an optimization method of our choice on the loss function, which are specified next. There are many <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#algorithms">optimizers</a> available. (See this <a class="reference external" href="https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc">post</a> for a brief explanation of many common optimizers.) Here we use SGD as the optimizer. And the loss function is the MSE. A quick tutorial is <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Choosing the right number of passes (i.e. epochs) through the data requires some experimenting. Here <span class="math notranslate nohighlight">\(10^4\)</span> suffices. But in the interest of time, we will run it only for <span class="math notranslate nohighlight">\(100\)</span> epochs. As you will see from the results, this is not quite enough. On each pass, we compute the output of the current model, use <code class="docutils literal notranslate"><span class="pre">backward()</span></code> to obtain the gradient, and then perform a descent update with <code class="docutils literal notranslate"><span class="pre">step()</span></code>. We also have to reset the gradients first (otherwise they add up by default).</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The final parameters and loss are:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">weights</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Weights:"</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Bias:"</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Weights: [[0.05736413 0.11314777 0.08020781]]
Bias: [-0.02631279]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Evaluate the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean Squared Error on Training Set: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Mean Squared Error on Training Set: 7.885213494300842
</pre></div>
</div>
</div>
</div>
&#13;

<h3><span class="section-number">8.7.2.2. </span>Convolutional neural networks<a class="headerlink" href="#convolutional-neural-networks" title="Link to this heading">#</a></h3>
<p>We return to the Fashion MNIST dataset. One can do even better than we did before using a neural network tailored for images, known as <a class="reference external" href="https://cs231n.github.io/convolutional-networks/">convolutional neural networks</a>. From <a class="reference external" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Wikipedia</a>:</p>
<blockquote>
<div><p>In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.</p>
</div></blockquote>
<p>More background can be found in this excellent <a class="reference external" href="http://cs231n.github.io/convolutional-networks/">module</a> from Stanford’s <a class="reference external" href="http://cs231n.github.io/">CS231n</a>. Our CNN will be a composition of <a class="reference external" href="http://cs231n.github.io/convolutional-networks/#conv">convolutional layers</a> and <a class="reference external" href="http://cs231n.github.io/convolutional-networks/#pool">pooling layers</a>.</p>
<p><strong>CHAT &amp; LEARN</strong> Convolutional neural networks (CNNs) are powerful for image classification. Ask your favorite AI chatbot to explain the basic concepts of CNNs, including convolutional layers and pooling layers. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                               <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                      <span class="k">else</span> <span class="p">(</span><span class="s2">"mps"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                            <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using device:"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Using device: mps
</pre></div>
</div>
</div>
</div>
<p>The new model is the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="c1"># First convolution, operating upon a 28x28 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Second convolution, operating upon a 14x14 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Third convolution, operating upon a 7x7 image</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

    <span class="c1"># Flatten the tensor</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>

    <span class="c1"># Fully connected layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We train and test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 1/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 2/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 3/3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 88.6% accuracy
</pre></div>
</div>
</div>
</div>
<p>Note the higher accuracy.</p>
<p>Finally, we try the original MNIST dataset. We use the same CNN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                      <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                     <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 1/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 2/3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Epoch 3/3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 98.6% accuracy
</pre></div>
</div>
</div>
</div>
<p>Note the very high accuracy on this (easy - as it turns out) dataset.</p>
&#13;

<h2><span class="section-number">8.7.3. </span>Additional proofs<a class="headerlink" href="#additional-proofs" title="Link to this heading">#</a></h2>
<p><strong>Proofs of Lagrange Multipliers Conditions</strong> We first prove the <em>Lagrange Multipliers: First-Order Necessary Conditions</em>. We follow the excellent textbook [<a class="reference external" href="http://www.athenasc.com/nonlinbook.html">Ber</a>, Section 4.1]. The proof uses the concept of the Jacobian.</p>
<p><em>Proof idea:</em> We reduce the problem to an unconstrained optimization problem by penalizing the constraint in the objective function. We then apply the unconstrained <em>First-Order Necessary Conditions</em>.</p>
<p><em>Proof:</em> <em>(Lagrange Multipliers: First-Order Necessary Conditions)</em> We reduce the problem to an unconstrained optimization problem by penalizing the constraint in the objective function. We also add a regularization term to ensure that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is the unique local minimizer in a neighborhood. Specifically, for each non-negative integer <span class="math notranslate nohighlight">\(k\)</span>, consider the objective function</p>
<div class="math notranslate nohighlight">
\[
F^k(\mathbf{x})
= f(\mathbf{x})
+ \frac{k}{2} \|\mathbf{h}(\mathbf{x})\|^2
+ \frac{\alpha}{2} \|\mathbf{x} - \mathbf{x}^*\|^2
\]</div>
<p>for some positive constant <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>. Note that as <span class="math notranslate nohighlight">\(k\)</span> gets larger, the penalty becomes more significant and, therefore, enforcing the constraint becomes more desirable. The proof proceeds in several steps.</p>
<p>We first consider a version minimizing <span class="math notranslate nohighlight">\(F^k\)</span> constrained to lie in a neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>. Because <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local minimizer of <span class="math notranslate nohighlight">\(f\)</span> subject to <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{x}) = \mathbf{0}\)</span>, there is <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(f(\mathbf{x}^*)\leq f(\mathbf{x})\)</span> for all feasible <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> within</p>
<div class="math notranslate nohighlight">
\[
\mathscr{X}
=
B_{\delta}(\mathbf{x}^*)
= 
\{\mathbf{x}:\|\mathbf{x} - \mathbf{x}^*\| \leq \delta\}.
\]</div>
<p><strong>LEMMA</strong> <strong>(Step I: Solving the Penalized Problem in a Neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>)</strong> For <span class="math notranslate nohighlight">\(k \geq 1\)</span>, let <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> be a global minimizer of the minimization problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathscr{X}} F^k(\mathbf{x}).
\]</div>
<p>a) The sequence <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>.</p>
<p>b) For <span class="math notranslate nohighlight">\(k\)</span> sufficiently large, <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> is a local minimizer of the objective function <span class="math notranslate nohighlight">\(F^k\)</span> <em>without any constraint</em>.</p>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The set <span class="math notranslate nohighlight">\(\mathscr{X}\)</span> is closed and bounded, and <span class="math notranslate nohighlight">\(F^k\)</span> is continuous. Hence, the sequence <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> is well-defined by the <em>Extreme Value Theorem</em>. Let <span class="math notranslate nohighlight">\(\bar{\mathbf{x}}\)</span> be any limit point of <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span>. We show that <span class="math notranslate nohighlight">\(\bar{\mathbf{x}} = \mathbf{x}^*\)</span>. That will imply a). It also implied b) since hen, for large enough <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> must be an interior point of <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(-\infty &lt; m \leq M &lt; + \infty\)</span> be the smallest and largest values of <span class="math notranslate nohighlight">\(f\)</span> on <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>, which exist by the <em>Extreme Value Theorem</em>. Then, for all <span class="math notranslate nohighlight">\(k\)</span>, by definition of <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> and the fact that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is feasible</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(*)
\qquad 
f(\mathbf{x}^k)
&amp;+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^k)\|^2
+ \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\\
&amp;\leq
f(\mathbf{x}^*)
+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^*)\|^2
+ \frac{\alpha}{2} \|\mathbf{x}^* - \mathbf{x}^*\|^2
= f(\mathbf{x}^*).
\end{align*}\]</div>
<p>Rearraning gives</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{h}(\mathbf{x}^k)\|^2
\leq \frac{2}{k} \left[f(\mathbf{x}^*) - f(\mathbf{x}^k) - \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\right]
\leq \frac{2}{k} \left[ f(\mathbf{x}^*) - m\right].
\]</div>
<p>So <span class="math notranslate nohighlight">\(\lim_{k \to \infty} \|\mathbf{h}(\mathbf{x}^k)\|^2 = 0\)</span>, which, by the continuity of <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> and of the Frobenius norm, implies that
<span class="math notranslate nohighlight">\(\|\mathbf{h}(\bar{\mathbf{x}})\|^2 = 0\)</span>, that is, <span class="math notranslate nohighlight">\(\mathbf{h}(\bar{\mathbf{x}}) = \mathbf{0}\)</span>. In other words, any limit point <span class="math notranslate nohighlight">\(\bar{\mathbf{x}}\)</span> is feasible.</p>
<p>In addition to being feasible, <span class="math notranslate nohighlight">\(\bar{\mathbf{x}} \in \mathscr{X}\)</span> because that constraint set is closed. So, by the choice of <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>, we have <span class="math notranslate nohighlight">\(f(\mathbf{x}^*)
\leq f(\bar{\mathbf{x}})\)</span>. Furthermore, by <span class="math notranslate nohighlight">\((*)\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}^*)
\leq
f(\bar{\mathbf{x}}) + \frac{\alpha}{2} \|\bar{\mathbf{x}} - \mathbf{x}^*\|^2
\leq
f(\mathbf{x}^*).
\]</div>
<p>This is only possible if <span class="math notranslate nohighlight">\(\|\bar{\mathbf{x}} - \mathbf{x}^*\|^2 = 0\)</span> or, put differently, <span class="math notranslate nohighlight">\(\bar{\mathbf{x}} = \mathbf{x}^*\)</span>. That proves the lemma. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>LEMMA</strong> <strong>(Step II: Applying the Unconstrained Necessary Conditions)</strong> Let <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> be the sequence in the previous lemma.</p>
<p>a) For sufficiently large <span class="math notranslate nohighlight">\(k\)</span>, the vectors <span class="math notranslate nohighlight">\(\nabla h_i(\mathbf{x}^k)\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,\ell\)</span>, are linearly independent.</p>
<p>b) Let <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x})\)</span> be the Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>, that is, the matrix whose rows are the (row) vectors <span class="math notranslate nohighlight">\(\nabla h_i(\mathbf{x})^T\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,\ell\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x}^*)
+ \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
= \mathbf{0}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\blambda^*
=
- (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \, \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*) \nabla f(\mathbf{x}^*).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By the previous lemma, for <span class="math notranslate nohighlight">\(k\)</span> large enough, <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> is an unconstrained local minimizer of <span class="math notranslate nohighlight">\(F^k\)</span>. So by the (unconstrained) <em>First-Order Necessary Conditions</em>, it holds that</p>
<div class="math notranslate nohighlight">
\[
\nabla F^k(\mathbf{x}^k)
=
\mathbf{0}.
\]</div>
<p>To compute the gradient of <span class="math notranslate nohighlight">\(F^k\)</span> we note that</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{h}(\mathbf{x})\|^2
= 
\sum_{i=1}^\ell
(h_i(\mathbf{x}))^2.
\]</div>
<p>The partial derivatives are</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2
=
\sum_{i=1}^\ell
\frac{\partial}{\partial x_j} (h_i(\mathbf{x}))^2
=
\sum_{i=1}^\ell
2  h_i(\mathbf{x}) \frac{\partial h_i(\mathbf{x})}{\partial x_j},
\]</div>
<p>by the <em>Chain Rule</em>. So, in vector form,</p>
<div class="math notranslate nohighlight">
\[
\nabla \|\mathbf{h}(\mathbf{x})\|^2
= 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \mathbf{h}(\mathbf{x}).
\]</div>
<p>The term <span class="math notranslate nohighlight">\(\|\mathbf{x} - \mathbf{x}^*\|^2\)</span> can be rewritten as the quadratic function</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{x} - \mathbf{x}^*\|^2
= \frac{1}{2}\mathbf{x}^T (2 I_{d \times d}) \mathbf{x} - 2 (\mathbf{x}^*)^T \mathbf{x} + (\mathbf{x}^*)^T \mathbf{x}^*.
\]</div>
<p>Using a previous formula with <span class="math notranslate nohighlight">\(P = 2 I_{d \times d}\)</span> (which is symmetric), <span class="math notranslate nohighlight">\(\mathbf{q} = -2 \mathbf{x}^*\)</span> and <span class="math notranslate nohighlight">\(r = (\mathbf{x}^*)^T \mathbf{x}^*\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
\nabla \|\mathbf{x} - \mathbf{x}^*\|^2
= 2\mathbf{x} -2 \mathbf{x}^*.
\]</div>
<p>So, putting everything together,</p>
<div class="math notranslate nohighlight">
\[
(**)
\qquad \mathbf{0}
=
\nabla F^k(\mathbf{x}^k)
=
\nabla f(\mathbf{x}^k)
+
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T (k \mathbf{h}(\mathbf{x}^k))
+ \alpha(\mathbf{x}^k - \mathbf{x}^*).
\]</div>
<p>By the previous lemma, <span class="math notranslate nohighlight">\(\mathbf{x}^k \to \mathbf{x}^*\)</span>, <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}^k) \to \nabla f(\mathbf{x}^*)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)</span> as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>.</p>
<p>So it remains to derive the limit of <span class="math notranslate nohighlight">\(k \mathbf{h}(\mathbf{x}^k)\)</span>. By assumption, the columns of <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T\)</span> are linearly independent. That implies that for any unit vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^\ell\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} = \|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}\|^2 &gt; 0
\]</div>
<p>otherwise we would have <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} = \mathbf{0}\)</span>, contradicting the linear independence assumption. By the <em>Extreme Value Theorem</em>, there is <span class="math notranslate nohighlight">\(\beta &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} \geq \beta
\]</div>
<p>for all unit vectors <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^\ell\)</span>. Since <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)</span>, it follows from a previous lemma that, for <span class="math notranslate nohighlight">\(k\)</span> large enough and any unit vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^\ell\)</span>,</p>
<div class="math notranslate nohighlight">
\[
|\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}
- 
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T \mathbf{z}|
\leq 
\|
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
-
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
\|_F
\leq 
\frac{\beta}{2}.
\]</div>
<p>That implies</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T \mathbf{z} \geq \frac{\beta}{2},
\]</div>
<p>so by the same argument as above the columns of <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)</span> are linearly independent for <span class="math notranslate nohighlight">\(k\)</span> large enough and
<span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)</span> is invertible. That proves a).</p>
<p>Going back to <span class="math notranslate nohighlight">\((**)\)</span>, multiplying both sides by <span class="math notranslate nohighlight">\((\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T)^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\)</span>, and taking a limit <span class="math notranslate nohighlight">\(k \to +\infty\)</span>, we get after rearranging that</p>
<div class="math notranslate nohighlight">
\[
k \mathbf{h}(\mathbf{x}^k)
\to
- (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) ^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)  \nabla f(\mathbf{x}^*) = \blambda^*.
\]</div>
<p>Plugging back we get</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x}^*)
+ \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
= \mathbf{0}
\]</div>
<p>as claimed. That proves b). <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Combining the lemmas establishes the theorem. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Next, we prove the <em>Lagrange Multipliers: Second-Order Sufficient Conditions</em>. Again, we follow [<a class="reference external" href="http://www.athenasc.com/nonlinbook.html">Ber</a>, Section 4.2]. We will need the following lemma. The proof can be skipped.</p>
<p><em>Proof idea:</em> We consider a slightly modified problem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
&amp;\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}
\end{align*}\]</div>
<p>which has the same local minimizers. Applying the <em>Second-Order Sufficient Conditions</em> to the Lagrangian of the modified problem gives the result when <span class="math notranslate nohighlight">\(c\)</span> is chosen large enough.</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> be symmetric matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{n \times n}\)</span>. Assume that <span class="math notranslate nohighlight">\(Q\)</span> is positive semidefinite and that <span class="math notranslate nohighlight">\(P\)</span> is positive definite on the null space of <span class="math notranslate nohighlight">\(Q\)</span>, that is, <span class="math notranslate nohighlight">\(\mathbf{w}^T P \mathbf{w} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{w} \neq \mathbf{0}\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{w}^T Q \mathbf{w} = \mathbf{0}\)</span>. Then there is a scalar <span class="math notranslate nohighlight">\(\bar{c} \geq 0\)</span> such that <span class="math notranslate nohighlight">\(P + c Q\)</span> is positive definite for all <span class="math notranslate nohighlight">\(c &gt; \bar{c}\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> <em>(lemma)</em> We argue by contradiction. Suppose there is an non-negative, increasing, diverging sequence <span class="math notranslate nohighlight">\(\{c_k\}_{k=1}^{+\infty}\)</span> and a sequence of unit vectors <span class="math notranslate nohighlight">\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k \leq 0
\]</div>
<p>for all <span class="math notranslate nohighlight">\(k\)</span>. Because the sequence is bounded, it has a limit point <span class="math notranslate nohighlight">\(\bar{\mathbf{x}}\)</span>. Assume without loss of generality that <span class="math notranslate nohighlight">\(\mathbf{x}^k \to \bar{\mathbf{x}}\)</span>, as <span class="math notranslate nohighlight">\(k \to \infty\)</span>. Since <span class="math notranslate nohighlight">\(c_k \to +\infty\)</span> and <span class="math notranslate nohighlight">\((\mathbf{x}^k)^T Q \mathbf{x}^k \geq 0\)</span> by assumption, we must have <span class="math notranslate nohighlight">\((\bar{\mathbf{x}})^T Q \bar{\mathbf{x}} = 0\)</span>, otherwise <span class="math notranslate nohighlight">\((\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k\)</span> would diverge. Hence, by the assumption in the statement, it must be that <span class="math notranslate nohighlight">\((\bar{\mathbf{x}})^T P \bar{\mathbf{x}} &gt; 0\)</span>. This contradicts the inequality in the display above. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em>Proof:</em> <em>(Lagrange Multipliers: Second-Order Sufficient Conditions)</em> We consider the modified problem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
&amp;\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}.
\end{align*}\]</div>
<p>It has the same local minimizers as the orginal problem as the additional term in the objective is zero for feasible vectors. That extra term will allow us to use the previous lemma. For notational convenience, we define</p>
<div class="math notranslate nohighlight">
\[
g_c(\mathbf{x})
= f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2.
\]</div>
<p>The Lagrangian of the modified problem is</p>
<div class="math notranslate nohighlight">
\[
L_c(\mathbf{x}, \blambda)
= g_c(\mathbf{x}) + \mathbf{h}(\mathbf{x})^T \blambda.
\]</div>
<p>We will apply the <em>Second-Order Sufficient Conditions</em> to problem of minimizing <span class="math notranslate nohighlight">\(L_c\)</span> over <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. We indicate the Hessian with respect to only the variables <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as <span class="math notranslate nohighlight">\(\nabla^2_{\mathbf{x},\mathbf{x}}\)</span>.</p>
<p>Recall from the proof of  the <em>Lagrange Multipliers: First-Order Necessary Conditions</em> that</p>
<div class="math notranslate nohighlight">
\[
\nabla \|\mathbf{h}(\mathbf{x})\|^2
= 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \mathbf{h}(\mathbf{x}).
\]</div>
<p>To compute the Hessian of that function, we note</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial}{\partial x_i}\left(
\frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2\right)
&amp;=
\frac{\partial}{\partial x_i}\left(
\sum_{k=1}^\ell
2  h_k(\mathbf{x}) \frac{\partial h_k(\mathbf{x})}{\partial x_j}\right)\\
&amp;=
2 \sum_{k=1}^\ell\left(
 \frac{\partial h_k(\mathbf{x})}{\partial x_i}  \frac{\partial h_k(\mathbf{x})}{\partial x_j}
+
h_k(\mathbf{x}) \frac{\partial^2 h_k(\mathbf{x})}{\partial x_i \partial x_j}
\right)\\
&amp;= 2 \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}) +  \sum_{k=1}^\ell h_k(\mathbf{x}) \, \mathbf{H}_{h_k}(\mathbf{x})\right)_{i,j}.
\end{align*}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
=
\nabla f(\mathbf{x}^*)
+ 
c \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{h}(\mathbf{x}^*)
+ 
\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
= 
\mathbf{H}_{f}(\mathbf{x}^*)
+
c \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) +  \sum_{k=1}^\ell h_k(\mathbf{x}^*) \, \mathbf{H}_{h_k}(\mathbf{x}^*)\right) 
+
\sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*).
\]</div>
<p>By the assumptions of the theorem, this simplifies to</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
=
\mathbf{0}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
=\underbrace{\left\{
\mathbf{H}_{f}(\mathbf{x}^*)
+
\sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*)
\right\}}_{P}
+
c \underbrace{\left\{\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \right\}}_{Q},
\]</div>
<p>where further <span class="math notranslate nohighlight">\(\mathbf{v}^T P \mathbf{v} &gt; 0\)</span> for any <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{v}  = \mathbf{0}\)</span> (which itself implies <span class="math notranslate nohighlight">\(\mathbf{v}^T Q \mathbf{v} = \mathbf{0}\)</span>). Furthermore, <span class="math notranslate nohighlight">\(Q \succeq \mathbf{0}\)</span> since</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^T Q \mathbf{w}
= \mathbf{w}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)  \mathbf{w}
= \left\|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)  \mathbf{w}\right\|^2 \geq 0
\]</div>
<p>for any <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^d\)</span>. The previous lemma allows us to take <span class="math notranslate nohighlight">\(c\)</span> large enough that <span class="math notranslate nohighlight">\(\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) \succ \mathbf{0}\)</span>.</p>
<p>As a result, the unconstrained <em>Second-Order Sufficient Conditions</em> are satisfied at <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> for the problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^d} L_c(\mathbf{x}, \blambda^*).
\]</div>
<p>That is, there is <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
L_c(\mathbf{x}^*, \blambda^*)
&lt; L_c(\mathbf{x}, \blambda^*),
\qquad \forall \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}.
\]</div>
<p>Restricting this to the feasible vectors of the modified constrained problem (i.e., those such that <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{x}) = \mathbf{0}\)</span>) implies after simplification</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}^*)
&lt; f(\mathbf{x}),
\qquad \forall \mathbf{x} \in \{\mathbf{x} : \mathbf{h}(\mathbf{x}) = \mathbf{0}\} \cap (B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}).
\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a strict local minimizer of the modified constrained problem (and, in turn, of the original constrained problem). That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
    
</body>
</html>