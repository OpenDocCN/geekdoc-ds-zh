<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>17.5¬†Moravian Spanning Treesüîó</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>17.5¬†Moravian Spanning Treesüîó</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://dcic-world.org/2025-08-27/mst.html">https://dcic-world.org/2025-08-27/mst.html</a></blockquote><table cellspacing="0" cellpadding="0"><tr><td><p>¬†¬†¬†¬†<a href="#%28part._.The_.Problem%29" class="toclink" data-pltdoc="x">17.5.1¬†The Problem</a></p></td></tr><tr><td><p>¬†¬†¬†¬†<a href="#%28part._.A_.Greedy_.Solution%29" class="toclink" data-pltdoc="x">17.5.2¬†A Greedy Solution</a></p></td></tr><tr><td><p>¬†¬†¬†¬†<a href="#%28part._.Another_.Greedy_.Solution%29" class="toclink" data-pltdoc="x">17.5.3¬†Another Greedy Solution</a></p></td></tr><tr><td><p>¬†¬†¬†¬†<a href="#%28part._.A_.Third_.Solution%29" class="toclink" data-pltdoc="x">17.5.4¬†A Third Solution</a></p></td></tr><tr><td><p>¬†¬†¬†¬†<a href="#%28part._union-find-functional%29" class="toclink" data-pltdoc="x">17.5.5¬†Checking Component Connectedness</a></p></td></tr></table><p>At the turn of the milennium, the US National Academy of Engineering
surveyed its members to determine the ‚ÄúGreatest Engineering
Achievements of the 20th Century‚Äù. The list contained the usual
suspects: electronics, computers, the Internet, and so on. But a
perhaps surprising idea topped the list: (rural)
electrification.Read more about it
<a href="http://www.greatachievements.org/">on their site</a>.</p><section class="SsectionLevel4" id="section 17.5.1"><h4 class="heading">17.5.1¬†<a name="(part._.The_.Problem)"/>The Problem<a href="#(part._.The_.Problem)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>To understand the history of national electrical grids, it helps to go
back to <a href="http://en.wikipedia.org/wiki/Moravia">Moravia</a>
in the 1920s. Like many parts of the world, it was beginning to
realize the benefits of electricity and intended to spread it around
the region. A Moravian academia named Otakar Bor≈Øvka heard about the
problem, and in a remarkable effort, described the problem abstractly,
so that it could be understood without reference to Moravia or
electrical networks. He modeled it as a problem about graphs.</p><p>Bor≈Øvka observed that at least initially, any solution to the problem
of creating a network must have the following characteristics:
</p><ul><li><p>The electrical network must reach all the towns intended to be
covered by it. In graph terms, the solution must be spanning,
meaning it must visit every node in the graph.</p></li><li><p>Redundancy is a valuable property in any network: that way, if
one set of links goes down, there might be another way to get a
payload to its destination. When starting out, however, redundancy
may be too expensive, especially if it comes at the cost of not
giving someone a payload at all. Thus, the initial solution was best
set up without loops or even redundant paths. In graph terms, the
solution had to be a tree.</p></li><li><p>Finally, the goal was to solve this problem for the least cost
possible. In graph terms, the graph would be weighted, and the
solution had to be a minimum.</p></li></ul><p>Thus Bor≈Øvka defined the Moravian Spanning Tree (MST) problem.</p></section><section class="SsectionLevel4" id="section 17.5.2"><h4 class="heading">17.5.2¬†<a name="(part._.A_.Greedy_.Solution)"/>A Greedy Solution<a href="#(part._.A_.Greedy_.Solution)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>Bor≈Øvka had published his problem, and another Czech mathematician,
<a href="http://en.wikipedia.org/wiki/Vojt%C4%9Bch_Jarn%C3%ADk">Vojtƒõch Jarn√≠k</a>,
came across it. Jarn√≠k came up with a solution that should sound
familiar:
</p><ul><li><p>Begin with a solution consisting of a single node, chosen
arbitrarily. For the graph consisting of this one node, this
solution is clearly a minimum, spanning, and a tree.</p></li><li><p>Of all the edges incident on nodes in the solution that
connect to a node not already in the solution, pick the edge with
the least weight.Note that we consider only the
incident edges, not their weight added to the weight of the node to
which they are incident.</p></li><li><p>Add this edge to the solution. The claim is that for the new
solution will be a tree (by construction), spanning (also by
construction), and a minimum. The minimality follows by an argument
similar to that used for Dijkstra‚Äôs Algorithm.</p></li></ul><p>Jarn√≠k had the misfortune of publishing this work in Czech in 1930,
and it went largely ignored. It was rediscovered by others, most
notably by R.C. Prim in 1957, and is now generally known as
Prim‚Äôs Algorithm, though calling it Jarn√≠k‚Äôs Algorithm
would attribute credit in the right place.</p><p>Implementing this algorithm is pretty easy. At each point, we need to
know the lightest edge incident on the current solution tree. Finding
the lightest edge takes time linear in the number of these edges, but
the very lightest one may create a cycle. We therefore need to
efficiently check for whether adding an edge would create a cycle, a
problem we will return to multiple times [<a href="#%28part._union-find-functional%29" data-pltdoc="x">Checking Component Connectedness</a>].
Assuming we can do
that effectively, we then want to add the lightest edge and
iterate. Even given an efficient solution for checking cyclicity, this
would seem to require an operation linear in the number of edges for
each node. With better representations we can improve on this
complexity, but let‚Äôs look at other ideas first.</p></section><section class="SsectionLevel4" id="section 17.5.3"><h4 class="heading">17.5.3¬†<a name="(part._.Another_.Greedy_.Solution)"/>Another Greedy Solution<a href="#(part._.Another_.Greedy_.Solution)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>Recall that Jarn√≠k presented his algorithm in 1930, when computers
didn‚Äôt exist, and Prim his in 1957, when they were very much in their
infancy. Programming computers to track heaps was a non-trivial
problem, and many algorithms were implemented by hand, where keeping
track of a complex data structure without making errors was harder
still. There was need for a solution that was required less manual
bookkeeping (literally speaking).</p><p>In 1956,
<a href="http://en.wikipedia.org/wiki/Joseph_Kruskal">Joseph Kruskal</a>
presented such a solution. His idea was elegantly simple. The Jarn√≠k
algorithm suffers from the problem that each time the tree grows, we
have to revise the content of the heap, which is already a messy
structure to track. Kruskal noted the following.</p><p>To obtain a minimum solution, surely we want to include one of the
edges of least weight in the graph. Because if not, we can take an
otherwise minimal solution, add this edge, and remove one other edge;
the graph would still be just as connected, but the overall weight
would be no more and, if the removed edge were heavier, would be
less.Note the careful wording: there may be many edges
of the same least weight, so adding one of them may remove another,
and therefore not produce a lighter tree; but the key point is that it
certainly will not produce a heavier one. By the same argument we can
add the next lightest edge, and the next lightest, and so on. The only
time we cannot add the next lightest edge is when it would create a
cycle (that problem again!).</p><p>Therefore, Kruskal‚Äôs algorithm is utterly straightforward. We first
sort all the edges, ordered by ascending weight. We then take each
edge in ascending weight order and add it to the solution provided it
will not create a cycle. When we have thus processed all the edges, we
will have a solution that is a tree (by construction), spanning
(because every connected vertex must be the endpoint of some edge),
and of minimum weight (by the argument above). The complexity is that
of sorting (which is \([e \rightarrow e \log e]\) where \(e\) is the
size of the edge set. We then iterate over each element in \(e\),
which takes time linear in the size of that set‚Äî<wbr/>modulo the time to
check for cycles. This algorithm is also easy to implement on paper,
because we sort all the edges once, then keep checking them off in
order, crossing out the ones that create cycles‚Äî<wbr/>with no dynamic
updating of the list needed.</p></section><section class="SsectionLevel4" id="section 17.5.4"><h4 class="heading">17.5.4¬†<a name="(part._.A_.Third_.Solution)"/>A Third Solution<a href="#(part._.A_.Third_.Solution)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>Both the Jarn√≠k and Kruskal solutions have one flaw: they require a
centralized data structure (the priority heap, or the sorted list) to
incrementally build the solution. As parallel computers became
available, and graph problems grew large, computer scientists looked
for solutions that could be implemented more efficiently in
parallel‚Äî<wbr/>which typically meant avoiding any centralized points of
synchronization, such as these centralized data structures.</p><p>In 1965, M. Sollin constructed an algorithm that met these needs
beautifully. In this algorithm, instead of constructing a single
solution, we grow multiple solution components (potentially in
parallel if we so wish). Each node starts out as a solution component
(as it was at the first step of Jarn√≠k‚Äôs Algorithm). Each node
considers the edges incident to it, and picks the lightest one that
connects to a different component (that problem again!). If
such an edge can be found, the edge becomes part of the solution, and
the two components combine to become a single component. The entire
process repeats.</p><p>Because every node begins as part of the solution, this algorithm
naturally spans. Because it checks for cycles and avoids them, it
naturally forms a tree.Note that avoiding cycles yields
a DAG and is not automatically guaranteed to yield a tree. We have
been a bit lax about this difference throughout this section.
Finally, minimality follows through similar reasoning as we used in
the case of Jarn√≠k‚Äôs Algorithm, which we have essentially run in
parallel, once from each node, until the parallel solution components
join up to produce a global solution.</p><p>Of course, maintaining the data for this algorithm by hand is a
nightmare. Therefore, it would be no surprise that this algorithm was
coined in the digital age. The real surprise, therefore, is that it
was not: it was originally created by
<a href="http://en.wikipedia.org/wiki/Otakar_Bor%C5%AFvka">Otakar Bor≈Øvka</a>
himself.</p><p>Bor≈Øvka, you see, had figured it all out. He‚Äôd not only understood the
problem, he had:
</p><ul><li><p>pinpointed the real problem lying underneath the
electrification problem so it could be viewed in a
context-independent way,</p></li><li><p>created a descriptive language of graph theory to define it
precisely, and</p></li><li><p>even solved the problem in addition to defining it.</p></li></ul><p>He‚Äôd just come up with a solution so complex to implement by hand that
Jarn√≠k had in essence de-parallelized it so it could be done
sequentially. And thus this algorithm lay unnoticed until it was
reinvented
(<a href="http://en.wikipedia.org/wiki/Bor%C5%AFvka's_algorithm">several times, actually</a>)
by Sollin in time for parallel computing folks to notice a need for
it. But now we can just call this Bor≈Øvka‚Äôs Algorithm, which is
only fitting.</p><p>As you might have guessed by now, this problem is indeed called the
MST in other textbooks, but ‚ÄúM‚Äù stands not for Moravia but for
‚ÄúMinimum‚Äù. But given Bor≈Øvka‚Äôs forgotten place in history, we prefer
the more whimsical name.</p></section><section class="SsectionLevel4" id="section 17.5.5"><h4 class="heading">17.5.5¬†<a name="(part._union-find-functional)"/>Checking Component Connectedness<a href="#(part._union-find-functional)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>As we‚Äôve seen, we need to be able to efficiently tell whether two
nodes are in the same component. One way to do this is to conduct a
depth-first traversal (or breadth-first traversal) starting from the
first node and checking whether we ever visit the second one. (Using
one of these traversal strategies ensures that we terminate in the
presence of loops.) Unfortunately, this takes a linear amount of time
(in the size of the graph) for every pair of nodes‚Äî<wbr/>and
depending on the graph and choice of node, we might do this for every
node in the graph on every edge addition! So we‚Äôd clearly like to do
this better.</p><p>It is helpful to reduce this problem from graph connectivity to a more
general one: of disjoint-set structure (colloquially known as
union-find for reasons that will soon be clear). If we think of
each connected component as a set, then we‚Äôre asking whether two nodes
are in the same set. But casting it as a set membership problem makes
it applicable in several other applications as well.</p><p>The setup is as follows. For arbitrary values, we want the ability to
think of them as elements in a set.
We are interested in two operations. One is obviously <code data-lang="pyret" class="sourceCode">union</code>,
which merges two sets into one. The other would seem to be something
like <code data-lang="pyret" class="sourceCode">is-in-same-set</code> that takes two elements and determines
whether they‚Äôre in the same set. Over time, however, it has proven
useful to instead define the operator <code data-lang="pyret" class="sourceCode">find</code> that, given an
element, ‚Äúnames‚Äù the set (more on this in a moment) that the element
belongs to. To check whether two elements are in the same set, we then
have to get the ‚Äúset name‚Äù for each element, and check whether these
names are the same. This certainly sounds more roundabout, but this
means we have a primitive that may be useful in other contexts, and
from which we can easily implement <code data-lang="pyret" class="sourceCode">is-in-same-set</code>.</p><p>Now the question is, how do we name sets? The real question we should
ask is, what operations do we care to perform on these names? All we
care about is, given two names, they represent the same set precisely
when the names are the same. Therefore, we could construct a new
string, or number, or something else, but we have another option:
simply pick some element of the set to represent it, i.e., to serve as
its name. Thus we will associate each set element with an
indicator of the ‚Äúset name‚Äù for that element; if there isn‚Äôt one,
then its name is itself (the <code data-lang="pyret" class="sourceCode">none</code> case of <code data-lang="pyret" class="sourceCode">parent</code>):
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Element&lt;T&gt;:
  | elt(val :: T, parent :: Option&lt;Element&gt;)
end</code></pre><p>We will assume we have some equality predicate for checking when two
elements are the same, which we do by comparing their value parts,
ignoring their parent values:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun is-same-element(e1, e2): e1.val &lt;=&gt; e2.val end</code></pre><blockquote class="Incercise"><p class="IncerciseHeader">Do Now!</p><blockquote class="IncerciseBody"><p>Why do we check only the value parts?</p></blockquote></blockquote><p>We will assume that for a given set, we always return the
same representative element. (Otherwise, equality will fail
even though we have the same set.) Thus:We‚Äôve used the
name <code data-lang="pyret" class="sourceCode">fynd</code> because <code data-lang="pyret" class="sourceCode">find</code> is already defined to mean
something else in Pyret. If you don‚Äôt like the misspelling, you‚Äôre
welcome to use a longer name like <code data-lang="pyret" class="sourceCode">find-root</code>.
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun is-in-same-set(e1 :: Element, e2 :: Element, s :: Sets)
    -&gt; Boolean:
  s1 = fynd(e1, s)
  s2 = fynd(e2, s)
  identical(s1, s2)
end</code></pre><p>where <code data-lang="pyret" class="sourceCode">Sets</code> is the list of all elements:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">type Sets = List&lt;Element&gt;</code></pre><p>How do we find the representative element for a set? We first find it
using <code data-lang="pyret" class="sourceCode">is-same-element</code>; when we do, we check the
element‚Äôs <code data-lang="pyret" class="sourceCode">parent</code> field. If it is <code data-lang="pyret" class="sourceCode">none</code>, that means this
very element names its set; this can happen either because the element
is a singleton set (we‚Äôll initialize all elements with <code data-lang="pyret" class="sourceCode">none</code>),
or it‚Äôs the name for some larger set. Either way, we‚Äôre
done. Otherwise, we have to recursively find the parent:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun fynd(e :: Element, s :: Sets) -&gt; Element:
  cases (List) s:
    | empty =&gt; raise("fynd: shouldn't have gotten here")
    | link(f, r) =&gt;
      if is-same-element(f, e):
        cases (Option) f.parent:
          | none =&gt; f
          | some(p) =&gt; fynd(p, s)
        end
      else:
        fynd(e, r)
      end
  end
end</code></pre><blockquote class="Exercise"><p class="ExerciseHeader">Exercise</p><blockquote class="ExerciseBody"><p>Why is there a recursive call in the nested <code data-lang="pyret" class="sourceCode">cases</code>?</p></blockquote></blockquote><p>What‚Äôs left is to implement <code data-lang="pyret" class="sourceCode">union</code>. For this, we find the
representative elements of the two sets we‚Äôre trying to union; if they
are the same, then the two sets are already in a union; otherwise, we
have to update the data structure:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun union(e1 :: Element, e2 :: Element, s :: Sets) -&gt; Sets:
  s1 = fynd(e1, s)
  s2 = fynd(e2, s)
  if identical(s1, s2):
    s
  else:
    update-set-with(s, s1, s2)
  end
end</code></pre><p>To update, we arbitrarily choose one of the set names to be the name
of the new compound set. We then have to update the parent of the
other set‚Äôs name element to be this one:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun update-set-with(s :: Sets, child :: Element, parent :: Element)
    -&gt; Sets:
  cases (List) s:
    | empty =&gt; raise("update: shouldn't have gotten here")
    | link(f, r) =&gt;
      if is-same-element(f, child):
        link(elt(f.val, some(parent)), r)
      else:
        link(f, update-set-with(r, child, parent))
      end
  end
end</code></pre><p>Here are some tests to illustrate this working:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">check:
  s0 = map(elt(_, none), [list: 0, 1, 2, 3, 4, 5, 6, 7])
  s1 = union(get(s0, 0), get(s0, 2), s0)
  s2 = union(get(s1, 0), get(s1, 3), s1)
  s3 = union(get(s2, 3), get(s2, 5), s2)
  print(s3)
  is-same-element(fynd(get(s0, 0), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 2), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 3), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 5), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 7), s3), fynd(get(s0, 7), s3)) is true
end</code></pre><p>Unfortunately, this implementation suffers from two major problems:
</p><ul><li><p>First, because we are performing functional updates, the value
of the <code data-lang="pyret" class="sourceCode">parent</code> reference keeps ‚Äúchanging‚Äù, but these changes
are not visible to older copies of the ‚Äúsame‚Äù value. An element
from different stages of unioning has different parent references,
even though it is arguably the same element throughout. This is a
place where functional programming hurts.</p></li><li><p>Relatedly, the performance of this implementation is quite
bad. <code data-lang="pyret" class="sourceCode">fynd</code> recursively traverses parents to find the set‚Äôs
name, but the elements traversed are not updated to record this new
name. We certainly could update them by reconstructing the set
afresh each time, but that complicates the implementation and, as we
will soon see, we can do much better.</p></li></ul><p>Even worse, it may not even be correct!</p><blockquote class="Exercise"><p class="ExerciseHeader">Exercise</p><blockquote class="ExerciseBody"><p>Is it? Consider constructing <code data-lang="pyret" class="sourceCode">union</code>s that are not quite so skewed as
above, and see whether you get the results you expect.</p></blockquote></blockquote><p>The bottom line is that pure functional programming is not a
great fit with this problem. We need a better implementation
strategy: <a href="union-find.html" data-pltdoc="x">Union-Find</a>.</p></section>&#13;
<h4 class="heading">17.5.1¬†<a name="(part._.The_.Problem)"/>The Problem<a href="#(part._.The_.Problem)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>To understand the history of national electrical grids, it helps to go
back to <a href="http://en.wikipedia.org/wiki/Moravia">Moravia</a>
in the 1920s. Like many parts of the world, it was beginning to
realize the benefits of electricity and intended to spread it around
the region. A Moravian academia named Otakar Bor≈Øvka heard about the
problem, and in a remarkable effort, described the problem abstractly,
so that it could be understood without reference to Moravia or
electrical networks. He modeled it as a problem about graphs.</p><p>Bor≈Øvka observed that at least initially, any solution to the problem
of creating a network must have the following characteristics:
</p><ul><li><p>The electrical network must reach all the towns intended to be
covered by it. In graph terms, the solution must be spanning,
meaning it must visit every node in the graph.</p></li><li><p>Redundancy is a valuable property in any network: that way, if
one set of links goes down, there might be another way to get a
payload to its destination. When starting out, however, redundancy
may be too expensive, especially if it comes at the cost of not
giving someone a payload at all. Thus, the initial solution was best
set up without loops or even redundant paths. In graph terms, the
solution had to be a tree.</p></li><li><p>Finally, the goal was to solve this problem for the least cost
possible. In graph terms, the graph would be weighted, and the
solution had to be a minimum.</p></li></ul><p>Thus Bor≈Øvka defined the Moravian Spanning Tree (MST) problem.</p>&#13;
<h4 class="heading">17.5.2¬†<a name="(part._.A_.Greedy_.Solution)"/>A Greedy Solution<a href="#(part._.A_.Greedy_.Solution)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>Bor≈Øvka had published his problem, and another Czech mathematician,
<a href="http://en.wikipedia.org/wiki/Vojt%C4%9Bch_Jarn%C3%ADk">Vojtƒõch Jarn√≠k</a>,
came across it. Jarn√≠k came up with a solution that should sound
familiar:
</p><ul><li><p>Begin with a solution consisting of a single node, chosen
arbitrarily. For the graph consisting of this one node, this
solution is clearly a minimum, spanning, and a tree.</p></li><li><p>Of all the edges incident on nodes in the solution that
connect to a node not already in the solution, pick the edge with
the least weight.Note that we consider only the
incident edges, not their weight added to the weight of the node to
which they are incident.</p></li><li><p>Add this edge to the solution. The claim is that for the new
solution will be a tree (by construction), spanning (also by
construction), and a minimum. The minimality follows by an argument
similar to that used for Dijkstra‚Äôs Algorithm.</p></li></ul><p>Jarn√≠k had the misfortune of publishing this work in Czech in 1930,
and it went largely ignored. It was rediscovered by others, most
notably by R.C. Prim in 1957, and is now generally known as
Prim‚Äôs Algorithm, though calling it Jarn√≠k‚Äôs Algorithm
would attribute credit in the right place.</p><p>Implementing this algorithm is pretty easy. At each point, we need to
know the lightest edge incident on the current solution tree. Finding
the lightest edge takes time linear in the number of these edges, but
the very lightest one may create a cycle. We therefore need to
efficiently check for whether adding an edge would create a cycle, a
problem we will return to multiple times [<a href="#%28part._union-find-functional%29" data-pltdoc="x">Checking Component Connectedness</a>].
Assuming we can do
that effectively, we then want to add the lightest edge and
iterate. Even given an efficient solution for checking cyclicity, this
would seem to require an operation linear in the number of edges for
each node. With better representations we can improve on this
complexity, but let‚Äôs look at other ideas first.</p>&#13;
<h4 class="heading">17.5.3¬†<a name="(part._.Another_.Greedy_.Solution)"/>Another Greedy Solution<a href="#(part._.Another_.Greedy_.Solution)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>Recall that Jarn√≠k presented his algorithm in 1930, when computers
didn‚Äôt exist, and Prim his in 1957, when they were very much in their
infancy. Programming computers to track heaps was a non-trivial
problem, and many algorithms were implemented by hand, where keeping
track of a complex data structure without making errors was harder
still. There was need for a solution that was required less manual
bookkeeping (literally speaking).</p><p>In 1956,
<a href="http://en.wikipedia.org/wiki/Joseph_Kruskal">Joseph Kruskal</a>
presented such a solution. His idea was elegantly simple. The Jarn√≠k
algorithm suffers from the problem that each time the tree grows, we
have to revise the content of the heap, which is already a messy
structure to track. Kruskal noted the following.</p><p>To obtain a minimum solution, surely we want to include one of the
edges of least weight in the graph. Because if not, we can take an
otherwise minimal solution, add this edge, and remove one other edge;
the graph would still be just as connected, but the overall weight
would be no more and, if the removed edge were heavier, would be
less.Note the careful wording: there may be many edges
of the same least weight, so adding one of them may remove another,
and therefore not produce a lighter tree; but the key point is that it
certainly will not produce a heavier one. By the same argument we can
add the next lightest edge, and the next lightest, and so on. The only
time we cannot add the next lightest edge is when it would create a
cycle (that problem again!).</p><p>Therefore, Kruskal‚Äôs algorithm is utterly straightforward. We first
sort all the edges, ordered by ascending weight. We then take each
edge in ascending weight order and add it to the solution provided it
will not create a cycle. When we have thus processed all the edges, we
will have a solution that is a tree (by construction), spanning
(because every connected vertex must be the endpoint of some edge),
and of minimum weight (by the argument above). The complexity is that
of sorting (which is \([e \rightarrow e \log e]\) where \(e\) is the
size of the edge set. We then iterate over each element in \(e\),
which takes time linear in the size of that set‚Äî<wbr/>modulo the time to
check for cycles. This algorithm is also easy to implement on paper,
because we sort all the edges once, then keep checking them off in
order, crossing out the ones that create cycles‚Äî<wbr/>with no dynamic
updating of the list needed.</p>&#13;
<h4 class="heading">17.5.4¬†<a name="(part._.A_.Third_.Solution)"/>A Third Solution<a href="#(part._.A_.Third_.Solution)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>Both the Jarn√≠k and Kruskal solutions have one flaw: they require a
centralized data structure (the priority heap, or the sorted list) to
incrementally build the solution. As parallel computers became
available, and graph problems grew large, computer scientists looked
for solutions that could be implemented more efficiently in
parallel‚Äî<wbr/>which typically meant avoiding any centralized points of
synchronization, such as these centralized data structures.</p><p>In 1965, M. Sollin constructed an algorithm that met these needs
beautifully. In this algorithm, instead of constructing a single
solution, we grow multiple solution components (potentially in
parallel if we so wish). Each node starts out as a solution component
(as it was at the first step of Jarn√≠k‚Äôs Algorithm). Each node
considers the edges incident to it, and picks the lightest one that
connects to a different component (that problem again!). If
such an edge can be found, the edge becomes part of the solution, and
the two components combine to become a single component. The entire
process repeats.</p><p>Because every node begins as part of the solution, this algorithm
naturally spans. Because it checks for cycles and avoids them, it
naturally forms a tree.Note that avoiding cycles yields
a DAG and is not automatically guaranteed to yield a tree. We have
been a bit lax about this difference throughout this section.
Finally, minimality follows through similar reasoning as we used in
the case of Jarn√≠k‚Äôs Algorithm, which we have essentially run in
parallel, once from each node, until the parallel solution components
join up to produce a global solution.</p><p>Of course, maintaining the data for this algorithm by hand is a
nightmare. Therefore, it would be no surprise that this algorithm was
coined in the digital age. The real surprise, therefore, is that it
was not: it was originally created by
<a href="http://en.wikipedia.org/wiki/Otakar_Bor%C5%AFvka">Otakar Bor≈Øvka</a>
himself.</p><p>Bor≈Øvka, you see, had figured it all out. He‚Äôd not only understood the
problem, he had:
</p><ul><li><p>pinpointed the real problem lying underneath the
electrification problem so it could be viewed in a
context-independent way,</p></li><li><p>created a descriptive language of graph theory to define it
precisely, and</p></li><li><p>even solved the problem in addition to defining it.</p></li></ul><p>He‚Äôd just come up with a solution so complex to implement by hand that
Jarn√≠k had in essence de-parallelized it so it could be done
sequentially. And thus this algorithm lay unnoticed until it was
reinvented
(<a href="http://en.wikipedia.org/wiki/Bor%C5%AFvka's_algorithm">several times, actually</a>)
by Sollin in time for parallel computing folks to notice a need for
it. But now we can just call this Bor≈Øvka‚Äôs Algorithm, which is
only fitting.</p><p>As you might have guessed by now, this problem is indeed called the
MST in other textbooks, but ‚ÄúM‚Äù stands not for Moravia but for
‚ÄúMinimum‚Äù. But given Bor≈Øvka‚Äôs forgotten place in history, we prefer
the more whimsical name.</p>&#13;
<h4 class="heading">17.5.5¬†<a name="(part._union-find-functional)"/>Checking Component Connectedness<a href="#(part._union-find-functional)" class="heading-anchor" title="Link to here">üîó</a> </h4><p>As we‚Äôve seen, we need to be able to efficiently tell whether two
nodes are in the same component. One way to do this is to conduct a
depth-first traversal (or breadth-first traversal) starting from the
first node and checking whether we ever visit the second one. (Using
one of these traversal strategies ensures that we terminate in the
presence of loops.) Unfortunately, this takes a linear amount of time
(in the size of the graph) for every pair of nodes‚Äî<wbr/>and
depending on the graph and choice of node, we might do this for every
node in the graph on every edge addition! So we‚Äôd clearly like to do
this better.</p><p>It is helpful to reduce this problem from graph connectivity to a more
general one: of disjoint-set structure (colloquially known as
union-find for reasons that will soon be clear). If we think of
each connected component as a set, then we‚Äôre asking whether two nodes
are in the same set. But casting it as a set membership problem makes
it applicable in several other applications as well.</p><p>The setup is as follows. For arbitrary values, we want the ability to
think of them as elements in a set.
We are interested in two operations. One is obviously <code data-lang="pyret" class="sourceCode">union</code>,
which merges two sets into one. The other would seem to be something
like <code data-lang="pyret" class="sourceCode">is-in-same-set</code> that takes two elements and determines
whether they‚Äôre in the same set. Over time, however, it has proven
useful to instead define the operator <code data-lang="pyret" class="sourceCode">find</code> that, given an
element, ‚Äúnames‚Äù the set (more on this in a moment) that the element
belongs to. To check whether two elements are in the same set, we then
have to get the ‚Äúset name‚Äù for each element, and check whether these
names are the same. This certainly sounds more roundabout, but this
means we have a primitive that may be useful in other contexts, and
from which we can easily implement <code data-lang="pyret" class="sourceCode">is-in-same-set</code>.</p><p>Now the question is, how do we name sets? The real question we should
ask is, what operations do we care to perform on these names? All we
care about is, given two names, they represent the same set precisely
when the names are the same. Therefore, we could construct a new
string, or number, or something else, but we have another option:
simply pick some element of the set to represent it, i.e., to serve as
its name. Thus we will associate each set element with an
indicator of the ‚Äúset name‚Äù for that element; if there isn‚Äôt one,
then its name is itself (the <code data-lang="pyret" class="sourceCode">none</code> case of <code data-lang="pyret" class="sourceCode">parent</code>):
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Element&lt;T&gt;:
  | elt(val :: T, parent :: Option&lt;Element&gt;)
end</code></pre><p>We will assume we have some equality predicate for checking when two
elements are the same, which we do by comparing their value parts,
ignoring their parent values:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun is-same-element(e1, e2): e1.val &lt;=&gt; e2.val end</code></pre><blockquote class="Incercise"><p class="IncerciseHeader">Do Now!</p><blockquote class="IncerciseBody"><p>Why do we check only the value parts?</p></blockquote></blockquote><p>We will assume that for a given set, we always return the
same representative element. (Otherwise, equality will fail
even though we have the same set.) Thus:We‚Äôve used the
name <code data-lang="pyret" class="sourceCode">fynd</code> because <code data-lang="pyret" class="sourceCode">find</code> is already defined to mean
something else in Pyret. If you don‚Äôt like the misspelling, you‚Äôre
welcome to use a longer name like <code data-lang="pyret" class="sourceCode">find-root</code>.
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun is-in-same-set(e1 :: Element, e2 :: Element, s :: Sets)
    -&gt; Boolean:
  s1 = fynd(e1, s)
  s2 = fynd(e2, s)
  identical(s1, s2)
end</code></pre><p>where <code data-lang="pyret" class="sourceCode">Sets</code> is the list of all elements:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">type Sets = List&lt;Element&gt;</code></pre><p>How do we find the representative element for a set? We first find it
using <code data-lang="pyret" class="sourceCode">is-same-element</code>; when we do, we check the
element‚Äôs <code data-lang="pyret" class="sourceCode">parent</code> field. If it is <code data-lang="pyret" class="sourceCode">none</code>, that means this
very element names its set; this can happen either because the element
is a singleton set (we‚Äôll initialize all elements with <code data-lang="pyret" class="sourceCode">none</code>),
or it‚Äôs the name for some larger set. Either way, we‚Äôre
done. Otherwise, we have to recursively find the parent:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun fynd(e :: Element, s :: Sets) -&gt; Element:
  cases (List) s:
    | empty =&gt; raise("fynd: shouldn't have gotten here")
    | link(f, r) =&gt;
      if is-same-element(f, e):
        cases (Option) f.parent:
          | none =&gt; f
          | some(p) =&gt; fynd(p, s)
        end
      else:
        fynd(e, r)
      end
  end
end</code></pre><blockquote class="Exercise"><p class="ExerciseHeader">Exercise</p><blockquote class="ExerciseBody"><p>Why is there a recursive call in the nested <code data-lang="pyret" class="sourceCode">cases</code>?</p></blockquote></blockquote><p>What‚Äôs left is to implement <code data-lang="pyret" class="sourceCode">union</code>. For this, we find the
representative elements of the two sets we‚Äôre trying to union; if they
are the same, then the two sets are already in a union; otherwise, we
have to update the data structure:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun union(e1 :: Element, e2 :: Element, s :: Sets) -&gt; Sets:
  s1 = fynd(e1, s)
  s2 = fynd(e2, s)
  if identical(s1, s2):
    s
  else:
    update-set-with(s, s1, s2)
  end
end</code></pre><p>To update, we arbitrarily choose one of the set names to be the name
of the new compound set. We then have to update the parent of the
other set‚Äôs name element to be this one:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun update-set-with(s :: Sets, child :: Element, parent :: Element)
    -&gt; Sets:
  cases (List) s:
    | empty =&gt; raise("update: shouldn't have gotten here")
    | link(f, r) =&gt;
      if is-same-element(f, child):
        link(elt(f.val, some(parent)), r)
      else:
        link(f, update-set-with(r, child, parent))
      end
  end
end</code></pre><p>Here are some tests to illustrate this working:
</p><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">check:
  s0 = map(elt(_, none), [list: 0, 1, 2, 3, 4, 5, 6, 7])
  s1 = union(get(s0, 0), get(s0, 2), s0)
  s2 = union(get(s1, 0), get(s1, 3), s1)
  s3 = union(get(s2, 3), get(s2, 5), s2)
  print(s3)
  is-same-element(fynd(get(s0, 0), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 2), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 3), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 5), s3), fynd(get(s0, 5), s3)) is true
  is-same-element(fynd(get(s0, 7), s3), fynd(get(s0, 7), s3)) is true
end</code></pre><p>Unfortunately, this implementation suffers from two major problems:
</p><ul><li><p>First, because we are performing functional updates, the value
of the <code data-lang="pyret" class="sourceCode">parent</code> reference keeps ‚Äúchanging‚Äù, but these changes
are not visible to older copies of the ‚Äúsame‚Äù value. An element
from different stages of unioning has different parent references,
even though it is arguably the same element throughout. This is a
place where functional programming hurts.</p></li><li><p>Relatedly, the performance of this implementation is quite
bad. <code data-lang="pyret" class="sourceCode">fynd</code> recursively traverses parents to find the set‚Äôs
name, but the elements traversed are not updated to record this new
name. We certainly could update them by reconstructing the set
afresh each time, but that complicates the implementation and, as we
will soon see, we can do much better.</p></li></ul><p>Even worse, it may not even be correct!</p><blockquote class="Exercise"><p class="ExerciseHeader">Exercise</p><blockquote class="ExerciseBody"><p>Is it? Consider constructing <code data-lang="pyret" class="sourceCode">union</code>s that are not quite so skewed as
above, and see whether you get the results you expect.</p></blockquote></blockquote><p>The bottom line is that pure functional programming is not a
great fit with this problem. We need a better implementation
strategy: <a href="union-find.html" data-pltdoc="x">Union-Find</a>.</p>    
</body>
</html>