- en: 18.4¬†Hashes, Sets, and Key-Valuesüîó
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://dcic-world.org/2025-08-27/hash-set-kv.html](https://dcic-world.org/2025-08-27/hash-set-kv.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| ¬†¬†¬†¬†[18.4.1¬†A Hash Function for Strings](#%28part._hash-string%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.2¬†Sets from Hashing](#%28part._.Sets_from_.Hashing%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.3¬†Arrays](#%28part._.Arrays%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.4¬†Sets from Hashing and Arrays](#%28part._hash-tables%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.5¬†Collisions](#%28part._.Collisions%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.6¬†Resolving Collisions](#%28part._.Resolving_.Collisions%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.7¬†Complexity](#%28part._hash-comp%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.8¬†Bloom Filters](#%28part._bloom-filters%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[18.4.9¬†Generalizing from Sets to Key-Values](#%28part._.Generalizing_from_.Sets_to_.Key-.Values%29)
    |'
  prefs: []
  type: TYPE_TB
- en: 'We have seen several solutions to set membership [[Several Variations on Sets](part_sets.html)].
    In particular, trees [[Making Sets Grow on Trees](sets-from-trees.html)] gave
    us logarithmic complexity for insection and membership. Now we will see one more
    implementation of sets, with different complexity. To set this up, we assume you
    are familiar with the concept of hashing [[Converting Values to Ordered Values](orderability.html#%28part._hashing-values%29)],
    which we saw was useful for constructing search trees. Here, we will use it to
    construct sets in a very different way. We will then generalize sets to another
    important data structure: key-value repositories. But first‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.1¬†A Hash Function for Strings[üîó](#(part._hash-string) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we have seen in [Converting Values to Ordered Values](orderability.html#%28part._hashing-values%29),
    we have multiple strategies for converting arbitrary values into numbers, which
    we will rely on here. Therefore, we could write this material around numbers alone.
    To make the examples more interesting, and to better illustrate some real-world
    issues, we will instead use strings. To hash them, we will use `hash-of`, defined
    there, which simply adds up a string‚Äôs code points.
  prefs: []
  type: TYPE_NORMAL
- en: We use this function for multiple reasons. First, it is sufficient to illustrate
    some of the consequences of hashing. Second, in practice, when built-in hashing
    does not suffice, we do write (more complex versions of) functions like it. And
    finally, because it‚Äôs all laid bare, it‚Äôs easy for us to experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.2¬†Sets from Hashing[üîó](#(part._.Sets_from_.Hashing) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we are given a set of strings. We can hash each element of that set.
    Each string is now mapped to a number. Each of these numbers is a member of the
    set; every other number is not a member of this set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a simple representation is to just store this list of numbers. For
    instance, we can store the list `[list: "Hello", "World!", "üè¥‚Äç‚ò†Ô∏è"] as [list: 500,
    553, 195692]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this does not help very much. Insertion can be done in constant
    time, but checking membership requires us to traverse the entire list, which takes
    linear time in the worst case. Alternatively, maybe we have some clever scheme
    that involves sorting the list. But note:'
  prefs: []
  type: TYPE_NORMAL
- en: inserting the element can now take as much as linear time; or,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we store the elements as a tree instead of a list, but then
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have to make sure the tree is balanced, so
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: we will have essentially reconstructed the BBST.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In other words, we are recapitulating the discussion from [Representing Sets
    as Lists](sets-from-lists.html) and [Making Sets Grow on Trees](sets-from-trees.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the problem here is traversal: if we have to visit more than a
    constant number of elements, we have probably not improved anything over the BBST.
    So, given a hash, how can we perform only a constant amount of work? For that,
    lists and trees don‚Äôt work: they both require at least some amount of (non-constant)
    traversal to get to an arbitrary element. Instead we need a different data structure‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.3¬†Arrays[üîó](#(part._.Arrays) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Arrays are another linear data structure, like lists. There are two key differences
    between lists and arrays that reflect each one‚Äôs strength and weakness.
  prefs: []
  type: TYPE_NORMAL
- en: The main benefit to arrays is that we can access any element in the array in
    constant time. This is in contrast to lists where, to get to the \(n\)th element,
    we have to first traverse the previous \(n-1\) elements (using successive `rest`s).
  prefs: []
  type: TYPE_NORMAL
- en: However, this benefit comes at a cost. The reason arrays can support constant-time
    access is because the size of an array is fixed at creation time. Thus, while
    we can keep extending a list using link, we cannot grow the size of an array ‚Äúin
    place‚Äù; rather, we must make a new array and copy the entire array‚Äôs content into
    the new array, which takes linear time. (We can do a better job of this by using
    [Halloween Analysis](amortized-analysis.html), but there is no real free ride.)
  prefs: []
  type: TYPE_NORMAL
- en: The arrays in Pyret are [documented here](https://www.pyret.org/docs/latest/arrays.html).
    While not necessary in principle, it is conventional to think of arrays as data
    structures that support mutation, and that is how we will use them here.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.4¬†Sets from Hashing and Arrays[üîó](#(part._hash-tables) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Okay, so now we have a strategy. When we want to insert a string into the set,
    we compute its hash, go to the corresponding location in the array, and record
    the presence of that string. If we want to check for membership, we similarly
    compute its hash and see whether the corresponding location has been set. Traditionally,
    each location in the array is called a bucket, and this data structure is called
    a hashtable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Observe that if this were to work, we would have constant time insertion and
    membership checking. Unfortunately, two things make this plan untenable in general.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.5¬†Collisions[üîó](#(part._.Collisions) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, our choice of hash function. For the above scheme to work, two different
    strings have to map to two different locations.
  prefs: []
  type: TYPE_NORMAL
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is the above hash function invertible?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We just need to find two strings that have the same hash. Given the definition
    of `hash-of`, it‚Äôs easy to see that any rearrangement of the letters produces
    the same hash:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, this test suite passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When multiple values hash to the same location, we call this a hash collision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hash-collisions are problematic! With the above hash function, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: where two of these tests are desirable but the third is definitely not.
  prefs: []
  type: TYPE_NORMAL
- en: Note that collisions are virtually inevitable. If we have uniformly distributed
    data, then collisions show up sooner than we might expect.This follows from the
    reasoning behind what is known as the [birthday problem](http://en.wikipedia.org/wiki/Birthday_problem),
    commonly presented as how many people need to be in a room before the likelihood
    that two of them share a birthday exceeds some percentage. For the likelihood
    to exceed half we need just 23 people! Therefore, it is wise to prepare for the
    possibility of collisions.
  prefs: []
  type: TYPE_NORMAL
- en: The key is to know something about the distribution of hash values. For instance,
    if we knew our hash values are all multiples of 10, then using a table size of
    10 would be a terrible idea (because all elements would hash to the same bucket,
    turning our hash table into a list). In practice, it is common to use uncommon
    prime numbers as the table size, since a random value is unlikely to have it as
    a divisor. This does not yield a theoretical improvement (unless you can make
    certain assumptions about the input, or work through the math very carefully),
    but it works well in practice.In particular, since the typical hashing function
    uses memory addresses for objects on the heap, and on most systems these addresses
    are multiples of 4, using a prime like 31 is often a fairly good bet.
  prefs: []
  type: TYPE_NORMAL
- en: While collisions are probabilistic, and depend on the choice of hash function,
    we have an even more fundamental and unavoidable reason for collisions. We have
    to store an array of the largest possible hash size. However, not only can hash
    values be very large (try to run `insert("üè¥‚Äç‚ò†Ô∏è")` and see what happens), there
    isn‚Äôt even an a priori limit to the size of a hash. This fundamentally flies in
    the face of arrays, which must have a fixed size.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle arbitrarily large values, we:'
  prefs: []
  type: TYPE_NORMAL
- en: use an array size that is reasonable given our memory constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the remainder of the hash relative to the array‚Äôs size to find the bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This addresses the second problem: we can also store the pirate flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe, however, we have simply created yet another source of collisions:
    the remainder computation. If we have 10 buckets, then the hashes 5, 15, 25, 35,
    ‚Ä¶ all refer to the same bucket. Thus, there are two sources of collision, and
    we have to deal with them both.'
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.6¬†Resolving Collisions[üîó](#(part._.Resolving_.Collisions) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Surprisingly or disappointingly, we have a very simple solution to the collision
    problems. Each bucket is not a single Boolean value, but rather a list of the
    actual values that hashed to that bucket. Then, we just check for membership in
    that list.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will abstract over finding the bucket number in `insert` and `is-in`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we change what is held in each bucket: not a Boolean, but rather a list
    of the actual strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write the more nuanced membership checker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, when inserting, we first make sure the element isn‚Äôt already there
    (to avoid the complexity problems caused by having duplicates), and only then
    insert it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our tests pass as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 18.4.7¬†Complexity[üîó](#(part._hash-comp) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now we have yet another working implementation for (some primitives of) sets.
    The use of arrays supposedly enables us to get constant-time complexity. Yet we
    should feel at least some discomfort. After all, the constant time applied when
    the arrays contained only Boolean values. However, that solution was weak in two
    ways: it could not handle hash-collisions by non-invertible hash functions, and
    it required potentially enormous arrays. If we relaxed either assumption, the
    implementation was simply wrong, in that it was easily fooled by values that caused
    collisions either through hashing or through computing the remainder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution we have shown above is called hash chaining, where ‚Äúchain‚Äù refers
    to the list stored in each bucket. The benefit of hash-chaining is that insertion
    can still be constant-time: it takes a constant amount of time to find a bucket,
    and inserting can be as cheap as link. Of course, this assumes that we don‚Äôt mind
    duplicates; otherwise we will pay the same price we saw earlier in [Representing
    Sets as Lists](sets-from-lists.html). But lookup takes time linear in the size
    of the bucket (which, with duplicates, could be arbitrarily larger relative to
    the number of distinct elements). And even if we check for duplicates, we run
    the risk that most or even all the elements could end up in the same bucket (e.g.,
    suppose the elements are `"Where"`, `"Weird"`, `"Wired"`, `"Whine"`). In that
    case, our sophisticated implementation reduces to the list-based representation
    and its complexity!'
  prefs: []
  type: TYPE_NORMAL
- en: There‚Äôs an additional subtlety here. When we check membership of the string
    in the list of strings, we have to consider the cost of comparing each pair of
    strings. In the worst case, that is proportional to the length of the shorter
    string. Usually this is bounded by a small constant, but one can imagine settings
    where this is not guaranteed to be true. However, this same cost has to be borne
    by all set implementations; it is not a new complexity introduced here.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in theory, hash-based sets can support insertion and membership in as
    little as constant time, and (ignoring the cost of string comparisons) as much
    as linear time, where ‚Äúlinear‚Äù has the same caveats about duplicates as the list-based
    representation. In many cases‚Äî<wbr>depending on the nature of the data and parameters
    set for the array‚Äî<wbr>they can be much closer to constant time. As a result,
    they tend to be very popular in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.8¬†Bloom Filters[üîó](#(part._bloom-filters) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to improve the space and time complexity is to relax the properties
    we expect of the operations. Right now, set membership gives perfect answers,
    in that it answers `true` exactly when the element being checked was previously
    inserted into the set. But suppose we‚Äôre in a setting where we can accept a more
    relaxed notion of correctness, where membership tests can ‚Äúlie‚Äù slightly in one
    direction or the other (but not both, because that makes the representation almost
    useless). Specifically, let‚Äôs say that ‚Äúno means no‚Äù (i.e., if the set representation
    says the element isn‚Äôt present, it really isn‚Äôt) but ‚Äúyes sometimes means no‚Äù
    (i.e., if the set representation says an element is present, sometimes it might
    not be). In short, if the set says the element isn‚Äôt in it, this should be guaranteed;
    but if the set says the element is present, it may not be. In the latter case,
    we either need some other‚Äî<wbr>more expensive‚Äî<wbr>technique to determine truth,
    or we might just not care.
  prefs: []
  type: TYPE_NORMAL
- en: Where is such a data structure of use? Suppose we are building a Web site that
    uses password-based authentication. Because many passwords have been leaked in
    well-publicized breaches, it is safe to assume that hackers have them and will
    guess them. As a result, we want to not allow users to select any of these as
    passwords. We could use a hash-table to reject precisely the known leaked passwords.
    But for efficiency, we could use this imperfect hash instead. If it says ‚Äúno‚Äù,
    then we allow the user to use that password. But if it says ‚Äúyes‚Äù, then either
    they are using a password that has been leaked, or they have an entirely different
    password that, purely by accident, has the same hash value, but no matter; we
    can just disallow that password as well.A related use is for filtering out malicious
    Web sites. The URL shortening system, bitly, [uses it for this purpose](http://word.bitly.com/post/28558800777/dablooms-an-open-source-scalable-counting-bloom).
    It‚Äôs also used by ad networks; here‚Äôs a [talk](https://youtu.be/T3Bt9Tn6P5c?si=t8U33orccRCgkSw0&t=1277)
    (the segment from about 20m to about 45m) about that. But sometimes, a Bloom filter
    is overkill, as this Cloudflare blog post [discusses](https://blog.cloudflare.com/when-bloom-filters-dont-bloom/)‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is in updating databases or memory stores. Suppose we have
    a database of records, which we update frequently. It is often more efficient
    to maintain a journal of changes: i.e., a list that sequentially records all the
    changes that have occurred. At some interval (say overnight), the journal is ‚Äúflushed‚Äù,
    meaning all these changes are applied to the database proper. But that means every
    read operation has become highly inefficient, because it has to check the entire
    journal first (for updates) before accessing the database. Again, here we can
    use this faulty notion of a hash table: if the hash of the record locator says
    ‚Äúno‚Äù, then the record certainly hasn‚Äôt been modified and we go directly to the
    database; if it says ‚Äúyes‚Äù then we have to check the journal.'
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen a simple example implementation of this idea earlier, when
    we used a single array, with modular arithmetic, to represent the set. When an
    element was not present in the array, we knew for a fact that it was definitely
    not present. When the array indicated an element was present, we couldn‚Äôt be sure
    that what was present was the exact value we were looking for. To get around this
    uncertainty, we used chaining.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is something else we could have done. Chaining costs both space
    (to store all the actual values) and time (to look through all the values). Suppose,
    instead, a bucket is only a Boolean value. This results in a slightly useful,
    but potentially very inaccurate, data structure; furthermore, it exhibits correlated
    failure tied to the modulus.
  prefs: []
  type: TYPE_NORMAL
- en: But suppose we have not only one array, but several! When an element is added
    to the set, it is added to each array; when checking for membership, every array
    is consulted. The set only answers affirmatively to membership if all the arrays
    do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, using multiple arrays offers absolutely no advantage if the arrays
    are all the same size: since both insertion and lookup are deterministic, all
    will yield the same answer. However, there is a simple antidote to this: use different
    array sizes. In particular, by using array sizes that are relatively prime to
    one another, we minimize the odds of a clash (only hashes that are the product
    of all the array sizes will fool the array).'
  prefs: []
  type: TYPE_NORMAL
- en: This data structure, called a Bloom Filter, is a probabilistic data structure.
    Unlike our earlier set data structure, this one is not guaranteed to always give
    the right answer; but contrary to the [‚òõ space-time tradeoff](glossary.html#%28elem._glossary-space-time._tradeoff%29),
    we save both space and time by changing the problem slightly to accept incorrect
    answers. If we know something about the distribution of hash values, and we have
    some acceptable bound of error, we can design hash table sizes so that with high
    probability, the Bloom Filter will lie within the acceptable error bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.9¬†Generalizing from Sets to Key-Values[üîó](#(part._.Generalizing_from_.Sets_to_.Key-.Values)
    "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Above, we focused on sets: that is, a string effectively mapped to a Boolean
    value, indicating whether it was present or not. However, there are many settings
    where it is valuable to associate one value with another. For instance, given
    an identity number we might want to pull up a person‚Äôs records; given a computer‚Äôs
    name, we might want to retrieve its routing information; given a star‚Äôs catalog
    entry, we might want its astronomical information. This kind of data structure
    is so ubiquitous that it has several names, some of which are more general and
    some implying specific implementations: key-value store, associative array, hash
    map, dictionary, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the names ‚Äúkey-value‚Äù and ‚Äúdictionary‚Äù are useful because they suggest
    a behavioral interface. In contrast, associative array implies the use of arrays,
    and hash table suggests the use of an array (and of hashing). In fact, real systems
    use a variety of implementation strategies, including balanced binary search trees.
    The names ‚Äúkey-value‚Äù and ‚Äúdictionary‚Äù avoid commitment to a particular implementation.
    Here, too, ‚Äúdictionary‚Äù evokes a common mental image of unique words that map
    to descriptions. The term ‚Äúkey value‚Äù is even more technically useful because
    keys are meant to all be distinct (i.e., no two different key-value pairs can
    have the same key; alternatively, one key can map to only one value). This makes
    sense because we view this as a generalization of sets, so the keys are the set
    elements, which must necessarily have no duplicates; the values take the place
    of the Boolean.
  prefs: []
  type: TYPE_NORMAL
- en: 'To extend our set representation to handle a dictionary or key-value store,
    we need to make a few changes. First, we introduce the key-value representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Each bucket is still an empty list, but we understand it to be a list of key-value
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we only had `is-in` to check whether an element was present in a
    set or not. That element is now the key, and we could have a similar function
    to check whether the key is present. However, we rarely want to know just that;
    in fact, because we already know the key, we usually want the associated value.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can just have this one function:We use Pyret‚Äôs naming convention
    of `-now` to indicate that this result might change later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, `getkv-now` may fail: the key may not be present. That is, it has
    become a partial function [[Partial Domains](partial-domains.html)]. We therefore
    have all the usual strategies for dealing with partial functions. Here, for simplicity
    we choose to return an error if the key is not present, but all the other strategies
    we discuss for handling partiality are valid (and often better in a robust implementation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the generalization of `insert`. However, `insert` had no reason to
    return an error: inserting an element twice was harmless. However, because keys
    must now be associated with only one value, insertion has to check whether the
    key is already present, and signal an error otherwise. In short, it is also partial.This
    is not partial due to a mathematical reason, but rather because of state: the
    same key may have been inserted previously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have agreed on this interface, getting a value is a natural extension
    of checking for membership:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Having found the index, we look in the bucket for whether any key-value pair
    has the desired key. If it does, then we return the corresponding value. Otherwise,
    we error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inserting a key-value pair similarly generalizes adding an element to the set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we check the bucket for whether the key is already present. If it
    is, we choose to halt with an error. Otherwise, we make the key-value pair and
    link it to the existing bucket contents, and modify the array to refer to the
    new list.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do the above pair of functions do all the necessary error-checking?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Above, `setkv-now` raises an error if a key already has a name associated with
    it. A natural variation is to instead override the associated value, so that the
    new value is now associated with that key. Modify the implementation to do that
    instead, and make sure you test it thoroughly! Note that you may need to modify
    the `KV` datatype also.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This concludes our brief tour of sets (yet again!) and key-value stores or
    dictionaries. We have chosen to implement both using arrays, which required us
    to employ hashes. For more on string dictionaries, see the [Pyret documentation](https://www.pyret.org/docs/latest/string-dict.html).
    Observe that Pyret offers two kinds of dictionaries: one mutable (like we have
    shown here) and one (the default) functional.'
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.1¬†A Hash Function for Strings[üîó](#(part._hash-string) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we have seen in [Converting Values to Ordered Values](orderability.html#%28part._hashing-values%29),
    we have multiple strategies for converting arbitrary values into numbers, which
    we will rely on here. Therefore, we could write this material around numbers alone.
    To make the examples more interesting, and to better illustrate some real-world
    issues, we will instead use strings. To hash them, we will use `hash-of`, defined
    there, which simply adds up a string‚Äôs code points.
  prefs: []
  type: TYPE_NORMAL
- en: We use this function for multiple reasons. First, it is sufficient to illustrate
    some of the consequences of hashing. Second, in practice, when built-in hashing
    does not suffice, we do write (more complex versions of) functions like it. And
    finally, because it‚Äôs all laid bare, it‚Äôs easy for us to experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.2¬†Sets from Hashing[üîó](#(part._.Sets_from_.Hashing) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we are given a set of strings. We can hash each element of that set.
    Each string is now mapped to a number. Each of these numbers is a member of the
    set; every other number is not a member of this set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a simple representation is to just store this list of numbers. For
    instance, we can store the list `[list: "Hello", "World!", "üè¥‚Äç‚ò†Ô∏è"] as [list: 500,
    553, 195692]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this does not help very much. Insertion can be done in constant
    time, but checking membership requires us to traverse the entire list, which takes
    linear time in the worst case. Alternatively, maybe we have some clever scheme
    that involves sorting the list. But note:'
  prefs: []
  type: TYPE_NORMAL
- en: inserting the element can now take as much as linear time; or,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we store the elements as a tree instead of a list, but then
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have to make sure the tree is balanced, so
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: we will have essentially reconstructed the BBST.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In other words, we are recapitulating the discussion from [Representing Sets
    as Lists](sets-from-lists.html) and [Making Sets Grow on Trees](sets-from-trees.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the problem here is traversal: if we have to visit more than a
    constant number of elements, we have probably not improved anything over the BBST.
    So, given a hash, how can we perform only a constant amount of work? For that,
    lists and trees don‚Äôt work: they both require at least some amount of (non-constant)
    traversal to get to an arbitrary element. Instead we need a different data structure‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.3¬†Arrays[üîó](#(part._.Arrays) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Arrays are another linear data structure, like lists. There are two key differences
    between lists and arrays that reflect each one‚Äôs strength and weakness.
  prefs: []
  type: TYPE_NORMAL
- en: The main benefit to arrays is that we can access any element in the array in
    constant time. This is in contrast to lists where, to get to the \(n\)th element,
    we have to first traverse the previous \(n-1\) elements (using successive `rest`s).
  prefs: []
  type: TYPE_NORMAL
- en: However, this benefit comes at a cost. The reason arrays can support constant-time
    access is because the size of an array is fixed at creation time. Thus, while
    we can keep extending a list using link, we cannot grow the size of an array ‚Äúin
    place‚Äù; rather, we must make a new array and copy the entire array‚Äôs content into
    the new array, which takes linear time. (We can do a better job of this by using
    [Halloween Analysis](amortized-analysis.html), but there is no real free ride.)
  prefs: []
  type: TYPE_NORMAL
- en: The arrays in Pyret are [documented here](https://www.pyret.org/docs/latest/arrays.html).
    While not necessary in principle, it is conventional to think of arrays as data
    structures that support mutation, and that is how we will use them here.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.4¬†Sets from Hashing and Arrays[üîó](#(part._hash-tables) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Okay, so now we have a strategy. When we want to insert a string into the set,
    we compute its hash, go to the corresponding location in the array, and record
    the presence of that string. If we want to check for membership, we similarly
    compute its hash and see whether the corresponding location has been set. Traditionally,
    each location in the array is called a bucket, and this data structure is called
    a hashtable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Observe that if this were to work, we would have constant time insertion and
    membership checking. Unfortunately, two things make this plan untenable in general.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.5¬†Collisions[üîó](#(part._.Collisions) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, our choice of hash function. For the above scheme to work, two different
    strings have to map to two different locations.
  prefs: []
  type: TYPE_NORMAL
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is the above hash function invertible?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We just need to find two strings that have the same hash. Given the definition
    of `hash-of`, it‚Äôs easy to see that any rearrangement of the letters produces
    the same hash:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, this test suite passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: When multiple values hash to the same location, we call this a hash collision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hash-collisions are problematic! With the above hash function, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: where two of these tests are desirable but the third is definitely not.
  prefs: []
  type: TYPE_NORMAL
- en: Note that collisions are virtually inevitable. If we have uniformly distributed
    data, then collisions show up sooner than we might expect.This follows from the
    reasoning behind what is known as the [birthday problem](http://en.wikipedia.org/wiki/Birthday_problem),
    commonly presented as how many people need to be in a room before the likelihood
    that two of them share a birthday exceeds some percentage. For the likelihood
    to exceed half we need just 23 people! Therefore, it is wise to prepare for the
    possibility of collisions.
  prefs: []
  type: TYPE_NORMAL
- en: The key is to know something about the distribution of hash values. For instance,
    if we knew our hash values are all multiples of 10, then using a table size of
    10 would be a terrible idea (because all elements would hash to the same bucket,
    turning our hash table into a list). In practice, it is common to use uncommon
    prime numbers as the table size, since a random value is unlikely to have it as
    a divisor. This does not yield a theoretical improvement (unless you can make
    certain assumptions about the input, or work through the math very carefully),
    but it works well in practice.In particular, since the typical hashing function
    uses memory addresses for objects on the heap, and on most systems these addresses
    are multiples of 4, using a prime like 31 is often a fairly good bet.
  prefs: []
  type: TYPE_NORMAL
- en: While collisions are probabilistic, and depend on the choice of hash function,
    we have an even more fundamental and unavoidable reason for collisions. We have
    to store an array of the largest possible hash size. However, not only can hash
    values be very large (try to run `insert("üè¥‚Äç‚ò†Ô∏è")` and see what happens), there
    isn‚Äôt even an a priori limit to the size of a hash. This fundamentally flies in
    the face of arrays, which must have a fixed size.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle arbitrarily large values, we:'
  prefs: []
  type: TYPE_NORMAL
- en: use an array size that is reasonable given our memory constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the remainder of the hash relative to the array‚Äôs size to find the bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This addresses the second problem: we can also store the pirate flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe, however, we have simply created yet another source of collisions:
    the remainder computation. If we have 10 buckets, then the hashes 5, 15, 25, 35,
    ‚Ä¶ all refer to the same bucket. Thus, there are two sources of collision, and
    we have to deal with them both.'
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.6¬†Resolving Collisions[üîó](#(part._.Resolving_.Collisions) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Surprisingly or disappointingly, we have a very simple solution to the collision
    problems. Each bucket is not a single Boolean value, but rather a list of the
    actual values that hashed to that bucket. Then, we just check for membership in
    that list.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will abstract over finding the bucket number in `insert` and `is-in`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we change what is held in each bucket: not a Boolean, but rather a list
    of the actual strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write the more nuanced membership checker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, when inserting, we first make sure the element isn‚Äôt already there
    (to avoid the complexity problems caused by having duplicates), and only then
    insert it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our tests pass as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 18.4.7¬†Complexity[üîó](#(part._hash-comp) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now we have yet another working implementation for (some primitives of) sets.
    The use of arrays supposedly enables us to get constant-time complexity. Yet we
    should feel at least some discomfort. After all, the constant time applied when
    the arrays contained only Boolean values. However, that solution was weak in two
    ways: it could not handle hash-collisions by non-invertible hash functions, and
    it required potentially enormous arrays. If we relaxed either assumption, the
    implementation was simply wrong, in that it was easily fooled by values that caused
    collisions either through hashing or through computing the remainder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution we have shown above is called hash chaining, where ‚Äúchain‚Äù refers
    to the list stored in each bucket. The benefit of hash-chaining is that insertion
    can still be constant-time: it takes a constant amount of time to find a bucket,
    and inserting can be as cheap as link. Of course, this assumes that we don‚Äôt mind
    duplicates; otherwise we will pay the same price we saw earlier in [Representing
    Sets as Lists](sets-from-lists.html). But lookup takes time linear in the size
    of the bucket (which, with duplicates, could be arbitrarily larger relative to
    the number of distinct elements). And even if we check for duplicates, we run
    the risk that most or even all the elements could end up in the same bucket (e.g.,
    suppose the elements are `"Where"`, `"Weird"`, `"Wired"`, `"Whine"`). In that
    case, our sophisticated implementation reduces to the list-based representation
    and its complexity!'
  prefs: []
  type: TYPE_NORMAL
- en: There‚Äôs an additional subtlety here. When we check membership of the string
    in the list of strings, we have to consider the cost of comparing each pair of
    strings. In the worst case, that is proportional to the length of the shorter
    string. Usually this is bounded by a small constant, but one can imagine settings
    where this is not guaranteed to be true. However, this same cost has to be borne
    by all set implementations; it is not a new complexity introduced here.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in theory, hash-based sets can support insertion and membership in as
    little as constant time, and (ignoring the cost of string comparisons) as much
    as linear time, where ‚Äúlinear‚Äù has the same caveats about duplicates as the list-based
    representation. In many cases‚Äî<wbr>depending on the nature of the data and parameters
    set for the array‚Äî<wbr>they can be much closer to constant time. As a result,
    they tend to be very popular in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.8¬†Bloom Filters[üîó](#(part._bloom-filters) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to improve the space and time complexity is to relax the properties
    we expect of the operations. Right now, set membership gives perfect answers,
    in that it answers `true` exactly when the element being checked was previously
    inserted into the set. But suppose we‚Äôre in a setting where we can accept a more
    relaxed notion of correctness, where membership tests can ‚Äúlie‚Äù slightly in one
    direction or the other (but not both, because that makes the representation almost
    useless). Specifically, let‚Äôs say that ‚Äúno means no‚Äù (i.e., if the set representation
    says the element isn‚Äôt present, it really isn‚Äôt) but ‚Äúyes sometimes means no‚Äù
    (i.e., if the set representation says an element is present, sometimes it might
    not be). In short, if the set says the element isn‚Äôt in it, this should be guaranteed;
    but if the set says the element is present, it may not be. In the latter case,
    we either need some other‚Äî<wbr>more expensive‚Äî<wbr>technique to determine truth,
    or we might just not care.
  prefs: []
  type: TYPE_NORMAL
- en: Where is such a data structure of use? Suppose we are building a Web site that
    uses password-based authentication. Because many passwords have been leaked in
    well-publicized breaches, it is safe to assume that hackers have them and will
    guess them. As a result, we want to not allow users to select any of these as
    passwords. We could use a hash-table to reject precisely the known leaked passwords.
    But for efficiency, we could use this imperfect hash instead. If it says ‚Äúno‚Äù,
    then we allow the user to use that password. But if it says ‚Äúyes‚Äù, then either
    they are using a password that has been leaked, or they have an entirely different
    password that, purely by accident, has the same hash value, but no matter; we
    can just disallow that password as well.A related use is for filtering out malicious
    Web sites. The URL shortening system, bitly, [uses it for this purpose](http://word.bitly.com/post/28558800777/dablooms-an-open-source-scalable-counting-bloom).
    It‚Äôs also used by ad networks; here‚Äôs a [talk](https://youtu.be/T3Bt9Tn6P5c?si=t8U33orccRCgkSw0&t=1277)
    (the segment from about 20m to about 45m) about that. But sometimes, a Bloom filter
    is overkill, as this Cloudflare blog post [discusses](https://blog.cloudflare.com/when-bloom-filters-dont-bloom/)‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is in updating databases or memory stores. Suppose we have
    a database of records, which we update frequently. It is often more efficient
    to maintain a journal of changes: i.e., a list that sequentially records all the
    changes that have occurred. At some interval (say overnight), the journal is ‚Äúflushed‚Äù,
    meaning all these changes are applied to the database proper. But that means every
    read operation has become highly inefficient, because it has to check the entire
    journal first (for updates) before accessing the database. Again, here we can
    use this faulty notion of a hash table: if the hash of the record locator says
    ‚Äúno‚Äù, then the record certainly hasn‚Äôt been modified and we go directly to the
    database; if it says ‚Äúyes‚Äù then we have to check the journal.'
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen a simple example implementation of this idea earlier, when
    we used a single array, with modular arithmetic, to represent the set. When an
    element was not present in the array, we knew for a fact that it was definitely
    not present. When the array indicated an element was present, we couldn‚Äôt be sure
    that what was present was the exact value we were looking for. To get around this
    uncertainty, we used chaining.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is something else we could have done. Chaining costs both space
    (to store all the actual values) and time (to look through all the values). Suppose,
    instead, a bucket is only a Boolean value. This results in a slightly useful,
    but potentially very inaccurate, data structure; furthermore, it exhibits correlated
    failure tied to the modulus.
  prefs: []
  type: TYPE_NORMAL
- en: But suppose we have not only one array, but several! When an element is added
    to the set, it is added to each array; when checking for membership, every array
    is consulted. The set only answers affirmatively to membership if all the arrays
    do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, using multiple arrays offers absolutely no advantage if the arrays
    are all the same size: since both insertion and lookup are deterministic, all
    will yield the same answer. However, there is a simple antidote to this: use different
    array sizes. In particular, by using array sizes that are relatively prime to
    one another, we minimize the odds of a clash (only hashes that are the product
    of all the array sizes will fool the array).'
  prefs: []
  type: TYPE_NORMAL
- en: This data structure, called a Bloom Filter, is a probabilistic data structure.
    Unlike our earlier set data structure, this one is not guaranteed to always give
    the right answer; but contrary to the [‚òõ space-time tradeoff](glossary.html#%28elem._glossary-space-time._tradeoff%29),
    we save both space and time by changing the problem slightly to accept incorrect
    answers. If we know something about the distribution of hash values, and we have
    some acceptable bound of error, we can design hash table sizes so that with high
    probability, the Bloom Filter will lie within the acceptable error bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4.9¬†Generalizing from Sets to Key-Values[üîó](#(part._.Generalizing_from_.Sets_to_.Key-.Values)
    "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Above, we focused on sets: that is, a string effectively mapped to a Boolean
    value, indicating whether it was present or not. However, there are many settings
    where it is valuable to associate one value with another. For instance, given
    an identity number we might want to pull up a person‚Äôs records; given a computer‚Äôs
    name, we might want to retrieve its routing information; given a star‚Äôs catalog
    entry, we might want its astronomical information. This kind of data structure
    is so ubiquitous that it has several names, some of which are more general and
    some implying specific implementations: key-value store, associative array, hash
    map, dictionary, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the names ‚Äúkey-value‚Äù and ‚Äúdictionary‚Äù are useful because they suggest
    a behavioral interface. In contrast, associative array implies the use of arrays,
    and hash table suggests the use of an array (and of hashing). In fact, real systems
    use a variety of implementation strategies, including balanced binary search trees.
    The names ‚Äúkey-value‚Äù and ‚Äúdictionary‚Äù avoid commitment to a particular implementation.
    Here, too, ‚Äúdictionary‚Äù evokes a common mental image of unique words that map
    to descriptions. The term ‚Äúkey value‚Äù is even more technically useful because
    keys are meant to all be distinct (i.e., no two different key-value pairs can
    have the same key; alternatively, one key can map to only one value). This makes
    sense because we view this as a generalization of sets, so the keys are the set
    elements, which must necessarily have no duplicates; the values take the place
    of the Boolean.
  prefs: []
  type: TYPE_NORMAL
- en: 'To extend our set representation to handle a dictionary or key-value store,
    we need to make a few changes. First, we introduce the key-value representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Each bucket is still an empty list, but we understand it to be a list of key-value
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we only had `is-in` to check whether an element was present in a
    set or not. That element is now the key, and we could have a similar function
    to check whether the key is present. However, we rarely want to know just that;
    in fact, because we already know the key, we usually want the associated value.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can just have this one function:We use Pyret‚Äôs naming convention
    of `-now` to indicate that this result might change later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, `getkv-now` may fail: the key may not be present. That is, it has
    become a partial function [[Partial Domains](partial-domains.html)]. We therefore
    have all the usual strategies for dealing with partial functions. Here, for simplicity
    we choose to return an error if the key is not present, but all the other strategies
    we discuss for handling partiality are valid (and often better in a robust implementation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the generalization of `insert`. However, `insert` had no reason to
    return an error: inserting an element twice was harmless. However, because keys
    must now be associated with only one value, insertion has to check whether the
    key is already present, and signal an error otherwise. In short, it is also partial.This
    is not partial due to a mathematical reason, but rather because of state: the
    same key may have been inserted previously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have agreed on this interface, getting a value is a natural extension
    of checking for membership:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Having found the index, we look in the bucket for whether any key-value pair
    has the desired key. If it does, then we return the corresponding value. Otherwise,
    we error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inserting a key-value pair similarly generalizes adding an element to the set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we check the bucket for whether the key is already present. If it
    is, we choose to halt with an error. Otherwise, we make the key-value pair and
    link it to the existing bucket contents, and modify the array to refer to the
    new list.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do the above pair of functions do all the necessary error-checking?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Above, `setkv-now` raises an error if a key already has a name associated with
    it. A natural variation is to instead override the associated value, so that the
    new value is now associated with that key. Modify the implementation to do that
    instead, and make sure you test it thoroughly! Note that you may need to modify
    the `KV` datatype also.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This concludes our brief tour of sets (yet again!) and key-value stores or
    dictionaries. We have chosen to implement both using arrays, which required us
    to employ hashes. For more on string dictionaries, see the [Pyret documentation](https://www.pyret.org/docs/latest/string-dict.html).
    Observe that Pyret offers two kinds of dictionaries: one mutable (like we have
    shown here) and one (the default) functional.'
  prefs: []
  type: TYPE_NORMAL
