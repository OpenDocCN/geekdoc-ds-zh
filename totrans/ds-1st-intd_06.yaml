- en: Chapter 2 Reading in data locally and from the web
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 本地及从网络读取数据
- en: 原文：[https://datasciencebook.ca/reading.html](https://datasciencebook.ca/reading.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://datasciencebook.ca/reading.html](https://datasciencebook.ca/reading.html)
- en: 2.1 Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 概述
- en: In this chapter, you’ll learn to read tabular data of various formats into R
    from your local device (e.g., your laptop) and the web. “Reading” (or “loading”)
    is the process of converting data (stored as plain text, a database, HTML, etc.)
    into an object (e.g., a data frame) that R can easily access and manipulate. Thus
    reading data is the gateway to any data analysis; you won’t be able to analyze
    data unless you’ve loaded it first. And because there are many ways to store data,
    there are similarly many ways to read data into R. The more time you spend upfront
    matching the data reading method to the type of data you have, the less time you
    will have to devote to re-formatting, cleaning and wrangling your data (the second
    step to all data analyses). It’s like making sure your shoelaces are tied well
    before going for a run so that you don’t trip later on!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何从你的本地设备（例如，你的笔记本电脑）和网络读取各种格式的表格数据到 R。 “读取”（或“加载”）是将数据（以纯文本、数据库、HTML
    等形式存储）转换为 R 可以轻松访问和操作的对象（例如，数据框）的过程。因此，读取数据是任何数据分析的入门；除非你首先加载了数据，否则你将无法分析数据。由于数据可以以多种方式存储，因此将数据读取到
    R 的方法也有很多。你前期花更多的时间来匹配数据读取方法与你的数据类型，你将需要花更少的时间来重新格式化、清理和整理你的数据（所有数据分析的第二步）。这就像在跑步前确保你的鞋带系得很好，以免后来摔倒！
- en: 2.2 Chapter learning objectives
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 章节学习目标
- en: 'By the end of the chapter, readers will be able to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，读者将能够完成以下任务：
- en: 'Define the types of path and use them to locate files:'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义路径类型并使用它们定位文件：
- en: absolute file path
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对文件路径
- en: relative file path
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对文件路径
- en: Uniform Resource Locator (URL)
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一资源定位符 (URL)
- en: 'Read data into R from various types of path using:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下方法从各种类型的路径读取数据到 R：
- en: '`read_csv`'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_csv`'
- en: '`read_tsv`'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_tsv`'
- en: '`read_csv2`'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_csv2`'
- en: '`read_delim`'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_delim`'
- en: '`read_excel`'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_excel`'
- en: Compare and contrast the `read_*` functions.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较和对比 `read_*` 函数。
- en: 'Describe when to use the following `read_*` function arguments:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述何时使用以下 `read_*` 函数参数：
- en: '`skip`'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip`'
- en: '`delim`'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delim`'
- en: '`col_names`'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col_names`'
- en: Choose the appropriate `tidyverse` `read_*` function and function arguments
    to load a given plain text tabular data set into R.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的 `tidyverse` `read_*` 函数和函数参数，将给定的纯文本表格数据集加载到 R 中。
- en: Use the `rename` function to rename columns in a data frame.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `rename` 函数重命名数据框中的列。
- en: Use `read_excel` function and arguments to load a sheet from an excel file into
    R.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `read_excel` 函数和参数将电子表格文件中的工作表加载到 R 中。
- en: 'Work with databases using functions from `dbplyr` and `DBI`:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `dbplyr` 和 `DBI` 函数与数据库一起工作：
- en: Connect to a database with `dbConnect`.
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `dbConnect` 连接到数据库。
- en: List tables in the database with `dbListTables`.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `dbListTables` 列出数据库中的表。
- en: Create a reference to a database table with `tbl`.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `tbl` 创建数据库表的引用。
- en: Bring data from a database into R using `collect`.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `collect` 将数据库中的数据引入 R。
- en: Use `write_csv` to save a data frame to a `.csv` file.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `write_csv` 将数据框保存到 `.csv` 文件。
- en: '(*Optional*) Obtain data from the web using scraping and application programming
    interfaces (APIs):'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (*可选*) 使用抓取和应用程序编程接口 (API) 从网络获取数据：
- en: Read HTML source code from a URL using the `rvest` package.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `rvest` 包从 URL 读取 HTML 源代码。
- en: Read data from the NASA “Astronomy Picture of the Day” API using the `httr2`
    package.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `httr2` 包从 NASA “每日天文图片” API 读取数据。
- en: Compare downloading tabular data from a plain text file (e.g., `.csv`), accessing
    data from an API, and scraping the HTML source code from a website.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较从纯文本文件（例如，`.csv`）下载表格数据、从 API 访问数据以及从网站抓取 HTML 源代码。
- en: 2.3 Absolute and relative file paths
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 绝对和相对文件路径
- en: This chapter will discuss the different functions we can use to import data
    into R, but before we can talk about *how* we read the data into R with these
    functions, we first need to talk about *where* the data lives. When you load a
    data set into R, you first need to tell R where those files live. The file could
    live on your computer (*local*) or somewhere on the internet (*remote*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论我们可以使用的不同函数来导入数据到 R，但在我们讨论如何使用这些函数将数据读取到 R 之前，我们首先需要讨论数据“在哪里”。当你将数据集加载到
    R 中时，你首先需要告诉 R 这些文件在哪里。文件可能存储在你的电脑上（*本地*）或互联网上的某个地方（*远程*）。
- en: 'The place where the file lives on your computer is referred to as its “path”.
    You can think of the path as directions to the file. There are two kinds of paths:
    *relative* paths and *absolute* paths. A relative path indicates where the file
    is with respect to your *working directory* (i.e., “where you are currently”)
    on the computer. On the other hand, an absolute path indicates where the file
    is with respect to the computer’s filesystem base (or *root*) folder, regardless
    of where you are working.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文件在计算机上的位置被称为其“路径”。你可以把路径看作是到达文件的指示。有两种类型的路径：*相对* 路径和 *绝对* 路径。相对路径表示文件相对于你的
    *工作目录*（即，“你现在所在的位置”）在计算机上的位置。另一方面，绝对路径表示文件相对于计算机文件系统的基础（或 *根*）文件夹的位置，无论你在哪里工作。
- en: Suppose our computer’s filesystem looks like the picture in Figure [2.1](reading.html#fig:file-system-for-export-to-intro-datascience).
    We are working in a file titled `project3.ipynb`, and our current working directory
    is `project3`; typically, as is the case here, the working directory is the directory
    containing the file you are currently working on.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的计算机文件系统看起来像图 [2.1](reading.html#fig:file-system-for-export-to-intro-datascience)
    中的那样。我们正在处理一个名为 `project3.ipynb` 的文件，并且我们的当前工作目录是 `project3`；通常情况下，就像这里一样，工作目录是你当前正在工作的目录。
- en: '![Example file system.](../Images/9fd3036c264ffe4949dd0ea85733528c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![示例文件系统](../Images/9fd3036c264ffe4949dd0ea85733528c.png)'
- en: 'Figure 2.1: Example file system.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：示例文件系统。
- en: 'Let’s say we wanted to open the `happiness_report.csv` file. We have two options
    to indicate where the file is: using a relative path, or using an absolute path.
    The absolute path of the file always starts with a slash `/`—representing the
    root folder on the computer—and proceeds by listing out the sequence of folders
    you would have to enter to reach the file, each separated by another slash `/`.
    So in this case, `happiness_report.csv` would be reached by starting at the root,
    and entering the `home` folder, then the `dsci-100` folder, then the `project3`
    folder, and then finally the `data` folder. So its absolute path would be `/home/dsci-100/project3/data/happiness_report.csv`.
    We can load the file using its absolute path as a string passed to the `read_csv`
    function.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想打开 `happiness_report.csv` 文件。我们有两种方式来指示文件的位置：使用相对路径或使用绝对路径。文件的绝对路径始终以一个斜杠
    `/` 开头——代表计算机上的根文件夹——然后通过列出到达文件的文件夹序列，每个文件夹之间用另一个斜杠 `/` 分隔。所以在这种情况下，`happiness_report.csv`
    可以通过从根目录开始，进入 `home` 文件夹，然后是 `dsci-100` 文件夹，然后是 `project3` 文件夹，最后是 `data` 文件夹来到达。因此，它的绝对路径是
    `/home/dsci-100/project3/data/happiness_report.csv`。我们可以通过将绝对路径作为字符串传递给 `read_csv`
    函数来加载该文件。
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If we instead wanted to use a relative path, we would need to list out the sequence
    of steps needed to get from our current working directory to the file, with slashes
    `/` separating each step. Since we are currently in the `project3` folder, we
    just need to enter the `data` folder to reach our desired file. Hence the relative
    path is `data/happiness_report.csv`, and we can load the file using its relative
    path as a string passed to `read_csv`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用相对路径，我们需要列出从当前工作目录到文件的步骤序列，每个步骤之间用斜杠 `/` 分隔。由于我们目前位于 `project3` 文件夹中，我们只需进入
    `data` 文件夹即可到达我们想要的文件。因此，相对路径是 `data/happiness_report.csv`，我们可以通过将相对路径作为字符串传递给
    `read_csv` 函数来加载该文件。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that there is no forward slash at the beginning of a relative path; if
    we accidentally typed `"/data/happiness_report.csv"`, R would look for a folder
    named `data` in the root folder of the computer—but that doesn’t exist!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，相对路径的开头没有正斜杠；如果我们不小心输入了 `"/data/happiness_report.csv"`，R 会去计算机根目录下寻找名为 `data`
    的文件夹——但这个文件夹并不存在！
- en: 'Aside from specifying places to go in a path using folder names (like `data`
    and `project3`), we can also specify two additional special places: the *current
    directory* and the *previous directory*. We indicate the current working directory
    with a single dot `.`, and the previous directory with two dots `..`. So for instance,
    if we wanted to reach the `bike_share.csv` file from the `project3` folder, we
    could use the relative path `../project2/bike_share.csv`. We can even combine
    these two; for example, we could reach the `bike_share.csv` file using the (very
    silly) path `../project2/../project2/./bike_share.csv` with quite a few redundant
    directions: it says to go back a folder, then open `project2`, then go back a
    folder again, then open `project2` again, then stay in the current directory,
    then finally get to `bike_share.csv`. Whew, what a long trip!'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在路径中使用文件夹名称（如`data`和`project3`）来指定位置外，我们还可以指定两个额外的特殊位置：*当前目录*和*上一目录*。我们用一个点`.`来表示当前工作目录，用两个点`..`来表示上一目录。例如，如果我们想从`project3`文件夹中到达`bike_share.csv`文件，我们可以使用相对路径`../project2/bike_share.csv`。我们甚至可以将这两个路径结合起来；例如，我们可以使用（非常愚蠢的）路径`../project2/../project2/./bike_share.csv`来到达`bike_share.csv`文件，其中包含很多冗余的方向：它指示我们返回一个文件夹，然后打开`project2`，然后再次返回一个文件夹，然后再次打开`project2`，然后停留在当前目录，最后到达`bike_share.csv`。哇，真是一次漫长的旅程！
- en: 'So which kind of path should you use: relative, or absolute? Generally speaking,
    you should use relative paths. Using a relative path helps ensure that your code
    can be run on a different computer (and as an added bonus, relative paths are
    often shorter—easier to type!). This is because a file’s relative path is often
    the same across different computers, while a file’s absolute path (the names of
    all of the folders between the computer’s root, represented by `/`, and the file)
    isn’t usually the same across different computers. For example, suppose Fatima
    and Jayden are working on a project together on the `happiness_report.csv` data.
    Fatima’s file is stored at'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你应该使用哪种类型的路径：相对路径还是绝对路径？一般来说，你应该使用相对路径。使用相对路径可以帮助确保你的代码可以在不同的电脑上运行（并且作为一个额外的优点，相对路径通常更短——更容易输入！）这是因为文件相对于不同电脑的路径通常是相同的，而文件的绝对路径（从电脑根目录到文件的文件夹名称，由`/`表示）通常在不同电脑上是不相同的。例如，假设Fatima和Jayden正在一起在`happiness_report.csv`数据上工作。Fatima的文件存储在
- en: '`/home/Fatima/project3/data/happiness_report.csv`,'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`/home/Fatima/project3/data/happiness_report.csv`,'
- en: while Jayden’s is stored at
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Jayden的存储位置是
- en: '`/home/Jayden/project3/data/happiness_report.csv`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`/home/Jayden/project3/data/happiness_report.csv`.'
- en: Even though Fatima and Jayden stored their files in the same place on their
    computers (in their home folders), the absolute paths are different due to their
    different usernames. If Jayden has code that loads the `happiness_report.csv`
    data using an absolute path, the code won’t work on Fatima’s computer. But the
    relative path from inside the `project3` folder (`data/happiness_report.csv`)
    is the same on both computers; any code that uses relative paths will work on
    both! In the additional resources section, we include a link to a short video
    on the difference between absolute and relative paths. You can also check out
    the `here` package, which provides methods for finding and constructing file paths
    in R.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Fatima和Jayden将文件存储在他们的电脑上的相同位置（在他们的主文件夹中），但由于他们的用户名不同，绝对路径是不同的。如果Jayden的代码使用绝对路径加载`happiness_report.csv`数据，那么这段代码在Fatima的电脑上将无法工作。但是，从`project3`文件夹内部（`data/happiness_report.csv`）的相对路径在两台电脑上都是相同的；任何使用相对路径的代码都可以在两台电脑上运行！在附加资源部分，我们包含了一个关于绝对路径和相对路径差异的简短视频链接。您还可以查看`here`包，它提供了在R中查找和构建文件路径的方法。
- en: Beyond files stored on your computer (i.e., locally), we also need a way to
    locate resources stored elsewhere on the internet (i.e., remotely). For this purpose
    we use a *Uniform Resource Locator (URL)*, i.e., a web address that looks something
    like [https://datasciencebook.ca/](https://datasciencebook.ca/). URLs indicate
    the location of a resource on the internet, and start with a web domain, followed
    by a forward slash `/`, and then a path to where the resource is located on the
    remote machine.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储在您电脑上的文件（即本地文件）之外，我们还需要一种方法来定位存储在互联网其他地方（即远程）的资源。为此，我们使用*统一资源定位符（URL）*，即看起来像[https://datasciencebook.ca/](https://datasciencebook.ca/)的网页地址。URL指示互联网上资源的位置，以一个网络域名开始，后面跟着一个正斜杠`/`，然后是远程机器上资源所在路径。
- en: 2.4 Reading tabular data from a plain text file into R
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 从纯文本文件中读取表格数据到 R
- en: 2.4.1 `read_csv` to read in comma-separated values files
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 使用`read_csv`读取逗号分隔值文件
- en: Now that we have learned about *where* data could be, we will learn about *how*
    to import data into R using various functions. Specifically, we will learn how
    to *read* tabular data from a plain text file (a document containing only text)
    *into* R and *write* tabular data to a file *out of* R. The function we use to
    do this depends on the file’s format. For example, in the last chapter, we learned
    about using the `tidyverse` `read_csv` function when reading `.csv` (**c**omma-**s**eparated
    **v**alues) files. In that case, the separator or *delimiter* that divided our
    columns was a comma (`,`). We only learned the case where the data matched the
    expected defaults of the `read_csv` function (column names are present, and commas
    are used as the delimiter between columns). In this section, we will learn how
    to read files that do not satisfy the default expectations of `read_csv`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了数据可能存在的位置，我们将学习如何使用各种函数将数据导入 R。具体来说，我们将学习如何从纯文本文件（仅包含文本的文档）中 *读取* 表格数据到
    R，以及如何将表格数据 *写入* 到文件中。我们使用的函数取决于文件的格式。例如，在上一个章节中，我们学习了在读取 `.csv`（**逗号**分隔**值**）文件时使用
    `tidyverse` 的 `read_csv` 函数。在那个案例中，分隔我们的列的 *分隔符* 是逗号（`,`）。我们只学习了数据符合 `read_csv`
    函数预期默认值的情况（存在列名，并且逗号用作列之间的分隔符）。在本节中，我们将学习如何读取不满足 `read_csv` 默认期望的文件。
- en: Before we jump into the cases where the data aren’t in the expected default
    format for `tidyverse` and `read_csv`, let’s revisit the more straightforward
    case where the defaults hold, and the only argument we need to give to the function
    is the path to the file, `data/can_lang.csv`. The `can_lang` data set contains
    language data from the 2016 Canadian census. We put `data/` before the file’s
    name when we are loading the data set because this data set is located in a sub-folder,
    named `data`, relative to where we are running our R code. Here is what the text
    in the file `data/can_lang.csv` looks like.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨数据不是 `tidyverse` 和 `read_csv` 预期默认格式的案例之前，让我们回顾一下默认值保持更简单直接的情况，在这种情况下，我们只需要提供给函数的参数是文件的路径，`data/can_lang.csv`。`can_lang`
    数据集包含了 2016 年加拿大人口普查的语言数据。当我们加载数据集时，我们在文件名前放置 `data/`，因为此数据集位于一个名为 `data` 的子文件夹中，该子文件夹相对于我们运行
    R 代码的位置。以下是文件 `data/can_lang.csv` 中的文本内容。
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And here is a review of how we can use `read_csv` to load it into R. First we
    load the `tidyverse` package to gain access to useful functions for reading the
    data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何使用 `read_csv` 将其加载到 R 中的回顾。首先，我们加载 `tidyverse` 包以获取读取数据的 useful functions。
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next we use `read_csv` to load the data into R, and in that call we specify
    the relative path to the file. Note that it is normal and expected that a message
    is printed out after using the `read_csv` and related functions. This message
    lets you know the data types of each of the columns that R inferred while reading
    the data into R. In the future when we use this and related functions to load
    data in this book, we will silence these messages to help with the readability
    of the book.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `read_csv` 将数据加载到 R 中，并在该调用中指定文件的相对路径。请注意，在使用 `read_csv` 和相关函数后打印出消息是正常和预期的。这个消息会告诉你
    R 在将数据读入 R 时推断出的每一列的数据类型。在将来，当我们使用这些和相关函数来加载本书中的数据时，我们将关闭这些消息以帮助提高书籍的可读性。
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, to view the first 10 rows of the data frame, we must call it:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要查看数据框的前 10 行，我们必须调用它：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 2.4.2 Skipping rows when reading in data
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 在读取数据时跳过行
- en: Oftentimes, information about how data was collected, or other relevant information,
    is included at the top of the data file. This information is usually written in
    sentence and paragraph form, with no delimiter because it is not organized into
    columns. An example of this is shown below. This information gives the data scientist
    useful context and information about the data, however, it is not well formatted
    or intended to be read into a data frame cell along with the tabular data that
    follows later in the file.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，关于数据收集方式或其他相关信息会被包含在数据文件的最顶部。这些信息通常以句子和段落的形式书写，没有分隔符，因为它们没有被组织成列。下面是一个例子。这些信息为数据科学家提供了关于数据的
    useful context 和信息，然而，它们并没有很好地格式化，也不打算与文件后面跟随的表格数据一起读入数据框单元格中。
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With this extra information being present at the top of the file, using `read_csv`
    as we did previously does not allow us to correctly load the data into R. In the
    case of this file we end up only reading in one column of the data set. In contrast
    to the normal and expected messages above, this time R prints out a warning for
    us indicating that there might be a problem with how our data is being read in.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种额外信息出现在文件顶部，使用我们之前使用的`read_csv`方法无法正确将数据加载到R中。在这种情况下，我们最终只读取了数据集的一个列。与上述正常和预期的消息相反，这次R打印出警告告诉我们，我们的数据读取可能存在问题。
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To successfully read data like this into R, the `skip` argument can be useful
    to tell R how many lines to skip before it should start reading in the data. In
    the example above, we would set this value to 3.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功将此类数据读入R，`skip`参数可以很有用，它告诉R在开始读取数据之前应跳过多少行。在上面的例子中，我们将此值设置为3。
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'How did we know to skip three lines? We looked at the data! The first three
    lines of the data had information we didn’t need to import:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何知道要跳过三行的？我们查看数据！数据的前三行包含我们不需要导入的信息：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The column names began at line 4, so we skipped the first three lines.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列名从第4行开始，所以我们跳过了前3行。
- en: 2.4.3 `read_tsv` to read in tab-separated values files
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 使用`read_tsv`读取制表符分隔值文件
- en: Another common way data is stored is with tabs as the delimiter. Notice the
    data file, `can_lang.tsv`, has tabs in between the columns instead of commas.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储的另一种常见方式是使用制表符作为分隔符。注意数据文件`can_lang.tsv`中的列之间使用的是制表符而不是逗号。
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can use the `read_tsv` function to read in `.tsv` (**t**ab **s**eparated
    **v**alues) files.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`read_tsv`函数来读取`.tsv`（**t**ab **s**eparated **v**alues）文件。
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you compare the data frame here to the data frame we obtained in Section
    [2.4.1](reading.html#readcsv) using `read_csv`, you’ll notice that they look identical:
    they have the same number of columns and rows, the same column names, and the
    same entries! So even though we needed to use a different function depending on
    the file format, our resulting data frame (`canlang_data`) in both cases was the
    same.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这里的数据框与我们在第[2.4.1](reading.html#readcsv)节中使用`read_csv`获得的数据框进行比较，你会注意到它们看起来完全相同：它们有相同数量的列和行，相同的列名，以及相同的条目！所以尽管我们需要根据文件格式使用不同的函数，但两种情况下我们得到的结果数据框（`canlang_data`）都是相同的。
- en: 2.4.4 `read_delim` as a more flexible method to get tabular data into R
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 使用`read_delim`作为将表格数据读入R的更灵活的方法
- en: 'The `read_csv` and `read_tsv` functions are actually just special cases of
    the more general `read_delim` function. We can use `read_delim` to import both
    comma and tab-separated values files, and more; we just have to specify the delimiter.
    For example, the `can_lang_no_names.tsv` file contains a different version of
    this same data set with no column names and uses tabs as the delimiter instead
    of commas. Here is how the file would look in a plain text editor:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv`和`read_tsv`函数实际上是更通用`read_delim`函数的特殊情况。我们可以使用`read_delim`来导入逗号和制表符分隔的值文件，以及更多；我们只需指定分隔符。例如，`can_lang_no_names.tsv`文件包含同一数据集的不同版本，没有列名，并使用制表符而不是逗号作为分隔符。以下是该文件在纯文本编辑器中的外观：'
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To read this into R using the `read_delim` function, we specify the path to
    the file as the first argument, provide the tab character `"\t"` as the `delim`
    argument, and set the `col_names` argument to `FALSE` to denote that there are
    no column names provided in the data. Note that the `read_csv`, `read_tsv`, and
    `read_delim` functions all have a `col_names` argument with the default value
    `TRUE`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`read_delim`函数将数据读入R，我们指定文件路径作为第一个参数，提供制表符`"\t"`作为`delim`参数，并将`col_names`参数设置为`FALSE`以表示数据中没有提供列名。请注意，`read_csv`、`read_tsv`和`read_delim`函数都有一个`col_names`参数，默认值为`TRUE`。
- en: '**Note:** `\t` is an example of an *escaped character*, which always starts
    with a backslash (`\`). Escaped characters are used to represent non-printing
    characters (like the tab) or those with special meanings (such as quotation marks).'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** `\t`是一个**转义字符**的例子，它始终以反斜杠（`\`）开头。转义字符用于表示非打印字符（如制表符）或具有特殊意义的字符（如引号）。'
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Data frames in R need to have column names. Thus if you read in data without
    column names, R will assign names automatically. In this example, R assigns the
    column names `X1, X2, X3, X4, X5, X6`. It is best to rename your columns manually
    in this scenario. The current column names (`X1, X2`, etc.) are not very descriptive
    and will make your analysis confusing. To rename your columns, you can use the
    `rename` function from [the `dplyr` R package](https://dplyr.tidyverse.org/) ([Wickham,
    François, et al. 2021](#ref-dplyr)) (one of the packages loaded with `tidyverse`,
    so we don’t need to load it separately). The first argument is the data set, and
    in the subsequent arguments you write `new_name = old_name` for the selected variables
    to rename. We rename the `X1, X2, ..., X6` columns in the `canlang_data` data
    frame to more descriptive names below.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: R 中的数据框需要具有列名。因此，如果你读取的数据没有列名，R 将会自动分配名称。在这个例子中，R 分配的列名为 `X1, X2, X3, X4, X5,
    X6`。在这种情况下，最好手动重命名你的列。当前的列名（`X1, X2` 等）描述性不强，会使你的分析变得混乱。要重命名列，你可以使用来自 [`dplyr`
    R 包](https://dplyr.tidyverse.org/)（`tidyverse` 一起加载的包之一，因此我们不需要单独加载它）的 `rename`
    函数（Wickham, François, et al. 2021）。第一个参数是数据集，在后续参数中，你为要重命名的变量写入 `new_name = old_name`。我们将在下面的
    `canlang_data` 数据框中将 `X1, X2, ..., X6` 列重命名为更具描述性的名称。
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 2.4.5 Reading tabular data directly from a URL
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 直接从 URL 读取表格数据
- en: We can also use `read_csv`, `read_tsv`, or `read_delim` (and related functions)
    to read in data directly from a **U**niform **R**esource **L**ocator (URL) that
    contains tabular data. Here, we provide the URL of a remote file to `read_*`,
    instead of a path to a local file on our computer. We need to surround the URL
    with quotes similar to when we specify a path on our local computer. All other
    arguments that we use are the same as when using these functions with a local
    file on our computer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `read_csv`、`read_tsv` 或 `read_delim`（和相关函数）直接从包含表格数据的 **U**niform **R**esource
    **L**ocator（URL）读取数据。在这里，我们提供远程文件的 URL 给 `read_*`，而不是我们电脑上本地文件的路径。我们需要用引号包围 URL，就像我们在本地电脑上指定路径时一样。我们使用的所有其他参数与在电脑上使用本地文件时相同。
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 2.4.6 Downloading data from a URL
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.6 从 URL 下载数据
- en: Occasionally the data available at a URL is not formatted nicely enough to use
    `read_csv`, `read_tsv`, `read_delim`, or other related functions to read the data
    directly into R. In situations where it is necessary to download a file to our
    local computer prior to working with it in R, we can use the `download.file` function.
    The first argument is the URL, and the second is a path where we would like to
    store the downloaded file.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有时 URL 上可用的数据格式不够好，不能使用 `read_csv`、`read_tsv`、`read_delim` 或其他相关函数直接将数据读入 R。在需要在我们用
    R 处理它之前先下载文件到我们本地电脑的情况中，我们可以使用 `download.file` 函数。第一个参数是 URL，第二个是我们想要存储下载文件的路径。
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 2.4.7 Previewing a data file before reading it into R
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.7 在将数据文件读入 R 之前预览它
- en: 'In many of the examples above, we gave you previews of the data file before
    we read it into R. Previewing data is essential to see whether or not there are
    column names, what the delimiters are, and if there are lines you need to skip.
    You should do this yourself when trying to read in data files: open the file in
    whichever text editor you prefer to inspect its contents prior to reading it into
    R.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述许多示例中，我们在将数据文件读入 R 之前都提供了数据文件的预览。预览数据对于查看是否有列名、分隔符是什么以及是否有需要跳过的行是至关重要的。当尝试读取数据文件时，你应该自己这样做：使用你偏好的任何文本编辑器打开文件，在将其读入
    R 之前检查其内容。
- en: 2.5 Reading tabular data from a Microsoft Excel file
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 从 Microsoft Excel 文件中读取表格数据
- en: 'There are many other ways to store tabular data sets beyond plain text files,
    and similarly, many ways to load those data sets into R. For example, it is very
    common to encounter, and need to load into R, data stored as a Microsoft Excel
    spreadsheet (with the file name extension `.xlsx`). To be able to do this, a key
    thing to know is that even though `.csv` and `.xlsx` files look almost identical
    when loaded into Excel, the data themselves are stored completely differently.
    While `.csv` files are plain text files, where the characters you see when you
    open the file in a text editor are exactly the data they represent, this is not
    the case for `.xlsx` files. Take a look at a snippet of what a `.xlsx` file would
    look like in a text editor:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了纯文本文件之外，还有许多其他方式可以存储表格数据集，同样，也有许多方式将这些数据集加载到R中。例如，非常常见的情况是需要将存储为Microsoft
    Excel电子表格（文件扩展名为`.xlsx`）的数据加载到R中。为了能够做到这一点，一个关键的知识点是，尽管`.csv`和`.xlsx`文件在加载到Excel中时看起来几乎相同，但数据本身存储的方式完全不同。虽然`.csv`文件是纯文本文件，你在文本编辑器中打开文件时看到的字符正是它们所代表的数据，但`.xlsx`文件并非如此。看看一个`.xlsx`文件在文本编辑器中会是什么样子：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This type of file representation allows Excel files to store additional things
    that you cannot store in a `.csv` file, such as fonts, text formatting, graphics,
    multiple sheets and more. And despite looking odd in a plain text editor, we can
    read Excel spreadsheets into R using the `readxl` package developed specifically
    for this purpose.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种文件表示方式允许Excel文件存储`.csv`文件无法存储的额外内容，例如字体、文本格式、图形、多个工作表等。尽管在纯文本编辑器中看起来很奇怪，但我们仍然可以使用专门为此目的开发的`readxl`包将Excel工作表读取到R中。
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If the `.xlsx` file has multiple sheets, you have to use the `sheet` argument
    to specify the sheet number or name. You can also specify cell ranges using the
    `range` argument. This functionality is useful when a single sheet contains multiple
    tables (a sad thing that happens to many Excel spreadsheets since this makes reading
    in data more difficult).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`.xlsx`文件包含多个工作表，您必须使用`sheet`参数来指定工作表编号或名称。您还可以使用`range`参数指定单元格范围。当单个工作表中包含多个表格（这是许多Excel工作表都会遇到的一个令人难过的事情，因为这使得读取数据变得更加困难）时，此功能非常有用。
- en: As with plain text files, you should always explore the data file before importing
    it into R. Exploring the data beforehand helps you decide which arguments you
    need to load the data into R successfully. If you do not have the Excel program
    on your computer, you can use other programs to preview the file. Examples include
    Google Sheets and Libre Office.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与纯文本文件一样，在将数据文件导入R之前，您应该始终探索数据文件。事先探索数据可以帮助您决定需要哪些参数才能成功将数据加载到R中。如果您没有在计算机上安装Excel程序，您可以使用其他程序来预览文件。例如，包括Google
    Sheets和Libre Office。
- en: In Table [2.1](reading.html#tab:read-table) we summarize the `read_*` functions
    we covered in this chapter. We also include the `read_csv2` function for data
    separated by semicolons `;`, which you may run into with data sets where the decimal
    is represented by a comma instead of a period (as with some data sets from European
    countries).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[2.1](reading.html#tab:read-table)中，我们总结了本章中涵盖的`read_*`函数。我们还包括了用于分号`;`分隔数据的`read_csv2`函数，您可能会在十进制以逗号而不是点（例如，一些来自欧洲国家的数据集）表示的数据集中遇到这种情况。
- en: 'Table 2.1: Summary of `read_*` functions'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：`read_*`函数摘要
- en: '| Data File Type | R Function | R Package |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 数据文件类型 | R 函数 | R 包 |'
- en: '| --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Comma (`,`) separated files | `read_csv` | `readr` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 逗号（`,`）分隔的文件 | `read_csv` | `readr` |'
- en: '| Tab (`\t`) separated files | `read_tsv` | `readr` |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 制表符（`\t`）分隔的文件 | `read_tsv` | `readr` |'
- en: '| Semicolon (`;`) separated files | `read_csv2` | `readr` |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 分号（`;`）分隔的文件 | `read_csv2` | `readr` |'
- en: '| Various formats (`.csv`, `.tsv`) | `read_delim` | `readr` |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 各种格式（`.csv`, `.tsv`） | `read_delim` | `readr` |'
- en: '| Excel files (`.xlsx`) | `read_excel` | `readxl` |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Excel文件（`.xlsx`） | `read_excel` | `readxl` |'
- en: '**Note:** `readr` is a part of the `tidyverse` package so we did not need to
    load this package separately since we loaded `tidyverse`.'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** `readr`是`tidyverse`包的一部分，因此我们不需要单独加载此包，因为我们已经加载了`tidyverse`。'
- en: 2.6 Reading data from a database
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 从数据库中读取数据
- en: Another very common form of data storage is the relational database. Databases
    are great when you have large data sets or multiple users working on a project.
    There are many relational database management systems, such as SQLite, MySQL,
    PostgreSQL, Oracle, and many more. These different relational database management
    systems each have their own advantages and limitations. Almost all employ SQL
    (*structured query language*) to obtain data from the database. But you don’t
    need to know SQL to analyze data from a database; several packages have been written
    that allow you to connect to relational databases and use the R programming language
    to obtain data. In this book, we will give examples of how to do this using R
    with SQLite and PostgreSQL databases.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非常常见的数据存储形式是关系型数据库。当你拥有大量数据集或多个用户在项目上工作时，数据库是非常棒的。有许多关系型数据库管理系统，例如SQLite、MySQL、PostgreSQL、Oracle等等。这些不同的关系型数据库管理系统各自都有其优势和局限性。几乎所有的数据库都使用SQL（*结构化查询语言*）从数据库中获取数据。但你不需要知道SQL来分析数据库中的数据；已经编写了几个包，允许你连接到关系型数据库并使用R编程语言获取数据。在这本书中，我们将通过使用R和SQLite以及PostgreSQL数据库的例子来展示如何做到这一点。
- en: 2.6.1 Reading data from a SQLite database
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.1 从SQLite数据库读取数据
- en: SQLite is probably the simplest relational database system that one can use
    in combination with R. SQLite databases are self-contained, and are usually stored
    and accessed locally on one computer from a file with a `.db` extension (or sometimes
    an `.sqlite` extension). Similar to Excel files, these are not plain text files
    and cannot be read in a plain text editor.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: SQLite可能是可以与R结合使用的最简单的关系型数据库系统。SQLite数据库是自包含的，通常存储和访问在单个计算机上，通过一个以`.db`扩展名（或有时是`.sqlite`扩展名）的文件进行（或有时是一个`.sqlite`扩展名）。类似于Excel文件，这些不是纯文本文件，不能在纯文本编辑器中读取。
- en: The first thing you need to do to read data into R from a database is to connect
    to the database. We do that using the `dbConnect` function from the `DBI` (database
    interface) package. This does not read in the data, but simply tells R where the
    database is and opens up a communication channel that R can use to send SQL commands
    to the database.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据从数据库读取到R中的第一步是连接到数据库。我们使用`DBI`（数据库接口）包中的`dbConnect`函数来完成这个操作。这不会读取数据，但只是告诉R数据库的位置并打开一个R可以用来向数据库发送SQL命令的通信通道。
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Often relational databases have many tables; thus, in order to retrieve data
    from a database, you need to know the name of the table in which the data is stored.
    You can get the names of all the tables in the database using the `dbListTables`
    function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常关系型数据库有许多表；因此，为了从数据库中检索数据，你需要知道数据存储的表名。你可以使用`dbListTables`函数获取数据库中所有表的名称：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `dbListTables` function returned only one name, which tells us that there
    is only one table in this database. To reference a table in the database (so that
    we can perform operations like selecting columns and filtering rows), we use the
    `tbl` function from the `dbplyr` package. The object returned by the `tbl` function
    allows us to work with data stored in databases as if they were just regular data
    frames; but secretly, behind the scenes, `dbplyr` is turning your function calls
    (e.g., `select` and `filter`) into SQL queries!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`dbListTables`函数只返回了一个名称，这告诉我们这个数据库中只有一个表。为了在数据库中引用一个表（以便我们可以执行选择列和过滤行等操作），我们使用`dbplyr`包中的`tbl`函数。`tbl`函数返回的对象允许我们像处理常规数据框一样处理存储在数据库中的数据；但幕后，`dbplyr`正在将你的函数调用（例如`select`和`filter`）转换为SQL查询！'
- en: '[PRE34]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Although it looks like we just got a data frame from the database, we didn’t!
    It’s a *reference*; the data is still stored only in the SQLite database. The
    `dbplyr` package works this way because databases are often more efficient at
    selecting, filtering and joining large data sets than R. And typically the database
    will not even be stored on your computer, but rather a more powerful machine somewhere
    on the web. So R is lazy and waits to bring this data into memory until you explicitly
    tell it to using the `collect` function. Figure [2.2](reading.html#fig:01-ref-vs-tibble)
    highlights the difference between a `tibble` object in R and the output we just
    created. Notice in the table on the right, the first two lines of the output indicate
    the source is SQL. The last line doesn’t show how many rows there are (R is trying
    to avoid performing expensive query operations), whereas the output for the `tibble`
    object does.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来我们只是从数据库中得到了一个数据框，但我们并没有！它是一个 *引用*；数据仍然只存储在 SQLite 数据库中。`dbplyr` 包以这种方式工作，因为数据库通常在选择、过滤和连接大型数据集方面比
    R 更有效率。通常，数据库甚至不会存储在你的电脑上，而是在网络上的某个更强大的机器上。所以 R 是懒惰的，它会在你明确使用 `collect` 函数告诉它之前，等待将数据带入内存。图
    [2.2](reading.html#fig:01-ref-vs-tibble) 强调了 R 中的 `tibble` 对象与我们刚刚创建的输出之间的差异。注意在右边的表格中，输出的前两行表明来源是
    SQL。最后一行没有显示行数（R 正在尝试避免执行昂贵的查询操作），而 `tibble` 对象的输出则显示了行数。
- en: '![Comparison of a reference to data in a database and a tibble in R.](../Images/644b8e6e6420e21cc8ece51f5a45cc35.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![数据库中的数据引用与 R 中的 tibble 的比较](../Images/644b8e6e6420e21cc8ece51f5a45cc35.png)'
- en: 'Figure 2.2: Comparison of a reference to data in a database and a tibble in
    R.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：数据库中的数据引用与 R 中的 tibble 的比较。
- en: We can look at the SQL commands that are sent to the database when we write
    `tbl(canlang_conn, "lang")` in R with the `show_query` function from the `dbplyr`
    package.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `dbplyr` 包中的 `show_query` 函数查看当我们在 R 中编写 `tbl(canlang_conn, "lang")`
    时发送到数据库的 SQL 命令。
- en: '[PRE36]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The output above shows the SQL code that is sent to the database. When we write
    `tbl(canlang_conn, "lang")` in R, in the background, the function is translating
    the R code into SQL, sending that SQL to the database, and then translating the
    response for us. So `dbplyr` does all the hard work of translating from R to SQL
    and back for us; we can just stick with R!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的输出显示了发送到数据库的 SQL 代码。当我们用 R 中的 `tbl(canlang_conn, "lang")` 编写时，在后台，该函数正在将
    R 代码转换为 SQL，将那个 SQL 发送到数据库，然后为我们转换响应。所以 `dbplyr` 为我们做了从 R 到 SQL 以及回转的所有繁重工作；我们只需坚持使用
    R 即可！
- en: 'With our `lang_db` table reference for the 2016 Canadian Census data in hand,
    we can mostly continue onward as if it were a regular data frame. For example,
    let’s do the same exercise from Chapter [1](intro.html#intro): we will obtain
    only those rows corresponding to Aboriginal languages, and keep only the `language`
    and `mother_tongue` columns. We can use the `filter` function to obtain only certain
    rows. Below we filter the data to include only Aboriginal languages.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了 `lang_db` 表引用，用于 2016 年加拿大人口普查数据，我们可以继续像处理常规数据框一样进行操作。例如，让我们做第 [1](intro.html#intro)
    章中的相同练习：我们将只获取与土著语言相对应的行，并只保留 `language` 和 `mother_tongue` 列。我们可以使用 `filter` 函数来获取特定的行。下面我们过滤数据，只包括土著语言。
- en: '[PRE38]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Above you can again see the hints that this data is not actually stored in
    R yet: the source is `SQL [?? x 6]` and the output says `... more rows` at the
    end (both indicating that R does not know how many rows there are in total!),
    and a database type `sqlite` is listed. We didn’t use the `collect` function because
    we are not ready to bring the data into R yet. We can still use the database to
    do some work to obtain *only* the small amount of data we want to work with locally
    in R. Let’s add the second part of our database query: selecting only the `language`
    and `mother_tongue` columns using the `select` function.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，你又可以再次看到这些提示，表明这些数据实际上还没有存储在 R 中：源是 `SQL [?? x 6]`，输出在末尾显示 `... more rows`（两者都表明
    R 还不知道总共有多少行！），并且列出了一个数据库类型 `sqlite`。我们没有使用 `collect` 函数，因为我们还没有准备好将数据带入 R。我们仍然可以使用数据库做一些工作，以获取我们想要在
    R 中本地处理的少量数据。让我们添加我们数据库查询的第二部分：使用 `select` 函数仅选择 `language` 和 `mother_tongue`
    列。
- en: '[PRE40]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now you can see that the database will return only the two columns we asked
    for with the `select` function. In order to actually retrieve this data in R as
    a data frame, we use the `collect` function. Below you will see that after running
    `collect`, R knows that the retrieved data has 67 rows, and there is no database
    listed any more.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到，数据库将只返回我们用 `select` 函数请求的两个列。为了实际上将此数据作为数据框检索到 R 中，我们使用 `collect` 函数。下面您将看到，在运行
    `collect` 之后，R 知道检索到的数据有 67 行，并且不再列出任何数据库。
- en: '[PRE42]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Aside from knowing the number of rows, the data looks pretty similar in both
    outputs shown above. And `dbplyr` provides many more functions (not just `filter`)
    that you can use to directly feed the database reference (`lang_db`) into downstream
    analysis functions (e.g., `ggplot2` for data visualization). But `dbplyr` does
    not provide *every* function that we need for analysis; we do eventually need
    to call `collect`. For example, look what happens when we try to use `nrow` to
    count rows in a data frame:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了知道行数外，上面显示的两个输出中的数据看起来相当相似。`dbplyr` 提供了许多更多函数（不仅仅是 `filter`），您可以直接将这些数据库引用（`lang_db`）输入到下游分析函数中（例如，`ggplot2`
    用于数据可视化）。但是 `dbplyr` 并不提供我们需要的所有分析函数；我们最终仍然需要调用 `collect`。例如，看看当我们尝试使用 `nrow`
    来计算数据框中的行数时会发生什么：
- en: '[PRE44]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'or `tail` to preview the last six rows of a data frame:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 或者 `tail` 来预览数据框的最后六行：
- en: '[PRE46]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Additionally, some operations will not work to extract columns or single values
    from the reference given by the `tbl` function. Thus, once you have finished your
    data wrangling of the `tbl` database reference object, it is advisable to bring
    it into R as a data frame using `collect`. But be very careful using `collect`:
    databases are often *very* big, and reading an entire table into R might take
    a long time to run or even possibly crash your machine. So make sure you use `filter`
    and `select` on the database table to reduce the data to a reasonable size before
    using `collect` to read it into R!'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些操作将无法从 `tbl` 函数提供的引用中提取列或单个值。因此，一旦您完成了对 `tbl` 数据库引用对象的整理，建议使用 `collect`
    将其作为数据框引入 R。但使用 `collect` 时要非常小心：数据库通常非常大，将整个表读入 R 可能需要很长时间运行，甚至可能使您的机器崩溃。因此，确保在使用
    `collect` 将其读入 R 之前，使用 `filter` 和 `select` 在数据库表上减少数据到合理的大小！
- en: 2.6.2 Reading data from a PostgreSQL database
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.2 从 PostgreSQL 数据库读取数据
- en: 'PostgreSQL (also called Postgres) is a very popular and open-source option
    for relational database software. Unlike SQLite, PostgreSQL uses a client–server
    database engine, as it was designed to be used and accessed on a network. This
    means that you have to provide more information to R when connecting to Postgres
    databases. The additional information that you need to include when you call the
    `dbConnect` function is listed below:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: PostgreSQL（也称为 Postgres）是关系型数据库软件的一个非常流行和开源的选择。与 SQLite 不同，PostgreSQL 使用客户端-服务器数据库引擎，因为它被设计用于在网络上使用和访问。这意味着您在连接到
    Postgres 数据库时必须提供更多信息。在调用 `dbConnect` 函数时需要包含的附加信息如下：
- en: '`dbname`: the name of the database (a single PostgreSQL instance can host more
    than one database)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dbname`: 数据库名称（单个 PostgreSQL 实例可以托管多个数据库）'
- en: '`host`: the URL pointing to where the database is located'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host`: 指向数据库位置的 URL'
- en: '`port`: the communication endpoint between R and the PostgreSQL database (usually
    `5432`)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`port`: R 和 PostgreSQL 数据库之间的通信端点（通常是 `5432`）'
- en: '`user`: the username for accessing the database'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user`: 访问数据库的用户名'
- en: '`password`: the password for accessing the database'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`password`: 访问数据库的密码'
- en: Additionally, we must use the `RPostgres` package instead of `RSQLite` in the
    `dbConnect` function call. Below we demonstrate how to connect to a version of
    the `can_mov_db` database, which contains information about Canadian movies. Note
    that the `host` (`fakeserver.stat.ubc.ca`), `user` (`user0001`), and `password`
    (`abc123`) below are *not real*; you will not actually be able to connect to a
    database using this information.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 `dbConnect` 函数调用中，我们必须使用 `RPostgres` 包而不是 `RSQLite`。以下我们演示如何连接到包含有关加拿大电影信息的
    `can_mov_db` 数据库版本。请注意，下面的 `host`（`fakeserver.stat.ubc.ca`）、`user`（`user0001`）和
    `password`（`abc123`）都不是真实的；您实际上无法使用这些信息连接到数据库。
- en: '[PRE48]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'After opening the connection, everything looks and behaves almost identically
    to when we were using an SQLite database in R. For example, we can again use `dbListTables`
    to find out what tables are in the `can_mov_db` database:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开连接后，一切看起来和表现几乎与我们在 R 中使用 SQLite 数据库时完全相同。例如，我们再次可以使用 `dbListTables` 来找出 `can_mov_db`
    数据库中有哪些表：
- en: '[PRE49]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We see that there are 10 tables in this database. Let’s first look at the `"ratings"`
    table to find the lowest rating that exists in the `can_mov_db` database:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这个数据库中有 10 个表。让我们首先查看 `"ratings"` 表，以找到在 `can_mov_db` 数据库中存在的最低评分：
- en: '[PRE51]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To find the lowest rating that exists in the data base, we first need to extract
    the `average_rating` column using `select`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到数据库中存在的最低评分，我们首先需要使用 `select` 提取 `average_rating` 列：
- en: '[PRE53]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next we use `min` to find the minimum rating in that column:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们使用 `min` 来找到该列中的最小评分：
- en: '[PRE55]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Instead of the minimum, we get an error! This is another example of when we
    need to use the `collect` function to bring the data into R for further computation:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 不是最小值，而是出现了一个错误！这是我们需要使用 `collect` 函数将数据带入 R 进行进一步计算的另一个例子：
- en: '[PRE57]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We see the lowest rating given to a movie is 1, indicating that it must have
    been a really bad movie…
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到给予电影的最低评分是 1，这表明它肯定是一部非常糟糕的电影…
- en: 2.6.3 Why should we bother with databases at all?
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.3 我们为什么要费心使用数据库呢？
- en: 'Opening a database involved a lot more effort than just opening a `.csv`, `.tsv`,
    or any of the other plain text or Excel formats. We had to open a connection to
    the database, then use `dbplyr` to translate `tidyverse`-like commands (`filter`,
    `select` etc.) into SQL commands that the database understands, and then finally
    `collect` the results. And not all `tidyverse` commands can currently be translated
    to work with databases. For example, we can compute a mean with a database but
    can’t easily compute a median. So you might be wondering: why should we use databases
    at all?'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 打开数据库比仅仅打开 `.csv`、`.tsv` 或其他任何纯文本或 Excel 格式要复杂得多。我们必须打开数据库的连接，然后使用 `dbplyr`
    将 `tidyverse` 类似的命令（`filter`、`select` 等）转换为数据库理解的 SQL 命令，最后最终 `collect` 结果。并且并非所有
    `tidyverse` 命令都可以当前转换为与数据库一起使用。例如，我们可以使用数据库计算平均值，但无法轻松计算中位数。所以你可能想知道：我们为什么要使用数据库呢？
- en: 'Databases are beneficial in a large-scale setting:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模环境中，数据库是有益的：
- en: They enable storing large data sets across multiple computers with backups.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们使得可以在多台计算机上存储大型数据集并进行备份。
- en: They provide mechanisms for ensuring data integrity and validating input.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们提供确保数据完整性和验证输入的机制。
- en: They provide security and data access control.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们提供安全和数据访问控制。
- en: They allow multiple users to access data simultaneously and remotely without
    conflicts and errors. For example, there are billions of Google searches conducted
    daily in 2021 ([Real Time Statistics Project 2021](#ref-googlesearches)). Can
    you imagine if Google stored all of the data from those searches in a single `.csv`
    file!? Chaos would ensue!
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们允许多个用户同时且远程访问数据，而不会发生冲突和错误。例如，2021 年每天有数十亿次 Google 搜索（[实时统计项目 2021](#ref-googlesearches)）。你能想象如果
    Google 将这些搜索的所有数据存储在一个单独的 `.csv` 文件中会发生什么吗？将会陷入混乱！
- en: 2.7 Writing data from R to a `.csv` file
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 将数据从 R 写入 `.csv` 文件
- en: 'At the middle and end of a data analysis, we often want to write a data frame
    that has changed (either through filtering, selecting, mutating or summarizing)
    to a file to share it with others or use it for another step in the analysis.
    The most straightforward way to do this is to use the `write_csv` function from
    the `tidyverse` package. The default arguments for this file are to use a comma
    (`,`) as the delimiter and include column names. Below we demonstrate creating
    a new version of the Canadian languages data set without the official languages
    category according to the Canadian 2016 Census, and then writing this to a `.csv`
    file:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析的中期和后期，我们经常想要将已更改的数据框（无论是通过过滤、选择、修改或汇总）写入文件，与他人分享或用于分析的另一个步骤。最直接的方法是使用
    `tidyverse` 包中的 `write_csv` 函数。此文件的默认参数是使用逗号（`,`）作为分隔符并包含列名。以下我们演示创建一个新版本的加拿大语言数据集，该数据集不包括根据加拿大
    2016 年人口普查的官方语言类别，然后将此写入 `.csv` 文件：
- en: '[PRE59]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 2.8 Obtaining data from the web
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 从网络获取数据
- en: '**Note:** This section is not required reading for the remainder of the textbook.
    It is included for those readers interested in learning a little bit more about
    how to obtain different types of data from the web.'
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**本节不是教科书其余部分所必需阅读的内容。它包括给那些对从网络获取不同类型数据感兴趣的学习者。'
- en: Data doesn’t just magically appear on your computer; you need to get it from
    somewhere. Earlier in the chapter we showed you how to access data stored in a
    plain text, spreadsheet-like format (e.g., comma- or tab-separated) from a web
    URL using one of the `read_*` functions from the `tidyverse`. But as time goes
    on, it is increasingly uncommon to find data (especially large amounts of data)
    in this format available for download from a URL. Instead, websites now often
    offer something known as an **a**pplication **p**rogramming **i**nterface (API),
    which provides a programmatic way to ask for subsets of a data set. This allows
    the website owner to control *who* has access to the data, *what portion* of the
    data they have access to, and *how much* data they can access. Typically, the
    website owner will give you a *token* or *key* (a secret string of characters
    somewhat like a password) that you have to provide when accessing the API.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并非神奇地出现在你的电脑上；你需要从某处获取它。在本章的早期部分，我们向您展示了如何使用`tidyverse`中的`read_*`函数之一从网络URL访问以纯文本、类似电子表格的格式（例如，逗号或制表符分隔）存储的数据。但随着时间的推移，越来越难以找到以这种格式可供从URL下载的数据（尤其是大量数据）。相反，网站现在通常提供一种称为**应用程序编程接口（API**）的东西，它提供了一种以编程方式请求数据集子集的方法。这允许网站所有者控制**谁**有权访问数据，他们可以访问数据的**哪一部分**，以及他们可以访问多少**数据**。通常，网站所有者会给你一个**令牌**或**密钥**（一个类似于密码的字符秘密字符串），你必须在访问API时提供。
- en: 'Another interesting thought: websites themselves *are* data! When you type
    a URL into your browser window, your browser asks the *web server* (another computer
    on the internet whose job it is to respond to requests for the website) to give
    it the website’s data, and then your browser translates that data into something
    you can see. If the website shows you some information that you’re interested
    in, you could *create* a data set for yourself by copying and pasting that information
    into a file. This process of taking information directly from what a website displays
    is called *web scraping* (or sometimes *screen scraping*). Now, of course, copying
    and pasting information manually is a painstaking and error-prone process, especially
    when there is a lot of information to gather. So instead of asking your browser
    to translate the information that the web server provides into something you can
    see, you can collect that data programmatically—in the form of **h**yper**t**ext
    **m**arkup **l**anguage (HTML) and **c**ascading **s**tyle **s**heet (CSS) code—and
    process it to extract useful information. HTML provides the basic structure of
    a site and tells the webpage how to display the content (e.g., titles, paragraphs,
    bullet lists etc.), whereas CSS helps style the content and tells the webpage
    how the HTML elements should be presented (e.g., colors, layouts, fonts etc.).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的思考：网站本身**就是**数据！当你将URL输入到浏览器窗口中时，你的浏览器会请求**网络服务器**（互联网上另一台计算机，其任务是响应对网站的请求）提供网站的数据，然后你的浏览器将那些数据转换成你可以看到的东西。如果你对网站显示的一些信息感兴趣，你可以通过将那些信息复制粘贴到文件中来**创建**自己的数据集。从网站显示的信息中直接获取信息的过程称为**网络爬取**（有时也称为**屏幕爬取**）。当然，手动复制粘贴信息是一个费时且容易出错的过程，尤其是当需要收集大量信息时。因此，你不必要求你的浏览器将网络服务器提供的信息转换成你可以看到的东西，你可以以**超文本标记语言（HTML**）和**层叠样式表（CSS**）代码的形式以编程方式收集这些数据，并对其进行处理以提取有用信息。HTML提供了网站的基本结构，并告诉网页如何显示内容（例如，标题、段落、项目符号列表等），而CSS帮助样式化内容，并告诉网页HTML元素应该如何呈现（例如，颜色、布局、字体等）。
- en: This subsection will show you the basics of both web scraping with the [`rvest`
    R package](https://rvest.tidyverse.org/) ([Wickham 2021a](#ref-rvest)) and accessing
    the NASA “Astronomy Picture of the Day” API using the [`httr2` R package](https://httr2.r-lib.org/)
    ([Wickham 2023](#ref-httr2)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节将向您展示使用[`rvest` R包](https://rvest.tidyverse.org/)（[Wickham 2021a](#ref-rvest)）进行网络爬取和利用`httr2`
    R包（[Wickham 2023](#ref-httr2)）访问NASA“每日天文图片”API的基本方法。
- en: 2.8.1 Web scraping
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.1 网络爬取
- en: HTML and CSS selectors
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HTML和CSS选择器
- en: When you enter a URL into your browser, your browser connects to the web server
    at that URL and asks for the *source code* for the website. This is the data that
    the browser translates into something you can see; so if we are going to create
    our own data by scraping a website, we have to first understand what that data
    looks like! For example, let’s say we are interested in knowing the average rental
    price (per square foot) of the most recently available one-bedroom apartments
    in Vancouver on [Craiglist](https://vancouver.craigslist.org). When we visit the
    Vancouver Craigslist website and search for one-bedroom apartments, we should
    see something similar to Figure [2.3](reading.html#fig:craigslist-human).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在浏览器中输入一个URL时，你的浏览器会连接到该URL的网页服务器，并请求网站的*源代码*。这是浏览器将其转换为你可以看到的数据；因此，如果我们打算通过抓取网站来创建自己的数据，我们必须首先了解这些数据看起来像什么！例如，假设我们感兴趣的是了解温哥华最近可用的单间公寓的平均租金（每平方英尺）。当我们访问温哥华克雷格列表网站并搜索单间公寓时，我们应该看到类似于图[2.3](reading.html#fig:craigslist-human)的内容。
- en: '![Craigslist webpage of advertisements for one-bedroom apartments.](../Images/fce9741a1b5a134651ced601de368d33.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![克雷格列表单间公寓广告网页](../Images/fce9741a1b5a134651ced601de368d33.png)'
- en: 'Figure 2.3: Craigslist webpage of advertisements for one-bedroom apartments.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：克雷格列表单间公寓广告网页。
- en: 'Based on what our browser shows us, it’s pretty easy to find the size and price
    for each apartment listed. But we would like to be able to obtain that information
    using R, without any manual human effort or copying and pasting. We do this by
    examining the *source code* that the web server actually sent our browser to display
    for us. We show a snippet of it below; the entire source is [included with the
    code for this book](https://github.com/UBC-DSCI/introduction-to-datascience/blob/main/img/reading/website_source.txt):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的浏览器显示的内容，找到每个公寓的大小和价格非常容易。但我们希望能够使用R获取这些信息，而不需要任何人工努力或复制粘贴。我们通过检查网页服务器实际发送给浏览器以显示给我们的*源代码*来实现这一点。下面我们展示了一段代码片段；整个源代码[包含在这本书的代码中](https://github.com/UBC-DSCI/introduction-to-datascience/blob/main/img/reading/website_source.txt)：
- en: '[PRE60]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Oof…you can tell that the source code for a web page is not really designed
    for humans to understand easily. However, if you look through it closely, you
    will find that the information we’re interested in is hidden among the muck. For
    example, near the top of the snippet above you can see a line that looks like
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀……你可以看出网页的源代码并不是为人类容易理解而设计的。然而，如果你仔细查看，你会发现我们感兴趣的信息隐藏在混乱之中。例如，在上面的代码片段顶部附近，你可以看到一行看起来像
- en: '[PRE61]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: That snippet is definitely storing the price of a particular apartment. With
    some more investigation, you should be able to find things like the date and time
    of the listing, the address of the listing, and more. So this source code most
    likely contains all the information we are interested in!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 那个片段肯定存储了一个特定公寓的价格。通过进一步的调查，你应该能够找到诸如列表的日期和时间、列表的地址等信息。因此，这段源代码很可能包含我们感兴趣的所有信息！
- en: 'Let’s dig into that line above a bit more. You can see that that bit of code
    has an *opening tag* (words between `<` and `>`, like `<span>`) and a *closing
    tag* (the same with a slash, like `</span>`). HTML source code generally stores
    its data between opening and closing tags like these. Tags are keywords that tell
    the web browser how to display or format the content. Above you can see that the
    information we want (`$800`) is stored between an opening and closing tag (`<span>`
    and `</span>`). In the opening tag, you can also see a very useful “class” (a
    special word that is sometimes included with opening tags): `class="result-price"`.
    Since we want R to programmatically sort through all of the source code for the
    website to find apartment prices, maybe we can look for all the tags with the
    `"result-price"` class, and grab the information between the opening and closing
    tag. Indeed, take a look at another line of the source snippet above:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地分析上面的那一行。你可以看到那段代码有一个*开始标签*（位于`<`和`>`之间的单词，例如`<span>`）和一个*结束标签*（与斜杠相同，例如`</span>`）。HTML源代码通常将这些数据存储在这些开始和结束标签之间。标签是关键词，告诉网页浏览器如何显示或格式化内容。在上面，你可以看到我们想要的信息（`$800`）存储在开始和结束标签（`<span>`和`</span>`）之间。在开始标签中，你还可以看到一个非常有用的“class”（有时与开始标签一起包含的特殊单词）：`class="result-price"`。由于我们希望R通过编程方式遍历网站的所有源代码以查找公寓价格，也许我们可以查找所有带有`"result-price"`类的标签，并获取开始和结束标签之间的信息。确实，看看上面源代码的另一行：
- en: '[PRE62]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: It’s yet another price for an apartment listing, and the tags surrounding it
    have the `"result-price"` class. Wonderful! Now that we know what pattern we are
    looking for—a dollar amount between opening and closing tags that have the `"result-price"`
    class—we should be able to use code to pull out all of the matching patterns from
    the source code to obtain our data. This sort of “pattern” is known as a *CSS
    selector* (where CSS stands for **c**ascading **s**tyle **s**heet).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是另一个公寓列表的价格，围绕它的标签具有`"result-price"`类。太棒了！现在我们知道了我们要寻找的模式——一个位于具有`"result-price"`类的开闭标签之间的金额——我们应该能够使用代码从源代码中提取所有匹配的模式以获取我们的数据。这种“模式”被称为*CSS选择器*（其中CSS代表**层叠**
    **s**tyle **s**heet）。
- en: The above was a simple example of “finding the pattern to look for”; many websites
    are quite a bit larger and more complex, and so is their website source code.
    Fortunately, there are tools available to make this process easier. For example,
    [SelectorGadget](https://selectorgadget.com/) is an open-source tool that simplifies
    identifying the generating and finding of CSS selectors. At the end of the chapter
    in the additional resources section, we include a link to a short video on how
    to install and use the SelectorGadget tool to obtain CSS selectors for use in
    web scraping. After installing and enabling the tool, you can click the website
    element for which you want an appropriate selector. For example, if we click the
    price of an apartment listing, we find that SelectorGadget shows us the selector
    `.result-price` in its toolbar, and highlights all the other apartment prices
    that would be obtained using that selector (Figure [2.4](reading.html#fig:sg1)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是一个“寻找要查找的模式”的简单例子；许多网站都相当大且复杂，它们的网站源代码也是如此。幸运的是，有一些工具可以使这个过程更容易。例如，[SelectorGadget](https://selectorgadget.com/)
    是一个开源工具，它简化了识别生成和查找CSS选择器的过程。在附录资源部分的章节末尾，我们包含了一个关于如何安装和使用SelectorGadget工具以获取用于网络爬取的CSS选择器的简短视频链接。安装并启用该工具后，您可以点击您想要获取适当选择器的网站元素。例如，如果我们点击公寓列表的价格，SelectorGadget在其工具栏中显示`.result-price`选择器，并突出显示使用该选择器可以获取的所有其他公寓价格（图2.4）。
- en: '![Using the SelectorGadget on a Craigslist webpage to obtain the CCS selector
    useful for obtaining apartment prices.](../Images/6bf9321fa7f33a1d701629a65a413364.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![在Craigslist网页上使用SelectorGadget以获取用于获取公寓价格的CCS选择器](../Images/6bf9321fa7f33a1d701629a65a413364.png)'
- en: 'Figure 2.4: Using the SelectorGadget on a Craigslist webpage to obtain the
    CCS selector useful for obtaining apartment prices.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：在Craigslist网页上使用SelectorGadget以获取用于获取公寓价格的CCS选择器。
- en: If we then click the size of an apartment listing, SelectorGadget shows us the
    `span` selector, and highlights many of the lines on the page; this indicates
    that the `span` selector is not specific enough to capture only apartment sizes
    (Figure [2.5](reading.html#fig:sg3)).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们然后点击公寓列表的大小，SelectorGadget显示`span`选择器，并突出显示页面上的许多行；这表明`span`选择器不够具体，无法仅捕获公寓大小（图2.5）。
- en: '![Using the SelectorGadget on a Craigslist webpage to obtain a CCS selector
    useful for obtaining apartment sizes.](../Images/f2bbd137a9a2a37135db206878c2f424.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![在Craigslist网页上使用SelectorGadget以获取用于获取公寓大小的CCS选择器](../Images/f2bbd137a9a2a37135db206878c2f424.png)'
- en: 'Figure 2.5: Using the SelectorGadget on a Craigslist webpage to obtain a CCS
    selector useful for obtaining apartment sizes.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：在Craigslist网页上使用SelectorGadget以获取用于获取公寓大小的CCS选择器。
- en: To narrow the selector, we can click one of the highlighted elements that we
    *do not* want. For example, we can deselect the “pic/map” links, resulting in
    only the data we want highlighted using the `.housing` selector (Figure [2.6](reading.html#fig:sg2)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 要缩小选择器，我们可以点击一个我们不想要的突出显示的元素。例如，我们可以取消选择“图片/地图”链接，结果只使用`.housing`选择器突出显示我们想要的数据（图2.6）。
- en: '![Using the SelectorGadget on a Craigslist webpage to refine the CCS selector
    to one that is most useful for obtaining apartment sizes.](../Images/823b84515ed7ca73018a23f0291e87fb.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![在Craigslist网页上使用SelectorGadget精炼CCS选择器，使其最适用于获取公寓大小](../Images/823b84515ed7ca73018a23f0291e87fb.png)'
- en: 'Figure 2.6: Using the SelectorGadget on a Craigslist webpage to refine the
    CCS selector to one that is most useful for obtaining apartment sizes.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：在Craigslist网页上使用SelectorGadget精炼CCS选择器，使其最适用于获取公寓大小。
- en: So to scrape information about the square footage and rental price of apartment
    listings, we need to use the two CSS selectors `.housing` and `.result-price`,
    respectively. The selector gadget returns them to us as a comma-separated list
    (here `.housing , .result-price`), which is exactly the format we need to provide
    to R if we are using more than one CSS selector.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了抓取关于公寓列表的面积和租金信息，我们需要分别使用两个CSS选择器`.housing`和`.result-price`。选择器小工具将它们作为逗号分隔的列表返回给我们（这里为`.housing
    , .result-price`），这正是我们需要提供给R的格式，如果我们使用多个CSS选择器的话。
- en: '**Caution: are you allowed to scrape that website?**'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：你是否有权抓取该网站？**'
- en: '*Before* scraping data from the web, you should always check whether or not
    you are *allowed* to scrape it! There are two documents that are important for
    this: the `robots.txt` file and the Terms of Service document. If we take a look
    at [Craigslist’s Terms of Service document](https://www.craigslist.org/about/terms.of.use),
    we find the following text: *“You agree not to copy/collect CL content via robots,
    spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g.,
    by hand).”* So unfortunately, without explicit permission, we are not allowed
    to scrape the website.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*在*抓取网页数据之前，你应该始终检查你是否*被允许*抓取它！有两个文件对此很重要：`robots.txt`文件和《服务条款》文档。如果我们查看[Craigslist的《服务条款》文档](https://www.craigslist.org/about/terms.of.use)，我们会发现以下文本：“*你同意不要通过机器人、蜘蛛、脚本、抓取器、爬虫或任何自动化或手动等效方式（例如，手工）复制/收集CL内容。”*所以很遗憾，没有明确的许可，我们不允许抓取该网站。'
- en: What to do now? Well, we *could* ask the owner of Craigslist for permission
    to scrape. However, we are not likely to get a response, and even if we did they
    would not likely give us permission. The more realistic answer is that we simply
    cannot scrape Craigslist. If we still want to find data about rental prices in
    Vancouver, we must go elsewhere. To continue learning how to scrape data from
    the web, let’s instead scrape data on the population of Canadian cities from Wikipedia.
    We have checked the [Terms of Service document](https://foundation.wikimedia.org/wiki/Terms_of_Use/en),
    and it does not mention that web scraping is disallowed. We will use the SelectorGadget
    tool to pick elements that we are interested in (city names and population counts)
    and deselect others to indicate that we are not interested in them (province names),
    as shown in Figure [2.7](reading.html#fig:sg4).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该怎么办？好吧，我们*可以*请求Craigslist的所有者允许我们抓取。然而，我们不太可能得到回复，即使我们得到了回复，他们也不太可能给我们许可。更现实的答案是，我们根本不能抓取Craigslist。如果我们仍然想找到关于温哥华的租金数据，我们必须去别处。为了继续学习如何从网络上抓取数据，让我们转而抓取来自维基百科的加拿大城市人口数据。我们已经检查了[《服务条款》文档](https://foundation.wikimedia.org/wiki/Terms_of_Use/en)，并且它没有提到禁止网络抓取。我们将使用SelectorGadget工具选择我们感兴趣的元素（城市名称和人口计数），并取消选择其他元素以表明我们不感兴趣（省份名称），如图[2.7](reading.html#fig:sg4)所示。
- en: '![Using the SelectorGadget on a Wikipedia webpage.](../Images/269904fe994aa7a23ceec98ef4e61929.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![在维基百科网页上使用SelectorGadget。](../Images/269904fe994aa7a23ceec98ef4e61929.png)'
- en: 'Figure 2.7: Using the SelectorGadget on a Wikipedia webpage.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：在维基百科网页上使用SelectorGadget。
- en: 'We include a link to a short video tutorial on this process at the end of the
    chapter in the additional resources section. SelectorGadget provides in its toolbar
    the following list of CSS selectors to use:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在章节末尾的附加资源部分提供了一个关于此过程的简短视频教程链接。SelectorGadget在其工具栏中提供了以下CSS选择器列表以供使用：
- en: '[PRE63]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now that we have the CSS selectors that describe the properties of the elements
    that we want to target, we can use them to find certain elements in web pages
    and extract data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了描述我们想要定位的元素属性的CSS选择器，我们可以使用它们在网页中找到特定的元素并提取数据。
- en: Using `rvest`
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用`rvest`
- en: 'We will use the `rvest` R package to scrape data from the Wikipedia page. We
    start by loading the `rvest` package:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`rvest` R包从维基百科页面抓取数据。我们首先加载`rvest`包：
- en: '[PRE64]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we tell R what page we want to scrape by providing the webpage’s URL
    in quotations to the function `read_html`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过向函数`read_html`提供网页的URL（用引号括起来）来告诉R我们想要抓取哪个页面：
- en: '[PRE65]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The `read_html` function directly downloads the source code for the page at
    the URL you specify, just like your browser would if you navigated to that site.
    But instead of displaying the website to you, the `read_html` function just returns
    the HTML source code itself, which we have stored in the `page` variable. Next,
    we send the page object to the `html_nodes` function, along with the CSS selectors
    we obtained from the SelectorGadget tool. Make sure to surround the selectors
    with quotation marks; the function, `html_nodes`, expects that argument is a string.
    We store the result of the `html_nodes` function in the `population_nodes` variable.
    Note that below we use the `paste` function with a comma separator (`sep=","`)
    to build the list of selectors. The `paste` function converts elements to characters
    and combines the values into a list. We use this function to build the list of
    selectors to maintain code readability; this avoids having a very long line of
    code.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_html` 函数直接下载您指定的 URL 页面的源代码，就像您导航到该网站时浏览器会做的那样。但与显示网站给您不同，`read_html`
    函数只返回 HTML 源代码本身，我们将其存储在 `page` 变量中。接下来，我们将页面对象发送到 `html_nodes` 函数，同时附带我们从 SelectorGadget
    工具获得的 CSS 选择器。确保将选择器用引号括起来；函数 `html_nodes` 预期该参数是一个字符串。我们将 `html_nodes` 函数的结果存储在
    `population_nodes` 变量中。注意，下面我们使用带有逗号分隔符（`sep=","`）的 `paste` 函数构建选择器列表。`paste`
    函数将元素转换为字符并将值组合成一个列表。我们使用此函数构建选择器列表以保持代码的可读性；这避免了代码行非常长的情况。'
- en: '[PRE66]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '**Note:** `head` is a function that is often useful for viewing only a short
    summary of an R object, rather than the whole thing (which may be quite a lot
    to look at). For example, here `head` shows us only the first 6 items in the `population_nodes`
    object. Note that some R objects by default print only a small summary. For example,
    `tibble` data frames only show you the first 10 rows. But not *all* R objects
    do this, and that’s where the `head` function helps summarize things for you.'
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** `head` 是一个常用于查看 R 对象的简短摘要而不是整个对象（可能查看起来相当多）的函数。例如，在这里 `head` 只显示了 `population_nodes`
    对象中的前 6 项。请注意，一些 R 对象默认只打印一个小摘要。例如，`tibble` 数据框只显示前 10 行。但并非所有 R 对象都这样做，这就是 `head`
    函数帮助您总结信息的地方。'
- en: 'Each of the items in the `population_nodes` list is a *node* from the HTML
    document that matches the CSS selectors you specified. A *node* is an HTML tag
    pair (e.g., `<td>` and `</td>` which defines the cell of a table) combined with
    the content stored between the tags. For our CSS selector `td:nth-child(4)`, an
    example node that would be selected would be:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`population_nodes` 列表中的每个项目都是来自 HTML 文档的节点，该节点与您指定的 CSS 选择器匹配。一个 *节点* 是一个 HTML
    标签对（例如，`<td>` 和 `</td>`，它定义了表格的单元格）与存储在标签之间的内容相结合。对于我们的 CSS 选择器 `td:nth-child(4)`，一个将被选中的示例节点将是：'
- en: '[PRE68]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Next we extract the meaningful data—in other words, we get rid of the HTML code
    syntax and tags—from the nodes using the `html_text` function. In the case of
    the example node above, `html_text` function returns `"London"`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `html_text` 函数从节点中提取有意义的数据——换句话说，我们通过该函数去除节点中的 HTML 代码语法和标签。在上述示例节点的情况下，`html_text`
    函数返回 `"London"`。
- en: '[PRE69]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Fantastic! We seem to have extracted the data of interest from the raw HTML
    source code. But we are not quite done; the data is not yet in an optimal format
    for data analysis. Both the city names and population are encoded as characters
    in a single vector, instead of being in a data frame with one character column
    for city and one numeric column for population (like a spreadsheet). Additionally,
    the populations contain commas (not useful for programmatically dealing with numbers),
    and some even contain a line break character at the end (`\n`). In Chapter [3](wrangling.html#wrangling),
    we will learn more about how to *wrangle* data such as this into a more useful
    format for data analysis using R.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们似乎已经从原始 HTML 源代码中提取了感兴趣的数据。但我们还没有完成；数据尚未以最佳格式进行数据分析。城市名称和人口都编码在一个单独的向量中，而不是在一个数据框中，其中有一个字符列用于城市，一个数值列用于人口（就像电子表格一样）。此外，人口中包含逗号（对程序化处理数字没有用），甚至有些在末尾包含换行符（`\n`）。在第
    [3](wrangling.html#wrangling) 章中，我们将学习更多关于如何使用 R 将此类数据 *整理* 成对数据分析更有用的格式的方法。
- en: 2.8.2 Using an API
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.2 使用 API
- en: Rather than posting a data file at a URL for you to download, many websites
    these days provide an API that must be accessed through a programming language
    like R. The benefit of using an API is that data owners have much more control
    over the data they provide to users. However, unlike web scraping, there is no
    consistent way to access an API across websites. Every website typically has its
    own API designed especially for its own use case. Therefore we will just provide
    one example of accessing data through an API in this book, with the hope that
    it gives you enough of a basic idea that you can learn how to use another API
    if needed. In particular, in this book we will show you the basics of how to use
    the `httr2` package in R to access data from the NASA “Astronomy Picture of the
    Day” API (a great source of desktop backgrounds, by the way—take a look at the
    stunning picture of the Rho-Ophiuchi cloud complex ([NASA et al. 2023](#ref-rhoophiuchi))
    in Figure [2.8](reading.html#fig:NASA-API-Rho-Ophiuchi) from July 13, 2023!).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将数据文件发布在URL上供您下载，如今许多网站提供必须通过编程语言如R访问的API。使用API的好处是数据所有者对其提供给用户的控制权更大。然而，与网络爬取不同，没有一种一致的方法可以跨网站访问API。每个网站通常都有自己的API，专门为其自己的用例设计。因此，我们在这本书中只提供一个通过API访问数据的例子，希望这能给您提供一个足够的基本概念，以便您在需要时学习如何使用另一个API。特别是，在这本书中，我们将向您展示如何使用R中的`httr2`包的基本方法来访问NASA“每日天文图片”API（顺便说一句，这是一个桌面背景的绝佳来源——看看2023年7月13日图[2.8](reading.html#fig:NASA-API-Rho-Ophiuchi)中令人惊叹的奥菲乌斯星云复合体的图片([NASA
    et al. 2023](#ref-rhoophiuchi))！）中的数据。
- en: '![The James Webb Space Telescope’s NIRCam image of the Rho Ophiuchi molecular
    cloud complex.](../Images/f6be3afe960e9cd751fbe8e84a464f01.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![詹姆斯·韦伯太空望远镜的NIRCam拍摄的奥菲乌斯分子云复合体图像。](../Images/f6be3afe960e9cd751fbe8e84a464f01.png)'
- en: 'Figure 2.8: The James Webb Space Telescope’s NIRCam image of the Rho Ophiuchi
    molecular cloud complex.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：詹姆斯·韦伯太空望远镜的NIRCam拍摄的奥菲乌斯分子云复合体图像。
- en: First, you will need to visit the [NASA APIs page](https://api.nasa.gov/) and
    generate an API key (i.e., a password used to identify you when accessing the
    API). Note that a valid email address is required to associate with the key. The
    signup form looks something like Figure [2.9](reading.html#fig:NASA-API-signup).
    After filling out the basic information, you will receive the token via email.
    Make sure to store the key in a safe place, and keep it private.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要访问[NASA API页面](https://api.nasa.gov/)并生成一个API密钥（即用于在访问API时识别您的密码）。请注意，需要一个有效的电子邮件地址与密钥关联。注册表单看起来像图[2.9](reading.html#fig:NASA-API-signup)。填写基本信息后，您将通过电子邮件收到令牌。请确保将密钥保存在安全的地方，并保持其私密性。
- en: '![Generating the API access token for the NASA API](../Images/f9d351d8fb5738968f95efbcdae55ebc.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![生成NASA API的访问令牌](../Images/f9d351d8fb5738968f95efbcdae55ebc.png)'
- en: 'Figure 2.9: Generating the API access token for the NASA API'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：生成NASA API的访问令牌
- en: '**Caution: think about your API usage carefully!**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：仔细考虑您的API使用情况！**'
- en: When you access an API, you are initiating a transfer of data from a web server
    to your computer. Web servers are expensive to run and do not have infinite resources.
    If you try to ask for *too much data* at once, you can use up a huge amount of
    the server’s bandwidth. If you try to ask for data *too frequently*—e.g., if you
    make many requests to the server in quick succession—you can also bog the server
    down and make it unable to talk to anyone else. Most servers have mechanisms to
    revoke your access if you are not careful, but you should try to prevent issues
    from happening in the first place by being extra careful with how you write and
    run your code. You should also keep in mind that when a website owner grants you
    API access, they also usually specify a limit (or *quota*) of how much data you
    can ask for. Be careful not to overrun your quota! So *before* we try to use the
    API, we will first visit [the NASA website](https://api.nasa.gov/) to see what
    limits we should abide by when using the API. These limits are outlined in Figure
    [2.10](reading.html#fig:NASA-API-limits).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当您访问API时，您正在从网络服务器向您的计算机传输数据。网络服务器运行成本高昂，且资源有限。如果您一次性请求*过多数据*，可能会消耗服务器的大量带宽。如果您尝试*过于频繁地*请求数据——例如，如果您在短时间内向服务器发送多个请求——您也可能使服务器过载，使其无法与其他人通信。大多数服务器都有机制在您不小心时撤销您的访问权限，但您应该通过格外小心地编写和运行代码来防止问题发生。您还应该记住，当网站所有者授予您API访问权限时，他们通常也会指定您可以请求的数据量限制（或*配额*）。务必注意不要超出配额！因此，在我们尝试使用API之前，我们首先访问[NASA网站](https://api.nasa.gov/)，看看在使用API时应遵守哪些限制。这些限制在图[2.10](reading.html#fig:NASA-API-limits)中概述。
- en: '![The NASA website specifies an hourly limit of 1,000 requests.](../Images/52a5a5613d26a12c2dd51cefdcd85827.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![NASA网站指定每小时请求限制为1,000次。](../Images/52a5a5613d26a12c2dd51cefdcd85827.png)'
- en: 'Figure 2.10: The NASA website specifies an hourly limit of 1,000 requests.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：NASA网站指定每小时请求限制为1,000次。
- en: After checking the NASA website, it seems like we can send at most 1,000 requests
    per hour. That should be more than enough for our purposes in this section.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查了NASA网站后，看起来我们每小时最多可以发送1,000个请求。这应该足以满足本节的目的。
- en: Accessing the NASA API
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 访问NASA API
- en: 'The NASA API is what is known as an *HTTP API*: this is a particularly common
    kind of API, where you can obtain data simply by accessing a particular URL as
    if it were a regular website. To make a query to the NASA API, we need to specify
    three things. First, we specify the URL *endpoint* of the API, which is simply
    a URL that helps the remote server understand which API you are trying to access.
    NASA offers a variety of APIs, each with its own endpoint; in the case of the
    NASA “Astronomy Picture of the Day” API, the URL endpoint is `https://api.nasa.gov/planetary/apod`.
    Second, we write `?`, which denotes that a list of *query parameters* will follow.
    And finally, we specify a list of query parameters of the form `parameter=value`,
    separated by `&` characters. The NASA “Astronomy Picture of the Day” API accepts
    the parameters shown in Figure [2.11](reading.html#fig:NASA-API-parameters).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 美国国家航空航天局（NASA）的API被称为*HTTP API*：这是一种特别常见的API类型，您可以通过访问特定的URL来获取数据，就像访问一个普通的网站一样。要对NASA
    API进行查询，我们需要指定三件事。首先，我们指定API的URL*端点*，这只是一个帮助远程服务器理解您试图访问哪个API的URL。NASA提供各种API，每个API都有自己的端点；在NASA的“每日天文图片”API的情况下，URL端点是`https://api.nasa.gov/planetary/apod`。其次，我们写`?`，表示将跟随一系列*查询参数*。最后，我们指定一系列以`parameter=value`形式的查询参数，这些参数由`&`字符分隔。NASA的“每日天文图片”API接受图[2.11](reading.html#fig:NASA-API-parameters)中显示的参数。
- en: '![The set of parameters that you can specify when querying the NASA "Astronomy
    Picture of the Day" API, along with syntax, default settings, and a description
    of each.](../Images/ac798b62914337d9da99f6715321b5f4.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![在查询NASA“每日天文图片”API时可以指定的参数集，包括语法、默认设置以及每个参数的描述。](../Images/ac798b62914337d9da99f6715321b5f4.png)'
- en: 'Figure 2.11: The set of parameters that you can specify when querying the NASA
    “Astronomy Picture of the Day” API, along with syntax, default settings, and a
    description of each.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：在查询NASA“每日天文图片”API时可以指定的参数集，包括语法、默认设置以及每个参数的描述。
- en: 'So for example, to obtain the image of the day from July 13, 2023, the API
    query would have two parameters: `api_key=YOUR_API_KEY` and `date=2023-07-13`.
    Remember to replace `YOUR_API_KEY` with the API key you received from NASA in
    your email! Putting it all together, the query will look like the following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要获取2023年7月13日的每日图片，API查询将有两个参数：`api_key=YOUR_API_KEY`和`date=2023-07-13`。记得用你在NASA电子邮件中收到的API密钥替换`YOUR_API_KEY`！将所有这些放在一起，查询将如下所示：
- en: '[PRE71]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'If you try putting this URL into your web browser, you’ll actually find that
    the server responds to your request with some text:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试将此URL放入你的网络浏览器中，你实际上会发现服务器会以一些文本响应你的请求：
- en: '[PRE72]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Neat! There is definitely some data there, but it’s a bit hard to see what
    it all is. As it turns out, this is a common format for data called *JSON* (JavaScript
    Object Notation). We won’t encounter this kind of data much in this book, but
    for now you can interpret this data as `key : value` pairs separated by commas.
    For example, if you look closely, you’ll see that the first entry is `"date":"2023-07-13"`,
    which indicates that we indeed successfully received data corresponding to July
    13, 2023.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '真棒！确实有一些数据，但看不太清楚。实际上，这是一种称为*JSON*（JavaScript Object Notation）的数据的常见格式。在这本书中，我们不会遇到很多这种类型的数据，但就目前而言，你可以将此数据解释为以逗号分隔的`key
    : value`对。例如，如果你仔细观察，你会看到第一条记录是`"date":"2023-07-13"`，这表明我们确实成功接收了与2023年7月13日相对应的数据。'
- en: So now our job is to do all of this programmatically in R. We will load the
    `httr2` package, and construct the query using the `request` function, which takes
    a single URL argument; you will recognize the same query URL that we pasted into
    the browser earlier. We will then send the query using the `req_perform` function,
    and finally obtain a JSON representation of the response using the `resp_body_json`
    function.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们的任务是使用R编程来完成所有这些工作。我们将加载`httr2`包，并使用`request`函数构建查询，该函数接受一个URL参数；你会认出我们之前粘贴到浏览器中的相同查询URL。然后我们将使用`req_perform`函数发送查询，并最终使用`resp_body_json`函数获取响应的JSON表示。
- en: '[PRE73]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We can obtain more records at once by using the `start_date` and `end_date`
    parameters, as shown in the table of parameters in [2.11](reading.html#fig:NASA-API-parameters).
    Let’s obtain all the records between May 1, 2023, and July 13, 2023, and store
    the result in an object called `nasa_data`; now the response will take the form
    of an R *list* (you’ll learn more about these in Chapter [3](wrangling.html#wrangling)).
    Each item in the list will correspond to a single day’s record (just like the
    `nasa_data_single` object), and there will be 74 items total, one for each day
    between the start and end dates:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`start_date`和`end_date`参数一次性获取更多记录，如[2.11](reading.html#fig:NASA-API-parameters)参数表所示。现在让我们获取2023年5月1日至2023年7月13日之间的所有记录，并将结果存储在一个名为`nasa_data`的对象中；现在响应将以R
    *列表*的形式出现（你将在第[3](wrangling.html#wrangling)章中了解更多关于这些内容）。列表中的每一项将对应于一天的数据记录（就像`nasa_data_single`对象一样），总共有74项，对应于起始日期和结束日期之间的每一天：
- en: '[PRE75]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: For further data processing using the techniques in this book, you’ll need to
    turn this list of items into a data frame. Here we will extract the `date`, `title`,
    `copyright`, and `url` variables from the JSON data, and construct a data frame
    using the extracted information.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步使用本书中的技术进行数据处理，你需要将这个项目列表转换成数据框。在这里，我们将从JSON数据中提取`date`、`title`、`copyright`和`url`变量，并使用提取的信息构建数据框。
- en: '**Note:** Understanding this code is not required for the remainder of the
    textbook. It is included for those readers who would like to parse JSON data into
    a data frame in their own data analyses.'
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**理解这段代码对于本书剩余部分不是必需的。它包含在这里是为了那些希望在他们的数据分析中将JSON数据解析为数据框的读者。'
- en: '[PRE77]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Success—we have created a small data set using the NASA API! This data is also
    quite different from what we obtained from web scraping; the extracted information
    is readily available in a JSON format, as opposed to raw HTML code (although not
    *every* API will provide data in such a nice format). From this point onward,
    the `nasa_df` data frame is stored on your machine, and you can play with it to
    your heart’s content. For example, you can use `write_csv` to save it to a file
    and `read_csv` to read it into R again later; and after reading the next few chapters
    you will have the skills to do even more interesting things! If you decide that
    you want to ask any of the various NASA APIs for more data (see [the list of awesome
    NASA APIS here](https://api.nasa.gov/) for more examples of what is possible),
    just be mindful as usual about how much data you are requesting and how frequently
    you are making requests.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 成功——我们已经使用 NASA API 创建了一个小数据集！这些数据与我们从网络爬取得到的数据也相当不同；提取的信息以 JSON 格式直接可用，而不是原始的
    HTML 代码（尽管并非 *所有* API 都会以这种令人愉悦的格式提供数据）。从现在开始，`nasa_df` 数据框将存储在你的机器上，你可以随心所欲地玩弄它。例如，你可以使用
    `write_csv` 将其保存到文件，并使用 `read_csv` 在稍后将其读入 R；在阅读了接下来的几章后，你将拥有进行更多有趣操作的能力！如果你决定想要从各种
    NASA API 中请求更多数据（有关更多示例，请参阅[这里](https://api.nasa.gov/)），请像往常一样注意你请求的数据量以及你请求的频率。
- en: 2.9 Exercises
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 练习
- en: Practice exercises for the material covered in this chapter can be found in
    the accompanying [worksheets repository](https://worksheets.datasciencebook.ca)
    in the “Reading in data locally and from the web” row. You can launch an interactive
    version of the worksheet in your browser by clicking the “launch binder” button.
    You can also preview a non-interactive version of the worksheet by clicking “view
    worksheet.” If you instead decide to download the worksheet and run it on your
    own machine, make sure to follow the instructions for computer setup found in
    Chapter [13](setup.html#setup). This will ensure that the automated feedback and
    guidance that the worksheets provide will function as intended.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的练习可以在配套的 [worksheets 仓库](https://worksheets.datasciencebook.ca) 中找到，位于“从本地和网络上读取数据”行。你可以通过点击“启动
    binder”按钮在你的浏览器中启动工作表的交互式版本。你也可以通过点击“查看工作表”预览非交互式版本的工作表。如果你决定下载工作表并在自己的机器上运行它，请确保遵循第
    [13](setup.html#setup) 章中找到的计算机设置说明。这将确保工作表提供的自动反馈和指导能够按预期工作。
- en: 2.10 Additional resources
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.10 其他资源
- en: The [`readr` documentation](https://readr.tidyverse.org/) provides the documentation
    for many of the reading functions we cover in this chapter. It is where you should
    look if you want to learn more about the functions in this chapter, the full set
    of arguments you can use, and other related functions. The site also provides
    a very nice cheat sheet that summarizes many of the data wrangling functions from
    this chapter.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`readr` 文档](https://readr.tidyverse.org/) 提供了本章中我们涵盖的许多读取函数的文档。如果你想了解更多关于本章中函数的信息、你可以使用的完整参数集以及其他相关函数，你应该查看这个网站。该网站还提供了一个非常棒的速查表，总结了本章中的许多数据处理函数。'
- en: Sometimes you might run into data in such poor shape that none of the reading
    functions we cover in this chapter work. In that case, you can consult the [data
    import chapter](https://r4ds.had.co.nz/data-import.html) from *R for Data Science*
    ([Wickham and Grolemund 2016](#ref-wickham2016r)), which goes into a lot more
    detail about how R parses text from files into data frames.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时候你可能会遇到数据状况非常糟糕的情况，以至于本章中我们涵盖的任何读取函数都无法工作。在这种情况下，你可以参考 *R for Data Science*
    中的 [数据导入章节](https://r4ds.had.co.nz/data-import.html) ([Wickham and Grolemund 2016](#ref-wickham2016r))，它详细介绍了
    R 如何将文件中的文本解析成数据框。
- en: The [`here` R package](https://here.r-lib.org/) ([Müller 2020](#ref-here)) provides
    a way for you to construct or find your files’ paths.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`here` R 包](https://here.r-lib.org/) ([Müller 2020](#ref-here)) 为你提供了构建或查找文件路径的方法。'
- en: The [`readxl` documentation](https://readxl.tidyverse.org/) provides more details
    on reading data from Excel, such as reading in data with multiple sheets, or specifying
    the cells to read in.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`readxl` 文档](https://readxl.tidyverse.org/) 提供了关于从 Excel 读取数据的更多详细信息，例如读取具有多个工作表的或指定要读取的单元格的数据。'
- en: The [`rio` R package](https://github.com/leeper/rio) ([Leeper 2021](#ref-rio))
    provides an alternative set of tools for reading and writing data in R. It aims
    to be a “Swiss army knife” for data reading/writing/converting, and supports a
    wide variety of data types (including data formats generated by other statistical
    software like SPSS and SAS).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[rio R包](https://github.com/leeper/rio) ([Leeper 2021](#ref-rio)) 提供了一套用于在R中读取和写入数据的替代工具。它旨在成为数据读取/写入/转换的“瑞士军刀”，并支持多种数据类型（包括由其他统计软件（如SPSS和SAS）生成的数据格式）。'
- en: A [video](https://www.youtube.com/embed/ephId3mYu9o) from the Udacity course
    *Linux Command Line Basics* provides a good explanation of absolute versus relative
    paths.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Udacity课程 *Linux命令行基础* 的一个[视频](https://www.youtube.com/embed/ephId3mYu9o)对绝对路径与相对路径进行了很好的解释。
- en: 'If you read the subsection on obtaining data from the web via scraping and
    APIs, we provide two companion tutorial video links for how to use the SelectorGadget
    tool to obtain desired CSS selectors for:'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你阅读了关于通过抓取和API从网络获取数据的子节，我们提供了两个配套教程视频链接，介绍如何使用SelectorGadget工具获取所需的CSS选择器：
- en: '[extracting the data for apartment listings on Craigslist](https://www.youtube.com/embed/YdIWI6K64zo),
    and'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从Craigslist提取公寓列表数据](https://www.youtube.com/embed/YdIWI6K64zo)，以及'
- en: '[extracting Canadian city names and populations from Wikipedia](https://www.youtube.com/embed/O9HKbdhqYzk).'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从维基百科提取加拿大城市名称和人口](https://www.youtube.com/embed/O9HKbdhqYzk)。'
- en: The [`polite` R package](https://dmi3kno.github.io/polite/) ([Perepolkin 2021](#ref-polite))
    provides a set of tools for responsibly scraping data from websites.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[礼貌的R包](https://dmi3kno.github.io/polite/) ([Perepolkin 2021](#ref-polite))
    提供了一套从网站负责任地抓取数据的工具。'
- en: References
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Leeper, Thomas. 2021\. *rio R package*. [https://cloud.r-project.org/web/packages/rio/index.html](https://cloud.r-project.org/web/packages/rio/index.html).Müller,
    Kirill. 2020\. *here R package*. [https://here.r-lib.org/](https://here.r-lib.org/).NASA,
    ESA, CSA, STScI, K. Pontoppidan (STScI), and A. Pagan (STScI). 2023\. “Rho Ophiuchi
    Cloud Complex.” *URL: Https://Esawebb.org/Images/Weic2316a/*.Perepolkin, Dmytro.
    2021\. *polite R package*. [https://dmi3kno.github.io/polite/](https://dmi3kno.github.io/polite/).Real
    Time Statistics Project. 2021\. “Internet Live Stats: Google Search Statistics.”
    [https://www.internetlivestats.com/google-search-statistics/](https://www.internetlivestats.com/google-search-statistics/).———.
    2021a. *rvest R package*. [https://rvest.tidyverse.org/](https://rvest.tidyverse.org/).———.
    2023\. *Httr2: Perform HTTP Requests and Process the Responses*. [https://httr2.r-lib.org](https://httr2.r-lib.org).Wickham,
    Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021\. *dplyr R package*.
    [https://dplyr.tidyverse.org/](https://dplyr.tidyverse.org/).Wickham, Hadley,
    and Garrett Grolemund. 2016\. *R for Data Science: Import, Tidy, Transform, Visualize,
    and Model Data*. O’Reilly. [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/).'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 'Leeper, Thomas. 2021\. *rio R包*. [https://cloud.r-project.org/web/packages/rio/index.html](https://cloud.r-project.org/web/packages/rio/index.html).Müller,
    Kirill. 2020\. *here R包*. [https://here.r-lib.org/](https://here.r-lib.org/).NASA,
    ESA, CSA, STScI, K. Pontoppidan (STScI), and A. Pagan (STScI). 2023\. “Rho Ophiuchi
    Cloud Complex.” *URL: Https://Esawebb.org/Images/Weic2316a/*.Perepolkin, Dmytro.
    2021\. *polite R包*. [https://dmi3kno.github.io/polite/](https://dmi3kno.github.io/polite/).Real
    Time Statistics Project. 2021\. “Internet Live Stats: Google Search Statistics.”
    [https://www.internetlivestats.com/google-search-statistics/](https://www.internetlivestats.com/google-search-statistics/).———.
    2021a. *rvest R包*. [https://rvest.tidyverse.org/](https://rvest.tidyverse.org/).———.
    2023\. *Httr2: Perform HTTP Requests and Process the Responses*. [https://httr2.r-lib.org](https://httr2.r-lib.org).Wickham,
    Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021\. *dplyr R包*. [https://dplyr.tidyverse.org/](https://dplyr.tidyverse.org/).Wickham,
    Hadley, and Garrett Grolemund. 2016\. *R for Data Science: Import, Tidy, Transform,
    Visualize, and Model Data*. O’Reilly. [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/).'
