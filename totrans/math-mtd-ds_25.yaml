- en: 3.8\. Online supplementary materials#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap03_opt/supp/roch-mmids-opt-supp.html](https://mmids-textbook.github.io/chap03_opt/supp/roch-mmids-opt-supp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_opt_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.1:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(x_1, x_2) &= \left(\frac{\partial f}{\partial x_1},
    \frac{\partial f}{\partial x_2}\right) \\ &= (6x_1 - 2x_2 - 5, -2x_1 + 8x_2 +
    2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((1, -1)\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(1, -1) = (6(1) - 2(-1) - 5, -2(1) + 8(-1) + 2) = (3, -8). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.3:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f}{\partial x_1} &= \cos(x_1) \cos(x_2), \\
    \frac{\partial f}{\partial x_2} &= -\sin(x_1) \sin(x_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((\frac{\pi}{4}, \frac{\pi}{3})\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f}{\partial x_1}(\frac{\pi}{4}, \frac{\pi}{3})
    &= \cos(\frac{\pi}{4}) \cos(\frac{\pi}{3}) = \frac{\sqrt{2}}{2} \cdot \frac{1}{2}
    = \frac{\sqrt{2}}{4}, \\ \frac{\partial f}{\partial x_2}(\frac{\pi}{4}, \frac{\pi}{3})
    &= -\sin(\frac{\pi}{4}) \sin(\frac{\pi}{3}) = -\frac{\sqrt{2}}{2} \cdot \frac{\sqrt{3}}{2}
    = -\frac{\sqrt{6}}{4}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.5: The Hessian matrix of \(f(x_1, x_2)\)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 6x_1 & 6x_2 \\ 6x_2
    & 6x_1 - 12x_2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((1, 2)\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(1, 2) = \begin{pmatrix} 6 & 12 \\ 12 & -18 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We can see that \(\frac{\partial^2 f}{\partial x_1 \partial x_2}(1, 2) = \frac{\partial^2
    f}{\partial x_2 \partial x_1}(1, 2) = 12\), confirming the Symmetry of the Hessian
    Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.7:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial^2 f}{\partial x_1^2} &= 2 \sin(x_2), \\ \frac{\partial^2
    f}{\partial x_1 \partial x_2} &= 2x_1 \cos(x_2), \\ \frac{\partial^2 f}{\partial
    x_2 \partial x_1} &= 2x_1 \cos(x_2), \\ \frac{\partial^2 f}{\partial x_2^2} &=
    -x_1^2 \sin(x_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.9: The Hessian matrix is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2, x_3) = \begin{pmatrix} 2 & -2 & 4 \\
    -2 & 4 & -6 \\ 4 & -6 & 6 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.11: \(\frac{\partial f}{\partial x} = 3x^2y^2
    - 2y^3\) and \(\frac{\partial f}{\partial y} = 2x^3y - 6xy^2 + 4y^3\), obtained
    by differentiating \(f\) with respect to each variable while holding the other
    constant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.13: The Hessian matrix is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_g(x, y) = \begin{pmatrix} -\sin(x) \cos(y) & -\cos(x)
    \sin(y) \\ -\cos(x) \sin(y) & -\sin(x) \cos(y) \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.15: \(\frac{\partial^2 q}{\partial x^2} =
    6x\) and \(\frac{\partial^2 q}{\partial y^2} = -6x\). Adding these gives \(6x
    - 6x = 0\), so \(q\) satisfies Laplace’s equation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.17: By the chain rule, the rate of change
    of temperature experienced by the particle is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{d}{dt}u(\mathbf{c}(t)) = \nabla u(\mathbf{c}(t))^T \mathbf{c}'(t).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We have \(\nabla u(x, y) = (-2xe^{-x^2 - y^2}, -2ye^{-x^2 - y^2})\) and \(\mathbf{c}'(t)
    = (2t, 3t^2)\). Evaluating at \(t = 1\) gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{d}{dt}u(\mathbf{c}(1)) = (-2e^{-2}, -2e^{-2})^T (2, 3) = -10e^{-2}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.19: \(\frac{d}{dt} f(\mathbf{g}(t)) = 2t
    \cos t - t^2 \sin t\). Justification: \(\nabla f = (y, x)\), and \(\mathbf{g}''(t)
    = (2t, -\sin t)\). Then, \(\frac{d}{dt} f(\mathbf{g}(t)) = \cos t \cdot 2t + t^2
    \cdot (-\sin t)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.1: \(\nabla f(x_1, x_2) = (2x_1, 4x_2)\).
    Setting this equal to zero yields \(2x_1 = 0\) and \(4x_2 = 0\), which implies
    \(x_1 = 0\) and \(x_2 = 0\). Thus, the only point where \(\nabla f(x_1, x_2) =
    0\) is \((0, 0)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.3: The second directional derivative is given
    by \(\frac{\partial^2 f(\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0)
    \mathbf{v}\). We have \(\mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 2 & 2 \\ 2 &
    2 \end{pmatrix}\). Thus,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \frac{\partial^2 f(1, 1)}{\partial \mathbf{v}^2} = (\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}})^T \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix} (\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}}) = \frac{1}{2}(2 + 2 + 2 + 2) = 4. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.5: The first-order necessary conditions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla_{x_1, x_2} L(x_1, x_2, \lambda) &= 0, \\ h(x_1, x_2)
    &= 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial L}{\partial x_1} &= 2x_1 + \lambda = 0, \\ \frac{\partial
    L}{\partial x_2} &= 2x_2 + \lambda = 0, \\ x_1 + x_2 &= 1. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first two equations, we have \(x_1 = x_2 = -\frac{\lambda}{2}\). Substituting
    into the third equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ -\frac{\lambda}{2} - \frac{\lambda}{2} = 1 \implies \lambda = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(x_1 = x_2 = \frac{1}{2}\), and the only point satisfying the first-order
    necessary conditions is \((\frac{1}{2}, \frac{1}{2}, -1)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.7: The Lagrangian is \(L(x_1, x_2, x_3, \lambda)
    = x_1^2 + x_2^2 + x_3^2 + \lambda(x_1 + 2x_2 + 3x_3 - 6)\). The first-order necessary
    conditions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} 2x_1 + \lambda &= 0, \\ 2x_2 + 2\lambda &= 0, \\ 2x_3 + 3\lambda
    &= 0, \\ x_1 + 2x_2 + 3x_3 &= 6. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first three equations, we have \(x_1 = -\frac{\lambda}{2}\), \(x_2
    = -\lambda\), \(x_3 = -\frac{3\lambda}{2}\). Substituting into the fourth equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ -\frac{\lambda}{2} - 2\lambda - \frac{9\lambda}{2} = 6 \implies -6\lambda
    = 6 \implies \lambda = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(x_1 = \frac{1}{2}\), \(x_2 = 1\), \(x_3 = \frac{3}{2}\), and the only
    point satisfying the first-order necessary conditions is \((\frac{1}{2}, 1, \frac{3}{2},
    -1)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.9: The gradient of \(f\) is \(\nabla f(x_1,
    x_2) = (3x_1^2 - 3x_2^2, -6x_1x_2)\). At the point \((1, 0)\), the gradient is
    \(\nabla f(1, 0) = (3, 0)\). The directional derivative of \(f\) at \((1, 0)\)
    in the direction \(\mathbf{v} = (1, 1)\) is \(\nabla f(1, 0)^T \mathbf{v} = (3,
    0)^T (1, 1) = 3\). Since this is positive, \(v\) is not a descent direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.11: The Hessian matrix of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{bmatrix} -2 & 0 \\ 0 & -2 \end{bmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the second directional derivative of \(f\) at \((0, 0)\) in the direction
    \(v = (1, 0)\) is \(\mathbf{v}^T \mathbf{H}_f(0, 0) \mathbf{v} = (1, 0) \begin{bmatrix}
    -2 & 0 \\ 0 & -2 \end{bmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = -2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.13: The Hessian matrix is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(1,1) = \begin{pmatrix} 6 & -3 \\ -3 & 6 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Justification: Compute the second partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f}{\partial x^2} = 6x, \quad \frac{\partial^2 f}{\partial
    y^2} = 6y, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2
    f}{\partial y \partial x} = -3. \]
  prefs: []
  type: TYPE_NORMAL
- en: At \((1,1)\), these values are \(6\), \(6\), and \(-3\), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.1: The convex combination is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(2, 3) + \alpha(4, 5) = 0.7(2, 3) + 0.3(4, 5) = (0.7 \cdot 2
    + 0.3 \cdot 4, 0.7 \cdot 3 + 0.3 \cdot 5) = (2.6, 3.6). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.3: \(S_1\) and \(S_2\) are halfspaces, which
    are convex sets. By the lemma in the text, the intersection of convex sets is
    also convex. Therefore, \(S_1 \cap S_2\) is a convex set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.5: The function \(f\) is a quadratic function
    with \(P = 2\), \(q = 2\), and \(r = 1\). Since \(P > 0\), \(f\) is strictly convex.
    The unique global minimizer is found by setting the gradient to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(x) = 2x + 2 = 0 \implies x^* = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.7: The Hessian matrix of \(f\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'For any \((x, y) \in \mathbb{R}^2\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla^2 f(x, y) \succeq 2I_{2 \times 2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: so \(f\) is strongly convex with \(m = 2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.9: \(f\) is not convex. We have \(f''''(x)
    = 12x^2 - 4\), which is negative for \(x \in (-\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}})\).
    Since the second derivative is not always nonnegative, \(f\) is not convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.11: \(f\) is strongly convex. We have \(f''''(x)
    = 2 > 0\), so \(f\) is 2-strongly convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.13: \(D\) is convex. To show this, let \((x_1,
    y_1), (x_2, y_2) \in D\) and \(\alpha \in (0, 1)\). We need to show that \((1
    - \alpha)(x_1, y_1) + \alpha(x_2, y_2) \in D\). Compute:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) = ((1 - \alpha)x_1 + \alpha x_2,
    (1 - \alpha)y_1 + \alpha y_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: Now,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ((1 - \alpha)x_1 + \alpha x_2)^2 + ((1 - \alpha)y_1 + \alpha y_2)^2 < (1
    - \alpha)(x_1^2 + y_1^2) + \alpha(x_2^2 + y_2^2) < 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(D\) is convex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.15: \(D\) is not convex. For example, let
    \((x_1, y_1) = (2, \sqrt{3})\) and \((x_2, y_2) = (2, -\sqrt{3})\), both of which
    are in \(D\). For \(\alpha = \frac{1}{2}\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(2, \sqrt{3}) + \alpha(2, -\sqrt{3}) = \left(2, 0\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: Now,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left(2\right)^2 - \left(0\right)^2 = 4 > 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.1: The gradient is \(\nabla f(x, y) = (2x,
    8y)\). At \((1, 1)\), it is \((2, 8)\). The direction of steepest descent is \(-\nabla
    f(1, 1) = (-2, -8)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.3: \(\nabla f(x) = 3x^2 - 12x + 9\). At \(x_0
    = 0\), \(\nabla f(0) = 9\). The first iteration gives \(x_1 = x_0 - \alpha \nabla
    f(x_0) = 0 - 0.1 \cdot 9 = -0.9\). At \(x_1 = -0.9\), \(\nabla f(-0.9) = 3 \cdot
    (-0.9)^2 - 12 \cdot (-0.9) + 9 = 2.43 + 10.8 + 9 = 22.23\). The second iteration
    gives \(x_2 = x_1 - \alpha \nabla f(x_1) = -0.9 - 0.1 \cdot 22.23 = -3.123\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.5: We have \(\nabla f(x) = 2x\), so \(\nabla
    f(2) = 4\). Thus, the gradient descent update is \(x_1 = x_0 - \alpha \nabla f(x_0)
    = 2 - 0.1 \cdot 4 = 1.6\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.7: \(f''''(x) = 4\) for all \(x \in \mathbb{R}\).
    Therefore, \(f''''(x) \geq 4\) for all \(x \in \mathbb{R}\), which implies that
    \(f\) is \(4\)-strongly convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.9: No. We have \(f''''(x) = 12x^2\), which
    can be arbitrarily large as \(x\) increases. Thus, there is no constant \(L\)
    such that \(-L \le f''''(x) \le L\) for all \(x \in \mathbb{R}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.11: We have \(f''''(x) = 2\) for all \(x
    \in \mathbb{R}\). Thus, we can take \(m = 2\), and we have \(f''''(x) \ge 2\)
    for all \(x \in \mathbb{R}\), which is the condition for \(m\)-strong convexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.13: The gradient is \(\nabla f(x, y) = (2x,
    2y)\). At \((1, 1)\), it is \((2, 2)\). The first update is \((0.8, 0.8)\). The
    gradient at \((0.8, 0.8)\) is \((1.6, 1.6)\). The second update is \((0.64, 0.64)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.1: The log-odds is given by \(\log \frac{p}{1-p}
    = \log \frac{0.25}{0.75} = \log \frac{1}{3} = -\log 3\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.3:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \boldsymbol{\alpha}^T \mathbf{x} = (-0.2 \cdot 1) + (0.4 \cdot 3) = -0.2
    + 1.2 = 1.0 \]\[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(1.0) = \frac{1}{1
    + e^{-1}} \approx 0.731 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.5: By the quotient rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sigma'(z) &= \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 +
    e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\ &= \sigma(z) (1 - \sigma(z)). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.7: We have \(b_1 - \sigma(\boldsymbol{\alpha}_1^T
    \mathbf{x}) \approx 0.05\), \(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x})
    \approx -0.73\), \(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}) \approx 0.73\).
    Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla \ell(\mathbf{x}; A, b) &= -\frac{1}{3} \sum_{i=1}^3
    (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \boldsymbol{\alpha}_i \\ &\approx
    -\frac{1}{3} \{(0.05)(1, 2) + (-0.73)(-1, 1) + 0.73(0, -1)\}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.9: We have \(b_1 - \sigma(\boldsymbol{\alpha}_1^T
    \mathbf{x}^0) = 1 - \sigma(0) = 0.5\), \(b_2 - \sigma(\boldsymbol{\alpha}_2^T
    \mathbf{x}^0) = -0.5\), \(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}^0) =
    0.5\). Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{x}^1 &= \mathbf{x}^0 - \beta \nabla \ell(\mathbf{x}^0;
    A, \mathbf{b}) \\ &= (0, 0) + \frac{0.1}{3} \{0.5(1, 2) + (-0.5)(-1, 1) + 0.5(0,
    -1)\} \\ &= (0.1, 0.05). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define and calculate partial derivatives and gradients for functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute second-order partial derivatives and construct the Hessian matrix for
    twice continuously differentiable functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Chain Rule to compute derivatives of composite functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the multivariable version of the Mean Value Theorem using the
    Chain Rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate gradients and Hessians for affine and quadratic functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define global and local minimizers for unconstrained optimization problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the first-order necessary conditions for a local minimizer using the
    gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of a descent direction and its relation to the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of directional derivatives and compute directional derivatives
    using the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the second-order necessary and sufficient conditions for a local minimizer
    using the Hessian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the gradient and Hessian matrix for a given multivariable function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate optimization problems with equality constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the method of Lagrange multipliers to derive the first-order necessary
    conditions for a constrained local minimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the second-order sufficient conditions for a constrained local minimizer
    using the Hessian matrix and Lagrange multipliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the regularity condition in the context of constrained optimization
    problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a constrained optimization problem by finding points that satisfy the
    first-order necessary conditions and checking the second-order sufficient conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define convex sets and convex functions, and provide examples of each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify operations that preserve convexity of sets and functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characterize convex functions using the first-order convexity condition based
    on the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the convexity of a function using the second-order convexity condition
    based on the Hessian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between convexity and optimization, particularly how
    local minimizers of convex functions are also global minimizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the first-order optimality condition for convex functions on
    R^d and on convex sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define strong convexity and its implications for the existence and uniqueness
    of global minimizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the convexity and strong convexity of quadratic functions and least-squares
    objectives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the concept of convexity to solve optimization problems, such as finding
    the projection of a point onto a convex set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define gradient descent and explain its motivation as a numerical optimization
    method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the negative gradient is the steepest descent direction for a continuously
    differentiable function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the convergence of gradient descent for smooth functions, proving that
    it produces a sequence of points with decreasing objective values and vanishing
    gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the convergence rate of gradient descent for smooth functions in terms
    of the number of iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define strong convexity for twice continuously differentiable functions and
    relate it to the function value and gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove faster convergence rates for gradient descent when applied to smooth and
    strongly convex functions, showing exponential convergence to the global minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement gradient descent in Python and apply it to simple examples to illustrate
    the theoretical convergence results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the role of the sigmoid function in transforming a linear function of
    features into a probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the gradient and Hessian of the logistic regression objective function
    (cross-entropy loss).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the logistic regression objective function is convex and smooth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement gradient descent to minimize the logistic regression objective function
    in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply logistic regression to real-world datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3.8.2.1\. Logistic regression: illustration of convergence result[#](#logistic-regression-illustration-of-convergence-result
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We return to our proof of convergence for smooth functions using a special case
    of logistic regression. We first define the functions \(\hat{f}\), \(\mathcal{L}\)
    and \(\frac{\partial}{\partial x}\mathcal{L}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We illustrate GD on a random dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We plot the upper and lower bounds in the *Quadratic Bound for Smooth Functions*
    around \(x = x_0\). It turns out we can take \(L=1\) because all features are
    uniformly random between \(-1\) and \(1\). Observe that minimizing the upper quadratic
    bound leads to a decrease in \(\mathcal{L}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png](../Images/097a341c91c4cf50443a8e9f81f433b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '3.8.2.2\. Logistic regression: another dataset[#](#logistic-regression-another-dataset
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that to run gradient descent, we first implement a function computing
    a descent update. It takes as input a function `grad_fn` computing the gradient
    itself, as well as a current iterate and a step size. We now also feed a dataset
    as additional input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We analyze with a simple dataset from UC Berkeley’s [DS100](http://www.ds100.org)
    course. The file `lebron.csv` is available [here](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets).
    Quoting a previous version of the course’s textbook:'
  prefs: []
  type: TYPE_NORMAL
- en: In basketball, players score by shooting a ball through a hoop. One such player,
    LeBron James, is widely considered one of the best basketball players ever for
    his incredible ability to score. LeBron plays in the National Basketball Association
    (NBA), the United States’s premier basketball league. We’ve collected a dataset
    of all of LeBron’s attempts in the 2017 NBA Playoff Games using the NBA statistics
    website ([https://stats.nba.com/](https://stats.nba.com/)).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We first load the data and look at its summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|  | game_date | minute | opponent | action_type | shot_type | shot_distance
    | shot_made |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 20170415 | 10 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 20170415 | 11 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 20170415 | 14 | IND | Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 20170415 | 15 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 20170415 | 18 | IND | Alley Oop Dunk Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|  | game_date | minute | shot_distance | shot_made |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 3.840000e+02 | 384.00000 | 384.000000 | 384.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 2.017052e+07 | 24.40625 | 10.695312 | 0.565104 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 6.948501e+01 | 13.67304 | 10.547586 | 0.496390 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 2.017042e+07 | 1.00000 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | 2.017050e+07 | 13.00000 | 1.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | 2.017052e+07 | 25.00000 | 6.500000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | 2.017060e+07 | 35.00000 | 23.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 2.017061e+07 | 48.00000 | 31.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: The two columns we will be interested in are `shot_distance` (LeBron’s distance
    from the basket when the shot was attempted (ft)) and `shot_made` (0 if the shot
    missed, 1 if the shot went in). As the summary table above indicates, the average
    distance was `10.6953` and the frequency of shots made was `0.565104`. We extract
    those two columns and display them on a scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png](../Images/3d7f7643b1fbf4e975e00b74d1d83929.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this kind of data is hard to vizualize because of the superposition
    of points with the same \(x\) and \(y\)-values. One trick is to jiggle the \(y\)’s
    a little bit by adding Gaussian noise. We do this next and plot again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png](../Images/8ed222a6929c72dd22f117b3334ede4e.png)'
  prefs: []
  type: TYPE_IMG
- en: We apply GD to logistic regression. We first construct the data matrices \(A\)
    and \(\mathbf{b}\). To allow an affine function of the features, we add a column
    of \(1\)’s as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We run GD starting from \((0,0)\) with a step size computed from the smoothness
    of the objective as above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Finally we plot the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png](../Images/b5fe636fb5ab3cb9e6e07cac190911dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 3.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_opt_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.1:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(x_1, x_2) &= \left(\frac{\partial f}{\partial x_1},
    \frac{\partial f}{\partial x_2}\right) \\ &= (6x_1 - 2x_2 - 5, -2x_1 + 8x_2 +
    2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((1, -1)\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(1, -1) = (6(1) - 2(-1) - 5, -2(1) + 8(-1) + 2) = (3, -8). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.3:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f}{\partial x_1} &= \cos(x_1) \cos(x_2), \\
    \frac{\partial f}{\partial x_2} &= -\sin(x_1) \sin(x_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((\frac{\pi}{4}, \frac{\pi}{3})\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f}{\partial x_1}(\frac{\pi}{4}, \frac{\pi}{3})
    &= \cos(\frac{\pi}{4}) \cos(\frac{\pi}{3}) = \frac{\sqrt{2}}{2} \cdot \frac{1}{2}
    = \frac{\sqrt{2}}{4}, \\ \frac{\partial f}{\partial x_2}(\frac{\pi}{4}, \frac{\pi}{3})
    &= -\sin(\frac{\pi}{4}) \sin(\frac{\pi}{3}) = -\frac{\sqrt{2}}{2} \cdot \frac{\sqrt{3}}{2}
    = -\frac{\sqrt{6}}{4}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.5: The Hessian matrix of \(f(x_1, x_2)\)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 6x_1 & 6x_2 \\ 6x_2
    & 6x_1 - 12x_2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((1, 2)\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(1, 2) = \begin{pmatrix} 6 & 12 \\ 12 & -18 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We can see that \(\frac{\partial^2 f}{\partial x_1 \partial x_2}(1, 2) = \frac{\partial^2
    f}{\partial x_2 \partial x_1}(1, 2) = 12\), confirming the Symmetry of the Hessian
    Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.7:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial^2 f}{\partial x_1^2} &= 2 \sin(x_2), \\ \frac{\partial^2
    f}{\partial x_1 \partial x_2} &= 2x_1 \cos(x_2), \\ \frac{\partial^2 f}{\partial
    x_2 \partial x_1} &= 2x_1 \cos(x_2), \\ \frac{\partial^2 f}{\partial x_2^2} &=
    -x_1^2 \sin(x_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.9: The Hessian matrix is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2, x_3) = \begin{pmatrix} 2 & -2 & 4 \\
    -2 & 4 & -6 \\ 4 & -6 & 6 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.11: \(\frac{\partial f}{\partial x} = 3x^2y^2
    - 2y^3\) and \(\frac{\partial f}{\partial y} = 2x^3y - 6xy^2 + 4y^3\), obtained
    by differentiating \(f\) with respect to each variable while holding the other
    constant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.13: The Hessian matrix is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_g(x, y) = \begin{pmatrix} -\sin(x) \cos(y) & -\cos(x)
    \sin(y) \\ -\cos(x) \sin(y) & -\sin(x) \cos(y) \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.15: \(\frac{\partial^2 q}{\partial x^2} =
    6x\) and \(\frac{\partial^2 q}{\partial y^2} = -6x\). Adding these gives \(6x
    - 6x = 0\), so \(q\) satisfies Laplace’s equation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.17: By the chain rule, the rate of change
    of temperature experienced by the particle is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{d}{dt}u(\mathbf{c}(t)) = \nabla u(\mathbf{c}(t))^T \mathbf{c}'(t).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We have \(\nabla u(x, y) = (-2xe^{-x^2 - y^2}, -2ye^{-x^2 - y^2})\) and \(\mathbf{c}'(t)
    = (2t, 3t^2)\). Evaluating at \(t = 1\) gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{d}{dt}u(\mathbf{c}(1)) = (-2e^{-2}, -2e^{-2})^T (2, 3) = -10e^{-2}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.19: \(\frac{d}{dt} f(\mathbf{g}(t)) = 2t
    \cos t - t^2 \sin t\). Justification: \(\nabla f = (y, x)\), and \(\mathbf{g}''(t)
    = (2t, -\sin t)\). Then, \(\frac{d}{dt} f(\mathbf{g}(t)) = \cos t \cdot 2t + t^2
    \cdot (-\sin t)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.1: \(\nabla f(x_1, x_2) = (2x_1, 4x_2)\).
    Setting this equal to zero yields \(2x_1 = 0\) and \(4x_2 = 0\), which implies
    \(x_1 = 0\) and \(x_2 = 0\). Thus, the only point where \(\nabla f(x_1, x_2) =
    0\) is \((0, 0)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.3: The second directional derivative is given
    by \(\frac{\partial^2 f(\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0)
    \mathbf{v}\). We have \(\mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 2 & 2 \\ 2 &
    2 \end{pmatrix}\). Thus,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \frac{\partial^2 f(1, 1)}{\partial \mathbf{v}^2} = (\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}})^T \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix} (\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}}) = \frac{1}{2}(2 + 2 + 2 + 2) = 4. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.5: The first-order necessary conditions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla_{x_1, x_2} L(x_1, x_2, \lambda) &= 0, \\ h(x_1, x_2)
    &= 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial L}{\partial x_1} &= 2x_1 + \lambda = 0, \\ \frac{\partial
    L}{\partial x_2} &= 2x_2 + \lambda = 0, \\ x_1 + x_2 &= 1. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first two equations, we have \(x_1 = x_2 = -\frac{\lambda}{2}\). Substituting
    into the third equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ -\frac{\lambda}{2} - \frac{\lambda}{2} = 1 \implies \lambda = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(x_1 = x_2 = \frac{1}{2}\), and the only point satisfying the first-order
    necessary conditions is \((\frac{1}{2}, \frac{1}{2}, -1)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.7: The Lagrangian is \(L(x_1, x_2, x_3, \lambda)
    = x_1^2 + x_2^2 + x_3^2 + \lambda(x_1 + 2x_2 + 3x_3 - 6)\). The first-order necessary
    conditions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} 2x_1 + \lambda &= 0, \\ 2x_2 + 2\lambda &= 0, \\ 2x_3 + 3\lambda
    &= 0, \\ x_1 + 2x_2 + 3x_3 &= 6. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first three equations, we have \(x_1 = -\frac{\lambda}{2}\), \(x_2
    = -\lambda\), \(x_3 = -\frac{3\lambda}{2}\). Substituting into the fourth equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ -\frac{\lambda}{2} - 2\lambda - \frac{9\lambda}{2} = 6 \implies -6\lambda
    = 6 \implies \lambda = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(x_1 = \frac{1}{2}\), \(x_2 = 1\), \(x_3 = \frac{3}{2}\), and the only
    point satisfying the first-order necessary conditions is \((\frac{1}{2}, 1, \frac{3}{2},
    -1)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.9: The gradient of \(f\) is \(\nabla f(x_1,
    x_2) = (3x_1^2 - 3x_2^2, -6x_1x_2)\). At the point \((1, 0)\), the gradient is
    \(\nabla f(1, 0) = (3, 0)\). The directional derivative of \(f\) at \((1, 0)\)
    in the direction \(\mathbf{v} = (1, 1)\) is \(\nabla f(1, 0)^T \mathbf{v} = (3,
    0)^T (1, 1) = 3\). Since this is positive, \(v\) is not a descent direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.11: The Hessian matrix of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{bmatrix} -2 & 0 \\ 0 & -2 \end{bmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the second directional derivative of \(f\) at \((0, 0)\) in the direction
    \(v = (1, 0)\) is \(\mathbf{v}^T \mathbf{H}_f(0, 0) \mathbf{v} = (1, 0) \begin{bmatrix}
    -2 & 0 \\ 0 & -2 \end{bmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = -2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.13: The Hessian matrix is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(1,1) = \begin{pmatrix} 6 & -3 \\ -3 & 6 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Justification: Compute the second partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f}{\partial x^2} = 6x, \quad \frac{\partial^2 f}{\partial
    y^2} = 6y, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2
    f}{\partial y \partial x} = -3. \]
  prefs: []
  type: TYPE_NORMAL
- en: At \((1,1)\), these values are \(6\), \(6\), and \(-3\), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.1: The convex combination is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(2, 3) + \alpha(4, 5) = 0.7(2, 3) + 0.3(4, 5) = (0.7 \cdot 2
    + 0.3 \cdot 4, 0.7 \cdot 3 + 0.3 \cdot 5) = (2.6, 3.6). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.3: \(S_1\) and \(S_2\) are halfspaces, which
    are convex sets. By the lemma in the text, the intersection of convex sets is
    also convex. Therefore, \(S_1 \cap S_2\) is a convex set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.5: The function \(f\) is a quadratic function
    with \(P = 2\), \(q = 2\), and \(r = 1\). Since \(P > 0\), \(f\) is strictly convex.
    The unique global minimizer is found by setting the gradient to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(x) = 2x + 2 = 0 \implies x^* = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.7: The Hessian matrix of \(f\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'For any \((x, y) \in \mathbb{R}^2\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla^2 f(x, y) \succeq 2I_{2 \times 2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: so \(f\) is strongly convex with \(m = 2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.9: \(f\) is not convex. We have \(f''''(x)
    = 12x^2 - 4\), which is negative for \(x \in (-\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}})\).
    Since the second derivative is not always nonnegative, \(f\) is not convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.11: \(f\) is strongly convex. We have \(f''''(x)
    = 2 > 0\), so \(f\) is 2-strongly convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.13: \(D\) is convex. To show this, let \((x_1,
    y_1), (x_2, y_2) \in D\) and \(\alpha \in (0, 1)\). We need to show that \((1
    - \alpha)(x_1, y_1) + \alpha(x_2, y_2) \in D\). Compute:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) = ((1 - \alpha)x_1 + \alpha x_2,
    (1 - \alpha)y_1 + \alpha y_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: Now,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ((1 - \alpha)x_1 + \alpha x_2)^2 + ((1 - \alpha)y_1 + \alpha y_2)^2 < (1
    - \alpha)(x_1^2 + y_1^2) + \alpha(x_2^2 + y_2^2) < 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(D\) is convex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.15: \(D\) is not convex. For example, let
    \((x_1, y_1) = (2, \sqrt{3})\) and \((x_2, y_2) = (2, -\sqrt{3})\), both of which
    are in \(D\). For \(\alpha = \frac{1}{2}\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(2, \sqrt{3}) + \alpha(2, -\sqrt{3}) = \left(2, 0\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: Now,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left(2\right)^2 - \left(0\right)^2 = 4 > 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.1: The gradient is \(\nabla f(x, y) = (2x,
    8y)\). At \((1, 1)\), it is \((2, 8)\). The direction of steepest descent is \(-\nabla
    f(1, 1) = (-2, -8)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.3: \(\nabla f(x) = 3x^2 - 12x + 9\). At \(x_0
    = 0\), \(\nabla f(0) = 9\). The first iteration gives \(x_1 = x_0 - \alpha \nabla
    f(x_0) = 0 - 0.1 \cdot 9 = -0.9\). At \(x_1 = -0.9\), \(\nabla f(-0.9) = 3 \cdot
    (-0.9)^2 - 12 \cdot (-0.9) + 9 = 2.43 + 10.8 + 9 = 22.23\). The second iteration
    gives \(x_2 = x_1 - \alpha \nabla f(x_1) = -0.9 - 0.1 \cdot 22.23 = -3.123\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.5: We have \(\nabla f(x) = 2x\), so \(\nabla
    f(2) = 4\). Thus, the gradient descent update is \(x_1 = x_0 - \alpha \nabla f(x_0)
    = 2 - 0.1 \cdot 4 = 1.6\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.7: \(f''''(x) = 4\) for all \(x \in \mathbb{R}\).
    Therefore, \(f''''(x) \geq 4\) for all \(x \in \mathbb{R}\), which implies that
    \(f\) is \(4\)-strongly convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.9: No. We have \(f''''(x) = 12x^2\), which
    can be arbitrarily large as \(x\) increases. Thus, there is no constant \(L\)
    such that \(-L \le f''''(x) \le L\) for all \(x \in \mathbb{R}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.11: We have \(f''''(x) = 2\) for all \(x
    \in \mathbb{R}\). Thus, we can take \(m = 2\), and we have \(f''''(x) \ge 2\)
    for all \(x \in \mathbb{R}\), which is the condition for \(m\)-strong convexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.13: The gradient is \(\nabla f(x, y) = (2x,
    2y)\). At \((1, 1)\), it is \((2, 2)\). The first update is \((0.8, 0.8)\). The
    gradient at \((0.8, 0.8)\) is \((1.6, 1.6)\). The second update is \((0.64, 0.64)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.1: The log-odds is given by \(\log \frac{p}{1-p}
    = \log \frac{0.25}{0.75} = \log \frac{1}{3} = -\log 3\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.3:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \boldsymbol{\alpha}^T \mathbf{x} = (-0.2 \cdot 1) + (0.4 \cdot 3) = -0.2
    + 1.2 = 1.0 \]\[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(1.0) = \frac{1}{1
    + e^{-1}} \approx 0.731 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.5: By the quotient rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sigma'(z) &= \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 +
    e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\ &= \sigma(z) (1 - \sigma(z)). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.7: We have \(b_1 - \sigma(\boldsymbol{\alpha}_1^T
    \mathbf{x}) \approx 0.05\), \(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x})
    \approx -0.73\), \(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}) \approx 0.73\).
    Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla \ell(\mathbf{x}; A, b) &= -\frac{1}{3} \sum_{i=1}^3
    (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \boldsymbol{\alpha}_i \\ &\approx
    -\frac{1}{3} \{(0.05)(1, 2) + (-0.73)(-1, 1) + 0.73(0, -1)\}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.9: We have \(b_1 - \sigma(\boldsymbol{\alpha}_1^T
    \mathbf{x}^0) = 1 - \sigma(0) = 0.5\), \(b_2 - \sigma(\boldsymbol{\alpha}_2^T
    \mathbf{x}^0) = -0.5\), \(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}^0) =
    0.5\). Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{x}^1 &= \mathbf{x}^0 - \beta \nabla \ell(\mathbf{x}^0;
    A, \mathbf{b}) \\ &= (0, 0) + \frac{0.1}{3} \{0.5(1, 2) + (-0.5)(-1, 1) + 0.5(0,
    -1)\} \\ &= (0.1, 0.05). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define and calculate partial derivatives and gradients for functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute second-order partial derivatives and construct the Hessian matrix for
    twice continuously differentiable functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Chain Rule to compute derivatives of composite functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the multivariable version of the Mean Value Theorem using the
    Chain Rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate gradients and Hessians for affine and quadratic functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define global and local minimizers for unconstrained optimization problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the first-order necessary conditions for a local minimizer using the
    gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of a descent direction and its relation to the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of directional derivatives and compute directional derivatives
    using the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the second-order necessary and sufficient conditions for a local minimizer
    using the Hessian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the gradient and Hessian matrix for a given multivariable function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate optimization problems with equality constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the method of Lagrange multipliers to derive the first-order necessary
    conditions for a constrained local minimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the second-order sufficient conditions for a constrained local minimizer
    using the Hessian matrix and Lagrange multipliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the regularity condition in the context of constrained optimization
    problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a constrained optimization problem by finding points that satisfy the
    first-order necessary conditions and checking the second-order sufficient conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define convex sets and convex functions, and provide examples of each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify operations that preserve convexity of sets and functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characterize convex functions using the first-order convexity condition based
    on the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the convexity of a function using the second-order convexity condition
    based on the Hessian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between convexity and optimization, particularly how
    local minimizers of convex functions are also global minimizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the first-order optimality condition for convex functions on
    R^d and on convex sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define strong convexity and its implications for the existence and uniqueness
    of global minimizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the convexity and strong convexity of quadratic functions and least-squares
    objectives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the concept of convexity to solve optimization problems, such as finding
    the projection of a point onto a convex set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define gradient descent and explain its motivation as a numerical optimization
    method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the negative gradient is the steepest descent direction for a continuously
    differentiable function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the convergence of gradient descent for smooth functions, proving that
    it produces a sequence of points with decreasing objective values and vanishing
    gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the convergence rate of gradient descent for smooth functions in terms
    of the number of iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define strong convexity for twice continuously differentiable functions and
    relate it to the function value and gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove faster convergence rates for gradient descent when applied to smooth and
    strongly convex functions, showing exponential convergence to the global minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement gradient descent in Python and apply it to simple examples to illustrate
    the theoretical convergence results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the role of the sigmoid function in transforming a linear function of
    features into a probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the gradient and Hessian of the logistic regression objective function
    (cross-entropy loss).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the logistic regression objective function is convex and smooth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement gradient descent to minimize the logistic regression objective function
    in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply logistic regression to real-world datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_opt_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 3.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_3_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-opt-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.1:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(x_1, x_2) &= \left(\frac{\partial f}{\partial x_1},
    \frac{\partial f}{\partial x_2}\right) \\ &= (6x_1 - 2x_2 - 5, -2x_1 + 8x_2 +
    2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((1, -1)\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(1, -1) = (6(1) - 2(-1) - 5, -2(1) + 8(-1) + 2) = (3, -8). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.3:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f}{\partial x_1} &= \cos(x_1) \cos(x_2), \\
    \frac{\partial f}{\partial x_2} &= -\sin(x_1) \sin(x_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((\frac{\pi}{4}, \frac{\pi}{3})\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f}{\partial x_1}(\frac{\pi}{4}, \frac{\pi}{3})
    &= \cos(\frac{\pi}{4}) \cos(\frac{\pi}{3}) = \frac{\sqrt{2}}{2} \cdot \frac{1}{2}
    = \frac{\sqrt{2}}{4}, \\ \frac{\partial f}{\partial x_2}(\frac{\pi}{4}, \frac{\pi}{3})
    &= -\sin(\frac{\pi}{4}) \sin(\frac{\pi}{3}) = -\frac{\sqrt{2}}{2} \cdot \frac{\sqrt{3}}{2}
    = -\frac{\sqrt{6}}{4}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.5: The Hessian matrix of \(f(x_1, x_2)\)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 6x_1 & 6x_2 \\ 6x_2
    & 6x_1 - 12x_2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point \((1, 2)\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(1, 2) = \begin{pmatrix} 6 & 12 \\ 12 & -18 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We can see that \(\frac{\partial^2 f}{\partial x_1 \partial x_2}(1, 2) = \frac{\partial^2
    f}{\partial x_2 \partial x_1}(1, 2) = 12\), confirming the Symmetry of the Hessian
    Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.7:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial^2 f}{\partial x_1^2} &= 2 \sin(x_2), \\ \frac{\partial^2
    f}{\partial x_1 \partial x_2} &= 2x_1 \cos(x_2), \\ \frac{\partial^2 f}{\partial
    x_2 \partial x_1} &= 2x_1 \cos(x_2), \\ \frac{\partial^2 f}{\partial x_2^2} &=
    -x_1^2 \sin(x_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.9: The Hessian matrix is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2, x_3) = \begin{pmatrix} 2 & -2 & 4 \\
    -2 & 4 & -6 \\ 4 & -6 & 6 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.11: \(\frac{\partial f}{\partial x} = 3x^2y^2
    - 2y^3\) and \(\frac{\partial f}{\partial y} = 2x^3y - 6xy^2 + 4y^3\), obtained
    by differentiating \(f\) with respect to each variable while holding the other
    constant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.13: The Hessian matrix is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_g(x, y) = \begin{pmatrix} -\sin(x) \cos(y) & -\cos(x)
    \sin(y) \\ -\cos(x) \sin(y) & -\sin(x) \cos(y) \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.15: \(\frac{\partial^2 q}{\partial x^2} =
    6x\) and \(\frac{\partial^2 q}{\partial y^2} = -6x\). Adding these gives \(6x
    - 6x = 0\), so \(q\) satisfies Laplace’s equation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.17: By the chain rule, the rate of change
    of temperature experienced by the particle is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{d}{dt}u(\mathbf{c}(t)) = \nabla u(\mathbf{c}(t))^T \mathbf{c}'(t).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We have \(\nabla u(x, y) = (-2xe^{-x^2 - y^2}, -2ye^{-x^2 - y^2})\) and \(\mathbf{c}'(t)
    = (2t, 3t^2)\). Evaluating at \(t = 1\) gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{d}{dt}u(\mathbf{c}(1)) = (-2e^{-2}, -2e^{-2})^T (2, 3) = -10e^{-2}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.2.19: \(\frac{d}{dt} f(\mathbf{g}(t)) = 2t
    \cos t - t^2 \sin t\). Justification: \(\nabla f = (y, x)\), and \(\mathbf{g}''(t)
    = (2t, -\sin t)\). Then, \(\frac{d}{dt} f(\mathbf{g}(t)) = \cos t \cdot 2t + t^2
    \cdot (-\sin t)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.1: \(\nabla f(x_1, x_2) = (2x_1, 4x_2)\).
    Setting this equal to zero yields \(2x_1 = 0\) and \(4x_2 = 0\), which implies
    \(x_1 = 0\) and \(x_2 = 0\). Thus, the only point where \(\nabla f(x_1, x_2) =
    0\) is \((0, 0)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.3: The second directional derivative is given
    by \(\frac{\partial^2 f(\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0)
    \mathbf{v}\). We have \(\mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 2 & 2 \\ 2 &
    2 \end{pmatrix}\). Thus,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \frac{\partial^2 f(1, 1)}{\partial \mathbf{v}^2} = (\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}})^T \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix} (\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}}) = \frac{1}{2}(2 + 2 + 2 + 2) = 4. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.5: The first-order necessary conditions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla_{x_1, x_2} L(x_1, x_2, \lambda) &= 0, \\ h(x_1, x_2)
    &= 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial L}{\partial x_1} &= 2x_1 + \lambda = 0, \\ \frac{\partial
    L}{\partial x_2} &= 2x_2 + \lambda = 0, \\ x_1 + x_2 &= 1. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first two equations, we have \(x_1 = x_2 = -\frac{\lambda}{2}\). Substituting
    into the third equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ -\frac{\lambda}{2} - \frac{\lambda}{2} = 1 \implies \lambda = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(x_1 = x_2 = \frac{1}{2}\), and the only point satisfying the first-order
    necessary conditions is \((\frac{1}{2}, \frac{1}{2}, -1)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.7: The Lagrangian is \(L(x_1, x_2, x_3, \lambda)
    = x_1^2 + x_2^2 + x_3^2 + \lambda(x_1 + 2x_2 + 3x_3 - 6)\). The first-order necessary
    conditions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} 2x_1 + \lambda &= 0, \\ 2x_2 + 2\lambda &= 0, \\ 2x_3 + 3\lambda
    &= 0, \\ x_1 + 2x_2 + 3x_3 &= 6. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first three equations, we have \(x_1 = -\frac{\lambda}{2}\), \(x_2
    = -\lambda\), \(x_3 = -\frac{3\lambda}{2}\). Substituting into the fourth equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ -\frac{\lambda}{2} - 2\lambda - \frac{9\lambda}{2} = 6 \implies -6\lambda
    = 6 \implies \lambda = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(x_1 = \frac{1}{2}\), \(x_2 = 1\), \(x_3 = \frac{3}{2}\), and the only
    point satisfying the first-order necessary conditions is \((\frac{1}{2}, 1, \frac{3}{2},
    -1)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.9: The gradient of \(f\) is \(\nabla f(x_1,
    x_2) = (3x_1^2 - 3x_2^2, -6x_1x_2)\). At the point \((1, 0)\), the gradient is
    \(\nabla f(1, 0) = (3, 0)\). The directional derivative of \(f\) at \((1, 0)\)
    in the direction \(\mathbf{v} = (1, 1)\) is \(\nabla f(1, 0)^T \mathbf{v} = (3,
    0)^T (1, 1) = 3\). Since this is positive, \(v\) is not a descent direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.11: The Hessian matrix of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{bmatrix} -2 & 0 \\ 0 & -2 \end{bmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the second directional derivative of \(f\) at \((0, 0)\) in the direction
    \(v = (1, 0)\) is \(\mathbf{v}^T \mathbf{H}_f(0, 0) \mathbf{v} = (1, 0) \begin{bmatrix}
    -2 & 0 \\ 0 & -2 \end{bmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = -2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.3.13: The Hessian matrix is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(1,1) = \begin{pmatrix} 6 & -3 \\ -3 & 6 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Justification: Compute the second partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f}{\partial x^2} = 6x, \quad \frac{\partial^2 f}{\partial
    y^2} = 6y, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2
    f}{\partial y \partial x} = -3. \]
  prefs: []
  type: TYPE_NORMAL
- en: At \((1,1)\), these values are \(6\), \(6\), and \(-3\), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.1: The convex combination is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(2, 3) + \alpha(4, 5) = 0.7(2, 3) + 0.3(4, 5) = (0.7 \cdot 2
    + 0.3 \cdot 4, 0.7 \cdot 3 + 0.3 \cdot 5) = (2.6, 3.6). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.3: \(S_1\) and \(S_2\) are halfspaces, which
    are convex sets. By the lemma in the text, the intersection of convex sets is
    also convex. Therefore, \(S_1 \cap S_2\) is a convex set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.5: The function \(f\) is a quadratic function
    with \(P = 2\), \(q = 2\), and \(r = 1\). Since \(P > 0\), \(f\) is strictly convex.
    The unique global minimizer is found by setting the gradient to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(x) = 2x + 2 = 0 \implies x^* = -1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.7: The Hessian matrix of \(f\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'For any \((x, y) \in \mathbb{R}^2\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla^2 f(x, y) \succeq 2I_{2 \times 2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: so \(f\) is strongly convex with \(m = 2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.9: \(f\) is not convex. We have \(f''''(x)
    = 12x^2 - 4\), which is negative for \(x \in (-\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}})\).
    Since the second derivative is not always nonnegative, \(f\) is not convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.11: \(f\) is strongly convex. We have \(f''''(x)
    = 2 > 0\), so \(f\) is 2-strongly convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.13: \(D\) is convex. To show this, let \((x_1,
    y_1), (x_2, y_2) \in D\) and \(\alpha \in (0, 1)\). We need to show that \((1
    - \alpha)(x_1, y_1) + \alpha(x_2, y_2) \in D\). Compute:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(x_1, y_1) + \alpha(x_2, y_2) = ((1 - \alpha)x_1 + \alpha x_2,
    (1 - \alpha)y_1 + \alpha y_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: Now,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ((1 - \alpha)x_1 + \alpha x_2)^2 + ((1 - \alpha)y_1 + \alpha y_2)^2 < (1
    - \alpha)(x_1^2 + y_1^2) + \alpha(x_2^2 + y_2^2) < 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(D\) is convex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.4.15: \(D\) is not convex. For example, let
    \((x_1, y_1) = (2, \sqrt{3})\) and \((x_2, y_2) = (2, -\sqrt{3})\), both of which
    are in \(D\). For \(\alpha = \frac{1}{2}\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1 - \alpha)(2, \sqrt{3}) + \alpha(2, -\sqrt{3}) = \left(2, 0\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: Now,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left(2\right)^2 - \left(0\right)^2 = 4 > 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.1: The gradient is \(\nabla f(x, y) = (2x,
    8y)\). At \((1, 1)\), it is \((2, 8)\). The direction of steepest descent is \(-\nabla
    f(1, 1) = (-2, -8)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.3: \(\nabla f(x) = 3x^2 - 12x + 9\). At \(x_0
    = 0\), \(\nabla f(0) = 9\). The first iteration gives \(x_1 = x_0 - \alpha \nabla
    f(x_0) = 0 - 0.1 \cdot 9 = -0.9\). At \(x_1 = -0.9\), \(\nabla f(-0.9) = 3 \cdot
    (-0.9)^2 - 12 \cdot (-0.9) + 9 = 2.43 + 10.8 + 9 = 22.23\). The second iteration
    gives \(x_2 = x_1 - \alpha \nabla f(x_1) = -0.9 - 0.1 \cdot 22.23 = -3.123\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.5: We have \(\nabla f(x) = 2x\), so \(\nabla
    f(2) = 4\). Thus, the gradient descent update is \(x_1 = x_0 - \alpha \nabla f(x_0)
    = 2 - 0.1 \cdot 4 = 1.6\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.7: \(f''''(x) = 4\) for all \(x \in \mathbb{R}\).
    Therefore, \(f''''(x) \geq 4\) for all \(x \in \mathbb{R}\), which implies that
    \(f\) is \(4\)-strongly convex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.9: No. We have \(f''''(x) = 12x^2\), which
    can be arbitrarily large as \(x\) increases. Thus, there is no constant \(L\)
    such that \(-L \le f''''(x) \le L\) for all \(x \in \mathbb{R}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.11: We have \(f''''(x) = 2\) for all \(x
    \in \mathbb{R}\). Thus, we can take \(m = 2\), and we have \(f''''(x) \ge 2\)
    for all \(x \in \mathbb{R}\), which is the condition for \(m\)-strong convexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.5.13: The gradient is \(\nabla f(x, y) = (2x,
    2y)\). At \((1, 1)\), it is \((2, 2)\). The first update is \((0.8, 0.8)\). The
    gradient at \((0.8, 0.8)\) is \((1.6, 1.6)\). The second update is \((0.64, 0.64)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.1: The log-odds is given by \(\log \frac{p}{1-p}
    = \log \frac{0.25}{0.75} = \log \frac{1}{3} = -\log 3\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.3:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \boldsymbol{\alpha}^T \mathbf{x} = (-0.2 \cdot 1) + (0.4 \cdot 3) = -0.2
    + 1.2 = 1.0 \]\[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(1.0) = \frac{1}{1
    + e^{-1}} \approx 0.731 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.5: By the quotient rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sigma'(z) &= \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 +
    e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\ &= \sigma(z) (1 - \sigma(z)). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.7: We have \(b_1 - \sigma(\boldsymbol{\alpha}_1^T
    \mathbf{x}) \approx 0.05\), \(b_2 - \sigma(\boldsymbol{\alpha}_2^T \mathbf{x})
    \approx -0.73\), \(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}) \approx 0.73\).
    Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla \ell(\mathbf{x}; A, b) &= -\frac{1}{3} \sum_{i=1}^3
    (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \boldsymbol{\alpha}_i \\ &\approx
    -\frac{1}{3} \{(0.05)(1, 2) + (-0.73)(-1, 1) + 0.73(0, -1)\}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer and justification for E3.6.9: We have \(b_1 - \sigma(\boldsymbol{\alpha}_1^T
    \mathbf{x}^0) = 1 - \sigma(0) = 0.5\), \(b_2 - \sigma(\boldsymbol{\alpha}_2^T
    \mathbf{x}^0) = -0.5\), \(b_3 - \sigma(\boldsymbol{\alpha}_3^T \mathbf{x}^0) =
    0.5\). Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{x}^1 &= \mathbf{x}^0 - \beta \nabla \ell(\mathbf{x}^0;
    A, \mathbf{b}) \\ &= (0, 0) + \frac{0.1}{3} \{0.5(1, 2) + (-0.5)(-1, 1) + 0.5(0,
    -1)\} \\ &= (0.1, 0.05). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define and calculate partial derivatives and gradients for functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute second-order partial derivatives and construct the Hessian matrix for
    twice continuously differentiable functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Chain Rule to compute derivatives of composite functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the multivariable version of the Mean Value Theorem using the
    Chain Rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate gradients and Hessians for affine and quadratic functions of several
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define global and local minimizers for unconstrained optimization problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the first-order necessary conditions for a local minimizer using the
    gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of a descent direction and its relation to the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of directional derivatives and compute directional derivatives
    using the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the second-order necessary and sufficient conditions for a local minimizer
    using the Hessian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the gradient and Hessian matrix for a given multivariable function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate optimization problems with equality constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the method of Lagrange multipliers to derive the first-order necessary
    conditions for a constrained local minimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the second-order sufficient conditions for a constrained local minimizer
    using the Hessian matrix and Lagrange multipliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the regularity condition in the context of constrained optimization
    problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a constrained optimization problem by finding points that satisfy the
    first-order necessary conditions and checking the second-order sufficient conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define convex sets and convex functions, and provide examples of each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify operations that preserve convexity of sets and functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characterize convex functions using the first-order convexity condition based
    on the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the convexity of a function using the second-order convexity condition
    based on the Hessian matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between convexity and optimization, particularly how
    local minimizers of convex functions are also global minimizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the first-order optimality condition for convex functions on
    R^d and on convex sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define strong convexity and its implications for the existence and uniqueness
    of global minimizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the convexity and strong convexity of quadratic functions and least-squares
    objectives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the concept of convexity to solve optimization problems, such as finding
    the projection of a point onto a convex set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define gradient descent and explain its motivation as a numerical optimization
    method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the negative gradient is the steepest descent direction for a continuously
    differentiable function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the convergence of gradient descent for smooth functions, proving that
    it produces a sequence of points with decreasing objective values and vanishing
    gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the convergence rate of gradient descent for smooth functions in terms
    of the number of iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define strong convexity for twice continuously differentiable functions and
    relate it to the function value and gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove faster convergence rates for gradient descent when applied to smooth and
    strongly convex functions, showing exponential convergence to the global minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement gradient descent in Python and apply it to simple examples to illustrate
    the theoretical convergence results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the role of the sigmoid function in transforming a linear function of
    features into a probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the gradient and Hessian of the logistic regression objective function
    (cross-entropy loss).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the logistic regression objective function is convex and smooth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement gradient descent to minimize the logistic regression objective function
    in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply logistic regression to real-world datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3.8.2.1\. Logistic regression: illustration of convergence result[#](#logistic-regression-illustration-of-convergence-result
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We return to our proof of convergence for smooth functions using a special case
    of logistic regression. We first define the functions \(\hat{f}\), \(\mathcal{L}\)
    and \(\frac{\partial}{\partial x}\mathcal{L}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We illustrate GD on a random dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We plot the upper and lower bounds in the *Quadratic Bound for Smooth Functions*
    around \(x = x_0\). It turns out we can take \(L=1\) because all features are
    uniformly random between \(-1\) and \(1\). Observe that minimizing the upper quadratic
    bound leads to a decrease in \(\mathcal{L}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png](../Images/097a341c91c4cf50443a8e9f81f433b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '3.8.2.2\. Logistic regression: another dataset[#](#logistic-regression-another-dataset
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that to run gradient descent, we first implement a function computing
    a descent update. It takes as input a function `grad_fn` computing the gradient
    itself, as well as a current iterate and a step size. We now also feed a dataset
    as additional input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We analyze with a simple dataset from UC Berkeley’s [DS100](http://www.ds100.org)
    course. The file `lebron.csv` is available [here](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets).
    Quoting a previous version of the course’s textbook:'
  prefs: []
  type: TYPE_NORMAL
- en: In basketball, players score by shooting a ball through a hoop. One such player,
    LeBron James, is widely considered one of the best basketball players ever for
    his incredible ability to score. LeBron plays in the National Basketball Association
    (NBA), the United States’s premier basketball league. We’ve collected a dataset
    of all of LeBron’s attempts in the 2017 NBA Playoff Games using the NBA statistics
    website ([https://stats.nba.com/](https://stats.nba.com/)).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We first load the data and look at its summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '|  | game_date | minute | opponent | action_type | shot_type | shot_distance
    | shot_made |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 20170415 | 10 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 20170415 | 11 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 20170415 | 14 | IND | Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 20170415 | 15 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 20170415 | 18 | IND | Alley Oop Dunk Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|  | game_date | minute | shot_distance | shot_made |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 3.840000e+02 | 384.00000 | 384.000000 | 384.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 2.017052e+07 | 24.40625 | 10.695312 | 0.565104 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 6.948501e+01 | 13.67304 | 10.547586 | 0.496390 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 2.017042e+07 | 1.00000 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | 2.017050e+07 | 13.00000 | 1.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | 2.017052e+07 | 25.00000 | 6.500000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | 2.017060e+07 | 35.00000 | 23.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 2.017061e+07 | 48.00000 | 31.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: The two columns we will be interested in are `shot_distance` (LeBron’s distance
    from the basket when the shot was attempted (ft)) and `shot_made` (0 if the shot
    missed, 1 if the shot went in). As the summary table above indicates, the average
    distance was `10.6953` and the frequency of shots made was `0.565104`. We extract
    those two columns and display them on a scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png](../Images/3d7f7643b1fbf4e975e00b74d1d83929.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this kind of data is hard to vizualize because of the superposition
    of points with the same \(x\) and \(y\)-values. One trick is to jiggle the \(y\)’s
    a little bit by adding Gaussian noise. We do this next and plot again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png](../Images/8ed222a6929c72dd22f117b3334ede4e.png)'
  prefs: []
  type: TYPE_IMG
- en: We apply GD to logistic regression. We first construct the data matrices \(A\)
    and \(\mathbf{b}\). To allow an affine function of the features, we add a column
    of \(1\)’s as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We run GD starting from \((0,0)\) with a step size computed from the smoothness
    of the objective as above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Finally we plot the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png](../Images/b5fe636fb5ab3cb9e6e07cac190911dd.png)'
  prefs: []
  type: TYPE_IMG
- en: '3.8.2.1\. Logistic regression: illustration of convergence result[#](#logistic-regression-illustration-of-convergence-result
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We return to our proof of convergence for smooth functions using a special case
    of logistic regression. We first define the functions \(\hat{f}\), \(\mathcal{L}\)
    and \(\frac{\partial}{\partial x}\mathcal{L}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We illustrate GD on a random dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We plot the upper and lower bounds in the *Quadratic Bound for Smooth Functions*
    around \(x = x_0\). It turns out we can take \(L=1\) because all features are
    uniformly random between \(-1\) and \(1\). Observe that minimizing the upper quadratic
    bound leads to a decrease in \(\mathcal{L}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/324fe7586bd9a9025def6d4dad22a04d4771badbb9de6c87d2f507e57fba625f.png](../Images/097a341c91c4cf50443a8e9f81f433b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '3.8.2.2\. Logistic regression: another dataset[#](#logistic-regression-another-dataset
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that to run gradient descent, we first implement a function computing
    a descent update. It takes as input a function `grad_fn` computing the gradient
    itself, as well as a current iterate and a step size. We now also feed a dataset
    as additional input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We analyze with a simple dataset from UC Berkeley’s [DS100](http://www.ds100.org)
    course. The file `lebron.csv` is available [here](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main/utils/datasets).
    Quoting a previous version of the course’s textbook:'
  prefs: []
  type: TYPE_NORMAL
- en: In basketball, players score by shooting a ball through a hoop. One such player,
    LeBron James, is widely considered one of the best basketball players ever for
    his incredible ability to score. LeBron plays in the National Basketball Association
    (NBA), the United States’s premier basketball league. We’ve collected a dataset
    of all of LeBron’s attempts in the 2017 NBA Playoff Games using the NBA statistics
    website ([https://stats.nba.com/](https://stats.nba.com/)).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We first load the data and look at its summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '|  | game_date | minute | opponent | action_type | shot_type | shot_distance
    | shot_made |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 20170415 | 10 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 20170415 | 11 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 20170415 | 14 | IND | Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 20170415 | 15 | IND | Driving Layup Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 20170415 | 18 | IND | Alley Oop Dunk Shot | 2PT Field Goal | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '|  | game_date | minute | shot_distance | shot_made |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 3.840000e+02 | 384.00000 | 384.000000 | 384.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 2.017052e+07 | 24.40625 | 10.695312 | 0.565104 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 6.948501e+01 | 13.67304 | 10.547586 | 0.496390 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 2.017042e+07 | 1.00000 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | 2.017050e+07 | 13.00000 | 1.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | 2.017052e+07 | 25.00000 | 6.500000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | 2.017060e+07 | 35.00000 | 23.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 2.017061e+07 | 48.00000 | 31.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: The two columns we will be interested in are `shot_distance` (LeBron’s distance
    from the basket when the shot was attempted (ft)) and `shot_made` (0 if the shot
    missed, 1 if the shot went in). As the summary table above indicates, the average
    distance was `10.6953` and the frequency of shots made was `0.565104`. We extract
    those two columns and display them on a scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ccf5a77835e56e6a43556d4f958a46f41dcda89b707e8c6937654150eafed13d.png](../Images/3d7f7643b1fbf4e975e00b74d1d83929.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this kind of data is hard to vizualize because of the superposition
    of points with the same \(x\) and \(y\)-values. One trick is to jiggle the \(y\)’s
    a little bit by adding Gaussian noise. We do this next and plot again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/0108a98a440f644ac9feb81bb3700cbea2ffcdf8fa56a7d2f355b1d652ae7cff.png](../Images/8ed222a6929c72dd22f117b3334ede4e.png)'
  prefs: []
  type: TYPE_IMG
- en: We apply GD to logistic regression. We first construct the data matrices \(A\)
    and \(\mathbf{b}\). To allow an affine function of the features, we add a column
    of \(1\)’s as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We run GD starting from \((0,0)\) with a step size computed from the smoothness
    of the objective as above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Finally we plot the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e630ab57029ca45289fcf7e97e68879d9893d32390e62c3bf04d6b2295b8c0b2.png](../Images/b5fe636fb5ab3cb9e6e07cac190911dd.png)'
  prefs: []
  type: TYPE_IMG
