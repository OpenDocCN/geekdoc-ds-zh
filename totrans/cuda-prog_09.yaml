- en: Chapter 9
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章
- en: Optimizing Your Application
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化你的应用程序
- en: In this chapter we provide a detailed breakdown of the main areas that limit
    performance in CUDA. Each section contains small examples to illustrate the issues.
    They should be read in order. The previous chapters introduced you to CUDA and
    programming GPUs. The sections here assume you have read the previous chapters
    and are comfortable with the concepts introduced there, or are already familiar
    with CUDA and are specifically interested in techniques for improving execution
    speed of your programs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细分析了限制 CUDA 性能的主要领域。每个部分都包含小示例来说明问题。它们应按顺序阅读。前几章介绍了 CUDA 和 GPU 编程。本章假设你已经阅读了前面的章节，并对其中介绍的概念感到熟悉，或者你已经熟悉
    CUDA，并且特别关心提高程序执行速度的技术。
- en: 'This chapter is broken up into a number of strategies:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为多个策略：
- en: 'Strategy 1: Understanding the problem and breaking it down correctly into serial
    and parallel workloads.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 1：理解问题并正确地将其分解为串行和并行工作负载。
- en: 'Strategy 2: Understanding and optimizing for memory bandwidth, latency and
    cache usage.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 2：理解并优化内存带宽、延迟和缓存使用。
- en: 'Strategy 3: Understanding the implications of needing to transfer data to or
    from the host. A look at the effects of pinned and zero-copy memory and bandwidth
    limits on a selection of hardware.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 3：理解将数据传输到主机或从主机传输数据的影响。探讨固定内存、零拷贝内存和带宽限制对一些硬件的影响。
- en: 'Strategy 4: Understanding the threading and computational abilities in detail
    and how these impact performance.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 4：深入理解线程和计算能力，以及它们如何影响性能。
- en: 'Strategy 5: Where to look for algorithm implementations, with a couple of examples
    of optimization of some general-purpose algorithms.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 5：在哪里寻找算法实现，并举例说明一些通用算法的优化。
- en: 'Strategy 6: Focus on profiling and identifying where in your applications the
    bottlenecks are occurring and why.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 6：专注于性能分析，找出应用程序中的瓶颈及其原因。
- en: 'Strategy 7: A look at how applications can tune themselves to the various hardware
    implementations out there.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 7：应用程序如何根据各种硬件实现进行自我调优的探讨。
- en: 'Strategy 1: Parallel/Serial GPU/CPU Problem Breakdown'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 1：并行/串行 GPU/CPU 问题分解
- en: Analyzing the problem
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析问题
- en: This is the first step in considering if trying to parallelize a problem is
    really the correct solution. Let’s look at some of the issues involved here.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是考虑将问题并行化是否真的正确解决方案的第一步。让我们看看其中的一些问题。
- en: Time
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间
- en: It’s important to define what an “acceptable” time period is for the execution
    time of the algorithm you have in mind. Now acceptable does not have to mean the
    best time humanly possible. When considering optimization, you have to realize
    as a software professional, your time costs money, and if you work in the western
    world, your time is not cheap. The faster a program needs to execute, the more
    effort is involved in making this happen [(Figure 9.1)](#F0010).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 确定你所考虑的算法执行时间的“可接受”时间段是很重要的。这里的可接受并不意味着最短的最佳时间。在考虑优化时，你必须意识到，作为一名软件专业人士，你的时间是有价值的，如果你在西方国家工作，你的时间并不便宜。程序执行所需的速度越快，涉及的工作就越多，来实现这一点的努力也就越大[(图
    9.1)](#F0010)。
- en: '![image](../images/F000090f09-01-9780124159334.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-01-9780124159334.jpg)'
- en: FIGURE 9.1 Programmer time versus speedup achieved.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 程序员时间与加速效果的关系。
- en: You will usually find with any optimization activity there is a certain amount
    of so-called “low-hanging fruit.” The changes required are easy and lead to a
    reasonable speedup. As these are removed, it becomes progressively harder to find
    optimizations and these require more complex restructuring, making them more costly
    in terms of time and the potential for errors they can introduce.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在任何优化活动中，你会发现有一些所谓的“低垂的果实”。所需的更改很简单，并能带来合理的加速效果。当这些被去除后，找到优化点会变得越来越困难，这些优化往往需要更复杂的重构，从而在时间和可能引入的错误方面变得更加昂贵。
- en: In most western countries, programming effort is quite expensive. Even if your
    programming time is free—for example, if you are student working on a project—time
    spent optimizing is still time that could be spent doing other activities. As
    engineers, we can sometimes get caught up in making things better than they need
    to be. Understand what is required and set a suitable goal.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数西方国家，编程的成本相当高。即使你的编程时间是免费的——比如说，你是一个正在做项目的学生——优化所花费的时间仍然是本可以用于其他活动的时间。作为工程师，我们有时会陷入让事情变得比必要的更完美的境地。了解需求并设定适当的目标。
- en: In setting a suitable speedup goal, you have to be aware of what is reasonable,
    given a set of hardware. If you have 20 terabytes of data that needs to be processed
    in a few seconds, a single-GPU machine is just not going to be able to cope. You
    have exactly this sort of issue when you consider Internet search engines. They
    have to, within seconds, return a set of search results to the user. Yet at the
    same time, it used to be “acceptable” for their indexes to take several days to
    update—that is, the time taken for them to pick up new content. In this modern
    world, even this is considered slow. Thus, what is acceptable today may not be
    acceptable tomorrow, next month, or next year.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在设定适当的加速目标时，你必须意识到，在给定的硬件条件下，什么是合理的。如果你有20TB的数据需要在几秒钟内处理，单个GPU机器是无法应对的。当你考虑到互联网搜索引擎时，正好会遇到这种问题。它们必须在几秒钟内返回一组搜索结果给用户。然而，曾几何时，搜索引擎的索引更新需要几天时间——也就是，它们需要多长时间才能获取到新的内容。在这个现代社会，即便这样也被认为是慢的。因此，今天可以接受的标准，可能在明天、下个月或明年就不再能接受了。
- en: In considering what the acceptable time is, ask yourself how far away you currently
    are from this. If it’s a factor of two or less, often it will be worth spending
    time optimizing the CPU implementation, rather than creating an entirely new,
    parallel approach to the problem. Multiple threads introduce all sorts of problems
    of dependencies, deadlock, synchronization, debugging, etc. If you can live with
    the serial CPU version, this may be a better solution in the short term.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑可接受的时间时，问问自己，你距离这个目标有多远。如果差距只有两倍以内，通常花时间优化CPU实现会比创造一种全新的并行方法来解决问题更值得。多个线程会引入各种各样的依赖性、死锁、同步、调试等问题。如果你能接受串行的CPU版本，这在短期内可能是一个更好的解决方案。
- en: Consider also the easy-fix solution to problems used for the past 30 or so years.
    Simply buy some faster hardware. Use profiling to identify where the application
    is spending it time to determine where it’s bound. Is there an input/output (I/O)
    bottleneck, a memory bottleneck, or a processor bottleneck? Buy a high-speed PCI-E
    RAID card and use SATA 3/SAS SSD drives for I/O issues. Move to a socket 2011
    system with a high clock rate on the memory, if memory bandwidth is an issue.
    If it’s simply raw compute throughput, install an Extreme Edition or Black Edition
    processor with the highest clock rate you can buy. Purchase an out-of-the-box,
    liquid-cooled, Sandybridge K or X series overclocked processor solution. These
    solutions typically cost much less than $3,000–$6,000 USD, a budget you could
    easily spend on programming time to convert a program from a serial to a parallel
    program.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑过去30年左右使用的简单解决方案。只需购买一些更快的硬件。使用分析工具找出应用程序花费时间最多的地方，确定其瓶颈所在。是输入/输出（I/O）瓶颈，内存瓶颈，还是处理器瓶颈？如果是I/O问题，可以购买一张高速PCI-E
    RAID卡，使用SATA 3/SAS SSD硬盘。如果是内存带宽问题，换一个内存时钟速率更高的Socket 2011系统。如果是单纯的计算吞吐量问题，可以安装一款极限版或黑版的处理器，选择你能买到的最高时钟速率。也可以购买一款现成的，液冷的Sandybridge
    K系列或X系列超频处理器解决方案。这些解决方案的费用通常远低于$3,000–$6,000美元，这个预算，你完全可以用来支付将程序从串行转换为并行程序的编程时间。
- en: However, while this approach works well when you have a small amount of difference
    between where you are and where you want to be, it’s not always a good approach.
    A high clock rate means high power consumption. The processor manufacturers have
    already abandoned that route in favor of multicore as the only long-term solution
    to providing more compute power. While the “buy new hardware” approach may work
    in the short term, it’s not a long-term solution. Sometimes the hardware you have
    may not easily be changeable, because it’s provided by a restrictive IT department,
    or because you have insufficient funds to purchase new hardware but lots of “free”
    programming time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管当你面临的差距不大时，这种方法效果很好，但并不总是最佳选择。高时钟频率意味着高功耗。处理器制造商已经放弃了这种路线，转而支持多核处理作为提供更多计算能力的唯一长期解决方案。虽然“购买新硬件”的方法在短期内可能有效，但这不是长期的解决方案。有时你手头的硬件可能无法轻易更换，因为它是由限制性较强的
    IT 部门提供的，或者是因为你没有足够的资金购买新硬件，但却有很多“免费”的编程时间。
- en: If you decide to go down the GPU route, which for many problems is a very good
    solution, then you should typically set your design goal to be around a 10× (ten
    times) improvement in execution time of the program. The actual amount you achieve
    depends on the knowledge of the programmers and the time available, plus a huge
    contribution from the next issue we’ll talk about, how much parallelism there
    is in the application. At least a 2× or 3× speedup is a relatively easy goal,
    even for those new to CUDA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定走 GPU 路线，这对于许多问题来说是一个非常好的解决方案，那么你通常应该将设计目标设置为程序执行时间提高大约 10×（十倍）。你实际达到的效果取决于程序员的知识水平和可用时间，再加上我们接下来要讨论的一个重要因素——应用中并行性的程度。至少
    2× 或 3× 的加速是一个相对容易实现的目标，即使对于 CUDA 新手也是如此。
- en: Problem decomposition
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题分解
- en: 'The fundamental question here is simply this: Can the problem you have be broken
    down into chunks that can run in parallel; that is, is there an opportunity to
    exploit concurrency in the problem? If the answer is no, then the GPU is not the
    answer for you. You instead have to look at optimization techniques for the CPU,
    such as cache optimizations, memory optimizations, SIMD optimizations, etc. At
    least some of these we have covered on the GPU side in previous chapters and others
    are covered in this chapter. Many of these optimization techniques work very well
    on serial CPU code.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的根本问题其实就是：你面临的问题是否可以拆解成可以并行运行的任务块；也就是说，问题中是否有机会利用并发性？如果答案是否定的，那么 GPU 就不是你的解决方案。你需要考虑
    CPU 的优化技术，比如缓存优化、内存优化、SIMD 优化等。我们在前几章已经介绍了一些 GPU 方面的优化技巧，而这一章也会涉及其他的一些技巧。这些优化技术在串行
    CPU 代码中通常效果很好。
- en: Assuming you are able to partition the problem into concurrent chunks, the question
    then is how many? One of the main limiting factors with CPU parallelization is
    that there is often just not enough large-granularity (or coarse-grained) parallel
    work to be done. GPUs run thousands of threads, so the problem needs to be decomposed
    into thousands of blocks, not just a handful of concurrent tasks as with the CPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你能够将问题划分为并发的任务块，那么问题就是多少个块？CPU 并行化的一个主要限制因素是，通常没有足够的大粒度（或粗粒度）并行工作可做。GPU 可以运行成千上万的线程，因此问题需要被拆解为成千上万个块，而不仅仅是像
    CPU 那样的少数并发任务。
- en: The problem decomposition should always start with the data first and the tasks
    to be performed second. You should try to represent the problem in terms of the
    output dataset. Can you construct a formula that represents the value of a given
    output point in the dataset as a transformation of the input dataset *for that
    single point*? You may need more than one formula, for example, one for most data
    points and one for the data points around the edge of the problem space. If you
    can do this, then the transformation of a problem into the GPU space is relatively
    easy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 问题分解应该始终从数据开始，然后是要执行的任务。你应该尽量将问题表示为输出数据集的形式。你能否构造出一个公式，将数据集中给定输出点的值表示为输入数据集的*该单个点*的变换？你可能需要不止一个公式，例如，一个用于大多数数据点，另一个用于问题空间边缘的数据点。如果你能做到这一点，那么将问题转化为
    GPU 处理空间相对容易。
- en: One of the issues with this type of approach is that you need to fully understand
    the problem for the best benefit. You can’t simply peek at the highest CPU “hogs”
    and try to make them parallel. The real benefit of this approach comes from making
    the chain from the input data points to the output data points completely parallel.
    There may be parts of this chain where you could use 100,000 processors if you
    had the hardware and points where you are reduced to a few hundred processors.
    Rarely are any problems truly single threaded. It’s just that as programmers,
    scientists, and engineers, this is the solution we may have learned many years
    ago at university. Thus, seeing the potential parallelism in a problem is often
    the first hurdle.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个问题是，你需要完全理解问题才能获得最佳的效果。你不能仅仅查看CPU使用最多的“占用者”并尝试使它们并行。此方法的真正好处来自于使从输入数据点到输出数据点的链条完全并行。这个链条中可能有些部分，如果你有硬件，可能使用100,000个处理器，而某些部分则可能仅能使用几百个处理器。几乎没有任何问题是完全单线程的。只是作为程序员、科学家和工程师，这是我们多年前在大学里学到的解决方案。因此，识别问题中的潜在并行性通常是第一个障碍。
- en: Now there are some problems where this single-output data point view is not
    practical—H264 video encoding, for example. In this particular problem, there
    are a number of stages defined, each of which defines a variable-length output
    data stream. However, there are aspects—filtering, in particular—within image
    encoding/processing that easily lend themselves to such approaches. Here the destination
    pixel is a function of *N* source pixels. This analogy works well in many scientific
    problems. The value of the forces of a given destination atom can be written as
    the sum of all the atoms that apply a force to the given destination atom. Where
    the input set is very large, simply apply a threshold or cutoff point such that
    those input data points that contribute very little are excluded from the dataset.
    This will contribute a small amount of error, but in some problems allows a huge
    section of the dataset to be eliminated from the calculation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有些问题，在这种单输出数据点的视角下并不实际——例如H264视频编码。在这个特定的问题中，定义了多个阶段，每个阶段都定义了一个可变长度的输出数据流。然而，在图像编码/处理中，有一些方面，特别是滤波操作，非常适合采用这种方法。在这里，目标像素是*N*个源像素的函数。这个类比在许多科学问题中都非常有效。给定目标原子的力值可以表示为所有对该目标原子施加力的原子的力之和。当输入集非常大时，只需应用一个阈值或截止点，将那些贡献极少的输入数据点从数据集中排除。这会产生一些小的误差，但在某些问题中，这样做可以消除计算中的大量数据集部分。
- en: Optimization used to be about how to optimize the operations or functions being
    performed on the data. However, as compute capacity has increased hugely in comparison
    to memory bandwidth, it’s now the data that is the primary consideration. Despite
    the fact GPUs have on the order of 5 to 10 times the memory bandwidth of CPUs,
    you have to decompose the problem such that this bandwidth can be used. This is
    something we’ll talk about in the following section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 优化曾经主要是关于如何优化对数据执行的操作或函数。然而，随着计算能力与内存带宽相比大幅提升，现在主要的考虑因素变成了数据。尽管GPU的内存带宽大约是CPU的5到10倍，但你必须将问题分解，以便能够充分利用这种带宽。这是我们将在下一节讨论的内容。
- en: One final consideration here, if you plan to use multiple GPUs or multiple GPU
    nodes, is how to decompose the problem and the dataset over the processor elements.
    Communication between nodes will be *very* expensive in terms of computation cycles
    so it needs to be minimized and overlapped with computation. This is something
    we’ll touch on later.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最后一个考虑因素是，如果你计划使用多个GPU或多个GPU节点，那么如何将问题和数据集分解到处理器元素上。节点之间的通信在计算周期方面将是*非常*昂贵的，因此需要尽量减少，并与计算重叠进行。这是我们稍后将讨论的内容。
- en: Dependencies
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 依赖性
- en: A dependency is where some calculation requires the result of a previous calculation,
    be that some calculation in the problem domain or simply an array index calculation.
    In either case, the dependency causes a problem in terms of parallel execution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖性是指某些计算需要依赖先前计算的结果，无论是问题领域中的某些计算，还是仅仅是数组索引计算。在任何情况下，依赖性都会导致并行执行方面的问题。
- en: Dependencies are seen in two main forms, where one element is dependent on one
    or more elements around it, or where there are multiple passes over a dataset
    and there exists a dependency from one pass to the next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖性主要有两种形式，一种是一个元素依赖于周围一个或多个元素，另一种是对数据集进行多次遍历，并且存在从一次遍历到下一次遍历的依赖关系。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`}`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: If you consider this example, you can see that both `a` and `c` have a dependency
    on `b`. You can also see that `d` has a dependency on both `a` and `c`. The calculation
    of `a` and `c` can be done in parallel, but the calculation of `d` requires the
    calculation of both `a` and `c` to have completed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑这个例子，你会发现`a`和`c`都依赖于`b`。你也会看到`d`依赖于`a`和`c`。`a`和`c`的计算可以并行进行，但`d`的计算要求`a`和`c`都已完成计算。
- en: In a typical superscalar CPU, there are multiple independent pipelines. The
    independent calculations of `a` and `c` would likely be dispatched to separate
    execution units that would perform the multiply. However, the results of those
    calculations would be needed prior to being able to compute the addition operation
    for `a` and `c`. The result of this addition operation would also need to be available
    before the final multiplication operation could be applied.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的超标量CPU中，有多个独立的管道。`a`和`c`的独立计算可能会被分配到不同的执行单元，这些单元将执行乘法操作。然而，在能够计算`a`和`c`的加法操作之前，需要这些计算的结果。这次加法操作的结果也需要在最终的乘法操作应用之前准备好。
- en: This type of code arrangement allows for little parallelism and causes a number
    of stalls in the pipeline, as the results from one instruction must feed into
    the next. While stalled, the CPU and GPU would otherwise be idle. Clearly this
    is a waste, and both CPUs and GPUs use multiple threads to cover this problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种代码安排允许的并行性很少，并且会导致管道中的许多停顿，因为一条指令的结果必须传递给下一条指令。在停顿期间，CPU和GPU将处于空闲状态。显然，这是浪费，因此CPU和GPU都使用多个线程来解决这个问题。
- en: 'On the CPU side, instruction streams from other virtual CPU cores fill in the
    gaps in the instruction pipeline (e.g., hyperthreading). However, this requires
    that the CPU know from which thread the instruction in the pipeline belongs, which
    complicates the hardware. On the GPU, multiple threads are also used, but in a
    time-switching manner, so the latency of the arithmetic operations is hidden with
    little or no cost. In fact, on the GPU you need around 20 clocks to cover such
    latency. However, this latency need not come from another thread. Consider the
    following example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU端，来自其他虚拟CPU核心的指令流填补了指令管道中的空隙（例如超线程技术）。然而，这需要CPU知道管道中的指令来自哪个线程，这增加了硬件的复杂性。在GPU上，也使用多个线程，但以时间切换的方式进行，因此算术操作的延迟通过几乎没有成本或无成本的方式被隐藏。事实上，在GPU上，你大约需要20个时钟周期来覆盖这种延迟。然而，这种延迟不一定来自另一个线程。考虑以下示例：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here the code has been rearranged and some new terms introduced. Notice if you
    insert some independent instructions between the calculation of `a` and `c` and
    their use in `d`, you allow these calculations more time to complete before the
    result is obtained. The calculations of `f`, `g`, and `h` in the example are also
    overlapped with the `d` calculation. In effect, you are hiding the arithmetic
    execution latency through overlapping nondependent instructions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，代码已被重新排列，并引入了一些新的术语。注意，如果你在`a`和`c`的计算与它们在`d`中的使用之间插入一些独立的指令，你可以为这些计算提供更多时间，在得到结果之前完成计算。示例中的`f`、`g`和`h`的计算也与`d`的计算重叠。实际上，你通过重叠非依赖指令来隐藏算术执行延迟。
- en: One way of handling dependencies and introducing additional nondependent instructions
    is through a technique called loop fusion, as shown here.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 处理依赖关系并引入额外非依赖指令的一种方法是通过一种叫做循环融合（loop fusion）的技术，具体如图所示。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`}`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: In this example, we have two independent calculations for results `a` and `d`.
    The number of iterations required in the second calculation is more than the first.
    However, the iteration space of the two calculations overlaps. You can, therefore,
    move part of the second calculation into the loop body of the first, as shown
    in function `loop_fusion_example_fused_01`. This has the effect of introducing
    additional, nondependent instructions, plus reducing the overall number of iterations,
    in this example, by one-third. Loop iterations are not free, as they need a loop
    iteration value and cause a branch. Thus, discarding a third of them brings us
    a significant benefit in terms of reducing the number of instructions executed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们有两个独立的计算，分别得到结果`a`和`d`。第二个计算所需的迭代次数多于第一个。然而，这两个计算的迭代空间是重叠的。因此，你可以将第二个计算的一部分移动到第一个计算的循环体内，如函数`loop_fusion_example_fused_01`所示。这会引入额外的非依赖指令，并在这个例子中将总迭代次数减少三分之一。循环迭代并非免费的，因为它们需要循环迭代值并会导致分支。因此，去掉三分之一的迭代给我们带来了显著的好处，减少了执行的指令数量。
- en: In the `loop_fusion_example_fused_02` we can further fuse the two loops by eliminating
    the second loop and fusing the operation into the first, adjusting the loop index
    accordingly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在`loop_fusion_example_fused_02`中，我们可以通过消除第二个循环并将操作合并到第一个循环中来进一步融合这两个循环，同时相应地调整循环索引。
- en: Now in the GPU it’s likely these loops would be unrolled into threads and a
    single kernel would calculate the value of `a` and `d`. There are a number of
    solutions, but the most likely is one block of 100 threads calculating `a` with
    an additional block of 200 threads calculating `d`. By combining the two calculations,
    you eliminate the need for an additional block to calculate `d`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在GPU中，这些循环很可能会被展开为线程，并且一个内核会计算`a`和`d`的值。有多种解决方案，但最可能的情况是一个包含100个线程的块计算`a`，另一个包含200个线程的块计算`d`。通过将这两种计算结合在一起，您可以消除为计算`d`而需要的额外块。
- en: However, there is one word of caution with this approach. By performing such
    operations, you are reducing the overall amount of parallelism available for thread/block-based
    scheduling. If this is already only a small amount, this will hurt the execution
    time. Also be aware that kernels, when fused, will usually consume more temporary
    registers. This may limit the amount of fusion you can practically achieve, as
    it will limit the number of blocks scheduled on an SM due to increased register
    usage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，采用这种方法时需要注意一点。通过执行这些操作，您会减少线程/块调度可用的并行度。如果并行度本身就已经很小，这会影响执行时间。还要注意，内核在融合后通常会消耗更多的临时寄存器。这可能会限制您实际能实现的融合量，因为它会限制在一个SM上调度的块数量，原因是寄存器使用量增加。
- en: Finally, you should consider algorithms where there are multiple passes. These
    are typically implemented with a number of sequential kernel calls, one for each
    pass over the data. As each pass reads and writes global data, this is typically
    very inefficient. Many of these algorithms can be written as kernels that represent
    a single or small set of destination data point(s). This provides the opportunity
    to hold data in shared memory or registers and considerably increases the amount
    of work done by a given kernel, compared with the number of global memory accesses.
    This will vastly improve the execution times of most kernels.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您应该考虑那些需要多次迭代的算法。通常这些算法会通过多个顺序的内核调用来实现，每次迭代处理一部分数据。由于每次迭代都需要读取和写入全局数据，这通常效率较低。许多这类算法可以被编写成内核，代表单个或少量的目标数据点。这样可以将数据保存在共享内存或寄存器中，并大大增加每个内核所做的工作量，相较于全局内存的访问次数。这将大大提高大多数内核的执行速度。
- en: Dataset size
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集大小
- en: 'The size of the dataset makes a huge difference as to how a problem can be
    handled. These fall into a number of categories on a typical CPU implementation:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的大小对问题的处理方式有巨大影响。这些数据集在典型的CPU实现中属于多个类别：
- en: • Dataset within L1 cache (~16 KB to 32 KB)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L1缓存中（约16 KB至32 KB）
- en: • Dataset within L2 cache (~256 KB to 1 MB)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L2缓存中（约256 KB至1 MB）
- en: • Dataset within L3 cache (~512 K to 16 MB)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L3缓存中（约512 K至16 MB）
- en: • Dataset within host memory on one machine (~1 GB to 128 GB)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在一台机器的主机内存中（约1 GB至128 GB）
- en: • Dataset within host-persistent storage (~500 GB to ~20 TB)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在主机持久存储中（约500 GB至约20 TB）
- en: • Dataset distributed among many machines (>20 TB)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集分布在多台机器上（大于20 TB）
- en: 'With a GPU the list looks slightly different:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU时，列表略有不同：
- en: • Dataset within L1 cache (~16 KB to 48 KB)^([1](CHP009_a.html#FN1))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L1缓存中（约16 KB至48 KB）^([1](CHP009_a.html#FN1))
- en: • Dataset within L2 cache (~512 KB to 1536 MB)^([2](CHP009_a.html#FN2))
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L2缓存中（约512 KB至1536 MB）^([2](CHP009_a.html#FN2))
- en: • Dataset within GPU memory (~512 K to 6 GB)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在GPU内存中（约512 K至6 GB）
- en: • Dataset within host memory on one machine (~1 GB to 128 GB)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在一台机器的主机内存中（约1 GB至128 GB）
- en: • Dataset within host-persistent storage (~500 GB to ~20 TB)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在主机持久存储中（约500 GB至约20 TB）
- en: • Dataset distributed among many machines (>20 TB)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集分布在多台机器上（大于20 TB）
- en: For very small problem sets, adding more CPU cores to a particular problem can
    result in a superscalar speedup. This is where you get more than a linear speedup
    by adding more CPU cores. What is happening in practice is that the dataset each
    processor core is given is now smaller. With a 16-core CPU, the problem space
    is typically reduced by a factor of 16\. If this now moves the problem from memory
    to the L3 cache or the L3 cache to the L2 cache, you see a very impressive speedup,
    not due to parallelism, but due instead to the much higher-memory bandwidth of
    the associated cache. Obviously the same applies when you transition from the
    L2 cache to holding the problem entirely in the L1 cache.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常小的问题集，向特定问题添加更多的CPU核心可能会带来超标量加速。这意味着通过添加更多的CPU核心，您获得的加速比线性加速更高。实际发生的情况是，每个处理器核心获得的数据集现在变得更小。对于16核的CPU，问题空间通常会缩小16倍。如果这将问题从内存移动到L3缓存，或者从L3缓存移动到L2缓存，您会看到非常显著的加速，这并非由于并行性，而是由于相关缓存的内存带宽更高。当然，当您将问题从L2缓存转移到完全存储在L1缓存时，情况也是如此。
- en: The major question for GPUs is not so much about cache, but about how much data
    can you hold on a single card. Transferring data to and from the host system is
    expensive in terms of compute time. To hide this, you overlap computation with
    data transfers. On the more advanced cards, you can do a transfer in and a transfer
    out at the same time. However, for this to work you need to use pinned memory
    on the host. As pinned memory can’t be swapped out by the virtual memory management
    system, it has to be real DRAM memory on the host.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU来说，主要的问题不是缓存，而是单个卡上可以容纳多少数据。将数据传输到主机系统和从主机传输回去是非常耗费计算时间的。为了掩盖这一点，您可以将计算和数据传输重叠进行。在更先进的显卡上，您可以同时进行一次数据传入和一次数据传出。然而，为了实现这一点，您需要在主机上使用固定内存。由于固定内存无法被虚拟内存管理系统交换出去，因此它必须是主机上的实际DRAM内存。
- en: On a 6 GB Tesla system you might have allocated this as a 1 GB input buffer,
    a 1 GB output buffer, and 4 GB compute or working memory. On commodity hardware,
    you have up to 2 GB available, so much less to work with, although some commodity
    cards support up to 4 GB of global memory.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个6 GB的Tesla系统上，您可能已将其分配为1 GB的输入缓冲区、1 GB的输出缓冲区和4 GB的计算或工作内存。在普通硬件上，您最多可用2 GB的内存，虽然一些普通卡支持高达4
    GB的全局内存，但可用的内存要少得多。
- en: On the host side, you need at least as much memory as you pin for the input
    and output buffers. You typically have up to 24 GB available (6 DIMMs at 4 GB)
    on most I7 Nehalem platforms, 32 GB (8 DIMMs at 4 GBs) on Sandybridge–EP I7, and
    16 GB on AMD platforms (4 DIMMs at 4 GB). As you’d typically pin only 2 GB maximum,
    you easily have room to support multiple GPUs. Most systems have support for at
    least two GPU cards. Four physical cards is the practical limit for a top-end
    system in one box.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机端，您需要至少和输入输出缓冲区固定的内存一样多。大多数I7 Nehalem平台上通常可以提供最多24 GB内存（6个DIMM，每个4 GB），Sandybridge–EP
    I7平台提供32 GB内存（8个DIMM，每个4 GB），AMD平台提供16 GB内存（4个DIMM，每个4 GB）。由于您通常只会固定最多2 GB内存，因此您很容易有足够的空间支持多个GPU。大多数系统至少支持两张GPU卡。四张物理卡是单台高端系统的实际限制。
- en: When the problem size is much larger than the host memory size, you have to
    consider the practical limits of the storage capacity on a single host. Multiterabyte
    disks can allow node storage into the tens of terabytes. Most motherboards are
    equipped with six or more SATA connectors and 4 TB-plus disks are readily available.
    Disks are easily transportable if the dataset is to be captured in some remote
    area. Next-day courier can often be the fastest way to transfer such data between
    sites.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题的规模远大于主机内存大小时，您必须考虑单台主机上存储容量的实际限制。多TB硬盘可以使节点存储达到数十TB。大多数主板都配备了六个或更多SATA连接器，4
    TB以上的硬盘也很容易获得。如果数据集需要在某个偏远地区捕获，硬盘非常易于运输。第二天的快递通常是不同站点之间传输此类数据的最快方式。
- en: Finally, when you cannot fit the dataset on a single machine, be it from compute,
    memory, storage, or power requirements, you have to look at multiple nodes. This
    brings you to the realm of internode communication. Internode communication is
    expensive in terms of time, at least an order of magnitude slower than any internal
    communication of data. You also have to learn another set of APIs, so this step
    is really best avoided if the problem can be contained to a single node.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当您无法将数据集放入单台机器时，不论是因为计算、内存、存储还是电力需求，您必须考虑使用多个节点。这就引出了节点间通信的领域。节点间通信在时间上是昂贵的，至少比任何内部数据通信慢一个数量级。您还需要学习另一套API，因此如果问题可以限制在单个节点内，最好避免这个步骤。
- en: Resolution
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分辨率
- en: Consider the question of what can be done with 10 times or 50 times as much
    processing power. An existing problem that previously took one hour to resolve
    can be done in just over a minute. How does this change the questions that can
    be asked with a given dataset? What can now be done in real time or near real
    time that was impossible in the past? The previous batch submission problem is
    now an interactive problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 想想如果有10倍或50倍的处理能力，能做些什么。一个以前需要一个小时才能解决的问题，现在可以在一分钟左右完成。这如何改变在给定数据集下能提出的问题？过去不可能实时或接近实时完成的任务，现在能做什么？之前的批处理提交问题，现在变成了一个互动性问题。
- en: Such a change allows for a step back from the problem, to consider how else
    it might be approached. Are there algorithms that were discarded in the past because
    they were too computationally expensive? Can you now process far more data points,
    or data points to a higher resolution, to produce a more accurate result? If you
    were previously happy with a runtime of a few hours or a day because that let
    you get on with other tasks, does increasing the resolution of the problem appeal
    more than the speedup? What does a more accurate result gain in your problem domain?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变化允许从问题中退后一步，考虑其他可能的解决方法。过去是否有一些算法因计算成本过高而被舍弃？现在你是否能处理更多的数据点，或者以更高的分辨率处理数据点，以产生更精确的结果？如果你过去因为几小时或一天的运行时间能够同时进行其他任务而感到满意，那么提高问题分辨率是否比加速更具吸引力？在你的问题领域，更准确的结果带来了什么收益？
- en: In finance applications, if your mathematical model of events is running ahead
    of the main market players, then you can react to changes faster than others,
    which can directly translate into making a better return on trading activities.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融应用中，如果你的事件数学模型领先于主要市场参与者，那么你就能比别人更快地对变化作出反应，这可以直接转化为更好的交易回报。
- en: In medical applications, being able to present the doctor with the result of
    a test before the patient has finished getting dressed and left allows much more
    efficient use of both the doctor’s and patient’s time as it avoids repeat appointments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学应用中，能够在病人穿好衣服并离开之前就向医生展示测试结果，可以更高效地利用医生和病人的时间，避免重复预约。
- en: In simulation applications, not having to wait a long time allows a much larger
    problem space to be explored within a given timeframe. It also allows for speculative
    execution. This is where you ask the system to explore all values of *x* between
    *n* and *m* in a given dataset. Equally, you might explore variables in the 2D
    or 3D space. With complex problems or a nonlinear system it’s not always clear
    what the optimal solution is, especially when changing one parameter impacts many
    other parameters. It may be quicker to simply explore the problem space and observe
    the result than it is to have an expert try to sit down and work out the optimal
    solution. This brute-force approach is remarkably effective and will often come
    up with solutions the “experts” would not have considered.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在仿真应用中，不必长时间等待，可以在给定的时间框架内探索更大的问题空间。它还允许进行推测性执行。这是指你要求系统在给定数据集中的*n*和*m*之间探索所有*x*值。同样，你可能会探索二维或三维空间中的变量。对于复杂问题或非线性系统，最佳解决方案并不总是显而易见，特别是当改变一个参数会影响许多其他参数时。与其让专家坐下来尝试计算出最佳解决方案，不如直接探索问题空间并观察结果，这可能会更快。这个暴力破解方法异常有效，常常会找到“专家”未曾考虑过的解决方案。
- en: As a student you can now kick off a problem between lectures on your personal
    desktop supercomputer, rather than submit a job to the university machine and
    wait a day for it to run, only to find out it crashed halfway through the job.
    You can prototype solutions and come up with answers far quicker than your non-CUDA-literate
    peers. Think what you could cover if their batch jobs take a day and yours are
    done locally in an hour.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为学生，你现在可以在个人桌面超级计算机上解决问题，而不需要将作业提交给学校的计算机并等待一天才完成，结果却发现作业在中途崩溃。你可以原型化解决方案，比没有CUDA知识的同学更快地得出答案。想一想，如果他们的批处理作业需要一天，而你的作业在本地一个小时内完成，你能解决多少问题。
- en: Identifying the bottlenecks
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别瓶颈
- en: Amdahl’s Law
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阿姆达尔定律
- en: Amdahl’s Law is often quoted in work on parallel architecture. It’s important
    because it tells us that, while serial elements of execution remain in the data
    flow, they will limit any speedup we can achieve.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 阿姆达尔定律通常在并行架构的研究中被引用。它很重要，因为它告诉我们，当执行流程中仍然存在串行元素时，它们会限制我们能够实现的加速。
- en: Consider the simple case where we have 50% of the program’s execution time spent
    on a section that could run in parallel and 50% that must be done serially. If
    you had an infinitely fast set of parallel processing units and you reduced the
    parallel aspect of the program down to zero time, you would still have the 50%
    serial code left to execute. The maximum possible speedup in this case is 2×,
    that is, the program executes in half the time period it did before. Not very
    impressive, really, given the huge amount of parallel processing power employed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设程序执行时间的50%是在一个可以并行运行的部分，另外50%是必须串行执行的部分。如果你有一组无限快的并行处理单元，并将程序的并行部分缩短为零时间，你仍然会剩下50%的串行代码需要执行。在这种情况下，最大可能的加速是2倍，也就是说，程序执行的时间是之前的一半。考虑到使用了大量的并行处理能力，这其实并不那么令人印象深刻。
- en: Even in the case where we have 90% of the program that could be parallelized,
    we still have the 10% serial code that remains. Thus, the maximum speedup is 9×,
    or nine times faster than the original, entirely serial, program.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们有90%程序可以并行化的情况下，我们仍然有10%的串行代码需要执行。因此，最大加速是9倍，也就是比原本完全串行的程序快9倍。
- en: The only way to scale a program infinitely is to eliminate all serial bottlenecks
    to program execution. Consider the diagram in [Figure 9.2](#F0015), where all
    the squares represent data items that need to be processed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 无限扩展一个程序的唯一方法是消除所有串行瓶颈。请参见[图9.2](#F0015)，其中所有的方块代表需要处理的数据项。
- en: '![image](../images/F000090f09-02-9780124159334.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-02-9780124159334.jpg)'
- en: FIGURE 9.2 Data flow bottlenecks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 数据流瓶颈。
- en: In this example, there are 10 threads, each processing one column of the data.
    In the center is a dependency, and thus all the threads must contribute their
    existing result to a single value before proceeding.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，有10个线程，每个线程处理数据的一个列。在中间是一个依赖关系，因此所有线程必须将各自的结果贡献给一个单一的值，然后才能继续处理。
- en: Imagine, for one moment, this is a field of crops, with each column a line of
    crops. Each thread is like a combine harvester, moving down the columns and collecting
    crops at each square. However, at the center of the field there is a wall with
    two gates.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，这是一片农田，每列都是一行作物。每个线程就像一台联合收割机，沿着列向下移动，在每个方块上收集作物。然而，在田地的中央有一堵墙，墙上有两个门。
- en: With 1 or even 2 combine harvesters, the gates pose a small problem and each
    combine harvester passes from one field to another. With 10 combine harvesters,
    one per column, getting each one through the gate takes time and slows down everyone
    in the process. This is one of the reasons why it’s far more efficient to have
    large, open fields, rather than smaller, bounded ones.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只有1个甚至2个联合收割机，门并不会构成太大问题，每台联合收割机可以从一个田地移动到另一个田地。而如果有10台联合收割机，每台收割机负责一列，想要让每台机器都通过门就需要时间，这会使整个过程变慢。这就是为什么大而开阔的田地比小而有限的田地更高效的原因之一。
- en: So how is this relevant to software? Each gate is like a serial point in the
    code. The program is doing well, churning through the chunks of work, and then
    all of a sudden it hits a serial point or synchronization point and everything
    backs up. It’s the same as everyone trying to leave the parking lot at the same
    time through a limited number of exits.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这与软件有什么关系呢？每个门就像代码中的串行点。程序正在顺利执行，处理着一块块的工作，突然之间它遇到一个串行点或同步点，所有的工作都停滞了。这就像每个人试图同时通过有限的出口离开停车场。
- en: The solution to this type of problem is to parallelize up the bottlenecks. If
    we have 10 gates in the field or 10 exits from the parking lot, there would be
    no bottleneck, just an orderly queue that would complete in *N* cycles.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种问题的方法是对瓶颈进行并行化。如果我们有10个田地门或10个停车场出口，就不会出现瓶颈，只有一个有序的队列，它将在*N*个周期内完成。
- en: When you consider algorithms like histograms, you see that having all threads
    add to the same set of bins forms exactly this sort of bottleneck. This is often
    done with atomic operations, which effectively introduce serial execution to a
    set of parallel threads. If, instead, you give every thread a set of its own bins
    and then add these sets together later, you remove the serialization bottleneck.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑像直方图这样的算法时，你会发现让所有线程都往同一组桶里添加数据就会形成这种瓶颈。通常这是通过原子操作来完成的，这实际上会在一组并行线程中引入串行执行。如果你给每个线程分配一组自己的桶，然后再将这些桶的结果合并，便可以消除串行瓶颈。
- en: Consider carefully in your code where you have such bottlenecks and how these
    might be eliminated. Often they will limit the maximum scaling available to your
    application. While this may not be an issue with two or even four CPU cores, with
    GPU code you need to think about tens of thousands of parallel threads.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑代码中可能存在的瓶颈，并思考如何消除这些瓶颈。通常这些瓶颈会限制你应用程序的最大扩展性。虽然在两个或四个CPU核心的情况下可能没有问题，但对于GPU代码，你需要考虑成千上万的并行线程。
- en: Profiling
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能分析
- en: Profiling is one of the most useful tasks in identifying where you are today
    and knowing where you should spend your time. Often people think they know where
    the bottlenecks are, then go off and optimize that routine, only to find it makes
    1% or 2% difference to the application’s overall execution time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析是识别你当前状态并确定时间分配的最有用的任务之一。很多人认为自己知道瓶颈在哪里，然后去优化那个程序，结果发现它对应用程序整体执行时间的改善只有1%或2%。
- en: In modern software development, there are usually many teams working on various
    aspects of a software package. It may not be possible to keep in contact with
    everyone who touches the software, especially in larger teams. Often what you
    may think is the bottleneck is not really that important.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代软件开发中，通常有多个团队在处理软件包的不同方面。与所有涉及软件开发的人保持联系可能并不现实，尤其是在更大的团队中。你可能认为的瓶颈往往并不是最重要的因素。
- en: Optimization should be based on hard numbers and facts, not speculation about
    what “might” be the best place to apply the software effort in terms of optimization.
    NVIDIA provides two good tools, CUDA Profiler and Parallel Nsight, that provide
    profiling information.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 优化应基于硬数据和事实，而不是对“可能”是最佳优化位置的推测。NVIDIA提供了两个很好的工具，CUDA Profiler和Parallel Nsight，它们提供了详细的性能分析信息。
- en: Profilers reveal, through looking at hardware counters, where the code spends
    it time, and also the occupancy level of the GPU. They provide useful counters
    such as the number of coalesced reads or writes, the cache hit/miss ratio, branch
    divergence, warp serialization, etc. The CUDA Memcheck tool is also very useful
    in identifying inefficient usage of memory bandwidth.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析工具通过查看硬件计数器，揭示代码的时间分布，以及GPU的占用率水平。它们提供了有用的计数器，如合并读取或写入的次数、缓存命中/未命中率、分支分歧、warp序列化等。CUDA
    Memcheck工具对于识别内存带宽使用不当也非常有用。
- en: Having done an initial run using the profiler, you should first look at the
    routine in which the code spends the most *total* time. Typical unoptimized programs
    spend 80% of their time in 20% of the code. Optimizing the 20% is the key to efficient
    use of your time and profiling is the key to identifying that 20% of the code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用性能分析器进行初步测试后，首先应查看代码在其中花费最多*总*时间的程序。典型的未优化程序在20%的代码中花费了80%的时间。优化这20%是高效利用时间的关键，而性能分析是识别这20%代码的关键。
- en: Of course once this has been optimized as best as it can be, it’s then progressively
    more and more time consuming to provide further speedups without a complete redesign.
    Measure the speedup and know when the time you’re spending is no longer providing
    a good return on that effort.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一旦优化达到最佳状态后，进一步加速将变得越来越耗时，而不进行完全重构将难以取得显著提升。衡量加速效果，并知道什么时候你投入的时间不再带来良好的回报。
- en: Parallel Nsight is a very useful tool in this regard as it provides a number
    of default “experiments.” That shed light on what your kernels are actually doing.
    Some off the more useful information you can take from the experiments is shown
    in [Figure 9.3](#F0020).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，Parallel Nsight 是一个非常有用的工具，因为它提供了多个默认的“实验”。这些实验可以揭示你的内核实际在做什么。从这些实验中，你可以获取的一些有用信息如[图
    9.3](#F0020)所示。
- en: '![image](../images/F000090f09-03-9780124159334.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-03-9780124159334.jpg)'
- en: FIGURE 9.3 Parallel Nsight experiments.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 Parallel Nsight 实验。
- en: The first experiment is the CUDA Memory Statistics, which provides a nice graphical
    view of how the caches are laid out and the bandwidth being achieved in the different
    parts of the device.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验是CUDA内存统计，它提供了一个图形化的视图，展示了缓存的布局以及在设备不同部分实现的带宽。
- en: This particular example (see [Figure 9.4](#F0025)) is taken from the odd/even
    sort we’ll look at a little later. What is interesting to note are the cache ratios.
    As we’re getting a 54% hit ratio in the L1 cache, we’re achieving an average throughput
    of 310 GB/s to global memory, in the order of double the actual bandwidth available
    from global memory. It also lists the number of transactions, which is important.
    If we can lower the number of transactions needed, through better coalescing and/or
    issuing larger reads/writes, we can significantly boost memory throughput.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具体的例子（见[图9.4](#F0025)）取自我们稍后将要讨论的奇偶排序。值得注意的是缓存命中率。由于在L1缓存中获得了54%的命中率，我们实现了每秒310GB的全球内存吞吐量，大约是全球内存实际带宽的两倍。文中还列出了事务数量，这一点很重要。如果我们能通过更好的合并和/或发出更大的读写请求来减少所需的事务数量，就可以显著提高内存吞吐量。
- en: '![image](../images/F000090f09-04-9780124159334.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-04-9780124159334.jpg)'
- en: FIGURE 9.4 Parallel Nsight memory overview.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 Parallel Nsight内存概览。
- en: The other important experiment is occupancy rates (see [Figure 9.5](#F0030)).
    In this experiment, notice the Achieved occupancy column and in particular the
    number of Active Warps. As this is a compute 2.0 device, we can have up to 48
    warps resident on a single SM. The achieved occupancy, as opposed to the theoretical
    occupancy, is the measured value of what was actually achieved. This will usually
    be significantly less than the theoretical maximum. Notice also that any limiting
    factor is highlighted in red, in this case the number of blocks per SM at six.
    The “occupancy” graphs tab allows you to understand this in somewhat more detail.
    It’s an extract from the occupancy calculation spreadsheet provided with the CUDA
    SDK.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的实验是占用率（见[图9.5](#F0030)）。在这个实验中，请注意“已实现占用率”栏，特别是“活动warp”的数量。由于这是一个compute
    2.0设备，我们最多可以在单个SM上保留48个warp。已实现的占用率与理论占用率不同，它是实际测量得到的占用率。通常情况下，这个值会显著低于理论最大值。还要注意，任何限制因素都会以红色突出显示，在这个例子中是每个SM的块数为6。图表中的“占用率”选项卡允许你更详细地了解这一点。这是CUDA
    SDK中提供的占用率计算电子表格的摘录。
- en: '![image](../images/F000090f09-05-9780124159334.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-05-9780124159334.jpg)'
- en: FIGURE 9.5 Parallel Nsight occupancy data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 Parallel Nsight占用率数据。
- en: The cause of this limit is actually the number of threads. Dropping this from
    256 to 192 would allow the hardware to schedule eight blocks. As this kernel has
    synchronization points, having more blocks available may introduce a better instruction
    mix. There will also be fewer warps that are unable to run due to the synchronization
    point.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个限制的原因实际上是线程的数量。将线程数从256减少到192，硬件就可以调度八个块。由于该内核存在同步点，更多的块可用可能会带来更好的指令混合。同时，由于同步点的存在，将会有更少的warp无法执行。
- en: In practice, making this change helps quite significantly. It improves occupancy
    from 98.17% to 98.22%, which is marginal at best. However, the execution time
    drops from 14 ms to just 10 ms. The answer to this is in the memory usage. With
    192 threads per block, we’re accessing a smaller range of addresses which increases
    the locality of the accesses and consequently improves cache utilization. The
    total number of memory transactions needed by each SM drops by about one-quarter.
    Consequently, we see a proportional drop in execution time.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，做出这个改变会有相当显著的帮助。它将占用率从98.17%提高到98.22%，提升幅度很小。但执行时间却从14毫秒下降到仅10毫秒。原因就在于内存使用情况。每个块有192个线程时，我们访问的是较小范围的地址，从而增加了访问的局部性，进而提高了缓存利用率。每个SM所需的内存事务总数减少了大约四分之一。因此，我们看到执行时间也相应地下降。
- en: Grouping the tasks for CPU and GPU
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将任务分配给CPU和GPU
- en: Dr. M. Fatica from NVIDIA gave a great talk at GTC2010 concerning how Linpack
    had been optimized for GPUs. Linpack is a benchmark based on linear algebra. It
    is used in the Top500 supercomputer benchmark ([*www.top500.org*](http://www.top500.org))
    to benchmark the various supercomputers around the world. One interesting fact
    from this talk was the GPU used at that time, a Fermi Tesla C2050 card, produced
    around 350 gigaflops of DGEMM (double-precision matrix multiply) performance.
    The CPU used produced around 80 gigaflops. The contribution of 80 gigaflops is
    a little under one-quarter of the GPU contribution, so not something that can
    be ignored. A quarter or so extra performance goes a long way to reducing execution
    time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 NVIDIA 的 Dr. M. Fatica 在 GTC2010 上做了一场精彩的演讲，介绍了 Linpack 如何针对 GPU 进行优化。Linpack
    是一个基于线性代数的基准测试。它被用于 Top500 超级计算机基准测试（[*www.top500.org*](http://www.top500.org)），用来评估全球各种超级计算机的性能。在这场演讲中，有一个有趣的事实是，当时使用的
    GPU ——Fermi Tesla C2050 卡——产生了约 350 吉弗洛普的 DGEMM（双精度矩阵乘法）性能。而使用的 CPU 则产生了约 80 吉弗洛普。80
    吉弗洛普的贡献稍微不到 GPU 贡献的四分之一，因此并不是可以忽视的部分。大约四分之一的额外性能对于减少执行时间有着非常重要的作用。
- en: In fact, the best applications tend to be those that play to the strengths of
    both the CPU *and* the GPU and split the data accordingly. The CPU must be considered
    in any GPU-based optimization, because it’s the total application time that is
    important. If you have a four-, six-, or eight-core CPU and one core is busy handling
    a GPU application, why not use the other cores to also work on the problem? The
    more cores you have available, the higher the potential gain is by offloading
    some work to the CPU.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，最好的应用程序通常是那些能够充分发挥 CPU *和* GPU 优势的应用，并根据情况拆分数据。任何基于 GPU 的优化都必须考虑到 CPU，因为关键是整体应用时间。如果你有一个四核、六核或八核的
    CPU，而其中一个核心正忙于处理 GPU 应用，为什么不利用其他核心一起工作解决问题呢？你拥有的核心越多，将一部分工作卸载到 CPU 上所获得的潜在增益就越高。
- en: If we say the CPU can handle work at one-tenth the rate of the GPU, then with
    just three CPU cores, you’re gaining a 30% additional throughput. If you had an
    eight-core device, potentially this is a 70% gain in performance, which is almost
    the same as having two GPUs working in tandem. In practice, however, often other
    constraints might limit the overall speed, such as memory, network, or I/O bandwidth.
    However, even so, you’re likely to see a significant speedup where the application
    is not already bound by one of these constraints on the host side.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们说 CPU 的处理速度是 GPU 的十分之一，那么仅仅通过三个 CPU 核心，你就能获得 30% 的额外吞吐量。如果你有一个八核设备，理论上这是一个
    70% 的性能提升，几乎等同于有两个 GPU 协同工作。然而，在实际应用中，其他约束（如内存、网络或 I/O 带宽）往往会限制整体速度。然而，即便如此，当应用没有受到主机端这些约束的限制时，你仍然可能会看到显著的加速效果。
- en: Of these constraints, I/O is an interesting one, because introducing more CPU
    threads or processes can often significantly improve the overall I/O throughput.
    This may seem a strange statement, as surely the physical limits to and from an
    I/O device dictate the speed? On modern machines with large amounts of memory,
    most I/O is in fact cached. Therefore, I/O can be more about moving data in memory
    than it is about moving to or from devices. A decent RAID controller has its own
    processor to do the I/O operations. Multiple CPU cores allow for multiple independent
    memory transfers, which often provide a higher overall bandwidth than a single
    CPU core.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些约束中，I/O 是一个有趣的因素，因为增加更多的 CPU 线程或进程通常能显著提高整体 I/O 吞吐量。这听起来可能有些奇怪，因为显然 I/O 设备的物理限制决定了速度？但在现代配备大量内存的机器上，大多数
    I/O 实际上是被缓存的。因此，I/O 更多的是关于在内存中移动数据，而不是在设备之间移动数据。一款不错的 RAID 控制器有自己的处理器来执行 I/O 操作。多个
    CPU 核心可以进行多个独立的内存传输，通常会提供比单个 CPU 核心更高的整体带宽。
- en: Separate CPU process or threads can create a separate GPU context and launch
    their own kernel onto the GPU. These additional kernels are then queued within
    the GPU for execution. When available resources become free the kernel is executed.
    If you look at the typical GPU usage you see that shown in [Figure 9.6](#F0035).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 独立的 CPU 进程或线程可以创建一个独立的 GPU 上下文，并将其内核启动到 GPU 上。这些额外的内核随后会在 GPU 内排队等待执行。当可用资源释放时，内核会被执行。如果你查看典型的
    GPU 使用情况，你会看到 [图 9.6](#F0035) 所示的内容。
- en: '![image](../images/F000090f09-06-9780124159334.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-06-9780124159334.jpg)'
- en: FIGURE 9.6 CPU and GPU idle time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 CPU 和 GPU 空闲时间。
- en: Notice there is significant idle time on both the GPU and the CPU. Idle time
    on the GPU is more expensive, as it’s typically 10 times more useful than the
    CPU time. Tools such as Parallel Nsight allow you to display just such a timeline
    and you’ll be amazed to see just how much idle time certain kernels can create.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，GPU和CPU上都有显著的空闲时间。GPU上的空闲时间更为昂贵，因为它通常比CPU时间更为有用，通常是10倍。像Parallel Nsight这样的工具可以让你显示这样的时间线，你会惊讶于看到某些内核能产生多少空闲时间。
- en: By placing multiple kernels onto a single GPU, these kernels then slot into
    the empty slots. This increases, marginally, the latency of the first set of kernels
    but greatly improves the overall throughput of the application. In a lot of applications,
    there can be as much as 30% idle time. Just consider what a typical application
    will do. First, fetch data from somewhere, typically a slow I/O device like a
    hard drive. Then transfer the data to the GPU and then sit and wait until the
    GPU kernel is complete. When it’s complete, the host transfers the data off the
    GPU. It then saves it somewhere, usually to slow I/O storage, fetches the next
    data block, and so on.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将多个内核放置到单个GPU上，这些内核将插入空闲的插槽中。这会稍微增加第一组内核的延迟，但大大提高了应用程序的整体吞吐量。在许多应用程序中，可能会有多达30%的空闲时间。试想一下一个典型的应用程序会做什么。首先，从某个地方获取数据，通常是一个慢速I/O设备，比如硬盘。然后将数据传输到GPU，接着等待直到GPU内核完成。当完成时，主机将数据从GPU中传出。然后将其保存到某个地方，通常是慢速I/O存储，获取下一个数据块，依此类推。
- en: While the GPU is executing the kernel, why not fetch the next data block from
    the slow I/O device, so it’s ready when the GPU kernel has completed? This is,
    in effect, what happens when you execute multiple processes. The I/O device blocks
    the second process, while fetching data for the first. When the first process
    is transferring data and invoking the kernel, the second process is accessing
    the I/O hardware. It then does a transfer, while process one is computing and
    the kernel invocation of the second process is queued. When the transfer back
    to the host for process one starts, the kernel from process two also starts executing.
    Thus, with the introduction of just a couple of processes, you have neatly overlapped
    the I/O, CPU, GPU, and transfer times, gaining a significant improvement in overall
    throughput. See the stream example in [Chapter 8](CHP008.html) for a detailed
    explanation of this.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当GPU执行内核时，为什么不从慢速I/O设备获取下一个数据块，这样等GPU内核完成时就能准备好呢？实际上，这就是执行多个进程时发生的情况。I/O设备阻塞第二个进程，同时为第一个进程获取数据。当第一个进程正在传输数据并调用内核时，第二个进程则在访问I/O硬件。然后它执行传输操作，而进程一在计算，并且第二个进程的内核调用排队。当第一个进程的数据传输回主机时，第二个进程的内核也开始执行。因此，通过引入少数几个进程，你就巧妙地重叠了I/O、CPU、GPU和传输时间，显著提高了整体吞吐量。有关此内容的详细解释，请参见[第8章](CHP008.html)中的流示例。
- en: Note that you can achieve the same results using threads or processes. Threads
    allow the application data to share a common data area and provide faster synchronization
    primitives. Processes allow for processor affinity, where you lock a process to
    a given CPU core, which can often improve performance because it allows for better
    core-specific cache reuse. The choice depends largely on how much, if any, synchronization
    is needed between the CPU tasks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以使用线程或进程来实现相同的结果。线程允许应用程序数据共享一个公共数据区域，并提供更快的同步原语。进程允许处理器亲和性，即将进程绑定到某个CPU核心，这通常能提高性能，因为它有助于更好的核心特定缓存重用。选择主要取决于CPU任务之间是否需要同步，以及需要多少同步。
- en: The other aspect of the CPU/GPU decision is knowing how best to split the task.
    CPUs are great at serial problems, where the data is sparsely distributed, or
    where the dataset is small. However, with a typical 10:1 ratio of performance
    on the GPU to the CPU, you have to be careful that you will not be holding up
    the GPU. For this reason, many applications simply use the CPU to load and store
    data. This can sometimes fully load a single core on the CPU, depending on how
    much computation time is required on the GPU.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: CPU/GPU决策的另一个方面是如何最佳地拆分任务。CPU擅长处理串行问题，尤其是数据分布稀疏或数据集较小的情况。然而，在GPU对CPU的典型10:1性能比下，你必须小心不要阻塞GPU。因此，许多应用程序简单地使用CPU来加载和存储数据。根据GPU需要的计算时间，这有时可能会使CPU的单个核心负载满。
- en: One usage you sometimes see a CPU being used for is the final stages of a reduction.
    A reduction operation typically reduces itself by a factor of two on every iteration
    of the reduction. If you start out with a million elements, within six iterations
    you are starting to hit the maximum number of schedulable threads on a GPU. Within
    a few more iterations, several of the SMs are idle. With the GT200 and prior generation
    of hardware, kernels were not overlapped, so the kernel had to continue to iterate
    down to the final elements before it freed up the idle SMs to do more work.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你会看到CPU被用来处理归约的最终阶段。归约操作通常会在每次归约迭代时将元素数减少一半。如果你从一百万个元素开始，在六次迭代后，你就开始接近GPU上可调度线程的最大数量。再经过几次迭代，几个SM会处于空闲状态。在GT200及之前的硬件上，内核不会重叠执行，因此内核必须继续迭代，直到处理完最后的元素，才能释放空闲的SM去执行更多的工作。
- en: Thus, one optimization when a certain threshold is reached, is to forward the
    remaining part of the computation to the CPU to complete. If the CPU was in fact
    idle anyway, and the remaining data being transferred is not huge, this strategy
    can show significant gains over waiting for the GPU to complete the entire reduction.
    With Fermi, NVIDIA addressed this issue, allowing those idle SMs to start work
    on the next queued kernel. However, for the SM to become idle, it’s necessary
    for all the thread blocks to have completed. Some nonoptimal kernels will have
    one or more active threads, even at the final levels of the reduction, which pins
    the kernel to the SM until the complete reduction is done. With algorithms like
    reduction, be sure you are reducing the number of active warps per iteration,
    not just the number of active threads.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当某一阈值达到时，一种优化方法是将剩余的计算任务转发给CPU来完成。如果CPU本身处于空闲状态，而且剩余的数据传输量不大，这种策略可以比等待GPU完成整个归约操作获得显著的性能提升。在Fermi架构中，NVIDIA解决了这个问题，允许空闲的SM开始处理下一个排队的内核。然而，为了使SM空闲，必须保证所有线程块都已完成。某些非最优内核在归约的最后阶段仍可能有一个或多个活跃线程，这会将内核锁定在SM上，直到完整的归约操作完成。像归约这样的算法，要确保你在每次迭代中减少活跃warp的数量，而不仅仅是减少活跃线程的数量。
- en: Section summary
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小节总结
- en: • Understand the problem and define your speedup goal in the context of the
    programming time and skills available to you.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: • 理解问题，并在可用的编程时间和技能范围内定义你的加速目标。
- en: • Identify the parallelism in the problem and think about how to best to allocate
    this between the CPU and one or more GPUs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: • 识别问题中的并行性，并思考如何在CPU和一个或多个GPU之间最佳分配这些并行性。
- en: • Consider what is more important, a lower execution time or processing the
    data to a higher resolution.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: • 考虑哪个更重要，是降低执行时间还是将数据处理成更高的分辨率。
- en: • Understand the implication of any serial code sections and think about how
    these might best be handled.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: • 理解任何串行代码部分的含义，并思考如何最好地处理这些部分。
- en: • Profile your application to ensure your understanding reflects the actual
    reality. Repeat your earlier analysis if appropriate with your enhanced understanding.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: • 对应用程序进行性能分析，确保你的理解反映了实际情况。如果适当的话，根据你更深入的理解，重复之前的分析。
- en: 'Strategy 2: Memory Considerations'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 2：内存考虑
- en: Memory bandwidth
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存带宽
- en: Memory bandwidth and latency are key considerations in almost all applications,
    but especially so for GPU applications. Bandwidth refers to the amount of data
    that can be moved to or from a given destination. In the GPU case we’re concerned
    primarily about the global memory bandwidth. Latency refers to the time the operation
    takes to complete.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 内存带宽和延迟是几乎所有应用中的关键考虑因素，尤其是在GPU应用中尤为重要。带宽指的是可以传输到或从给定目标位置移动的数据量。在GPU的情况下，我们主要关注的是全局内存带宽。延迟指的是操作完成所需的时间。
- en: Memory latency is designed to be hidden on GPUs by running threads from other
    warps. When a warp accesses a memory location that is not available, the hardware
    issues a read or write request to the memory. This request will be automatically
    combined or coalesced with requests from other threads in the same warp, provided
    the threads access adjacent memory locations and the start of the memory area
    is suitably aligned.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上，内存延迟的设计是通过运行来自其他warp的线程来隐藏的。当一个warp访问一个不可用的内存位置时，硬件会发出读写请求到内存。如果这些线程访问的是相邻的内存位置，并且内存区域的起始位置得到了适当对齐，那么这个请求会自动与同一个warp中其他线程的请求合并或聚合。
- en: The size of memory transactions varies significantly between Fermi and the older
    versions. In compute 1.x devices (G80, GT200), the coalesced memory transaction
    size would start off at 128 bytes per memory access. This would then be reduced
    to 64 or 32 bytes if the total region being accessed by the coalesced threads
    was small enough and within the same 32-byte aligned block. This memory was not
    cached, so if threads did not access consecutive memory addresses, it led to a
    rapid drop off in memory bandwidth. Thus, if thread 0 reads addresses 0, 1, 2,
    3, 4, …, 31 and thread 1 reads addresses 32, 32, 34, …, 63, they will not be coalesced.
    In fact, the hardware will issue one read request of at least 32 bytes for each
    thread. The bytes not used will be fetched from memory and simply be discarded.
    Thus, without careful consideration of how memory is used, you can easily receive
    a tiny fraction of the actual bandwidth available on the device.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi 和旧版本之间的内存事务大小差异显著。在计算 1.x 设备（如 G80，GT200）中，合并的内存事务大小通常是每次内存访问 128 字节。如果合并线程访问的总区域足够小，并且位于相同的
    32 字节对齐块内，事务大小会减少到 64 或 32 字节。这些内存不会被缓存，因此，如果线程没有访问连续的内存地址，就会导致内存带宽的快速下降。因此，如果线程
    0 读取地址 0、1、2、3、4、……、31，线程 1 读取地址 32、32、34、……、63，它们将无法合并。事实上，硬件将为每个线程发起至少 32 字节的读取请求。未使用的字节将从内存中读取并简单地丢弃。因此，如果不仔细考虑内存的使用方式，你可能会得到设备上可用带宽的一小部分。
- en: The situation in Fermi and Kepler is much improved from this perspective. Fermi,
    unlike compute 1.x devices, fetches memory in transactions of either 32 or 128
    bytes. A 64-byte fetch is not supported. By default every memory transaction is
    a 128-byte cache line fetch. Thus, one crucial difference is that access by a
    stride other than one, but within 128 bytes, now results in cached access instead
    of another memory fetch. This makes the GPU model from Fermi onwards considerably
    easier to program than previous generations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，Fermi 和 Kepler 的情况有了很大改善。Fermi 不像计算 1.x 设备那样以 32 或 128 字节的事务方式提取内存，不支持
    64 字节的提取。默认情况下，每个内存事务都是 128 字节的缓存行提取。因此，一个关键的区别是，如果访问步长不为 1，但仍在 128 字节以内，那么现在会导致缓存访问，而不是再次发起内存提取。这使得从
    Fermi 开始的 GPU 模型比前几代更容易编程。
- en: One of the key areas to consider is in the number of memory transactions in
    flight. Each memory transaction feeds into a queue and is individually executed
    by the memory subsystem. There is a certain amount of overhead with this. It’s
    less expensive for a thread to issue a read of four floats or four integers in
    one pass than to issue four individual reads. In fact, if you look at some of
    the graphs NVIDIA has produced, you see that to get anywhere near the peak bandwidth
    on Fermi and Kepler you need to adopt one of two approaches. First, fully load
    the processor with warps and achieve near 100% occupancy. Second, use the 64-/128-bit
    reads via the `float2`/`int2` or `float4`/`int4` vector types and your occupancy
    can be much less but still allow near 100% of peak memory bandwidth. In effect,
    by using the vector types you are issuing a smaller number of larger transactions
    that the hardware can more efficiently process. You also introduce a certain amount
    of instruction-level parallelism through processing more than one element per
    thread.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的关键领域之一是内存事务的数量。每个内存事务会进入队列，并由内存子系统单独执行。这样会有一定的开销。对于线程来说，发起一次读取四个浮点数或四个整数比发起四个单独的读取操作要便宜得多。事实上，如果你查看
    NVIDIA 提供的一些图表，你会看到，要接近 Fermi 和 Kepler 上的峰值带宽，你需要采取两种方法中的一种。首先，充分加载处理器，使用多个 warp，并实现接近
    100% 的占用率。其次，使用 `float2`/`int2` 或 `float4`/`int4` 向量类型的 64 位/128 位读取，即使你的占用率较低，仍然可以实现接近
    100% 的峰值内存带宽。实际上，通过使用向量类型，你发起了更少的更大事务，这些事务硬件可以更高效地处理。你还通过每个线程处理多个元素引入了某种程度的指令级并行性。
- en: However, be aware that the vector types (`int2`, `int4`, etc.) introduce an
    implicit alignment of 8 and 16 bytes, respectively. The data must support this,
    so for example, you cannot cast a pointer to `int` from array element `int[5]`
    to `int2∗` and expect it to work correctly. In such cases you’re better off performing
    back-to-back 32-bit reads or adding some padding to the data structure to allow
    aligned access. As we saw when optimizing the sample sort example, a value of
    four elements per thread often provides the optimal balance between additional
    register usage, providing increased memory throughput and opportunity for the
    processor to exploit instruction-level parallelism.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要注意，向量类型（`int2`、`int4` 等）会隐式引入 8 字节和 16 字节的对齐要求。数据必须支持这一点，例如，你不能将 `int[5]`
    数组元素的指针强制转换为 `int2∗` 并期望它正确工作。在这种情况下，最好进行背靠背的 32 位读取，或向数据结构中添加填充，以允许对齐访问。正如我们在优化示例排序示例时看到的，四个元素每个线程通常提供了额外寄存器使用、增加内存吞吐量和让处理器能够利用指令级并行的最佳平衡。
- en: Source of limit
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制的来源
- en: Kernels are typically limited by two key factors, memory latency/bandwidth and
    instruction latency/bandwidth. Optimizing for one when the other is the key limiter
    will result in a lot of effort and very little return on that effort. Therefore,
    being able to understand which of these two key factors is limiting performance
    is critical to knowing where to direct your efforts.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 内核通常受限于两个关键因素：内存延迟/带宽和指令延迟/带宽。当优化其中一个，而另一个才是主要限制因素时，会导致大量的努力却收效甚微。因此，能够了解是哪一个关键因素限制了性能，对于确定努力方向至关重要。
- en: The simplest way in which you can see where the balance of the code lies is
    to simply comment out all the arithmetic instructions and replace them with a
    straight assignment to the result. Arithmetic instructions include any calculations,
    branches, loops, etc. If you have a one-to-one mapping of input values to calculated
    outputs, this is very simple and a one-to-one assignment works well. Where you
    have a reduction operation of one form or another, simply replace it with a sum
    operation. Be sure to include all the parameters read from memory into the final
    output or the compiler will remove the apparently redundant memory reads/writes.
    Retime the execution of the kernel and you will see the approximate percentage
    of time that was spent on the arithmetic or algorithmic part. If this percentage
    is very high, you are arithmetically bound. Conversely, if very little changed
    on the overall timing, you are memory bound.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过最简单的方法来查看代码的平衡所在，那就是将所有算术指令注释掉，并用一个直接的赋值操作替换它们，赋值给结果。算术指令包括任何计算、分支、循环等。如果输入值与计算输出之间有一一对应关系，这非常简单，且一一赋值效果良好。当你有某种形式的归约操作时，只需将其替换为求和操作。确保将所有从内存读取的参数包含在最终输出中，否则编译器会删除那些看似冗余的内存读写操作。重新计时内核执行的时间，你将看到大致的算术或算法部分所花费的时间百分比。如果这个百分比非常高，你就是算术限制。相反，如果整体时序变化很小，那你就是内存限制。
- en: With the arithmetic code still commented out, run the kernel using Parallel
    Nsight, using the Analysis function and the Profile setting. Examine the instruction
    statistics it produces ([Figure 9.7](#F0040)). If the bar graph contains a significant
    amount of blue, then the kernel memory pattern is displaying poor coalescing and
    the GPU has to serialize the instruction stream to support scattered memory reads
    or writes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在算术代码仍被注释掉的情况下，使用 Parallel Nsight 运行内核，使用分析功能和性能设置。检查它生成的指令统计数据（[图 9.7](#F0040)）。如果条形图中有大量的蓝色，那么内核的内存模式显示出较差的合并性，GPU必须序列化指令流，以支持散乱的内存读写。
- en: '![image](../images/F000090f09-07-9780124159334.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-07-9780124159334.jpg)'
- en: FIGURE 9.7 High instruction reissue rate.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 高指令重发率。
- en: If this is the case, is it possible to rearrange the memory pattern so the GPU
    can coalesce the memory access pattern by thread? Remember, to do this, thread
    0 has to access address 0, thread 1 address 1, thread 2 address 2, and so on.
    Ideally, your data pattern should generate a column-based access pattern by thread,
    not a row-based access. If you can’t easily rearrange the data pattern, can you
    rearrange the thread pattern such that you can use them to load the data into
    shared memory before accessing the data? If so, you don’t have to worry about
    coalescing the reads when accessing them from shared memory.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，是否有可能重新排列内存模式，以便GPU能够按线程合并内存访问模式？记住，要做到这一点，线程0必须访问地址0，线程1访问地址1，线程2访问地址2，依此类推。理想情况下，你的数据模式应该生成按列访问的模式，而不是按行访问。如果你无法轻松地重新排列数据模式，是否可以重新排列线程模式，以便在访问数据之前将其加载到共享内存中？如果可以，你就不必担心从共享内存中访问时合并读取。
- en: Is it possible to expand the number of elements of the output dataset that are
    processed by a single thread? This will often help both memory- and arithmetic-bound
    kernels. If you do this, do it without introducing a loop into the thread, but
    by duplicating the code. If the code is nontrivial, this can also be done as a
    device function or a macro. Be sure to hoist the read operations up to the start
    of the kernel, so that the read operations have finished fetching data before
    they are needed. This will increase register usage, so be sure to monitor the
    number of warps being scheduled to see it does not suddenly drop off.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有可能扩大由单个线程处理的输出数据集元素的数量？这通常有助于内存绑定和算术绑定的内核。如果你这样做，应该通过复制代码来完成，而不是在线程中引入循环。如果代码比较复杂，也可以将其作为设备函数或宏来处理。确保将读取操作提到内核的开始位置，这样在需要时读取操作已经完成数据获取。这将增加寄存器的使用，因此要确保监控调度的warp数量，确保它不会突然下降。
- en: With arithmetic-bound kernels, look at the source code and think about how this
    would be translated into assembly (PTX) code. Don’t be afraid to have a look at
    the actual PTX code being generated. Array indexes can often be replaced with
    pointer-based code, replacing slow multiplies with much faster additions. Divide
    or multiply instructions that use a power of 2 can be replaced with much faster
    right and left shift operations, respectively. Anything that is constant within
    a loop body, an invariant, should be moved outside the loop body. If the thread
    contains a loop, does unrolling the loop speed up things (it usually does)? What
    loop unrolling factor works best? We look at these optimization strategies in
    detail a little later in this chapter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算术绑定的内核，查看源代码并思考它如何被转换为汇编（PTX）代码。不要害怕查看实际生成的PTX代码。数组索引通常可以用基于指针的代码替换，将较慢的乘法替换为更快的加法。使用2的幂次方的除法或乘法指令可以分别替换为更快的右移和左移操作。任何在循环体内保持不变的常量（不变量），都应该移到循环体外。如果线程中包含循环，展开循环会加速吗（通常是的）？哪种循环展开因子效果最好？我们稍后将在本章中详细讨论这些优化策略。
- en: Are you using single- or double-precision floats in reality, and what did you
    want to use? Look out for floating-point constants without an `F` postfix, which
    the compiler will treat as double precision. Do you really need 32 bits of precision
    in all of the calculations? Try the `-use_fast_math` compiler switch and see if
    the results are still accurate enough for your needs. This switch enables 24-bit
    floating-point arithmetic, which can be significantly quicker than the standard
    IEEE 32-bit floating-point math logic.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你在实际中使用的是单精度还是双精度浮点数，你想使用的是哪种？注意那些没有`F`后缀的浮点常量，编译器会将其视为双精度。你是否真的需要在所有计算中使用32位精度？尝试使用`-use_fast_math`编译选项，看看结果是否仍然足够准确以满足你的需求。此选项启用了24位浮点运算，比标准的IEEE
    32位浮点数学逻辑快得多。
- en: Finally, are you testing speed with the “release” version of the code? As we
    saw in some of the examples earlier, this alone can increase performance by 15%
    or more.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你是否在“发布”版本的代码中进行速度测试？正如我们之前一些例子所看到的，仅此一项就可以提高15%或更多的性能。
- en: Memory organization
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存组织
- en: Getting the memory pattern correct for a GPU is often the key consideration
    in many applications. CPU programs typically arrange the data in rows within memory.
    While Fermi and Kepler will tolerate noncoalesced reads and writes, as we mentioned
    earlier, compute 1.x devices will not. You have to try and arrange the memory
    pattern such that access to it by consecutive threads will be in columns. This
    is true of both global memory and shared memory. This means for a given warp (32
    threads) thread 0 should access address offset 0, thread 1 address offset 1, thread
    2 address offset 2, etc. Think about the fetch to global memory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 GPU 的内存模式通常是许多应用中的关键考虑因素。CPU 程序通常将数据按行排列在内存中。虽然 Fermi 和 Kepler 会容忍非合并读取和写入，但正如我们之前提到的，计算
    1.x 设备则不会。你必须尝试安排内存模式，使得连续线程对它的访问将是按列的。这对于全局内存和共享内存都适用。这意味着，对于一个给定的 warp（32 个线程），线程
    0 应该访问地址偏移 0，线程 1 访问地址偏移 1，线程 2 访问地址偏移 2，依此类推。想想访问全局内存的提取。
- en: However, assuming you have an aligned access, 128 bytes of data will come in
    from global memory at a time. With a single float or integer per thread, all 32
    threads in the warp will be given exactly one element of data each.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设你有对齐访问，那么每次会从全局内存中获取 128 字节的数据。每个线程有一个浮点数或整数时，warp 中的 32 个线程将每个获取一个数据元素。
- en: Note the `cudaMalloc` function will allocate memory in 128-byte aligned blocks,
    so for the most part alignment is not an issue. However, if using a structure
    that would straddle such a boundary, then there are two approaches. First, you
    can either add padding bytes/words explicitly to the structure. Alternatively,
    you can use the `cudaMallocPitch` function we covered in [Chapter 6](CHP006.html).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`cudaMalloc`函数会在 128 字节对齐的块中分配内存，因此大多数情况下对齐不是问题。然而，如果使用一个会跨越这样的边界的结构，那么有两种方法可以解决。首先，你可以显式地向结构中添加填充字节/字。或者，你可以使用我们在[第六章](CHP006.html)中介绍的`cudaMallocPitch`函数。
- en: 'Notice that alignment is a key criteria as to whether one or two memory transactions,
    or cache lines, need to be fetched. Suppose thread 0 accesses address offset 2
    instead of 0\. Perhaps you’re accessing some data structure that has a header
    at the start, such as:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对齐是决定是否需要获取一个或两个内存事务或缓存行的关键标准。假设线程 0 访问的是地址偏移 2 而不是 0\. 也许你正在访问某个数据结构，该结构的开头有一个头部，例如：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If the kernel processes `msg_data`, then threads 30 and 31 of the warp cannot
    be served by the single memory fetch. In fact, they generate an additional 128-byte
    memory transaction as shown in [Figure 9.8](#F0045). Any subsequent warps suffer
    from the same issue. You are halving your memory bandwidth, just by having a 2-byte
    header at the start of the data structure.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内核处理`msg_data`，那么 warp 的线程 30 和 31 无法通过单一的内存获取来服务。实际上，它们会生成一个额外的 128 字节内存事务，如[图
    9.8](#F0045)所示。任何后续的 warp 都会遇到相同的问题。仅仅在数据结构的开头有一个 2 字节的头部，你的内存带宽就被减半了。
- en: '![image](../images/F000090f09-08-9780124159334.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-08-9780124159334.jpg)'
- en: FIGURE 9.8 Cache line/memory transaction usage within structures.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 结构内的缓存行/内存事务使用情况。
- en: You’ll see this most acutely on compute 1.x devices where the additional fetch
    generated for threads 30/31 isn’t even used to prefill the cache, but just discarded.
    Loading the header into a separate chunk of memory somewhere else allows for aligned
    access to the data block. If you are unable to do this, then manually insert padding
    bytes into the structure definition to ensure that `msg_data` is aligned to a
    128-byte boundary. Note that simply reordering the structure elements to move
    ‘header’ after `msg_data` will also work, providing the structure is not subsequently
    used to create an array of structures. All of a sudden your threads match the
    memory organization and your memory throughput when working with the `msg_data`
    part of the structure will double.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在计算 1.x 设备上最为明显地看到这一点，其中为线程 30/31 生成的额外获取甚至没有用于预填充缓存，而是直接被丢弃。将头部加载到内存中的其他位置的单独块中，可以实现对数据块的对齐访问。如果你无法做到这一点，那么可以手动在结构定义中插入填充字节，以确保`msg_data`对齐到
    128 字节边界。请注意，简单地重新排序结构元素，将‘header’放在`msg_data`之后也可以解决问题，前提是该结构之后不会被用来创建结构数组。突然之间，你的线程与内存组织对齐，并且在处理结构中的`msg_data`部分时，内存吞吐量将翻倍。
- en: Consider also the case where prefix sum is used. Prefix sum allows for multiple
    independent processes or threads to read or write to independent areas of memory
    without interfering with one another. Multiple reads from the same address are
    actually hugely beneficial, in that the GPU will simply forward the value to whatever
    additional threads within the warp need it without additional memory fetches.
    Multiple writes are of course an issue, in that they need to be sequenced.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑使用前缀和的情况。前缀和允许多个独立的进程或线程读取或写入独立的内存区域，而互不干扰。多次从同一地址读取实际上是非常有益的，因为GPU会将值直接转发给warp内的其他需要它的线程，而无需额外的内存获取。多次写入当然是一个问题，因为它们需要排队。
- en: If we assume integers or floats for now, the size of each entry in the data
    array is 4 bytes. If the distribution of the prefix array is exactly equal then
    we don’t need prefix arrays to access the data anyway, as you could simply use
    a fixed offset per thread. Therefore, if you’re using a prefix sum to calculate
    an offset into the dataset, it’s highly likely there are a variable number of
    elements per bin. If you know the upper bounds of the number of elements per bin
    and you have a sufficient memory available, then just pad each bin to the alignment
    boundary. Use an additional array that holds the number of elements in the bin
    or calculate this value from the prefix sum index. In this way we can achieve
    aligned access to memory at the expense of unused cells at the end of most bins.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在假设使用整数或浮点数，那么数据数组中每个条目的大小为4字节。如果前缀数组的分布完全相等，那么无论如何我们不需要前缀数组来访问数据，因为你可以简单地为每个线程使用固定偏移量。因此，如果你使用前缀和来计算数据集的偏移量，那么每个桶内的元素数量很可能是可变的。如果你知道每个桶内元素数量的上限，并且有足够的内存可用，那么只需将每个桶填充到对齐边界。使用一个额外的数组来保存桶内的元素数量，或者通过前缀和索引计算这个值。通过这种方式，我们可以通过牺牲大多数桶末尾的未使用单元来实现对内存的对齐访问。
- en: One very simple solution to the alignment problem is to use a padding value
    that has no effect on the calculated result. For example, if you’re performing
    a sum over the values in each bin, padding with zero will mean no change to the
    end result, but will give a uniform memory pattern and execution path for all
    elements in the warp. For a `min` operation, you can use a padding value of 0xFFFFFFFF,
    and conversely 0 for a `max` operation. It is usually not hard to come up with
    a padding value that can be processed, yet contributes nothing to the result.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐问题的一个非常简单的解决方案是使用一个不会影响计算结果的填充值。例如，如果你在每个桶内执行求和操作，使用零填充意味着对最终结果没有改变，但会为warp中的所有元素提供一致的内存模式和执行路径。对于`min`操作，你可以使用填充值0xFFFFFFFF，反之对于`max`操作使用0。通常不难想到一个可以处理的填充值，但它对结果没有任何贡献。
- en: Once you move to fixed-sized bins, it’s also relatively simple to ensure the
    dataset is generated and accessed in columns, rather than rows. It’s often desirable
    to use shared memory as a staging buffer because of the lack of coalescing requirements.
    This can then be used to allow coalesced reads/writes to global memory.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你转向固定大小的桶，确保数据集是按列而非按行生成和访问也变得相对简单。由于没有合并要求，通常希望使用共享内存作为暂存缓冲区。然后可以用它来允许合并的全局内存读写操作。
- en: Memory accesses to computation ratio
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存访问与计算比率
- en: One question that you should often ask is what is the ratio of memory operations
    to arithmetic operations? You ideally want a ratio of at least 10:1\. That is,
    for every memory fetch the kernel makes from global memory it does 10 or more
    other instructions. These can be array index calculations, loop calculations,
    branches, or conditional evaluations. Every instruction should contribute to useful
    output. Loops, in particular, especially when not unrolled, often simply contribute
    toward instruction overhead and not to any useful work.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该经常问的一个问题是，内存操作与算术操作的比率是多少？理想情况下，你希望比率至少为10:1。也就是说，每次内核从全局内存获取内存时，它会执行10条或更多的其他指令。这些指令可以是数组索引计算、循环计算、分支或条件评估。每条指令都应该对有用的输出做出贡献。特别是循环，尤其是在没有展开的情况下，通常只是贡献到指令开销，而不是任何有用的工作。
- en: If we look inside an SM, architecturally, we see that warps are dispatched to
    sets of CUDA cores based on even and odd instruction dispatchers. Compute 1.x
    devices have a single warp dispatcher and compute 2.x devices have two. In the
    GF100/GF110 chipset (Fermi GTX480/GTX580) there are 32 CUDA cores and four SFUs
    (special-function units) per SM ([Figure 9.9](#F0050)). In the GF104/GF114-based
    devices (GTX460/GTX560) there are 48 CUDA cores and eight SFUs per SM ([Figure
    9.10](#F0055)). Each SM for both compute 2.0 and compute 2.1 devices has a single
    set of 16 LSUs (load store units) that are used to load values to and from memory
    (global, constant, shared, local, and cache).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看一个 SM 内部的结构，从架构上看，warp 是根据奇偶指令调度器分派到 CUDA 核心的。计算 1.x 设备有一个 warp 调度器，而计算
    2.x 设备有两个。在 GF100/GF110 芯片组（Fermi GTX480/GTX580）中，每个 SM 有 32 个 CUDA 核心和 4 个 SFU（特殊功能单元）([图
    9.9](#F0050))。在 GF104/GF114 基础的设备（GTX460/GTX560）中，每个 SM 有 48 个 CUDA 核心和 8 个 SFU
    ([图 9.10](#F0055))。对于计算 2.0 和计算 2.1 设备，每个 SM 都有一组 16 个 LSU，用于从内存（全局、常量、共享、局部和缓存）加载值。
- en: '![image](../images/F000090f09-09-9780124159334.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-09-9780124159334.jpg)'
- en: FIGURE 9.9 Dispatching of CUDA warps (GF100/GF110, compute 2.0).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 CUDA warp 的调度（GF100/GF110，计算 2.0）。
- en: '![image](../images/F000090f09-10-9780124159334.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-10-9780124159334.jpg)'
- en: FIGURE 9.10 Dispatching of CUDA warps (GF104/GF114).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 CUDA warp 的调度（GF104/GF114）。
- en: Thus, in a single cycle, the warp dispatchers issue (or dispatch) a total of
    two (compute 2.0) or four (compute 2.1) instructions, one set from each dispatcher.
    As these come from different warps, the instructions are entirely independent
    of one another. These are then pushed into the pipeline of the execution units
    (CUDA cores, SFUs, and LSUs).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在单个周期中，warp 调度器会发出（或调度）总共两条（计算 2.0）或四条（计算 2.1）指令，来自每个调度器的一组指令。由于这些指令来自不同的
    warps，它们完全独立。然后，这些指令会被推送到执行单元的流水线（CUDA 核心、SFU 和 LSU）。
- en: There are a few implications to this design. First, the absolute minimum number
    of warps that must be present is two for the GF100 series (compute 2.0) hardware
    and four for the GF104 series (compute 2.1) hardware. This in turn implies an
    absolute minimum of 64 or 128 threads per SM, respectively. Having less than this
    means that one or more of the instruction dispatch units will remain idle, effectively
    halving (GF100) the instruction dispatch speed. Using a number of threads other
    than a multiple of 32 will mean some elements of the CUDA cores will idle, again
    undesirable.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计有几个含义。首先，必须存在的 warps 最少数量是 GF100 系列（计算 2.0）硬件的两个，GF104 系列（计算 2.1）硬件的四个。由此推导出，分别至少需要每个
    SM 64 或 128 个线程。如果线程数少于这个数量，意味着一个或多个指令调度单元将保持空闲，实际上会使指令调度速度减半（GF100）。使用不是 32 的倍数的线程数会导致某些
    CUDA 核心元素空闲，这同样是不希望发生的。
- en: Having this minimum number of resident warps provides absolutely no hiding of
    latency, either memory or instruction, based on the ability to switch to another
    warp. A stall in the instruction stream will actually stall the CUDA cores, which
    is highly undesirable. In practice, multiple blocks are allocated to an SM to
    try to ensure this problem never occurs and, more importantly, a variable mix
    of instructions is generated.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个最小数量的驻留 warp 完全无法隐藏延迟，无论是内存延迟还是指令延迟，因为可以切换到另一个 warp。在指令流中出现停顿时，实际上会使 CUDA
    核心停顿，这非常不希望发生。实际上，会将多个块分配给一个 SM，试图确保这个问题永远不会发生，更重要的是，能够生成一个指令的多样化组合。
- en: The second implication is the shared resources limit the ability to continuously
    perform the same operation. Both the CUDA cores and the LSUs are pipelined, but
    are only 16 units wide. Thus, to dispatch an entire warp to either unit takes
    two cycles. On compute 2.0 hardware, only one instruction per dispatcher can be
    dispatched. Thus, to push an operation into the LSUs, one slot in the pipeline
    of one of the CUDA cores must be left empty. There are four possible receivers
    for the dispatch (CUDA, CUDA, SFUs and LSUs), yet only two suppliers per cycle.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个含义是共享资源限制了持续执行相同操作的能力。CUDA 核心和 LSU（负载存储单元）都采用流水线，但每个单元的宽度仅为 16 个。因此，要将整个
    warp 调度到任一单元需要两个周期。在计算 2.0 硬件中，每个调度器每次只能调度一条指令。因此，为了将操作推入 LSU，必须留出一个 CUDA 核心流水线的空槽。调度有四个可能的接收者（CUDA，CUDA，SFU
    和 LSU），但每个周期只有两个供应者。
- en: The situation is drastically improved in compute 2.1 hardware, in that the two
    dispatchers dispatch two instructions each, for a total of four per clock. With
    three sets of CUDA cores it would be possible to supply three arithmetic instructions
    plus a load/save instruction without creating holes in the pipeline.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 2.1 硬件中，情况有了极大改善，因为两个调度器每个调度两个指令，总共每个时钟周期可以调度四条指令。拥有三组 CUDA 核心的情况下，可以提供三条算术指令以及一条加载/保存指令，而不会在流水线中造成空洞。
- en: However, if all warps want to issue an instruction to the same execution unit,
    for example the LSU or SFU, there is a problem. Only a single warp can use the
    LSU per two clock cycles. As the SFU has just eight units, four on compute 2.0
    hardware, a warp can take up to eight cycles to be fully consumed by the SFUs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果所有的 warps 都想向同一个执行单元发出指令，例如 LSU 或 SFU，就会出现问题。每两个时钟周期只能有一个 warp 使用 LSU。由于
    SFU 只有八个单元，其中四个是在计算 2.0 硬件上，因此一个 warp 可能需要最多八个时钟周期才能被 SFU 完全消耗。
- en: Thus, the bandwidth available to and from the LSUs on a compute 2.1 device is
    50% less than a compute 2.0 device with the same number of CUDA cores. Consequently,
    the LSUs or SFUs can become a bottleneck. There need to be other instructions
    in the stream such that the CUDA cores can do some useful work while the memory
    and transcendental instructions progress through the LSU or SFU pipeline.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算 2.1 设备上 LSU 的带宽比相同数量 CUDA 核心的计算 2.0 设备少 50%。因此，LSU 或 SFU 可能成为瓶颈。需要在流中有其他指令，以便在内存和超越指令通过
    LSU 或 SFU 流水线时，CUDA 核心能够进行一些有用的工作。
- en: The Kepler GK104 device (GTX680/Tesla K10) further extends the GF104/114 (GTX460/560)
    design by extending the number of CUDA cores from 48 to 96, and then putting two
    of these within an SM. Thus there are four warp schedulers, eight dispatch units,
    two LSUs and two SFUs per SM.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Kepler GK104 设备（GTX680/Tesla K10）进一步扩展了 GF104/114（GTX460/560）的设计，通过将 CUDA 核心的数量从
    48 扩展到 96，并将其中两个核心放入一个 SM 中。因此，每个 SM 有四个 warp 调度器，八个调度单元，两个 LSU 和两个 SFU。
- en: 'Let’s expand a little on the example we looked at earlier. Consider the case
    of a typical kernel. At the start of the kernel, all threads in all warps fetch
    a 32-bit value from memory. The addresses are such that they can be coalesced.
    For example:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微展开一下之前看到的示例。考虑一个典型的内核的情况。在内核开始时，所有 warps 中的所有线程从内存中获取一个 32 位的值。地址安排得如此合理，以至于它们可以被合并。例如：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This would break down into a multiply and add (MADD) integer instruction, to
    calculate the value to put into the register for the variable `tid`. Variables
    `data`, `b`, and `c` are arrays somewhere in global memory. The variables `data`,
    `a`, and `b` are indexed by `tid` so the address to write to needs to be calculated
    by multiplying `tid` by the size of the elements making up the array. Let’s assume
    they all are integer arrays, so the size is 4 bytes per entry.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将分解为一个乘加（MADD）整数指令，用于计算存入寄存器中的 `tid` 变量的值。`data`、`b` 和 `c` 变量是全局内存中的数组。`data`、`a`
    和 `b` 变量通过 `tid` 索引，因此需要通过将 `tid` 乘以数组元素大小来计算写入地址。假设它们都是整数数组，因此每个条目的大小是 4 字节。
- en: We very quickly hit the first dependency in the calculation of `tid` ([Figure
    9.11](#F0060)). The warp dispatches the multiply of `blockIdx.x` and `blockDim.x`
    to the integer MADD units in the CUDA cores. Until the multiply and add instruction
    to calculate `tid` has completed we can continue no further, so the warp is marked
    as blocked and suspended.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在计算 `tid` 时很快遇到了第一个依赖（见[图 9.11](#F0060)）。warp 将 `blockIdx.x` 和 `blockDim.x`
    的乘法操作分派到 CUDA 核心的整数 MADD 单元中。在计算 `tid` 的乘加指令完成之前，我们无法继续，因此该 warp 被标记为阻塞并挂起。
- en: '![image](../images/F000090f09-11-9780124159334.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-11-9780124159334.jpg)'
- en: FIGURE 9.11 Data flow dependency.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 数据流依赖关系。
- en: At this point, the next warp is selected, which does the same operation and
    is again suspended at the calculation of `tid`. After all warps have progressed
    to this point, enough clocks have passed such that the value of `tid` in warp
    0 is now known and can be fed into the multiply for the destination address calculations.
    Thus, three additional MADD instructions are dispatched to the CUDA cores, to
    calculate the address offsets. The next instruction would be a couple of loads,
    but for this we need the address of `a` and `b` from the multiply instructions.
    At this point we again suspend the warp and the other warps execute.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，选择下一个warp，该warp执行相同的操作，并在计算`tid`时再次暂停。当所有warp都进展到这一点时，足够的时钟周期已经过去，warp 0中`tid`的值现在已知，并可以用于乘法运算中的目标地址计算。因此，三个额外的MADD指令被调度到CUDA核心，用于计算地址偏移量。接下来的指令将是几次加载，但为了实现这一点，我们需要从乘法指令中获取`a`和`b`的地址。在这一点，我们再次暂停该warp，其他warps继续执行。
- en: Once the address calculation of `a` is available, the load instruction can be
    dispatched. It’s likely, due to the address calculation of `b` being issued back
    to back with that of `a`, that the address calculation of `b` will be retired
    by the time the load for `a` has been dispatched. Thus, we immediately issue the
    load for the ‘`b`’. The next instruction in the stream would be a multiply of
    ‘`a`’ and ‘`b`’, neither of which will be available for some time yet as they
    have to be fetched from main memory to the SM. Thus, the warp is suspended and
    the subsequent warps execute to the same point.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`a`的地址计算可用，加载指令可以被调度。由于`b`的地址计算与`a`的计算是连续发布的，因此在`a`的加载指令被调度时，`b`的地址计算很可能已经被执行完成。因此，我们立即发布对`b`的加载指令。数据流中的下一条指令将是`a`和`b`的乘法，但它们都还需要一些时间才能到达，因为它们需要从主内存加载到SM中。因此，该warp被暂停，随后其他warps执行到相同的位置。
- en: As memory fetches take a long time, all warps dispatch the necessary load instructions
    to the LSU and are suspended. If there is no other work to do from other blocks,
    the SM will idle pending the memory transactions completing.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存获取需要较长时间，所有warps将必要的加载指令调度到LSU，并暂停。如果没有来自其他block的工作，SM将在等待内存事务完成时处于空闲状态。
- en: Sometime later `a` finally arrives from the memory subsystem as a coalesced
    read of 128 bytes, a single cache line, or a memory transaction. The 16 LSUs distributes
    64 of the 128 bytes to the registers used by the first half-warp of warp 0\. In
    the next cycle, the 16 LSUs distribute the remaining 64 bytes to the register
    used by the other half-warp. However, warp 0 still can not progress as it has
    only one of the two operands it needs for the multiply. It thus does not execute
    and the subsequent bytes arriving from the coalesced read of `a` for the other
    warps are distributed to the relevant registers for those warps.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，`a`终于从内存子系统到达，作为一次128字节的合并读取，一个缓存行，或者一个内存事务。16个LSU将128字节中的64字节分发到warp
    0的第一个半warp所使用的寄存器中。在下一个周期，16个LSU将剩余的64字节分发到另一个半warp所使用的寄存器中。然而，warp 0仍然无法推进，因为它只得到了进行乘法运算所需的两个操作数中的一个。因此，它不会执行，随后从`a`的合并读取中到达的字节会分发到其他warps相关的寄存器中。
- en: By the time all of the data from the coalesced read for `a` has been distributed
    to the registers of all the other warps, the data for `b` will likely have arrived
    in the L1 cache. Again, the 16 LSUs distribute the first 64 bytes to the registers
    of the first half-warp of warp 0\. In the subsequent cycle they distribute the
    second 64 bytes to the second half-warp.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有从`a`的合并读取中获得的数据分发到所有其他warps的寄存器时，`b`的数据可能已经到达L1缓存。同样，16个LSU将前64字节分发到warp
    0第一个半warp的寄存器中。在随后的周期，它们将第二64字节分发到第二个半warp。
- en: At the start of this second cycle, the first half-warp is able to progress the
    multiply instruction for `a[tid] ∗ b[tid]`. In the third cycle the LSUs start
    providing data to the first half-warp of warp 0\. Meanwhile, the second half-warp
    of warp 0 starts the execution of the multiply. As the next instruction in warp
    0 would be a store and is dependent on the multiply, warp 0 is suspended.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个周期的开始，第一个半warp能够推进`a[tid] ∗ b[tid]`的乘法指令。在第三个周期，LSUs开始为warp 0的第一个半warp提供数据。同时，warp
    0的第二个半warp开始执行乘法操作。由于warp 0的下一个指令是一个存储指令，并且依赖于乘法运算，因此warp 0被暂停。
- en: Providing there are on the order of 18–22 warps resident, by the time the last
    warp has dispatched the final multiply, the multiply will have completed for warp
    0\. It can then dispatch the store instructions to the 16 LSUs and complete its
    execution. The other warps then do exactly the same and the kernel is complete.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设大约有18到22个warp驻留，当最后一个warp调度完最后的乘法操作时，warp 0的乘法操作已经完成。它接着可以将存储指令调度到16个LSU并完成执行。其他的warp则做完全相同的事情，内核执行完毕。
- en: Now consider the case of (see [Figure 9.12](#F0065)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下情况（见[图 9.12](#F0065)）。
- en: '![image](../images/F000090f09-12-9780124159334.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-12-9780124159334.jpg)'
- en: FIGURE 9.12 Dual data flow dependency.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 双数据流依赖。
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By halving the number of blocks, we can process two elements per thread. Notice
    this introduces an independent execution stream into each thread of the warp.
    Thus, the arithmetic operations start to overlap with the load operations.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将块数减半，我们可以使每个线程处理两个元素。注意，这会在每个warp的线程中引入一个独立的执行流。因此，算术操作开始与加载操作重叠。
- en: 'However, as the example C code is written, this will not help. This is because
    the code contains dependencies that are not immediately obvious. The write operation
    to the first element of `data` could affect the value in either the `a` or the
    `b` array. That is, the address space of `data` may overlap with `a` or `b`. Where
    you have a write in the data flow to global memory, you need to lift out the reads
    to the start of the kernel. Use the following code instead:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如示例C代码所写的那样，这并不会起作用。因为代码包含了一些不立即显现的依赖关系。对`data`的第一个元素的写操作可能会影响`a`或`b`数组中的值。也就是说，`data`的地址空间可能与`a`或`b`重叠。当在数据流中有对全局内存的写操作时，你需要将读取操作提前到内核的开始处。请改用以下代码：
- en: '[PRE18]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: or
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE19]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`data[tid] = a_vect ∗ b_vect;`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`data[tid] = a_vect * b_vect;`'
- en: We have two choices, a scalar approach or a vector approach. The GPU supports
    only vector loads and saves, not vector operations, in hardware. Thus, the multiplication
    is actually done as an overloaded operator in C++ and simply multiplies the two
    integers independently of one another. However, the vector loads and saves two
    64-bit loads and a single 64-bit save, respectively, instead of the four separate
    32-bit loads and a single 32-bit save with the nonvector version. Thus, 40% of
    the memory transactions are eliminated. The memory bandwidth usage is the same,
    but less memory transactions mean less memory latency, and therefore any stall
    time waiting for memory is reduced.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种选择，一种是标量方法，一种是向量方法。GPU硬件只支持向量加载和保存，而不支持向量运算。因此，乘法实际上是作为C++中的重载运算符进行的，它仅仅独立地将两个整数相乘。然而，向量加载和保存分别是两个64位加载和一个64位保存，而非向量版本是四个32位加载和一个32位保存。因此，40%的内存事务被消除了。内存带宽的使用保持不变，但更少的内存事务意味着更少的内存延迟，因此等待内存的停顿时间得以减少。
- en: To use the vector types, simply declare all arrays as type `int2`, which is
    an in-built vector type of two integers. Supported types are `int2`, `int3`, `int4`,
    `float2`, `float3`, and `float4`. You can of course create your own types, such
    as `uchar4`, and define your own operators. Each vector type is actually just
    an aligned structure with *N* named member elements of the base type.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用向量类型，只需将所有数组声明为`int2`类型，这是一个内置的包含两个整数的向量类型。支持的类型有`int2`、`int3`、`int4`、`float2`、`float3`和`float4`。当然，你也可以创建自己的类型，如`uchar4`，并定义你自己的运算符。每种向量类型实际上只是一个对齐的结构，具有*N*个命名的基本类型成员元素。
- en: Thus, I hope you can actually see that a balance is therefore required between
    the different types of instructions. This becomes somewhat more critical with
    the compute 2.1 devices (GF104 series) where there are three sets of CUDA cores
    sharing the same resources within the SM. The change in compute 2.0 to compute
    2.1 devices added significantly more arithmetic capacity within the SM without
    providing additional data transport capacity. The compute 2.0 devices have up
    to 512 CUDA cores on a bus of up to 384 bits wide, giving a ratio of 1:3 of cores
    to memory bandwidth. The compute 2.1 devices have up to 384 CUDA cores on a bus
    of up to 256 bits, giving a ratio of 1:5 cores to memory bandwidth. Thus, compute
    2.0 devices are more suited to applications that are memory bound, whereas compute
    2.1 devices are more suited to applications that are compute bound.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我希望你能意识到，在不同类型的指令之间需要保持平衡。这在计算 2.1 设备（GF104 系列）中变得尤为关键，其中有三组 CUDA 核心共享 SM
    内相同的资源。从计算 2.0 到计算 2.1 设备的变化，在 SM 内显著增加了算术能力，但并未提供额外的数据传输能力。计算 2.0 设备在总线宽度最高为
    384 位的情况下，最多有 512 个 CUDA 核心，核心与内存带宽的比率为 1:3。计算 2.1 设备在总线宽度最高为 256 位的情况下，最多有 384
    个 CUDA 核心，核心与内存带宽的比率为 1:5。因此，计算 2.0 设备更适合内存受限的应用，而计算 2.1 设备更适合计算受限的应用。
- en: In practice, this is balanced in the compute 2.0 devices by having up to 33%
    more CUDA cores. The compute 2.1 devices, however, typically also run at somewhat
    higher clock rates, both in terms of the internal clock speed and also the external
    memory bus speed. This helps significantly in rebalancing the smaller memory bus
    width but is generally not sufficient to allow compute 2.1 devices to outperform
    their 2.0 counterparts.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，计算 2.0 设备通过拥有多达 33% 的更多 CUDA 核心来平衡这一点。然而，计算 2.1 设备通常也以稍高的时钟频率运行，无论是内部时钟速度，还是外部内存总线速度。这有助于在一定程度上重新平衡较小的内存总线宽度，但通常不足以让计算
    2.1 设备超越它们的 2.0 对应设备。
- en: What is important to realize, especially with compute 2.1 devices, is that there
    needs to be sufficient arithmetic density to the instruction stream to make good
    use of the CUDA cores present on the SMs. A kernel that simply does loads or stores
    and little else will not achieve anything like the peak performance available
    from these devices. Expand such kernels to also include independent instruction
    flow via processing two, four, or eight elements per thread. Use vector operations
    where possible.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于计算 2.1 设备，需要认识到的是，指令流中必须有足够的算术密度，才能充分利用 SM 上的 CUDA 核心。一个仅仅进行加载或存储的内核，如果做得不多，将无法达到这些设备所能提供的峰值性能。扩展这样的内核，使其通过每个线程处理两个、四个或八个元素来包含独立的指令流。尽可能使用向量操作。
- en: Loop and kernel fusion
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环与内核融合
- en: Another area where we can significantly save on memory bandwidth is a technique
    based on loop fusion we looked at in the last section. Loop fusion is where two
    apparently independent loops run over an intersecting range. For example, loop
    1 runs from 0 to 100 and loop 2 from 0 to 200\. The code for loop 2 can be fused
    with the code for loop 1, for at least the first 100 iterations. This increases
    the level of instruction-level parallelism, but also decreases the overall number
    of iterations by a third.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显著节省内存带宽的另一个技术是基于上节中提到的循环融合。循环融合是指两个看似独立的循环在交叉的范围内运行。例如，循环 1 从 0 运行到 100，循环
    2 从 0 运行到 200\. 循环 2 的代码可以与循环 1 的代码融合，至少在前 100 次迭代中。这增加了指令级并行性，但也减少了总的迭代次数，约为三分之一。
- en: Kernel fusion is a variation on loop fusion. If you have a number of kernels
    that are run in sequence, one after the other, are there elements of these kernels
    that can be fused? Be careful doing this with kernels you did not write or do
    not fully understand. Invoking two kernels in series generates an implicit synchronization
    between them. This may have been intended by design and, as it’s implicit, probably
    only the original designer is aware of it.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合是循环融合的一种变体。如果你有一系列顺序执行的内核，这些内核中是否有可以融合的部分？在处理你没有编写或完全理解的内核时要小心。连续调用两个内核会在它们之间产生隐式同步。这可能是设计时就有意为之的，并且由于是隐式的，可能只有原设计者知道这一点。
- en: In developing kernels it’s quite common to break down the operation into a number
    of phases or passes. For example, in the first pass you might calculate the results
    over the whole dataset. On the second pass you may filter data for certain criteria
    and perform some further processing on certain points. If the second pass can
    be localized to a block, the first and second pass can usually be combined into
    a single kernel. This eliminates the write to main memory of the first kernel
    and the subsequent read of the second, as well as the overhead of invoking an
    additional kernel. If the first kernel is able to write the results to shared
    memory, and you only need those results for the second pass, you eliminate the
    read/write to global memory entirely. Reduction operations often fall into this
    category and can benefit significantly from such an optimization, as the output
    of the second phase is usually many times smaller than the first phase, so it
    saves considerably on memory bandwidth.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发内核时，通常会将操作分解成多个阶段或多个通道。例如，在第一次通道中，你可能会计算整个数据集的结果。 在第二次通道中，你可能会对数据进行某些条件筛选，并对某些数据点进行进一步处理。
    如果第二次通道可以局部化到一个块中，第一次和第二次通道通常可以合并为一个内核。 这样可以消除第一次内核写入主内存和第二次内核读取的操作，以及调用额外内核的开销。
    如果第一次内核能够将结果写入共享内存，并且你只在第二次通道中需要这些结果，那么你就完全消除了对全局内存的读/写操作。 减少操作通常属于这一类，并且能够从这样的优化中受益，因为第二阶段的输出通常比第一阶段小得多，从而显著节省了内存带宽。
- en: Part of the reason why kernel fusion works so well is because of the data reuse
    it allows. Fetching data from global memory is slow, on the order of 400–600 clock
    cycles. Think of memory access like reading something from disk. If you’ve ever
    done any disk I/O, you’ll know that reading a file by fetching one character at
    time is very slow and using `fread` to read large blocks is far more efficient
    than repeatedly calling read character functions like `fgetch`. Having read the
    data in, you keep it in memory. Apply the same approach to accessing global memory.
    Fetch data in chunks of up to 16 bytes per thread (`float4`, `int4`), not in single
    bytes or words. Once you have each thread successfully processing a single element,
    switch to `int2` or `float2` and process two. Moving to four may or may not help,
    but moving from one to two often does. Once you have the data, store it in shared
    memory, or keep it in the register set and reuse it as much as possible.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合效果如此显著的部分原因在于它所允许的数据重用。 从全局内存中提取数据很慢，大约需要400-600个时钟周期。 可以将内存访问想象成从磁盘读取数据。
    如果你做过磁盘I/O操作，你就会知道，一次读取一个字符的速度非常慢，而使用`fread`一次读取大块数据比反复调用读取单个字符的函数（如`fgetch`）要高效得多。
    读取数据后，你将其保存在内存中。 将这种方法应用到全局内存的访问上。 每个线程一次提取最多16字节的数据（`float4`、`int4`），而不是一次提取单个字节或字。
    一旦每个线程成功处理了一个元素，切换到`int2`或`float2`，同时处理两个元素。 转向四个元素可能没有太大帮助，但从一个到两个的切换通常会有帮助。
    一旦你获得了数据，就将其存储在共享内存中，或者保存在寄存器集中并尽可能多次重用。
- en: Use of shared memory and cache
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用共享内存和缓存
- en: Using shared memory can provide a 10:1 increase in speed over global memory,
    but is limited in size—48 K on Fermi/Kepler devices and 16 K on all the previous
    devices. This may not sound like a great deal of space, especially with multigigabyte
    memory systems found on the host, but this is actually per SM. Thus, a GTX580
    or Tesla M2090 has 16 SMs active per GPU, each of which provides 48 K of shared
    memory, a total of 768 K. This is memory that runs at L1 cache speed. In addition,
    you have 768 K of L2 cache memory (on 16 SM devices) that is shared between all
    the SMs. This allows for an order of magnitude faster, global memory, atomic operations
    than in previous generation GPUs.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享内存相比全局内存可以提供10:1的速度提升，但其大小是有限的——在Fermi/Kepler设备上为48K，在所有较早的设备上为16K。 这听起来可能不算很大，特别是在主机上拥有数GB内存的情况下，但实际上这是每个SM的大小。
    因此，一块GTX580或Tesla M2090 GPU上每个GPU有16个SM，每个SM提供48K的共享内存，总共是768K。 这些内存以L1缓存的速度运行。
    此外，你还拥有768K的L2缓存内存（在16个SM设备上），它在所有SM之间共享。 这使得全局内存的原子操作比上一代GPU快一个数量级。
- en: When you consider that a GTX580 comes with 1.5 GB of memory, 768 K means just
    a tiny fraction of that memory space can be held in cache at any one point in
    time. The equivalent Tesla card comes with 6 GB of memory. Thus, kernels that
    iterate over datasets need to be aware that they may be using either the cache
    or shared memory in an ineffective manner, if they are not reusing data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到GTX580配备了1.5 GB的内存，768 K意味着在任何时刻，只有极小的一部分内存空间可以被缓存。相应的Tesla卡配备了6 GB的内存。因此，需要反复遍历数据集的内核需要意识到，如果没有重用数据，它们可能会以无效的方式使用缓存或共享内存。
- en: Rather than a number of passes over a large dataset, techniques such as kernel
    fusion can be used to move through the data as opposed to passing over it multiple
    times. Think of the problem in terms of the output data and not the input data.
    Construct the problem such that you assign threads to output data items, not input
    data items. Create a fan in and not a fan out in terms of data flow. Have a preference
    for gather (collecting data) primitives, rather than scatter (distributing data)
    primitives. The GPU will broadcast data, both from global memory and the L2 cache,
    directly to each SM. This supports high-speed gather-type operations.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 与其多次遍历大型数据集，不如使用像内核融合这样的技术，通过数据而不是多次遍历数据。将问题看作是关于输出数据，而不是输入数据。构建问题时，将线程分配给输出数据项，而不是输入数据项。在数据流的方向上要有扇入（fan
    in），而不是扇出（fan out）。倾向于使用收集（gather）数据的原语，而不是分发（scatter）数据的原语。GPU将从全局内存和L2缓存直接广播数据到每个SM，这支持高速度的收集类型操作。
- en: 'On Fermi and Kepler we have a very interesting choice, to configure the shared
    memory to either prefer L1 cache (48 K L1 cache, 16 K shared) or to prefer shared
    (48 K shared, 16 K cache). By default the device will prefer shared memory, and
    thus you’ll have 48 K of shared memory available. This decision is not fixed,
    but set at runtime, and thus can be set per kernel call. Kernels that do not make
    use of shared memory, or keep to the 16 K limit to ensure compatibility with earlier
    GPUs, usually benefit significantly (10% to 20% performance gain) by enabling
    the additional 32 K of cache, disabled by default:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fermi和Kepler架构中，我们有一个非常有趣的选择，可以将共享内存配置为偏好L1缓存（48 K L1缓存，16 K共享）或偏好共享内存（48 K共享，16
    K缓存）。默认情况下，设备将偏好共享内存，因此你将拥有48 K的共享内存可用。这个决定不是固定的，而是在运行时设置的，因此可以按每个内核调用来设置。那些不使用共享内存，或保持在16
    K限制内以确保与早期GPU兼容的内核，通常通过启用额外的32 K缓存（默认情况下禁用）会显著受益（性能提升10%到20%）：
- en: '[PRE20]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: where the `cache_prefer` parameter is `cudaFuncCachePreferShared` for 48 K of
    shared memory and 16 K of L1 cache, or `cudaFuncCachePreferL1` for 48 K of cache
    memory and 16 K of shared memory. Note, Kepler also allows a 32 K/32 K split.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`cache_prefer`参数为`cudaFuncCachePreferShared`时，表示48 K共享内存和16 K L1缓存，或`cudaFuncCachePreferL1`时，表示48
    K缓存内存和16 K共享内存。注意，Kepler还允许32 K/32 K的分割方式。
- en: There are, however, some areas where the cache causes Fermi and Kepler to operate
    slower than previous generation GPUs. On compute 1.x devices, memory transactions
    would be progressively reduced in size to as little as 32 bytes per access if
    the data item was small. Thus, a kernel that accesses one data element from a
    widely dispersed area in memory will perform poorly on any cache-based architecture,
    CPU, or GPU. The reason for this is that a single-element read will drag in 128
    bytes of data. For most programs, the data brought into the cache will then allow
    a cache hit on the next loop iteration. This is because programs typically access
    data close in memory to where they previously accessed data. Thus, for most programs
    this is a significant benefit. However, for programs that only need one data element,
    the other 124 bytes are wasted. For such kernels, you have to configure the memory
    subsystem to fetch only the memory transactions it needs, not one that is cache
    line sized. You can do this only at compile time via the `-Xptxas –dlcm=cg` flag.
    This reduces all access to 32 bytes per transaction and disables the L1 cache.
    For read only data consider also using either texture or constant memory.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一些地方，缓存导致Fermi和Kepler的性能比上一代GPU更慢。在计算1.x设备上，如果数据项较小，内存事务的大小会逐步减少，直到每次访问只有32字节。因此，一个从广泛分散的内存区域访问单个数据元素的内核，在任何基于缓存的架构、CPU或GPU上表现都不好。原因是单元素读取会引入128字节的数据。对于大多数程序来说，引入缓存的数据将在下一次循环迭代时产生缓存命中。这是因为程序通常会访问接近上次访问数据的位置。因此，对于大多数程序来说，这是一项重要的优势。然而，对于只需要一个数据元素的程序，其它124字节的数据会浪费掉。对于这种内核，你必须配置内存子系统，仅获取它所需要的内存事务，而不是缓存行大小的事务。你只能通过`-Xptxas
    –dlcm=cg`标志在编译时进行此操作。这将把所有访问减少到每次事务32字节，并禁用L1缓存。对于只读数据，也可以考虑使用纹理或常量内存。
- en: With G80/GT200, compute 1.x hardware, it’s essential that you make use of shared
    memory as an integral part of the kernel design. Without cached accessed to data,
    be it explicitly via shared memory or implicitly via a hardware-managed cache,
    memory latency times are just huge. The arrival of cache on GPUs via the Fermi
    architecture has made it much, much easier to write at a program, or kernel, that
    performs at least reasonably well on the GPU.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于G80/GT200，计算1.x硬件，确保将共享内存作为内核设计的一个组成部分是至关重要的。如果没有通过共享内存显式访问数据，或者通过硬件管理的缓存隐式访问数据，内存延迟时间将非常大。通过Fermi架构，GPU上缓存的到来使得编写至少在GPU上合理运行的程序或内核变得更加容易。
- en: Let’s look at some of the obstacles to using shared memory. The first is the
    size available—16 K on compute 1.x hardware and up to 48 K on compute 2.x hardware.
    It can be allocated statically at compile time via the `__shared__` prefix for
    variables. It is also one of the optional parameters in a kernel call, that is,
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下使用共享内存的一些障碍。第一个是可用的大小——在计算1.x硬件上为16 K，而在计算2.x硬件上为48 K。它可以通过`__shared__`前缀在编译时静态分配给变量。它也是内核调用中的一个可选参数，即，
- en: '[PRE21]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With runtime allocation, you additionally need a pointer to the start of the
    memory. For example,
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用运行时分配时，你还需要一个指向内存开始位置的指针。例如，
- en: '[PRE22]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that L2 cache size in Fermi is not always 768 K as stated in the CUDA C
    programmer guide. In fact, the L2 cache is based on the type of device being used
    and the number of SMs present. Compute 2.1 devices may have less L2 cache than
    compute 2.0 devices. Even compute 2.0 devices without all the SMs enabled (GTX470,
    GTX480, GTX570) have less than 768 K of L2 cache. The GTX460 device we’re using
    for testing has 512 K of L2 cache and the GTX470 device has 640 K.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Fermi中的L2缓存大小并不总是如CUDA C程序员指南中所述的768 K。事实上，L2缓存的大小取决于所使用的设备类型和存在的SM数量。计算2.1设备可能比计算2.0设备具有更少的L2缓存。即使是没有启用所有SM的计算2.0设备（如GTX470、GTX480、GTX570），它们的L2缓存也少于768
    K。我们用于测试的GTX460设备有512 K的L2缓存，而GTX470设备有640 K。
- en: The size of the L2 cache is returned from a call to `cudaGetDeviceProperties`
    API as `l2CacheSize` member.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: L2缓存的大小通过调用`cudaGetDeviceProperties` API返回，作为`l2CacheSize`成员。
- en: Section summary
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部分总结
- en: • Think carefully about the data your kernel processes and how best to arrange
    this in memory.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: • 仔细考虑你的内核处理的数据以及如何最好地将其安排在内存中。
- en: • Optimize memory access patterns for coalesced 128-byte access, aligning with
    the 128-byte memory fetch and L1 cache line size.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: • 优化内存访问模式，以实现合并的128字节访问，确保与128字节内存获取和L1缓存行大小对齐。
- en: • Consider the single-/double-precision tradeoff and how this impacts memory
    usage.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: • 考虑单精度/双精度的权衡，以及这对内存使用的影响。
- en: • Fuse multiple kernels to single kernels where appropriate.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: • 在适当的情况下，将多个内核融合为单个内核。
- en: • Make optimal use of shared memory and cache, ensuring you’re making full use
    of the expanded size on later compute levels.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: • 充分利用共享内存和缓存，确保在后续计算级别上充分利用扩展的内存大小。
- en: 'Strategy 3: Transfers'
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 3：数据传输
- en: Pinned memory
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 锁定内存
- en: To work on a dataset you need to transfer the data from the host to the device,
    work on the dataset, and transfer the results back to the host. Performed in a
    purely serial manner, this causes periods where both the host and GPU are inactive,
    both in terms of unused transfer capacity and compute capacity.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理数据集，你需要将数据从主机传输到设备，处理数据集，然后将结果传回主机。如果完全以串行方式执行，这将导致主机和GPU在某些时期都处于非活动状态，既包括未使用的传输容量，也包括未使用的计算容量。
- en: We looked in detail in the chapter on multi-GPU usage at how to use streams
    to ensure the GPU always has some work to do. With a simple double-buffering technique,
    while the GPU is transferring back the results and acquiring a new work packet,
    the other buffer is being used by the compute engine to process the next data
    block.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多GPU使用章节中详细讨论了如何使用流来确保GPU始终有工作可做。通过一个简单的双缓冲技术，当GPU将结果传输回并获取新的工作包时，另一个缓冲区将被计算引擎用来处理下一个数据块。
- en: The host processor supports a virtual memory system where a physical memory
    page can be marked as swapped out. It can then be paged to disk. Upon an access
    by the host processor to that page, the processor loads the page back in from
    disk. It allows the programmer to use a much larger virtual address space than
    is actually present on the hardware. Given that the programs typically exhibit
    quite good locality, this allows the total memory space to be much larger than
    the physical limits allow. However, if the program really does need 8 GB and the
    host only has 4 GB, the performance will typically be poor.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 主机处理器支持虚拟内存系统，可以将物理内存页面标记为已交换出，然后将其分页到磁盘。当主机处理器访问该页面时，处理器将从磁盘加载该页面。它允许程序员使用比硬件实际存在的内存空间要大的虚拟地址空间。考虑到程序通常表现出较好的局部性，这使得总内存空间可以比物理限制更大。然而，如果程序确实需要8
    GB的内存，而主机只有4 GB内存，性能通常会较差。
- en: Arguably the use of virtual memory is a hangover from a time when memory capacities
    were very limited. Today you can purchase 16 GB of memory for a little over 100
    euros/dollars/pounds, meaning the host’s need to use virtual memory is almost
    eliminated for most applications.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，虚拟内存的使用是从内存容量非常有限的时代遗留下来的。今天，你可以花费略高于100欧元/美元/英镑购买16 GB内存，这意味着大多数应用程序几乎不再需要使用虚拟内存。
- en: Most programs, except for big data problems, will generally fit within the host
    memory space. If not, then there are special server solutions that can hold up
    to 128 GB of memory per node. Such solutions are often preferable, as they allow
    you to keep the data within one node rather than add the complexity of a multinode
    solution. Of course, loading the dataset in chunks is perfectly feasible, but
    then you are ultimately limited by the throughput of the I/O hardware.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 除了大数据问题外，大多数程序通常可以适应主机内存空间。如果不能，便有一些特殊的服务器解决方案，每个节点可支持多达128 GB的内存。此类解决方案通常更为理想，因为它们能让你将数据保持在单个节点内，而不是增加多节点解决方案的复杂性。当然，将数据集分块加载也是完全可行的，但这样最终会受到I/O硬件吞吐量的限制。
- en: You should always be using page-locked memory on a system that has a reasonable
    amount of host memory. Page-locked memory allows the DMA (direct memory access)
    controller on the GPU to request a transfer to and from host memory without the
    involvement of the CPU host processor. Thus, no load is placed onto the host processor
    in terms of managing a transfer or having to bring back from disk any pages that
    have been swapped out.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有合理主机内存的系统上，你应该始终使用页锁定内存。页锁定内存允许GPU的DMA（直接内存访问）控制器请求进行主机内存之间的传输，而无需CPU主机处理器参与。因此，主机处理器在管理传输或从磁盘加载任何被交换出去的页面时，不会增加负担。
- en: The PCI-E transfers in practice can only be performed using DMA-based transfer.
    The driver does this in the background when you don’t use page-locked memory directly.
    Thus, the driver has to allocate (or malloc) a block of paged-locked memory, do
    a host copy from the regular memory to the page-locked memory, initiate the transfer,
    wait for the transfer to complete, and then free the page-locked memory. All of
    this takes time and consumes precious CPU cycles that could be used more productively.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，PCI-E传输只能通过基于DMA的传输执行。当你不直接使用页面锁定内存时，驱动程序会在后台执行此操作。因此，驱动程序必须分配（或malloc）一个页面锁定内存块，从常规内存到页面锁定内存进行主机复制，启动传输，等待传输完成，然后释放页面锁定内存。所有这些都会耗费时间，并消耗宝贵的CPU周期，这些CPU周期本可以用于更高效的工作。
- en: Memory allocated on the GPU is by default allocated as page locked simply because
    the GPU does not support swapping memory to disk. It’s the memory allocated on
    the host processor we’re concerned with. To allocate page-locked memory we need
    to either allocate it using the special `cudaHostMalloc` function or allocate
    it with the regular `malloc` function and register it as page-locked memory.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上分配的内存默认是页面锁定的，因为GPU不支持将内存换出到磁盘。我们关注的是主机处理器上分配的内存。为了分配页面锁定内存，我们需要通过特殊的`cudaHostMalloc`函数分配它，或者使用常规的`malloc`函数并将其注册为页面锁定内存。
- en: Registering memory simply sets some internal flags to ensure the memory is never
    swapped out and also tells the CUDA driver that this memory is page-locked memory
    so it is able to use it directly rather than using a staging buffer.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 注册内存简单地设置一些内部标志，确保内存永远不会被换出，并且还告诉CUDA驱动程序该内存是页面锁定内存，因此它可以直接使用这块内存，而不是使用暂存缓冲区。
- en: As with `malloc`, if you use `cudaHostAlloc` you need to use the `cudaFreeHost`
    function to free this memory. Do not call the regular C free function with pointers
    allocated from `cudaHostAlloc` or you will likely get a crash, some undefined
    behavior, or a strange error later in your program.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与`malloc`一样，如果你使用`cudaHostAlloc`，需要使用`cudaFreeHost`函数来释放这块内存。不要对通过`cudaHostAlloc`分配的指针调用常规C语言的free函数，否则可能会导致程序崩溃、出现未定义行为或在程序后续执行时出现奇怪的错误。
- en: The prototype for `cudaHostAlloc` is
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAlloc`的原型是'
- en: '[PRE27]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The flags consist of the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标志包括以下内容：
- en: '`cudaHostAllocDefault`—Use for most cases. Simply specifies the default behavior.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocDefault`—用于大多数情况。仅指定默认行为。'
- en: '`cudaHostAllocWriteCombined`—Use for memory regions that will be transferred
    *to the device only.* Do not use this flag when the host will read from this memory
    area. This turns off the caching of the memory region on the host processor, which
    means it completely ignores the memory region during transfers. This speeds up
    transfer to the device with certain hardware configurations.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocWriteCombined`—用于仅将内存区域传输*到设备*。当主机从此内存区域读取时，不要使用此标志。这将关闭主机处理器对内存区域的缓存，也就是说，它在传输过程中完全忽略该内存区域。这会在某些硬件配置下加快向设备的传输速度。'
- en: '`cudaHostAllocPortable`—The page-locked memory becomes page locked and visible
    in all CUDA contexts. By default the allocation belongs to the context creating
    it. You must use this flag if you plan to pass the pointer between CUDA contexts
    or threads on the host processor.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocPortable`—页面锁定内存变为页面锁定，并在所有CUDA上下文中可见。默认情况下，分配内存属于创建它的上下文。如果你计划在CUDA上下文或主机处理器的线程之间传递指针，必须使用此标志。'
- en: '`cudaHostAllocMapped`—We’ll look at this shortly. It allocates host memory
    into device memory space, allowing the GPU kernel to directly read and write with
    all transfers being implicitly handled.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocMapped`—我们稍后会详细讨论。它将主机内存分配到设备内存空间，使得GPU内核可以直接读写，所有传输都将被隐式处理。'
- en: To demonstrate the effect of paged memory versus nonpaged memory, we wrote a
    short program. This simply does a number of transfers, varied by size to and from
    a device, and invokes a dummy kernel to ensure the transfers actually take place.
    The results are shown in [Figure 9.13](#F0070).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示页面内存和非页面内存的效果，我们编写了一个简短的程序。该程序进行了一些大小不同的传输，来回传输数据，并调用一个虚拟内核以确保实际发生了传输。结果显示在[图
    9.13](#F0070)中。
- en: '![image](../images/F000090f09-13-9780124159334.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-13-9780124159334.jpg)'
- en: FIGURE 9.13 Transfer speed to and from the device (AMD Phenom II X4 905e, PCI-E
    2.0 X8 link).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 设备与设备之间的传输速度（AMD Phenom II X4 905e，PCI-E 2.0 X8链接）。
- en: On the Y axis we have MB/second to or from the device and the transfer size
    in bytes along the X axis. What we can see from the chart is that there is a considerable
    difference between using paged memory and nonpaged memory, the page-locked (pinned)
    memory being 1.4× faster for writes and 1.8× faster for reads. It took 194 ms
    to send out 512 MB of data to the card using page-locked memory, as opposed to
    278 ms to do this with nonpaged memory. Timings to transfer data from the device,
    for comparison, were 295 ms for paged memory versus 159 ms for pinned memory.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Y 轴上，我们显示了设备到设备的数据传输速度（MB/秒），而 X 轴则表示传输数据的大小（字节）。从图表中可以看到，使用分页内存和非分页内存之间有明显的差异，页面锁定（固定）内存在写入速度上比非分页内存快
    1.4 倍，在读取速度上快 1.8 倍。使用页面锁定内存向显卡发送 512 MB 数据需要 194 毫秒，而使用非分页内存则需要 278 毫秒。为了做对比，从设备传输数据的时间是：分页内存需要
    295 毫秒，而固定内存只需要 159 毫秒。
- en: 'On the input side, we see a strange issue: With page-locked memory, the bandwidth
    *from* the device is 20% higher than *to* the device. Given that PCI-E provides
    for a full duplex connection of the same speed to and from the device, you’d expect
    to see a similar transfer speed for both reads and writes. This variation, as
    you will see in subsequent tests, is very hardware dependent. All the systems
    tested except the Intel Nehalem I7 system exhibiting it to varying degrees.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入端，我们看到一个奇怪的问题：使用页面锁定内存时，从设备传输的数据带宽比向设备传输的数据带宽高出 20%。考虑到 PCI-E 提供的全双工连接应当是相同的速度来回传输，你本应该看到读取和写入的传输速度相似。正如你在后续测试中看到的那样，这种差异非常依赖于硬件。所有被测试的系统中，除了
    Intel Nehalem I7 系统外，都有不同程度的这种现象。
- en: Transfer rates to and from the four devices were almost identical, which is
    to be expected given the bandwidth of global memory on all of the cards is at
    least an order of magnitude greater than the PCI-E bandwidth.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从四个设备的传输速率来看，它们几乎是相同的，这也符合预期，因为所有显卡的全局内存带宽至少比 PCI-E 带宽高一个数量级。
- en: What is also very noticable is that to get near-peak bandwidth, even with pinned
    memory, the transfer size needs to be on the order of 2 MB of data. In fact, we
    don’t achieve the absolute peak until the transfer size is 16 MB or beyond.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个显著的现象是，即使使用固定内存，要接近最大带宽，传输数据的大小也需要在 2 MB 左右。事实上，直到传输数据大小达到 16 MB 或更大时，我们才能达到绝对的最大带宽。
- en: For comparison, the results are also shown in [Figures 9.14](#F0075), [9.15](#F0080)
    and [9.16](#F0085) for a number of systems we tested.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做对比，测试中使用的多个系统的结果也显示在 [图 9.14](#F0075)、[9.15](#F0080) 和 [9.16](#F0085) 中。
- en: '![image](../images/F000090f09-14-9780124159334.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-14-9780124159334.jpg)'
- en: FIGURE 9.14 Transfer speed to and from the device (Intel Atom D525, PCI-E 2.0
    X1 link).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 设备的传输速率（Intel Atom D525，PCI-E 2.0 X1 链接）。
- en: '![image](../images/F000090f09-15-9780124159334.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-15-9780124159334.jpg)'
- en: FIGURE 9.15 Transfer speed to and from the device (Intel I3 540, PCI-E X16 link).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 设备的传输速率（Intel I3 540，PCI-E X16 链接）。
- en: '![image](../images/F000090f09-16-9780124159334.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-16-9780124159334.jpg)'
- en: FIGURE 9.16 Transfer speed to and from the device (Intel I7 920, PCI-E X16 link).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 设备的传输速率（Intel I7 920，PCI-E X16 链接）。
- en: '[Figure 9.14](#F0075) shows a small netbook based on Intel’s low-power ATOM
    device, equipped with a dedicated GT218 NVIDIA ION graphics card. The peak PCI-E
    bandwidth you can typically see is up to 5 GB/s when using a 2.0 X16 link. As
    this netbook uses an X1 link, we could expect a maximum of 320 MB/s and we see
    in the order of 200 MB/s.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.14](#F0075) 显示了一款基于英特尔低功耗 ATOM 设备的小型笔记本，配备了专用的 GT218 NVIDIA ION 显卡。当使用
    2.0 X16 链接时，通常可以看到的最大 PCI-E 带宽高达 5 GB/s。由于这款笔记本使用的是 X1 链接，我们预计最大带宽为 320 MB/s，而实际看到的带宽大约为
    200 MB/s。'
- en: However, we see a very similar pattern to the AMD system, in that we need around
    2 MB plus transfer sizes before we start to achieve anything like the peak transfer
    rate. The only difference we see is there is a noticable difference between transfers
    to the device and transfers from the device.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们看到的模式与 AMD 系统非常相似，即在开始接近最大传输速率之前，我们需要大约 2 MB 或更多的传输数据量。唯一的区别是，设备到设备的传输与设备到主机的传输之间存在明显差异。
- en: A midrange system quite common in the consumer enviroment is the i3/i5 system
    from Intel. This particular one is the i3 540 running with a H55 chipset. As this
    device has a single GPU only, it’s running at X16 the peak speed PCI-E 2.0 ([Figure
    9.15](#F0080)).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者环境中，i3/i5 系统是非常常见的中端设备，这款特定的系统是运行 H55 芯片组的 i3 540。由于该设备仅配备了单一显卡，它的 PCI-E
    2.0 X16 最大带宽速度为 [图 9.15](#F0080) 所示。
- en: Again we can see the very large difference between pinned and nonpinned transfers,
    in excess of 2×. However, notice the absolute speed difference, approximately
    a 2× increase over the AMD system. This is largely due to the AMD system using
    an X8 PCI-E link, whereas the Intel system here uses an X16 PCI-E link.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看到固定内存传输和非固定内存传输之间的巨大差异，超过2倍。然而，请注意绝对速度差异，约为AMD系统的2倍增速。这主要是因为AMD系统使用的是X8 PCI-E连接，而Intel系统则使用X16
    PCI-E连接。
- en: The Intel I3 is a typical consumer processor. Anyone writing consumer-based
    applications should be very much aware by now that they need to be using pinned
    memory transfers, as we can see the huge difference it makes.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Intel I3是典型的消费级处理器。任何编写消费级应用程序的人现在应该非常清楚，他们需要使用固定内存传输，因为我们可以看到它带来的巨大差异。
- en: Finally, we look at one further system, this time from the server arena, using
    the Intel I7 920 Nehalem processor and the ASUS supercomputer socket 1366 motherboard.
    This is a common motherboard for very high-end GPUs, as it allows up to four PCI-E
    slots. This particular one is equipped with 3× GTX290 GPUs each using an PCI-E
    2.0 X16 connection.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看另一个系统，这次来自服务器领域，使用的是Intel I7 920 Nehalem处理器和ASUS超级计算机1366插槽主板。这是一款常见的高端GPU主板，因为它支持最多四个PCI-E插槽。这个特定的主板配备了3个GTX290
    GPU，每个都使用PCI-E 2.0 X16连接。
- en: What we see from the diagram is again interesting. Pinned and paged memory transfers
    are equal until transfer sizes larger than 512 KB, after which the pinned memory
    transfers lead by up to 1.8× over the paged memory–based transfers. Unlike the
    Nehalem I3 system, notice the Nehalem I7 system is more consistent and there is
    not a huge variation between inbound and outbound transfer speeds. However, also
    note the peak transfer speed, despite both devices being on a X16 PCI-E 2.0 link,
    is only 5400 MB/s as opposed to the I3, which achieved a peak of 6300 MB/s ([Figure
    9.16](#F0085)).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中我们再次可以看到有趣的现象。固定内存和分页内存传输在传输大小大于512 KB之前是相等的，之后固定内存传输的速度比基于分页内存的传输快最多1.8倍。与Nehalem
    I3系统不同，请注意Nehalem I7系统更为一致，进出传输速度之间没有巨大的波动。然而，也要注意，尽管两台设备都使用X16 PCI-E 2.0连接，峰值传输速度仅为5400
    MB/s，而I3系统的峰值为6300 MB/s（[图9.16](#F0085)）。
- en: So in summary, we can say that across a selection of today’s computing hardware,
    pinned memory transfers are approximately twice as fast as nonpinned transfers.
    Also we see there can be a considerable variance in performance between read and
    write speeds from and to the various devices. We can also see that we need to
    use larger, rather than smaller, block sizes, perhaps combining multiple transfers
    to increase the overall bandwidth utilization of the bus.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以说，在今天的各种计算硬件中，固定内存传输的速度大约是非固定内存传输的两倍。同时，我们也看到，从各种设备读取和写入速度之间可能存在显著的性能差异。我们还可以看到，我们需要使用更大的块大小，而不是更小的块大小，可能需要将多个传输合并，以提高总带宽利用率。
- en: Zero-copy memory
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零拷贝内存
- en: Zero-copy memory is a special form of memory mapping that allows you to map
    host memory into the memory space of the GPU directly. Thus, when you dereference
    memory on the GPU, if it’s GPU based, then you get high-speed (180 GB/s) bandwidth
    to global memory. If the GPU code reads a host-mapped variable it issues a PCI-E
    read transaction, and a (very) long time later the host will return the data over
    the PCI-E bus.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 零拷贝内存是一种特殊的内存映射方式，它允许将主机内存直接映射到GPU的内存空间。因此，当你在GPU上解除引用内存时，如果它是基于GPU的，你就能获得高速（180
    GB/s）的全局内存带宽。如果GPU代码读取一个主机映射的变量，它会发起一个PCI-E读取事务，然后过很长时间，主机通过PCI-E总线返回数据。
- en: After looking at the PCI-E bus bandwidth in the previous section, this doesn’t,
    at first glance, make a lot of sense. Big transfers are efficient and small transfers
    inefficient. If we rerun the test program we used for the previous examples, we
    see that the median transfer time is 0.06 ms on our sample AMD Phenom X4 platform.
    However, these are explicit, individual transfers, so it’s possible the zero-copy
    implementation may be more efficient.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了PCI-E总线带宽，乍一看，这似乎不太合理。大数据传输是高效的，而小数据传输效率较低。如果我们重新运行之前用于示例的测试程序，我们会看到，在我们的AMD
    Phenom X4平台上，中位数传输时间为0.06毫秒。然而，这些都是显式的、单独的传输，所以零拷贝实现可能会更高效。
- en: If you think about what happens with access to global memory, an entire cache
    line is brought in from memory on compute 2.x hardware. Even on compute 1.x hardware
    the same 128 bytes, potentially reduced to 64 or 32, is fetched from global memory.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑一下对全局内存的访问，会发现整个缓存行从内存中被读取到计算2.x硬件中。即使在计算1.x硬件上，仍然会从全局内存中获取相同的128字节，可能会减少到64或32字节。
- en: NVIDIA does not publish the size of the PCI-E transfers it uses, or details
    on how zero copy is actually implemented. However, the coalescing approach used
    for global memory could be used with PCI-E transfer. The warp memory latency hiding
    model can equally be applied to PCI-E transfers, providing there is enough arithmetic
    density to hide the latency of the PCI-E transfers. This is, in fact, the key
    to getting this to work. If you do very little for each global memory fetch and
    your application is already memory bound, this approach is unlikely to help you.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA并没有公开它使用的PCI-E传输的大小，或零拷贝是如何实现的细节。然而，全球内存的合并方法可以与PCI-E传输一起使用。只要有足够的算术密度来隐藏PCI-E传输的延迟，warp内存延迟隐藏模型同样可以应用于PCI-E传输。这实际上是实现这一功能的关键。如果每次全局内存获取时所做的工作很少，而你的应用程序已经是内存瓶颈，那么这种方法可能对你没有帮助。
- en: However, if your application is arithmetically bound, zero-copy memory can be
    a very useful technique. It saves you the explicit transfer time to and from the
    device. In effect, you are overlapping computation with data transfers without
    having to do explicit stream management. The catch, of course, is that you have
    to be efficient with your data usage. If you fetch or write the same data point
    more than once, this will create multiple PCI-E transactions. As each and every
    one of these is expensive in terms of latency, the fewer there are the better.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你的应用程序是算术瓶颈，零拷贝内存可能是一个非常有用的技术。它可以节省显式传输到设备和从设备传输的数据时间。实际上，你可以在不需要显式流管理的情况下将计算和数据传输重叠。需要注意的是，你必须高效地使用数据。如果你多次获取或写入相同的数据点，这将导致多个PCI-E事务。由于每一个这样的事务在延迟上都是昂贵的，事务越少越好。
- en: This can also be used very effectively on systems where the host and GPU share
    the same memory space, such as on the low-end NVIDIA ION-based netbooks. Here
    a malloc of global memory on the GPU actually results in a malloc of memory on
    the host. Clearly it doesn’t make sense to copy from one memory area on the host
    to another memory area on the host. Zero-copy memory can eliminate the need to
    perform these copies in such systems, without the impact of a PCI-E bus transfer.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术在主机和GPU共享相同内存空间的系统中也能非常有效地使用，例如低端的基于NVIDIA ION的上网本。在这种情况下，GPU上的全局内存malloc实际上会导致主机上的内存malloc。显然，将主机上的一个内存区域复制到另一个内存区域没有意义。零拷贝内存可以消除在这些系统中执行这些复制的需求，而且不会受到PCI-E总线传输的影响。
- en: Zero-copy memory also has one very useful use case. This is during the phase
    where you are initially porting a CPU application to a GPU. During this development
    phase there will often be sections of code that exist on the host that have not
    yet been ported over to the GPU. By declaring such data references as zero-copy
    memory regions, it allows the code to be ported in sections and still have it
    work. The performance will be generally poor until all the intended parts are
    present on the GPU. It simply allows this to be done in smaller steps so it’s
    not an “everything or nothing” problem.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 零拷贝内存还有一个非常有用的应用场景。这通常出现在你最初将一个CPU应用程序移植到GPU的阶段。在这个开发阶段，通常会有一些代码部分仍然存在于主机上，而尚未移植到GPU上。通过将这些数据引用声明为零拷贝内存区域，它允许分步移植代码并使其正常工作。直到所有预期的部分都出现在GPU上，性能通常会较差。它实际上使得这个过程可以分步骤进行，避免了“要么全有，要么全无”的问题。
- en: Let’s start by taking the existing `memcpy` program and expanding the kernel
    so it does the read of the data instead of relying on an explicit copy. For this
    we absolutely must coalesce accesses to memory, which when reading a simple one-dimensional
    array is easy. Thus, our kernel becomes
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从现有的`memcpy`程序开始，并扩展内核，使其执行数据读取，而不是依赖显式的复制。为此，我们必须完全合并对内存的访问，在读取简单的一维数组时这很容易实现。因此，我们的内核变成了
- en: '[PRE28]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the kernel we simply make the `x` and `y` grid dimensions into a single linear
    array and assign one element from the source dataset to the destination dataset.
    Next we have to do three critical things to use zero-copy or host-mapped memory—that
    is, first to enable it, second to allocate memory using it, and finally to convert
    the regular host pointer to the device memory space.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，我们只需将 `x` 和 `y` 网格维度合并为一个线性数组，并将源数据集中的一个元素分配给目标数据集。接下来，我们必须做三件关键的事情来使用零拷贝或主机映射内存——即：首先启用它，其次使用它分配内存，最后将常规主机指针转换为设备内存空间中的指针。
- en: 'Prior to any creation of a CUDA context, we need to make the following call:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 CUDA 上下文之前，我们需要进行以下调用：
- en: '[PRE30]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When the CUDA context is created the driver will know it also has to support
    host-mapped memory. Without this the host-mapped (zero-copy) memory will not work.
    This will not work if it’s done after the CUDA context has been created. Be aware
    that calls to functions like `cudaHostAlloc`, despite operating on host memory,
    still create a GPU context.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 当 CUDA 上下文创建时，驱动程序将知道它也必须支持主机映射内存。如果没有这个，主机映射（零拷贝）内存将无法工作。如果在创建 CUDA 上下文后进行此操作，则无法工作。请注意，尽管
    `cudaHostAlloc` 等函数操作的是主机内存，但它们仍会创建 GPU 上下文。
- en: 'Although most devices support zero-copy memory, some earlier devices do not.
    It’s not part of the compute level, so it has to be checked for explicitly as
    follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数设备支持零拷贝内存，但一些早期设备不支持。它不是计算级别的一部分，因此必须显式检查，如下所示：
- en: '[PRE31]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The next stage is to allocate memory on the host such that it can be mapped
    into device memory. This is done with an additional flag `cudaHostAllocMapped`
    to the `cudaHostAlloc` function.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段是在主机上分配内存，以便可以映射到设备内存中。通过为 `cudaHostAlloc` 函数添加一个额外的标志 `cudaHostAllocMapped`
    来完成此操作。
- en: '[PRE32]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we need to convert the host pointer to a device pointer, which is
    done with the `cudaHostGetDevicePointer` function as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将主机指针转换为设备指针，这可以通过如下的 `cudaHostGetDevicePointer` 函数完成：
- en: '[PRE33]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In this call we convert the `host_data_to_device` previously allocated in the
    host memory space to an equvalent pointer, but within the GPU memory space. Do
    not confuse the pointers. Use the converted pointer only with GPU kernels and
    the original pointer only in code that executes on the host. Thus, for example,
    to free the memory later, an operation performed on the host, the existing call
    remains the same:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在此调用中，我们将之前在主机内存空间中分配的 `host_data_to_device` 转换为一个等效的指针，但该指针位于 GPU 内存空间中。不要混淆这些指针。只应在
    GPU 内核中使用转换后的指针，而仅在主机上执行的代码中使用原始指针。因此，例如，要稍后释放内存，这是在主机上执行的操作，现有的调用保持不变：
- en: '[PRE34]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As we’re using memory blocks up to 512 MB in size, to access one element per
    thread no matter how many threads we allocate per block means the number of blocks
    will exceed 64 K. This is the hard limit on the number of blocks in any single
    dimension. Thus, we have to introduce another dimension. This introduces grids,
    which we covered in [Chapter 5](CHP005.html). We can do this relatively simply
    by fixing the number of grids at some value that will be large enough to allow
    sufficient flexibility in selecting the number of threads per block.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的内存块最大为 512 MB，为了确保每个线程访问一个元素，无论我们每个块分配多少线程，这样会导致块的数量超过 64 K。这是单一维度上块的数量的硬性限制。因此，我们必须引入另一个维度。这引入了网格，我们在[第
    5 章](CHP005.html)中介绍过。我们可以通过将网格的数量固定为一个足够大的值来简单地做到这一点，从而允许在选择每个块的线程数时具有足够的灵活性。
- en: '[PRE35]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`dim3 blocks(num_grid, num_blocks_per_grid);`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim3 blocks(num_grid, num_blocks_per_grid);`'
- en: 'The `dim3` operation simply assigns the regular scalar values we calculated
    to a structure type holding a triplet that can be used as a single parameter in
    the kernel launch. It causes the kernel to launch 64 grids of *N* blocks. This
    simply ensures that for a given block index we do not exceed the 64 K limit. Thus,
    on the kernel launch, we replace `num blocks`, a scalar type, with `blocks`, a
    `dim3` type:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim3` 操作简单地将我们计算的常规标量值分配给一个结构类型，该类型持有一个三元组，可以作为内核启动中的单个参数使用。它使内核启动 64 个网格的
    *N* 块。这仅仅确保了在给定的块索引下，我们不会超过 64 K 限制。因此，在内核启动时，我们将 `num blocks`（一个标量类型）替换为 `blocks`（一个
    `dim3` 类型）：'
- en: '[PRE37]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: What we see for transfers *to the device* is that the overall figures are identical
    to the transfers using explicit memory copies. This has significant implications.
    Most applications that do not already use the stream API simply copy memory to
    the GPU at the start and copy back once the kernel is complete. We can shrink
    that time drastically using pinned memory copies, but the time is still cumulative
    because it’s a serial operation.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传输*到设备*的情况，我们看到整体数据与使用显式内存拷贝的传输数据相同。这具有重要的意义。大多数不使用流API的应用程序通常在开始时将内存拷贝到GPU，内核完成后再拷贝回来。我们可以通过使用固定内存拷贝大幅缩短这个时间，但由于这是串行操作，时间仍然是累积的。
- en: In effect, what happens with the zero-copy memory is we break both the transfer
    and the kernel operation into much smaller blocks, which execute them in a pipeline
    ([Figure 9.17](#F0090)). The overall time is reduced quite significantly.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用零拷贝内存时，我们将传输和内核操作拆解成更小的块，并通过流水线执行它们（见[图9.17](#F0090)）。总体执行时间显著缩短。
- en: '![image](../images/F000090f09-17-9780124159334.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-17-9780124159334.jpg)'
- en: FIGURE 9.17 Serial versus overlapped transfer/kernel execution.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 串行与重叠传输/内核执行。
- en: Notice we did not perform the same optimization with the copy from device. The
    reason for this is because consumer GPUs have only one copy engine enabled. Thus,
    they support only a single memory stream. When you do a read-kernel-write operation,
    if the write is pushed into the stream ahead of subsequent reads, it will block
    the read operations until the pending write has completed. Note this is not the
    case for Tesla devices, as both copy engines are enabled and thus Tesla cards
    are able to support independent to and from streams. Prior to Fermi, there was
    only ever one copy engine on any card.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们并未对从设备拷贝进行相同的优化。原因在于消费级GPU仅启用了一个拷贝引擎。因此，它们仅支持单一的内存流。当执行读取-内核-写入操作时，如果写入操作被推到流前面，则会阻塞后续的读取操作，直到待处理的写入完成。需要注意的是，特斯拉设备不适用这种情况，因为两个拷贝引擎都已启用，因此特斯拉卡能够支持独立的到和从流。在Fermi之前，任何卡上只会启用一个拷贝引擎。
- en: However, with zero-copy memory the transfers are actually quite small. The PCI-E
    bus has the same bandwidth in both directions. Due to the high latency of the
    PCI-E-based memory reads, actually most of the reads should have been pushed into
    the read queue ahead of any writes. We may be able to achieve significant execution
    time savings over the explicit memory copy version.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用零拷贝内存时，传输实际上非常小。PCI-E总线在两个方向上的带宽是相同的。由于基于PCI-E的内存读取具有较高的延迟，实际上大多数读取操作应该被提前推入读取队列，而不是等待写操作。通过零拷贝，我们可能能够比显式内存拷贝版本节省显著的执行时间。
- en: Note the diagram in [Figure 9.18](#F0095) is simplified in that it lists a single
    “Pinned To & From Device” line, yet we show the zero device copy times explicitly
    for the devices. The pinned memory time was effectively the same for all devices,
    so it was not shown per device.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图9.18](#F0095)中的图示进行了简化，列出了单一的“固定到/从设备”线，但我们显示了各设备的零设备拷贝时间。固定内存时间对所有设备基本相同，因此没有按设备分别显示。
- en: '![image](../images/F000090f09-18-9780124159334.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-18-9780124159334.jpg)'
- en: FIGURE 9.18 Zero-copy time versus explicit pinned copy time over different GPU
    generations.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 零拷贝时间与显式固定拷贝时间在不同GPU代际中的对比。
- en: We have listed the entire execution time of a single memory copy to device,
    kernel execution, and memory copy from device. Thus, there is some overhead that
    is not present when purely measuring the transfer to/from the device. As we’re
    using zero copy, the memory transactions and the kernel time cannot be pulled
    apart. However, as the kernel is doing very little, the overall execution time
    represents a fair comparison between the zero copy and explicit copy versions.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了从设备到内核执行的单次内存拷贝以及从设备到内存的拷贝的整个执行时间。因此，存在一些开销，这在仅测量到/从设备的传输时并不出现。由于我们使用了零拷贝，内存事务和内核时间无法分开。但由于内核执行的操作非常少，整体执行时间在零拷贝和显式拷贝版本之间提供了一个公正的比较。
- en: There is a considerable amount of variability. What we can see, however, is
    that for small transfer amounts, less than 512 KB, zero copy is faster than using
    explicit copies. Let’s now look at sizes larger than 512 KB in [Table 9.1](#T0010)
    and [Figure 9.19](#F0100).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 存在相当大的变动性。然而，我们可以看到，对于小于512 KB的传输量，零拷贝比显式拷贝要快。现在让我们查看[表9.1](#T0010)和[图9.19](#F0100)中大于512
    KB的大小。
- en: Table 9.1 Zero-Copy Results (execution time in ms)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 零拷贝结果（执行时间以毫秒为单位）
- en: '![Image](../images/T000090tabT0010.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0010.jpg)'
- en: '![image](../images/F000090f09-19-9780124159334.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-19-9780124159334.jpg)'
- en: FIGURE 9.19 Zero-copy graph (time in ms versus transfer size).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 零拷贝图（时间（毫秒）与传输大小的关系）。
- en: What is very interesting to see here is a considerable drop in execution time.
    On the Fermi hardware the overlapping of the kernel operation with the memory
    copies drops the execution time from 182 ms to 104 ms, a 1.75× speedup. The results
    are less impressive in the earlier devices, but still represent a significant
    speedup.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里非常有趣的是执行时间明显下降。在Fermi硬件上，内核操作与内存拷贝的重叠使得执行时间从182毫秒降至104毫秒，速度提升了1.75倍。在较早的设备中，结果不那么引人注目，但仍然代表了显著的加速。
- en: You can of course achieve this using streams and asynchronous memory copies,
    as demonstrated in [Chapter 8](CHP008.html). Zero copy simply presents an alternative,
    and somewhat simpler, interface you can work with.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以使用流和异步内存拷贝来实现这一点，如[第8章](CHP008.html)中所示。零拷贝仅仅提供了一种替代方案，并且提供了一个您可以使用的相对简单的接口。
- en: However, there are some caveats. Beware of exactly how many times the data is
    being fetched from memory. Re-reading data from global memory will usually exclude
    the use of zero-copy memory.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些需要注意的事项。请注意数据从内存中获取的次数。重新读取全局内存中的数据通常会排除使用零拷贝内存。
- en: If we modify the program to read the value from host memory twice instead of
    once, then the performance drops by half on the 9800 GT and GTX260 platforms,
    the compute 1.x devices. This is because each and every fetch from global memory
    on these platforms is not cached. Thus, the number of PCI-E transactions issued
    is doubled, as we double the amount of times the GPU accesses the zero-copy memory
    area.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们修改程序，使其从主机内存中读取值两次而不是一次，那么在9800 GT和GTX260平台（即compute 1.x设备）上，性能将下降一半。这是因为这些平台上每次从全局内存获取数据时都没有缓存。因此，发出的PCI-E事务数量会翻倍，因为我们使GPU访问零拷贝内存区域的次数翻倍。
- en: On Fermi the situation is somewhat different. It has an L1 and L2 cache and
    it’s highly likely the data fetched earlier in the kernel will still be in the
    cache when the latter access hits the same memory address. To be sure, you have
    to explicitly copy the data you plan to reuse to the shared memory. So in Fermi,
    depending on the data pattern, you typically do not see the device issuing multiple
    PCI-E transactions, as many of these hit the internal caches and therefore never
    create a global memory transaction.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fermi上，情况稍有不同。它有L1和L2缓存，并且很可能内核中较早获取的数据仍然会在缓存中，当后续访问命中相同的内存地址时，它会从缓存中获取。为了确保这一点，您必须显式地将计划重用的数据复制到共享内存中。因此，在Fermi上，取决于数据模式，通常不会看到设备发出多个PCI-E事务，因为这些大多数命中内部缓存，因此不会产生全局内存事务。
- en: Thus, zero-copy memory presents a relatively easy way to speed up your existing
    serial code without having to explicitly learn the stream API, providing you are
    careful about data reuse and have a reasonable amount of work to do with each
    data item.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，零拷贝内存为您提供了一种相对简单的方法，可以在不需要显式学习流API的情况下加速现有的串行代码，前提是您小心数据重用，并且每个数据项的工作量合理。
- en: However, be aware that the bandwidth of the PCI-E bus is nowhere near the bandwidth
    available on a CPU. The latest Sandybridge I7 processor (Socket 2011) achieves
    some 37 GB/s of memory bandwidth, from a theoretical peak of 51 GB/s. We’re achieving
    5–6 GB/s from a theoretical peak of 8 GB/s on the PCI-E 2.0 bus. You must have
    enough work in your application to justify the cost of moving the data over the
    PCI-E bus. Consider that the CPU can be a better alternative in situations where
    very little work is being done per element.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，PCI-E总线的带宽远远无法与CPU的带宽相比。最新的Sandybridge I7处理器（Socket 2011）可以实现约37 GB/s的内存带宽，而理论峰值为51
    GB/s。我们在PCI-E 2.0总线上实现了5–6 GB/s，理论峰值为8 GB/s。你必须在应用中有足够的工作量，才能证明通过PCI-E总线传输数据的成本是合理的。考虑到在每个元素的工作量很小的情况下，CPU可能是更好的选择。
- en: The program used for these measurements is shown here for reference.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 用于这些测量的程序如下所示，供参考。
- en: '[PRE38]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`         size_in_bytes));`'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '`         size_in_bytes));`'
- en: '[PRE50]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`   {`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`   {`'
- en: '[PRE60]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '`  // Adjust for doing a copy to and back`'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '`  // 调整以进行拷贝和返回`'
- en: '[PRE71]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Bandwidth limitations
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带宽限制
- en: The ultimate bandwidth limitation of a significant number of applications is
    the I/O speed of whatever devices the input and output data have to be acquired
    from and written to. This is often the limitation on the speedup of any application.
    If your application takes 20 minutes to run on a serial CPU implementation and
    can express enough parallelism, it’s quite feasible for that application to run
    on a GPU in less time than it takes to load and save the data from the storage
    device you are using.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序的最终带宽限制是输入和输出数据必须从中获取和写入的设备的I/O速度。这通常是应用程序加速的瓶颈。如果你的应用程序在串行CPU实现上运行需要20分钟，并且能够表达足够的并行性，那么该应用程序很有可能在GPU上运行，所需时间比加载和保存你使用的存储设备上的数据还要短。
- en: The first problem we have in terms of bandwidth is simply getting the data in
    and out of the machine. If you are using network-attached storage, the limit to
    this will be the speed of the network link. The best solution to this problem
    is a high-speed SATA3 RAID controller using many high-speed SSD drives. However,
    this will not solve your bandwidth issues unless you are using the drive efficiently.
    Each drive will have a peak transfer rate into host memory, which is actually
    a function of the transfer rate of the drive, the controller, and the route to
    host memory.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽方面的第一个问题就是如何将数据进出机器。如果你使用的是网络附加存储，那么限制因素将是网络连接的速度。解决这个问题的最佳方法是使用多个高速SSD硬盘的高速SATA3
    RAID控制器。然而，除非你高效地使用硬盘，否则这并不能解决你的带宽问题。每个硬盘都有一个向主机内存传输数据的峰值速率，这实际上是硬盘、控制器以及到主机内存路径的传输速率的函数。
- en: Running a benchmark on a drive, such as the commonly used ATTO benchmark, can
    show you the effect of using different size blocks (see [Figure 9.20](#F0105)).
    This benchmark simulates access to drives based on a certain size of reads and
    writes. Thus, it reads and writes a 2 GB file in blocks of 1 K, 2 K, 4 K, etc.
    to see the effect of changing the block size.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬盘上运行基准测试，例如常用的ATTO基准测试，可以显示使用不同块大小的效果（见[图9.20](#F0105)）。这个基准测试模拟了基于某一块大小的读写操作。因此，它以1
    K、2 K、4 K等块大小读取和写入2 GB的文件，以观察改变块大小的效果。
- en: '![image](../images/F000090f09-20-9780124159334.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-20-9780124159334.jpg)'
- en: FIGURE 9.20 Bandwidth (MB/s) for a single SSD versus five hard disks in RAID
    0.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 单个SSD与RAID 0中的五个硬盘的带宽（MB/s）对比。
- en: We can see from the results that only when we read data in 64 K chunks or more
    do we achieve the peak bandwidth from the single SSD drive. For the RAID 0 hard
    drive system we need at least 1 MB blocks to make use of the multiple disks. Thus,
    you need to make sure you’re using the `fread` function in C to read suitable
    sized blocks of data from the disk subsystem. If we fetch data in 1 K chunks,
    we get just 24 MB/s from the drive, less than 10% of its peak read bandwidth.
    The more drives you add to a RAID system, the larger the minimum block size becomes.
    If you are processing compressed music or image files, the size of a single file
    may only be a few megabytes.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中我们可以看到，只有当我们以64 K或更大的块大小读取数据时，才能从单个SSD硬盘驱动器中获得峰值带宽。对于RAID 0硬盘系统，我们需要至少1
    MB的块大小才能利用多个磁盘。因此，你需要确保在C语言中使用`fread`函数来从磁盘子系统读取合适大小的数据块。如果我们以1 K的块大小获取数据，我们只能从驱动器获得24
    MB/s，远低于其峰值读取带宽的10%。你在RAID系统中增加的硬盘越多，最小块大小就会变得越大。如果你处理的是压缩的音乐或图像文件，那么单个文件的大小可能只有几兆字节。
- en: Note also that whether the data is compressible or not has a big impact on drive
    performance. The server level drives, such as the OCZ Vertex 3, provide both higher
    peak values and sustained bandwidth with uncompressible data. Thus, if your dataset
    is in an already compressed format (MP3, MP4, WMV, H.264, JPG, etc.), then you
    need to make sure you use server drives. The bandwidth on many consumer-level
    SSD drives can fall to half of quoted peak when using uncompressible data streams.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，数据是否可压缩对硬盘性能有很大影响。服务器级硬盘，如OCZ Vertex 3，在处理不可压缩数据时提供更高的峰值值和持续带宽。因此，如果你的数据集已经是压缩格式（如MP3、MP4、WMV、H.264、JPG等），你需要确保使用服务器级硬盘。许多消费级SSD硬盘在使用不可压缩数据流时，带宽可能降至标称峰值的二分之一。
- en: The reason for this is the use of synchronous NAND memory in the high-end server
    SSDs versus the cheaper and much lower-performing asynchronous NAND memory used
    in consumer SSDs. Even with noncompressed data, synchronous NAND-based drives
    still outperform their asynchronous cousins, especially once the drive starts
    to contain some data. OCZ also provides the RevoDrive R4 PCI-E-based product,
    which claims speeds in the order of 2 GB/s plus at the expense of a PCI-E slot.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于高端服务器 SSD 使用同步 NAND 存储，而消费级 SSD 则使用更便宜、性能更低的异步 NAND 存储。即使是未压缩的数据，基于同步 NAND
    的驱动器仍然优于其异步同类，尤其是在驱动器开始包含一些数据时。OCZ 还提供了基于 PCI-E 的 RevoDrive R4 产品，声称其速度可达到每秒 2
    GB 以上，但需要占用一个 PCI-E 插槽。
- en: The next bandwidth limit you hit is that of host memory speed. This is typically
    not an issue until you introduce multiple GPUs per node, if you consider that
    you can fetch data at 6 GB/s off the PCI-E bus from a very high-speed SSD RAID
    system. We then have to send out data at 6 GB/s to and from the host memory to
    the GPU. Potentially you could also write data again at 6 GB/s to the RAID controller.
    That’s a potential 24 GB/s of pure data movement without the CPU actually doing
    anything useful except moving data. We’re already hitting the bandwidth limits
    of most modern processor designs and have already surpassed that available from
    the older-generation CPUs. In fact, only the latest quad channel I7 Sandybridge-E
    CPU has anything like the bandwidth we could start moving around, if we were to
    solve the slow I/O device issue.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的带宽限制是主机内存速度。通常，直到在每个节点上引入多个 GPU 时，才会遇到这个问题。如果考虑到您可以从非常高速的 SSD RAID 系统通过
    PCI-E 总线以 6 GB/s 的速度获取数据，那么我们必须以 6 GB/s 的速度在主机内存与 GPU 之间传输数据。您也可能会以每秒 6 GB 的速度将数据写回
    RAID 控制器。这意味着在没有 CPU 实际执行任何有用工作的情况下，最多可以达到 24 GB/s 的纯数据传输速度，CPU 只做数据移动的工作。我们已经达到了大多数现代处理器设计的带宽极限，并且已经超越了旧一代
    CPU 的带宽。事实上，只有最新的四通道 I7 Sandybridge-E CPU 才具有我们可以开始移动的带宽，前提是我们解决了慢速 I/O 设备问题。
- en: CUDA 4.0 SDK introduced Peer2Peer GPU communication. The CUDA 4.1 SDK also introduced
    Peer2Peer communication with non-NVIDIA hardware. Thus, with the correct hardware,
    GPUs can talk to any supported device. This is mostly limited to a small number
    of InfiniBand and other highspeed network cards. However, in principle, any PCI-E
    device can talk with the GPU. Thus, a RAID controller could send data directly
    to and from a GPU. There is a huge potential for such devices, as no host memory
    bandwidth, PCI-E, or memory is consumed. As data is not having to flow to a CPU
    and then back out again, latency is dropped considerably.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 4.0 SDK 引入了 Peer2Peer GPU 通信。CUDA 4.1 SDK 还引入了与非 NVIDIA 硬件的 Peer2Peer 通信。因此，凭借正确的硬件，GPU
    可以与任何支持的设备进行通信。这主要限于少数 InfiniBand 和其他高速网络卡。不过，原则上，任何 PCI-E 设备都可以与 GPU 通信。因此，RAID
    控制器可以直接向 GPU 发送数据并接收数据。这类设备具有巨大的潜力，因为它们不消耗主机内存带宽、PCI-E 或内存。由于数据无需先流向 CPU 再返回，因此延迟大大降低。
- en: Once the data has been moved to the GPU, there is a bandwidth limit of up to
    190 GB/s on GeForce cards and 177 GB for Tesla, to and from the global memory
    on the device. To achieve this you need to ensure coalescing of the data reads
    from the threads and ensure your application makes use of 100% of the data moved
    from memory to the GPU.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被传输到 GPU，GeForce 卡的带宽上限为 190 GB/s，Tesla 为 177 GB/s，数据在设备的全局内存之间进行读写。为了实现这一点，您需要确保线程的数据读取是合并的，并确保您的应用程序能够充分利用从内存到
    GPU 传输的 100% 数据。
- en: Finally, we have shared memory. Even if you partition data into tiles, move
    it into shared memory, and access it in a bank conflict–free manner, the bandwidth
    limit is on the order of 1.3 TB/s. For comparison the AMD Phenom II and Nehalem
    I7 CPUs for a 64 KB L1 cache block, the same capacity as the GPU L1 cache and
    shared memory, has around 330 GB/s bandwidth, some 25% of that of the GPU.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是共享内存。即使您将数据划分为瓦片，将其移动到共享内存中，并以无银行冲突的方式访问，带宽限制大约为 1.3 TB/s。相比之下，AMD Phenom
    II 和 Nehalem I7 CPU 的 64 KB L1 缓存块，其容量与 GPU 的 L1 缓存和共享内存相同，带宽大约为 330 GB/s，约为 GPU
    带宽的 25%。
- en: If we take a typical float or integer parameter, it’s 4 bytes wide. Thus, the
    bandwidth to global memory is a maximum of 47.5 giga-elements per second (190
    GB/s ÷ 4). Assuming you read and write just one value, we can halve this figure
    to 23.75 giga-elements per second. Thus, with no data reuse, this is the maximum
    upper throughput of your application.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以典型的浮点数或整数参数为例，它的宽度是 4 字节。因此，访问全局内存的带宽最大为每秒 47.5 吉元素（190 GB/s ÷ 4）。假设你只读取和写入一个值，我们可以将这个数值减半，得到每秒
    23.75 吉元素。因此，在没有数据复用的情况下，这是你的应用程序的最大上行吞吐量。
- en: The Fermi device is rated in excess of 1 teraflop, that is, it can process on
    the order of 1000 giga floating-point operations per second. Kepler is rated at
    in excess of 3 teraflops. The actual available flops depend on how you measure
    flops. The fastest measure is the FMADD instruction (floating-point multiply and
    add) instruction. This multiplies two floating-point numbers together and adds
    another number to it. As such, this counts as two flops, not one. Real instruction
    streams intermix memory loads, integer calculations, loops, branches, etc. Thus,
    in practice, kernels never get near to this peak figure.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi 设备的性能超过了 1 teraflop，即它可以每秒处理大约 1000 亿次浮点运算。Kepler 的性能超过了 3 teraflops。实际可用的浮点运算性能取决于你如何测量浮点运算。最快的测量方法是
    FMADD 指令（浮点数乘法加法）指令。它将两个浮点数相乘并加上另一个数。因此，这算作两个浮点运算，而不是一个。实际的指令流会交织着内存加载、整数计算、循环、分支等。因此，在实践中，内核的实际性能通常远未接近这个峰值。
- en: We can measure the real speed achievable by simply using the program we previously
    developed to visualize the PCI-E bandwidth. Simply performing a memory copy from
    global memory to global memory will show us the maximum possible read and write
    speed a kernel can achieve.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地使用之前开发的程序来可视化 PCI-E 带宽，从而测量实际可达到的速度。只需进行从全局内存到全局内存的内存拷贝，就能展示一个内核能够达到的最大读写速度。
- en: '[PRE73]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Note the values in the parentheses shows grids × blocks × threads. The above
    figures are plotted in [Figure 9.21](#F0110).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 注意括号中的数值表示网格 × 块 × 线程。上述数据在[图 9.21](#F0110)中绘制。
- en: '![image](../images/F000090f09-21-9780124159334.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-21-9780124159334.jpg)'
- en: FIGURE 9.21 Global memory bandwidth across devices.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 各设备的全局内存带宽。
- en: These results are created by pushing 16 kernels into an asynchronous stream,
    with each call surrounded by a stop and start event. Each kernel performs a single-element
    copy from the source to the destination for every memory location. The execution
    time of the first kernel in each batch is ignored. The remaining kernels contribute
    to the total time, which is then averaged over the kernels. The quoted bandwidth
    for the GTX470 is 134 GB/s, so we’re falling short of this, despite having a simple
    kernel and obviously hitting the peak at the larger transfer sizes.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果是通过将 16 个内核推送到异步流中生成的，每个调用都被停止和启动事件包围。每个内核对每个内存位置执行从源到目标的单元素拷贝。每批次第一个内核的执行时间被忽略，剩下的内核贡献了总时间，这些时间会被平均到所有内核上。GTX470
    的带宽为 134 GB/s，因此我们仍然没有达到这一点，尽管我们使用的是简单的内核，而且在较大的传输大小时显然达到了峰值。
- en: What we see from this chart is that to achieve anywhere near the peak memory
    performance you need to have enough threads. We start off by using 32 threads
    per block until we launch a total of 64 blocks. This ensures that all the SMs
    are given work, rather than one SM getting a large number of threads and therefore
    most of the work. We then increase the thread count per block up to 256 threads
    once there is a reasonable distribution of blocks to the SMs.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图表中我们可以看到，要实现接近内存性能峰值的效果，你需要有足够的线程。我们首先使用每个块 32 个线程，直到我们启动了总共 64 个块。这确保所有
    SM 都能分配到工作，而不是一个 SM 获取大量线程并因此承担大部分工作。然后，一旦块的分布在 SM 之间合理，我们就将每个块的线程数增加到 256 个。
- en: Changing the element type from `uint1` to `uint2`, `uint3`, and `uint4` produces
    some interesting results. As you increase the size of a single element, the total
    number of transactions issued to the memory subsystem is reduced. On the GTX470,
    going from the 4-byte read (single-element integer or float) to an 8-byte read
    (dual-element integer, float, or single-element double) resulted in up to a peak
    23% increase in measured bandwidth to and from global memory ([Figure 9.22](#F0115)).
    The average improvement was somewhat lower at just 7%, but this still represents
    a reasonable improvement in execution time by simply switching from `int1`/`float1`
    to `int2`/`float2` vector types. The GTX460 presents a similar, but more pronounced
    pattern ([Figure 9.23](#F0120)).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 将元素类型从`uint1`改为`uint2`、`uint3`和`uint4`会产生一些有趣的结果。当你增加单个元素的大小时，发往内存子系统的事务总数会减少。在GTX470上，将4字节读取（单元素整数或浮点数）改为8字节读取（双元素整数、浮点数或单元素双精度浮点数）时，全球内存带宽的峰值提高了23%。平均改善为7%，但通过简单地将`int1`/`float1`类型改为`int2`/`float2`向量类型，这仍然代表了执行时间的合理改善。GTX460呈现出类似但更明显的模式（[图9.23](#F0120)）。
- en: '![image](../images/F000090f09-22-9780124159334.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-22-9780124159334.jpg)'
- en: FIGURE 9.22 Global memory bandwidth GTX470/compute 2.0 (transaction size in
    bytes).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.22 全局内存带宽 GTX470/计算能力2.0（事务大小，单位字节）。
- en: '![image](../images/F000090f09-23-9780124159334.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-23-9780124159334.jpg)'
- en: FIGURE 9.23 Global memory bandwidth GTX460/compute 2.1 (transaction size in
    bytes).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.23 全局内存带宽 GTX460/计算能力2.1（事务大小，单位字节）。
- en: To achieve optimum bandwidth, the CUDA code was compiled specifically for compute
    2.1 devices. We also found that thread blocks that were a multiple of 48 threads
    worked best. This is not surprising given that there are three sets of 16 cores
    per SM instead of the usual two. When moving from 4 bytes per element to 8 or
    16 bytes per element, the bandwidth was increased by an average of 19%, but a
    best case of 38%.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现最佳带宽，CUDA代码专门为计算能力2.1的设备编译。我们还发现，线程块大小为48的倍数时效果最好。这并不令人惊讶，因为每个SM中有三组16个核心，而不是通常的两组。当元素大小从4字节增加到8字节或16字节时，带宽平均提高了19%，但最好的情况是提高了38%。
- en: A single warp transaction for 8 bytes per thread would result in a total of
    256 bytes moving over the memory bus. The GTX460 we are using has a 256-bit-wide
    bus to the global memory. This would clearly indicate that, regardless of any
    occupancy considerations, on such devices you should always be processing either
    8 or 16 bytes (two or four elements) per thread. This is most likely due to the
    higher ratio of CUDA codes within the SM causing some contention for the single
    set of LSUs (load/store units).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程8字节的单次warp事务会导致总共256字节的数据通过内存总线传输。我们使用的GTX460显卡具有256位宽的总线连接到全局内存。这明显表明，无论占用率如何，在这种设备上，你应该始终每个线程处理8字节或16字节（两个或四个元素）。这很可能是由于SM中CUDA代码的比例较高，导致单个LSU（加载/存储单元）出现一些争用。
- en: The GTX260 for comparison, a compute 1.3 device similar to the Tesla C2050 device,
    gained, on average, 5% by moving from 4 to 8 bytes per element. However, its performance
    was drastically reduced when moving beyond this. The 9800 GT did not show any
    significant improvement, suggesting this device is already achieving the peak
    when using 4 bytes per element.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 与GTX260进行比较，该设备为计算能力1.3的设备，类似于Tesla C2050设备，移至每个元素8字节时，平均提高了5%。然而，当超过这个值时，其性能急剧下降。9800
    GT没有显示出任何显著的改善，这表明该设备在使用每个元素4字节时已经达到了峰值。
- en: Finally, note that Fermi-based Tesla devices implement an ECC (error checking
    and correction) based memory protocol. Disabling this can boost transfer speeds
    by around 10% at the expense of the error detection and correction ability. In
    a single machine versus a server room, this may be an acceptable tradeoff.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，基于Fermi架构的Tesla设备实现了基于ECC（错误检查和修正）的内存协议。禁用此功能可以将传输速度提高约10%，但代价是失去了错误检测和修正能力。在单台机器与服务器机房之间，这可能是一个可接受的折中。
- en: GPU timing
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU计时
- en: Single GPU timing
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单GPU计时
- en: Timing data on the GPU is not particularly straightforward. Using a timer that
    is CPU based is not a good solution, as the best way to use the GPU and CPU is
    to operate asynchronously. That is, both the GPU and CPU are running at the same
    time. CPU timing is only semi-accurate when you force sequential operation of
    the GPU and CPU. As this is not what we want in practice, it’s a poor solution.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 上的计时数据并不是特别直观。使用基于 CPU 的定时器并不是一个好的解决方案，因为最好的方式是让 GPU 和 CPU 异步操作。也就是说，GPU
    和 CPU 是同时运行的。当你强制执行 GPU 和 CPU 的顺序操作时，CPU 计时只会是半准确的。由于这不是我们在实际应用中想要的，因此这是一个较差的解决方案。
- en: The GPU, by default, operates in a synchronous mode in that the `memcpy` operations
    implicitly synchronize. The programmer expects to copy to the device, run the
    kernel, copy back from the device, and have the results in CPU memory to save
    to disk or for further processing. While this is an easy model to understand,
    it’s also a slow model. It’s one aimed at getting kernels to work, but not one
    aimed at performance.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GPU 以同步模式运行，`memcpy`操作会隐式地进行同步。程序员期望将数据复制到设备、运行内核、从设备复制回并将结果存储到 CPU 内存中，以便保存到磁盘或进行进一步处理。虽然这种模型易于理解，但它也是一种较慢的模型。它的目标是让内核能够工作，而不是优化性能。
- en: We examined the use of streams, in detail, in [Chapter 8](CHP008.html). A stream
    is effectively a work queue. Stream 0 is used as the default work queue when you
    do not specify a stream to the CUDA API. However, stream 0 has many operations
    that implicitly synchronize with the host. You might be expecting an asynchronous
    operation, but in practice certain API calls have implicit synchronization when
    using stream 0.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 8 章](CHP008.html)中详细讨论了流的使用。流实际上是一个工作队列。当你没有为 CUDA API 指定流时，流 0 被用作默认的工作队列。然而，流
    0 有许多操作会隐式地与主机同步。你可能期待一个异步操作，但在实践中，使用流 0 时某些 API 调用会有隐式同步。
- en: To use asynchronous operations, we need to first create a stream such as
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用异步操作，首先需要创建一个流，例如
- en: '[PRE74]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: For the bandwidth test, we created an array of events.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 对于带宽测试，我们创建了一个事件数组。
- en: '[PRE75]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The GPU provides events that can be time-stamped by the GPU hardware ([Figure
    9.24](#F0125)). Thus, to time a particular action on the GPU, you need to push
    a start event into the queue, then the action you wish to time, and finally a
    stop event. Streams are simply a FIFO (first in, first out) queue of operations
    for the GPU to perform. Each stream represents an independent queue of operations.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 提供了可以通过 GPU 硬件时间戳的事件（[图 9.24](#F0125)）。因此，要在 GPU 上计时某个特定操作，你需要将一个启动事件推送到队列中，然后是你想要计时的操作，最后是一个停止事件。流只是
    GPU 执行操作的 FIFO（先进先出）队列。每个流表示一个独立的操作队列。
- en: '![image](../images/F000090f09-24-9780124159334.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-24-9780124159334.jpg)'
- en: FIGURE 9.24 Timing an action on the GPU.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24 在 GPU 上计时一个操作。
- en: '![image](../images/F000090f09-25-9780124159334.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-25-9780124159334.jpg)'
- en: FIGURE 9.25 Multi-GPU timeline.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25 多 GPU 时间线。
- en: Having created a stream, you need to create one or more events.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了流之后，你需要创建一个或多个事件。
- en: '[PRE76]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Here we have a simple loop creating `MAX_NUM_TESTS` events—a start event and
    a stop event. We then need to push the events into the stream on either side of
    the action to measure.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个简单的循环，创建了`MAX_NUM_TESTS`个事件——一个启动事件和一个停止事件。然后，我们需要将这些事件推送到流中，放置在动作的两侧以进行计时。
- en: '[PRE77]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '`// Run the kernel`'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '`// 运行内核`'
- en: '[PRE78]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: To calculate the time, either per CUDA call or in total, call the CUDA function
    `cudaEventElapsedTime` to get the time difference between two time-stamped events.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算时间（无论是每个 CUDA 调用还是总计），调用 CUDA 函数`cudaEventElapsedTime`来获取两个时间戳事件之间的时间差。
- en: '[PRE80]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: You should realize that in performing such a timed event, there is no guarantee
    of ordering of events between streams. The CUDA runtime could execute your start
    event in stream 0 and then switch to a previously suspended kernel execution in
    stream 5, sometime later come back to stream 0, kick off the kernel, jump to another
    stream to process a number of other start events, and finally come back to stream
    0 and timestamp the stop event. The delta time is the time from the start period
    to the end period.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该意识到，在执行这样的定时事件时，不保证不同流之间事件的顺序。CUDA 运行时可能先在流 0 中执行你的启动事件，然后切换到流 5 中先前挂起的内核执行，过一段时间后再回到流
    0，启动内核，跳转到另一个流处理其他启动事件，最后回到流 0 并时间戳停止事件。时间差是从开始阶段到结束阶段的时间。
- en: In this example, notice we have created only a single stream. We have multiple
    events, but they all execute from the same stream. With only a single stream the
    runtime can only execute events in order, so we guarantee achieving the correct
    timing.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，请注意我们只创建了一个流。我们有多个事件，但它们都从同一个流中执行。由于只有一个流，运行时只能按顺序执行事件，因此我们可以确保正确的时序。
- en: Notice the call to the `cudaEventSynchronize` API. This call causes the CPU
    thread to block should it be called when the event has not completed. As we’re
    doing nothing useful on the CPU, this is perfectly fine for our purposes.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意调用了`cudaEventSynchronize` API。这个调用会导致CPU线程阻塞，如果事件尚未完成时调用它。由于我们在CPU上没有做任何有用的操作，这对我们的目的来说是完全没问题的。
- en: At the end of the host program we must ensure that with any resources we allocated
    are freed up.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机程序的末尾，我们必须确保释放我们分配的所有资源。
- en: '[PRE84]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Destroying an event before it’s actually been used will result in undefined
    runtime errors when executing the kernels.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件尚未被实际使用之前销毁它会导致执行内核时出现未定义的运行时错误。
- en: Finally, you should be aware that events are not free. It takes some resources
    to handle the events at runtime. In this example we specifically wanted to time
    each kernel to ensure there was not significant variability. In most cases a single
    start and stop event at the start and end of the work queue will be entirely sufficient
    for the overall timing.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该知道事件并不是免费的。处理事件需要一些运行时资源。在这个例子中，我们特别希望计时每个内核，以确保没有显著的变动。在大多数情况下，在工作队列的开始和结束处设置一个开始和结束事件就足够完成整体的计时。
- en: Multi GPU timing
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多GPU时序
- en: Multi GPU timing is a little more complex, but based on the same principles.
    Again, we create a number of streams and push events into the streams.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 多GPU时序稍微复杂一点，但基于相同的原理。我们再次创建多个流并将事件推送到这些流中。
- en: Unfortunately, there is no function provided in the API to obtain the absolute
    timestamp from an event. You can only obtain the delta between two events. However,
    by pushing an event into the start of the stream, you can use this as time point
    zero and thus obtain the time relative to the start of the stream. However, asking
    for the delta time between events on different GPUs causes the API to return an
    error. This complicates creating a timeline when using multiple GPUs, as you may
    need to adjust the time based on when the start events actually happened. We can
    see in [Figure 9.29](#F0150) a copy to the device, a kernel execution, a copy
    from the device, a copy to the device, a second kernel invocation, and finally
    a copy from the device.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，API中没有提供获取事件绝对时间戳的函数。你只能获取两个事件之间的时间差。然而，通过将一个事件推送到流的开始位置，你可以将其作为时间点零，从而获得相对于流开始的时间。然而，当请求不同GPU之间事件的时间差时，API会返回错误。这使得在使用多个GPU时创建时间线变得复杂，因为你可能需要根据实际发生的开始事件来调整时间。我们可以在[图9.29](#F0150)中看到：一次设备复制、一次内核执行、一次设备复制、一次设备复制、第二次内核调用，最后一次设备复制。
- en: Notice that with different devices, the copy times are largely similar but the
    kernels’ time will vary considerably. In the second-to-last copy from device operation
    for the GTX470 device (CFD 2), notice the bar is somewhat smaller (258 ms versus
    290 ms). This is because the GTX470 starts its transfer first and only toward
    the tail end of the transfer do the other devices also initiate a transfer. The
    GT9800, being a much slower device, still has its kernel being executed while
    GTX470 has in fact completed its transfer. With different device generations,
    you will get such a pattern. The transfer rates are largely similar, but the kernel
    times cause shifts in the points where the transfers are initiated.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然不同设备的复制时间大致相似，但内核的执行时间会有显著差异。在GTX470设备（CFD 2）进行倒数第二次从设备复制操作时，请注意条形图稍微小一些（258毫秒对比290毫秒）。这是因为GTX470首先开始传输，而其他设备只有在传输的尾端才开始传输。GT9800作为一个速度较慢的设备，尽管GTX470已经完成传输，它的内核仍然在执行。不同设备代际之间会出现这种模式。传输速率大致相似，但内核时间会导致传输开始的时间点发生变化。
- en: '[Figure 9.25](#F0130) was generated using timers, but tools such as Parallel
    Nsight and the Visual Profiler will draw the timeline for you automatically, along
    with the CPU timeline so you can clearly see what has happened and when.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.25](#F0130)是通过计时器生成的，但像Parallel Nsight和Visual Profiler这样的工具会自动为你绘制时间线，并且还会显示CPU的时间线，这样你可以清楚地看到何时发生了什么。'
- en: Note that it’s possible with `cudaEventQuery` API to simply query if the event
    has completed without causing a blocking call as with `cudaEventSynchronize`.
    Thus, the CPU can continue to do useful work, or simply move onto the next stream
    to see if it has completed yet.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过`cudaEventQuery` API，您可以仅查询事件是否完成，而不会像`cudaEventSynchronize`那样产生阻塞调用。因此，CPU可以继续做有用的工作，或者直接进入下一个流，查看它是否已经完成。
- en: '[PRE85]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: In this particular example, taken from another program, we have an array of
    events, `memcpy_to_stop`, indexed by device number and test number. We check if
    the event has completed by a call to `cudaEventQuery`, which returns `cudaSuccess`
    if the event has already completed. If so, we get the delta time between this
    event and the start event `memcpy_to_start` from the same device, but for test
    0, we get the start event for the whole kernel stream on that GPU. To obtain the
    delta time we simply call the `cudaEventElapsedTime` function.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定示例中，来自另一个程序，我们有一个事件数组`memcpy_to_stop`，按设备编号和测试编号索引。我们通过调用`cudaEventQuery`检查事件是否完成，如果事件已经完成，它会返回`cudaSuccess`。如果是这样，我们获取该事件与起始事件`memcpy_to_start`之间的时间差，该事件来自同一设备，但对于测试0，我们获取整个内核流在该GPU上的开始事件。为了获得时间差，我们只需调用`cudaEventElapsedTime`函数。
- en: Note as this will generate an error if the event has not yet completed, it is
    guarded by the check with `cudaEventQuery`. We could equally call `cudaEventSynchronize`
    if we simply wanted a blocking call that would wait for the event to complete.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果事件尚未完成，这将会产生一个错误，因此它通过`cudaEventQuery`进行检查。如果我们只是希望有一个阻塞调用等待事件完成，我们也可以调用`cudaEventSynchronize`。
- en: 'If we’re particularly interested in the absolute time, the GPU does provide
    access to the low-level timers with the help of some embedded PTX code:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们特别关心绝对时间，GPU确实可以通过一些嵌入的PTX代码提供对低级定时器的访问：
- en: '[PRE89]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: This section of code loads the raw clock value into a C variable that can then
    later be stored in a history buffer and transferred back to the host. The special
    `%clock` value is simply a 32-bit counter that wraps at max(u32). Compute 2.x
    hardware provides a 64-bit clock, thus allowing a wider time range over which
    values can be timed. Note, the CUDA API provides functions to access these register
    values through the use of the `clock` and `clock64` functions.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将原始时钟值加载到C变量中，稍后可以将其存储到历史缓冲区并传回主机。特殊的`%clock`值只是一个32位计数器，在最大值（u32）时会回绕。Compute
    2.x硬件提供了一个64位时钟，因此允许在更宽的时间范围内进行计时。请注意，CUDA API提供了通过`clock`和`clock64`函数访问这些寄存器值的功能。
- en: You can use this to measure the times of device functions within kernel calls
    or sections of code. Such measurements are not shown with either the Visual Profiler
    or Parallel Nsight, as their resolution onto the timing stops at the global-level
    kernel functions. You can also use this to store the times warps arrive at a barrier
    point. Simply create a store on a per-warp basis prior to a call to a barrier
    primitive such as `syncthreads`. You can then see the distribution of the warps
    to the synchronization point.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用这个来测量内核调用或代码段中设备函数的时间。这些测量不会显示在Visual Profiler或Parallel Nsight中，因为它们的时间分辨率仅限于全局级别的内核函数。你也可以用它来存储warps到达同步点的时间。只需在调用像`syncthreads`这样的屏障原语之前，按warp逐个存储。然后你可以看到warps到达同步点的分布情况。
- en: However, one very important caveat here is you must understand that a given
    warp in a kernel will not be running all the time. Thus, as with timing multiple
    streams, a warp may store a start time, get suspended, sometime later get resumed,
    and meet the next timer store event. The delta is only the overall real time difference,
    not the time the SM spent executing code from the given warp.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个非常重要的警告，你必须理解，内核中的一个warp并不会一直运行。因此，和计时多个流一样，一个warp可能会存储一个开始时间，然后被挂起，稍后恢复，并遇到下一个定时器存储事件。这个差值只是总体的实际时间差，而不是SM花费在执行该warp的代码上的时间。
- en: You should also realize that instrumenting code in this way may well affect
    its timing and execution order relative to other warps. You will be making global
    memory stores to later transfer this data back to the host where it can be analyzed.
    Consequently, your instrumentation impacts not only execution flow, but memory
    accesses. The effect of this can be minimized by running a single block of 32
    threads, that is, a single warp. However, this entirely discounts the quite necessary
    effects of running with other warps present on the SM and across multiple SMs
    within the GPU.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: Overlapping GPU transfers
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two strategies for trying to overlap transfer; first, to overlap transfer
    times with the compute time. We’ve look at this in detail in the last section,
    explicitly with the use of streams and implicitly with the use of zero-copy memory.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Streams are a very useful feature of GPU computing. By building independent
    work queues we can drive the GPU device in an asynchronous manner. That is, the
    CPU can push a number of work elements into a queue and then go off and do something
    else before having to service the GPU again.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: To some extent, operating the GPU synchronously with stream 0 is like polling
    a serial device with a single character buffer. Such devices were used in the
    original serial port implementations for devices like modems that operated over
    the RS232 interface. These are now obsolete and have been replaced with USB1,
    USB2, and USB3 interfaces. The original serial controller, a UART, would raise
    an interrupt request to the processor to say it had received enough bits to decode
    one character and its single character buffer was full. Only once the CPU serviced
    the interrupt could the communications continue. One character at a time communication
    was never very fast, and highly CPU intensive. Such devices were rapidly replaced
    with UARTs that had a 16-character buffer in them. Thus, the frequency of the
    device raising an interrupt to the CPU was reduced by a factor of 16\. It could
    process the incoming characters and accumulate them to create a reasonably sized
    transfer to the CPU’s memory.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: By creating a stream of work for the GPU we’re effectively doing something similar.
    Instead of the GPU working in a synchronous manner with the CPU, and the CPU having
    to poll the GPU all the time to find out if it’s ready, we just give it a chunk
    of work to be getting on with. We then only periodically have to check if it’s
    now out of work, and if so, push some more work into the stream or work queue.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: Through the CUDA stream interface we can also drive multiple GPU devices, providing
    you remember to switch the desired device before trying to access it. For asynchronous
    operation, pinned or page-locked memory is required for any transfers to and from
    the GPU.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: On a single-processor system, all the GPUs will be connected to a single PCI-E
    switch. The purpose of a PCI-E switch is to connect the various high-speed components
    to the PCI-E bus. It also functions as a means for PCI-E cards to talk to one
    another without having to go to host memory.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Although we may have multiple PCI-E devices, in the case of our test machine,
    four GPUs on four separate X8 PCI-E 2.0 links, they are still connected to a *single*
    PCI-E controller. In addition, depending on the implementation, this controller
    may actually be on the CPU itself. Thus, if we perform a set of transfers to multiple
    GPUs at any one point in time, although the individual bandwidth to each device
    may be in the order of 5 GB/s in each direction, can the PCI-E switch, the CPU,
    the memory, and other components work at that speed if all devices become active?
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: With four GPUs present on a system, what scaling can be expected? With our I7
    920 Nehalem system, we measured around 5 GB/s to a single card using a PCI-E 2.0
    X16 link. With the AMD system, we have around 2.5–3 GB/s on the PCI-E 2.0 X8 link.
    As the number of PCI-E lanes are half that of the I7 system, these sorts of numbers
    are around what you might expect to achieve.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: We modified the bandwidth test program we used earlier for measuring the PCI-E
    bandwidth to measure the bandwidth as we introduce more cards and more concurrent
    transfers. Any number of things can affect the transfers once we start introducing
    concurrent transfers to different GPUs. Anyone familiar with the multi-GPU scaling
    within the games industry will appreciate that simply inserting a second GPU does
    not guarantee twice the performance. Many benchmarks show that most commercial
    games benefit significantly from two GPU cards. Adding a third card often introduces
    some noticeable benefit, but nothing like the almost times two scaling that is
    often seen with a second card. Adding a fourth card will often cause the performance
    to drop.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Now this may not seem very intuitive, adding more hardware equals lower speed.
    However, it’s the same issue we see on CPUs when the core count becomes too high
    for the surrounding components. A typical high-end motherboard/CPU solution will
    dedicate at most 32 PCI-E lands to the PCI-E bus. This means only two cards can
    run at full X16 PCI-E 2.0 speed. Anything more than this is achieved by the use
    of PCI-E switch chips, which multiplex the PCI-E lines. This works well until
    the two cards on the PCI-E multiplexer both need to do a transfer at the same
    time.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: The AMD system we’ve run most of these tests in this book on does not use a
    multiplexer, but drops the speed of each connected GPU to an X8 link when four
    GPUs are present. Thus, at 2.5–3 GB/s per device, we could achieve a theoretical
    maximum of 10–12.5 GB/s. In addition, being an AMD solution, the PCI-E controller
    is built into the processor, which also sits between the PCI-E system and main
    memory. The bandwidth to main memory is approximately 12.5 GB/s. Therefore, you
    can see this system would be unlikely to achieve the full potential of four GPUs.
    See [Tables 9.2](#T0015) and [9.3](#T0020) and [Figures 9.26](#F0135) and [9.27](#F0140).
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.2 Bandwidth Effects of Multiple PCI-E Transfers to the Device
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0015.jpg)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
- en: Table 9.3 Bandwidth Effects of Multiple PCI-E Transfers from the Device
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0020.jpg)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000090f09-26-9780124159334.jpg)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.26 Multi-GPU PCI-E bandwidth to device AMD 905e Phenom II.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-27-9780124159334.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.27 Multi-GPU PCI-E bandwidth from device AMD 905e Phenom II.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: What you can see from [Tables 9.2](#T0015) and [9.3](#T0020) is that transfers
    scale quite nicely to three GPUs. We’re seeing approximately linear scaling. However,
    when the four GPUs compete for the available resources (CPU, memory bandwidth,
    and PCI-E switch bandwidth) the overall rate is slower.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: The other multi-GPU platform we have to work with is a six-GPU system based
    on the Nehalem I7 platform and the ASUS supercomputer motherboard (P6T7WS) with
    3 GTX295 Dual GPU cards. This uses dual NF200 PCI-E switch chips allowing each
    PCI-E card to work with a full X16 link. While this might be useful for inter-GPU
    communication, the P2P (peer-to-peer) model supported in CUDA 4.x, it does not
    extend the bandwidth available to and from the host if both cards are simultaneously
    using the bus. We are using GTX290 cards, which are a dual-GPU device. Internally,
    each GPU has to share the X16 PCI-E 2.0 link. [Table 9.4](#T0025) and [Figure
    9.28](#F0145) show what effect this has.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.4 I7 Bandwidth to Device
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0025.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000090f09-28-9780124159334.jpg)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.28 I7 bandwidth to device.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from [Table 9.4](#T0025), we see an approximate linear increase
    in total bandwidth to the device. We achieve a peak of just over 10 GB/s, 20%
    or so higher than our AMD-based system.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: We can see the bandwidth from the device is a different story ([Table 9.5](#T0030)
    and [Figure 9.29](#F0150)). Bandwidth peaks with two devices, and is not significantly
    higher than our AMD system. This is not altogether unexpected if you consider
    the design in most GPU systems is based around gaming. In a game, most of the
    data is being sent *to* the GPU with very little if any coming back to the CPU
    host. Thus, we see a near linear scaling of up to three cards, which coincides
    with the top-end triple SLI (scalable link interface) gaming platforms. Vendors
    have little incentive to provide PCI-E bandwidth beyond this setup. As the GTX290
    is actually a dual-GPU card, we may also be seeing that the internal SLI interface
    is not really able to push the limits of the card. We’re clearly seeing some resource
    contention.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.5 I7 Bandwidth from Device
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0030.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
- en: Section summary
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Understand and plan for the fact you will have limited PCI-E bandwidth capability.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: • Always use pinned memory where possible.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: • Use transfer sizes of at least 2 MB.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: • Understand the use of zero-copy memory as an alternative to the stream API.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: • Think about how to overlap transfer time with kernel execution time.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: • Do not expect a linear scaling of bandwidth when using multiple GPUs.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 4: Thread Usage, Calculations, and Divergence'
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thread memory patterns
  id: totrans-505
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Breaking down the application into *suitably* sized grids, blocks, and threads
    is often one of the key aspects of performance of CUDA kernels. Memory is the
    bottleneck in almost any computer design, the GPU included. A bad choice of thread
    layout typically also leads to a bad memory pattern, which will significantly
    harm performance.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: Consider the first example, a 2 × 32 layout of threads ([Figure 9.30](#F0155))
    versus a 32 × 2 layout of threads. Think about how they would typically overlay
    memory if they were processing floating-point values. In the 2 × 32 example, thread
    0 cannot be coalesced with any other thread than thread 1\. In this case the hardware
    issues a total of 16 memory fetches. The warp cannot progress until at least the
    first half-warp has acquired all the data it needs. Therefore, at least eight
    of these very long memory transactions need to complete prior to any compute activity
    on the SM. As most warps will be following the same pattern, the SM will be swamped
    with issuing memory requests while the compute part of the SM is almost idle.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-29-9780124159334.jpg)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.29 I7 bandwidth from device.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-30-9780124159334.jpg)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.30 2 × 32 thread grid layout.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: We saw from the bandwidth analysis in the previous section that there is a limit
    to the number of memory requests the SM can push out from the warps. The SM services
    the data request for any single warp over two clock cycles. In our example, the
    request has to be broken into 16 × 8 byte memory transactions.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: On Fermi, the first of these would cause a read miss in the L1 cache. The L1
    cache would request the minimum size of data possible, 128 bytes from the L2 cache,
    and some 16 times more data than the thread needs. Thus, when data is moved from
    the L2 cache to the L1 cache, just 3.125% of the data moved is consumed by thread
    0\. As thread 1 also wants the adjacent address, we can increase this to 6.25%,
    which is still terrible.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: On the first run through the code the L2 cache is unlikely to contain the data.
    It issues a 128-byte fetch also to slow global memory. This latency-expensive
    operation is finally performed and 128 bytes arrive at the L2 cache.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: The L2 cache is 768 K in size on a 16 SM device. Assuming we’re using a GTX580,
    we have 16 SMs. That is just 48 KB per SM, the maximum size of the L1 cache. Using
    128-byte cache lines we have just 384 entries in the cache per SM. If we assume
    the SM is fully loaded with 48 warps (Kepler supports 64), each warp will issue
    16 separate reads, which is 768 reads in total. This means we’d need 768 cache
    lines, not the 384 we have, just to cache the data needed so each warp can hold
    a single block in memory.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: The cache is effectively far too small to be used for temporal locality in this
    example. By temporal locality we mean we expect the data to remain in the cache
    from one read to the next. Halfway through processing the warps in each SM, the
    cache is full and the hardware starts filling it with new data. Consequently,
    there is absolutely no data reuse with the L2 cache, but a significant overhead
    in having to fetch entire cache lines. In fact, the only saving grace is that
    Fermi, unlike previous generations, will now forward the data it fetched to the
    other thread in our example.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: The cache model is one that can cause problems in that it allows people to think
    the hardware will save them from poor programming. Let’s assume for a moment we
    have to use this thread pattern and we would have processed the element we fetched
    from memory a number of times. The thread pattern for fetching data does not have
    to be the same thread pattern for using the data. We can fetch data into shared
    memory in a 32 × 2 pattern, synchronize the threads, and then switch to a 2 ×
    32 usage pattern if we wish. Despite the shared memory bank conflicts this would
    then incur, it would still be an order of magnitude faster than doing the global
    memory fetches. We can also simply add a padding element to the shared memory
    by declaring it as 33 × 2 to ensure when we access it, these bank conflicts are
    removed.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: Consider for a moment the difference in handling of the memory system. We issue
    1 coalesced read for 128 bytes instead of 16 separate reads. There’s a factor
    of 16:1 improvement in both the number of memory transactions in flight and also
    bandwidth usage. Data can be moved from the L2 to the L1 cache in just one transaction,
    not 16.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: The LSUs in the SM have to issue only a single fetch transaction instead of
    16 separate fetches, taking just 2 clock cycles instead of 32 and freeing up the
    LSUs for other tasks from other warps.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: Each warp consumes a single cache line, 48 maximum per SM. Thus, of the 384
    cache lines we have per SM in the L2 cache, we’re using only 100, just 12.5% of
    the L2 cache instead of 200%. Thus, it’s absolutely critical that to get anywhere
    near the full performance, even in Fermi with its multilevel caches, you have
    to fetch data in coalesced blocks of 128 bytes across a warp.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: Now we could configure the L2 cache to fetch only 32 bytes instead of 128 bytes
    using the `-Xptxas -dlcm=cg` compiler flag. However, this also disables global
    memory storage in the L1 cache. It’s an easy fix but a poor solution to the fact
    that you are not fetching data in large enough blocks from global memory. To get
    the best performance from a given device, you need to understand what’s going
    on down inside or use libraries that are coded by someone who does.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: We can see this quite clearly with the effects on memory bandwidth with Parallel
    Nsight if you select “Custom” experiment and then add in the L1 and L2 cache counters.
    The particular counters we are interested in are shown in [Table 9.6](#T0035).
    These can be set up in Parallel Nsight using the “Custom” experiment, shown in
    [Figure 9.31](#F0160).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.6 Parallel Nsight Cache Counters
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '| Nsight Counter | Usage |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: '| L1 global load hits | The number of global memory load requests met by the
    L1 cache. |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
- en: '| L1 global load misses | The number of global memory load requests not met
    by the L1 cache. |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 0 read section misses | Half the number of L2 misses. |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 1 read section misses | The other half of the number of L2
    misses. |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 0 read section queries | Half the number of L2 access attempts.
    |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 1 read section queries | The other half of the number of
    L2 access attempts. |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '![image](../images/F000090f09-31-9780124159334.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.31 Setting up Parallel Nsight to capture cache statistics.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: From these counters we can manually work out the L1 and L2 cache hit ratio.
    The hit ratio is the percentage of reads (or writes) that we cached. Every cached
    access saves us several hundreds of cycles of global memory latency.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: When we look at the results for the sample sort algorithm in [Table 9.7](#T0040),
    we can instantly see the L2 cache hit ratio drops off sharply as soon as the kernel
    exceeds 64 threads. Occupancy increases, but performance drops off. This is not
    at all surprising given the usage of the L2 cache a prefix sum array will generate.
    If each thread is processing one bin, as we extend the number of threads, the
    size of the memory area being cached increases. As soon as it exceeds the L2 cache
    size the hit ratio rapidly drops off.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.7 Cache Hit Ratio for Sample Sort
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0040.jpg)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
- en: The solution to the problem is to replace the existing algorithm that uses one
    thread per bin with one where the threads all work on a single bin at a time.
    This way we’d achieve coalesced memory accesses on each iteration and significantly
    better locality of memory access. An alternative solution would be to use shared
    memory to handle the transition between noncoalesced access by the threads and
    the necessary coalesced access when reading or writing to global memory.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: Inactive threads
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Threads, despite there being many thousands of them, are not free, even if they
    are inactive. The problem with inactive threads is twofold. First, a warp will
    remain active, scheduled, and using resources if just one of its threads is active.
    There are a limited number of warps that can be dispatched in a dispatch period
    (two clock cycles). This is two on compute 2.0 hardware, four on compute 2.1 hardware
    and eight in compute 3.x hardware. There is no point in the hardware dispatching
    a warp with a single thread to a set of CUDA cores and having it use just a single
    CUDA core while the other 15 idle. However, this is exactly what the hardware
    has to do if there is divergence of execution flow within a warp down to just
    one thread being active.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: You sometimes see a parallel reduction–type operation that has been written
    by a programmer who does not understand the hardware well. They will perform the
    reduction operation within every warp, going from 32 to 16, to 8, to 4, to 2,
    and finally to 1 active thread. Regardless of whether you use 32 threads or 1
    thread the hardware still allocates 32 and simply masks out the inactive ones.
    Because the warps are still active, even if they have only one thread active,
    they still need to be scheduled onto the hardware.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: A much better approach to this is to have all 32 threads in every block compute
    a set of partial results. Let’s use the sum operation, as it’s easy to understand.
    With 32 threads per warp, you can compute 64 additions in one cycle. Now have
    each thread store its value into shared memory. Thus, the first warp stores to
    element 0..31, the second to 32..63, etc. Now divide *N*, the number of elements
    of the reduction, by 2\. Repeat the reduction using the threshold `if (threadIdx.x
    < (N/2))` until such time as *N* equals 2.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..255 read values 0..511 (eight active warps).
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..127 read values 0..256 (four active warps).
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..63 read values 0..127 (two active warps).
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..31 read values 0..63 (one active warp).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..15 read values 0..31 (half an active warp).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: Etc.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: Warps with thread numbers greater than the threshold simply no longer get scheduled.
    The warps with values less than *N* are fully populated with work, until such
    time as *N* equals some value less than 32\. At this point we can simply do an
    addition or all remaining elements, or continue to iterate toward the final addition.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: Inactive warps are not in themselves free either. Although the SM internally
    cares about warps, not blocks, the external scheduler can *only schedule blocks*
    into an SM, not warps. Thus, if each block contains only one active warp, we can
    have as little as 6 to 8 warps for the SM to select from for scheduling. Usually
    we’d have up to 64 warps active in an SM, depending on the compute version and
    resource usage. This is a problem because the thread-level parallelism (TLP) model
    relies on having lots of threads to hide memory and instruction latency. As the
    number of active warps drops, the ability of the SM to hide latency using TLP
    also dramatically drops. As some point this will hurt the performance, especially
    if the warp is still making global memory accesses.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, at the last levels of such a reduction-type operation, or any operation
    where progressively larger numbers of warps will drop out, we need to introduce
    some instruction level parallelism (ILP). We want to terminate the last warp as
    soon as possible so the entire block can be retired and replaced with another
    block that will likely have a fresh set of active warps.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: We look at reduction in detail later in this chapter.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: Arithmetic density
  id: totrans-553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Arithmetic density is a term that measures the relative number of calculations
    per memory fetch. Thus, a kernel that fetches two values from memory, multiplies
    them, and stores the result back to memory has very low arithmetic density.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: The fetch and store operations may well involve some index calculations. The
    real work being done is the multiplication. However, with only one operation being
    performed per three memory transactions (two reads and one write), the kernel
    is very much memory bound.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: The total execution time is
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090si1.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
- en: or
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090si2.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
- en: Notice we use here *A* + *B* as opposed to multiplying *A*, the single memory
    fetch time, by 2\. The individual read times are not easy to predict. In fact
    neither *A*, *B*, or *C* are constant, as they are affected by the loads other
    SMs are making on the memory subsystem. Fetching of *A* may also bring into the
    cache *B*, so the access time for *B* is considerably less than *A*. Writing *C*
    may evict from the cache *A* or *B*. Changes to the resident lines in the L2 cache
    may be the result of the activity of an entirely different SM. Thus, we can see
    caching makes timing very unpredictable.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: When looking at the arithmetic density, our goal is to increase the ratio of
    useful work done relative to memory fetches and other overhead operations. However,
    we have to consider what we define as a memory fetch. Clearly, a fetch from global
    memory would qualify for this, but what about a shared memory, or cache fetch?
    As the processor must physically move data from shared memory to a register to
    operate on it, we must consider this also as a memory operation. If the data comes
    from the L1, L2, or constant cache, it too has to be moved to a register before
    we can operate on it.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: However, in the case of a shared memory or L1 cache access, the cost of such
    operations is reduced by an order of magnitude compared to global memory accesses.
    Thus, a global memory fetch should be weighted at 10× if a shared memory fetch
    equates to 1×.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do we increase the arithmetic density of such instruction flows? First,
    we have to understand the underlying instruction set. The maximum operand size
    of an instruction is 128 bytes, a four-element vector load/store operation. This
    tells us the ideal chunk size for our data is four elements, assuming we’re using
    floats or integers, two if we’re using doubles. Thus, our operation should be
    in the first instance:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: I’ve written this in long-hand form to make the operations clear. If you extend
    the vector-type class yourself and provide a multiplication operator that performs
    this expanded code, you can simply write
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Unfortunately, the GPU hardware currently doesn’t support such vector manipulations,
    only loads, stores, moves, and pack/unpack from scalar types.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: With such vector-based operations, we amortize the cost of the associated operations
    (load `A`, load `B`, write `C`, calculate `idx_A`, calculate `idx_B`, calculate
    `idx_C`) over four multiplies instead of one. The load and store operations take
    marginally longer as we have to introduce a pack and unpack operation that was
    not needed when accessing scalar parameters. We reduce the loop iterations by
    a factor of four with a consequential drop in the number of memory requests, issuing
    a much smaller number of larger requests to the memory system. This vastly improves
    performance (~20%), as we have seen with some examples in this book.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: Transcendental operations
  id: totrans-570
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The GPU hardware is aimed at speeding up gaming environments. Often these require
    the manipulation of hundreds of thousands of polygons, modeling the real world
    in some way. There are certain accelerators built into the GPU hardware. These
    are dedicated sections of hardware designed for a single purpose. GPUs have the
    following such accelerators:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: • Division
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: • Square root
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: • Reciprocal square root
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: • Sine
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: • Cosine
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: • Log²
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: • Base 2 exponent Ex²
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: These various instructions perform operations to 24-bit accuracy, in line with
    the typical 24-bit RGB setup used in many game environments. None of these operations
    are enabled by default. Compute 1.x devices take various shortcuts that make single-precision
    math not IEEE 754 compliant. These will not be relevant to many applications,
    but be aware they are there. Fermi (compute 2.x) hardware brings IEEE compliance
    with regard to floating-point operations by default.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like the faster but less precise operation, you have to enable them
    using either the compile switch (`-use_fast_math`) or explicitly using intrinsic
    operations. The first step is simply to enable the option in the compiler and
    check the outcome of your existing application. The answer will be different,
    but by how much and how important this is, are the key questions. In the gaming
    industry it doesn’t matter if the flying globe projectile is one pixel off to
    the left or right of the target—no one will notice. In compute applications it
    can make a very real difference.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: Individual operations can also be selectively enabled in 24-bit math using an
    explicit compiler intrinsic such as `__logf(x)`, etc. For a complete list of these
    and an explanation of the drawbacks of using them, see Appendix C.2 of the CUDA
    C programming guide. They can considerably speed up your kernels so it’s worth
    investigating if this is an option for your particular code.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: Approximation
  id: totrans-582
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Approximation is a useful technique in problems that explore a certain search
    space. Double-precision math is particularly expensive, in the order of at least
    twice as slow as floating-point math. Single-precision math uses 24 bits for the
    mantissa and 8 bits for the exponent. Thus, in the compute 1.x devices a fast
    24-bit integer approximation could be used to provide an additional computation
    path to the single- and double-precision math. Note in Fermi, the 24-bit native
    integer support was replaced with 32-bit integer support, so an integer approximation
    in 24-bit math is actually slower than if the same approximation was made in 32-bit
    math.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: In all compute hardware versions that natively support double precision (compute
    onwards), approximation in single precision is at least twice the speed of double-precision
    math. Sometimes a much higher speedup can be achieved because the single-precision
    calculations require less registers and thus potentially more blocks can be loaded
    into the hardware. Memory fetches are also half the size, doubling the effective
    per-element memory bandwidth. Consumer-based GPUs also have less double-precision
    units enabled in the hardware than their Tesla counterparts, making single-precision
    approximation a far more attractive proposition for such hardware.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, with approximating you are performing a tradeoff between speed and
    accuracy and introducing additional complexity into the program. Often this is
    a tradeoff worth exploring, for it can bring a significant speedup.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: Once we have done the approximation, the kernel can test the result to see if
    it is within a certain range or meets some criteria by which further analysis
    is warranted. For this subset of the dataset, the single- or double-precision
    calculation is performed as necessary.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: The initial pass simply acts as a filter on the data. For every data point that
    falls outside the criteria of interest, you have saved the expensive double-precision
    calculations. For every point that falls into it, you have added an additional
    24- or 32-bit filtering calculation. Thus, the benefit of this approach depends
    on the relative cost of the additional filtering calculation versus the cost of
    double-precision math required for the full calculation. If the filters remove
    90% of the double-precision calculations, you have a huge speedup. However, if
    90% of the calculations require a further double-precision calculation, then this
    strategy is not useful.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA claims Tesla Fermi has in the order of 8× faster double-precision math
    over the previous compute 1.3 implementations (GT200 series). However, consumer-level
    Fermi cards are artificially restricted to one-quarter the double-precision performance
    of Tesla cards. Therefore, if double precision is key to your application, clearly
    a Tesla is the easy-fix solution to the problem. However, some may prefer the
    alternative of using multiple consumer GPUs. Two 3 GB 580 GTXs would likely provide
    a faster solution than a single Fermi Tesla for considerably less money.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: If double precision is secondary or you simply wish to prototype a solution
    on commonly available hardware, then single precision of 24-bit filtered may be
    an attractive solution to this issue. Alternatively, if you have a mixture of
    GPUs, with an older card that is still good for single-precision usage, you can
    use the older card to scan the problem space for interesting sections, and the
    second card to investigate problem space in detail based on the likely candidates
    from the first card’s quick evaluation. Of course with a suitable Tesla card,
    you can perform both passes with just a single card.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: Lookup tables
  id: totrans-590
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One common optimization technique used for complex algorithms is a lookup table.
    On CPUs where computation is quite expensive, these generally work reasonably
    well. The principle is that you calculate a number of representative points in
    the data space. You then apply an interpolation method between points based on
    the proportional distance to either edge point. This is typically used in modeling
    of the real world in that a linear interpolation method with a sufficient number
    of key sample points provides a good approximation of the actual signal.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: A variation on this technique is used in brute-force attacks on ciphers. Passwords
    on most systems are stored as hashes, an apparently unintelligible series of digits.
    Hashes are designed so that it’s difficult to calculate the password from the
    hash by reversing the calculation. Otherwise, it would be trivial to calculate
    the original password based on a compromised hash table.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: One method of attack on this type of system involves a CPU spending a considerable
    time generating all possible permutations based on the use of common and/or short
    passwords. The attacker then simply matches the precomputer hash against the target
    hash until such time as a match is made.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the lookup table method trades memory space for compute time.
    By simply storing the result, you have instant access to the answer. Many people
    will have learned multiplication tables in their heads as children. It’s the same
    principle; instead of tediously calculating *a* × *b*, for the most common set
    of values, we simply memorize the result.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: This optimization technique works well on CPUs, especially older ones, where
    the compute time may be significant. However, as the compute resources have become
    faster and faster, it can be cheaper to calculate the results than to look them
    up from memory.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: If you consider the average arithmetic instruction latency will be between 18
    to 24 cycles and the average memory fetch in the order of 400 to 600 cycles, you
    can clearly see we can do a lot of calculation work in the time it takes for the
    memory fetch to come back from global memory. This, however, assumes we have to
    go out to global memory for the result and that it’s not stored in shared memory
    or the cache. It also does not consider that the GPU, unlike the CPU, will not
    idle during this memory fetch time. In fact, the GPU will likely have switched
    to another thread and be performing some other operation. This, of course, depends
    on the number of available warps you have scheduled onto the device.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: In many cases the lookup may win over the calculation, especially where you
    are achieving a high level of GPU utilization. Where you have low utilization,
    the calculation method often wins out, depending of course on how complex the
    calculation really is. Let’s assume we have 20-cycle instruction latency for arithmetic
    operations and 600-cycle latency for memory operations. Clearly, if the calculation
    takes less than 30 operations it would be much faster than lookup in memory when
    we have low GPU utilization. In this case the SM is behaving like a serial processor,
    in that it has to wait for the memory fetch. With a reasonable utilization the
    memory fetch effectively becomes free, as the SM is simply executing other warps.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: It’s often a case of trying this and seeing how well it works. Also be prepared
    to take it back out again should you suddenly manage to increase utilization of
    the GPU through other means.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: Note, in the case of linear interpolation, a low-precision floating point–based
    linear interpolation is available in the GPU hardware. This is a feature of the
    texture memory hardware, something we do not cover in this text. Texture memory
    was useful for its cache features (24 K per SM) in compute 1.x hardware, but this
    use has largely been made redundant by the L1/L2 cache introduced in Fermi. However,
    the linear interpolation in hardware may still be useful for some problems. See
    the “Texture and Surface Memory” chapter of the CUDA programming guide if this
    is of interest to you.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: Some common compiler optimizations
  id: totrans-600
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll take a quick look at some compiler optimizations and how they affect GPUs.
    We cover these here to highlight cases where the optimizer may struggle and also
    to give you some understanding of how optimizations may be applied at the source
    code level where the automatic optimizations fail.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: Some compilers are well known for producing efficient code on certain targets.
    Not surprisingly, the Intel ICC compiler produces extremely efficient code for
    the Intel platform. New features of the processor are incorporated rapidly to
    showcase the technology. Mainstream compilers often come from a code base that
    supports many targets. This allows for more efficient development, but means the
    compiler may not be so easy to customize for a single target.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: As of the 4.1 SDK CUDA moved from using an Open64-based compiler to a more modern
    LLVM-based compiler. The most significant benefit from the user perspective is
    significantly faster compile times. NVIDIA also claims a 10% improvement in code
    speed. We saw noticeable improvements in code generation with this move. However,
    as with any new technology, there is room for improvement and I’m sure this will
    happen over time.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: The optimizations compilers apply are well documented. What we present here
    is a broad overview of some common ones. For most programmers, simply setting
    the optimization level is entirely sufficient. Others prefer to know what exactly
    is going on and check the output. This is of course a tradeoff of your programming
    time versus the potential gain and the relative costs of these.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: Strength reduction
  id: totrans-605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When accessing an array index, typically nonoptimized compiler code will use
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: This can be more efficiently replaced by one of two techniques. First, we must
    load the array base address (element 0) into a base register. Then we have the
    option of accessing an index as base + offset. We can also simply add a fixed
    offset, the size of an array element in bytes, to the base register after each
    loop iteration.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: In terms of C this is the same as writing
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: In terms of GPU usage, this optimization relies on the fact that certain instructions
    (multiply, divide) are computationally expensive and others (addition) are cheaper.
    It tries to replace the expensive operations with cheaper (or faster) ones. This
    technique works well on CPUs as well as on GPUs. This is especially the case with
    compute 2.1 devices where integer addition has three times the throughput of integer
    multiplication.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: Notice also that the pointer version of the code creates a dependency between
    loop iterations. The value of `ptr` must be known to execute the assignment. The
    first example is much easier to parallelize because there is no dependency on
    the loop iteration and the address of `a[i]` can easily be statically calculated.
    In fact, simply adding the `#pragma unroll` directive would have caused the compiler
    to unroll the entire loop, as the boundaries in this simple example are literals.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: It’s a typical example of a CPU-based optimization that may have been applied
    and to parallelize the loop you need to reverse-engineer back to the original
    code. It’s shown here because it helps you understand how C code may have been
    changed in the past to provide faster execution time for a given target. Like
    most optimizations at the C source code level, it can lead to the purpose of the
    source code being obscured.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: Loop invariant analysis
  id: totrans-619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loop invariant analysis looks for expressions that are constant within the loop
    body and moves them outside the loop body. Thus, for example,
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'In this example, the parameter `j` is constant within the loop body for parameter
    `i`. Thus, the compiler can easily detect this and will move the calculation of
    `b` outside the inner loop and generate the following code:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'This optimized code removes thousands of unnecessary calculations of `b`, where
    `j`, and thus `b`, are constant in the inner loop. However, consider the case
    where `b` is an external to the function, a global variable, instead of a local
    variable. For example:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '`}`'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: The compiler cannot safely make this optimization because the write to `q` may
    affect `b`. That is, the memory space of `q` and `b` may intersect. It cannot
    even safely reuse the result of `j ∗ 200` in the assignment to `q`, but must reload
    it from memory, as the contents of `b` may have changed since the assignment in
    the prior line.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: If you consider each line individually, then the issue becomes somewhat clearer.
    Any memory transaction, a read or write, will likely cause a switch to another
    warp, if that transaction involves accessing anything that is not immediately
    available. That area of global memory is accessible to any thread in any warp,
    on any active block in any SM. From one instruction to the next you get the very
    real possibility that any writable non register data could have changed.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: You might say, well I’ve split up the application into *N* tiles and the tiles
    do not intersect, so this is not necessary. As the programmer you may know this,
    but it is very difficult for the compiler to figure that out. Consequently, it
    opts for the safe route and does not perform such optimizations. Many programmers
    do not understand what the optimization stage of a compiler does, and thus when
    it does something that breaks the code, they blame the compiler. Consequently,
    compilers tend to be rather conservative in how they optimize code.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: As the programmer, understanding this allows you to make such optimization at
    the source level. Remember to think of global memory as you might a slow I/O device.
    Read from it once and reuse the data.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: Loop unrolling
  id: totrans-633
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Loop unrolling is a technique that seeks to ensure you do a reasonable number
    of data operations for the overhead of running through a loop. Take the following
    code:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'In terms of assembly code, this will generate:'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: • A load of a register with 0 for parameter `i`.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: • A test of the register with 100.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: • A branch to either exit or execute the loop.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: • An increment of the register holding the loop counter.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: • An address calculation of array `q` indexed by `i`.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: • A store of `i` to the calculated address.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: Only the last of these instructions actually does some *real* work. The rest
    of the instructions are overhead.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: We can rewrite this C code as
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '`}`'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the ratio of useful work to overhead of using the loop is much increased.
    However, the size of the C source code is somewhat increased and it’s now less
    obvious what exactly it was doing compared to the first loop.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: In terms of PTX code, we see each C statement translated into PTX. For every
    branch test, there are now four memory copy operations. Thus, the GPU is executing
    more instructions than before, but a higher percentage of the memory copy operations
    are doing useful work.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: In the CPU domain often there are limited registers, so the same registers will
    be reused in each step. This reduces register overhead, but means `q[i+1]` cannot
    start processing until `q[i]` has completed. We’d see the same overhead on the
    GPU with this approach. Each instruction has 20 cycles of latency. Therefore,
    the GPU assigns each address calculation to a separate register, so we have a
    set of four parallel instructions, rather than four sequential instructions executing.
    Each set is pushed into the pipelines and thus comes out one after another almost
    back to back.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: With this approach the limit is the number of registers. As the GPU has 64 (compute
    2.x,3.0) and 128 (compute 1.x) maximum, there is considerable scope for unrolling
    small loop bodies and achieving a good speedup.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: The NVCC compiler supports the `#pragma unroll` directive, which will automatically
    unroll fully such loops when the iteration count is constant or silently do nothing
    when it’s not. The latter is less than helpful, if the programmer has specified
    the loop should be unrolled. If the compiler is not able to, it should complain
    about this until the code is amended or the pragma removed.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify `#pragma unroll 4` where four is replaced by any number
    the programmer wishes. Typically four or eight will work well, but beyond that
    too many registers will be used and this will result in register spilling. On
    compute 1.x hardware, this will cause a huge performance drop as registers are
    spilled to global memory. From compute 2.x hardware onwards, registers are spilled
    to the L1 cache and then to global memory if necessary. The best solution is to
    try it and see which value works best for each loop.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: Loop peeling
  id: totrans-653
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loop peeling is an enhancement to the loop unrolling, when the number of iterations
    is not an exact multiple of the loop unrolling size. Here the last few iterations
    are peeled away and done separately, and then the main body of the loop is unrolled.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have 101 loop iterations and plan to use four levels of loop
    unrolling, the first 100 iterations of the loop are unrolled and the final iteration
    is peeled away to allow the bulk of the code to operate on the unrolled code.
    The final few iterations are then handled as either a loop or explicitly.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: Loop peeling can be equally applied to the start of a loop as to the end. It
    can be used in such cases to allow a nonaligned structure to be accessed as an
    aligned structure. For example, copying a byte-aligned memory section to another
    byte-aligned memory is slow because it has to be done one byte at a time. The
    first few iterations can be peeled away such that a 32-, 64-, or 128-byte alignment
    is achieved. Then the loop can switch to much faster word, double-, or quad-word
    based copies. At the end of the loop the byte-based copies can be used again.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: When using the `#pragma loop unroll N` directive, the compiler will unroll the
    loop such that the number of iterations does not exceed the loop boundaries and
    insert the end of loop peeling code automatically.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: Peephole optimization
  id: totrans-658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This optimization simple searches for combinations of instructions that can
    be replaced by more complex instructions with the same functionality. The classic
    example of this is multiply followed by an add instruction, as you might see in
    a gain and offset type calculation. This type of construct can be replaced with
    the more complex `madd` (multiply and add) instruction, reducing the number of
    instructions from two to one.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: Other types of peephole optimizations include simplification of flow of control,
    algebraic simplifications, and removal of unreachable code.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: Common subexpressions and folding
  id: totrans-661
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many programmers write code that repeats some operation, for example,
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: or
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: In the first example, arrays `b` and `c` are indexed by the `base` and `i` parameters.
    Providing these parameters are within local scope, the compiler can simply calculate
    the index (`base + i`), and add this value to the start address of arrays `b`
    and `c` and to the work address for each parameter. However, if either of the
    index parameters are global variables, then the calculation must be repeated,
    since either could have changed once multiple threads are used. With a single
    thread it would be safe to eliminate the second calculation. With multiple threads
    it may also be safe to do so, but the compiler doesn’t know for sure, so will
    typically perform two calculations.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, the term `NUM_ELEMENTS-1` is repeated. If we assume that
    `NUM_ELEMENTS` is a define, then the preprocessor will substitute the actual value,
    so we get `b[1024-1] ∗ c[1024-1]`. Clearly, 1024 − 1 can in both instances be
    replaced by 1023\. However, if `NUM_ELEMENTS` was actually a formal parameter,
    as it is in many kernel calls, this type of optimization is not available. In
    this case we have to drop back to common subexpression optimization.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, be aware that in making such constants parameters of a function,
    or by having such parameters in global memory, you may be limiting the compiler’s
    ability to optimize the code. You then have to ensure such common subexpressions
    are not present in the source code. Often eliminating the common subexpressions
    makes the code simpler to understand and improves the performance.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: Divergence
  id: totrans-669
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPUs execute code in blocks, or warps. A single instruction is decoded once
    and dispatched to a warp scheduler. There it remains in a queue until the warp
    dispatcher dispatches it to a set of 32 execution units, which execute that instruction.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: This approach amortizes the instruction fetch and decoding time over *N* execution
    units. This in itself is very similar to the old vector machines. However, the
    main difference is that CUDA does not require that every instruction execute in
    this way. If there is a branch in the code and only some instructions follow this
    branch, those instructions diverge while the others wait at the point of divergence.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: The single fetch/decode logic then fetches the instruction stream for the divergent
    threads and the other threads simply ignore it. In effect, each thread within
    the warp has a mask that enables its execution or not. Those threads not following
    the divergence have the mask cleared. Conversely, those following the branch have
    the bit set.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: This type of arrangement is called predication. A predicate is created, which
    results in a single bit being set for those threads within a warp that follow
    the branch. Most PTX op-codes support an optional predicate allowing selective
    threads to execute an instruction.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, for example, consider the following code:'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: In the first line of code the program eliminates all other warps in the current
    block except the first warp, the first 32 threads. This does not result in any
    divergence within the warp. The other warps in the block are simply not scheduled
    for this section of the code. They do not stall, but fall through the code and
    continue the execution of subsequent code.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: The first warp then meets a test for `threadIdx.x < 16`, which splits the warp
    exactly in half. This is a special scenario where the warp does not actually diverge.
    Although the warp size is 32, the divergence criteria are actually a half-warp.
    If you noticed earlier, the CUDA cores are arranged in banks of 16 cores, not
    32 cores. The scheduler issues instructions to two or more sets of 16 cores per
    cycle. Thus both the true and false path of the conditional are executed.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent step, threads 16 to 31 call the function `func_b`. However,
    threads 0..15 hit another conditional. This time it’s not half-warp based, but
    quarter-warp based. The minimum scheduling quantity is 16 threads. Thus, the first
    set of eight threads jump off to call function `func_a1` while the second set
    of eight threads (8..15) stall.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: Functions `func_b` and `func_a1` will continue to independently fetch instructions
    and dispatch them to the two half-warps. This is somewhat less efficient than
    a single instruction fetch, but nonetheless better than sequential execution.
    Eventually `func_a1` will complete and `func_a2` will start, stalling the threads
    0..7\. In the meantime `func_b` may have also completed. We can write a short
    test program to demonstrate this.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '`  const u32 ∗ const b,`'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '`   {`'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '`      }`'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Here we have set up a number of kernels, each of which exhibit different levels
    of divergence. The first is the optimal with no divergence. The second diverges
    based on half-warps. These half-warps should execute in parallel. We then further
    subdivide the first half-warp into two groups. These should execute in series.
    We then subdivide again the first group into a total of four serial execution
    paths. The results we see are as follows:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: We can see this somewhat better in a graphical format in [Figure 9.32](#F0165).
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-32-9780124159334.jpg)'
  id: totrans-700
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.32 How thread divergence affects execution time.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the thread divergence is not such a significant problem on the compute
    1.x devices (the 9800 GT and GTX260). It has an effect, but takes the maximum
    time to just 145% of the optimal time. By comparison, the Fermi compute 2.x cards
    (GTX460, GTX470) suffer over a 4× slowdown when diverging significantly within
    a warp. The GTX460 seems especially sensitive to warp divergence. Notice the GTX470
    is almost 10× faster in absolute terms than the 9800 GT when there is no divergence,
    which is a massive improvement for just two generations of cards.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: If you are curious to know how much a 32-way divergence costs, it leads to a
    27× slowdown on the compute 1.x cards and a massive 125× to 134× slowdown on the
    compute 2.x cards. Note that the code for this test was a simple switch statement
    based on the thread index, so it is not directly comparable to the code we’re
    using here. However, clearly such divergence needs to be avoided at all costs.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: The easiest method of avoiding divergence within a warp is to simply mask out
    the sections of the warp you don’t wish to contribute to the result. How can you
    do this? Just perform the same calculation on every thread in the warp, but select
    a value that does not contribute for the threads you wish to mask out.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: For example, for a `min` operation on 32-bit integers, select 0xFFFFFFFF as
    the value for threads that should not contribute. Conversely for `max`, `sum`,
    and many other arithmetic-type operations, just use 0 in the threads you do not
    wish to contribute. This will usually be much quicker than branching within a
    warp.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the low-level assembly code
  id: totrans-706
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GPU compiles code into a virtual assembly system called PTX (Parallel Thread
    eXecution Instruction Set Architecture). This is a lot like Java byte-code in
    that it is a virtual assembly language. This can either be translated at compile
    time or runtime into the real code, which executes on the device. The compile
    time translation simply inserts a number of real binaries into the application,
    depending on which architectures you specify on the command line (the `–arch`
    switch).
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: To look at the virtual assembly generated, you simply add the `–keep` flag to
    the compiler command line. For Visual Studio users, the default NVIDIA projects
    contain an option to keep the PTX files (`–keep`) ([Figure 9.33](#F0170)). You
    can also specify the place to store them if you prefer they do not clutter up
    the project directory using the `–keep-dir <directory>` option.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-33-9780124159334.jpg)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.33 Visual C options—how to keep PTX files.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: 'However, PTX is not what is really executed on the hardware, so it’s useful
    only to a certain degree. You can also see the actual binary post translation
    using the `cuobjdump` utility as follows:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: '`cuobjdump –sass global_mem_sample_sort.sm_20.cubin > out.txt`'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at a small device function, this is what we see at the various levels:'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'In PTX:'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '`{`'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-719
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-720
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'And the actual generated code for a compute 2.0 device:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-722
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-723
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: I’ve removed from the final generated code the actual raw hex codes, as they
    are not useful. Both PTX and the target assembler code use the format
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: The PTX code is extensively documented in the PTX ISA found in the “doc” directory
    of the NVIDIA GPU Computing Toolkit as the “ptx_isa_3.0.pdf” file for the CUDA
    4.1 SDK release. The binary instruction set is listed for GT200 and Fermi in the
    “cuobjdump.pdf” file found in the same directory. There is no detailed explanation
    of the actual instruction set as yet, as with the PTX, but it’s fairly easy to
    see which instructions map back to the PTX ISA.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: While NVIDIA supports forward compatibility with the PTX ISA between revisions
    of hardware, that is, PTX for compute 1.x will run on compute 2.x, the binaries
    are not compatible. This support of older versions of PTX will usually involve
    the CUDA driver recompiling the code for the actual target hardware on-the-fly.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: You should read the PTX ISA document and understand it well. It refers to CTAs
    a lot, which are cooperative thread arrays. This is what is termed a “block” (of
    threads) at the CUDA runtime layer.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the C code will drastically affect the final assembly code generated.
    It’s always good practice to look at the code being generated and ensure it is
    doing what is expected. If the compiler is reloading something from memory or
    doing something you would not expect, there is usually a good reason. You can
    usually then identify the cause in the C source code and eliminate the problem.
    In certain instances, you can also create inline PTX to get the exact functionality
    you require, although a lot of the very low-level instructions have equivalent
    compiler intrinsic functions that can be used.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the easiest ways to look at and understand the low-level assembly functions
    is to view the interleaved source and assembly listing via the “View Disassembly”
    option from within Parallel Nsight. Simply set a breakpoint within the CUDA code,
    run the code from the Nsight menu (“Start CUDA Debugging”), and wait for the breakpoint
    to be hit. Then right-click near the breakpoint and the context menu will show
    “View Disassembly.” This brings up a new window showing the interleaved C, PTX,
    and SASS code. For example:'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-731
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '`0x0002caf0     MOV R11, R9;`'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  id: totrans-733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: Here you can easily see how the C source code, a test for `threadIdx.x < 128`,
    is translated into PTX and how each PTX instruction is itself translated into
    one or more SASS instructions.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: Register usage
  id: totrans-735
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Registers are the fastest storage mechanism on the GPU. They are the only way
    of achieving anything like the peak performance of the device. However, they are
    limited in their availability.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: To launch a block onto an SM, the CUDA runtime will look at the block’s usage
    of registers and shared memory. If there are sufficient resources, the block will
    be launched. If not, the block will not. The number of blocks that are resident
    in an SM will vary, but typically you can achieve up to six blocks with reasonably
    complex kernels, and up to eight with simple ones (up to 16 on Kepler). The number
    of blocks is not really the main concern. It’s the overall number of threads as
    a percentage of the maximum number supported, which is the key factor.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: We listed a number of tables in [Chapter 5](CHP005.html) that gave an overview
    of how the number of registers per block affects the number of blocks that can
    be scheduled onto an SM, and consequentially the number of threads that the device
    will select from.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: 'The compiler provides a `–v` option, which provides some more detailed output
    of what is currently allocated. An example of a typical kernel is:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: The output is useful, but only if you understand what the compiler is telling
    you. The first item of interest is the `for sm_20` message, which tells you the
    code being created here is for the compute 2.x architecture (Fermi). If you’re
    using exclusively Fermi devices for your target deployment, then make sure your
    target is set correctly. By default you will generate compute 1.0 code unless
    you specify otherwise, which will restrict the available operations and generate
    code that is not as efficient as it could be for Fermi.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: The next interesting point is `40 bytes of stack frame`, which generally means
    you have local variables you are taking the address of, or that you declared a
    local array. The term “local” in C refers to the scope of a variable, and in C++
    was replaced with the keyword “private,” which more accurately reflects what is
    meant.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: In CUDA the term “local” refers to the scope of a variable for a given thread.
    Thus, the CUDA documentation also uses the term “local memory,” meaning thread
    private data. Unfortunately, “local” implies near or close, which in memory terms
    might imply the data is held close to the processor. In fact, “local data” is
    stored in either global memory for compute 1.x devices or in the L1 cache on Fermi
    devices. Thus, only on Fermi is it really “local” to the processor, and even in
    this case, its size is limited.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: The stack frame is something you typically see with compute 2.x device code,
    especially if using atomic operations. The stack frame will also exist in the
    L1 cache unless it becomes too large. Where possible the CUDA compiler will simply
    inline calls to device functions, thereby removing the need to pass formal parameters
    to the called functions. If the stack frame is being created simply to pass values
    by reference (i.e., pointers) to the device function, it is often better to remove
    the call and manually inline the functions into the caller. This will eliminate
    the stack frame and generate a significant improvement in speed.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: The next section lists `8+0 bytes lmem`. By “lmem” the compiler is referring
    to local memory. Thus, for 8 bytes, probably a couple of floats or integers have
    been placed into local memory. Again this is typically not a good indication as,
    especially in compute 1.x devices, there will be implicit memory fetches/writes
    to and from slow global memory. It’s an indication you need to think about how
    you might rewrite the kernel, perhaps placing these values into shared memory
    or constant memory if possible.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: Note the *a* + *b* notation used here denotes the total amount of variables
    declared in those sections (the first number), and then the amount used by the
    system (the second number). Also `smem` (shared memory) usages will be listed
    in addition to `lmem` if shared memory is used by the kernel.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: Next we see `80 bytes cmem[0]`. This says the compiler has used 80 bytes of
    constant memory. Constant memory is typically used for parameter passing, as most
    formal parameters do not change across calls. The value in the square brackets
    is the constant memory bank used and is not relevant. Simply add all the `cmem`
    figures to obtain the total constant memory usage.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
- en: Register usage can also be controlled, or forced, using the `–maxrregcount n`
    option in the compiler. You can use this to instruct the compiler to use more
    or less registers than it currently is. You may wish to have fewer registers to
    squeeze another block onto the SM. You may already be limited by some other criteria
    such as shared memory usage, so you may wish to allow the compiler to use more
    registers. By using more registers the compiler may be able to reuse more values
    in registers, rather than store/fetch them again. Conversely, asking for less
    registers usage will usually cause more memory accesses.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: Asking for less registers to get an additional block is a tradeoff exercise.
    The lower register count and the additional block may bring higher occupancy,
    but this does not necessarily make the code run faster. This is a concept most
    programmers starting with CUDA struggle with. The various analyzer tools try to
    get you to achieve higher occupancy rates. For the most part this is a good thing,
    as it allows the hardware scheduler to have a wider choice of warps to run. However,
    *only if* the scheduler actually runs out of warps at some point, and thus the
    SM stalls, does adding more available warps actually help. Fermi, due to its dual
    warp dispatcher and higher number of CUDA cores per SM, executes warps with a
    higher frequency than earlier models. The effect varies between applications,
    but generally asking for less register usage usually results in slower code. Try
    it for your particular application and see. We look at how you can see if the
    SMs are stalling in the later section on analysis tools.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: A better approach to asking for less registers is to understand the register
    usage and allocation of variables. To do this, you need to look into the PTX code,
    using the `–keep` compiler flag. PTX, the virtual assembly language used by CUDA,
    defines a number of state spaces. A variable exists in one of these state spaces.
    These are shown in [Table 9.8](#T0045). Thus, you can always look into the PTX
    code to see where a variable has been placed.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.8 PTX State Space
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0045.jpg)'
  id: totrans-752
  prefs: []
  type: TYPE_IMG
- en: Reducing register usage from say 26 to 25 per kernel will have little effect.
    However, transitioning over a register boundary (16, 20, 24, and 32) will usually
    allow for more blocks to be scheduled. This will bring a greater selection of
    warps and will usually improve performance. This is not always the case. More
    blocks can mean more contention for shared resources (shared memory, L1/L2 caches).
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: Register usage can often be reduced simply be rearranging the C source code.
    By bringing the assignment and usage of a variable closer together you enable
    the compiler to reuse registers. Thus, at the start of the kernel you might assign
    `a`, `b`, and `c`. If in fact they are used only later in the kernel, you’ll often
    find reduced register usage by moving the creation and assignment close to the
    usage. The compiler may then be able to use a single register for all three variables,
    as they exist in distinct and disjoint phases of the kernel.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  id: totrans-755
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Understand how thread layout impacts memory and cache access patterns.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
- en: • Use only multiples of 32 when specifying the thread count for kernel launch.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: • Think about how to increase the amount of work performed per memory fetch.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
- en: • Understand at least a little of how compilers work when optimizing code and
    adapt your source code to aid the compiler.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: • Consider how branching within a warp can be avoided.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: • Look at the PTX and final target code to ensure the compiler is not generating
    inefficient code. If it is, understand why and make changes at the source level
    to address it.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: • Be aware and understand where data is being placed and what the compiler is
    telling you.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 5: Algorithms'
  id: totrans-763
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Selecting an efficient algorithm on the GPU can be challenging. The best algorithm
    in the CPU domain is not necessarily the best for the GPU. The GPU has its own
    unique challenges. To get the best performance you need to understand the hardware.
    Thus, when considering algorithms, we need to think about:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: • How to decompose the problem into blocks or tiles and then how to decompose
    those blocks into threads.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: • How the threads will access the data and what sort of memory pattern this
    will generate.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
- en: • What data reuse opportunities are present and how these can be realized.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: • How much work the algorithm will be performing in total and whether there
    is a significantly difference from a serial implementation.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an 800-plus-page book published by Morgan Kaufman entitled *GPU Computing
    Gems* that covers in detail implementation of various algorithms for the following
    areas:'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: • Scientific simulation
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: • Life sciences
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: • Statistical modeling
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: • Data-intensive applications
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: • Electronic design and automation
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: • Ray tracing and rendering
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
- en: • Computer vision
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: • Video and image processing
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: • Medical imaging
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this section is not to look at algorithms that are specific to
    certain fields, as they are of limited general interest. Here we look at a few
    common algorithms that can be implemented, which in turn may form building blocks
    for more complex algorithms. This book is not about providing sets of examples
    you can copy and paste, but providing examples where you can learn the concepts
    of what makes good CUDA programs.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
- en: Sorting
  id: totrans-780
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many, many sorting algorithms available, some of which can easily
    and efficiently be implemented on the GPU and many of which are not well suited.
    We’ve looked already in previous chapters at merge sort, radix sort, and the more
    exotic sample sort. We’ll look here at one more parallel sort that is useful in
    terms of looking at how algorithms are implemented in GPUs.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
- en: Odd/even sort
  id: totrans-782
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An odd/even sort works by selecting every even array index and comparing it
    with the higher adjacent odd array index ([Figure 9.34](#F0175)). If the number
    at the even element is larger than the element at the odd index, the elements
    are swapped. The process is then repeated, starting with the odd indexes and comparing
    them with the higher adjacent even index. This is repeated until we make no swaps,
    at which point the list is sorted.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-34-9780124159334.jpg)'
  id: totrans-784
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.34 Odd/even sort.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: An odd/even sort is a variation of a bubble sort. A bubble sort works by selecting
    the number at the first index and comparing and swapping it with the index to
    the right until such time as it’s no longer larger than the number to its right.
    The odd/even sort simply extends this to use *P* independent threads to do this,
    where *P* is half the number of elements in the list.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: If we define the number of elements in an array as *N*, then the ability to
    deploy half of *N* threads may be appealing. The sort is also quite easy to conceptualize,
    but raises some interesting problems when trying to implement on the GPU, so it
    is a good example to look at.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: The first issue is that odd/even sort is designed for parallel systems where
    individual processor elements can exchange data with their immediate neighbor.
    It requires a connection to the left and right neighbor only. A connection for
    our purposes will be via shared memory.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: Having thread 0 access array elements zero *and* one and thread 1 access elements
    two *and* three causes a sequence issue for the coalescing hardware. It needs
    each thread to access a contiguous pattern for a coalesced access. Thus, on compute
    1.x hardware this access pattern is terrible, resulting in multiple 32-byte fetches.
    However, on compute 2.x hardware, the accesses fetch at most two cache lines.
    The additional data fetched from the even cycle will likely be available for the
    odd cycle and vice versa. There is also a significant amount of data reuse with
    a high degree of locality, suggesting cache and/or shared memory would be a good
    choice. Shared memory would likely be the only choice for compute 1.x devices
    due to the poor coalescing.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: If we consider shared memory, we need to think about bank conflicts. Thread
    0 would need to read banks 0 and 1, plus write to bank 0\. Thread 1 would need
    to reads banks 2 and 3 and write to bank 2\. In a compute 1.x system with 16 banks,
    thread 8 would wrap around and start accessing banks 0 and 1\. On compute 2.0
    hardware, we’d see the same effect at thread 16\. Thus, we’d have four bank conflicts
    per thread on compute 1.x hardware and two bank conflicts per thread on compute
    2.x hardware with a shared memory implementation.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: 'The CPU code for odd/even sort is quite simple:'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-793
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '`  else`'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-800
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: The code iterates over the dataset from array element 0 to `num_elem-1` and
    then from element 1 to `num_elem-2`. The two data elements are read into local
    variables and compared. They are swapped if necessary and a counter `num_swaps`
    is used to keep track of the number of swaps done. When no swaps are necessary,
    the list is sorted.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: For a mostly sorted list, such algorithms work well. The reverse sorted list
    is the worst case, where we have to move elements all through the list to the
    end. The output of a reverse sorted list is shown here. We can see in each stage
    how the values move between the cells.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  id: totrans-803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'For the GPU implementation, we’ll use global memory on a compute 2.x device.
    The GPU implementation is:'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  id: totrans-805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-806
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '`  u32 num_swaps;`'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  id: totrans-808
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-810
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: Instead of the traditional `for` loop construct, the CPU code uses a `do..while`
    construct. The obvious choice for parallelism from the algorithm is the compare
    and swap operation, meaning we need *N*/2 threads where *N* is the number of elements
    in the array. Given that most lists we’d bother sorting on the GPU will be large,
    this gives us potential to make use of the maximum number of threads on a given
    device (24,576 threads on GTX580).
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: As each thread processes two elements we cannot simply use `tid` as the array
    index, so create a new local parameter `tid_idx`, which is used to index into
    the array. We also create a parameter `tid_idx_max`, which is set to the last
    value in the array, or the last value in the current block where there is more
    than one block.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  id: totrans-818
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: The end condition is somewhat problematic. The parameter `num_swap`s in the
    serial version is written to only once per iteration. In the parallel version
    we need to know if *any* thread did a swap. We could therefore use an atomic `add`,
    `increment`, `AND`, or `OR` operation for this, but this would represent a serial
    bottleneck, as every thread that did a write would have to be serialized.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: We could mitigate the cost of the atomic operations somewhat by using a shared
    memory atomic operation. Note that shared memory atomics are supported only on
    compute 1.2 hardware or later (the GT200 series). For the older compute 1.1 hardware
    (the 9000 series) we’d need to use global memory atomics. The definition of the
    `num_swaps` variable would need to be changed accordingly.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: For compute 2.x hardware, there is a much faster solution that we will use here.
    As we have to wait at the end of each round anyway, we can make use of the newly
    provided primitive, `__syncthreads_count`, to which we pass a predicate. If the
    predicate is nonzero in any of the threads, then the result to all threads is
    also nonzero. Thus, if just one thread does a swap, all threads again iterate
    around the loop.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  id: totrans-822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: The host function to call the kernel is also shown here for completeness.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  id: totrans-824
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-826
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-827
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '`    %d threads (%u active)", num_blocks,`'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  id: totrans-830
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-831
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-833
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-834
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-835
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'One question that should be in your mind about this code is what happens at
    the block boundaries. Let’s look at the results with one and two blocks with a
    dataset small enough to print here:'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  id: totrans-837
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: and
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  id: totrans-839
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: Notice in the second output the sort occurred only within the block. The values
    on the right needed to propagate to the left and vice versa. However, as the blocks
    do not overlap, they are not able to do this. The obvious solution would be to
    overlap the blocks, but this would not be an ideal solution.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
- en: CUDA was designed to allow blocks to run in any order, without a means for cross-block
    synchronization within a single kernel run. It’s possible by issuing multiple
    kernels to synchronize between blocks, but this mechanism works well only where
    you have a small number of synchronization steps. In this kernel we’d need to
    overlap the blocks on every iteration. This would lose all locality, as now two
    SMs need to share the same dataset. We also need a global memory atomic or a reduction
    operation to keep track of whether any blocks performed a swap and have to continue
    issuing kernels until no swaps had taken place in any block—a lot of host interaction.
    That would not be a good route to go down.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: So we’re left with the two choices found in most sorts that decompose into blocks.
    Either presort the input lists so the values in list *N*[−1] are less than *N*[0],
    which are larger than *N*[1], the solution we used with the sample sort, or merge
    *N* separate lists, the merge sort problem we also looked at earlier.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: Reduction
  id: totrans-843
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reduction is used significantly in parallel programming. We’ll look at some
    of the many ways we can perform a reduction to see which method produces the best
    results on the various compute platforms and to understand why.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look first at computing the sum of *N* 32-bit integers, some 48 million
    to give a reasonable sized dataset. With such a large number of values one of
    the first issues we need to consider is overflow. If we add 0xFFFFFFFF and 0x00000001
    then we have an overflow condition with a 32-bit number. Therefore, we need to
    accumulate into a 64-bit number. This presents some issues.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: First, any atomic-based accumulation would require an atomic 64-bit integer
    add. Unfortunately, this is supported in shared memory only with compute 2.x hardware
    and in global memory only in compute 1.2 hardware onward.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: Global atomic add
  id: totrans-847
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s look first at the simplest form of reduction:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  id: totrans-849
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-850
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: In this first example, each thread reads a single element from memory and adds
    it to a single result in global memory. This, although very simple, is probably
    one of the worst forms of a reduce operation. The interblock atomic operation
    means the value needs to be shared across all of the SMs.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
- en: 'In the older hardware this means physically writing to global memory. In the
    compute 2.x hardware this means maintaining an L2 cache entry, shared among all
    the SMs, and eventually writing this to global memory. The results we see are
    as follows:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  id: totrans-853
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: We’ll look here at the compute 2.x devices, as these support 64-bit integer
    atomics.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: The issue with the atomic writes, even to L2 cache, is they force a serialization
    of the threads. We have six blocks in each SM, 256 threads per block, generating
    1536 threads per SM. On the GTX470 we have 14 SMs, so a total of 21, 504 active
    threads. On the GTX460 we have 7 SMs, so a total of 10,752 active threads. Performing
    an atomic operation on a single global memory cell means we create a lineup, or
    serialization, of 10 K to 21 K threads. Every thread has to queue, once for every
    single element it processes. Clearly a poor solution, even if it is a somewhat
    simple one.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: Reduction within the threads
  id: totrans-856
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can improve this situation by performing some of the reduction within the
    thread. We can do this very simply by changing the data type and adjusting the
    kernel to ensure we don’t go out of bounds.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  id: totrans-858
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '`// address in GMEM`'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  id: totrans-860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-861
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-863
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-865
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-867
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'In the first example we process two elements per thread and four in the second
    using the built-in vector types `uint2` and `uint4`. This produces the following
    timings:'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-870
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Although, a dramatic reduction, we’ve not really solved the problem. All we
    have done is to half or quarter the number of times each thread has to queue by
    performing a local reduction. This drops the overall time to approximately one-half
    and one-quarter of the original. However, there is still a 5 K thread lineup trying
    to write to the global memory.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: 'Note in performing the addition locally, we reduce the number of global writes
    by a factor equal to the level of ILP. However, we have to be careful about how
    the addition is performed. You could write:'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  id: totrans-873
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: In C, an expression is typically evaluated from left to right. A promotion of
    the left operator generates an implicit promotion of the right operator. Thus,
    you might expect `element.x` to be promoted to an unsigned 64-bit type, and as
    `element.y` is to be added to it, it will also be promoted. As `element.z` and
    `element.w` will subsequently be added, you might also expect these to be promoted.
    You are, however, thinking like a serial programmer. The `z` and `w` elements
    can be calculated independently of `x` and `y`. This is exactly what the PTX code
    does. As neither `z` nor `w` has been promoted to a 64-bit value, the addition
    is done as a 32-bit addition, which may result in an overflow.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
- en: The problem lies in that C permits any order of evaluation where the operator
    is commutative. However, as you typically see a left to right evaluation, people
    assume this is how all compilers work. This is one of the portability issues between
    C compilers. When we move to a superscalar processor such as a GPU, it performs
    the two sets of additions independently to make the maximum use of the pipeline.
    We don’t want it to wait 18–22 plus cycles for the first addition to complete
    then make the subsequent additions in series.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the correct way to write such additions is:'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  id: totrans-877
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: Here every value is converted to a 64-bit number before the addition takes place.
    Then any ordering of the addition is fine for integer values. Note for floating-point
    values simply converting to doubles is not enough. Due to the way floating-point
    numbers work adding a very tiny number to a very large number will result in the
    small number being discarded, as the floating-point notation does not have the
    required resolution to hold such values. The best approach to this type of problem
    is to first sort the floating-point values and work from the smallest number to
    the largest.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
- en: We can take the ILP technique a little further by using multiple elements of
    `uint4` and adjusting the kernel accordingly.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  id: totrans-880
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-882
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '`  u64 value = ((u64)element.x) + `'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  id: totrans-884
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-888
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-890
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '`     ((u64)element.y) + `'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE199]'
  id: totrans-894
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-895
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-897
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-899
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-901
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '`     ((u64)element.w);`'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  id: totrans-904
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-906
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-907
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: 'Notice that we’re mixing the loading of data with the addition. We could move
    all the loads to the start of the function. However, consider that each `uint4`
    type requires four registers. Thus, the `ILP32` example would require 32 registers
    just to hold the values from a single read iteration. In addition, some are needed
    for the addition and final write. If we use too many registers, the number of
    blocks that can be scheduled is reduced or the kernel spills registers to “local”
    memory. Such local memory is the L1 cache for compute 2.x devices and global memory
    for the compute 1.x devices. The results for these ILP kernels are shown here:'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  id: totrans-909
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-910
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-911
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: We can see that ILP significantly decreases the execution time, providing it’s
    not taken too far. Note the `ILP32` solution actually takes longer. Despite achieving
    a 20× speedup over the simplest version, we have still not solved the atomic write
    queuing problem, just reduced the overall number of elements. There are still
    too many active threads (10–21 K) all trying to write to the single accumulator.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
- en: Reduction of the number of blocks
  id: totrans-913
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Currently, we’re invoking *N* blocks where *N* is the problem size, 12 million
    elements (48 MB) divided by the number of threads per block multiplied by the
    number of elements processed per block. We finally get *N* atomic writes, all
    of which are serialized and cause a bottleneck.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
- en: We can reduce the number of contentions if we create far, far less blocks and
    greatly increase the amount of work each block performs. However, we have to do
    this without increasing the register usage, something the `ILP32` example did.
    This, in turn, caused a slowdown due to local memory reads and writes.
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: Currently, we launch 48 K blocks, but could reduce this to 16, 32, 64, 128,
    or 256 blocks. We can then have each thread march through memory, accumulating
    the result to a register, and only when the block is complete, write out the result.
    Depending on the number of blocks, this should generate quite good locality of
    memory references between the SMs, thus making good use of the memory bandwidth
    and L2 cache if present.
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  id: totrans-917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-918
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-920
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-922
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-923
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '`}`'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
- en: The first task is to work out how many iterations over the data each thread
    needs to make. The parameter `gridDim.x` holds the number of blocks launched.
    Each block consists of `blockDim.x` threads. Thus, we can work out how many elements
    of data each thread must accumulate. We then accumulate these in `local_result`,
    and only when the block is complete, do a single write to global memory.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
- en: This reduces the contention from a thread-level contention to a block-level
    contention. As we’re only launching a few hundred blocks, the probability of them
    all requiring the write at the same time is reasonably low. Clearly as we increase
    the number of blocks, the potential contention increases. Once we have loaded
    all the SMs with the maximum number of permitted blocks, there is little reason
    to increase the number of blocks further, other than for work balancing.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: 'The GTX460 is perhaps the worst example, as with only 7 SMs, each with 6 blocks,
    we should saturate the device at only 42 blocks. The GTX470 would need 90 blocks.
    We, therefore, try all number of blocks (49,152) down to 16 in powers of two,
    fewer blocks than would be necessary to fully populate the SMs. This generates
    the following results:'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  id: totrans-928
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-929
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: If we look at this first on the very large number of blocks we see a fairly
    linear drop as we halve the number of blocks for each run, for both the GTX470
    and GTX460 cards. We’re halving the number of blocks each cycle by increasing
    the amount of work done per thread, but without increasing the ILP (indicated
    here with `loop1`).
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the GTX460 has consistently outperformed the GTX470 in the previous
    examples. It does this until such time as we get down to a very small number of
    blocks ([Figure 9.35](#F0180)). At 384 blocks we see the GTX470 outperform the
    GTX460\. The GTX470’s larger number of smaller SMs (32 CUDA cores versus 48 CUDA
    cores each) and larger cache starts to impact performance.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-35-9780124159334.jpg)'
  id: totrans-932
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.35 Time (ms) versus number of blocks (large number of blocks).
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
- en: If you then look at the timing with a very small number of blocks, you can see
    that around 64 blocks is the minimum needed before the number of SM scheduling/occupancy
    issues come into play ([Figure 9.36](#F0185)). In the figure, we’ve split the
    graphs into one with a large number of blocks and one with a smaller number, so
    we can see the time at small block numbers.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-36-9780124159334.jpg)'
  id: totrans-935
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.36 Time (ms) versus number of blocks (small number of blocks).
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
- en: Note so far we’ve used no ILP (instruction-level parallelism). However, we know
    that introducing ILP allows us to achieve better timing. This is especially the
    case when we have a small number of blocks. The optimal timing is for 64 blocks.
    The GTX470 would have just over 4 blocks, 32 warps per SM. With 32-bit memory
    fetches we need a fully loaded SM, 48 warps, to achieve peak bandwidth from the
    global memory. We can achieve this only with ILP while maintaining this number
    of warps.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  id: totrans-938
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-939
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-940
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-942
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-943
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-944
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '` const uint4 ∗ const data,`'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  id: totrans-947
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-948
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-949
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-950
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-951
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-952
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-953
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: 'The effect on introducing ILP has one additional benefit: The time spent performing
    the loop (overhead) is amortized over more useful instructions (memory fetch,
    add). We therefore see the following results:'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  id: totrans-955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: In `loop1` we use a single 32-bit element, in `loop2` we use two elements (`uint2`),
    and in `loop4` we use four elements (`uint4`). In each case we use 64 blocks,
    the best result from the previous test. You can see that moving from 32-bit elements
    per thread to 64-bit elements per thread we gain on the order of 20–25%. Moving
    from 64-bit reads to 128-bit reads gains us almost nothing on the GTX470, but
    on the order of an 8% gain on the GTX460\. This is entirely consistent with the
    bandwidth results we looked at earlier where the GTX460 (compute 2.1) device achieved
    a significantly higher bandwidth when using 128-bit reads instead of 64-bit reads.
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: Reduction using shared memory
  id: totrans-957
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we look at the last instruction of the kernel to date, we still have one
    issue, using an atomic add to write out the result. With 256 threads per block
    and 64 blocks resident, we have 16 K threads all trying to write to this final
    accumulated value. What we actually need is a reduction across the threads within
    the block. This would drop the number of writes from 16 K to just 64, the number
    of blocks. This should reduce the overall timing considerably, as we’re removing
    the serialization bottleneck.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
- en: However, going back to the first section in this chapter, know when fast is
    fast enough and appreciate the additional effort required to squeeze that last
    few percent out of the problem. Notice as the speed has improved, the kernels
    become more and more complex.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory is a bank-switched set of 32 banks (16 in compute 1.x). Providing
    each thread uses a unique bank index (0..31) the shared memory can process one
    element per clock, per thread. This is its peak performance, for a single warp.
    As we introduce more warps, if they too want to access shared memory, the ability
    of one warp to use the full bandwidth of shared memory is reduced as it must share
    the LSUs with other competing warps. Once the LSUs are running at 100% capacity,
    we’re limited by the bandwidth from the combined 64 K of L1 cache/shared memory
    on the SM.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
- en: We could simply perform a block-level reduction into a single shared memory
    value for each SM. Thus, with 256 threads we’d have a 256:1 reduction ratio. However,
    this proves not to be particularly effective, as each of the 256 threads is serialized.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
- en: The execution units within an SM can execute a half-warp, a group of 16 threads.
    Therefore, it makes sense to perform a reduction across half-warps. We could then
    either perform an additional reduction across the set of 16 half-warps, or we
    could simply write out the set of values to shared memory. It turns out there
    is almost no difference in execution time between the two approaches.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
- en: The problem, however, with a subsequent intrablock reduction in shared memory
    is where to locate the shared memory parameter to perform the reduction. If you
    place it after the set of 64 bytes occupied by the intrawarp reduction parameters,
    it causes the next block of intrawarp not to be 64-byte aligned. The different
    blocks interact with one another to cause bank conflicts in the shared memory.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
- en: We opted for the direct write to global memory, as this was the simpler solution
    and shows marginal if any difference in performance. Thus, instead of reducing
    the 16 K conflicting writes to 64 potentially conflicting writes, we have 512
    potentially conflicting writes, which is a factor of 32 reduction.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  id: totrans-965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-966
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-967
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-968
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '[PRE244]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: '[PRE245]'
  id: totrans-970
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: '[PRE246]'
  id: totrans-971
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: '[PRE247]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: '[PRE248]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: '[PRE249]'
  id: totrans-974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '[PRE250]'
  id: totrans-975
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '[PRE251]'
  id: totrans-976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: '`}`'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: 'This results in the following:'
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  id: totrans-979
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '[PRE253]'
  id: totrans-980
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: In this example, `loopB` has 512 atomic writes to global memory. The second
    kernel, `loopC`, performs an additional intrablock reduction before making 64
    atomic writes to global memory. As you can see, there is little if any difference
    in performance, demonstrating the additional reduction step gains us nothing and
    therefore was removed from the final solution. This is not really too surprising,
    as if the latency of the 512 memory writes is already hidden by the considerable
    computation workload, reducing this to just 64 writes would bring us nothing.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
- en: If we compare the best result from the previous section, using an accumulation
    into registers and then writing out the 16 K values we see on the GTX470 (compute
    2.0), this took 1.14 ms. By adding this further reduction step in shared memory
    we’ve reduced this to just 0.93 ms, a 19% saving in execution time. As the GTX470
    has 14 SMs, this intra-SM reduction step significantly reduces the number of final
    atomic global memory writes that must be coordinated between these SMs.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, the GTX460 device (compute 2.1) reduced from 1.38 ms to 1.33 ms,
    just 4%. The absolute difference is of course clear in that the GTX470 has a 320-bit
    memory bus compared with the 256-bit memory bus on the GTX460\. It’s the relative
    speedup difference that is interesting.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: Such a small speedup would indicate that the multiple global memory atomic operations
    were not in fact the bottleneck as they were on the GTX470\. It could also indicate
    that perhaps we were already using the LSUs to their full capacity. The ratio
    of LSUs to CUDA cores is much less on the compute 2.1 devices than on the compute
    2.0 devices. Both global memory and shared memory accesses require the LSUs.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the shared memory–based reduction, based on half-warps, gains us a significant
    reduction over the purely atomic/global memory–based solution in the previous
    section.
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach
  id: totrans-986
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As with any implementation, you should always look to what previous work has
    been done and how this could be used to improve existing designs. Mark Harris
    wrote an excellent study of parallel reduction^([3](#FN3)) back in the early GPU
    days based on the G80 device. Instead of performing a 512:16 reduction, it writes
    the entire set of values to shared memory and then uses shared memory to perform
    a series of partial reductions, always accumulating the result to shared memory.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
- en: The results are impressive. He used unsigned integer elements and achieved a
    total time of 0.268 ms on 4 million elements. Scaling this to the 12 million elements
    (48 MB data) we used in the example works out to 1.14 ms, a comparable number
    to the 0.93 ms we achieved on the GTX470.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: However, the GTX470 has 448 CUDA cores, compared to the 128 CUDA cores of the
    G80, a factor of 3.5× improvement in arithmetic capacity. Memory bandwidth has
    increased from 86 GB/s to 134 GB/s, a factor of 1.5×. However, Mark’s kernel accumulates
    into 32-bit integers, whereas we accumulate into 64-bit integers to avoid the
    overflow problem. Therefore, the kernels are not directly comparable.
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless the method proposed may produce good results. Accumulation into
    a register will clearly be faster than accumulation into shared memory. As the
    hardware does not support operations that directly operate on shared memory, to
    perform any operation we need to move the data to and from shared memory. One
    of the reasons for selecting register-based accumulation was the elimination of
    this overhead. However, that is not to say we have an optimum set of code for
    this part of reduction yet.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: Some time has passed since this chapter was originally written and this late
    addition comes after a transition from CUDA 4.0 to CUDA 4.1 SDK, which moved us
    from the Open64 compiler to an LLVM-based compiler. This should bring a performance
    boost, and indeed we find the more efficient compiler generates an execution time
    of 0.74 ms instead of our previous 0.93 ms, a huge improvement just from changing
    compilers.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: However, of this time, how much is actually due to the reduction at the end
    of the code? We can find out simply by commenting out the final reduction. When
    we do this, the time drops to 0.58 ms, a drop of 0.16 ms or some 21%. Further
    investigation reveals that actually all but 0.1 ms of this time can be attributed
    to the atomic add operation.
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the 2.1 version of Parallel Nsight we can extract a number of useful
    facts from the data:'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: • Of the 48 scheduled warps, on average we get only 32 active warps.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: • The workload is unevenly distributed between the SMs.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: • Most issue dependencies are the short class.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
- en: • There is very little divergent branching.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: • Around 8% of the time the SMs stalled. This was due mostly to either instruction
    fetch or instruction dependencies.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
- en: This occupancy issue is a somewhat misleading one, in that it is caused by the
    uneven distribution rather than some runtime issue. The problem is the number
    of blocks launched. With 14 SMs, we can have 84 blocks resident with 6 blocks
    per SM. Unfortunately we only launch 64, so in fact some of the SMs are not fully
    loaded with blocks. This drops the average executed warps per SM and means some
    SMs idle at the end of the workload.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
- en: 'We ended up with a value of 64 due to it being identified as an ideal number
    from the earlier experiments. However, these were based on 16 K competing atomic
    writes to global memory. We’ve since reduced this to just 512 writes with most
    of the atomic writes being within the SM. Once we remove this global bottleneck,
    it would appear that 64 blocks in total is not the ideal number. Running a sample
    we see:'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE254]'
  id: totrans-1001
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: '`ID:0 GeForce GTX 470:GMEM loopC 48 passed Time 0.82 ms`'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  id: totrans-1003
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: Notice the best number of blocks on the GTX470 is 384, while on the GTX460 it
    is 96\. A value of 192 works well on both devices. Clearly, however, a value of
    64 blocks does not work well.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: 'However, what about the last issue we noticed, that 8% of the time the SMs
    were idle? Well this improves to 7% when there are additional blocks, so this
    is helping. However, what is the cause of the problem? Looking to the kernel output
    gives us a clue to the issue:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE256]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE256]'
- en: 'Notice, unlike the CUDA 4.0 SDK compiler, the 4.1 compiler places `uint4` types
    into local memory. This local memory on Fermi is the L1 cache, so should you care?
    We can rewrite the `uint4` access to use a `uint4` pointer. As the `uint4` types
    are 128-bit aligned (4 × 32 bit words), they are guaranteed to sit on a cache
    line and memory transaction boundary. Thus, an access to the first element of
    the `uint4` by any thread will pull the remaining three elements into the L1 cache.
    Consequently, we have L1 local memory access versus L1 direct cache access. There
    should be no difference, in theory. Let’s see:'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE257]'
  id: totrans-1008
  prefs: []
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[PRE258]'
  id: totrans-1009
  prefs: []
  type: TYPE_PRE
  zh: '[PRE258]'
- en: Both the GTX470 and GTX460 devices show a significant drop in the execution
    time. Looking to the cache utilization statistics, we can see the L1 cache hit
    rate has jumped from 61.1% to 74.5% as we have moved from the local memory version
    (`loopC`) to the pointer version (`loopD`). We also see the percentage of stalls
    in the SMs drops to 5%. Actually for this statistic, the difference on the GTX460
    is quite pronounced, as it started off at 9%, slightly higher than the GTX470\.
    This is likely to be because we’re now able to share the L1 cache data between
    threads as the data is no longer “thread private.”
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why we simply do not just use 84 blocks as we calculated
    earlier. The issue is one of rounding. The 12 million element dataset does not
    equally divide into 84 blocks. Thus, some blocks would need to process more than
    others. This means the logic would need to be more complex, but more complex for
    *every* block executed. Just running 84 blocks without solving this issue shows
    a time of 0.62 ms, a gain of 0.06 ms over the 384-block version. This demonstrates
    that the 384-block version introduces small enough blocks that the existing load-balancing
    mechanism handles it quite well. The value of making the code more complex significantly
    outweighs the benefits and is only necessary if we in fact do not know the size
    of the input dataset.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the question of shared memory versus atomics, which is faster?
    We can replace the atomic-based reduction with the following code:'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE259]'
  id: totrans-1013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE259]'
- en: '[PRE260]'
  id: totrans-1014
  prefs: []
  type: TYPE_PRE
  zh: '[PRE260]'
- en: '[PRE261]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: '[PRE262]'
  id: totrans-1016
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: '[PRE263]'
  id: totrans-1017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: Notice how the code works. First, all 256 threads (warps 0..6) write out their
    current `local_result` to an array of 256 64-bit values in shared memory. Then
    those threads numbered 0 to 127 (warps 0..3) add to their result, the result from
    the upper set of warps. As the warps within a block are cooperating with one another,
    we need to ensure each warp runs to completion, so add the necessary `__syncthreads()`call.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: We continue this reduction until the point at which we reach 32 threads, the
    size of a single warp. At this point all threads within the warp are synchronous.
    Thus, we no longer need to synchronize the threads, as the thread sync operation
    is really a warp sync operation within a single block.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
- en: We now have a couple of choices. We could continue the `if threadIdx.x < threshold`
    operation or we can simply ignore the fact that the redundant threads within the
    warp perform a useless operation. The additional test actually generates a considerable
    number of additional instructions, so we simply calculated all values within the
    warp. Note that this is different than running multiple warps, as in the case
    where we have the 128 and 64 test. Within a single warp, reducing the number of
    threads gains us nothing. By comparison, the prior tests eliminate entire warps.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: So does this gain us anything compared to the atomic reduction?
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: Compared with the last version, we moved from 0.68 ms to 0.64 ms on the GTX470
    and 0.8 ms to 0.79 ms on the GTX460\. Not a significant gain, but nonetheless
    a gain in execution speed. We can provide one last optimization to this code before
    we move on.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: Compilers typically generate less than optimal code for array indexing where
    the value of the array index is not a constant. The CUDA compiler is no exception.
    We can replace the array code with pointer code, which runs somewhat faster. We
    can also reduce the number of reads/writes to the shared memory area. However,
    as with most optimized solutions the code becomes more complex to understand and
    less easy to maintain and debug.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE265]'
  id: totrans-1025
  prefs: []
  type: TYPE_PRE
  zh: '[PRE265]'
- en: '[PRE266]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: '[PRE267]'
  id: totrans-1027
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: '[PRE268]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: '[PRE269]'
  id: totrans-1029
  prefs: []
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '` local_result += ∗(smem_ptr+64);`'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE270]'
  id: totrans-1031
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: '[PRE271]'
  id: totrans-1032
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: '[PRE272]'
  id: totrans-1033
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '[PRE273]'
  id: totrans-1034
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: '[PRE274]'
  id: totrans-1035
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '[PRE275]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: '[PRE276]'
  id: totrans-1037
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '[PRE277]'
  id: totrans-1038
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '[PRE278]'
  id: totrans-1039
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: 'The approach taken here is that, as we already have the current threads result
    stored in `local_result`, there is little point in accumulating into the shared
    memory. The only shared memory stores needed are those from the upper set of threads
    sending their data to the lower set. Thus, in each reduction step only the top
    set of threads write to shared memory. Once we get to a single warp, the code
    for this test takes longer than the reads/writes it saves from the shared memory,
    so we drop the test and write anyway. Also to avoid any address calculations,
    other than simple pointer addition, the address of the shared memory area is taken
    as a pointer at the start of the code section. The revised timings are:'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  id: totrans-1041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: Thus, we gained 0.02 ms on both the GTX470 and GTX460\. We have also largely
    eliminated the shared memory based atomic reduction operations, which in turn
    allows for implementation on older hardware. To remove the final reduction to
    global memory, you’d need to write to an array indexed by `blockIdx.x` and then
    run a further kernel to add up the individual results.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: An alternative CPU version
  id: totrans-1043
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For reference, the CPU serial and parallel implementations are provided so we
    can see the same reduction on the CPU side.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE280]'
  id: totrans-1045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: '[PRE281]'
  id: totrans-1046
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: '[PRE282]'
  id: totrans-1047
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: '[PRE283]'
  id: totrans-1048
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: '[PRE284]'
  id: totrans-1049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: '[PRE285]'
  id: totrans-1050
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: On an AMD Phenom II X4 processor (four cores) running at 2.5 MHz, this resulted
    in a timing of 10.65 ms for the serial version and 5.25 ms for the parallel version.
    The parallel version was created using OpenMP and the “reduction” primitive. To
    enable these quite useful pragma in the NVCC compiler simply use the `-Xcompiler
    –openmp` flag and you can use any of the OpenMP directives for CPU-level thread
    parallelism.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
- en: This code spawns *N* threads where *N* is the number of cores. The threads are
    then free to run on any available core. The work is split into *N* chunks and
    finally the results are combined.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: As can often be the case with parallel programming on CPUs, we see sublinear
    scaling as the number of cores increases. We can see that the scaling works well
    from one core to two cores, with a 35% drop in time when using two cores and a
    50% drop when using three. However, the addition of the fourth core drops the
    execution time by just an additional 2% so is effectively noncontributing ([Figure
    9.37](#F0190)). You typically see a U shape as the number of cores is further
    increased.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-37-9780124159334.jpg)'
  id: totrans-1054
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.37 OpenMP scaling on four cores.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is, while the compute performance is being scaled by the
    introduction of more cores, the memory bandwidth to the socket is shared between
    all cores. Taking our test system as an example, the AMD 905e processor has a
    typical memory bandwidth of 12.5 MB/s. Just to read the 48 MB of data from memory
    without any compute operations would therefore take 3.8 seconds, a considerable
    chunk of the 5.25 ms execution time. Thus, the issue here is not OpenMP versus
    CUDA but one of memory bandwidth available *per core* on a CPU versus that of
    a GPU.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
- en: Parallel reduction summary
  id: totrans-1057
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The original, very simplistic GPU implementation took 197 ms and 164 ms (GTX470
    and GTX460). Compared with the CPU parallel four-core result of 5.25 ms this is
    really terrible and an example of how an apparently fast device can be brought
    to its knees by poor programming practices.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
- en: The final GPU version uses atomic operations as little as possible outside of
    the SM. It achieves, in pure compute terms, a 6.8× (GTX460) or 8.4× (GTX470) speedup
    over a four-core CPU. However, 0.62 ms is very little compute time to hide any
    transfer time. At 5 GB/s to the device the PCI-E 2.0 bandwidth is around 40% of
    the bandwidth to main memory on our test platform (12 GB/s). A 5 GB per second
    transfer rate gives us around 5 MB per millisecond. Thus the transfer time of
    the 48 MB of data would be 9.6 ms alone. We’d be able to overlap less than 10%
    of compute time with this, which limits the overall execution to no faster than
    the PCI-E 2.0 transfer speed.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
- en: This is actually all too often a problem with GPUs in general. They need to
    have a sufficiently complex problem that the benefit of their huge compute power
    can be applied. In such cases, they can drastically outperform a CPU. A simple
    problem like performing a `sum`, `min`, `max`, or other simplistic task just doesn’t
    provide enough of a problem to justify the time for the PCI-E transfer, unless
    we can discount the transfer time by ensuring the data is already resident on
    the device and stays there. This is one of the reasons why the 6 GB Teslas are
    more attractive than the much cheaper consumer cards that have a maximum capacity
    of 4 GB.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
- en: To increase the overall amount of data held in the GPU memory space, you can
    simply install multiple cards in a system, typically up to four per node, or more
    if you use exotic cooling methods. Thus, up to 24 GB in total data can be held
    on four Tesla class cards within a single node. The host memory space can be directly
    augmented with the GPU memory space using the UVA (universal virtual addressing)
    feature if this is available to you (requires a compute 2.x device onwards, a
    64-bit OS, Linux or the TCC driver under Windows, CUDA 4.x runtime). Inter-GPU
    communication (peer-to-peer, P2P) can also be performed without routing the data
    through the CPU, saving hugely on PCI-E bandwidth.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
- en: As we move from PCI-E 2.0 (5 GB/s) to PCI-E 3.0 the bandwidth per PCI-E slot
    should effectively double, significantly alleviating this problem for GPU devices
    supporting the new PCI-E 3.0 standard. As of the start of 2012 we saw motherboards
    start to support PCI-E 3.0 standard with the Ivybridge/Ivybridge-E processor.
    PCI-E graphics cards will start to appear through 2012 and beyond. In addition
    to increased PCI-E bandwidth came increased host memory bandwidth.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
- en: This also highlights another point that we’ve made throughout this book. The
    CPU can be a useful partner in dealing with all the simple problems in conjunction
    with a GPU. For example, where tiles of data need to communicate, it can process
    the halo cases where they need to share data while the GPU is processing the bulk
    of the data. Often such cases present a lot of branching, which is not efficient
    on the GPU and therefore can be better suited to a cooperative approach.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  id: totrans-1064
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • There are now well-documented sources that detail algorithms for specific
    fields. Many are available in the form of plug-in libraries.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: • Be aware that not all parallel algorithms have obvious implementations on
    GPUs. Consider factors such as coalescing and communications when thinking about
    how to implement such algorithms.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: • New functions such as `__syncthreads_count` may have been introduced to address
    certain types of problems as the API develops. Study carefully the various additions
    to the API and understand possible usage.
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
- en: • Use multiple elements per thread wherever possible. However, using too many
    elements per thread may adversely affect performance.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: • As our reduction example shows, the simplest kernel is often the slowest.
    To achieve the absolute best performance often takes significant programming time
    and a good understanding of the underlying hardware.
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
- en: • A multicore CPU is more than a capable partner in calculating workloads, but
    will often be memory bandwidth constrained, which in turn may limit your ability
    to make effective use of all the cores.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: • OpenMP can provide an easy-to-use multithreaded interface for threads on the
    CPU side and is included as part of the standard CUDA compiler SDK.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 6: Resource Contentions'
  id: totrans-1072
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Identifying bottlenecks
  id: totrans-1073
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s often not clear to a programmer what, if anything, is wrong with a program.
    Most GPU programs, if they contain a reasonable amount of work for the GPU to
    do, show significant performance gains over their CPU counterparts. The question
    is how much is significant? The problem this question raises is that GPUs can
    be very good at some tasks, adequate at other tasks, and terrible with certain
    tasks. Anything that has a lot of arithmetic work and can be split into many independent
    problems works well.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that have significant branching or are mostly sequential are not
    suited to GPU, or most parallel architectures for that matter. In going down the
    parallel route, you almost always see a tradeoff of single-thread performance
    versus multiple-thread performance. The GPUs are typically clocked at up to 1000
    MHz, one-third or less than that of a typical CPU. They contain none of the fancy
    branch prediction logic that is necessary for large pipelines.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
- en: The CPU has had decades of development and we’re pretty much at the end game
    of any significant single-thread performance gains. Consequently, largely serial
    code performs terribly on a GPU compared to a CPU. This may change with future
    hybrid architectures, especially if we see them include the dedicated CPU as it
    is proposed with NVIDIA’s “Project Denver.” This aims to embed an ARM-based CPU
    core into the GPU fabric. We already see the inclusion of GPU elements onto common
    CPU platforms, so it’s fairly certain the future for both the CPU and GPU world
    is likely to be a hybrid, taking the most useful parts of each.
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: However, restricting ourselves to the data parallel problems that run well on
    current GPUs, what is a good baseline for your kernel? What should you compare
    it against? What is a realistic target?
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many fields now where CUDA is used to accelerate problems. One of
    the best resources to provide both some idea of what you can achieve and to see
    if there is already a solution that you can just buy in is [http://www.nvidia.com/object/cuda_app_tesla.html](http://www.nvidia.com/object/cuda_app_tesla.html).
    Here they list the following types of applications:'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
- en: • Government and Defense
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: • Molecular Dynamic, Computation Chemistry
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
- en: • Life Sciences, Bio-Informatics
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
- en: • Electrodynamics and Electromagnetic
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
- en: • Medical Imagining, CR, MRI
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
- en: • Oil and Gas
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
- en: • Financial Computing and Options Pricing
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
- en: • Matlab, Labview, Mathematica
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
- en: • Electronic Design Automation
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: • Weather and Ocean Modeling
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: • Video, Imaging, and Vision Applications
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if your field is options pricing, you can go to the relevant section,
    browse through a few of the sites, and see that the Monte Carlo pricing model
    is somewhere from a 30× to 50× speedup over a single-core CPU according to the
    particular vendor’s analysis. Of course, you have to ask what CPU, what clock
    speed, how many cores were used, etc. to get a reasonable comparison. You also
    have to remember that any vendor-provided figures are trying to sell their product.
    Thus, any figures will be the best case and may well ignore certain difficult
    aspects of the problem to present a more compelling reason to purchase their product
    over their competitor’s product. However, a few hours of research can tell you
    what would be a reasonable target figure for your particular field. You will also
    get an appreciation of what other people have done and more importantly what still
    needs to be developed.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
- en: However, don’t be disappointed with your initial GPU results in comparison with
    many of these applications. Often these arise from years of effort, which can
    be a great advantage, but can also mean they have to carry a lot of legacy code.
    A new approach to the problem, or a long forgotten approach used in the time of
    vector machines, may be the best approach today.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: Also remember that many of these projects are from startup companies, although
    as CUDA has become more mainstream, there are now more and more corporate offerings.
    Often startups come from talented PhD students who want to continue their field
    of research or thesis into the commercial world. Thus, they often contain a small
    number of individuals who understand a particular problem domain well, but who
    may not come from a computing background. Thus, as someone with a detailed understanding
    of CUDA *and* a detailed understanding of the application field, you may well
    be able to do much better than the existing commercial or research offerings.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: Analysis tools
  id: totrans-1093
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Visual profiler
  id: totrans-1094
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One of the first places to start, at least with existing code, is the analysis
    tools provided with the SDK. The first of these is the NVIDIA Visual Profiler
    tool. This is a multiplatform tool. It has the very useful feature of pointing
    out what it thinks is wrong with your kernel, at least pointing you toward what
    you need to do.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
- en: To use this tool, you simply compile your CUDA kernel and then select File→New
    Session, selecting the executable you just created. You can also input any working
    directory and command line arguments if applicable. Finally, you have to tell
    the profiler how long the application run is, so it knows when the kernel has
    simply crashed and does not wait forever to start processing the results. Note
    with Windows, you need to disable the default Aero desktop and select the standard
    desktop theme.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: You are probably unlikely to be able to see the detail on the timeline in [Figure
    9.38](#F0195), but should be able to make out the major sections. The first thing
    that is striking about the timeline is how little compute is being performed (the
    green bar in the middle of the figure). This is a series of kernels using the
    default stream in sequence on a number of GPUs. We see that using the default
    stream causes implicit synchronization and the huge impact this has on overall
    timing.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-38-9780124159334.jpg)'
  id: totrans-1098
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.38 Visual Profiler timeline.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
- en: Switching to a streaming example now, we get a different view. Here we can see
    a kernel pushed into a stream with a `memcpy to` and `memcpy from` device around
    it. Although we can see the two GPUs are being used together this time, the tool
    warns us that there is little kernel memory transfer overlap. This is entirely
    correct. It’s caused by the fact that a typical kernel will have some input data
    *and* some output data. Although on all Fermi devices there are two `memcpy` engines
    in the physical hardware, only one is enabled in consumer devices such as the
    GTX470 and GTX460 used here. Thus, all transfers must go into the same `memcpy`
    stream and be executed in order. As the kernel does a “copy to” followed by a
    “copy from” on the first stream, the subsequent stream’s “copy to” gets held up.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
- en: Thus, on Tesla devices where both copy engines are present, we do not see such
    an issue. For consumer-level hardware, we need to adopt a different approach.
    We simply do not issue any copy back transfers into the streams, until such time
    as all the `memcpy to` and kernel invocations have been issued. At this point
    we then push a set of “copy back” commands into the streams and do the transfers.
    There may be some kernel overlap with the last kernel and transfer back, but this
    will be minimal.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: The other issue the analysis presents is the bandwidth to and from the device
    is being underutilized (the “Low Memcpy/Compute Overlap” message). In this example,
    we’re using 32 MB chunks of data. If you look back to earlier sections of this
    chapter, you’ll see this is plenty enough to achieve the peak bandwidth of the
    PCI-E bus. However, this issue here is the compute part is taking up most of the
    time. Even if we were to overlap the transfer and kernel execution, the benefit
    would be marginal. Therefore, it’s important to understand the implications of
    what exactly the tools are telling you and if the associated effort will actually
    be worth the saving in execution time.
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: Overall it’s a very useful tool and quite easy to set up and use. It produces
    reasonable results quite quickly and is supported on multiple platforms ([Figure
    9.39](#F0200)).
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-39-9780124159334.jpg)'
  id: totrans-1104
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.39 Visual Profiler, multi-GPU.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight
  id: totrans-1106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Visual Profiler can, unfortunately, only tell you so much. A much better level
    of detail can be found with the Parallel Nsight tool, which is a Windows-only
    visual analyzer and debugger. Even if Windows is not your primary development
    environment, it’s worth dedicating a spare PC to this tool for its analysis features
    alone.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight is a far more in-depth tool than Visual Profiler. It will tell
    you a lot more about the kernels and what they are doing. However, as with any
    more complex tool, it takes a little time to learn how to use it well. The Visual
    Profiler tool is far simpler to set up and use. It’s a beginner’s tool, whereas
    Parallel Nsight is more of an intermediate to advanced tool.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight is best set up with a single PC using one or more compute 2.x
    (Fermi) graphics cards. Parallel Nsight will also run remotely using two PCs,
    each of which has a NVIDIA graphics card. However, you’ll find it much easier
    to have one PC, rather than wait whilst data is copied to/from a remote machine.
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight presents a number of options for debugging and profiling. The
    two main choices are “Application Trace” and “Profile.” The “Application Trace”
    feature allows you to generate a timeline as with Visual Profiler. This is particularly
    useful for seeing how the CPU interacts with the GPU and shows the times taken
    for host/device interaction. You should also use the timeline to verify correct
    operation of streams and overlapping kernel/memory copy operations.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
- en: Multiple concurrent GPU timelines are also supported. For example, the timeline
    in [Figure 9.40](#F0205) shows we’re failing to provide enough work to keep all
    GPUs busy. Only the computation parts are shown. The Fermi GPUs are shown in red
    as the first and last context, while the older GPUs are shown in green as the
    middle two bars. Each red square represents one kernel invocation on a given stream.
    You can see the first set of kernels end prior to the next set running. We have
    a huge time period where the first GPU is idle. It’s only through using tools
    such as Parallel Nsight you can see issues such as this. It’s difficult to see
    this using host/GPU timers alone.
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-40-9780124159334.jpg)'
  id: totrans-1112
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.40 Parallel Nsight, multi-GPU timeline.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: The next useful feature is the “Profile” option under the Activity Type menu
    ([Figure 9.41](#F0210)). This allows us to profile the CUDA kernels. However,
    as many of the experiments require multiple runs of the kernel, no timeline can
    be produced when selecting this option.
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-41-9780124159334.jpg)'
  id: totrans-1115
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.41 Parallel Nsight Activity Type selection.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Experiments to Run as “All” from the dropdown box is the simplest
    option. As you can see from the list of experiments in [Figure 9.42](#F0215),
    they are quite extensive. To start acquiring data, simply press the “Launch” button
    in the application control panel ([Figure 9.43](#F0220)). Note the green Connection
    Status circle. This tells you the Parallel Nsight monitor has successfully connected
    with the target devices. This needs to be green before any other options work.
    See the help options for details about setting up the monitor.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-42-9780124159334.jpg)'
  id: totrans-1118
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.42 Parallel Nsight Experiments.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-43-9780124159334.jpg)'
  id: totrans-1120
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.43 Parallel Nsight application Launch control.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: Once you press the “Launch” button your application will run until such time
    as it exits. You then will have a number of options in a dropdown box on the top
    of the screen, the last of which is “GPU Devices” ([Figure 9.44](#F0225)). Select
    this and you will see an overview of the GPU devices in the system.
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-44-9780124159334.jpg)'
  id: totrans-1123
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.44 Parallel Nsight, GPU devices present.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful dialog if you are not sure exactly what the properties of a
    particular device in your system are. Next, change the dropdown menu from “GPU
    Devices” to “CUDA Launches.” You’ll then see a list of kernels that were executed
    and various statistics. You’ll also find “Experimental Results” in the panel below
    the expandable list.
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, we have six kernels. We can see from the results
    a number of issues. First, none of the kernels achieve a theoretical occupancy
    above 33% ([Figure 9.45](#F0230)). In the case of the first kernel, this is caused
    by the block limit (8) being hit before we’ve achieved the maximum of 48 warps
    that can be resident on the device. Also note that the first kernel does not set
    the cache configuration and the CUDA runtime uses the `PREFER_SHARED` option,
    allocating 48 K to shared memory instead of the cache. As the kernel does not
    use shared memory, this is pointless. We’re missing a call in the host code to
    set to cache configuration to `PREFER_L1` prior to the first kernel call.
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-45-9780124159334.jpg)'
  id: totrans-1127
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.45 Parallel Nsight occupancy.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
- en: The next experiment to look at is the “Instruction Statistics” ([Figure 9.46](#F0235)).
    Here we see a few issues. There is a very high level of instructions that are
    being issued but not executed. This is indicative of the SM having to serialize
    and thus reissue the same instructions. We also see a huge spike of activity on
    SM 2\. This is in fact very bad, as it means one of the blocks that were allocated
    to this SM performed a huge amount of additional work compared with the other
    blocks. This indicates the blocks are not equally distributed in terms of work
    per block, and this is something we need to solve at the algorithm level. Some
    balancing of the work per block is needed.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-46-9780124159334.jpg)'
  id: totrans-1130
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.46 Parallel Nsight “Instruction Statistics.”
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
- en: The next experiment is the “Branch Statistics,” which tells us how much the
    execution within a warp diverges ([Figure 9.47](#F0240)). We ideally want a very
    small if not zero value for branch divergence. Here we see 16% of the branches
    diverge, which contributes to the reissuing of instructions we saw in the “Instruction
    Statistics” experiment. This too originates from the algorithm in that the amount
    of work per thread varies. It points to the need to balance the workload between
    the work blocks.
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-47-9780124159334.jpg)'
  id: totrans-1133
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.47 Parallel Nsight “Branch Statistics.”
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: The next experiment looks at the ability of the SM to issue and execute instructions.
    We’d expect to see a roughly equal distribute in terms of the “Active Warps per
    Cycle” chart. It shows that despite SM 2 taking a very long time to execute, it
    was actually only given a small number of warps to execute. This confirms that
    it was likely that one of the blocks given to it contained much more work than
    the other blocks. We also have a very low level of “Eligible Warps per Active
    Cycle,” which may in turn suggest the SMs are stalling at some point ([Figure
    9.48](#F0245)).
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-48-9780124159334.jpg)'
  id: totrans-1136
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.48 Parallel Nsight issue efficiency, eligible warps.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the next tab we see the distribution of instruction dependencies
    ([Figure 9.49](#F0250)). Instruction dependencies are caused by the output of
    one operation feeding into the input of the next. As the GPU uses a lazy evaluation
    model, the GPU operates best with long instruction dependencies.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-49-9780124159334.jpg)'
  id: totrans-1139
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.49 Parallel Nsight issue efficiency, instruction dependencies.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: The graph in [Figure 9.49](#F0250) shows there are too many immediate dependencies.
    The easiest method to solve this is by introducing some ILP on the thread level.
    As we in fact have very few blocks, we have a significant number of unused registers
    that could be used to introduce ILP. We could do this via the vector types or
    by expanding the loop to process *N* elements per iteration. We could also use
    one or more registers to prefetch the values from the next loop iteration.
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
- en: The next tab confirms what we saw in the “Eligible Warps” tab, that the SMs
    are in fact hitting a stall condition. The first pie chart in [Figure 9.50](#F0255)
    shows that in 69% of the time, the SM has no eligible warp to execute, meaning
    it will stall or idle, which is of course not good. The second pie chart in [Figure
    9.50](#F0255) shows the reason for the stall, which we can see is 85% of the time
    related to execution dependencies.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-50-9780124159334.jpg)'
  id: totrans-1143
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.50 Parallel Nsight issue efficiency, issue stalls.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: This can be solved in one of two ways. Currently, we have only 64 threads per
    block, meaning we get too few warps that are resident (16 out of a possible 48).
    Increasing the number of threads per block will increase the number of resident
    warps. From this perspective only, we’d need to move from 64 to 192 threads per
    block. This in itself may well resolve the issue. However, the effect of this
    issue on the overall timing is significantly less than issues concerning memory.
    Increasing the number of resident blocks will affect cache usage, which may have
    a bigger impact on the overall timing.
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
- en: We can see this in practice by looking at the total amount of data fetched from
    global memory by creating two versions, one that uses 128 threads per block and
    another that uses 64 threads per block. As we have registers to spare, we’ll also
    fetch 16 elements in the 64-register version and 12 elements in the 128-register
    version. This maximizes the register usage while still maintaining eight blocks
    per SM.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: Sure enough the “Warp Issue Efficiency” improves, reducing the “No Eligible”
    warps from 75% down to just 25%. The number of theoretical warps per SM also increases
    from 16 to 32 (13.25 versus 26.96 actual). The occupancy increases from 27% to
    56%. These are all improvements, but they are secondary effects. The kernel is
    performing a sort, so is likely, as with almost all sorts, to be memory bound.
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: In fact, when we compare the two kernels with the “CUDA Memory Statistics” experiment,
    there is a difference. The increased number of blocks per SM means that the ratio
    of L1 cache to each block is reduced. This in turn results in a doubling of the
    number of global memory fetch operations that are not cached in the L1 or L2 cache.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
- en: In the first kernel, using 64 threads per block, we achieve a 93.7% cache hit
    rate, which is very good ([Figure 9.51](#F0260)). Of the 6.3% of the transactions
    the L1 cache misses, the L2 cache picks up 30%, or around one-third. Thus, very
    few read transactions actually make it to global memory and we stay mostly on
    chip.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-51-9780124159334.jpg)'
  id: totrans-1150
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.51 Memory statistics, memory overview (256 blocks × 64 threads).
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: When we extend this to 128 threads per block, the overall number of blocks halves
    to 128 blocks in total ([Figure 9.52](#F0265)). However, this is not an issue,
    as with 14 SMs on the device and a maximum of eight resident blocks, we can only
    accommodate a maximum of 112 blocks at any given time anyway. Thus, we can increase
    the number of resident warps without any SMs running out of blocks.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-52-9780124159334.jpg)'
  id: totrans-1153
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.52 Memory statistics, memory overview (128 blocks × 128 threads).
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: Notice the problem with the cache hit ratio. Both the L1 and L2 caches achieve
    a lower hit ratio than before. The amount of memory fetched from global memory
    approximately doubles from 272 MB to 449 MB. This takes the execution time from
    35 ms to 46 ms, despite the apparent improvements in utilization of the SMs. Note
    that due to the allocation of one thread to each sample block, these memory fetches
    are all uncoalesced, so they are in fact very expensive.
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: Note that a design in which the threads from a thread block cooperated on sorting
    a single sample block would be far less sensitive to this effect. This analysis
    shows us this dependency. Through using a different mapping of threads to work
    in the sort stage, or by balancing or adjusting the bin boundaries, we may well
    be able to significantly improve the throughput.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: Resolving bottlenecks
  id: totrans-1157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s all very well knowing what the code you are running is doing, but it’s
    often another matter to both understand and fix the issue. The three types of
    bottlenecks you typically see, in order of importance, are:'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: • PCI-E transfer bottlenecks
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
- en: • Memory bandwidth bottlenecks
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: • Compute bottlenecks
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: PCI-E transfer bottlenecks
  id: totrans-1162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCI-E transfer bottlenecks are often a key consideration. As we saw from the
    earlier sections, PCI-E bus bandwidth is limited and you can expect to achieve
    a peak of around 5 GB/s on PCI-E 2.0 depending on the host hardware. However,
    to achieve this peak you need to be using pinned memory and an appropriately sized
    transfer. Adding more GPUs to a node typically reduces the overall bandwidth,
    but allows the overall amount of GPU to be increased. If you can keep everything
    in the GPU memory space, be that a single Tesla GPU or multiple GPUs, then the
    transfer cost can be eliminated from the equation. The extent of the reduction
    in bandwidth by adding more cards is very much dependent on the host hardware.
    You therefore need to be aware of how much data you are transferring and its usage.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: Compression techniques are one way to increase this apparently hard limit on
    PCI-E transfer rates. Do you really need to transfer all the data you are sending?
    For example, image data often contains an alpha channel that is used for transparency.
    If you are not using this on the GPU, then you can discard it and transfer from
    the host only the RGB (red, green, and blue) components, eliminating 25% of the
    data to be transferred. Although this may then mean you have 24 bits per pixel,
    the transfer time saving may significantly outweigh the nonaligned access pattern
    this might cause.
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: The other question is can you infer some data from others? This is very much
    problem dependent, but you may be able to compress the data using a simple algorithm
    such as run-length encoding. A long series of the same numbers can be replaced
    with a value, count pair and reconstructed at the GPU end in very little time.
    You may have lots of activity from a sensor and then no “interesting” activity
    for quite a period of time. Clearly, you can transfer the “interesting” data in
    full and either throw away the “uninteresting” data at the host end, or transfer
    it in some compressed form.
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: Interleaving transfer with computation using streams or zero-copy memory is
    another essential technique we have already covered. In the situation where your
    PCI-E transfer time is in excess of your kernel time, you effectively have the
    computation time for free. Without overlapping, the two times must be added and
    you end up with large gaps where no computation is taking place. See [Chapter
    8](CHP008.html) for more information on using streams.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: PCI-E is not the only transfer bottleneck you need to consider. The host will
    have a limit on the amount of memory bandwidth there is. Hosts such as the Intel
    Sandybridge-E processors use quad-banked memory, meaning they can achieve much
    higher host memory bandwidth than other solutions. Host memory bandwidth can also
    be saved by using P2P (Peer to Peer) transfers if your problem allows for this.
    Unfortunately, at the time of writing, to use the P2P function you need to use
    an OS other than Windows 7\. With the exception of those using Tesla cards and
    thus the TCC (Tesla Compute Cluster) driver, Windows 7 is the only major OS not
    currently supported for this feature.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: The speed at which the node can load and save data to storage devices, be they
    local devices or network devices, will also be a limiting factor. High-speed SSD
    drives connected in RAID 0 mode will help with this. These are all considerations
    for selecting host hardware. We look at a number of these in detail in [Chapter
    11](CHP011.html).
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
- en: Memory bottlenecks
  id: totrans-1169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assuming you can get the data on and off the GPU, the next issue is memory bandwidth
    to or from global memory. Moving data is expensive in terms of time and power
    usage. Therefore, being able to efficiently fetch/store and reuse data are essential
    criteria for selecting an appropriate algorithm. The GPU has huge amounts of compute
    resources, so an inefficient algorithm with a memory pattern favorable to a GPU
    (coalesced, tiled, high locality) may outperform a more computationally intensive
    algorithm that exhibits less GPU-friendly memory pattern.
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: When considering memory, think also about thread cooperation and appreciate
    the cooperation is best limited to a single block of threads. Generic algorithms
    that assume any thread can talk to any other thread are less useful than those
    that value locality of threads to one another. Algorithms designed for use on
    older vector machines are often far more efficient than those designed around
    distributing work over *N* independent processing nodes, as commonly found in
    today’s cluster machines.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: On modern GPUs, the L1 and L2 caches can significantly affect the execution
    time of kernels in sometimes rather unpredictable ways. Shared memory should be
    used where you have data reuse, want a more predictable outcome, or are developing
    for compute 1.x hardware. Even with the full 48 K allocation to the L1 cache,
    there is still 16 K of local shared memory storage available on each SM.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: A fully populated Fermi GPU has 16 SMs, so this amounts to a total of 256 K
    of high-speed memory in addition to the 768 K of L1 cache. This can be swapped,
    giving 768 K of programmer-managed shared memory and 256 K of L1 cache. Data reuse
    through either or both mechanisms is critical to achieving high throughput. This
    is typically achieved by ensuring locality of the calculation. Instead of multiple
    passes over large datasets, break the data into tiny tiles, use multiple passes
    over individual tiles, and then repeat for the other tiles. This allows the data
    to remain on chip throughout whatever transformation is being made on it, without
    multiple read/writes to and from global memory.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: Memory coalescing is key to achieving high memory throughput, although a sufficiently
    high number of memory transactions is also required. On Fermi and Kepler devices,
    to achieve anything like the full bandwidth when using 32-bit values per thread
    (i.e., floats or integers), you need to have the GPU almost fully populated with
    threads (48 to 64 resident warps, 1536 to 2048 threads per SM). Increased transaction
    sizes through the use of the various vector types help improve both ILP and memory
    bandwidth. Having each thread process four values instead of one tends to work
    well for many applications.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: Compute bottlenecks
  id: totrans-1175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Complexity
  id: totrans-1176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Surprisingly, despite the immense computing throughput of the GPU, there are
    still problems that are compute bound. These are usually problems where the overall
    amount of data is very large, such as the various forms of medical image scanning
    or data processing from devices that generate large amounts of sample data. These
    types of problems were previously processed on clusters. However, now due to the
    huge processing power available from a multi-GPU computer, many problems can be
    processed on a single standalone PC.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that contain a lot of computations work really well on GPUs compared
    to their CPU counterparts. However, algorithms that also include a lot of control
    complexity do not. Take the example of boundary cells in a typical tiled algorithm.
    If the cells collect data from their immediate neighbors, then a cell at the corner
    of a tile needs to collect data from the corner points of three other tiles.
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 9.53](#F0270) you can see there is a large block of green cells in
    the centre that have no boundary condition. They can safely calculate some value
    from the surrounding cells within the current block. Unfortunately, some programmers
    write programs that deal with the problem cases first. Thus, their kernel goes
    along the lines
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-53-9780124159334.jpg)'
  id: totrans-1180
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.53 Halo cells needed.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE286]'
  id: totrans-1182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: '`else if (right row)`'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  id: totrans-1184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: Particularly, control complex algorithms are not well suited to GPUs. If each
    thread runs the same kernel, the center elements have nine conditions to test
    before the thread does any work on them. Reversing the order of the tests, so
    the center elements are tested first, means we need four boundary tests. This
    would be an improvement, but is still far from optimal. The solution is to write
    customized kernels for each special case or let the CPU handle these complex conditionals.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: The type of problem here is a stencil one, where cells *N* levels from the center
    contribute in some way to the result. In this simple example, *N* is 1, as the
    immediate neighbors are used. As *N* is increased, typically some factor is applied,
    as values that are a long way from the center often do not contribute as much
    to the result.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
- en: As each cell will need values from the surrounding cells, each cell value will
    be read multiple times. Thus, a common approach to such problems is to use many
    threads to read a tile of data into shared memory. This allows for high-performance
    coalesced access to global memory, both when reading the data and also when writing
    it back. However, shared memory is not visible between blocks and there is no
    mechanism to pass shared data directly between blocks. This is due to the design
    of CUDA where there is only ever a subset of the total number of blocks executing.
    Thus, shared memory is reused as old blocks are retired and new blocks scheduled.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to load the halo cells, the cells outside the boundary of our particular
    tile, you can either read them from global memory or also load these into shared
    memory. Reading the rows from global memory gives a nice coalesced memory pattern.
    However, the columns generate a number of separate memory transactions, one for
    each cell we load. As these cells may be read a number of times, reading the columns
    can be a memory-intensive operation that will limit performance. Thus, at least
    the columns are usually placed into shared memory.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
- en: Thus, writing multiple kernels is usually a good solution to the problem of
    eliminating the control flow complexity. We can have one kernel that handles corner
    elements, another for rows, another for columns, and another for the center elements.
    If appropriate, each of these can call a common routine that processes the data
    as a series of values, and now the complexity of where the data came from has
    been removed.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
- en: Note that for compute 1.x and compute 2.x different solutions are applicable.
    As compute 1.x hardware has no cache for global memory, each memory transaction
    would generate a considerable amount of latency. Thus, for these devices it can
    make sense to manually cache the necessary data from the surrounding tiles in
    shared memory or give the calculation to the CPU.
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
- en: However, compute 2.x devices have both an L1 and L2 cache. As each tile will
    have to process its own elements, it’s likely that the tiles above, above left,
    and left will have already been loaded into the cache by previous activity of
    other blocks. The tiles to the right, right bottom, and bottom will usually not
    be present unless there are multiple passes over quite a small dataset. Accessing
    these from global memory will bring them into the cache for the subsequent block.
    You can also explicitly request cache lines be brought into the cache using the
    prefetch PTX instruction (see PTX ISA).
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence of the caching, we can eliminate a large amount of the control
    complexity necessary to manage shared memory by simply selecting a 48 K L1 cache
    and not using shared memory at all. Elimination of complexity is often useful
    in speeding up compute bound kernels.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
- en: Instruction throughput
  id: totrans-1193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As with many processors, not all instructions take the same amount of time to
    execute on every device. Selecting the correct instruction mix for a given processor
    is something the compiler should be able to perform quite well, but it’s also
    something the programmer needs to be aware of.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
- en: First of all, you need to ensure you are targeting the correct binaries for
    your hardware. Ideally, you should have one compute level specification for each
    target hardware platform. In Visual Studio this is done in the project options
    and is something we’ve already covered. For those people using command line it’s
    the `-arch` flag that specifies this.
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
- en: In terms of single-precision floating-point operations, all compute levels achieve
    a throughput of one instruction per clock, per thread. Remember, however, as this
    is per thread. In absolute terms we need to consider this is warp wide times the
    number of simultaneous warps per SM times the number of SMs on the GPU. Thus on
    Kepler GTX680 we have a 32 wide warp x 8 warp dispatch x 8 SMs = 2048 instructions
    per clock. Now throughput is not the same as instruction latency. It may take
    up to the order of 20 clock cycles for the result to become available to feed
    into a subsequent operation. A series of floating-point operations fed into the
    instruction pipeline would therefore appear 20 cycles later, one each cycle. The
    throughput would be one instruction per cycle, per thread but the latency would
    be 20 cycles.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
- en: Double-precision floating-point hardware, however, does not achieve this. For
    compute 2.0 hardware, it’s half the speed of single precision. For compute 2.1
    hardware, it’s actually only one-third of the speed. Compute 2.1 hardware (GTX460/560)
    and compute 3.0 hardware (GTX680) was aimed more toward the gaming market, so
    it lacks the same level of double-precision floating-point performance.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
- en: We see a similar issue with 32-bit integer values. Add and logical instructions
    only run at full speed. All other integer instructions (multiply, multiply-add,
    shift, compare, etc.) run at half speed on compute 2.0 hardware and one-third
    speed on compute 2.1 hardware. As usual, division and modulus operations are the
    exception. These are expensive on all compute levels, taking “tens of instructions”
    on compute 1.x hardware and “below 20 instructions” on compute 2.x hardware [NVIDIA
    CUDA C Programming Guide, v4.1, [chapter 5](CHP005.html)].
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
- en: Type conversion instructions operate at half speed on compute 2.0 devices and
    one-third speed on compute 2.1 devices. These are necessary when 8- or 16-bit
    integer types are used, as the hardware supports only native integer types (32-bit
    on compute 2.x, 24-bit on compute 1.x). Thus, the addition of two byte values
    results in promotion of these values to two integer values. The subsequent result
    then again needs to be demoted to a byte value. Similarly, conversions to and
    from single-/double-precision floating-point values cause additional type conversion
    instructions to be inserted.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
- en: In C all whole numbers are by default signed integers. All numbers containing
    a decimal place are treated as double-precision floating-point values unless an
    `F` postfix is placed immediately after the number. Thus,
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE288]'
  id: totrans-1201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: creates a double-precision definition and
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE289]'
  id: totrans-1203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: creates a single-precision definition.
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
- en: Using a non-postfixed constant in a floating-point expression causes an implicit
    conversion to double precision during the calculation. An implicit conversion
    to single precision is also performing when the result is assigned to a single-precision
    variable. Thus, forgetting to use the `F` postfix is a common cause of creating
    unnecessary conversion instructions.
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization and atomics
  id: totrans-1206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Synchronization points are often necessary in many algorithms. Synchronization
    within a thread block is not costly, but does potentially impact performance.
    The CUDA scheduler will try to schedule up to sixteen blocks per SM, which it
    can do unless you start using larger numbers of threads (see [Chapter 5](CHP005.html)).
    As the number of threads increases, the number of blocks that can be scheduled
    decreases. This in itself is not too bad, but when combined with synchronization
    points it can lead to the SM stalling.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
- en: When a block performs a synchronization, a number of warps out of the available
    set (24 on compute 1.x, 48 on compute 2.x, 64 on compute 3.x) effectively drop
    out of the scheduling availability, as all but the last warp hits the synchronization
    point. In the extreme case of 1024 threads per block (two blocks per SM), up to
    half of the resident warps would be at the synchronization barrier. Without any
    ILP, the ability of the SM to hide memory latency through running multiple threads
    then becomes insufficient. The SM stops running at peak efficiency. Clearly, we
    want maximum throughput from all the SMs for as much time as possible.
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
- en: The solution to the synchronization issue is not to use large thread blocks.
    You should aim to fully populate the SM where possible, so 192 threads is an ideal
    number, which results in eight blocks per SM on compute 2.x hardware, 256 being
    better for compute 3.x hardware.
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, if we’re using interthread synchronization it is likely we’ll
    also need interblock synchronization. It’s more efficient to synchronize data
    between threads than between blocks. For block-based synchronization we need to
    use global memory, whereas interthread synchronization can be performed with shared
    memory. Thus, it’s a tradeoff between the two scenarios best resolved by simply
    running both and seeing which is the fastest.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations act very much like synchronization points in that all the
    threads in a warp have to line up one after another to perform the operation.
    It takes time for all the threads in a block to line up in groups of 32 to move
    through the atomic operation. However, unlike synchronization points, they are
    free to continue at full speed afterward. This helps in terms of increasing the
    availability of warps that can be run, but doesn’t help the overall execution
    time of the block. The block cannot be retired from the SM until all the threads
    have completed. Thus, a single atomic operation effectively serializes and spreads
    out, in terms of execution time, the warps in a given block. The block can’t finish
    until all the stragglers have completed.
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
- en: The effect of synchronization and atomics on your kernel can be seen using the
    “CUDA Issue Efficiency” experiment within Parallel Nsight.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
- en: Control flow
  id: totrans-1213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As we saw earlier, branch divergence can have a serious impact on execution
    time as both paths have to be executed separately. The compiler is aware of this
    and thus uses something called predication.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
- en: Most of the PTX instructions can be predicated using the `.p` notation of the
    PTX ISA. For example,
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE290]'
  id: totrans-1216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: Here we set up a predicate register in each thread, testing virtual register
    295 for the value 1 and setting predicate register 16 accordingly. In the next
    instruction the predicate register 16 is used to predicate the `bra` (branch to)
    instruction. Thus, only those threads meeting the test condition of the earlier
    `setp.eq.s32` instruction follow the branch. We could replace the branch with
    a `mov` or similar instruction. Typically, you see the compiler generate this
    for small `if-else` constructs. For example,
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE291]'
  id: totrans-1218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: will be translated to
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE292]'
  id: totrans-1220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE292]'
- en: 'This works well in avoiding branches, as in fact all threads in the warp execute
    the predicate instruction, but those threads without the predicate bit set simply
    ignore it. The compiler has a strong preference for predication, even when other
    approaches would be better. The criteria is simply based on the size of the body
    of the `if` statement. Consider the following example:'
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE293]'
  id: totrans-1222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '[PRE294]'
  id: totrans-1223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: '[PRE295]'
  id: totrans-1224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: '[PRE296]'
  id: totrans-1225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: This code simply selects one of *N* local variables (registers) based on an
    index. The local variables are individually named, as creating an array causes
    the compiler to place this into local memory. Unfortunately, the compiler implements
    a series of `if-else-if` type statements, which means at element 16 we have to
    perform 15 prior tests. I’d have expected it to implement a jump table, creating
    an assignment at the target of each jump. This would be two instructions, load
    `local_idx` into a register and then an indirect jump to some base address plus
    the value in the register. The jump table itself is set up at compile time.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
- en: Thus, you need to ensure the control flow you expect is the control flow the
    compiler generates. You can do this relatively easily by inspecting the PTX code
    and/or the actual target code if you are still unsure. Predication works well
    in many but not all instances.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  id: totrans-1228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Use profiling tools to really see into what is happening as opposed to what
    you *think* is happening.
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
- en: • Avoid overly complex kernels by generating a general case and exception case
    kernel, or by using the caching features to eliminate the complex kernel altogether.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
- en: • Understand how predication works in control flow.
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
- en: • Don’t assume the compiler will provide the same scope of optimizations found
    with more mature compilers. CUDA is still quite new and things will take time.
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 7: Self-Tuning Applications'
  id: totrans-1233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPU optimization is not like CPU optimization. Many techniques overlap, while
    others have undesirable effects. I’ve tried to cover the major areas of optimization
    in the preceding sections. However, optimization is never an exact science, not
    when practiced by human programmers anyway. There are lots of factors that need
    to be considered when designing code for the GPU. Getting an optimal solution
    is not easy and it takes considerable time to become familiar with what works,
    try different solutions, and understand why one works when another doesn’t.
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider some of the major factors:'
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
- en: • Transfer to and from the host.
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
- en: • Memory coalescing.
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
- en: • Launch configuration.
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
- en: • Theoretical and achieved occupancy.
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
- en: • Cache utilization.
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory usage/conflicts.
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
- en: • Branch divergence.
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
- en: • Instruction-level parallelism.
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
- en: • Device compute level.
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
- en: For someone starting out with CUDA, there is a lot to think about and it will
    take time to become proficient with each of these areas. However, the most challenging
    aspect of this is that what works on one device many not work on another. Throughout
    this book we’ve used the whole range of available devices and a number of different
    host platforms where necessary to highlight differences.
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
- en: In the same way as different CPUs provide different levels of performance and
    functionality, so do GPUs. The CPU world is largely stuck with an x86 architecture,
    which reflects design goals of a system designed to run serial programs. There
    have been many extensions to provide additional functionality, such as MMX, SSE,
    AVX, etc. The x86 instruction set is today translated within the hardware to micro-instructions,
    which can be really for any target hardware. Sandybridge is perhaps the best example
    of this, where the micro-instructions themselves are actually cached instead of
    the x86 assembly code instructions.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
- en: GPU hardware is also not fixed and has changed significantly since the first
    CUDA-enabled devices were released back in the GTX8800 times. CUDA compiles to
    PTX, a virtual assembly code, aimed at a parallel processor–like architecture.
    PTX can itself be compiled to many targets, including CPUs, as the cooperative
    thread array concept lends itself to implementation in most parallel hardware.
    However, as far as we’re concerned, it’s compiled to a specified compute level
    for various NVIDIA GPUs. Therefore, you need to be familiar with what a given
    compute level provides, that is you need to understand for what hardware you are
    writing code. This has always been the basis of good optimization. Trends toward
    abstraction, layering, and hiding the architecture are all aimed at programmer
    productivity, but often at the expense of performance.
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
- en: Not every programmer is interested in the intricate workings of the hardware.
    Even with the previous list of issues to consider you’re unlikely to get an optimal
    solution the first time, the second time, or the *N*th time without considerable
    thought and a lot of trial and error. Thus, one approach to this issue that works
    well is simply to ask the program to work out the best use of the hardware for
    a given problem. This can either be done on a small set of the problem or the
    real problem itself.
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the hardware
  id: totrans-1249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in any optimization process is to know what hardware is available
    and what it is. To find out how many GPUs we have, you simply call
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE297]'
  id: totrans-1251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: This sets whatever parameter you pass as `count` to the number of devices available.
    If there is no CUDA hardware available the function returns `cudaErrorNoDevice`.
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
- en: Then for each device found we need to know what its capabilities are. For this
    we call
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE298]'
  id: totrans-1254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: 'We covered in detail the properties of a device in [Chapter 8](CHP008.html)
    so will not repeat this here. You should, however, be interested in at least the
    following:'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
- en: • Members `major` and `minor` that, when combined, provide the compute level
    of the device.
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: • The `integrated` flag, especially when combined with the `canMapHostMemory`
    flag. This allows you to use zero-copy memory (covered in Strategy 3) and avoid
    memory copies to and from the device for devices of which the GPU memory is actually
    on the host.
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
- en: • The `totalGlobalMem` value so you can maximize the use of GPU memory and ensure
    you don’t try to allocate too much memory space on the GPU.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: • The `sharedMemPerBlock` value so you know how much shared memory is available
    per SM.
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
- en: • The `multiProcessorCount`, which is the number of SMs present in the device.
    Multiply this number by the number of blocks you are able to run on an SM. The
    occupancy calculator, the Visual Profiler, and Parallel Nsight will all tell you
    the number of blocks you can run for a given kernel. It’s typically up to eight
    but can be as many as 16 on Kepler. This is the minimum number of blocks you need
    to schedule to this GPU.
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
- en: 'This information gives us some bounds with which we can define the problem
    space. We then have two choices: either analyze offline the best solution or try
    to work it out at runtime. The offline approach generally leads to better results
    and can greatly increase your understanding of the issues involved and may cause
    you to redesign certain aspects of the program. The runtime approach is necessary
    for optimal performance, even after significant analysis has taken place.'
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the first part of the optimization takes place offline, during the development
    phase. If you are targeting multiple compute levels, you’ll need a suitable card
    to test your application on. For consumer cards as a whole the most popular NVIDIA
    cards have always been the 9800 (compute 1.1), 8800 (compute 1.0), GTX260 (compute
    1.3), and GTX460 (compute 2.1). For more modern DirectX 11 cards, the 460/560
    cards dominate, with a smaller number of power users opting for the more expensive
    470/570 cards. Our choice of hardware for this book pretty much reflects the market
    trends to make the figures presented as useful as possible for people developing
    mass-market applications.
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve been working with CUDA since it release on the 8800 series of cards,
    we have a number of consumer cards at hand. Clearly, many of these are no longer
    available for sale but can easily be purchased on eBay or elsewhere. All you need
    is a motherboard with four dual-spaced PCI-E connectors all running at the same
    speed when fully populated. The primary board used in the development of this
    book was the (AMD) MSI 790FX-GD70, although this has now been replaced with the
    MSI 890FXX-G70\. Note the newest 990FX board in the series no longer provides
    four double-spaced connectors.
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
- en: Device utilization
  id: totrans-1264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having identified what hardware we have, we then have to make use of it. If
    there are multiple GPUs in the system, as is often the case, then be sure to make
    use of them. Multi-GPU programming, as of the CUDA 4.x SDK, is now much easier
    than before, so be sure you are not leaving a 100% performance gain on the table
    because you’re only using a single GPU. See [Chapter 8](CHP008.html) for more
    information on this.
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
- en: All applications are different, so the same primary performance factors may
    not always be the same. However, many will be. Primary among these is the launch
    configuration. The first part of this is ensuring you have multiple targets set
    up in the build process, one target for each compute level you plan on supporting.
    The target code will automatically be selected based on which GPU you are running
    the kernel on. Make sure also before running any performance tests you have the
    “Release” version selected as the build target, something in itself that can provide
    up to a 2× performance improvement. You’re not going to release the debug version,
    so don’t select this as your build target, other than for testing.
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
- en: Next we need some sort of check to ensure correctness. I suggest you run the
    GPU code back to back with the CPU code and then do a memory compare (`memcmp`)
    on the output of the two identical tests. Note this will detect any error, even
    if the error is not significant. This is especially the case with floating point,
    as the order in which the operations are combined will cause small rounding/precision
    errors. In such cases your check needs to iterate through both results and see
    if the answers differ by whatever you consider to be significant (0.01, 0.001,
    0.0001, etc.) for your particular problem.
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of launch configuration we’re trying to optimize for the following:'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
- en: • Number of threads per block.
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
- en: • Overall number of blocks.
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
- en: • Work performed per thread (ILP).
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
- en: The answer for each of these will vary between compute levels. A simple `for`
    loop is all that is needed to iterate through all possible combinations and record
    the timings for each. Then at the end simply print a summary of the results.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
- en: In terms of threads per block, start at 1 and increase in powers of two until
    you reach 16\. Then increase the thread count in 16-step intervals until you reach
    512 threads per block. Depending on the kernel resource usage (registers, shared
    memory) you may not be able to reach 512 threads on the earlier compute devices,
    so scale this back as necessary for these devices only.
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
- en: Note that we chose 16 here as the increment value, rather than 32, the warp
    size. This is because warp divergence is half-warp based. Certain devices such
    as the GTX460s are actually based on three sets of 16 CUDA cores, rather than
    two as found in other compute levels. Thus, a number of threads that is a multiple
    of 48 may work better on such devices.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, you’ll find well-written kernels work best with 128, 192,
    or 256 threads per block. You should use a consistent scaling from one thread
    per block up to a peak point where the performance will level off and then fall
    away. The plateau is usually hit when you achieve the maximum number of resident
    warps per SM and thus the instruction and memory latency hiding is working at
    its peak.
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
- en: Using a slightly smaller number of threads (e.g., 192 instead of 256) is often
    desirable if this increases the number of resident blocks per SM. This usually
    provides for a better instruction mix, as more blocks increases the chance they
    will not all hit the same resource contention at the same time.
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
- en: If you are hitting the maximum performance at 16, 32, or 64 threads then this
    usually indicates there is a contention issue, or that your kernel is highly geared
    toward ILP and you are using a lot of registers per thread.
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a baseline figure for the ideal number of threads per block, try
    increasing the amount of work done by each thread to two or four elements using
    the various `vector_N` types (e.g., `int2`, `int4`, `float2`, `float4`, etc.).
    You’ll typically see this will improve performance further. The easiest way of
    doing this is to create additional functions with the same name and simply overload
    the kernel function. CUDA will call the appropriate kernel depending on the type
    passed to it at runtime.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
- en: Using the vector types will increase register usage, which in turn may decrease
    the number of resident blocks per SM. This in turn may improve cache utilization.
    Memory throughput will also likely be increased as the overall number of memory
    transactions falls. However, kernels with synchronization points may suffer as
    the number of resident blocks drops and the SM has less choice of which warps
    are available to execute.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
- en: As with many optimizations, the outcome is difficult to predict with any degree
    of certainty, as some factors play in your favor while others don’t. The best
    solution is to try it and see. Then work backwards, to understand what factor(s)
    are the primary ones and which are secondary. Don’t waste your time worrying about
    secondary factors unless the primary ones are already addressed.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
- en: Sampling performance
  id: totrans-1281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final part of a self-tuning application is sampling. Although you can build
    a good performance model around compute level and number of SMs, there are many
    other factors. The same card model may be produced using GDD3 and GDD5 memory,
    the latter having significantly more global memory bandwidth. The same card may
    be clocked internally at 600 MHz yet also appear as a 900 MHz model. An optimization
    strategy that works well for a card with 16 SMs may not work well on one with
    half that number and vice versa. A mobile processor in a laptop may have been
    put on a PCI-E X1 link and may have dedicated or shared memory with the host.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
- en: It’s impossible to collect every card and address every variation that your
    product might have to address. Even if you could do this, next week NVIDIA will
    release another card. This is of course mostly a problem for those people writing
    consumer applications, rather than the somewhat less diverse Tesla population
    of cards. Nonetheless, when a new card is released people first expect their existing
    applications to run on it, and second, if they have upgraded, to see a suitable
    performance boost.
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
- en: Sampling is the answer to this issue. Each card will have a peak value in terms
    of a launch configuration that works best for its particular setup. As we’ve seen
    in some of the tests run throughout this book, different cards prefer different
    setups. The Fermi cards work well with 192 or 256 threads per block, yet the prior
    GPUs work well with 128 and 192 threads per block. The compute 2.1 cards perform
    best with 64- or 128-byte memory fetches, mixed with ILP, instead of 32-byte memory
    fetches and a single element per thread. The earlier cards are hugely sensitive
    to thread/memory ordering when coalescing. Global memory bandwidth on these cards
    is a fraction of the newer models, yet they can perform to a similar level with
    some problems if shared memory is used well. The cache in Fermi can play a big
    part to the extent that very low thread numbers (32 or 64) can outperform higher
    occupancy rates if the data is then entirely contained in the cache.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
- en: When the program is installed, run a short test suite as part of the installation
    procedure. Run a loop through all feasible numbers of threads. Try ILP values
    from one to four elements per thread. Enable and disable shared memory usage.
    Run a number of experiments, repeating each a number of times, and average the
    result. Store in a data file or program configuration file the ideal values and
    for which GPU these relate. If the user later upgrades the CPU or GPU, then rerun
    the experiments and update the configuration. As long as you don’t do this on
    every startup, the user will be happy you are tuning the application to make the
    best possible use of their hardware.
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  id: totrans-1286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • There are too many factors to say with certainty the effect of a change without
    actually trying it.
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
- en: • Some experimentation is often required during development to get the optimal
    solution.
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
- en: • The optimal solution will be different on different hardware platforms.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
- en: • Write your applications to be aware of the different hardware out there and
    what works best on each platform, either statically or dynamically.
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-1291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve looked in detail at a number of strategies for trying to improve the throughput
    of your kernels with various examples throughout this chapter. You should be aware
    of the factors that affect performance and their relative importance (primary
    ones are transfers, memory/data patterns, and finally SM utilization).
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
- en: Correctness is a key issue in optimizing code. You cannot reliably optimize
    code without automatic regression testing. This doesn’t have to be hugely complex.
    A back-to-back run against a known working version with several known datasets
    is entirely sufficient. You should aim to spot 95% plus of the errors before any
    program leaves your desk. Testing is not the job of some test group, but your
    responsibility as a professional to produce reliable and working code. Optimization
    often breaks code and breaks it many times. The wrong answer in one minute instead
    of the correct answer in one hour is no use to anyone. Always test for correctness
    after every change and you’ll see the errors there and then, as and when they
    are introduced.
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
- en: You should also be aware that optimization is a time-consuming and iterative
    process that will grow your understanding of your code and how the hardware functions.
    This in turn will lead you to design and write better code from the outset as
    you become more familiar with what does and what does not work well on GPUs.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: Questions on Optimization
  id: totrans-1295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1. Take an existing program that has one or more GPU kernels. Run the Visual
    Profiler and Parallel Nsight to analyze the kernels. What are the key indicators
    you need to look for? How would you optimize this program?
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
- en: 2. A colleague brings a printout of a GPU kernel to you and asks your advice
    about how to make it run faster. What would be your advice?
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
- en: 3. Another colleague proposes to implement a web server using CUDA. Do you think
    this is a good idea? What issues, if any, would you expect with such a program?
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
- en: 4. Implement a shared memory version of the odd–even sort, which produces a
    single sorted list. What issues might you expect to deal with?
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: Answers
  id: totrans-1300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1. You should be looking first to the execution time of each kernel. If one
    or more kernels dominate the timing, then, until these are optimized, trying to
    optimize the others is a waste of your time.
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
- en: Second, you should be looking to the timeline, specifically concerning transfers.
    Are they overlapped with kernel operations and are they using pinned memory or
    not? Is the GPU busy all the time or only periodically given work by the host?
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
- en: Of the two longest executing kernels, what is causing them to take this time?
    Is there a sufficient number of threads overall? Are there enough blocks to populate
    all the SMs? Are there any peaks on one SM, and if so, why? What is the thread
    to memory pattern and can this be coalesced by the hardware? Are there any serialization
    points, for example, shared memory bank conflicts, atomics, synchronization points?
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
- en: 2. First, you need to understand the problem before looking at specifics. The
    “look at the code” optimization strategy can be hit or miss. Sure you can probably
    optimize the code on the paper in some way, but you need much more information
    to provide a good answer to the question the person is really asking.
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
- en: Probably the best answer would be to tell your colleague to profile the application,
    including the host timeline, and then come back with the results. In doing this
    they will likely see what the problems are and these may well not even be related
    to the original kernel printout.
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
- en: 3. Applications that are highly data parallel are well suited to GPUs. Applications
    that are highly task parallel with lots of divergence threads are not. The typical
    implementation of a web server on a CPU is to spawn one thread per *N* connections
    and to distribute connections dynamically over a cluster of servers to prevent
    overloading any single node.
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
- en: GPUs execute code in groups of 32 warps, effectively a vector processor with
    the ability to follow single-thread control flow when necessary, but at a large
    performance penalty. Constructing in real time a dynamic web page is very expensive
    in terms of control flow, a significant amount of which will diverge on a per-user
    basis. PCI-E transfers would be small and not efficient.
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
- en: A GPU would not be a good choice, with the CPU host being a much better choice.
    However, the GPU may be able to be used in the back-end operations of the server,
    performing some analytical work, churning through the user-generated data to make
    sense of it, etc.
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
- en: 4. This is a useful exercise to think about how to solve some open-ended problems.
    First, the question does not specify how to combine the output of *N* blocks.
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
- en: The quickest solution for largest datasets should be the sample sort method
    as it completely eliminates the merge sort step. The framework for sample sort
    is provided in the text, but is nonetheless quite a complex sort. However, it
    suffers from a variable number of elements per bin. A prefix sum that padded the
    bins to 128-byte boundaries would help significantly.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
- en: Merge sort is much easier to implement, allows for fixed block sizes, and is
    what I’d expect most implementations to opt for.
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the odd/even sort, the coalescing problems with global memory are
    largely hidden by the cache in Fermi due to the locality being extremely high.
    A compute 1.x implementation would need to use shared memory/registers for the
    sort. It would need to access the global memory in a coalesced manner in terms
    of loading and writing back.
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
- en: '[¹](CHP009.html#CFN1)L1 cache is only available on Fermi architecture and is
    configurable between 16 KB and 48 KB. L1 cache on GT200/G80 is only via texture
    memory that is 24 KB in size.'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
- en: '[²](CHP009.html#CFN2)L2 cache is zero K on compute 1.x devices, up to 768 K
    on compute 2.x (Fermi) devices and up to 1536 K on compute 3.x (Kepler) devices.'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
- en: '[³](#CFN3)Mark Harris, NVIDIA Developer Technology, “Optimizing Parallel Reduction
    in CUDA,” 2007.'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
