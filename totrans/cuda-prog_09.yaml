- en: Chapter 9
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章
- en: Optimizing Your Application
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化你的应用程序
- en: In this chapter we provide a detailed breakdown of the main areas that limit
    performance in CUDA. Each section contains small examples to illustrate the issues.
    They should be read in order. The previous chapters introduced you to CUDA and
    programming GPUs. The sections here assume you have read the previous chapters
    and are comfortable with the concepts introduced there, or are already familiar
    with CUDA and are specifically interested in techniques for improving execution
    speed of your programs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细分析了限制 CUDA 性能的主要领域。每个部分都包含小示例来说明问题。它们应按顺序阅读。前几章介绍了 CUDA 和 GPU 编程。本章假设你已经阅读了前面的章节，并对其中介绍的概念感到熟悉，或者你已经熟悉
    CUDA，并且特别关心提高程序执行速度的技术。
- en: 'This chapter is broken up into a number of strategies:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为多个策略：
- en: 'Strategy 1: Understanding the problem and breaking it down correctly into serial
    and parallel workloads.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 1：理解问题并正确地将其分解为串行和并行工作负载。
- en: 'Strategy 2: Understanding and optimizing for memory bandwidth, latency and
    cache usage.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 2：理解并优化内存带宽、延迟和缓存使用。
- en: 'Strategy 3: Understanding the implications of needing to transfer data to or
    from the host. A look at the effects of pinned and zero-copy memory and bandwidth
    limits on a selection of hardware.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 3：理解将数据传输到主机或从主机传输数据的影响。探讨固定内存、零拷贝内存和带宽限制对一些硬件的影响。
- en: 'Strategy 4: Understanding the threading and computational abilities in detail
    and how these impact performance.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 4：深入理解线程和计算能力，以及它们如何影响性能。
- en: 'Strategy 5: Where to look for algorithm implementations, with a couple of examples
    of optimization of some general-purpose algorithms.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 5：在哪里寻找算法实现，并举例说明一些通用算法的优化。
- en: 'Strategy 6: Focus on profiling and identifying where in your applications the
    bottlenecks are occurring and why.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 6：专注于性能分析，找出应用程序中的瓶颈及其原因。
- en: 'Strategy 7: A look at how applications can tune themselves to the various hardware
    implementations out there.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 7：应用程序如何根据各种硬件实现进行自我调优的探讨。
- en: 'Strategy 1: Parallel/Serial GPU/CPU Problem Breakdown'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 1：并行/串行 GPU/CPU 问题分解
- en: Analyzing the problem
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析问题
- en: This is the first step in considering if trying to parallelize a problem is
    really the correct solution. Let’s look at some of the issues involved here.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是考虑将问题并行化是否真的正确解决方案的第一步。让我们看看其中的一些问题。
- en: Time
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间
- en: It’s important to define what an “acceptable” time period is for the execution
    time of the algorithm you have in mind. Now acceptable does not have to mean the
    best time humanly possible. When considering optimization, you have to realize
    as a software professional, your time costs money, and if you work in the western
    world, your time is not cheap. The faster a program needs to execute, the more
    effort is involved in making this happen [(Figure 9.1)](#F0010).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 确定你所考虑的算法执行时间的“可接受”时间段是很重要的。这里的可接受并不意味着最短的最佳时间。在考虑优化时，你必须意识到，作为一名软件专业人士，你的时间是有价值的，如果你在西方国家工作，你的时间并不便宜。程序执行所需的速度越快，涉及的工作就越多，来实现这一点的努力也就越大[(图
    9.1)](#F0010)。
- en: '![image](../images/F000090f09-01-9780124159334.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-01-9780124159334.jpg)'
- en: FIGURE 9.1 Programmer time versus speedup achieved.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 程序员时间与加速效果的关系。
- en: You will usually find with any optimization activity there is a certain amount
    of so-called “low-hanging fruit.” The changes required are easy and lead to a
    reasonable speedup. As these are removed, it becomes progressively harder to find
    optimizations and these require more complex restructuring, making them more costly
    in terms of time and the potential for errors they can introduce.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在任何优化活动中，你会发现有一些所谓的“低垂的果实”。所需的更改很简单，并能带来合理的加速效果。当这些被去除后，找到优化点会变得越来越困难，这些优化往往需要更复杂的重构，从而在时间和可能引入的错误方面变得更加昂贵。
- en: In most western countries, programming effort is quite expensive. Even if your
    programming time is free—for example, if you are student working on a project—time
    spent optimizing is still time that could be spent doing other activities. As
    engineers, we can sometimes get caught up in making things better than they need
    to be. Understand what is required and set a suitable goal.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数西方国家，编程的成本相当高。即使你的编程时间是免费的——比如说，你是一个正在做项目的学生——优化所花费的时间仍然是本可以用于其他活动的时间。作为工程师，我们有时会陷入让事情变得比必要的更完美的境地。了解需求并设定适当的目标。
- en: In setting a suitable speedup goal, you have to be aware of what is reasonable,
    given a set of hardware. If you have 20 terabytes of data that needs to be processed
    in a few seconds, a single-GPU machine is just not going to be able to cope. You
    have exactly this sort of issue when you consider Internet search engines. They
    have to, within seconds, return a set of search results to the user. Yet at the
    same time, it used to be “acceptable” for their indexes to take several days to
    update—that is, the time taken for them to pick up new content. In this modern
    world, even this is considered slow. Thus, what is acceptable today may not be
    acceptable tomorrow, next month, or next year.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在设定适当的加速目标时，你必须意识到，在给定的硬件条件下，什么是合理的。如果你有20TB的数据需要在几秒钟内处理，单个GPU机器是无法应对的。当你考虑到互联网搜索引擎时，正好会遇到这种问题。它们必须在几秒钟内返回一组搜索结果给用户。然而，曾几何时，搜索引擎的索引更新需要几天时间——也就是，它们需要多长时间才能获取到新的内容。在这个现代社会，即便这样也被认为是慢的。因此，今天可以接受的标准，可能在明天、下个月或明年就不再能接受了。
- en: In considering what the acceptable time is, ask yourself how far away you currently
    are from this. If it’s a factor of two or less, often it will be worth spending
    time optimizing the CPU implementation, rather than creating an entirely new,
    parallel approach to the problem. Multiple threads introduce all sorts of problems
    of dependencies, deadlock, synchronization, debugging, etc. If you can live with
    the serial CPU version, this may be a better solution in the short term.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑可接受的时间时，问问自己，你距离这个目标有多远。如果差距只有两倍以内，通常花时间优化CPU实现会比创造一种全新的并行方法来解决问题更值得。多个线程会引入各种各样的依赖性、死锁、同步、调试等问题。如果你能接受串行的CPU版本，这在短期内可能是一个更好的解决方案。
- en: Consider also the easy-fix solution to problems used for the past 30 or so years.
    Simply buy some faster hardware. Use profiling to identify where the application
    is spending it time to determine where it’s bound. Is there an input/output (I/O)
    bottleneck, a memory bottleneck, or a processor bottleneck? Buy a high-speed PCI-E
    RAID card and use SATA 3/SAS SSD drives for I/O issues. Move to a socket 2011
    system with a high clock rate on the memory, if memory bandwidth is an issue.
    If it’s simply raw compute throughput, install an Extreme Edition or Black Edition
    processor with the highest clock rate you can buy. Purchase an out-of-the-box,
    liquid-cooled, Sandybridge K or X series overclocked processor solution. These
    solutions typically cost much less than $3,000–$6,000 USD, a budget you could
    easily spend on programming time to convert a program from a serial to a parallel
    program.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑过去30年左右使用的简单解决方案。只需购买一些更快的硬件。使用分析工具找出应用程序花费时间最多的地方，确定其瓶颈所在。是输入/输出（I/O）瓶颈，内存瓶颈，还是处理器瓶颈？如果是I/O问题，可以购买一张高速PCI-E
    RAID卡，使用SATA 3/SAS SSD硬盘。如果是内存带宽问题，换一个内存时钟速率更高的Socket 2011系统。如果是单纯的计算吞吐量问题，可以安装一款极限版或黑版的处理器，选择你能买到的最高时钟速率。也可以购买一款现成的，液冷的Sandybridge
    K系列或X系列超频处理器解决方案。这些解决方案的费用通常远低于$3,000–$6,000美元，这个预算，你完全可以用来支付将程序从串行转换为并行程序的编程时间。
- en: However, while this approach works well when you have a small amount of difference
    between where you are and where you want to be, it’s not always a good approach.
    A high clock rate means high power consumption. The processor manufacturers have
    already abandoned that route in favor of multicore as the only long-term solution
    to providing more compute power. While the “buy new hardware” approach may work
    in the short term, it’s not a long-term solution. Sometimes the hardware you have
    may not easily be changeable, because it’s provided by a restrictive IT department,
    or because you have insufficient funds to purchase new hardware but lots of “free”
    programming time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管当你面临的差距不大时，这种方法效果很好，但并不总是最佳选择。高时钟频率意味着高功耗。处理器制造商已经放弃了这种路线，转而支持多核处理作为提供更多计算能力的唯一长期解决方案。虽然“购买新硬件”的方法在短期内可能有效，但这不是长期的解决方案。有时你手头的硬件可能无法轻易更换，因为它是由限制性较强的
    IT 部门提供的，或者是因为你没有足够的资金购买新硬件，但却有很多“免费”的编程时间。
- en: If you decide to go down the GPU route, which for many problems is a very good
    solution, then you should typically set your design goal to be around a 10× (ten
    times) improvement in execution time of the program. The actual amount you achieve
    depends on the knowledge of the programmers and the time available, plus a huge
    contribution from the next issue we’ll talk about, how much parallelism there
    is in the application. At least a 2× or 3× speedup is a relatively easy goal,
    even for those new to CUDA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定走 GPU 路线，这对于许多问题来说是一个非常好的解决方案，那么你通常应该将设计目标设置为程序执行时间提高大约 10×（十倍）。你实际达到的效果取决于程序员的知识水平和可用时间，再加上我们接下来要讨论的一个重要因素——应用中并行性的程度。至少
    2× 或 3× 的加速是一个相对容易实现的目标，即使对于 CUDA 新手也是如此。
- en: Problem decomposition
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题分解
- en: 'The fundamental question here is simply this: Can the problem you have be broken
    down into chunks that can run in parallel; that is, is there an opportunity to
    exploit concurrency in the problem? If the answer is no, then the GPU is not the
    answer for you. You instead have to look at optimization techniques for the CPU,
    such as cache optimizations, memory optimizations, SIMD optimizations, etc. At
    least some of these we have covered on the GPU side in previous chapters and others
    are covered in this chapter. Many of these optimization techniques work very well
    on serial CPU code.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的根本问题其实就是：你面临的问题是否可以拆解成可以并行运行的任务块；也就是说，问题中是否有机会利用并发性？如果答案是否定的，那么 GPU 就不是你的解决方案。你需要考虑
    CPU 的优化技术，比如缓存优化、内存优化、SIMD 优化等。我们在前几章已经介绍了一些 GPU 方面的优化技巧，而这一章也会涉及其他的一些技巧。这些优化技术在串行
    CPU 代码中通常效果很好。
- en: Assuming you are able to partition the problem into concurrent chunks, the question
    then is how many? One of the main limiting factors with CPU parallelization is
    that there is often just not enough large-granularity (or coarse-grained) parallel
    work to be done. GPUs run thousands of threads, so the problem needs to be decomposed
    into thousands of blocks, not just a handful of concurrent tasks as with the CPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你能够将问题划分为并发的任务块，那么问题就是多少个块？CPU 并行化的一个主要限制因素是，通常没有足够的大粒度（或粗粒度）并行工作可做。GPU 可以运行成千上万的线程，因此问题需要被拆解为成千上万个块，而不仅仅是像
    CPU 那样的少数并发任务。
- en: The problem decomposition should always start with the data first and the tasks
    to be performed second. You should try to represent the problem in terms of the
    output dataset. Can you construct a formula that represents the value of a given
    output point in the dataset as a transformation of the input dataset *for that
    single point*? You may need more than one formula, for example, one for most data
    points and one for the data points around the edge of the problem space. If you
    can do this, then the transformation of a problem into the GPU space is relatively
    easy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 问题分解应该始终从数据开始，然后是要执行的任务。你应该尽量将问题表示为输出数据集的形式。你能否构造出一个公式，将数据集中给定输出点的值表示为输入数据集的*该单个点*的变换？你可能需要不止一个公式，例如，一个用于大多数数据点，另一个用于问题空间边缘的数据点。如果你能做到这一点，那么将问题转化为
    GPU 处理空间相对容易。
- en: One of the issues with this type of approach is that you need to fully understand
    the problem for the best benefit. You can’t simply peek at the highest CPU “hogs”
    and try to make them parallel. The real benefit of this approach comes from making
    the chain from the input data points to the output data points completely parallel.
    There may be parts of this chain where you could use 100,000 processors if you
    had the hardware and points where you are reduced to a few hundred processors.
    Rarely are any problems truly single threaded. It’s just that as programmers,
    scientists, and engineers, this is the solution we may have learned many years
    ago at university. Thus, seeing the potential parallelism in a problem is often
    the first hurdle.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个问题是，你需要完全理解问题才能获得最佳的效果。你不能仅仅查看CPU使用最多的“占用者”并尝试使它们并行。此方法的真正好处来自于使从输入数据点到输出数据点的链条完全并行。这个链条中可能有些部分，如果你有硬件，可能使用100,000个处理器，而某些部分则可能仅能使用几百个处理器。几乎没有任何问题是完全单线程的。只是作为程序员、科学家和工程师，这是我们多年前在大学里学到的解决方案。因此，识别问题中的潜在并行性通常是第一个障碍。
- en: Now there are some problems where this single-output data point view is not
    practical—H264 video encoding, for example. In this particular problem, there
    are a number of stages defined, each of which defines a variable-length output
    data stream. However, there are aspects—filtering, in particular—within image
    encoding/processing that easily lend themselves to such approaches. Here the destination
    pixel is a function of *N* source pixels. This analogy works well in many scientific
    problems. The value of the forces of a given destination atom can be written as
    the sum of all the atoms that apply a force to the given destination atom. Where
    the input set is very large, simply apply a threshold or cutoff point such that
    those input data points that contribute very little are excluded from the dataset.
    This will contribute a small amount of error, but in some problems allows a huge
    section of the dataset to be eliminated from the calculation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有些问题，在这种单输出数据点的视角下并不实际——例如H264视频编码。在这个特定的问题中，定义了多个阶段，每个阶段都定义了一个可变长度的输出数据流。然而，在图像编码/处理中，有一些方面，特别是滤波操作，非常适合采用这种方法。在这里，目标像素是*N*个源像素的函数。这个类比在许多科学问题中都非常有效。给定目标原子的力值可以表示为所有对该目标原子施加力的原子的力之和。当输入集非常大时，只需应用一个阈值或截止点，将那些贡献极少的输入数据点从数据集中排除。这会产生一些小的误差，但在某些问题中，这样做可以消除计算中的大量数据集部分。
- en: Optimization used to be about how to optimize the operations or functions being
    performed on the data. However, as compute capacity has increased hugely in comparison
    to memory bandwidth, it’s now the data that is the primary consideration. Despite
    the fact GPUs have on the order of 5 to 10 times the memory bandwidth of CPUs,
    you have to decompose the problem such that this bandwidth can be used. This is
    something we’ll talk about in the following section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 优化曾经主要是关于如何优化对数据执行的操作或函数。然而，随着计算能力与内存带宽相比大幅提升，现在主要的考虑因素变成了数据。尽管GPU的内存带宽大约是CPU的5到10倍，但你必须将问题分解，以便能够充分利用这种带宽。这是我们将在下一节讨论的内容。
- en: One final consideration here, if you plan to use multiple GPUs or multiple GPU
    nodes, is how to decompose the problem and the dataset over the processor elements.
    Communication between nodes will be *very* expensive in terms of computation cycles
    so it needs to be minimized and overlapped with computation. This is something
    we’ll touch on later.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最后一个考虑因素是，如果你计划使用多个GPU或多个GPU节点，那么如何将问题和数据集分解到处理器元素上。节点之间的通信在计算周期方面将是*非常*昂贵的，因此需要尽量减少，并与计算重叠进行。这是我们稍后将讨论的内容。
- en: Dependencies
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 依赖性
- en: A dependency is where some calculation requires the result of a previous calculation,
    be that some calculation in the problem domain or simply an array index calculation.
    In either case, the dependency causes a problem in terms of parallel execution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖性是指某些计算需要依赖先前计算的结果，无论是问题领域中的某些计算，还是仅仅是数组索引计算。在任何情况下，依赖性都会导致并行执行方面的问题。
- en: Dependencies are seen in two main forms, where one element is dependent on one
    or more elements around it, or where there are multiple passes over a dataset
    and there exists a dependency from one pass to the next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖性主要有两种形式，一种是一个元素依赖于周围一个或多个元素，另一种是对数据集进行多次遍历，并且存在从一次遍历到下一次遍历的依赖关系。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`}`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: If you consider this example, you can see that both `a` and `c` have a dependency
    on `b`. You can also see that `d` has a dependency on both `a` and `c`. The calculation
    of `a` and `c` can be done in parallel, but the calculation of `d` requires the
    calculation of both `a` and `c` to have completed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑这个例子，你会发现`a`和`c`都依赖于`b`。你也会看到`d`依赖于`a`和`c`。`a`和`c`的计算可以并行进行，但`d`的计算要求`a`和`c`都已完成计算。
- en: In a typical superscalar CPU, there are multiple independent pipelines. The
    independent calculations of `a` and `c` would likely be dispatched to separate
    execution units that would perform the multiply. However, the results of those
    calculations would be needed prior to being able to compute the addition operation
    for `a` and `c`. The result of this addition operation would also need to be available
    before the final multiplication operation could be applied.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的超标量CPU中，有多个独立的管道。`a`和`c`的独立计算可能会被分配到不同的执行单元，这些单元将执行乘法操作。然而，在能够计算`a`和`c`的加法操作之前，需要这些计算的结果。这次加法操作的结果也需要在最终的乘法操作应用之前准备好。
- en: This type of code arrangement allows for little parallelism and causes a number
    of stalls in the pipeline, as the results from one instruction must feed into
    the next. While stalled, the CPU and GPU would otherwise be idle. Clearly this
    is a waste, and both CPUs and GPUs use multiple threads to cover this problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种代码安排允许的并行性很少，并且会导致管道中的许多停顿，因为一条指令的结果必须传递给下一条指令。在停顿期间，CPU和GPU将处于空闲状态。显然，这是浪费，因此CPU和GPU都使用多个线程来解决这个问题。
- en: 'On the CPU side, instruction streams from other virtual CPU cores fill in the
    gaps in the instruction pipeline (e.g., hyperthreading). However, this requires
    that the CPU know from which thread the instruction in the pipeline belongs, which
    complicates the hardware. On the GPU, multiple threads are also used, but in a
    time-switching manner, so the latency of the arithmetic operations is hidden with
    little or no cost. In fact, on the GPU you need around 20 clocks to cover such
    latency. However, this latency need not come from another thread. Consider the
    following example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU端，来自其他虚拟CPU核心的指令流填补了指令管道中的空隙（例如超线程技术）。然而，这需要CPU知道管道中的指令来自哪个线程，这增加了硬件的复杂性。在GPU上，也使用多个线程，但以时间切换的方式进行，因此算术操作的延迟通过几乎没有成本或无成本的方式被隐藏。事实上，在GPU上，你大约需要20个时钟周期来覆盖这种延迟。然而，这种延迟不一定来自另一个线程。考虑以下示例：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here the code has been rearranged and some new terms introduced. Notice if you
    insert some independent instructions between the calculation of `a` and `c` and
    their use in `d`, you allow these calculations more time to complete before the
    result is obtained. The calculations of `f`, `g`, and `h` in the example are also
    overlapped with the `d` calculation. In effect, you are hiding the arithmetic
    execution latency through overlapping nondependent instructions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，代码已被重新排列，并引入了一些新的术语。注意，如果你在`a`和`c`的计算与它们在`d`中的使用之间插入一些独立的指令，你可以为这些计算提供更多时间，在得到结果之前完成计算。示例中的`f`、`g`和`h`的计算也与`d`的计算重叠。实际上，你通过重叠非依赖指令来隐藏算术执行延迟。
- en: One way of handling dependencies and introducing additional nondependent instructions
    is through a technique called loop fusion, as shown here.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 处理依赖关系并引入额外非依赖指令的一种方法是通过一种叫做循环融合（loop fusion）的技术，具体如图所示。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`}`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: In this example, we have two independent calculations for results `a` and `d`.
    The number of iterations required in the second calculation is more than the first.
    However, the iteration space of the two calculations overlaps. You can, therefore,
    move part of the second calculation into the loop body of the first, as shown
    in function `loop_fusion_example_fused_01`. This has the effect of introducing
    additional, nondependent instructions, plus reducing the overall number of iterations,
    in this example, by one-third. Loop iterations are not free, as they need a loop
    iteration value and cause a branch. Thus, discarding a third of them brings us
    a significant benefit in terms of reducing the number of instructions executed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们有两个独立的计算，分别得到结果`a`和`d`。第二个计算所需的迭代次数多于第一个。然而，这两个计算的迭代空间是重叠的。因此，你可以将第二个计算的一部分移动到第一个计算的循环体内，如函数`loop_fusion_example_fused_01`所示。这会引入额外的非依赖指令，并在这个例子中将总迭代次数减少三分之一。循环迭代并非免费的，因为它们需要循环迭代值并会导致分支。因此，去掉三分之一的迭代给我们带来了显著的好处，减少了执行的指令数量。
- en: In the `loop_fusion_example_fused_02` we can further fuse the two loops by eliminating
    the second loop and fusing the operation into the first, adjusting the loop index
    accordingly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在`loop_fusion_example_fused_02`中，我们可以通过消除第二个循环并将操作合并到第一个循环中来进一步融合这两个循环，同时相应地调整循环索引。
- en: Now in the GPU it’s likely these loops would be unrolled into threads and a
    single kernel would calculate the value of `a` and `d`. There are a number of
    solutions, but the most likely is one block of 100 threads calculating `a` with
    an additional block of 200 threads calculating `d`. By combining the two calculations,
    you eliminate the need for an additional block to calculate `d`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在GPU中，这些循环很可能会被展开为线程，并且一个内核会计算`a`和`d`的值。有多种解决方案，但最可能的情况是一个包含100个线程的块计算`a`，另一个包含200个线程的块计算`d`。通过将这两种计算结合在一起，您可以消除为计算`d`而需要的额外块。
- en: However, there is one word of caution with this approach. By performing such
    operations, you are reducing the overall amount of parallelism available for thread/block-based
    scheduling. If this is already only a small amount, this will hurt the execution
    time. Also be aware that kernels, when fused, will usually consume more temporary
    registers. This may limit the amount of fusion you can practically achieve, as
    it will limit the number of blocks scheduled on an SM due to increased register
    usage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，采用这种方法时需要注意一点。通过执行这些操作，您会减少线程/块调度可用的并行度。如果并行度本身就已经很小，这会影响执行时间。还要注意，内核在融合后通常会消耗更多的临时寄存器。这可能会限制您实际能实现的融合量，因为它会限制在一个SM上调度的块数量，原因是寄存器使用量增加。
- en: Finally, you should consider algorithms where there are multiple passes. These
    are typically implemented with a number of sequential kernel calls, one for each
    pass over the data. As each pass reads and writes global data, this is typically
    very inefficient. Many of these algorithms can be written as kernels that represent
    a single or small set of destination data point(s). This provides the opportunity
    to hold data in shared memory or registers and considerably increases the amount
    of work done by a given kernel, compared with the number of global memory accesses.
    This will vastly improve the execution times of most kernels.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您应该考虑那些需要多次迭代的算法。通常这些算法会通过多个顺序的内核调用来实现，每次迭代处理一部分数据。由于每次迭代都需要读取和写入全局数据，这通常效率较低。许多这类算法可以被编写成内核，代表单个或少量的目标数据点。这样可以将数据保存在共享内存或寄存器中，并大大增加每个内核所做的工作量，相较于全局内存的访问次数。这将大大提高大多数内核的执行速度。
- en: Dataset size
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集大小
- en: 'The size of the dataset makes a huge difference as to how a problem can be
    handled. These fall into a number of categories on a typical CPU implementation:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的大小对问题的处理方式有巨大影响。这些数据集在典型的CPU实现中属于多个类别：
- en: • Dataset within L1 cache (~16 KB to 32 KB)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L1缓存中（约16 KB至32 KB）
- en: • Dataset within L2 cache (~256 KB to 1 MB)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L2缓存中（约256 KB至1 MB）
- en: • Dataset within L3 cache (~512 K to 16 MB)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L3缓存中（约512 K至16 MB）
- en: • Dataset within host memory on one machine (~1 GB to 128 GB)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在一台机器的主机内存中（约1 GB至128 GB）
- en: • Dataset within host-persistent storage (~500 GB to ~20 TB)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在主机持久存储中（约500 GB至约20 TB）
- en: • Dataset distributed among many machines (>20 TB)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集分布在多台机器上（大于20 TB）
- en: 'With a GPU the list looks slightly different:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU时，列表略有不同：
- en: • Dataset within L1 cache (~16 KB to 48 KB)^([1](CHP009_a.html#FN1))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L1缓存中（约16 KB至48 KB）^([1](CHP009_a.html#FN1))
- en: • Dataset within L2 cache (~512 KB to 1536 MB)^([2](CHP009_a.html#FN2))
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在L2缓存中（约512 KB至1536 MB）^([2](CHP009_a.html#FN2))
- en: • Dataset within GPU memory (~512 K to 6 GB)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在GPU内存中（约512 K至6 GB）
- en: • Dataset within host memory on one machine (~1 GB to 128 GB)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在一台机器的主机内存中（约1 GB至128 GB）
- en: • Dataset within host-persistent storage (~500 GB to ~20 TB)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集在主机持久存储中（约500 GB至约20 TB）
- en: • Dataset distributed among many machines (>20 TB)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据集分布在多台机器上（大于20 TB）
- en: For very small problem sets, adding more CPU cores to a particular problem can
    result in a superscalar speedup. This is where you get more than a linear speedup
    by adding more CPU cores. What is happening in practice is that the dataset each
    processor core is given is now smaller. With a 16-core CPU, the problem space
    is typically reduced by a factor of 16\. If this now moves the problem from memory
    to the L3 cache or the L3 cache to the L2 cache, you see a very impressive speedup,
    not due to parallelism, but due instead to the much higher-memory bandwidth of
    the associated cache. Obviously the same applies when you transition from the
    L2 cache to holding the problem entirely in the L1 cache.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常小的问题集，向特定问题添加更多的CPU核心可能会带来超标量加速。这意味着通过添加更多的CPU核心，您获得的加速比线性加速更高。实际发生的情况是，每个处理器核心获得的数据集现在变得更小。对于16核的CPU，问题空间通常会缩小16倍。如果这将问题从内存移动到L3缓存，或者从L3缓存移动到L2缓存，您会看到非常显著的加速，这并非由于并行性，而是由于相关缓存的内存带宽更高。当然，当您将问题从L2缓存转移到完全存储在L1缓存时，情况也是如此。
- en: The major question for GPUs is not so much about cache, but about how much data
    can you hold on a single card. Transferring data to and from the host system is
    expensive in terms of compute time. To hide this, you overlap computation with
    data transfers. On the more advanced cards, you can do a transfer in and a transfer
    out at the same time. However, for this to work you need to use pinned memory
    on the host. As pinned memory can’t be swapped out by the virtual memory management
    system, it has to be real DRAM memory on the host.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU来说，主要的问题不是缓存，而是单个卡上可以容纳多少数据。将数据传输到主机系统和从主机传输回去是非常耗费计算时间的。为了掩盖这一点，您可以将计算和数据传输重叠进行。在更先进的显卡上，您可以同时进行一次数据传入和一次数据传出。然而，为了实现这一点，您需要在主机上使用固定内存。由于固定内存无法被虚拟内存管理系统交换出去，因此它必须是主机上的实际DRAM内存。
- en: On a 6 GB Tesla system you might have allocated this as a 1 GB input buffer,
    a 1 GB output buffer, and 4 GB compute or working memory. On commodity hardware,
    you have up to 2 GB available, so much less to work with, although some commodity
    cards support up to 4 GB of global memory.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个6 GB的Tesla系统上，您可能已将其分配为1 GB的输入缓冲区、1 GB的输出缓冲区和4 GB的计算或工作内存。在普通硬件上，您最多可用2 GB的内存，虽然一些普通卡支持高达4
    GB的全局内存，但可用的内存要少得多。
- en: On the host side, you need at least as much memory as you pin for the input
    and output buffers. You typically have up to 24 GB available (6 DIMMs at 4 GB)
    on most I7 Nehalem platforms, 32 GB (8 DIMMs at 4 GBs) on Sandybridge–EP I7, and
    16 GB on AMD platforms (4 DIMMs at 4 GB). As you’d typically pin only 2 GB maximum,
    you easily have room to support multiple GPUs. Most systems have support for at
    least two GPU cards. Four physical cards is the practical limit for a top-end
    system in one box.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机端，您需要至少和输入输出缓冲区固定的内存一样多。大多数I7 Nehalem平台上通常可以提供最多24 GB内存（6个DIMM，每个4 GB），Sandybridge–EP
    I7平台提供32 GB内存（8个DIMM，每个4 GB），AMD平台提供16 GB内存（4个DIMM，每个4 GB）。由于您通常只会固定最多2 GB内存，因此您很容易有足够的空间支持多个GPU。大多数系统至少支持两张GPU卡。四张物理卡是单台高端系统的实际限制。
- en: When the problem size is much larger than the host memory size, you have to
    consider the practical limits of the storage capacity on a single host. Multiterabyte
    disks can allow node storage into the tens of terabytes. Most motherboards are
    equipped with six or more SATA connectors and 4 TB-plus disks are readily available.
    Disks are easily transportable if the dataset is to be captured in some remote
    area. Next-day courier can often be the fastest way to transfer such data between
    sites.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题的规模远大于主机内存大小时，您必须考虑单台主机上存储容量的实际限制。多TB硬盘可以使节点存储达到数十TB。大多数主板都配备了六个或更多SATA连接器，4
    TB以上的硬盘也很容易获得。如果数据集需要在某个偏远地区捕获，硬盘非常易于运输。第二天的快递通常是不同站点之间传输此类数据的最快方式。
- en: Finally, when you cannot fit the dataset on a single machine, be it from compute,
    memory, storage, or power requirements, you have to look at multiple nodes. This
    brings you to the realm of internode communication. Internode communication is
    expensive in terms of time, at least an order of magnitude slower than any internal
    communication of data. You also have to learn another set of APIs, so this step
    is really best avoided if the problem can be contained to a single node.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当您无法将数据集放入单台机器时，不论是因为计算、内存、存储还是电力需求，您必须考虑使用多个节点。这就引出了节点间通信的领域。节点间通信在时间上是昂贵的，至少比任何内部数据通信慢一个数量级。您还需要学习另一套API，因此如果问题可以限制在单个节点内，最好避免这个步骤。
- en: Resolution
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分辨率
- en: Consider the question of what can be done with 10 times or 50 times as much
    processing power. An existing problem that previously took one hour to resolve
    can be done in just over a minute. How does this change the questions that can
    be asked with a given dataset? What can now be done in real time or near real
    time that was impossible in the past? The previous batch submission problem is
    now an interactive problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 想想如果有10倍或50倍的处理能力，能做些什么。一个以前需要一个小时才能解决的问题，现在可以在一分钟左右完成。这如何改变在给定数据集下能提出的问题？过去不可能实时或接近实时完成的任务，现在能做什么？之前的批处理提交问题，现在变成了一个互动性问题。
- en: Such a change allows for a step back from the problem, to consider how else
    it might be approached. Are there algorithms that were discarded in the past because
    they were too computationally expensive? Can you now process far more data points,
    or data points to a higher resolution, to produce a more accurate result? If you
    were previously happy with a runtime of a few hours or a day because that let
    you get on with other tasks, does increasing the resolution of the problem appeal
    more than the speedup? What does a more accurate result gain in your problem domain?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变化允许从问题中退后一步，考虑其他可能的解决方法。过去是否有一些算法因计算成本过高而被舍弃？现在你是否能处理更多的数据点，或者以更高的分辨率处理数据点，以产生更精确的结果？如果你过去因为几小时或一天的运行时间能够同时进行其他任务而感到满意，那么提高问题分辨率是否比加速更具吸引力？在你的问题领域，更准确的结果带来了什么收益？
- en: In finance applications, if your mathematical model of events is running ahead
    of the main market players, then you can react to changes faster than others,
    which can directly translate into making a better return on trading activities.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融应用中，如果你的事件数学模型领先于主要市场参与者，那么你就能比别人更快地对变化作出反应，这可以直接转化为更好的交易回报。
- en: In medical applications, being able to present the doctor with the result of
    a test before the patient has finished getting dressed and left allows much more
    efficient use of both the doctor’s and patient’s time as it avoids repeat appointments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学应用中，能够在病人穿好衣服并离开之前就向医生展示测试结果，可以更高效地利用医生和病人的时间，避免重复预约。
- en: In simulation applications, not having to wait a long time allows a much larger
    problem space to be explored within a given timeframe. It also allows for speculative
    execution. This is where you ask the system to explore all values of *x* between
    *n* and *m* in a given dataset. Equally, you might explore variables in the 2D
    or 3D space. With complex problems or a nonlinear system it’s not always clear
    what the optimal solution is, especially when changing one parameter impacts many
    other parameters. It may be quicker to simply explore the problem space and observe
    the result than it is to have an expert try to sit down and work out the optimal
    solution. This brute-force approach is remarkably effective and will often come
    up with solutions the “experts” would not have considered.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在仿真应用中，不必长时间等待，可以在给定的时间框架内探索更大的问题空间。它还允许进行推测性执行。这是指你要求系统在给定数据集中的*n*和*m*之间探索所有*x*值。同样，你可能会探索二维或三维空间中的变量。对于复杂问题或非线性系统，最佳解决方案并不总是显而易见，特别是当改变一个参数会影响许多其他参数时。与其让专家坐下来尝试计算出最佳解决方案，不如直接探索问题空间并观察结果，这可能会更快。这个暴力破解方法异常有效，常常会找到“专家”未曾考虑过的解决方案。
- en: As a student you can now kick off a problem between lectures on your personal
    desktop supercomputer, rather than submit a job to the university machine and
    wait a day for it to run, only to find out it crashed halfway through the job.
    You can prototype solutions and come up with answers far quicker than your non-CUDA-literate
    peers. Think what you could cover if their batch jobs take a day and yours are
    done locally in an hour.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为学生，你现在可以在个人桌面超级计算机上解决问题，而不需要将作业提交给学校的计算机并等待一天才完成，结果却发现作业在中途崩溃。你可以原型化解决方案，比没有CUDA知识的同学更快地得出答案。想一想，如果他们的批处理作业需要一天，而你的作业在本地一个小时内完成，你能解决多少问题。
- en: Identifying the bottlenecks
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别瓶颈
- en: Amdahl’s Law
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阿姆达尔定律
- en: Amdahl’s Law is often quoted in work on parallel architecture. It’s important
    because it tells us that, while serial elements of execution remain in the data
    flow, they will limit any speedup we can achieve.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 阿姆达尔定律通常在并行架构的研究中被引用。它很重要，因为它告诉我们，当执行流程中仍然存在串行元素时，它们会限制我们能够实现的加速。
- en: Consider the simple case where we have 50% of the program’s execution time spent
    on a section that could run in parallel and 50% that must be done serially. If
    you had an infinitely fast set of parallel processing units and you reduced the
    parallel aspect of the program down to zero time, you would still have the 50%
    serial code left to execute. The maximum possible speedup in this case is 2×,
    that is, the program executes in half the time period it did before. Not very
    impressive, really, given the huge amount of parallel processing power employed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设程序执行时间的50%是在一个可以并行运行的部分，另外50%是必须串行执行的部分。如果你有一组无限快的并行处理单元，并将程序的并行部分缩短为零时间，你仍然会剩下50%的串行代码需要执行。在这种情况下，最大可能的加速是2倍，也就是说，程序执行的时间是之前的一半。考虑到使用了大量的并行处理能力，这其实并不那么令人印象深刻。
- en: Even in the case where we have 90% of the program that could be parallelized,
    we still have the 10% serial code that remains. Thus, the maximum speedup is 9×,
    or nine times faster than the original, entirely serial, program.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们有90%程序可以并行化的情况下，我们仍然有10%的串行代码需要执行。因此，最大加速是9倍，也就是比原本完全串行的程序快9倍。
- en: The only way to scale a program infinitely is to eliminate all serial bottlenecks
    to program execution. Consider the diagram in [Figure 9.2](#F0015), where all
    the squares represent data items that need to be processed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 无限扩展一个程序的唯一方法是消除所有串行瓶颈。请参见[图9.2](#F0015)，其中所有的方块代表需要处理的数据项。
- en: '![image](../images/F000090f09-02-9780124159334.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-02-9780124159334.jpg)'
- en: FIGURE 9.2 Data flow bottlenecks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 数据流瓶颈。
- en: In this example, there are 10 threads, each processing one column of the data.
    In the center is a dependency, and thus all the threads must contribute their
    existing result to a single value before proceeding.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，有10个线程，每个线程处理数据的一个列。在中间是一个依赖关系，因此所有线程必须将各自的结果贡献给一个单一的值，然后才能继续处理。
- en: Imagine, for one moment, this is a field of crops, with each column a line of
    crops. Each thread is like a combine harvester, moving down the columns and collecting
    crops at each square. However, at the center of the field there is a wall with
    two gates.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，这是一片农田，每列都是一行作物。每个线程就像一台联合收割机，沿着列向下移动，在每个方块上收集作物。然而，在田地的中央有一堵墙，墙上有两个门。
- en: With 1 or even 2 combine harvesters, the gates pose a small problem and each
    combine harvester passes from one field to another. With 10 combine harvesters,
    one per column, getting each one through the gate takes time and slows down everyone
    in the process. This is one of the reasons why it’s far more efficient to have
    large, open fields, rather than smaller, bounded ones.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只有1个甚至2个联合收割机，门并不会构成太大问题，每台联合收割机可以从一个田地移动到另一个田地。而如果有10台联合收割机，每台收割机负责一列，想要让每台机器都通过门就需要时间，这会使整个过程变慢。这就是为什么大而开阔的田地比小而有限的田地更高效的原因之一。
- en: So how is this relevant to software? Each gate is like a serial point in the
    code. The program is doing well, churning through the chunks of work, and then
    all of a sudden it hits a serial point or synchronization point and everything
    backs up. It’s the same as everyone trying to leave the parking lot at the same
    time through a limited number of exits.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这与软件有什么关系呢？每个门就像代码中的串行点。程序正在顺利执行，处理着一块块的工作，突然之间它遇到一个串行点或同步点，所有的工作都停滞了。这就像每个人试图同时通过有限的出口离开停车场。
- en: The solution to this type of problem is to parallelize up the bottlenecks. If
    we have 10 gates in the field or 10 exits from the parking lot, there would be
    no bottleneck, just an orderly queue that would complete in *N* cycles.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种问题的方法是对瓶颈进行并行化。如果我们有10个田地门或10个停车场出口，就不会出现瓶颈，只有一个有序的队列，它将在*N*个周期内完成。
- en: When you consider algorithms like histograms, you see that having all threads
    add to the same set of bins forms exactly this sort of bottleneck. This is often
    done with atomic operations, which effectively introduce serial execution to a
    set of parallel threads. If, instead, you give every thread a set of its own bins
    and then add these sets together later, you remove the serialization bottleneck.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑像直方图这样的算法时，你会发现让所有线程都往同一组桶里添加数据就会形成这种瓶颈。通常这是通过原子操作来完成的，这实际上会在一组并行线程中引入串行执行。如果你给每个线程分配一组自己的桶，然后再将这些桶的结果合并，便可以消除串行瓶颈。
- en: Consider carefully in your code where you have such bottlenecks and how these
    might be eliminated. Often they will limit the maximum scaling available to your
    application. While this may not be an issue with two or even four CPU cores, with
    GPU code you need to think about tens of thousands of parallel threads.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑代码中可能存在的瓶颈，并思考如何消除这些瓶颈。通常这些瓶颈会限制你应用程序的最大扩展性。虽然在两个或四个CPU核心的情况下可能没有问题，但对于GPU代码，你需要考虑成千上万的并行线程。
- en: Profiling
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能分析
- en: Profiling is one of the most useful tasks in identifying where you are today
    and knowing where you should spend your time. Often people think they know where
    the bottlenecks are, then go off and optimize that routine, only to find it makes
    1% or 2% difference to the application’s overall execution time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析是识别你当前状态并确定时间分配的最有用的任务之一。很多人认为自己知道瓶颈在哪里，然后去优化那个程序，结果发现它对应用程序整体执行时间的改善只有1%或2%。
- en: In modern software development, there are usually many teams working on various
    aspects of a software package. It may not be possible to keep in contact with
    everyone who touches the software, especially in larger teams. Often what you
    may think is the bottleneck is not really that important.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代软件开发中，通常有多个团队在处理软件包的不同方面。与所有涉及软件开发的人保持联系可能并不现实，尤其是在更大的团队中。你可能认为的瓶颈往往并不是最重要的因素。
- en: Optimization should be based on hard numbers and facts, not speculation about
    what “might” be the best place to apply the software effort in terms of optimization.
    NVIDIA provides two good tools, CUDA Profiler and Parallel Nsight, that provide
    profiling information.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 优化应基于硬数据和事实，而不是对“可能”是最佳优化位置的推测。NVIDIA提供了两个很好的工具，CUDA Profiler和Parallel Nsight，它们提供了详细的性能分析信息。
- en: Profilers reveal, through looking at hardware counters, where the code spends
    it time, and also the occupancy level of the GPU. They provide useful counters
    such as the number of coalesced reads or writes, the cache hit/miss ratio, branch
    divergence, warp serialization, etc. The CUDA Memcheck tool is also very useful
    in identifying inefficient usage of memory bandwidth.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析工具通过查看硬件计数器，揭示代码的时间分布，以及GPU的占用率水平。它们提供了有用的计数器，如合并读取或写入的次数、缓存命中/未命中率、分支分歧、warp序列化等。CUDA
    Memcheck工具对于识别内存带宽使用不当也非常有用。
- en: Having done an initial run using the profiler, you should first look at the
    routine in which the code spends the most *total* time. Typical unoptimized programs
    spend 80% of their time in 20% of the code. Optimizing the 20% is the key to efficient
    use of your time and profiling is the key to identifying that 20% of the code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用性能分析器进行初步测试后，首先应查看代码在其中花费最多*总*时间的程序。典型的未优化程序在20%的代码中花费了80%的时间。优化这20%是高效利用时间的关键，而性能分析是识别这20%代码的关键。
- en: Of course once this has been optimized as best as it can be, it’s then progressively
    more and more time consuming to provide further speedups without a complete redesign.
    Measure the speedup and know when the time you’re spending is no longer providing
    a good return on that effort.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一旦优化达到最佳状态后，进一步加速将变得越来越耗时，而不进行完全重构将难以取得显著提升。衡量加速效果，并知道什么时候你投入的时间不再带来良好的回报。
- en: Parallel Nsight is a very useful tool in this regard as it provides a number
    of default “experiments.” That shed light on what your kernels are actually doing.
    Some off the more useful information you can take from the experiments is shown
    in [Figure 9.3](#F0020).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，Parallel Nsight 是一个非常有用的工具，因为它提供了多个默认的“实验”。这些实验可以揭示你的内核实际在做什么。从这些实验中，你可以获取的一些有用信息如[图
    9.3](#F0020)所示。
- en: '![image](../images/F000090f09-03-9780124159334.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-03-9780124159334.jpg)'
- en: FIGURE 9.3 Parallel Nsight experiments.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 Parallel Nsight 实验。
- en: The first experiment is the CUDA Memory Statistics, which provides a nice graphical
    view of how the caches are laid out and the bandwidth being achieved in the different
    parts of the device.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验是CUDA内存统计，它提供了一个图形化的视图，展示了缓存的布局以及在设备不同部分实现的带宽。
- en: This particular example (see [Figure 9.4](#F0025)) is taken from the odd/even
    sort we’ll look at a little later. What is interesting to note are the cache ratios.
    As we’re getting a 54% hit ratio in the L1 cache, we’re achieving an average throughput
    of 310 GB/s to global memory, in the order of double the actual bandwidth available
    from global memory. It also lists the number of transactions, which is important.
    If we can lower the number of transactions needed, through better coalescing and/or
    issuing larger reads/writes, we can significantly boost memory throughput.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具体的例子（见[图9.4](#F0025)）取自我们稍后将要讨论的奇偶排序。值得注意的是缓存命中率。由于在L1缓存中获得了54%的命中率，我们实现了每秒310GB的全球内存吞吐量，大约是全球内存实际带宽的两倍。文中还列出了事务数量，这一点很重要。如果我们能通过更好的合并和/或发出更大的读写请求来减少所需的事务数量，就可以显著提高内存吞吐量。
- en: '![image](../images/F000090f09-04-9780124159334.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-04-9780124159334.jpg)'
- en: FIGURE 9.4 Parallel Nsight memory overview.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 Parallel Nsight内存概览。
- en: The other important experiment is occupancy rates (see [Figure 9.5](#F0030)).
    In this experiment, notice the Achieved occupancy column and in particular the
    number of Active Warps. As this is a compute 2.0 device, we can have up to 48
    warps resident on a single SM. The achieved occupancy, as opposed to the theoretical
    occupancy, is the measured value of what was actually achieved. This will usually
    be significantly less than the theoretical maximum. Notice also that any limiting
    factor is highlighted in red, in this case the number of blocks per SM at six.
    The “occupancy” graphs tab allows you to understand this in somewhat more detail.
    It’s an extract from the occupancy calculation spreadsheet provided with the CUDA
    SDK.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的实验是占用率（见[图9.5](#F0030)）。在这个实验中，请注意“已实现占用率”栏，特别是“活动warp”的数量。由于这是一个compute
    2.0设备，我们最多可以在单个SM上保留48个warp。已实现的占用率与理论占用率不同，它是实际测量得到的占用率。通常情况下，这个值会显著低于理论最大值。还要注意，任何限制因素都会以红色突出显示，在这个例子中是每个SM的块数为6。图表中的“占用率”选项卡允许你更详细地了解这一点。这是CUDA
    SDK中提供的占用率计算电子表格的摘录。
- en: '![image](../images/F000090f09-05-9780124159334.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-05-9780124159334.jpg)'
- en: FIGURE 9.5 Parallel Nsight occupancy data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 Parallel Nsight占用率数据。
- en: The cause of this limit is actually the number of threads. Dropping this from
    256 to 192 would allow the hardware to schedule eight blocks. As this kernel has
    synchronization points, having more blocks available may introduce a better instruction
    mix. There will also be fewer warps that are unable to run due to the synchronization
    point.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个限制的原因实际上是线程的数量。将线程数从256减少到192，硬件就可以调度八个块。由于该内核存在同步点，更多的块可用可能会带来更好的指令混合。同时，由于同步点的存在，将会有更少的warp无法执行。
- en: In practice, making this change helps quite significantly. It improves occupancy
    from 98.17% to 98.22%, which is marginal at best. However, the execution time
    drops from 14 ms to just 10 ms. The answer to this is in the memory usage. With
    192 threads per block, we’re accessing a smaller range of addresses which increases
    the locality of the accesses and consequently improves cache utilization. The
    total number of memory transactions needed by each SM drops by about one-quarter.
    Consequently, we see a proportional drop in execution time.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，做出这个改变会有相当显著的帮助。它将占用率从98.17%提高到98.22%，提升幅度很小。但执行时间却从14毫秒下降到仅10毫秒。原因就在于内存使用情况。每个块有192个线程时，我们访问的是较小范围的地址，从而增加了访问的局部性，进而提高了缓存利用率。每个SM所需的内存事务总数减少了大约四分之一。因此，我们看到执行时间也相应地下降。
- en: Grouping the tasks for CPU and GPU
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将任务分配给CPU和GPU
- en: Dr. M. Fatica from NVIDIA gave a great talk at GTC2010 concerning how Linpack
    had been optimized for GPUs. Linpack is a benchmark based on linear algebra. It
    is used in the Top500 supercomputer benchmark ([*www.top500.org*](http://www.top500.org))
    to benchmark the various supercomputers around the world. One interesting fact
    from this talk was the GPU used at that time, a Fermi Tesla C2050 card, produced
    around 350 gigaflops of DGEMM (double-precision matrix multiply) performance.
    The CPU used produced around 80 gigaflops. The contribution of 80 gigaflops is
    a little under one-quarter of the GPU contribution, so not something that can
    be ignored. A quarter or so extra performance goes a long way to reducing execution
    time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 NVIDIA 的 Dr. M. Fatica 在 GTC2010 上做了一场精彩的演讲，介绍了 Linpack 如何针对 GPU 进行优化。Linpack
    是一个基于线性代数的基准测试。它被用于 Top500 超级计算机基准测试（[*www.top500.org*](http://www.top500.org)），用来评估全球各种超级计算机的性能。在这场演讲中，有一个有趣的事实是，当时使用的
    GPU ——Fermi Tesla C2050 卡——产生了约 350 吉弗洛普的 DGEMM（双精度矩阵乘法）性能。而使用的 CPU 则产生了约 80 吉弗洛普。80
    吉弗洛普的贡献稍微不到 GPU 贡献的四分之一，因此并不是可以忽视的部分。大约四分之一的额外性能对于减少执行时间有着非常重要的作用。
- en: In fact, the best applications tend to be those that play to the strengths of
    both the CPU *and* the GPU and split the data accordingly. The CPU must be considered
    in any GPU-based optimization, because it’s the total application time that is
    important. If you have a four-, six-, or eight-core CPU and one core is busy handling
    a GPU application, why not use the other cores to also work on the problem? The
    more cores you have available, the higher the potential gain is by offloading
    some work to the CPU.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，最好的应用程序通常是那些能够充分发挥 CPU *和* GPU 优势的应用，并根据情况拆分数据。任何基于 GPU 的优化都必须考虑到 CPU，因为关键是整体应用时间。如果你有一个四核、六核或八核的
    CPU，而其中一个核心正忙于处理 GPU 应用，为什么不利用其他核心一起工作解决问题呢？你拥有的核心越多，将一部分工作卸载到 CPU 上所获得的潜在增益就越高。
- en: If we say the CPU can handle work at one-tenth the rate of the GPU, then with
    just three CPU cores, you’re gaining a 30% additional throughput. If you had an
    eight-core device, potentially this is a 70% gain in performance, which is almost
    the same as having two GPUs working in tandem. In practice, however, often other
    constraints might limit the overall speed, such as memory, network, or I/O bandwidth.
    However, even so, you’re likely to see a significant speedup where the application
    is not already bound by one of these constraints on the host side.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们说 CPU 的处理速度是 GPU 的十分之一，那么仅仅通过三个 CPU 核心，你就能获得 30% 的额外吞吐量。如果你有一个八核设备，理论上这是一个
    70% 的性能提升，几乎等同于有两个 GPU 协同工作。然而，在实际应用中，其他约束（如内存、网络或 I/O 带宽）往往会限制整体速度。然而，即便如此，当应用没有受到主机端这些约束的限制时，你仍然可能会看到显著的加速效果。
- en: Of these constraints, I/O is an interesting one, because introducing more CPU
    threads or processes can often significantly improve the overall I/O throughput.
    This may seem a strange statement, as surely the physical limits to and from an
    I/O device dictate the speed? On modern machines with large amounts of memory,
    most I/O is in fact cached. Therefore, I/O can be more about moving data in memory
    than it is about moving to or from devices. A decent RAID controller has its own
    processor to do the I/O operations. Multiple CPU cores allow for multiple independent
    memory transfers, which often provide a higher overall bandwidth than a single
    CPU core.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些约束中，I/O 是一个有趣的因素，因为增加更多的 CPU 线程或进程通常能显著提高整体 I/O 吞吐量。这听起来可能有些奇怪，因为显然 I/O 设备的物理限制决定了速度？但在现代配备大量内存的机器上，大多数
    I/O 实际上是被缓存的。因此，I/O 更多的是关于在内存中移动数据，而不是在设备之间移动数据。一款不错的 RAID 控制器有自己的处理器来执行 I/O 操作。多个
    CPU 核心可以进行多个独立的内存传输，通常会提供比单个 CPU 核心更高的整体带宽。
- en: Separate CPU process or threads can create a separate GPU context and launch
    their own kernel onto the GPU. These additional kernels are then queued within
    the GPU for execution. When available resources become free the kernel is executed.
    If you look at the typical GPU usage you see that shown in [Figure 9.6](#F0035).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 独立的 CPU 进程或线程可以创建一个独立的 GPU 上下文，并将其内核启动到 GPU 上。这些额外的内核随后会在 GPU 内排队等待执行。当可用资源释放时，内核会被执行。如果你查看典型的
    GPU 使用情况，你会看到 [图 9.6](#F0035) 所示的内容。
- en: '![image](../images/F000090f09-06-9780124159334.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-06-9780124159334.jpg)'
- en: FIGURE 9.6 CPU and GPU idle time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 CPU 和 GPU 空闲时间。
- en: Notice there is significant idle time on both the GPU and the CPU. Idle time
    on the GPU is more expensive, as it’s typically 10 times more useful than the
    CPU time. Tools such as Parallel Nsight allow you to display just such a timeline
    and you’ll be amazed to see just how much idle time certain kernels can create.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，GPU和CPU上都有显著的空闲时间。GPU上的空闲时间更为昂贵，因为它通常比CPU时间更为有用，通常是10倍。像Parallel Nsight这样的工具可以让你显示这样的时间线，你会惊讶于看到某些内核能产生多少空闲时间。
- en: By placing multiple kernels onto a single GPU, these kernels then slot into
    the empty slots. This increases, marginally, the latency of the first set of kernels
    but greatly improves the overall throughput of the application. In a lot of applications,
    there can be as much as 30% idle time. Just consider what a typical application
    will do. First, fetch data from somewhere, typically a slow I/O device like a
    hard drive. Then transfer the data to the GPU and then sit and wait until the
    GPU kernel is complete. When it’s complete, the host transfers the data off the
    GPU. It then saves it somewhere, usually to slow I/O storage, fetches the next
    data block, and so on.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将多个内核放置到单个GPU上，这些内核将插入空闲的插槽中。这会稍微增加第一组内核的延迟，但大大提高了应用程序的整体吞吐量。在许多应用程序中，可能会有多达30%的空闲时间。试想一下一个典型的应用程序会做什么。首先，从某个地方获取数据，通常是一个慢速I/O设备，比如硬盘。然后将数据传输到GPU，接着等待直到GPU内核完成。当完成时，主机将数据从GPU中传出。然后将其保存到某个地方，通常是慢速I/O存储，获取下一个数据块，依此类推。
- en: While the GPU is executing the kernel, why not fetch the next data block from
    the slow I/O device, so it’s ready when the GPU kernel has completed? This is,
    in effect, what happens when you execute multiple processes. The I/O device blocks
    the second process, while fetching data for the first. When the first process
    is transferring data and invoking the kernel, the second process is accessing
    the I/O hardware. It then does a transfer, while process one is computing and
    the kernel invocation of the second process is queued. When the transfer back
    to the host for process one starts, the kernel from process two also starts executing.
    Thus, with the introduction of just a couple of processes, you have neatly overlapped
    the I/O, CPU, GPU, and transfer times, gaining a significant improvement in overall
    throughput. See the stream example in [Chapter 8](CHP008.html) for a detailed
    explanation of this.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当GPU执行内核时，为什么不从慢速I/O设备获取下一个数据块，这样等GPU内核完成时就能准备好呢？实际上，这就是执行多个进程时发生的情况。I/O设备阻塞第二个进程，同时为第一个进程获取数据。当第一个进程正在传输数据并调用内核时，第二个进程则在访问I/O硬件。然后它执行传输操作，而进程一在计算，并且第二个进程的内核调用排队。当第一个进程的数据传输回主机时，第二个进程的内核也开始执行。因此，通过引入少数几个进程，你就巧妙地重叠了I/O、CPU、GPU和传输时间，显著提高了整体吞吐量。有关此内容的详细解释，请参见[第8章](CHP008.html)中的流示例。
- en: Note that you can achieve the same results using threads or processes. Threads
    allow the application data to share a common data area and provide faster synchronization
    primitives. Processes allow for processor affinity, where you lock a process to
    a given CPU core, which can often improve performance because it allows for better
    core-specific cache reuse. The choice depends largely on how much, if any, synchronization
    is needed between the CPU tasks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以使用线程或进程来实现相同的结果。线程允许应用程序数据共享一个公共数据区域，并提供更快的同步原语。进程允许处理器亲和性，即将进程绑定到某个CPU核心，这通常能提高性能，因为它有助于更好的核心特定缓存重用。选择主要取决于CPU任务之间是否需要同步，以及需要多少同步。
- en: The other aspect of the CPU/GPU decision is knowing how best to split the task.
    CPUs are great at serial problems, where the data is sparsely distributed, or
    where the dataset is small. However, with a typical 10:1 ratio of performance
    on the GPU to the CPU, you have to be careful that you will not be holding up
    the GPU. For this reason, many applications simply use the CPU to load and store
    data. This can sometimes fully load a single core on the CPU, depending on how
    much computation time is required on the GPU.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: CPU/GPU决策的另一个方面是如何最佳地拆分任务。CPU擅长处理串行问题，尤其是数据分布稀疏或数据集较小的情况。然而，在GPU对CPU的典型10:1性能比下，你必须小心不要阻塞GPU。因此，许多应用程序简单地使用CPU来加载和存储数据。根据GPU需要的计算时间，这有时可能会使CPU的单个核心负载满。
- en: One usage you sometimes see a CPU being used for is the final stages of a reduction.
    A reduction operation typically reduces itself by a factor of two on every iteration
    of the reduction. If you start out with a million elements, within six iterations
    you are starting to hit the maximum number of schedulable threads on a GPU. Within
    a few more iterations, several of the SMs are idle. With the GT200 and prior generation
    of hardware, kernels were not overlapped, so the kernel had to continue to iterate
    down to the final elements before it freed up the idle SMs to do more work.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你会看到CPU被用来处理归约的最终阶段。归约操作通常会在每次归约迭代时将元素数减少一半。如果你从一百万个元素开始，在六次迭代后，你就开始接近GPU上可调度线程的最大数量。再经过几次迭代，几个SM会处于空闲状态。在GT200及之前的硬件上，内核不会重叠执行，因此内核必须继续迭代，直到处理完最后的元素，才能释放空闲的SM去执行更多的工作。
- en: Thus, one optimization when a certain threshold is reached, is to forward the
    remaining part of the computation to the CPU to complete. If the CPU was in fact
    idle anyway, and the remaining data being transferred is not huge, this strategy
    can show significant gains over waiting for the GPU to complete the entire reduction.
    With Fermi, NVIDIA addressed this issue, allowing those idle SMs to start work
    on the next queued kernel. However, for the SM to become idle, it’s necessary
    for all the thread blocks to have completed. Some nonoptimal kernels will have
    one or more active threads, even at the final levels of the reduction, which pins
    the kernel to the SM until the complete reduction is done. With algorithms like
    reduction, be sure you are reducing the number of active warps per iteration,
    not just the number of active threads.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当某一阈值达到时，一种优化方法是将剩余的计算任务转发给CPU来完成。如果CPU本身处于空闲状态，而且剩余的数据传输量不大，这种策略可以比等待GPU完成整个归约操作获得显著的性能提升。在Fermi架构中，NVIDIA解决了这个问题，允许空闲的SM开始处理下一个排队的内核。然而，为了使SM空闲，必须保证所有线程块都已完成。某些非最优内核在归约的最后阶段仍可能有一个或多个活跃线程，这会将内核锁定在SM上，直到完整的归约操作完成。像归约这样的算法，要确保你在每次迭代中减少活跃warp的数量，而不仅仅是减少活跃线程的数量。
- en: Section summary
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小节总结
- en: • Understand the problem and define your speedup goal in the context of the
    programming time and skills available to you.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: • 理解问题，并在可用的编程时间和技能范围内定义你的加速目标。
- en: • Identify the parallelism in the problem and think about how to best to allocate
    this between the CPU and one or more GPUs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: • 识别问题中的并行性，并思考如何在CPU和一个或多个GPU之间最佳分配这些并行性。
- en: • Consider what is more important, a lower execution time or processing the
    data to a higher resolution.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: • 考虑哪个更重要，是降低执行时间还是将数据处理成更高的分辨率。
- en: • Understand the implication of any serial code sections and think about how
    these might best be handled.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: • 理解任何串行代码部分的含义，并思考如何最好地处理这些部分。
- en: • Profile your application to ensure your understanding reflects the actual
    reality. Repeat your earlier analysis if appropriate with your enhanced understanding.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: • 对应用程序进行性能分析，确保你的理解反映了实际情况。如果适当的话，根据你更深入的理解，重复之前的分析。
- en: 'Strategy 2: Memory Considerations'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 2：内存考虑
- en: Memory bandwidth
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存带宽
- en: Memory bandwidth and latency are key considerations in almost all applications,
    but especially so for GPU applications. Bandwidth refers to the amount of data
    that can be moved to or from a given destination. In the GPU case we’re concerned
    primarily about the global memory bandwidth. Latency refers to the time the operation
    takes to complete.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 内存带宽和延迟是几乎所有应用中的关键考虑因素，尤其是在GPU应用中尤为重要。带宽指的是可以传输到或从给定目标位置移动的数据量。在GPU的情况下，我们主要关注的是全局内存带宽。延迟指的是操作完成所需的时间。
- en: Memory latency is designed to be hidden on GPUs by running threads from other
    warps. When a warp accesses a memory location that is not available, the hardware
    issues a read or write request to the memory. This request will be automatically
    combined or coalesced with requests from other threads in the same warp, provided
    the threads access adjacent memory locations and the start of the memory area
    is suitably aligned.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上，内存延迟的设计是通过运行来自其他warp的线程来隐藏的。当一个warp访问一个不可用的内存位置时，硬件会发出读写请求到内存。如果这些线程访问的是相邻的内存位置，并且内存区域的起始位置得到了适当对齐，那么这个请求会自动与同一个warp中其他线程的请求合并或聚合。
- en: The size of memory transactions varies significantly between Fermi and the older
    versions. In compute 1.x devices (G80, GT200), the coalesced memory transaction
    size would start off at 128 bytes per memory access. This would then be reduced
    to 64 or 32 bytes if the total region being accessed by the coalesced threads
    was small enough and within the same 32-byte aligned block. This memory was not
    cached, so if threads did not access consecutive memory addresses, it led to a
    rapid drop off in memory bandwidth. Thus, if thread 0 reads addresses 0, 1, 2,
    3, 4, …, 31 and thread 1 reads addresses 32, 32, 34, …, 63, they will not be coalesced.
    In fact, the hardware will issue one read request of at least 32 bytes for each
    thread. The bytes not used will be fetched from memory and simply be discarded.
    Thus, without careful consideration of how memory is used, you can easily receive
    a tiny fraction of the actual bandwidth available on the device.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi 和旧版本之间的内存事务大小差异显著。在计算 1.x 设备（如 G80，GT200）中，合并的内存事务大小通常是每次内存访问 128 字节。如果合并线程访问的总区域足够小，并且位于相同的
    32 字节对齐块内，事务大小会减少到 64 或 32 字节。这些内存不会被缓存，因此，如果线程没有访问连续的内存地址，就会导致内存带宽的快速下降。因此，如果线程
    0 读取地址 0、1、2、3、4、……、31，线程 1 读取地址 32、32、34、……、63，它们将无法合并。事实上，硬件将为每个线程发起至少 32 字节的读取请求。未使用的字节将从内存中读取并简单地丢弃。因此，如果不仔细考虑内存的使用方式，你可能会得到设备上可用带宽的一小部分。
- en: The situation in Fermi and Kepler is much improved from this perspective. Fermi,
    unlike compute 1.x devices, fetches memory in transactions of either 32 or 128
    bytes. A 64-byte fetch is not supported. By default every memory transaction is
    a 128-byte cache line fetch. Thus, one crucial difference is that access by a
    stride other than one, but within 128 bytes, now results in cached access instead
    of another memory fetch. This makes the GPU model from Fermi onwards considerably
    easier to program than previous generations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，Fermi 和 Kepler 的情况有了很大改善。Fermi 不像计算 1.x 设备那样以 32 或 128 字节的事务方式提取内存，不支持
    64 字节的提取。默认情况下，每个内存事务都是 128 字节的缓存行提取。因此，一个关键的区别是，如果访问步长不为 1，但仍在 128 字节以内，那么现在会导致缓存访问，而不是再次发起内存提取。这使得从
    Fermi 开始的 GPU 模型比前几代更容易编程。
- en: One of the key areas to consider is in the number of memory transactions in
    flight. Each memory transaction feeds into a queue and is individually executed
    by the memory subsystem. There is a certain amount of overhead with this. It’s
    less expensive for a thread to issue a read of four floats or four integers in
    one pass than to issue four individual reads. In fact, if you look at some of
    the graphs NVIDIA has produced, you see that to get anywhere near the peak bandwidth
    on Fermi and Kepler you need to adopt one of two approaches. First, fully load
    the processor with warps and achieve near 100% occupancy. Second, use the 64-/128-bit
    reads via the `float2`/`int2` or `float4`/`int4` vector types and your occupancy
    can be much less but still allow near 100% of peak memory bandwidth. In effect,
    by using the vector types you are issuing a smaller number of larger transactions
    that the hardware can more efficiently process. You also introduce a certain amount
    of instruction-level parallelism through processing more than one element per
    thread.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的关键领域之一是内存事务的数量。每个内存事务会进入队列，并由内存子系统单独执行。这样会有一定的开销。对于线程来说，发起一次读取四个浮点数或四个整数比发起四个单独的读取操作要便宜得多。事实上，如果你查看
    NVIDIA 提供的一些图表，你会看到，要接近 Fermi 和 Kepler 上的峰值带宽，你需要采取两种方法中的一种。首先，充分加载处理器，使用多个 warp，并实现接近
    100% 的占用率。其次，使用 `float2`/`int2` 或 `float4`/`int4` 向量类型的 64 位/128 位读取，即使你的占用率较低，仍然可以实现接近
    100% 的峰值内存带宽。实际上，通过使用向量类型，你发起了更少的更大事务，这些事务硬件可以更高效地处理。你还通过每个线程处理多个元素引入了某种程度的指令级并行性。
- en: However, be aware that the vector types (`int2`, `int4`, etc.) introduce an
    implicit alignment of 8 and 16 bytes, respectively. The data must support this,
    so for example, you cannot cast a pointer to `int` from array element `int[5]`
    to `int2∗` and expect it to work correctly. In such cases you’re better off performing
    back-to-back 32-bit reads or adding some padding to the data structure to allow
    aligned access. As we saw when optimizing the sample sort example, a value of
    four elements per thread often provides the optimal balance between additional
    register usage, providing increased memory throughput and opportunity for the
    processor to exploit instruction-level parallelism.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要注意，向量类型（`int2`、`int4` 等）会隐式引入 8 字节和 16 字节的对齐要求。数据必须支持这一点，例如，你不能将 `int[5]`
    数组元素的指针强制转换为 `int2∗` 并期望它正确工作。在这种情况下，最好进行背靠背的 32 位读取，或向数据结构中添加填充，以允许对齐访问。正如我们在优化示例排序示例时看到的，四个元素每个线程通常提供了额外寄存器使用、增加内存吞吐量和让处理器能够利用指令级并行的最佳平衡。
- en: Source of limit
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制的来源
- en: Kernels are typically limited by two key factors, memory latency/bandwidth and
    instruction latency/bandwidth. Optimizing for one when the other is the key limiter
    will result in a lot of effort and very little return on that effort. Therefore,
    being able to understand which of these two key factors is limiting performance
    is critical to knowing where to direct your efforts.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 内核通常受限于两个关键因素：内存延迟/带宽和指令延迟/带宽。当优化其中一个，而另一个才是主要限制因素时，会导致大量的努力却收效甚微。因此，能够了解是哪一个关键因素限制了性能，对于确定努力方向至关重要。
- en: The simplest way in which you can see where the balance of the code lies is
    to simply comment out all the arithmetic instructions and replace them with a
    straight assignment to the result. Arithmetic instructions include any calculations,
    branches, loops, etc. If you have a one-to-one mapping of input values to calculated
    outputs, this is very simple and a one-to-one assignment works well. Where you
    have a reduction operation of one form or another, simply replace it with a sum
    operation. Be sure to include all the parameters read from memory into the final
    output or the compiler will remove the apparently redundant memory reads/writes.
    Retime the execution of the kernel and you will see the approximate percentage
    of time that was spent on the arithmetic or algorithmic part. If this percentage
    is very high, you are arithmetically bound. Conversely, if very little changed
    on the overall timing, you are memory bound.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过最简单的方法来查看代码的平衡所在，那就是将所有算术指令注释掉，并用一个直接的赋值操作替换它们，赋值给结果。算术指令包括任何计算、分支、循环等。如果输入值与计算输出之间有一一对应关系，这非常简单，且一一赋值效果良好。当你有某种形式的归约操作时，只需将其替换为求和操作。确保将所有从内存读取的参数包含在最终输出中，否则编译器会删除那些看似冗余的内存读写操作。重新计时内核执行的时间，你将看到大致的算术或算法部分所花费的时间百分比。如果这个百分比非常高，你就是算术限制。相反，如果整体时序变化很小，那你就是内存限制。
- en: With the arithmetic code still commented out, run the kernel using Parallel
    Nsight, using the Analysis function and the Profile setting. Examine the instruction
    statistics it produces ([Figure 9.7](#F0040)). If the bar graph contains a significant
    amount of blue, then the kernel memory pattern is displaying poor coalescing and
    the GPU has to serialize the instruction stream to support scattered memory reads
    or writes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在算术代码仍被注释掉的情况下，使用 Parallel Nsight 运行内核，使用分析功能和性能设置。检查它生成的指令统计数据（[图 9.7](#F0040)）。如果条形图中有大量的蓝色，那么内核的内存模式显示出较差的合并性，GPU必须序列化指令流，以支持散乱的内存读写。
- en: '![image](../images/F000090f09-07-9780124159334.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-07-9780124159334.jpg)'
- en: FIGURE 9.7 High instruction reissue rate.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 高指令重发率。
- en: If this is the case, is it possible to rearrange the memory pattern so the GPU
    can coalesce the memory access pattern by thread? Remember, to do this, thread
    0 has to access address 0, thread 1 address 1, thread 2 address 2, and so on.
    Ideally, your data pattern should generate a column-based access pattern by thread,
    not a row-based access. If you can’t easily rearrange the data pattern, can you
    rearrange the thread pattern such that you can use them to load the data into
    shared memory before accessing the data? If so, you don’t have to worry about
    coalescing the reads when accessing them from shared memory.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，是否有可能重新排列内存模式，以便GPU能够按线程合并内存访问模式？记住，要做到这一点，线程0必须访问地址0，线程1访问地址1，线程2访问地址2，依此类推。理想情况下，你的数据模式应该生成按列访问的模式，而不是按行访问。如果你无法轻松地重新排列数据模式，是否可以重新排列线程模式，以便在访问数据之前将其加载到共享内存中？如果可以，你就不必担心从共享内存中访问时合并读取。
- en: Is it possible to expand the number of elements of the output dataset that are
    processed by a single thread? This will often help both memory- and arithmetic-bound
    kernels. If you do this, do it without introducing a loop into the thread, but
    by duplicating the code. If the code is nontrivial, this can also be done as a
    device function or a macro. Be sure to hoist the read operations up to the start
    of the kernel, so that the read operations have finished fetching data before
    they are needed. This will increase register usage, so be sure to monitor the
    number of warps being scheduled to see it does not suddenly drop off.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有可能扩大由单个线程处理的输出数据集元素的数量？这通常有助于内存绑定和算术绑定的内核。如果你这样做，应该通过复制代码来完成，而不是在线程中引入循环。如果代码比较复杂，也可以将其作为设备函数或宏来处理。确保将读取操作提到内核的开始位置，这样在需要时读取操作已经完成数据获取。这将增加寄存器的使用，因此要确保监控调度的warp数量，确保它不会突然下降。
- en: With arithmetic-bound kernels, look at the source code and think about how this
    would be translated into assembly (PTX) code. Don’t be afraid to have a look at
    the actual PTX code being generated. Array indexes can often be replaced with
    pointer-based code, replacing slow multiplies with much faster additions. Divide
    or multiply instructions that use a power of 2 can be replaced with much faster
    right and left shift operations, respectively. Anything that is constant within
    a loop body, an invariant, should be moved outside the loop body. If the thread
    contains a loop, does unrolling the loop speed up things (it usually does)? What
    loop unrolling factor works best? We look at these optimization strategies in
    detail a little later in this chapter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算术绑定的内核，查看源代码并思考它如何被转换为汇编（PTX）代码。不要害怕查看实际生成的PTX代码。数组索引通常可以用基于指针的代码替换，将较慢的乘法替换为更快的加法。使用2的幂次方的除法或乘法指令可以分别替换为更快的右移和左移操作。任何在循环体内保持不变的常量（不变量），都应该移到循环体外。如果线程中包含循环，展开循环会加速吗（通常是的）？哪种循环展开因子效果最好？我们稍后将在本章中详细讨论这些优化策略。
- en: Are you using single- or double-precision floats in reality, and what did you
    want to use? Look out for floating-point constants without an `F` postfix, which
    the compiler will treat as double precision. Do you really need 32 bits of precision
    in all of the calculations? Try the `-use_fast_math` compiler switch and see if
    the results are still accurate enough for your needs. This switch enables 24-bit
    floating-point arithmetic, which can be significantly quicker than the standard
    IEEE 32-bit floating-point math logic.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你在实际中使用的是单精度还是双精度浮点数，你想使用的是哪种？注意那些没有`F`后缀的浮点常量，编译器会将其视为双精度。你是否真的需要在所有计算中使用32位精度？尝试使用`-use_fast_math`编译选项，看看结果是否仍然足够准确以满足你的需求。此选项启用了24位浮点运算，比标准的IEEE
    32位浮点数学逻辑快得多。
- en: Finally, are you testing speed with the “release” version of the code? As we
    saw in some of the examples earlier, this alone can increase performance by 15%
    or more.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你是否在“发布”版本的代码中进行速度测试？正如我们之前一些例子所看到的，仅此一项就可以提高15%或更多的性能。
- en: Memory organization
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存组织
- en: Getting the memory pattern correct for a GPU is often the key consideration
    in many applications. CPU programs typically arrange the data in rows within memory.
    While Fermi and Kepler will tolerate noncoalesced reads and writes, as we mentioned
    earlier, compute 1.x devices will not. You have to try and arrange the memory
    pattern such that access to it by consecutive threads will be in columns. This
    is true of both global memory and shared memory. This means for a given warp (32
    threads) thread 0 should access address offset 0, thread 1 address offset 1, thread
    2 address offset 2, etc. Think about the fetch to global memory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 GPU 的内存模式通常是许多应用中的关键考虑因素。CPU 程序通常将数据按行排列在内存中。虽然 Fermi 和 Kepler 会容忍非合并读取和写入，但正如我们之前提到的，计算
    1.x 设备则不会。你必须尝试安排内存模式，使得连续线程对它的访问将是按列的。这对于全局内存和共享内存都适用。这意味着，对于一个给定的 warp（32 个线程），线程
    0 应该访问地址偏移 0，线程 1 访问地址偏移 1，线程 2 访问地址偏移 2，依此类推。想想访问全局内存的提取。
- en: However, assuming you have an aligned access, 128 bytes of data will come in
    from global memory at a time. With a single float or integer per thread, all 32
    threads in the warp will be given exactly one element of data each.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设你有对齐访问，那么每次会从全局内存中获取 128 字节的数据。每个线程有一个浮点数或整数时，warp 中的 32 个线程将每个获取一个数据元素。
- en: Note the `cudaMalloc` function will allocate memory in 128-byte aligned blocks,
    so for the most part alignment is not an issue. However, if using a structure
    that would straddle such a boundary, then there are two approaches. First, you
    can either add padding bytes/words explicitly to the structure. Alternatively,
    you can use the `cudaMallocPitch` function we covered in [Chapter 6](CHP006.html).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`cudaMalloc`函数会在 128 字节对齐的块中分配内存，因此大多数情况下对齐不是问题。然而，如果使用一个会跨越这样的边界的结构，那么有两种方法可以解决。首先，你可以显式地向结构中添加填充字节/字。或者，你可以使用我们在[第六章](CHP006.html)中介绍的`cudaMallocPitch`函数。
- en: 'Notice that alignment is a key criteria as to whether one or two memory transactions,
    or cache lines, need to be fetched. Suppose thread 0 accesses address offset 2
    instead of 0\. Perhaps you’re accessing some data structure that has a header
    at the start, such as:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对齐是决定是否需要获取一个或两个内存事务或缓存行的关键标准。假设线程 0 访问的是地址偏移 2 而不是 0\. 也许你正在访问某个数据结构，该结构的开头有一个头部，例如：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If the kernel processes `msg_data`, then threads 30 and 31 of the warp cannot
    be served by the single memory fetch. In fact, they generate an additional 128-byte
    memory transaction as shown in [Figure 9.8](#F0045). Any subsequent warps suffer
    from the same issue. You are halving your memory bandwidth, just by having a 2-byte
    header at the start of the data structure.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内核处理`msg_data`，那么 warp 的线程 30 和 31 无法通过单一的内存获取来服务。实际上，它们会生成一个额外的 128 字节内存事务，如[图
    9.8](#F0045)所示。任何后续的 warp 都会遇到相同的问题。仅仅在数据结构的开头有一个 2 字节的头部，你的内存带宽就被减半了。
- en: '![image](../images/F000090f09-08-9780124159334.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-08-9780124159334.jpg)'
- en: FIGURE 9.8 Cache line/memory transaction usage within structures.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 结构内的缓存行/内存事务使用情况。
- en: You’ll see this most acutely on compute 1.x devices where the additional fetch
    generated for threads 30/31 isn’t even used to prefill the cache, but just discarded.
    Loading the header into a separate chunk of memory somewhere else allows for aligned
    access to the data block. If you are unable to do this, then manually insert padding
    bytes into the structure definition to ensure that `msg_data` is aligned to a
    128-byte boundary. Note that simply reordering the structure elements to move
    ‘header’ after `msg_data` will also work, providing the structure is not subsequently
    used to create an array of structures. All of a sudden your threads match the
    memory organization and your memory throughput when working with the `msg_data`
    part of the structure will double.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在计算 1.x 设备上最为明显地看到这一点，其中为线程 30/31 生成的额外获取甚至没有用于预填充缓存，而是直接被丢弃。将头部加载到内存中的其他位置的单独块中，可以实现对数据块的对齐访问。如果你无法做到这一点，那么可以手动在结构定义中插入填充字节，以确保`msg_data`对齐到
    128 字节边界。请注意，简单地重新排序结构元素，将‘header’放在`msg_data`之后也可以解决问题，前提是该结构之后不会被用来创建结构数组。突然之间，你的线程与内存组织对齐，并且在处理结构中的`msg_data`部分时，内存吞吐量将翻倍。
- en: Consider also the case where prefix sum is used. Prefix sum allows for multiple
    independent processes or threads to read or write to independent areas of memory
    without interfering with one another. Multiple reads from the same address are
    actually hugely beneficial, in that the GPU will simply forward the value to whatever
    additional threads within the warp need it without additional memory fetches.
    Multiple writes are of course an issue, in that they need to be sequenced.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑使用前缀和的情况。前缀和允许多个独立的进程或线程读取或写入独立的内存区域，而互不干扰。多次从同一地址读取实际上是非常有益的，因为GPU会将值直接转发给warp内的其他需要它的线程，而无需额外的内存获取。多次写入当然是一个问题，因为它们需要排队。
- en: If we assume integers or floats for now, the size of each entry in the data
    array is 4 bytes. If the distribution of the prefix array is exactly equal then
    we don’t need prefix arrays to access the data anyway, as you could simply use
    a fixed offset per thread. Therefore, if you’re using a prefix sum to calculate
    an offset into the dataset, it’s highly likely there are a variable number of
    elements per bin. If you know the upper bounds of the number of elements per bin
    and you have a sufficient memory available, then just pad each bin to the alignment
    boundary. Use an additional array that holds the number of elements in the bin
    or calculate this value from the prefix sum index. In this way we can achieve
    aligned access to memory at the expense of unused cells at the end of most bins.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在假设使用整数或浮点数，那么数据数组中每个条目的大小为4字节。如果前缀数组的分布完全相等，那么无论如何我们不需要前缀数组来访问数据，因为你可以简单地为每个线程使用固定偏移量。因此，如果你使用前缀和来计算数据集的偏移量，那么每个桶内的元素数量很可能是可变的。如果你知道每个桶内元素数量的上限，并且有足够的内存可用，那么只需将每个桶填充到对齐边界。使用一个额外的数组来保存桶内的元素数量，或者通过前缀和索引计算这个值。通过这种方式，我们可以通过牺牲大多数桶末尾的未使用单元来实现对内存的对齐访问。
- en: One very simple solution to the alignment problem is to use a padding value
    that has no effect on the calculated result. For example, if you’re performing
    a sum over the values in each bin, padding with zero will mean no change to the
    end result, but will give a uniform memory pattern and execution path for all
    elements in the warp. For a `min` operation, you can use a padding value of 0xFFFFFFFF,
    and conversely 0 for a `max` operation. It is usually not hard to come up with
    a padding value that can be processed, yet contributes nothing to the result.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐问题的一个非常简单的解决方案是使用一个不会影响计算结果的填充值。例如，如果你在每个桶内执行求和操作，使用零填充意味着对最终结果没有改变，但会为warp中的所有元素提供一致的内存模式和执行路径。对于`min`操作，你可以使用填充值0xFFFFFFFF，反之对于`max`操作使用0。通常不难想到一个可以处理的填充值，但它对结果没有任何贡献。
- en: Once you move to fixed-sized bins, it’s also relatively simple to ensure the
    dataset is generated and accessed in columns, rather than rows. It’s often desirable
    to use shared memory as a staging buffer because of the lack of coalescing requirements.
    This can then be used to allow coalesced reads/writes to global memory.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你转向固定大小的桶，确保数据集是按列而非按行生成和访问也变得相对简单。由于没有合并要求，通常希望使用共享内存作为暂存缓冲区。然后可以用它来允许合并的全局内存读写操作。
- en: Memory accesses to computation ratio
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存访问与计算比率
- en: One question that you should often ask is what is the ratio of memory operations
    to arithmetic operations? You ideally want a ratio of at least 10:1\. That is,
    for every memory fetch the kernel makes from global memory it does 10 or more
    other instructions. These can be array index calculations, loop calculations,
    branches, or conditional evaluations. Every instruction should contribute to useful
    output. Loops, in particular, especially when not unrolled, often simply contribute
    toward instruction overhead and not to any useful work.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该经常问的一个问题是，内存操作与算术操作的比率是多少？理想情况下，你希望比率至少为10:1。也就是说，每次内核从全局内存获取内存时，它会执行10条或更多的其他指令。这些指令可以是数组索引计算、循环计算、分支或条件评估。每条指令都应该对有用的输出做出贡献。特别是循环，尤其是在没有展开的情况下，通常只是贡献到指令开销，而不是任何有用的工作。
- en: If we look inside an SM, architecturally, we see that warps are dispatched to
    sets of CUDA cores based on even and odd instruction dispatchers. Compute 1.x
    devices have a single warp dispatcher and compute 2.x devices have two. In the
    GF100/GF110 chipset (Fermi GTX480/GTX580) there are 32 CUDA cores and four SFUs
    (special-function units) per SM ([Figure 9.9](#F0050)). In the GF104/GF114-based
    devices (GTX460/GTX560) there are 48 CUDA cores and eight SFUs per SM ([Figure
    9.10](#F0055)). Each SM for both compute 2.0 and compute 2.1 devices has a single
    set of 16 LSUs (load store units) that are used to load values to and from memory
    (global, constant, shared, local, and cache).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看一个 SM 内部的结构，从架构上看，warp 是根据奇偶指令调度器分派到 CUDA 核心的。计算 1.x 设备有一个 warp 调度器，而计算
    2.x 设备有两个。在 GF100/GF110 芯片组（Fermi GTX480/GTX580）中，每个 SM 有 32 个 CUDA 核心和 4 个 SFU（特殊功能单元）([图
    9.9](#F0050))。在 GF104/GF114 基础的设备（GTX460/GTX560）中，每个 SM 有 48 个 CUDA 核心和 8 个 SFU
    ([图 9.10](#F0055))。对于计算 2.0 和计算 2.1 设备，每个 SM 都有一组 16 个 LSU，用于从内存（全局、常量、共享、局部和缓存）加载值。
- en: '![image](../images/F000090f09-09-9780124159334.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-09-9780124159334.jpg)'
- en: FIGURE 9.9 Dispatching of CUDA warps (GF100/GF110, compute 2.0).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 CUDA warp 的调度（GF100/GF110，计算 2.0）。
- en: '![image](../images/F000090f09-10-9780124159334.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-10-9780124159334.jpg)'
- en: FIGURE 9.10 Dispatching of CUDA warps (GF104/GF114).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 CUDA warp 的调度（GF104/GF114）。
- en: Thus, in a single cycle, the warp dispatchers issue (or dispatch) a total of
    two (compute 2.0) or four (compute 2.1) instructions, one set from each dispatcher.
    As these come from different warps, the instructions are entirely independent
    of one another. These are then pushed into the pipeline of the execution units
    (CUDA cores, SFUs, and LSUs).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在单个周期中，warp 调度器会发出（或调度）总共两条（计算 2.0）或四条（计算 2.1）指令，来自每个调度器的一组指令。由于这些指令来自不同的
    warps，它们完全独立。然后，这些指令会被推送到执行单元的流水线（CUDA 核心、SFU 和 LSU）。
- en: There are a few implications to this design. First, the absolute minimum number
    of warps that must be present is two for the GF100 series (compute 2.0) hardware
    and four for the GF104 series (compute 2.1) hardware. This in turn implies an
    absolute minimum of 64 or 128 threads per SM, respectively. Having less than this
    means that one or more of the instruction dispatch units will remain idle, effectively
    halving (GF100) the instruction dispatch speed. Using a number of threads other
    than a multiple of 32 will mean some elements of the CUDA cores will idle, again
    undesirable.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计有几个含义。首先，必须存在的 warps 最少数量是 GF100 系列（计算 2.0）硬件的两个，GF104 系列（计算 2.1）硬件的四个。由此推导出，分别至少需要每个
    SM 64 或 128 个线程。如果线程数少于这个数量，意味着一个或多个指令调度单元将保持空闲，实际上会使指令调度速度减半（GF100）。使用不是 32 的倍数的线程数会导致某些
    CUDA 核心元素空闲，这同样是不希望发生的。
- en: Having this minimum number of resident warps provides absolutely no hiding of
    latency, either memory or instruction, based on the ability to switch to another
    warp. A stall in the instruction stream will actually stall the CUDA cores, which
    is highly undesirable. In practice, multiple blocks are allocated to an SM to
    try to ensure this problem never occurs and, more importantly, a variable mix
    of instructions is generated.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个最小数量的驻留 warp 完全无法隐藏延迟，无论是内存延迟还是指令延迟，因为可以切换到另一个 warp。在指令流中出现停顿时，实际上会使 CUDA
    核心停顿，这非常不希望发生。实际上，会将多个块分配给一个 SM，试图确保这个问题永远不会发生，更重要的是，能够生成一个指令的多样化组合。
- en: The second implication is the shared resources limit the ability to continuously
    perform the same operation. Both the CUDA cores and the LSUs are pipelined, but
    are only 16 units wide. Thus, to dispatch an entire warp to either unit takes
    two cycles. On compute 2.0 hardware, only one instruction per dispatcher can be
    dispatched. Thus, to push an operation into the LSUs, one slot in the pipeline
    of one of the CUDA cores must be left empty. There are four possible receivers
    for the dispatch (CUDA, CUDA, SFUs and LSUs), yet only two suppliers per cycle.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个含义是共享资源限制了持续执行相同操作的能力。CUDA 核心和 LSU（负载存储单元）都采用流水线，但每个单元的宽度仅为 16 个。因此，要将整个
    warp 调度到任一单元需要两个周期。在计算 2.0 硬件中，每个调度器每次只能调度一条指令。因此，为了将操作推入 LSU，必须留出一个 CUDA 核心流水线的空槽。调度有四个可能的接收者（CUDA，CUDA，SFU
    和 LSU），但每个周期只有两个供应者。
- en: The situation is drastically improved in compute 2.1 hardware, in that the two
    dispatchers dispatch two instructions each, for a total of four per clock. With
    three sets of CUDA cores it would be possible to supply three arithmetic instructions
    plus a load/save instruction without creating holes in the pipeline.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 2.1 硬件中，情况有了极大改善，因为两个调度器每个调度两个指令，总共每个时钟周期可以调度四条指令。拥有三组 CUDA 核心的情况下，可以提供三条算术指令以及一条加载/保存指令，而不会在流水线中造成空洞。
- en: However, if all warps want to issue an instruction to the same execution unit,
    for example the LSU or SFU, there is a problem. Only a single warp can use the
    LSU per two clock cycles. As the SFU has just eight units, four on compute 2.0
    hardware, a warp can take up to eight cycles to be fully consumed by the SFUs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果所有的 warps 都想向同一个执行单元发出指令，例如 LSU 或 SFU，就会出现问题。每两个时钟周期只能有一个 warp 使用 LSU。由于
    SFU 只有八个单元，其中四个是在计算 2.0 硬件上，因此一个 warp 可能需要最多八个时钟周期才能被 SFU 完全消耗。
- en: Thus, the bandwidth available to and from the LSUs on a compute 2.1 device is
    50% less than a compute 2.0 device with the same number of CUDA cores. Consequently,
    the LSUs or SFUs can become a bottleneck. There need to be other instructions
    in the stream such that the CUDA cores can do some useful work while the memory
    and transcendental instructions progress through the LSU or SFU pipeline.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算 2.1 设备上 LSU 的带宽比相同数量 CUDA 核心的计算 2.0 设备少 50%。因此，LSU 或 SFU 可能成为瓶颈。需要在流中有其他指令，以便在内存和超越指令通过
    LSU 或 SFU 流水线时，CUDA 核心能够进行一些有用的工作。
- en: The Kepler GK104 device (GTX680/Tesla K10) further extends the GF104/114 (GTX460/560)
    design by extending the number of CUDA cores from 48 to 96, and then putting two
    of these within an SM. Thus there are four warp schedulers, eight dispatch units,
    two LSUs and two SFUs per SM.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Kepler GK104 设备（GTX680/Tesla K10）进一步扩展了 GF104/114（GTX460/560）的设计，通过将 CUDA 核心的数量从
    48 扩展到 96，并将其中两个核心放入一个 SM 中。因此，每个 SM 有四个 warp 调度器，八个调度单元，两个 LSU 和两个 SFU。
- en: 'Let’s expand a little on the example we looked at earlier. Consider the case
    of a typical kernel. At the start of the kernel, all threads in all warps fetch
    a 32-bit value from memory. The addresses are such that they can be coalesced.
    For example:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微展开一下之前看到的示例。考虑一个典型的内核的情况。在内核开始时，所有 warps 中的所有线程从内存中获取一个 32 位的值。地址安排得如此合理，以至于它们可以被合并。例如：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This would break down into a multiply and add (MADD) integer instruction, to
    calculate the value to put into the register for the variable `tid`. Variables
    `data`, `b`, and `c` are arrays somewhere in global memory. The variables `data`,
    `a`, and `b` are indexed by `tid` so the address to write to needs to be calculated
    by multiplying `tid` by the size of the elements making up the array. Let’s assume
    they all are integer arrays, so the size is 4 bytes per entry.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将分解为一个乘加（MADD）整数指令，用于计算存入寄存器中的 `tid` 变量的值。`data`、`b` 和 `c` 变量是全局内存中的数组。`data`、`a`
    和 `b` 变量通过 `tid` 索引，因此需要通过将 `tid` 乘以数组元素大小来计算写入地址。假设它们都是整数数组，因此每个条目的大小是 4 字节。
- en: We very quickly hit the first dependency in the calculation of `tid` ([Figure
    9.11](#F0060)). The warp dispatches the multiply of `blockIdx.x` and `blockDim.x`
    to the integer MADD units in the CUDA cores. Until the multiply and add instruction
    to calculate `tid` has completed we can continue no further, so the warp is marked
    as blocked and suspended.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在计算 `tid` 时很快遇到了第一个依赖（见[图 9.11](#F0060)）。warp 将 `blockIdx.x` 和 `blockDim.x`
    的乘法操作分派到 CUDA 核心的整数 MADD 单元中。在计算 `tid` 的乘加指令完成之前，我们无法继续，因此该 warp 被标记为阻塞并挂起。
- en: '![image](../images/F000090f09-11-9780124159334.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-11-9780124159334.jpg)'
- en: FIGURE 9.11 Data flow dependency.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 数据流依赖关系。
- en: At this point, the next warp is selected, which does the same operation and
    is again suspended at the calculation of `tid`. After all warps have progressed
    to this point, enough clocks have passed such that the value of `tid` in warp
    0 is now known and can be fed into the multiply for the destination address calculations.
    Thus, three additional MADD instructions are dispatched to the CUDA cores, to
    calculate the address offsets. The next instruction would be a couple of loads,
    but for this we need the address of `a` and `b` from the multiply instructions.
    At this point we again suspend the warp and the other warps execute.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，选择下一个warp，该warp执行相同的操作，并在计算`tid`时再次暂停。当所有warp都进展到这一点时，足够的时钟周期已经过去，warp 0中`tid`的值现在已知，并可以用于乘法运算中的目标地址计算。因此，三个额外的MADD指令被调度到CUDA核心，用于计算地址偏移量。接下来的指令将是几次加载，但为了实现这一点，我们需要从乘法指令中获取`a`和`b`的地址。在这一点，我们再次暂停该warp，其他warps继续执行。
- en: Once the address calculation of `a` is available, the load instruction can be
    dispatched. It’s likely, due to the address calculation of `b` being issued back
    to back with that of `a`, that the address calculation of `b` will be retired
    by the time the load for `a` has been dispatched. Thus, we immediately issue the
    load for the ‘`b`’. The next instruction in the stream would be a multiply of
    ‘`a`’ and ‘`b`’, neither of which will be available for some time yet as they
    have to be fetched from main memory to the SM. Thus, the warp is suspended and
    the subsequent warps execute to the same point.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`a`的地址计算可用，加载指令可以被调度。由于`b`的地址计算与`a`的计算是连续发布的，因此在`a`的加载指令被调度时，`b`的地址计算很可能已经被执行完成。因此，我们立即发布对`b`的加载指令。数据流中的下一条指令将是`a`和`b`的乘法，但它们都还需要一些时间才能到达，因为它们需要从主内存加载到SM中。因此，该warp被暂停，随后其他warps执行到相同的位置。
- en: As memory fetches take a long time, all warps dispatch the necessary load instructions
    to the LSU and are suspended. If there is no other work to do from other blocks,
    the SM will idle pending the memory transactions completing.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存获取需要较长时间，所有warps将必要的加载指令调度到LSU，并暂停。如果没有来自其他block的工作，SM将在等待内存事务完成时处于空闲状态。
- en: Sometime later `a` finally arrives from the memory subsystem as a coalesced
    read of 128 bytes, a single cache line, or a memory transaction. The 16 LSUs distributes
    64 of the 128 bytes to the registers used by the first half-warp of warp 0\. In
    the next cycle, the 16 LSUs distribute the remaining 64 bytes to the register
    used by the other half-warp. However, warp 0 still can not progress as it has
    only one of the two operands it needs for the multiply. It thus does not execute
    and the subsequent bytes arriving from the coalesced read of `a` for the other
    warps are distributed to the relevant registers for those warps.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，`a`终于从内存子系统到达，作为一次128字节的合并读取，一个缓存行，或者一个内存事务。16个LSU将128字节中的64字节分发到warp
    0的第一个半warp所使用的寄存器中。在下一个周期，16个LSU将剩余的64字节分发到另一个半warp所使用的寄存器中。然而，warp 0仍然无法推进，因为它只得到了进行乘法运算所需的两个操作数中的一个。因此，它不会执行，随后从`a`的合并读取中到达的字节会分发到其他warps相关的寄存器中。
- en: By the time all of the data from the coalesced read for `a` has been distributed
    to the registers of all the other warps, the data for `b` will likely have arrived
    in the L1 cache. Again, the 16 LSUs distribute the first 64 bytes to the registers
    of the first half-warp of warp 0\. In the subsequent cycle they distribute the
    second 64 bytes to the second half-warp.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有从`a`的合并读取中获得的数据分发到所有其他warps的寄存器时，`b`的数据可能已经到达L1缓存。同样，16个LSU将前64字节分发到warp
    0第一个半warp的寄存器中。在随后的周期，它们将第二64字节分发到第二个半warp。
- en: At the start of this second cycle, the first half-warp is able to progress the
    multiply instruction for `a[tid] ∗ b[tid]`. In the third cycle the LSUs start
    providing data to the first half-warp of warp 0\. Meanwhile, the second half-warp
    of warp 0 starts the execution of the multiply. As the next instruction in warp
    0 would be a store and is dependent on the multiply, warp 0 is suspended.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个周期的开始，第一个半warp能够推进`a[tid] ∗ b[tid]`的乘法指令。在第三个周期，LSUs开始为warp 0的第一个半warp提供数据。同时，warp
    0的第二个半warp开始执行乘法操作。由于warp 0的下一个指令是一个存储指令，并且依赖于乘法运算，因此warp 0被暂停。
- en: Providing there are on the order of 18–22 warps resident, by the time the last
    warp has dispatched the final multiply, the multiply will have completed for warp
    0\. It can then dispatch the store instructions to the 16 LSUs and complete its
    execution. The other warps then do exactly the same and the kernel is complete.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设大约有18到22个warp驻留，当最后一个warp调度完最后的乘法操作时，warp 0的乘法操作已经完成。它接着可以将存储指令调度到16个LSU并完成执行。其他的warp则做完全相同的事情，内核执行完毕。
- en: Now consider the case of (see [Figure 9.12](#F0065)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下情况（见[图 9.12](#F0065)）。
- en: '![image](../images/F000090f09-12-9780124159334.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-12-9780124159334.jpg)'
- en: FIGURE 9.12 Dual data flow dependency.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 双数据流依赖。
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By halving the number of blocks, we can process two elements per thread. Notice
    this introduces an independent execution stream into each thread of the warp.
    Thus, the arithmetic operations start to overlap with the load operations.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将块数减半，我们可以使每个线程处理两个元素。注意，这会在每个warp的线程中引入一个独立的执行流。因此，算术操作开始与加载操作重叠。
- en: 'However, as the example C code is written, this will not help. This is because
    the code contains dependencies that are not immediately obvious. The write operation
    to the first element of `data` could affect the value in either the `a` or the
    `b` array. That is, the address space of `data` may overlap with `a` or `b`. Where
    you have a write in the data flow to global memory, you need to lift out the reads
    to the start of the kernel. Use the following code instead:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如示例C代码所写的那样，这并不会起作用。因为代码包含了一些不立即显现的依赖关系。对`data`的第一个元素的写操作可能会影响`a`或`b`数组中的值。也就是说，`data`的地址空间可能与`a`或`b`重叠。当在数据流中有对全局内存的写操作时，你需要将读取操作提前到内核的开始处。请改用以下代码：
- en: '[PRE18]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: or
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE19]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`data[tid] = a_vect ∗ b_vect;`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`data[tid] = a_vect * b_vect;`'
- en: We have two choices, a scalar approach or a vector approach. The GPU supports
    only vector loads and saves, not vector operations, in hardware. Thus, the multiplication
    is actually done as an overloaded operator in C++ and simply multiplies the two
    integers independently of one another. However, the vector loads and saves two
    64-bit loads and a single 64-bit save, respectively, instead of the four separate
    32-bit loads and a single 32-bit save with the nonvector version. Thus, 40% of
    the memory transactions are eliminated. The memory bandwidth usage is the same,
    but less memory transactions mean less memory latency, and therefore any stall
    time waiting for memory is reduced.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种选择，一种是标量方法，一种是向量方法。GPU硬件只支持向量加载和保存，而不支持向量运算。因此，乘法实际上是作为C++中的重载运算符进行的，它仅仅独立地将两个整数相乘。然而，向量加载和保存分别是两个64位加载和一个64位保存，而非向量版本是四个32位加载和一个32位保存。因此，40%的内存事务被消除了。内存带宽的使用保持不变，但更少的内存事务意味着更少的内存延迟，因此等待内存的停顿时间得以减少。
- en: To use the vector types, simply declare all arrays as type `int2`, which is
    an in-built vector type of two integers. Supported types are `int2`, `int3`, `int4`,
    `float2`, `float3`, and `float4`. You can of course create your own types, such
    as `uchar4`, and define your own operators. Each vector type is actually just
    an aligned structure with *N* named member elements of the base type.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用向量类型，只需将所有数组声明为`int2`类型，这是一个内置的包含两个整数的向量类型。支持的类型有`int2`、`int3`、`int4`、`float2`、`float3`和`float4`。当然，你也可以创建自己的类型，如`uchar4`，并定义你自己的运算符。每种向量类型实际上只是一个对齐的结构，具有*N*个命名的基本类型成员元素。
- en: Thus, I hope you can actually see that a balance is therefore required between
    the different types of instructions. This becomes somewhat more critical with
    the compute 2.1 devices (GF104 series) where there are three sets of CUDA cores
    sharing the same resources within the SM. The change in compute 2.0 to compute
    2.1 devices added significantly more arithmetic capacity within the SM without
    providing additional data transport capacity. The compute 2.0 devices have up
    to 512 CUDA cores on a bus of up to 384 bits wide, giving a ratio of 1:3 of cores
    to memory bandwidth. The compute 2.1 devices have up to 384 CUDA cores on a bus
    of up to 256 bits, giving a ratio of 1:5 cores to memory bandwidth. Thus, compute
    2.0 devices are more suited to applications that are memory bound, whereas compute
    2.1 devices are more suited to applications that are compute bound.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我希望你能意识到，在不同类型的指令之间需要保持平衡。这在计算 2.1 设备（GF104 系列）中变得尤为关键，其中有三组 CUDA 核心共享 SM
    内相同的资源。从计算 2.0 到计算 2.1 设备的变化，在 SM 内显著增加了算术能力，但并未提供额外的数据传输能力。计算 2.0 设备在总线宽度最高为
    384 位的情况下，最多有 512 个 CUDA 核心，核心与内存带宽的比率为 1:3。计算 2.1 设备在总线宽度最高为 256 位的情况下，最多有 384
    个 CUDA 核心，核心与内存带宽的比率为 1:5。因此，计算 2.0 设备更适合内存受限的应用，而计算 2.1 设备更适合计算受限的应用。
- en: In practice, this is balanced in the compute 2.0 devices by having up to 33%
    more CUDA cores. The compute 2.1 devices, however, typically also run at somewhat
    higher clock rates, both in terms of the internal clock speed and also the external
    memory bus speed. This helps significantly in rebalancing the smaller memory bus
    width but is generally not sufficient to allow compute 2.1 devices to outperform
    their 2.0 counterparts.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，计算 2.0 设备通过拥有多达 33% 的更多 CUDA 核心来平衡这一点。然而，计算 2.1 设备通常也以稍高的时钟频率运行，无论是内部时钟速度，还是外部内存总线速度。这有助于在一定程度上重新平衡较小的内存总线宽度，但通常不足以让计算
    2.1 设备超越它们的 2.0 对应设备。
- en: What is important to realize, especially with compute 2.1 devices, is that there
    needs to be sufficient arithmetic density to the instruction stream to make good
    use of the CUDA cores present on the SMs. A kernel that simply does loads or stores
    and little else will not achieve anything like the peak performance available
    from these devices. Expand such kernels to also include independent instruction
    flow via processing two, four, or eight elements per thread. Use vector operations
    where possible.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于计算 2.1 设备，需要认识到的是，指令流中必须有足够的算术密度，才能充分利用 SM 上的 CUDA 核心。一个仅仅进行加载或存储的内核，如果做得不多，将无法达到这些设备所能提供的峰值性能。扩展这样的内核，使其通过每个线程处理两个、四个或八个元素来包含独立的指令流。尽可能使用向量操作。
- en: Loop and kernel fusion
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环与内核融合
- en: Another area where we can significantly save on memory bandwidth is a technique
    based on loop fusion we looked at in the last section. Loop fusion is where two
    apparently independent loops run over an intersecting range. For example, loop
    1 runs from 0 to 100 and loop 2 from 0 to 200\. The code for loop 2 can be fused
    with the code for loop 1, for at least the first 100 iterations. This increases
    the level of instruction-level parallelism, but also decreases the overall number
    of iterations by a third.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显著节省内存带宽的另一个技术是基于上节中提到的循环融合。循环融合是指两个看似独立的循环在交叉的范围内运行。例如，循环 1 从 0 运行到 100，循环
    2 从 0 运行到 200\. 循环 2 的代码可以与循环 1 的代码融合，至少在前 100 次迭代中。这增加了指令级并行性，但也减少了总的迭代次数，约为三分之一。
- en: Kernel fusion is a variation on loop fusion. If you have a number of kernels
    that are run in sequence, one after the other, are there elements of these kernels
    that can be fused? Be careful doing this with kernels you did not write or do
    not fully understand. Invoking two kernels in series generates an implicit synchronization
    between them. This may have been intended by design and, as it’s implicit, probably
    only the original designer is aware of it.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合是循环融合的一种变体。如果你有一系列顺序执行的内核，这些内核中是否有可以融合的部分？在处理你没有编写或完全理解的内核时要小心。连续调用两个内核会在它们之间产生隐式同步。这可能是设计时就有意为之的，并且由于是隐式的，可能只有原设计者知道这一点。
- en: In developing kernels it’s quite common to break down the operation into a number
    of phases or passes. For example, in the first pass you might calculate the results
    over the whole dataset. On the second pass you may filter data for certain criteria
    and perform some further processing on certain points. If the second pass can
    be localized to a block, the first and second pass can usually be combined into
    a single kernel. This eliminates the write to main memory of the first kernel
    and the subsequent read of the second, as well as the overhead of invoking an
    additional kernel. If the first kernel is able to write the results to shared
    memory, and you only need those results for the second pass, you eliminate the
    read/write to global memory entirely. Reduction operations often fall into this
    category and can benefit significantly from such an optimization, as the output
    of the second phase is usually many times smaller than the first phase, so it
    saves considerably on memory bandwidth.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发内核时，通常会将操作分解成多个阶段或多个通道。例如，在第一次通道中，你可能会计算整个数据集的结果。 在第二次通道中，你可能会对数据进行某些条件筛选，并对某些数据点进行进一步处理。
    如果第二次通道可以局部化到一个块中，第一次和第二次通道通常可以合并为一个内核。 这样可以消除第一次内核写入主内存和第二次内核读取的操作，以及调用额外内核的开销。
    如果第一次内核能够将结果写入共享内存，并且你只在第二次通道中需要这些结果，那么你就完全消除了对全局内存的读/写操作。 减少操作通常属于这一类，并且能够从这样的优化中受益，因为第二阶段的输出通常比第一阶段小得多，从而显著节省了内存带宽。
- en: Part of the reason why kernel fusion works so well is because of the data reuse
    it allows. Fetching data from global memory is slow, on the order of 400–600 clock
    cycles. Think of memory access like reading something from disk. If you’ve ever
    done any disk I/O, you’ll know that reading a file by fetching one character at
    time is very slow and using `fread` to read large blocks is far more efficient
    than repeatedly calling read character functions like `fgetch`. Having read the
    data in, you keep it in memory. Apply the same approach to accessing global memory.
    Fetch data in chunks of up to 16 bytes per thread (`float4`, `int4`), not in single
    bytes or words. Once you have each thread successfully processing a single element,
    switch to `int2` or `float2` and process two. Moving to four may or may not help,
    but moving from one to two often does. Once you have the data, store it in shared
    memory, or keep it in the register set and reuse it as much as possible.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合效果如此显著的部分原因在于它所允许的数据重用。 从全局内存中提取数据很慢，大约需要400-600个时钟周期。 可以将内存访问想象成从磁盘读取数据。
    如果你做过磁盘I/O操作，你就会知道，一次读取一个字符的速度非常慢，而使用`fread`一次读取大块数据比反复调用读取单个字符的函数（如`fgetch`）要高效得多。
    读取数据后，你将其保存在内存中。 将这种方法应用到全局内存的访问上。 每个线程一次提取最多16字节的数据（`float4`、`int4`），而不是一次提取单个字节或字。
    一旦每个线程成功处理了一个元素，切换到`int2`或`float2`，同时处理两个元素。 转向四个元素可能没有太大帮助，但从一个到两个的切换通常会有帮助。
    一旦你获得了数据，就将其存储在共享内存中，或者保存在寄存器集中并尽可能多次重用。
- en: Use of shared memory and cache
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用共享内存和缓存
- en: Using shared memory can provide a 10:1 increase in speed over global memory,
    but is limited in size—48 K on Fermi/Kepler devices and 16 K on all the previous
    devices. This may not sound like a great deal of space, especially with multigigabyte
    memory systems found on the host, but this is actually per SM. Thus, a GTX580
    or Tesla M2090 has 16 SMs active per GPU, each of which provides 48 K of shared
    memory, a total of 768 K. This is memory that runs at L1 cache speed. In addition,
    you have 768 K of L2 cache memory (on 16 SM devices) that is shared between all
    the SMs. This allows for an order of magnitude faster, global memory, atomic operations
    than in previous generation GPUs.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享内存相比全局内存可以提供10:1的速度提升，但其大小是有限的——在Fermi/Kepler设备上为48K，在所有较早的设备上为16K。 这听起来可能不算很大，特别是在主机上拥有数GB内存的情况下，但实际上这是每个SM的大小。
    因此，一块GTX580或Tesla M2090 GPU上每个GPU有16个SM，每个SM提供48K的共享内存，总共是768K。 这些内存以L1缓存的速度运行。
    此外，你还拥有768K的L2缓存内存（在16个SM设备上），它在所有SM之间共享。 这使得全局内存的原子操作比上一代GPU快一个数量级。
- en: When you consider that a GTX580 comes with 1.5 GB of memory, 768 K means just
    a tiny fraction of that memory space can be held in cache at any one point in
    time. The equivalent Tesla card comes with 6 GB of memory. Thus, kernels that
    iterate over datasets need to be aware that they may be using either the cache
    or shared memory in an ineffective manner, if they are not reusing data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到GTX580配备了1.5 GB的内存，768 K意味着在任何时刻，只有极小的一部分内存空间可以被缓存。相应的Tesla卡配备了6 GB的内存。因此，需要反复遍历数据集的内核需要意识到，如果没有重用数据，它们可能会以无效的方式使用缓存或共享内存。
- en: Rather than a number of passes over a large dataset, techniques such as kernel
    fusion can be used to move through the data as opposed to passing over it multiple
    times. Think of the problem in terms of the output data and not the input data.
    Construct the problem such that you assign threads to output data items, not input
    data items. Create a fan in and not a fan out in terms of data flow. Have a preference
    for gather (collecting data) primitives, rather than scatter (distributing data)
    primitives. The GPU will broadcast data, both from global memory and the L2 cache,
    directly to each SM. This supports high-speed gather-type operations.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 与其多次遍历大型数据集，不如使用像内核融合这样的技术，通过数据而不是多次遍历数据。将问题看作是关于输出数据，而不是输入数据。构建问题时，将线程分配给输出数据项，而不是输入数据项。在数据流的方向上要有扇入（fan
    in），而不是扇出（fan out）。倾向于使用收集（gather）数据的原语，而不是分发（scatter）数据的原语。GPU将从全局内存和L2缓存直接广播数据到每个SM，这支持高速度的收集类型操作。
- en: 'On Fermi and Kepler we have a very interesting choice, to configure the shared
    memory to either prefer L1 cache (48 K L1 cache, 16 K shared) or to prefer shared
    (48 K shared, 16 K cache). By default the device will prefer shared memory, and
    thus you’ll have 48 K of shared memory available. This decision is not fixed,
    but set at runtime, and thus can be set per kernel call. Kernels that do not make
    use of shared memory, or keep to the 16 K limit to ensure compatibility with earlier
    GPUs, usually benefit significantly (10% to 20% performance gain) by enabling
    the additional 32 K of cache, disabled by default:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fermi和Kepler架构中，我们有一个非常有趣的选择，可以将共享内存配置为偏好L1缓存（48 K L1缓存，16 K共享）或偏好共享内存（48 K共享，16
    K缓存）。默认情况下，设备将偏好共享内存，因此你将拥有48 K的共享内存可用。这个决定不是固定的，而是在运行时设置的，因此可以按每个内核调用来设置。那些不使用共享内存，或保持在16
    K限制内以确保与早期GPU兼容的内核，通常通过启用额外的32 K缓存（默认情况下禁用）会显著受益（性能提升10%到20%）：
- en: '[PRE20]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: where the `cache_prefer` parameter is `cudaFuncCachePreferShared` for 48 K of
    shared memory and 16 K of L1 cache, or `cudaFuncCachePreferL1` for 48 K of cache
    memory and 16 K of shared memory. Note, Kepler also allows a 32 K/32 K split.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`cache_prefer`参数为`cudaFuncCachePreferShared`时，表示48 K共享内存和16 K L1缓存，或`cudaFuncCachePreferL1`时，表示48
    K缓存内存和16 K共享内存。注意，Kepler还允许32 K/32 K的分割方式。
- en: There are, however, some areas where the cache causes Fermi and Kepler to operate
    slower than previous generation GPUs. On compute 1.x devices, memory transactions
    would be progressively reduced in size to as little as 32 bytes per access if
    the data item was small. Thus, a kernel that accesses one data element from a
    widely dispersed area in memory will perform poorly on any cache-based architecture,
    CPU, or GPU. The reason for this is that a single-element read will drag in 128
    bytes of data. For most programs, the data brought into the cache will then allow
    a cache hit on the next loop iteration. This is because programs typically access
    data close in memory to where they previously accessed data. Thus, for most programs
    this is a significant benefit. However, for programs that only need one data element,
    the other 124 bytes are wasted. For such kernels, you have to configure the memory
    subsystem to fetch only the memory transactions it needs, not one that is cache
    line sized. You can do this only at compile time via the `-Xptxas –dlcm=cg` flag.
    This reduces all access to 32 bytes per transaction and disables the L1 cache.
    For read only data consider also using either texture or constant memory.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一些地方，缓存导致Fermi和Kepler的性能比上一代GPU更慢。在计算1.x设备上，如果数据项较小，内存事务的大小会逐步减少，直到每次访问只有32字节。因此，一个从广泛分散的内存区域访问单个数据元素的内核，在任何基于缓存的架构、CPU或GPU上表现都不好。原因是单元素读取会引入128字节的数据。对于大多数程序来说，引入缓存的数据将在下一次循环迭代时产生缓存命中。这是因为程序通常会访问接近上次访问数据的位置。因此，对于大多数程序来说，这是一项重要的优势。然而，对于只需要一个数据元素的程序，其它124字节的数据会浪费掉。对于这种内核，你必须配置内存子系统，仅获取它所需要的内存事务，而不是缓存行大小的事务。你只能通过`-Xptxas
    –dlcm=cg`标志在编译时进行此操作。这将把所有访问减少到每次事务32字节，并禁用L1缓存。对于只读数据，也可以考虑使用纹理或常量内存。
- en: With G80/GT200, compute 1.x hardware, it’s essential that you make use of shared
    memory as an integral part of the kernel design. Without cached accessed to data,
    be it explicitly via shared memory or implicitly via a hardware-managed cache,
    memory latency times are just huge. The arrival of cache on GPUs via the Fermi
    architecture has made it much, much easier to write at a program, or kernel, that
    performs at least reasonably well on the GPU.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于G80/GT200，计算1.x硬件，确保将共享内存作为内核设计的一个组成部分是至关重要的。如果没有通过共享内存显式访问数据，或者通过硬件管理的缓存隐式访问数据，内存延迟时间将非常大。通过Fermi架构，GPU上缓存的到来使得编写至少在GPU上合理运行的程序或内核变得更加容易。
- en: Let’s look at some of the obstacles to using shared memory. The first is the
    size available—16 K on compute 1.x hardware and up to 48 K on compute 2.x hardware.
    It can be allocated statically at compile time via the `__shared__` prefix for
    variables. It is also one of the optional parameters in a kernel call, that is,
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下使用共享内存的一些障碍。第一个是可用的大小——在计算1.x硬件上为16 K，而在计算2.x硬件上为48 K。它可以通过`__shared__`前缀在编译时静态分配给变量。它也是内核调用中的一个可选参数，即，
- en: '[PRE21]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With runtime allocation, you additionally need a pointer to the start of the
    memory. For example,
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用运行时分配时，你还需要一个指向内存开始位置的指针。例如，
- en: '[PRE22]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that L2 cache size in Fermi is not always 768 K as stated in the CUDA C
    programmer guide. In fact, the L2 cache is based on the type of device being used
    and the number of SMs present. Compute 2.1 devices may have less L2 cache than
    compute 2.0 devices. Even compute 2.0 devices without all the SMs enabled (GTX470,
    GTX480, GTX570) have less than 768 K of L2 cache. The GTX460 device we’re using
    for testing has 512 K of L2 cache and the GTX470 device has 640 K.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Fermi中的L2缓存大小并不总是如CUDA C程序员指南中所述的768 K。事实上，L2缓存的大小取决于所使用的设备类型和存在的SM数量。计算2.1设备可能比计算2.0设备具有更少的L2缓存。即使是没有启用所有SM的计算2.0设备（如GTX470、GTX480、GTX570），它们的L2缓存也少于768
    K。我们用于测试的GTX460设备有512 K的L2缓存，而GTX470设备有640 K。
- en: The size of the L2 cache is returned from a call to `cudaGetDeviceProperties`
    API as `l2CacheSize` member.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: L2缓存的大小通过调用`cudaGetDeviceProperties` API返回，作为`l2CacheSize`成员。
- en: Section summary
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部分总结
- en: • Think carefully about the data your kernel processes and how best to arrange
    this in memory.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: • 仔细考虑你的内核处理的数据以及如何最好地将其安排在内存中。
- en: • Optimize memory access patterns for coalesced 128-byte access, aligning with
    the 128-byte memory fetch and L1 cache line size.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: • 优化内存访问模式，以实现合并的128字节访问，确保与128字节内存获取和L1缓存行大小对齐。
- en: • Consider the single-/double-precision tradeoff and how this impacts memory
    usage.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: • 考虑单精度/双精度的权衡，以及这对内存使用的影响。
- en: • Fuse multiple kernels to single kernels where appropriate.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: • 在适当的情况下，将多个内核融合为单个内核。
- en: • Make optimal use of shared memory and cache, ensuring you’re making full use
    of the expanded size on later compute levels.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: • 充分利用共享内存和缓存，确保在后续计算级别上充分利用扩展的内存大小。
- en: 'Strategy 3: Transfers'
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 3：数据传输
- en: Pinned memory
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 锁定内存
- en: To work on a dataset you need to transfer the data from the host to the device,
    work on the dataset, and transfer the results back to the host. Performed in a
    purely serial manner, this causes periods where both the host and GPU are inactive,
    both in terms of unused transfer capacity and compute capacity.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理数据集，你需要将数据从主机传输到设备，处理数据集，然后将结果传回主机。如果完全以串行方式执行，这将导致主机和GPU在某些时期都处于非活动状态，既包括未使用的传输容量，也包括未使用的计算容量。
- en: We looked in detail in the chapter on multi-GPU usage at how to use streams
    to ensure the GPU always has some work to do. With a simple double-buffering technique,
    while the GPU is transferring back the results and acquiring a new work packet,
    the other buffer is being used by the compute engine to process the next data
    block.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多GPU使用章节中详细讨论了如何使用流来确保GPU始终有工作可做。通过一个简单的双缓冲技术，当GPU将结果传输回并获取新的工作包时，另一个缓冲区将被计算引擎用来处理下一个数据块。
- en: The host processor supports a virtual memory system where a physical memory
    page can be marked as swapped out. It can then be paged to disk. Upon an access
    by the host processor to that page, the processor loads the page back in from
    disk. It allows the programmer to use a much larger virtual address space than
    is actually present on the hardware. Given that the programs typically exhibit
    quite good locality, this allows the total memory space to be much larger than
    the physical limits allow. However, if the program really does need 8 GB and the
    host only has 4 GB, the performance will typically be poor.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 主机处理器支持虚拟内存系统，可以将物理内存页面标记为已交换出，然后将其分页到磁盘。当主机处理器访问该页面时，处理器将从磁盘加载该页面。它允许程序员使用比硬件实际存在的内存空间要大的虚拟地址空间。考虑到程序通常表现出较好的局部性，这使得总内存空间可以比物理限制更大。然而，如果程序确实需要8
    GB的内存，而主机只有4 GB内存，性能通常会较差。
- en: Arguably the use of virtual memory is a hangover from a time when memory capacities
    were very limited. Today you can purchase 16 GB of memory for a little over 100
    euros/dollars/pounds, meaning the host’s need to use virtual memory is almost
    eliminated for most applications.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，虚拟内存的使用是从内存容量非常有限的时代遗留下来的。今天，你可以花费略高于100欧元/美元/英镑购买16 GB内存，这意味着大多数应用程序几乎不再需要使用虚拟内存。
- en: Most programs, except for big data problems, will generally fit within the host
    memory space. If not, then there are special server solutions that can hold up
    to 128 GB of memory per node. Such solutions are often preferable, as they allow
    you to keep the data within one node rather than add the complexity of a multinode
    solution. Of course, loading the dataset in chunks is perfectly feasible, but
    then you are ultimately limited by the throughput of the I/O hardware.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 除了大数据问题外，大多数程序通常可以适应主机内存空间。如果不能，便有一些特殊的服务器解决方案，每个节点可支持多达128 GB的内存。此类解决方案通常更为理想，因为它们能让你将数据保持在单个节点内，而不是增加多节点解决方案的复杂性。当然，将数据集分块加载也是完全可行的，但这样最终会受到I/O硬件吞吐量的限制。
- en: You should always be using page-locked memory on a system that has a reasonable
    amount of host memory. Page-locked memory allows the DMA (direct memory access)
    controller on the GPU to request a transfer to and from host memory without the
    involvement of the CPU host processor. Thus, no load is placed onto the host processor
    in terms of managing a transfer or having to bring back from disk any pages that
    have been swapped out.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有合理主机内存的系统上，你应该始终使用页锁定内存。页锁定内存允许GPU的DMA（直接内存访问）控制器请求进行主机内存之间的传输，而无需CPU主机处理器参与。因此，主机处理器在管理传输或从磁盘加载任何被交换出去的页面时，不会增加负担。
- en: The PCI-E transfers in practice can only be performed using DMA-based transfer.
    The driver does this in the background when you don’t use page-locked memory directly.
    Thus, the driver has to allocate (or malloc) a block of paged-locked memory, do
    a host copy from the regular memory to the page-locked memory, initiate the transfer,
    wait for the transfer to complete, and then free the page-locked memory. All of
    this takes time and consumes precious CPU cycles that could be used more productively.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，PCI-E传输只能通过基于DMA的传输执行。当你不直接使用页面锁定内存时，驱动程序会在后台执行此操作。因此，驱动程序必须分配（或malloc）一个页面锁定内存块，从常规内存到页面锁定内存进行主机复制，启动传输，等待传输完成，然后释放页面锁定内存。所有这些都会耗费时间，并消耗宝贵的CPU周期，这些CPU周期本可以用于更高效的工作。
- en: Memory allocated on the GPU is by default allocated as page locked simply because
    the GPU does not support swapping memory to disk. It’s the memory allocated on
    the host processor we’re concerned with. To allocate page-locked memory we need
    to either allocate it using the special `cudaHostMalloc` function or allocate
    it with the regular `malloc` function and register it as page-locked memory.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上分配的内存默认是页面锁定的，因为GPU不支持将内存换出到磁盘。我们关注的是主机处理器上分配的内存。为了分配页面锁定内存，我们需要通过特殊的`cudaHostMalloc`函数分配它，或者使用常规的`malloc`函数并将其注册为页面锁定内存。
- en: Registering memory simply sets some internal flags to ensure the memory is never
    swapped out and also tells the CUDA driver that this memory is page-locked memory
    so it is able to use it directly rather than using a staging buffer.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 注册内存简单地设置一些内部标志，确保内存永远不会被换出，并且还告诉CUDA驱动程序该内存是页面锁定内存，因此它可以直接使用这块内存，而不是使用暂存缓冲区。
- en: As with `malloc`, if you use `cudaHostAlloc` you need to use the `cudaFreeHost`
    function to free this memory. Do not call the regular C free function with pointers
    allocated from `cudaHostAlloc` or you will likely get a crash, some undefined
    behavior, or a strange error later in your program.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与`malloc`一样，如果你使用`cudaHostAlloc`，需要使用`cudaFreeHost`函数来释放这块内存。不要对通过`cudaHostAlloc`分配的指针调用常规C语言的free函数，否则可能会导致程序崩溃、出现未定义行为或在程序后续执行时出现奇怪的错误。
- en: The prototype for `cudaHostAlloc` is
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAlloc`的原型是'
- en: '[PRE27]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The flags consist of the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标志包括以下内容：
- en: '`cudaHostAllocDefault`—Use for most cases. Simply specifies the default behavior.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocDefault`—用于大多数情况。仅指定默认行为。'
- en: '`cudaHostAllocWriteCombined`—Use for memory regions that will be transferred
    *to the device only.* Do not use this flag when the host will read from this memory
    area. This turns off the caching of the memory region on the host processor, which
    means it completely ignores the memory region during transfers. This speeds up
    transfer to the device with certain hardware configurations.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocWriteCombined`—用于仅将内存区域传输*到设备*。当主机从此内存区域读取时，不要使用此标志。这将关闭主机处理器对内存区域的缓存，也就是说，它在传输过程中完全忽略该内存区域。这会在某些硬件配置下加快向设备的传输速度。'
- en: '`cudaHostAllocPortable`—The page-locked memory becomes page locked and visible
    in all CUDA contexts. By default the allocation belongs to the context creating
    it. You must use this flag if you plan to pass the pointer between CUDA contexts
    or threads on the host processor.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocPortable`—页面锁定内存变为页面锁定，并在所有CUDA上下文中可见。默认情况下，分配内存属于创建它的上下文。如果你计划在CUDA上下文或主机处理器的线程之间传递指针，必须使用此标志。'
- en: '`cudaHostAllocMapped`—We’ll look at this shortly. It allocates host memory
    into device memory space, allowing the GPU kernel to directly read and write with
    all transfers being implicitly handled.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAllocMapped`—我们稍后会详细讨论。它将主机内存分配到设备内存空间，使得GPU内核可以直接读写，所有传输都将被隐式处理。'
- en: To demonstrate the effect of paged memory versus nonpaged memory, we wrote a
    short program. This simply does a number of transfers, varied by size to and from
    a device, and invokes a dummy kernel to ensure the transfers actually take place.
    The results are shown in [Figure 9.13](#F0070).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示页面内存和非页面内存的效果，我们编写了一个简短的程序。该程序进行了一些大小不同的传输，来回传输数据，并调用一个虚拟内核以确保实际发生了传输。结果显示在[图
    9.13](#F0070)中。
- en: '![image](../images/F000090f09-13-9780124159334.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-13-9780124159334.jpg)'
- en: FIGURE 9.13 Transfer speed to and from the device (AMD Phenom II X4 905e, PCI-E
    2.0 X8 link).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 设备与设备之间的传输速度（AMD Phenom II X4 905e，PCI-E 2.0 X8链接）。
- en: On the Y axis we have MB/second to or from the device and the transfer size
    in bytes along the X axis. What we can see from the chart is that there is a considerable
    difference between using paged memory and nonpaged memory, the page-locked (pinned)
    memory being 1.4× faster for writes and 1.8× faster for reads. It took 194 ms
    to send out 512 MB of data to the card using page-locked memory, as opposed to
    278 ms to do this with nonpaged memory. Timings to transfer data from the device,
    for comparison, were 295 ms for paged memory versus 159 ms for pinned memory.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Y 轴上，我们显示了设备到设备的数据传输速度（MB/秒），而 X 轴则表示传输数据的大小（字节）。从图表中可以看到，使用分页内存和非分页内存之间有明显的差异，页面锁定（固定）内存在写入速度上比非分页内存快
    1.4 倍，在读取速度上快 1.8 倍。使用页面锁定内存向显卡发送 512 MB 数据需要 194 毫秒，而使用非分页内存则需要 278 毫秒。为了做对比，从设备传输数据的时间是：分页内存需要
    295 毫秒，而固定内存只需要 159 毫秒。
- en: 'On the input side, we see a strange issue: With page-locked memory, the bandwidth
    *from* the device is 20% higher than *to* the device. Given that PCI-E provides
    for a full duplex connection of the same speed to and from the device, you’d expect
    to see a similar transfer speed for both reads and writes. This variation, as
    you will see in subsequent tests, is very hardware dependent. All the systems
    tested except the Intel Nehalem I7 system exhibiting it to varying degrees.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入端，我们看到一个奇怪的问题：使用页面锁定内存时，从设备传输的数据带宽比向设备传输的数据带宽高出 20%。考虑到 PCI-E 提供的全双工连接应当是相同的速度来回传输，你本应该看到读取和写入的传输速度相似。正如你在后续测试中看到的那样，这种差异非常依赖于硬件。所有被测试的系统中，除了
    Intel Nehalem I7 系统外，都有不同程度的这种现象。
- en: Transfer rates to and from the four devices were almost identical, which is
    to be expected given the bandwidth of global memory on all of the cards is at
    least an order of magnitude greater than the PCI-E bandwidth.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从四个设备的传输速率来看，它们几乎是相同的，这也符合预期，因为所有显卡的全局内存带宽至少比 PCI-E 带宽高一个数量级。
- en: What is also very noticable is that to get near-peak bandwidth, even with pinned
    memory, the transfer size needs to be on the order of 2 MB of data. In fact, we
    don’t achieve the absolute peak until the transfer size is 16 MB or beyond.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个显著的现象是，即使使用固定内存，要接近最大带宽，传输数据的大小也需要在 2 MB 左右。事实上，直到传输数据大小达到 16 MB 或更大时，我们才能达到绝对的最大带宽。
- en: For comparison, the results are also shown in [Figures 9.14](#F0075), [9.15](#F0080)
    and [9.16](#F0085) for a number of systems we tested.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做对比，测试中使用的多个系统的结果也显示在 [图 9.14](#F0075)、[9.15](#F0080) 和 [9.16](#F0085) 中。
- en: '![image](../images/F000090f09-14-9780124159334.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-14-9780124159334.jpg)'
- en: FIGURE 9.14 Transfer speed to and from the device (Intel Atom D525, PCI-E 2.0
    X1 link).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 设备的传输速率（Intel Atom D525，PCI-E 2.0 X1 链接）。
- en: '![image](../images/F000090f09-15-9780124159334.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-15-9780124159334.jpg)'
- en: FIGURE 9.15 Transfer speed to and from the device (Intel I3 540, PCI-E X16 link).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 设备的传输速率（Intel I3 540，PCI-E X16 链接）。
- en: '![image](../images/F000090f09-16-9780124159334.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-16-9780124159334.jpg)'
- en: FIGURE 9.16 Transfer speed to and from the device (Intel I7 920, PCI-E X16 link).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 设备的传输速率（Intel I7 920，PCI-E X16 链接）。
- en: '[Figure 9.14](#F0075) shows a small netbook based on Intel’s low-power ATOM
    device, equipped with a dedicated GT218 NVIDIA ION graphics card. The peak PCI-E
    bandwidth you can typically see is up to 5 GB/s when using a 2.0 X16 link. As
    this netbook uses an X1 link, we could expect a maximum of 320 MB/s and we see
    in the order of 200 MB/s.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.14](#F0075) 显示了一款基于英特尔低功耗 ATOM 设备的小型笔记本，配备了专用的 GT218 NVIDIA ION 显卡。当使用
    2.0 X16 链接时，通常可以看到的最大 PCI-E 带宽高达 5 GB/s。由于这款笔记本使用的是 X1 链接，我们预计最大带宽为 320 MB/s，而实际看到的带宽大约为
    200 MB/s。'
- en: However, we see a very similar pattern to the AMD system, in that we need around
    2 MB plus transfer sizes before we start to achieve anything like the peak transfer
    rate. The only difference we see is there is a noticable difference between transfers
    to the device and transfers from the device.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们看到的模式与 AMD 系统非常相似，即在开始接近最大传输速率之前，我们需要大约 2 MB 或更多的传输数据量。唯一的区别是，设备到设备的传输与设备到主机的传输之间存在明显差异。
- en: A midrange system quite common in the consumer enviroment is the i3/i5 system
    from Intel. This particular one is the i3 540 running with a H55 chipset. As this
    device has a single GPU only, it’s running at X16 the peak speed PCI-E 2.0 ([Figure
    9.15](#F0080)).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者环境中，i3/i5 系统是非常常见的中端设备，这款特定的系统是运行 H55 芯片组的 i3 540。由于该设备仅配备了单一显卡，它的 PCI-E
    2.0 X16 最大带宽速度为 [图 9.15](#F0080) 所示。
- en: Again we can see the very large difference between pinned and nonpinned transfers,
    in excess of 2×. However, notice the absolute speed difference, approximately
    a 2× increase over the AMD system. This is largely due to the AMD system using
    an X8 PCI-E link, whereas the Intel system here uses an X16 PCI-E link.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看到固定内存传输和非固定内存传输之间的巨大差异，超过2倍。然而，请注意绝对速度差异，约为AMD系统的2倍增速。这主要是因为AMD系统使用的是X8 PCI-E连接，而Intel系统则使用X16
    PCI-E连接。
- en: The Intel I3 is a typical consumer processor. Anyone writing consumer-based
    applications should be very much aware by now that they need to be using pinned
    memory transfers, as we can see the huge difference it makes.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Intel I3是典型的消费级处理器。任何编写消费级应用程序的人现在应该非常清楚，他们需要使用固定内存传输，因为我们可以看到它带来的巨大差异。
- en: Finally, we look at one further system, this time from the server arena, using
    the Intel I7 920 Nehalem processor and the ASUS supercomputer socket 1366 motherboard.
    This is a common motherboard for very high-end GPUs, as it allows up to four PCI-E
    slots. This particular one is equipped with 3× GTX290 GPUs each using an PCI-E
    2.0 X16 connection.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看另一个系统，这次来自服务器领域，使用的是Intel I7 920 Nehalem处理器和ASUS超级计算机1366插槽主板。这是一款常见的高端GPU主板，因为它支持最多四个PCI-E插槽。这个特定的主板配备了3个GTX290
    GPU，每个都使用PCI-E 2.0 X16连接。
- en: What we see from the diagram is again interesting. Pinned and paged memory transfers
    are equal until transfer sizes larger than 512 KB, after which the pinned memory
    transfers lead by up to 1.8× over the paged memory–based transfers. Unlike the
    Nehalem I3 system, notice the Nehalem I7 system is more consistent and there is
    not a huge variation between inbound and outbound transfer speeds. However, also
    note the peak transfer speed, despite both devices being on a X16 PCI-E 2.0 link,
    is only 5400 MB/s as opposed to the I3, which achieved a peak of 6300 MB/s ([Figure
    9.16](#F0085)).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中我们再次可以看到有趣的现象。固定内存和分页内存传输在传输大小大于512 KB之前是相等的，之后固定内存传输的速度比基于分页内存的传输快最多1.8倍。与Nehalem
    I3系统不同，请注意Nehalem I7系统更为一致，进出传输速度之间没有巨大的波动。然而，也要注意，尽管两台设备都使用X16 PCI-E 2.0连接，峰值传输速度仅为5400
    MB/s，而I3系统的峰值为6300 MB/s（[图9.16](#F0085)）。
- en: So in summary, we can say that across a selection of today’s computing hardware,
    pinned memory transfers are approximately twice as fast as nonpinned transfers.
    Also we see there can be a considerable variance in performance between read and
    write speeds from and to the various devices. We can also see that we need to
    use larger, rather than smaller, block sizes, perhaps combining multiple transfers
    to increase the overall bandwidth utilization of the bus.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以说，在今天的各种计算硬件中，固定内存传输的速度大约是非固定内存传输的两倍。同时，我们也看到，从各种设备读取和写入速度之间可能存在显著的性能差异。我们还可以看到，我们需要使用更大的块大小，而不是更小的块大小，可能需要将多个传输合并，以提高总带宽利用率。
- en: Zero-copy memory
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零拷贝内存
- en: Zero-copy memory is a special form of memory mapping that allows you to map
    host memory into the memory space of the GPU directly. Thus, when you dereference
    memory on the GPU, if it’s GPU based, then you get high-speed (180 GB/s) bandwidth
    to global memory. If the GPU code reads a host-mapped variable it issues a PCI-E
    read transaction, and a (very) long time later the host will return the data over
    the PCI-E bus.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 零拷贝内存是一种特殊的内存映射方式，它允许将主机内存直接映射到GPU的内存空间。因此，当你在GPU上解除引用内存时，如果它是基于GPU的，你就能获得高速（180
    GB/s）的全局内存带宽。如果GPU代码读取一个主机映射的变量，它会发起一个PCI-E读取事务，然后过很长时间，主机通过PCI-E总线返回数据。
- en: After looking at the PCI-E bus bandwidth in the previous section, this doesn’t,
    at first glance, make a lot of sense. Big transfers are efficient and small transfers
    inefficient. If we rerun the test program we used for the previous examples, we
    see that the median transfer time is 0.06 ms on our sample AMD Phenom X4 platform.
    However, these are explicit, individual transfers, so it’s possible the zero-copy
    implementation may be more efficient.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了PCI-E总线带宽，乍一看，这似乎不太合理。大数据传输是高效的，而小数据传输效率较低。如果我们重新运行之前用于示例的测试程序，我们会看到，在我们的AMD
    Phenom X4平台上，中位数传输时间为0.06毫秒。然而，这些都是显式的、单独的传输，所以零拷贝实现可能会更高效。
- en: If you think about what happens with access to global memory, an entire cache
    line is brought in from memory on compute 2.x hardware. Even on compute 1.x hardware
    the same 128 bytes, potentially reduced to 64 or 32, is fetched from global memory.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑一下对全局内存的访问，会发现整个缓存行从内存中被读取到计算2.x硬件中。即使在计算1.x硬件上，仍然会从全局内存中获取相同的128字节，可能会减少到64或32字节。
- en: NVIDIA does not publish the size of the PCI-E transfers it uses, or details
    on how zero copy is actually implemented. However, the coalescing approach used
    for global memory could be used with PCI-E transfer. The warp memory latency hiding
    model can equally be applied to PCI-E transfers, providing there is enough arithmetic
    density to hide the latency of the PCI-E transfers. This is, in fact, the key
    to getting this to work. If you do very little for each global memory fetch and
    your application is already memory bound, this approach is unlikely to help you.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA并没有公开它使用的PCI-E传输的大小，或零拷贝是如何实现的细节。然而，全球内存的合并方法可以与PCI-E传输一起使用。只要有足够的算术密度来隐藏PCI-E传输的延迟，warp内存延迟隐藏模型同样可以应用于PCI-E传输。这实际上是实现这一功能的关键。如果每次全局内存获取时所做的工作很少，而你的应用程序已经是内存瓶颈，那么这种方法可能对你没有帮助。
- en: However, if your application is arithmetically bound, zero-copy memory can be
    a very useful technique. It saves you the explicit transfer time to and from the
    device. In effect, you are overlapping computation with data transfers without
    having to do explicit stream management. The catch, of course, is that you have
    to be efficient with your data usage. If you fetch or write the same data point
    more than once, this will create multiple PCI-E transactions. As each and every
    one of these is expensive in terms of latency, the fewer there are the better.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你的应用程序是算术瓶颈，零拷贝内存可能是一个非常有用的技术。它可以节省显式传输到设备和从设备传输的数据时间。实际上，你可以在不需要显式流管理的情况下将计算和数据传输重叠。需要注意的是，你必须高效地使用数据。如果你多次获取或写入相同的数据点，这将导致多个PCI-E事务。由于每一个这样的事务在延迟上都是昂贵的，事务越少越好。
- en: This can also be used very effectively on systems where the host and GPU share
    the same memory space, such as on the low-end NVIDIA ION-based netbooks. Here
    a malloc of global memory on the GPU actually results in a malloc of memory on
    the host. Clearly it doesn’t make sense to copy from one memory area on the host
    to another memory area on the host. Zero-copy memory can eliminate the need to
    perform these copies in such systems, without the impact of a PCI-E bus transfer.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术在主机和GPU共享相同内存空间的系统中也能非常有效地使用，例如低端的基于NVIDIA ION的上网本。在这种情况下，GPU上的全局内存malloc实际上会导致主机上的内存malloc。显然，将主机上的一个内存区域复制到另一个内存区域没有意义。零拷贝内存可以消除在这些系统中执行这些复制的需求，而且不会受到PCI-E总线传输的影响。
- en: Zero-copy memory also has one very useful use case. This is during the phase
    where you are initially porting a CPU application to a GPU. During this development
    phase there will often be sections of code that exist on the host that have not
    yet been ported over to the GPU. By declaring such data references as zero-copy
    memory regions, it allows the code to be ported in sections and still have it
    work. The performance will be generally poor until all the intended parts are
    present on the GPU. It simply allows this to be done in smaller steps so it’s
    not an “everything or nothing” problem.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 零拷贝内存还有一个非常有用的应用场景。这通常出现在你最初将一个CPU应用程序移植到GPU的阶段。在这个开发阶段，通常会有一些代码部分仍然存在于主机上，而尚未移植到GPU上。通过将这些数据引用声明为零拷贝内存区域，它允许分步移植代码并使其正常工作。直到所有预期的部分都出现在GPU上，性能通常会较差。它实际上使得这个过程可以分步骤进行，避免了“要么全有，要么全无”的问题。
- en: Let’s start by taking the existing `memcpy` program and expanding the kernel
    so it does the read of the data instead of relying on an explicit copy. For this
    we absolutely must coalesce accesses to memory, which when reading a simple one-dimensional
    array is easy. Thus, our kernel becomes
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从现有的`memcpy`程序开始，并扩展内核，使其执行数据读取，而不是依赖显式的复制。为此，我们必须完全合并对内存的访问，在读取简单的一维数组时这很容易实现。因此，我们的内核变成了
- en: '[PRE28]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the kernel we simply make the `x` and `y` grid dimensions into a single linear
    array and assign one element from the source dataset to the destination dataset.
    Next we have to do three critical things to use zero-copy or host-mapped memory—that
    is, first to enable it, second to allocate memory using it, and finally to convert
    the regular host pointer to the device memory space.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，我们只需将 `x` 和 `y` 网格维度合并为一个线性数组，并将源数据集中的一个元素分配给目标数据集。接下来，我们必须做三件关键的事情来使用零拷贝或主机映射内存——即：首先启用它，其次使用它分配内存，最后将常规主机指针转换为设备内存空间中的指针。
- en: 'Prior to any creation of a CUDA context, we need to make the following call:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 CUDA 上下文之前，我们需要进行以下调用：
- en: '[PRE30]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When the CUDA context is created the driver will know it also has to support
    host-mapped memory. Without this the host-mapped (zero-copy) memory will not work.
    This will not work if it’s done after the CUDA context has been created. Be aware
    that calls to functions like `cudaHostAlloc`, despite operating on host memory,
    still create a GPU context.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 当 CUDA 上下文创建时，驱动程序将知道它也必须支持主机映射内存。如果没有这个，主机映射（零拷贝）内存将无法工作。如果在创建 CUDA 上下文后进行此操作，则无法工作。请注意，尽管
    `cudaHostAlloc` 等函数操作的是主机内存，但它们仍会创建 GPU 上下文。
- en: 'Although most devices support zero-copy memory, some earlier devices do not.
    It’s not part of the compute level, so it has to be checked for explicitly as
    follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数设备支持零拷贝内存，但一些早期设备不支持。它不是计算级别的一部分，因此必须显式检查，如下所示：
- en: '[PRE31]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The next stage is to allocate memory on the host such that it can be mapped
    into device memory. This is done with an additional flag `cudaHostAllocMapped`
    to the `cudaHostAlloc` function.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段是在主机上分配内存，以便可以映射到设备内存中。通过为 `cudaHostAlloc` 函数添加一个额外的标志 `cudaHostAllocMapped`
    来完成此操作。
- en: '[PRE32]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we need to convert the host pointer to a device pointer, which is
    done with the `cudaHostGetDevicePointer` function as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将主机指针转换为设备指针，这可以通过如下的 `cudaHostGetDevicePointer` 函数完成：
- en: '[PRE33]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In this call we convert the `host_data_to_device` previously allocated in the
    host memory space to an equvalent pointer, but within the GPU memory space. Do
    not confuse the pointers. Use the converted pointer only with GPU kernels and
    the original pointer only in code that executes on the host. Thus, for example,
    to free the memory later, an operation performed on the host, the existing call
    remains the same:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在此调用中，我们将之前在主机内存空间中分配的 `host_data_to_device` 转换为一个等效的指针，但该指针位于 GPU 内存空间中。不要混淆这些指针。只应在
    GPU 内核中使用转换后的指针，而仅在主机上执行的代码中使用原始指针。因此，例如，要稍后释放内存，这是在主机上执行的操作，现有的调用保持不变：
- en: '[PRE34]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As we’re using memory blocks up to 512 MB in size, to access one element per
    thread no matter how many threads we allocate per block means the number of blocks
    will exceed 64 K. This is the hard limit on the number of blocks in any single
    dimension. Thus, we have to introduce another dimension. This introduces grids,
    which we covered in [Chapter 5](CHP005.html). We can do this relatively simply
    by fixing the number of grids at some value that will be large enough to allow
    sufficient flexibility in selecting the number of threads per block.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的内存块最大为 512 MB，为了确保每个线程访问一个元素，无论我们每个块分配多少线程，这样会导致块的数量超过 64 K。这是单一维度上块的数量的硬性限制。因此，我们必须引入另一个维度。这引入了网格，我们在[第
    5 章](CHP005.html)中介绍过。我们可以通过将网格的数量固定为一个足够大的值来简单地做到这一点，从而允许在选择每个块的线程数时具有足够的灵活性。
- en: '[PRE35]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`dim3 blocks(num_grid, num_blocks_per_grid);`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim3 blocks(num_grid, num_blocks_per_grid);`'
- en: 'The `dim3` operation simply assigns the regular scalar values we calculated
    to a structure type holding a triplet that can be used as a single parameter in
    the kernel launch. It causes the kernel to launch 64 grids of *N* blocks. This
    simply ensures that for a given block index we do not exceed the 64 K limit. Thus,
    on the kernel launch, we replace `num blocks`, a scalar type, with `blocks`, a
    `dim3` type:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim3` 操作简单地将我们计算的常规标量值分配给一个结构类型，该类型持有一个三元组，可以作为内核启动中的单个参数使用。它使内核启动 64 个网格的
    *N* 块。这仅仅确保了在给定的块索引下，我们不会超过 64 K 限制。因此，在内核启动时，我们将 `num blocks`（一个标量类型）替换为 `blocks`（一个
    `dim3` 类型）：'
- en: '[PRE37]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: What we see for transfers *to the device* is that the overall figures are identical
    to the transfers using explicit memory copies. This has significant implications.
    Most applications that do not already use the stream API simply copy memory to
    the GPU at the start and copy back once the kernel is complete. We can shrink
    that time drastically using pinned memory copies, but the time is still cumulative
    because it’s a serial operation.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传输*到设备*的情况，我们看到整体数据与使用显式内存拷贝的传输数据相同。这具有重要的意义。大多数不使用流API的应用程序通常在开始时将内存拷贝到GPU，内核完成后再拷贝回来。我们可以通过使用固定内存拷贝大幅缩短这个时间，但由于这是串行操作，时间仍然是累积的。
- en: In effect, what happens with the zero-copy memory is we break both the transfer
    and the kernel operation into much smaller blocks, which execute them in a pipeline
    ([Figure 9.17](#F0090)). The overall time is reduced quite significantly.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用零拷贝内存时，我们将传输和内核操作拆解成更小的块，并通过流水线执行它们（见[图9.17](#F0090)）。总体执行时间显著缩短。
- en: '![image](../images/F000090f09-17-9780124159334.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-17-9780124159334.jpg)'
- en: FIGURE 9.17 Serial versus overlapped transfer/kernel execution.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 串行与重叠传输/内核执行。
- en: Notice we did not perform the same optimization with the copy from device. The
    reason for this is because consumer GPUs have only one copy engine enabled. Thus,
    they support only a single memory stream. When you do a read-kernel-write operation,
    if the write is pushed into the stream ahead of subsequent reads, it will block
    the read operations until the pending write has completed. Note this is not the
    case for Tesla devices, as both copy engines are enabled and thus Tesla cards
    are able to support independent to and from streams. Prior to Fermi, there was
    only ever one copy engine on any card.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们并未对从设备拷贝进行相同的优化。原因在于消费级GPU仅启用了一个拷贝引擎。因此，它们仅支持单一的内存流。当执行读取-内核-写入操作时，如果写入操作被推到流前面，则会阻塞后续的读取操作，直到待处理的写入完成。需要注意的是，特斯拉设备不适用这种情况，因为两个拷贝引擎都已启用，因此特斯拉卡能够支持独立的到和从流。在Fermi之前，任何卡上只会启用一个拷贝引擎。
- en: However, with zero-copy memory the transfers are actually quite small. The PCI-E
    bus has the same bandwidth in both directions. Due to the high latency of the
    PCI-E-based memory reads, actually most of the reads should have been pushed into
    the read queue ahead of any writes. We may be able to achieve significant execution
    time savings over the explicit memory copy version.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用零拷贝内存时，传输实际上非常小。PCI-E总线在两个方向上的带宽是相同的。由于基于PCI-E的内存读取具有较高的延迟，实际上大多数读取操作应该被提前推入读取队列，而不是等待写操作。通过零拷贝，我们可能能够比显式内存拷贝版本节省显著的执行时间。
- en: Note the diagram in [Figure 9.18](#F0095) is simplified in that it lists a single
    “Pinned To & From Device” line, yet we show the zero device copy times explicitly
    for the devices. The pinned memory time was effectively the same for all devices,
    so it was not shown per device.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图9.18](#F0095)中的图示进行了简化，列出了单一的“固定到/从设备”线，但我们显示了各设备的零设备拷贝时间。固定内存时间对所有设备基本相同，因此没有按设备分别显示。
- en: '![image](../images/F000090f09-18-9780124159334.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-18-9780124159334.jpg)'
- en: FIGURE 9.18 Zero-copy time versus explicit pinned copy time over different GPU
    generations.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 零拷贝时间与显式固定拷贝时间在不同GPU代际中的对比。
- en: We have listed the entire execution time of a single memory copy to device,
    kernel execution, and memory copy from device. Thus, there is some overhead that
    is not present when purely measuring the transfer to/from the device. As we’re
    using zero copy, the memory transactions and the kernel time cannot be pulled
    apart. However, as the kernel is doing very little, the overall execution time
    represents a fair comparison between the zero copy and explicit copy versions.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了从设备到内核执行的单次内存拷贝以及从设备到内存的拷贝的整个执行时间。因此，存在一些开销，这在仅测量到/从设备的传输时并不出现。由于我们使用了零拷贝，内存事务和内核时间无法分开。但由于内核执行的操作非常少，整体执行时间在零拷贝和显式拷贝版本之间提供了一个公正的比较。
- en: There is a considerable amount of variability. What we can see, however, is
    that for small transfer amounts, less than 512 KB, zero copy is faster than using
    explicit copies. Let’s now look at sizes larger than 512 KB in [Table 9.1](#T0010)
    and [Figure 9.19](#F0100).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 存在相当大的变动性。然而，我们可以看到，对于小于512 KB的传输量，零拷贝比显式拷贝要快。现在让我们查看[表9.1](#T0010)和[图9.19](#F0100)中大于512
    KB的大小。
- en: Table 9.1 Zero-Copy Results (execution time in ms)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 零拷贝结果（执行时间以毫秒为单位）
- en: '![Image](../images/T000090tabT0010.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0010.jpg)'
- en: '![image](../images/F000090f09-19-9780124159334.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-19-9780124159334.jpg)'
- en: FIGURE 9.19 Zero-copy graph (time in ms versus transfer size).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 零拷贝图（时间（毫秒）与传输大小的关系）。
- en: What is very interesting to see here is a considerable drop in execution time.
    On the Fermi hardware the overlapping of the kernel operation with the memory
    copies drops the execution time from 182 ms to 104 ms, a 1.75× speedup. The results
    are less impressive in the earlier devices, but still represent a significant
    speedup.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里非常有趣的是执行时间明显下降。在Fermi硬件上，内核操作与内存拷贝的重叠使得执行时间从182毫秒降至104毫秒，速度提升了1.75倍。在较早的设备中，结果不那么引人注目，但仍然代表了显著的加速。
- en: You can of course achieve this using streams and asynchronous memory copies,
    as demonstrated in [Chapter 8](CHP008.html). Zero copy simply presents an alternative,
    and somewhat simpler, interface you can work with.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以使用流和异步内存拷贝来实现这一点，如[第8章](CHP008.html)中所示。零拷贝仅仅提供了一种替代方案，并且提供了一个您可以使用的相对简单的接口。
- en: However, there are some caveats. Beware of exactly how many times the data is
    being fetched from memory. Re-reading data from global memory will usually exclude
    the use of zero-copy memory.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些需要注意的事项。请注意数据从内存中获取的次数。重新读取全局内存中的数据通常会排除使用零拷贝内存。
- en: If we modify the program to read the value from host memory twice instead of
    once, then the performance drops by half on the 9800 GT and GTX260 platforms,
    the compute 1.x devices. This is because each and every fetch from global memory
    on these platforms is not cached. Thus, the number of PCI-E transactions issued
    is doubled, as we double the amount of times the GPU accesses the zero-copy memory
    area.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们修改程序，使其从主机内存中读取值两次而不是一次，那么在9800 GT和GTX260平台（即compute 1.x设备）上，性能将下降一半。这是因为这些平台上每次从全局内存获取数据时都没有缓存。因此，发出的PCI-E事务数量会翻倍，因为我们使GPU访问零拷贝内存区域的次数翻倍。
- en: On Fermi the situation is somewhat different. It has an L1 and L2 cache and
    it’s highly likely the data fetched earlier in the kernel will still be in the
    cache when the latter access hits the same memory address. To be sure, you have
    to explicitly copy the data you plan to reuse to the shared memory. So in Fermi,
    depending on the data pattern, you typically do not see the device issuing multiple
    PCI-E transactions, as many of these hit the internal caches and therefore never
    create a global memory transaction.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fermi上，情况稍有不同。它有L1和L2缓存，并且很可能内核中较早获取的数据仍然会在缓存中，当后续访问命中相同的内存地址时，它会从缓存中获取。为了确保这一点，您必须显式地将计划重用的数据复制到共享内存中。因此，在Fermi上，取决于数据模式，通常不会看到设备发出多个PCI-E事务，因为这些大多数命中内部缓存，因此不会产生全局内存事务。
- en: Thus, zero-copy memory presents a relatively easy way to speed up your existing
    serial code without having to explicitly learn the stream API, providing you are
    careful about data reuse and have a reasonable amount of work to do with each
    data item.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，零拷贝内存为您提供了一种相对简单的方法，可以在不需要显式学习流API的情况下加速现有的串行代码，前提是您小心数据重用，并且每个数据项的工作量合理。
- en: However, be aware that the bandwidth of the PCI-E bus is nowhere near the bandwidth
    available on a CPU. The latest Sandybridge I7 processor (Socket 2011) achieves
    some 37 GB/s of memory bandwidth, from a theoretical peak of 51 GB/s. We’re achieving
    5–6 GB/s from a theoretical peak of 8 GB/s on the PCI-E 2.0 bus. You must have
    enough work in your application to justify the cost of moving the data over the
    PCI-E bus. Consider that the CPU can be a better alternative in situations where
    very little work is being done per element.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，PCI-E总线的带宽远远无法与CPU的带宽相比。最新的Sandybridge I7处理器（Socket 2011）可以实现约37 GB/s的内存带宽，而理论峰值为51
    GB/s。我们在PCI-E 2.0总线上实现了5–6 GB/s，理论峰值为8 GB/s。你必须在应用中有足够的工作量，才能证明通过PCI-E总线传输数据的成本是合理的。考虑到在每个元素的工作量很小的情况下，CPU可能是更好的选择。
- en: The program used for these measurements is shown here for reference.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 用于这些测量的程序如下所示，供参考。
- en: '[PRE38]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`         size_in_bytes));`'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '`         size_in_bytes));`'
- en: '[PRE50]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`   {`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`   {`'
- en: '[PRE60]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '`  // Adjust for doing a copy to and back`'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '`  // 调整以进行拷贝和返回`'
- en: '[PRE71]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Bandwidth limitations
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带宽限制
- en: The ultimate bandwidth limitation of a significant number of applications is
    the I/O speed of whatever devices the input and output data have to be acquired
    from and written to. This is often the limitation on the speedup of any application.
    If your application takes 20 minutes to run on a serial CPU implementation and
    can express enough parallelism, it’s quite feasible for that application to run
    on a GPU in less time than it takes to load and save the data from the storage
    device you are using.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序的最终带宽限制是输入和输出数据必须从中获取和写入的设备的I/O速度。这通常是应用程序加速的瓶颈。如果你的应用程序在串行CPU实现上运行需要20分钟，并且能够表达足够的并行性，那么该应用程序很有可能在GPU上运行，所需时间比加载和保存你使用的存储设备上的数据还要短。
- en: The first problem we have in terms of bandwidth is simply getting the data in
    and out of the machine. If you are using network-attached storage, the limit to
    this will be the speed of the network link. The best solution to this problem
    is a high-speed SATA3 RAID controller using many high-speed SSD drives. However,
    this will not solve your bandwidth issues unless you are using the drive efficiently.
    Each drive will have a peak transfer rate into host memory, which is actually
    a function of the transfer rate of the drive, the controller, and the route to
    host memory.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽方面的第一个问题就是如何将数据进出机器。如果你使用的是网络附加存储，那么限制因素将是网络连接的速度。解决这个问题的最佳方法是使用多个高速SSD硬盘的高速SATA3
    RAID控制器。然而，除非你高效地使用硬盘，否则这并不能解决你的带宽问题。每个硬盘都有一个向主机内存传输数据的峰值速率，这实际上是硬盘、控制器以及到主机内存路径的传输速率的函数。
- en: Running a benchmark on a drive, such as the commonly used ATTO benchmark, can
    show you the effect of using different size blocks (see [Figure 9.20](#F0105)).
    This benchmark simulates access to drives based on a certain size of reads and
    writes. Thus, it reads and writes a 2 GB file in blocks of 1 K, 2 K, 4 K, etc.
    to see the effect of changing the block size.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬盘上运行基准测试，例如常用的ATTO基准测试，可以显示使用不同块大小的效果（见[图9.20](#F0105)）。这个基准测试模拟了基于某一块大小的读写操作。因此，它以1
    K、2 K、4 K等块大小读取和写入2 GB的文件，以观察改变块大小的效果。
- en: '![image](../images/F000090f09-20-9780124159334.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-20-9780124159334.jpg)'
- en: FIGURE 9.20 Bandwidth (MB/s) for a single SSD versus five hard disks in RAID
    0.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 单个SSD与RAID 0中的五个硬盘的带宽（MB/s）对比。
- en: We can see from the results that only when we read data in 64 K chunks or more
    do we achieve the peak bandwidth from the single SSD drive. For the RAID 0 hard
    drive system we need at least 1 MB blocks to make use of the multiple disks. Thus,
    you need to make sure you’re using the `fread` function in C to read suitable
    sized blocks of data from the disk subsystem. If we fetch data in 1 K chunks,
    we get just 24 MB/s from the drive, less than 10% of its peak read bandwidth.
    The more drives you add to a RAID system, the larger the minimum block size becomes.
    If you are processing compressed music or image files, the size of a single file
    may only be a few megabytes.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中我们可以看到，只有当我们以64 K或更大的块大小读取数据时，才能从单个SSD硬盘驱动器中获得峰值带宽。对于RAID 0硬盘系统，我们需要至少1
    MB的块大小才能利用多个磁盘。因此，你需要确保在C语言中使用`fread`函数来从磁盘子系统读取合适大小的数据块。如果我们以1 K的块大小获取数据，我们只能从驱动器获得24
    MB/s，远低于其峰值读取带宽的10%。你在RAID系统中增加的硬盘越多，最小块大小就会变得越大。如果你处理的是压缩的音乐或图像文件，那么单个文件的大小可能只有几兆字节。
- en: Note also that whether the data is compressible or not has a big impact on drive
    performance. The server level drives, such as the OCZ Vertex 3, provide both higher
    peak values and sustained bandwidth with uncompressible data. Thus, if your dataset
    is in an already compressed format (MP3, MP4, WMV, H.264, JPG, etc.), then you
    need to make sure you use server drives. The bandwidth on many consumer-level
    SSD drives can fall to half of quoted peak when using uncompressible data streams.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，数据是否可压缩对硬盘性能有很大影响。服务器级硬盘，如OCZ Vertex 3，在处理不可压缩数据时提供更高的峰值值和持续带宽。因此，如果你的数据集已经是压缩格式（如MP3、MP4、WMV、H.264、JPG等），你需要确保使用服务器级硬盘。许多消费级SSD硬盘在使用不可压缩数据流时，带宽可能降至标称峰值的二分之一。
- en: The reason for this is the use of synchronous NAND memory in the high-end server
    SSDs versus the cheaper and much lower-performing asynchronous NAND memory used
    in consumer SSDs. Even with noncompressed data, synchronous NAND-based drives
    still outperform their asynchronous cousins, especially once the drive starts
    to contain some data. OCZ also provides the RevoDrive R4 PCI-E-based product,
    which claims speeds in the order of 2 GB/s plus at the expense of a PCI-E slot.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于高端服务器 SSD 使用同步 NAND 存储，而消费级 SSD 则使用更便宜、性能更低的异步 NAND 存储。即使是未压缩的数据，基于同步 NAND
    的驱动器仍然优于其异步同类，尤其是在驱动器开始包含一些数据时。OCZ 还提供了基于 PCI-E 的 RevoDrive R4 产品，声称其速度可达到每秒 2
    GB 以上，但需要占用一个 PCI-E 插槽。
- en: The next bandwidth limit you hit is that of host memory speed. This is typically
    not an issue until you introduce multiple GPUs per node, if you consider that
    you can fetch data at 6 GB/s off the PCI-E bus from a very high-speed SSD RAID
    system. We then have to send out data at 6 GB/s to and from the host memory to
    the GPU. Potentially you could also write data again at 6 GB/s to the RAID controller.
    That’s a potential 24 GB/s of pure data movement without the CPU actually doing
    anything useful except moving data. We’re already hitting the bandwidth limits
    of most modern processor designs and have already surpassed that available from
    the older-generation CPUs. In fact, only the latest quad channel I7 Sandybridge-E
    CPU has anything like the bandwidth we could start moving around, if we were to
    solve the slow I/O device issue.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的带宽限制是主机内存速度。通常，直到在每个节点上引入多个 GPU 时，才会遇到这个问题。如果考虑到您可以从非常高速的 SSD RAID 系统通过
    PCI-E 总线以 6 GB/s 的速度获取数据，那么我们必须以 6 GB/s 的速度在主机内存与 GPU 之间传输数据。您也可能会以每秒 6 GB 的速度将数据写回
    RAID 控制器。这意味着在没有 CPU 实际执行任何有用工作的情况下，最多可以达到 24 GB/s 的纯数据传输速度，CPU 只做数据移动的工作。我们已经达到了大多数现代处理器设计的带宽极限，并且已经超越了旧一代
    CPU 的带宽。事实上，只有最新的四通道 I7 Sandybridge-E CPU 才具有我们可以开始移动的带宽，前提是我们解决了慢速 I/O 设备问题。
- en: CUDA 4.0 SDK introduced Peer2Peer GPU communication. The CUDA 4.1 SDK also introduced
    Peer2Peer communication with non-NVIDIA hardware. Thus, with the correct hardware,
    GPUs can talk to any supported device. This is mostly limited to a small number
    of InfiniBand and other highspeed network cards. However, in principle, any PCI-E
    device can talk with the GPU. Thus, a RAID controller could send data directly
    to and from a GPU. There is a huge potential for such devices, as no host memory
    bandwidth, PCI-E, or memory is consumed. As data is not having to flow to a CPU
    and then back out again, latency is dropped considerably.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 4.0 SDK 引入了 Peer2Peer GPU 通信。CUDA 4.1 SDK 还引入了与非 NVIDIA 硬件的 Peer2Peer 通信。因此，凭借正确的硬件，GPU
    可以与任何支持的设备进行通信。这主要限于少数 InfiniBand 和其他高速网络卡。不过，原则上，任何 PCI-E 设备都可以与 GPU 通信。因此，RAID
    控制器可以直接向 GPU 发送数据并接收数据。这类设备具有巨大的潜力，因为它们不消耗主机内存带宽、PCI-E 或内存。由于数据无需先流向 CPU 再返回，因此延迟大大降低。
- en: Once the data has been moved to the GPU, there is a bandwidth limit of up to
    190 GB/s on GeForce cards and 177 GB for Tesla, to and from the global memory
    on the device. To achieve this you need to ensure coalescing of the data reads
    from the threads and ensure your application makes use of 100% of the data moved
    from memory to the GPU.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被传输到 GPU，GeForce 卡的带宽上限为 190 GB/s，Tesla 为 177 GB/s，数据在设备的全局内存之间进行读写。为了实现这一点，您需要确保线程的数据读取是合并的，并确保您的应用程序能够充分利用从内存到
    GPU 传输的 100% 数据。
- en: Finally, we have shared memory. Even if you partition data into tiles, move
    it into shared memory, and access it in a bank conflict–free manner, the bandwidth
    limit is on the order of 1.3 TB/s. For comparison the AMD Phenom II and Nehalem
    I7 CPUs for a 64 KB L1 cache block, the same capacity as the GPU L1 cache and
    shared memory, has around 330 GB/s bandwidth, some 25% of that of the GPU.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是共享内存。即使您将数据划分为瓦片，将其移动到共享内存中，并以无银行冲突的方式访问，带宽限制大约为 1.3 TB/s。相比之下，AMD Phenom
    II 和 Nehalem I7 CPU 的 64 KB L1 缓存块，其容量与 GPU 的 L1 缓存和共享内存相同，带宽大约为 330 GB/s，约为 GPU
    带宽的 25%。
- en: If we take a typical float or integer parameter, it’s 4 bytes wide. Thus, the
    bandwidth to global memory is a maximum of 47.5 giga-elements per second (190
    GB/s ÷ 4). Assuming you read and write just one value, we can halve this figure
    to 23.75 giga-elements per second. Thus, with no data reuse, this is the maximum
    upper throughput of your application.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以典型的浮点数或整数参数为例，它的宽度是 4 字节。因此，访问全局内存的带宽最大为每秒 47.5 吉元素（190 GB/s ÷ 4）。假设你只读取和写入一个值，我们可以将这个数值减半，得到每秒
    23.75 吉元素。因此，在没有数据复用的情况下，这是你的应用程序的最大上行吞吐量。
- en: The Fermi device is rated in excess of 1 teraflop, that is, it can process on
    the order of 1000 giga floating-point operations per second. Kepler is rated at
    in excess of 3 teraflops. The actual available flops depend on how you measure
    flops. The fastest measure is the FMADD instruction (floating-point multiply and
    add) instruction. This multiplies two floating-point numbers together and adds
    another number to it. As such, this counts as two flops, not one. Real instruction
    streams intermix memory loads, integer calculations, loops, branches, etc. Thus,
    in practice, kernels never get near to this peak figure.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi 设备的性能超过了 1 teraflop，即它可以每秒处理大约 1000 亿次浮点运算。Kepler 的性能超过了 3 teraflops。实际可用的浮点运算性能取决于你如何测量浮点运算。最快的测量方法是
    FMADD 指令（浮点数乘法加法）指令。它将两个浮点数相乘并加上另一个数。因此，这算作两个浮点运算，而不是一个。实际的指令流会交织着内存加载、整数计算、循环、分支等。因此，在实践中，内核的实际性能通常远未接近这个峰值。
- en: We can measure the real speed achievable by simply using the program we previously
    developed to visualize the PCI-E bandwidth. Simply performing a memory copy from
    global memory to global memory will show us the maximum possible read and write
    speed a kernel can achieve.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地使用之前开发的程序来可视化 PCI-E 带宽，从而测量实际可达到的速度。只需进行从全局内存到全局内存的内存拷贝，就能展示一个内核能够达到的最大读写速度。
- en: '[PRE73]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Note the values in the parentheses shows grids × blocks × threads. The above
    figures are plotted in [Figure 9.21](#F0110).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 注意括号中的数值表示网格 × 块 × 线程。上述数据在[图 9.21](#F0110)中绘制。
- en: '![image](../images/F000090f09-21-9780124159334.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-21-9780124159334.jpg)'
- en: FIGURE 9.21 Global memory bandwidth across devices.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 各设备的全局内存带宽。
- en: These results are created by pushing 16 kernels into an asynchronous stream,
    with each call surrounded by a stop and start event. Each kernel performs a single-element
    copy from the source to the destination for every memory location. The execution
    time of the first kernel in each batch is ignored. The remaining kernels contribute
    to the total time, which is then averaged over the kernels. The quoted bandwidth
    for the GTX470 is 134 GB/s, so we’re falling short of this, despite having a simple
    kernel and obviously hitting the peak at the larger transfer sizes.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果是通过将 16 个内核推送到异步流中生成的，每个调用都被停止和启动事件包围。每个内核对每个内存位置执行从源到目标的单元素拷贝。每批次第一个内核的执行时间被忽略，剩下的内核贡献了总时间，这些时间会被平均到所有内核上。GTX470
    的带宽为 134 GB/s，因此我们仍然没有达到这一点，尽管我们使用的是简单的内核，而且在较大的传输大小时显然达到了峰值。
- en: What we see from this chart is that to achieve anywhere near the peak memory
    performance you need to have enough threads. We start off by using 32 threads
    per block until we launch a total of 64 blocks. This ensures that all the SMs
    are given work, rather than one SM getting a large number of threads and therefore
    most of the work. We then increase the thread count per block up to 256 threads
    once there is a reasonable distribution of blocks to the SMs.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图表中我们可以看到，要实现接近内存性能峰值的效果，你需要有足够的线程。我们首先使用每个块 32 个线程，直到我们启动了总共 64 个块。这确保所有
    SM 都能分配到工作，而不是一个 SM 获取大量线程并因此承担大部分工作。然后，一旦块的分布在 SM 之间合理，我们就将每个块的线程数增加到 256 个。
- en: Changing the element type from `uint1` to `uint2`, `uint3`, and `uint4` produces
    some interesting results. As you increase the size of a single element, the total
    number of transactions issued to the memory subsystem is reduced. On the GTX470,
    going from the 4-byte read (single-element integer or float) to an 8-byte read
    (dual-element integer, float, or single-element double) resulted in up to a peak
    23% increase in measured bandwidth to and from global memory ([Figure 9.22](#F0115)).
    The average improvement was somewhat lower at just 7%, but this still represents
    a reasonable improvement in execution time by simply switching from `int1`/`float1`
    to `int2`/`float2` vector types. The GTX460 presents a similar, but more pronounced
    pattern ([Figure 9.23](#F0120)).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 将元素类型从`uint1`改为`uint2`、`uint3`和`uint4`会产生一些有趣的结果。当你增加单个元素的大小时，发往内存子系统的事务总数会减少。在GTX470上，将4字节读取（单元素整数或浮点数）改为8字节读取（双元素整数、浮点数或单元素双精度浮点数）时，全球内存带宽的峰值提高了23%。平均改善为7%，但通过简单地将`int1`/`float1`类型改为`int2`/`float2`向量类型，这仍然代表了执行时间的合理改善。GTX460呈现出类似但更明显的模式（[图9.23](#F0120)）。
- en: '![image](../images/F000090f09-22-9780124159334.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-22-9780124159334.jpg)'
- en: FIGURE 9.22 Global memory bandwidth GTX470/compute 2.0 (transaction size in
    bytes).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.22 全局内存带宽 GTX470/计算能力2.0（事务大小，单位字节）。
- en: '![image](../images/F000090f09-23-9780124159334.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-23-9780124159334.jpg)'
- en: FIGURE 9.23 Global memory bandwidth GTX460/compute 2.1 (transaction size in
    bytes).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.23 全局内存带宽 GTX460/计算能力2.1（事务大小，单位字节）。
- en: To achieve optimum bandwidth, the CUDA code was compiled specifically for compute
    2.1 devices. We also found that thread blocks that were a multiple of 48 threads
    worked best. This is not surprising given that there are three sets of 16 cores
    per SM instead of the usual two. When moving from 4 bytes per element to 8 or
    16 bytes per element, the bandwidth was increased by an average of 19%, but a
    best case of 38%.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现最佳带宽，CUDA代码专门为计算能力2.1的设备编译。我们还发现，线程块大小为48的倍数时效果最好。这并不令人惊讶，因为每个SM中有三组16个核心，而不是通常的两组。当元素大小从4字节增加到8字节或16字节时，带宽平均提高了19%，但最好的情况是提高了38%。
- en: A single warp transaction for 8 bytes per thread would result in a total of
    256 bytes moving over the memory bus. The GTX460 we are using has a 256-bit-wide
    bus to the global memory. This would clearly indicate that, regardless of any
    occupancy considerations, on such devices you should always be processing either
    8 or 16 bytes (two or four elements) per thread. This is most likely due to the
    higher ratio of CUDA codes within the SM causing some contention for the single
    set of LSUs (load/store units).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程8字节的单次warp事务会导致总共256字节的数据通过内存总线传输。我们使用的GTX460显卡具有256位宽的总线连接到全局内存。这明显表明，无论占用率如何，在这种设备上，你应该始终每个线程处理8字节或16字节（两个或四个元素）。这很可能是由于SM中CUDA代码的比例较高，导致单个LSU（加载/存储单元）出现一些争用。
- en: The GTX260 for comparison, a compute 1.3 device similar to the Tesla C2050 device,
    gained, on average, 5% by moving from 4 to 8 bytes per element. However, its performance
    was drastically reduced when moving beyond this. The 9800 GT did not show any
    significant improvement, suggesting this device is already achieving the peak
    when using 4 bytes per element.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 与GTX260进行比较，该设备为计算能力1.3的设备，类似于Tesla C2050设备，移至每个元素8字节时，平均提高了5%。然而，当超过这个值时，其性能急剧下降。9800
    GT没有显示出任何显著的改善，这表明该设备在使用每个元素4字节时已经达到了峰值。
- en: Finally, note that Fermi-based Tesla devices implement an ECC (error checking
    and correction) based memory protocol. Disabling this can boost transfer speeds
    by around 10% at the expense of the error detection and correction ability. In
    a single machine versus a server room, this may be an acceptable tradeoff.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，基于Fermi架构的Tesla设备实现了基于ECC（错误检查和修正）的内存协议。禁用此功能可以将传输速度提高约10%，但代价是失去了错误检测和修正能力。在单台机器与服务器机房之间，这可能是一个可接受的折中。
- en: GPU timing
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU计时
- en: Single GPU timing
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单GPU计时
- en: Timing data on the GPU is not particularly straightforward. Using a timer that
    is CPU based is not a good solution, as the best way to use the GPU and CPU is
    to operate asynchronously. That is, both the GPU and CPU are running at the same
    time. CPU timing is only semi-accurate when you force sequential operation of
    the GPU and CPU. As this is not what we want in practice, it’s a poor solution.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 上的计时数据并不是特别直观。使用基于 CPU 的定时器并不是一个好的解决方案，因为最好的方式是让 GPU 和 CPU 异步操作。也就是说，GPU
    和 CPU 是同时运行的。当你强制执行 GPU 和 CPU 的顺序操作时，CPU 计时只会是半准确的。由于这不是我们在实际应用中想要的，因此这是一个较差的解决方案。
- en: The GPU, by default, operates in a synchronous mode in that the `memcpy` operations
    implicitly synchronize. The programmer expects to copy to the device, run the
    kernel, copy back from the device, and have the results in CPU memory to save
    to disk or for further processing. While this is an easy model to understand,
    it’s also a slow model. It’s one aimed at getting kernels to work, but not one
    aimed at performance.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GPU 以同步模式运行，`memcpy`操作会隐式地进行同步。程序员期望将数据复制到设备、运行内核、从设备复制回并将结果存储到 CPU 内存中，以便保存到磁盘或进行进一步处理。虽然这种模型易于理解，但它也是一种较慢的模型。它的目标是让内核能够工作，而不是优化性能。
- en: We examined the use of streams, in detail, in [Chapter 8](CHP008.html). A stream
    is effectively a work queue. Stream 0 is used as the default work queue when you
    do not specify a stream to the CUDA API. However, stream 0 has many operations
    that implicitly synchronize with the host. You might be expecting an asynchronous
    operation, but in practice certain API calls have implicit synchronization when
    using stream 0.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 8 章](CHP008.html)中详细讨论了流的使用。流实际上是一个工作队列。当你没有为 CUDA API 指定流时，流 0 被用作默认的工作队列。然而，流
    0 有许多操作会隐式地与主机同步。你可能期待一个异步操作，但在实践中，使用流 0 时某些 API 调用会有隐式同步。
- en: To use asynchronous operations, we need to first create a stream such as
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用异步操作，首先需要创建一个流，例如
- en: '[PRE74]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: For the bandwidth test, we created an array of events.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 对于带宽测试，我们创建了一个事件数组。
- en: '[PRE75]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The GPU provides events that can be time-stamped by the GPU hardware ([Figure
    9.24](#F0125)). Thus, to time a particular action on the GPU, you need to push
    a start event into the queue, then the action you wish to time, and finally a
    stop event. Streams are simply a FIFO (first in, first out) queue of operations
    for the GPU to perform. Each stream represents an independent queue of operations.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 提供了可以通过 GPU 硬件时间戳的事件（[图 9.24](#F0125)）。因此，要在 GPU 上计时某个特定操作，你需要将一个启动事件推送到队列中，然后是你想要计时的操作，最后是一个停止事件。流只是
    GPU 执行操作的 FIFO（先进先出）队列。每个流表示一个独立的操作队列。
- en: '![image](../images/F000090f09-24-9780124159334.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-24-9780124159334.jpg)'
- en: FIGURE 9.24 Timing an action on the GPU.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24 在 GPU 上计时一个操作。
- en: '![image](../images/F000090f09-25-9780124159334.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-25-9780124159334.jpg)'
- en: FIGURE 9.25 Multi-GPU timeline.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25 多 GPU 时间线。
- en: Having created a stream, you need to create one or more events.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了流之后，你需要创建一个或多个事件。
- en: '[PRE76]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Here we have a simple loop creating `MAX_NUM_TESTS` events—a start event and
    a stop event. We then need to push the events into the stream on either side of
    the action to measure.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个简单的循环，创建了`MAX_NUM_TESTS`个事件——一个启动事件和一个停止事件。然后，我们需要将这些事件推送到流中，放置在动作的两侧以进行计时。
- en: '[PRE77]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '`// Run the kernel`'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '`// 运行内核`'
- en: '[PRE78]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: To calculate the time, either per CUDA call or in total, call the CUDA function
    `cudaEventElapsedTime` to get the time difference between two time-stamped events.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算时间（无论是每个 CUDA 调用还是总计），调用 CUDA 函数`cudaEventElapsedTime`来获取两个时间戳事件之间的时间差。
- en: '[PRE80]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: You should realize that in performing such a timed event, there is no guarantee
    of ordering of events between streams. The CUDA runtime could execute your start
    event in stream 0 and then switch to a previously suspended kernel execution in
    stream 5, sometime later come back to stream 0, kick off the kernel, jump to another
    stream to process a number of other start events, and finally come back to stream
    0 and timestamp the stop event. The delta time is the time from the start period
    to the end period.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该意识到，在执行这样的定时事件时，不保证不同流之间事件的顺序。CUDA 运行时可能先在流 0 中执行你的启动事件，然后切换到流 5 中先前挂起的内核执行，过一段时间后再回到流
    0，启动内核，跳转到另一个流处理其他启动事件，最后回到流 0 并时间戳停止事件。时间差是从开始阶段到结束阶段的时间。
- en: In this example, notice we have created only a single stream. We have multiple
    events, but they all execute from the same stream. With only a single stream the
    runtime can only execute events in order, so we guarantee achieving the correct
    timing.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，请注意我们只创建了一个流。我们有多个事件，但它们都从同一个流中执行。由于只有一个流，运行时只能按顺序执行事件，因此我们可以确保正确的时序。
- en: Notice the call to the `cudaEventSynchronize` API. This call causes the CPU
    thread to block should it be called when the event has not completed. As we’re
    doing nothing useful on the CPU, this is perfectly fine for our purposes.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意调用了`cudaEventSynchronize` API。这个调用会导致CPU线程阻塞，如果事件尚未完成时调用它。由于我们在CPU上没有做任何有用的操作，这对我们的目的来说是完全没问题的。
- en: At the end of the host program we must ensure that with any resources we allocated
    are freed up.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机程序的末尾，我们必须确保释放我们分配的所有资源。
- en: '[PRE84]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Destroying an event before it’s actually been used will result in undefined
    runtime errors when executing the kernels.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件尚未被实际使用之前销毁它会导致执行内核时出现未定义的运行时错误。
- en: Finally, you should be aware that events are not free. It takes some resources
    to handle the events at runtime. In this example we specifically wanted to time
    each kernel to ensure there was not significant variability. In most cases a single
    start and stop event at the start and end of the work queue will be entirely sufficient
    for the overall timing.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该知道事件并不是免费的。处理事件需要一些运行时资源。在这个例子中，我们特别希望计时每个内核，以确保没有显著的变动。在大多数情况下，在工作队列的开始和结束处设置一个开始和结束事件就足够完成整体的计时。
- en: Multi GPU timing
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多GPU时序
- en: Multi GPU timing is a little more complex, but based on the same principles.
    Again, we create a number of streams and push events into the streams.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 多GPU时序稍微复杂一点，但基于相同的原理。我们再次创建多个流并将事件推送到这些流中。
- en: Unfortunately, there is no function provided in the API to obtain the absolute
    timestamp from an event. You can only obtain the delta between two events. However,
    by pushing an event into the start of the stream, you can use this as time point
    zero and thus obtain the time relative to the start of the stream. However, asking
    for the delta time between events on different GPUs causes the API to return an
    error. This complicates creating a timeline when using multiple GPUs, as you may
    need to adjust the time based on when the start events actually happened. We can
    see in [Figure 9.29](#F0150) a copy to the device, a kernel execution, a copy
    from the device, a copy to the device, a second kernel invocation, and finally
    a copy from the device.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，API中没有提供获取事件绝对时间戳的函数。你只能获取两个事件之间的时间差。然而，通过将一个事件推送到流的开始位置，你可以将其作为时间点零，从而获得相对于流开始的时间。然而，当请求不同GPU之间事件的时间差时，API会返回错误。这使得在使用多个GPU时创建时间线变得复杂，因为你可能需要根据实际发生的开始事件来调整时间。我们可以在[图9.29](#F0150)中看到：一次设备复制、一次内核执行、一次设备复制、一次设备复制、第二次内核调用，最后一次设备复制。
- en: Notice that with different devices, the copy times are largely similar but the
    kernels’ time will vary considerably. In the second-to-last copy from device operation
    for the GTX470 device (CFD 2), notice the bar is somewhat smaller (258 ms versus
    290 ms). This is because the GTX470 starts its transfer first and only toward
    the tail end of the transfer do the other devices also initiate a transfer. The
    GT9800, being a much slower device, still has its kernel being executed while
    GTX470 has in fact completed its transfer. With different device generations,
    you will get such a pattern. The transfer rates are largely similar, but the kernel
    times cause shifts in the points where the transfers are initiated.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然不同设备的复制时间大致相似，但内核的执行时间会有显著差异。在GTX470设备（CFD 2）进行倒数第二次从设备复制操作时，请注意条形图稍微小一些（258毫秒对比290毫秒）。这是因为GTX470首先开始传输，而其他设备只有在传输的尾端才开始传输。GT9800作为一个速度较慢的设备，尽管GTX470已经完成传输，它的内核仍然在执行。不同设备代际之间会出现这种模式。传输速率大致相似，但内核时间会导致传输开始的时间点发生变化。
- en: '[Figure 9.25](#F0130) was generated using timers, but tools such as Parallel
    Nsight and the Visual Profiler will draw the timeline for you automatically, along
    with the CPU timeline so you can clearly see what has happened and when.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.25](#F0130)是通过计时器生成的，但像Parallel Nsight和Visual Profiler这样的工具会自动为你绘制时间线，并且还会显示CPU的时间线，这样你可以清楚地看到何时发生了什么。'
- en: Note that it’s possible with `cudaEventQuery` API to simply query if the event
    has completed without causing a blocking call as with `cudaEventSynchronize`.
    Thus, the CPU can continue to do useful work, or simply move onto the next stream
    to see if it has completed yet.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过`cudaEventQuery` API，您可以仅查询事件是否完成，而不会像`cudaEventSynchronize`那样产生阻塞调用。因此，CPU可以继续做有用的工作，或者直接进入下一个流，查看它是否已经完成。
- en: '[PRE85]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: In this particular example, taken from another program, we have an array of
    events, `memcpy_to_stop`, indexed by device number and test number. We check if
    the event has completed by a call to `cudaEventQuery`, which returns `cudaSuccess`
    if the event has already completed. If so, we get the delta time between this
    event and the start event `memcpy_to_start` from the same device, but for test
    0, we get the start event for the whole kernel stream on that GPU. To obtain the
    delta time we simply call the `cudaEventElapsedTime` function.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定示例中，来自另一个程序，我们有一个事件数组`memcpy_to_stop`，按设备编号和测试编号索引。我们通过调用`cudaEventQuery`检查事件是否完成，如果事件已经完成，它会返回`cudaSuccess`。如果是这样，我们获取该事件与起始事件`memcpy_to_start`之间的时间差，该事件来自同一设备，但对于测试0，我们获取整个内核流在该GPU上的开始事件。为了获得时间差，我们只需调用`cudaEventElapsedTime`函数。
- en: Note as this will generate an error if the event has not yet completed, it is
    guarded by the check with `cudaEventQuery`. We could equally call `cudaEventSynchronize`
    if we simply wanted a blocking call that would wait for the event to complete.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果事件尚未完成，这将会产生一个错误，因此它通过`cudaEventQuery`进行检查。如果我们只是希望有一个阻塞调用等待事件完成，我们也可以调用`cudaEventSynchronize`。
- en: 'If we’re particularly interested in the absolute time, the GPU does provide
    access to the low-level timers with the help of some embedded PTX code:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们特别关心绝对时间，GPU确实可以通过一些嵌入的PTX代码提供对低级定时器的访问：
- en: '[PRE89]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: This section of code loads the raw clock value into a C variable that can then
    later be stored in a history buffer and transferred back to the host. The special
    `%clock` value is simply a 32-bit counter that wraps at max(u32). Compute 2.x
    hardware provides a 64-bit clock, thus allowing a wider time range over which
    values can be timed. Note, the CUDA API provides functions to access these register
    values through the use of the `clock` and `clock64` functions.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将原始时钟值加载到C变量中，稍后可以将其存储到历史缓冲区并传回主机。特殊的`%clock`值只是一个32位计数器，在最大值（u32）时会回绕。Compute
    2.x硬件提供了一个64位时钟，因此允许在更宽的时间范围内进行计时。请注意，CUDA API提供了通过`clock`和`clock64`函数访问这些寄存器值的功能。
- en: You can use this to measure the times of device functions within kernel calls
    or sections of code. Such measurements are not shown with either the Visual Profiler
    or Parallel Nsight, as their resolution onto the timing stops at the global-level
    kernel functions. You can also use this to store the times warps arrive at a barrier
    point. Simply create a store on a per-warp basis prior to a call to a barrier
    primitive such as `syncthreads`. You can then see the distribution of the warps
    to the synchronization point.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用这个来测量内核调用或代码段中设备函数的时间。这些测量不会显示在Visual Profiler或Parallel Nsight中，因为它们的时间分辨率仅限于全局级别的内核函数。你也可以用它来存储warps到达同步点的时间。只需在调用像`syncthreads`这样的屏障原语之前，按warp逐个存储。然后你可以看到warps到达同步点的分布情况。
- en: However, one very important caveat here is you must understand that a given
    warp in a kernel will not be running all the time. Thus, as with timing multiple
    streams, a warp may store a start time, get suspended, sometime later get resumed,
    and meet the next timer store event. The delta is only the overall real time difference,
    not the time the SM spent executing code from the given warp.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个非常重要的警告，你必须理解，内核中的一个warp并不会一直运行。因此，和计时多个流一样，一个warp可能会存储一个开始时间，然后被挂起，稍后恢复，并遇到下一个定时器存储事件。这个差值只是总体的实际时间差，而不是SM花费在执行该warp的代码上的时间。
- en: You should also realize that instrumenting code in this way may well affect
    its timing and execution order relative to other warps. You will be making global
    memory stores to later transfer this data back to the host where it can be analyzed.
    Consequently, your instrumentation impacts not only execution flow, but memory
    accesses. The effect of this can be minimized by running a single block of 32
    threads, that is, a single warp. However, this entirely discounts the quite necessary
    effects of running with other warps present on the SM and across multiple SMs
    within the GPU.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该意识到，以这种方式对代码进行仪器化可能会影响其时序和执行顺序，相对于其他warp的执行。你将会进行全局内存存储，稍后将这些数据传回主机进行分析。因此，你的仪器化不仅会影响执行流程，还会影响内存访问。通过运行一个包含32个线程的单一块，即一个warp，可以将这种影响最小化。然而，这完全忽视了在SM上运行时其他warp的必要影响，尤其是在GPU的多个SM之间。
- en: Overlapping GPU transfers
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重叠GPU传输
- en: There are two strategies for trying to overlap transfer; first, to overlap transfer
    times with the compute time. We’ve look at this in detail in the last section,
    explicitly with the use of streams and implicitly with the use of zero-copy memory.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种策略可以尝试重叠传输；首先是将传输时间与计算时间重叠。我们在上一节中详细讨论了这一点，明确地使用了流，并隐式地使用了零拷贝内存。
- en: Streams are a very useful feature of GPU computing. By building independent
    work queues we can drive the GPU device in an asynchronous manner. That is, the
    CPU can push a number of work elements into a queue and then go off and do something
    else before having to service the GPU again.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 流是GPU计算中非常有用的功能。通过构建独立的工作队列，我们可以异步驱动GPU设备。也就是说，CPU可以将多个工作元素推送到队列中，然后去做其他事情，而不需要马上再次服务GPU。
- en: To some extent, operating the GPU synchronously with stream 0 is like polling
    a serial device with a single character buffer. Such devices were used in the
    original serial port implementations for devices like modems that operated over
    the RS232 interface. These are now obsolete and have been replaced with USB1,
    USB2, and USB3 interfaces. The original serial controller, a UART, would raise
    an interrupt request to the processor to say it had received enough bits to decode
    one character and its single character buffer was full. Only once the CPU serviced
    the interrupt could the communications continue. One character at a time communication
    was never very fast, and highly CPU intensive. Such devices were rapidly replaced
    with UARTs that had a 16-character buffer in them. Thus, the frequency of the
    device raising an interrupt to the CPU was reduced by a factor of 16\. It could
    process the incoming characters and accumulate them to create a reasonably sized
    transfer to the CPU’s memory.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，GPU与流0同步操作就像是通过单字符缓冲区轮询串行设备。这些设备曾在原始的串行端口实现中用于像调制解调器这样的设备，它们通过RS232接口进行通信。现在这些设备已经过时，取而代之的是USB1、USB2和USB3接口。原始的串行控制器UART会向处理器发送中断请求，表示它已经接收到足够的位来解码一个字符，并且它的单字符缓冲区已满。只有CPU处理了这个中断后，通信才能继续。每次传输一个字符的通信方式既不够快速，也非常依赖CPU。此类设备迅速被具有16字符缓冲区的UART替代。因此，设备向CPU发送中断的频率减少了16倍。它可以处理传入的字符并将其累积，创建一个合理大小的传输到CPU的内存中。
- en: By creating a stream of work for the GPU we’re effectively doing something similar.
    Instead of the GPU working in a synchronous manner with the CPU, and the CPU having
    to poll the GPU all the time to find out if it’s ready, we just give it a chunk
    of work to be getting on with. We then only periodically have to check if it’s
    now out of work, and if so, push some more work into the stream or work queue.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为GPU创建工作流，我们实际上是在做类似的事情。我们并不是让GPU与CPU同步工作，CPU也不需要一直轮询GPU来查看它是否准备好，而是直接给GPU一块工作任务，让它开始处理。然后我们只需要定期检查它是否已经完成任务，如果是，就将更多的工作推送到流或工作队列中。
- en: Through the CUDA stream interface we can also drive multiple GPU devices, providing
    you remember to switch the desired device before trying to access it. For asynchronous
    operation, pinned or page-locked memory is required for any transfers to and from
    the GPU.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 通过CUDA流接口，我们还可以驱动多个GPU设备，前提是你记得在访问设备之前切换到所需的设备。对于异步操作，任何传输到GPU和从GPU传输的数据都需要固定内存或页锁定内存。
- en: On a single-processor system, all the GPUs will be connected to a single PCI-E
    switch. The purpose of a PCI-E switch is to connect the various high-speed components
    to the PCI-E bus. It also functions as a means for PCI-E cards to talk to one
    another without having to go to host memory.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在单处理器系统中，所有GPU将连接到单一的PCI-E交换机。PCI-E交换机的作用是将各种高速组件连接到PCI-E总线。它还充当PCI-E卡之间进行通信的手段，而无需访问主机内存。
- en: Although we may have multiple PCI-E devices, in the case of our test machine,
    four GPUs on four separate X8 PCI-E 2.0 links, they are still connected to a *single*
    PCI-E controller. In addition, depending on the implementation, this controller
    may actually be on the CPU itself. Thus, if we perform a set of transfers to multiple
    GPUs at any one point in time, although the individual bandwidth to each device
    may be in the order of 5 GB/s in each direction, can the PCI-E switch, the CPU,
    the memory, and other components work at that speed if all devices become active?
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可能有多个 PCI-E 设备，但在我们的测试机器中，四个 GPU 通过四个独立的 X8 PCI-E 2.0 链接连接，它们仍然连接到一个*单一*的
    PCI-E 控制器。此外，根据实现方式，这个控制器实际上可能就在 CPU 本身上。因此，如果我们在任何时刻执行一组传输到多个 GPU，尽管每个设备的带宽在每个方向上可能达到大约
    5 GB/s，但如果所有设备都变为活跃，PCI-E 交换机、CPU、内存和其他组件能以这个速度工作吗？
- en: With four GPUs present on a system, what scaling can be expected? With our I7
    920 Nehalem system, we measured around 5 GB/s to a single card using a PCI-E 2.0
    X16 link. With the AMD system, we have around 2.5–3 GB/s on the PCI-E 2.0 X8 link.
    As the number of PCI-E lanes are half that of the I7 system, these sorts of numbers
    are around what you might expect to achieve.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统中安装四块 GPU 后，能期待什么样的扩展性？在我们的 I7 920 Nehalem 系统中，我们测得通过 PCI-E 2.0 X16 链接向单个卡传输的数据约为
    5 GB/s。而在 AMD 系统中，通过 PCI-E 2.0 X8 链接的带宽约为 2.5–3 GB/s。由于 PCI-E 通道数是 I7 系统的一半，这些数据大致是你可以期待的结果。
- en: We modified the bandwidth test program we used earlier for measuring the PCI-E
    bandwidth to measure the bandwidth as we introduce more cards and more concurrent
    transfers. Any number of things can affect the transfers once we start introducing
    concurrent transfers to different GPUs. Anyone familiar with the multi-GPU scaling
    within the games industry will appreciate that simply inserting a second GPU does
    not guarantee twice the performance. Many benchmarks show that most commercial
    games benefit significantly from two GPU cards. Adding a third card often introduces
    some noticeable benefit, but nothing like the almost times two scaling that is
    often seen with a second card. Adding a fourth card will often cause the performance
    to drop.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改了之前用来测量 PCI-E 带宽的带宽测试程序，以便在引入更多显卡和更多并发传输时测量带宽。一旦我们开始为不同的 GPU 引入并发传输，许多因素可能会影响传输速度。任何熟悉游戏行业中多
    GPU 扩展的人都会明白，简单地插入第二块 GPU 并不能保证性能翻倍。许多基准测试显示，大多数商业游戏在使用两块 GPU 时性能提升显著。增加第三块卡时，通常会带来一些明显的性能提升，但远不如第二块卡带来的接近翻倍的扩展效果。增加第四块卡时，性能往往会下降。
- en: Now this may not seem very intuitive, adding more hardware equals lower speed.
    However, it’s the same issue we see on CPUs when the core count becomes too high
    for the surrounding components. A typical high-end motherboard/CPU solution will
    dedicate at most 32 PCI-E lands to the PCI-E bus. This means only two cards can
    run at full X16 PCI-E 2.0 speed. Anything more than this is achieved by the use
    of PCI-E switch chips, which multiplex the PCI-E lines. This works well until
    the two cards on the PCI-E multiplexer both need to do a transfer at the same
    time.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这看起来可能不太直观，增加更多硬件意味着速度更低。然而，这就像我们在 CPU 上看到的现象，当核心数变得过高而周围组件无法跟上时。典型的高端主板/CPU
    解决方案最多会为 PCI-E 总线分配 32 个 PCI-E 引脚。这意味着最多只有两张卡可以以全速 X16 PCI-E 2.0 速度运行。超出这个数量时，便需要使用
    PCI-E 开关芯片来复用 PCI-E 线路。这在两个卡上的传输同时进行时表现良好，直到两个卡都需要同时传输数据。
- en: The AMD system we’ve run most of these tests in this book on does not use a
    multiplexer, but drops the speed of each connected GPU to an X8 link when four
    GPUs are present. Thus, at 2.5–3 GB/s per device, we could achieve a theoretical
    maximum of 10–12.5 GB/s. In addition, being an AMD solution, the PCI-E controller
    is built into the processor, which also sits between the PCI-E system and main
    memory. The bandwidth to main memory is approximately 12.5 GB/s. Therefore, you
    can see this system would be unlikely to achieve the full potential of four GPUs.
    See [Tables 9.2](#T0015) and [9.3](#T0020) and [Figures 9.26](#F0135) and [9.27](#F0140).
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中进行大部分测试的 AMD 系统并没有使用多路复用器，而是在连接四个 GPU 时，将每个 GPU 的速度降低为 X8 链接。因此，每个设备的带宽为
    2.5–3 GB/s，我们可以实现理论上的最大值 10–12.5 GB/s。此外，作为一款 AMD 解决方案，PCI-E 控制器集成在处理器中，且该处理器位于
    PCI-E 系统与主内存之间。主内存的带宽约为 12.5 GB/s。因此，可以看出，该系统不太可能实现四个 GPU 的全部潜力。请参见[表 9.2](#T0015)、[9.3](#T0020)和[图
    9.26](#F0135)、[9.27](#F0140)。
- en: Table 9.2 Bandwidth Effects of Multiple PCI-E Transfers to the Device
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.2 多个 PCI-E 传输到设备的带宽效应
- en: '![Image](../images/T000090tabT0015.jpg)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0015.jpg)'
- en: Table 9.3 Bandwidth Effects of Multiple PCI-E Transfers from the Device
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.3 多个 PCI-E 传输从设备的带宽效应
- en: '![Image](../images/T000090tabT0020.jpg)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0020.jpg)'
- en: '![image](../images/F000090f09-26-9780124159334.jpg)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-26-9780124159334.jpg)'
- en: FIGURE 9.26 Multi-GPU PCI-E bandwidth to device AMD 905e Phenom II.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26 多GPU PCI-E 带宽到设备 AMD 905e Phenom II。
- en: '![image](../images/F000090f09-27-9780124159334.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-27-9780124159334.jpg)'
- en: FIGURE 9.27 Multi-GPU PCI-E bandwidth from device AMD 905e Phenom II.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.27 多GPU PCI-E 带宽从设备 AMD 905e Phenom II。
- en: What you can see from [Tables 9.2](#T0015) and [9.3](#T0020) is that transfers
    scale quite nicely to three GPUs. We’re seeing approximately linear scaling. However,
    when the four GPUs compete for the available resources (CPU, memory bandwidth,
    and PCI-E switch bandwidth) the overall rate is slower.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表 9.2](#T0015)和[9.3](#T0020)中可以看到，传输在三张 GPU 上扩展得非常好，几乎呈线性扩展。然而，当四个 GPU 争夺可用资源（CPU、内存带宽和
    PCI-E 开关带宽）时，整体传输速率变得较慢。
- en: The other multi-GPU platform we have to work with is a six-GPU system based
    on the Nehalem I7 platform and the ASUS supercomputer motherboard (P6T7WS) with
    3 GTX295 Dual GPU cards. This uses dual NF200 PCI-E switch chips allowing each
    PCI-E card to work with a full X16 link. While this might be useful for inter-GPU
    communication, the P2P (peer-to-peer) model supported in CUDA 4.x, it does not
    extend the bandwidth available to and from the host if both cards are simultaneously
    using the bus. We are using GTX290 cards, which are a dual-GPU device. Internally,
    each GPU has to share the X16 PCI-E 2.0 link. [Table 9.4](#T0025) and [Figure
    9.28](#F0145) show what effect this has.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用的另一个多 GPU 平台是基于 Nehalem I7 平台和 ASUS 超级计算机主板（P6T7WS）以及 3 张 GTX295 双 GPU
    卡的六 GPU 系统。该系统使用了双 NF200 PCI-E 开关芯片，允许每个 PCI-E 卡与完整的 X16 链路进行工作。虽然这对于 GPU 之间的通信（在
    CUDA 4.x 中支持的 P2P（点对点）模型）可能有用，但如果两个卡同时使用总线，它并不会扩展主机的带宽。我们使用的是 GTX290 卡，这是一个双 GPU
    设备。在内部，每个 GPU 必须共享 X16 PCI-E 2.0 链路。[表 9.4](#T0025) 和 [图 9.28](#F0145) 显示了这一点的影响。
- en: Table 9.4 I7 Bandwidth to Device
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.4 I7 到设备带宽
- en: '![Image](../images/T000090tabT0025.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0025.jpg)'
- en: '![image](../images/F000090f09-28-9780124159334.jpg)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-28-9780124159334.jpg)'
- en: FIGURE 9.28 I7 bandwidth to device.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28 I7 带宽到设备。
- en: As you can see from [Table 9.4](#T0025), we see an approximate linear increase
    in total bandwidth to the device. We achieve a peak of just over 10 GB/s, 20%
    or so higher than our AMD-based system.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表 9.4](#T0025)中可以看出，我们观察到设备到带宽大致呈线性增长。我们达到了稍微超过 10 GB/s 的峰值，比我们的基于 AMD 的系统高出大约
    20%。
- en: We can see the bandwidth from the device is a different story ([Table 9.5](#T0030)
    and [Figure 9.29](#F0150)). Bandwidth peaks with two devices, and is not significantly
    higher than our AMD system. This is not altogether unexpected if you consider
    the design in most GPU systems is based around gaming. In a game, most of the
    data is being sent *to* the GPU with very little if any coming back to the CPU
    host. Thus, we see a near linear scaling of up to three cards, which coincides
    with the top-end triple SLI (scalable link interface) gaming platforms. Vendors
    have little incentive to provide PCI-E bandwidth beyond this setup. As the GTX290
    is actually a dual-GPU card, we may also be seeing that the internal SLI interface
    is not really able to push the limits of the card. We’re clearly seeing some resource
    contention.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，从设备的带宽是另一个情况（[表 9.5](#T0030) 和 [图 9.29](#F0150)）。带宽在使用两个设备时达到峰值，并且比我们的
    AMD 系统并没有显著增加。如果考虑到大多数 GPU 系统的设计是围绕游戏展开的，这并不令人意外。在游戏中，大部分数据都是发送*到*GPU，几乎没有数据返回到
    CPU 主机。因此，我们看到最多三个卡的带宽几乎呈线性扩展，这与顶级的三重 SLI（可扩展链路接口）游戏平台相吻合。厂商没有太大动力提供超出这个配置的 PCI-E
    带宽。由于 GTX290 实际上是一个双 GPU 卡，我们也可能看到内部的 SLI 接口实际上无法突破卡的极限。显然，我们看到了一些资源竞争。
- en: Table 9.5 I7 Bandwidth from Device
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.5 I7 从设备带宽
- en: '![Image](../images/T000090tabT0030.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0030.jpg)'
- en: Section summary
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小节总结
- en: • Understand and plan for the fact you will have limited PCI-E bandwidth capability.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: • 了解并规划有限的 PCI-E 带宽能力。
- en: • Always use pinned memory where possible.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: • 尽可能使用固定内存（pinned memory）。
- en: • Use transfer sizes of at least 2 MB.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用至少 2 MB 的传输大小。
- en: • Understand the use of zero-copy memory as an alternative to the stream API.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: • 了解零拷贝内存作为替代流 API 的使用。
- en: • Think about how to overlap transfer time with kernel execution time.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: • 考虑如何将传输时间与内核执行时间重叠。
- en: • Do not expect a linear scaling of bandwidth when using multiple GPUs.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: • 在使用多个 GPU 时，不要期望带宽按线性方式扩展。
- en: 'Strategy 4: Thread Usage, Calculations, and Divergence'
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 4：线程使用、计算与分歧
- en: Thread memory patterns
  id: totrans-505
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线程内存模式
- en: Breaking down the application into *suitably* sized grids, blocks, and threads
    is often one of the key aspects of performance of CUDA kernels. Memory is the
    bottleneck in almost any computer design, the GPU included. A bad choice of thread
    layout typically also leads to a bad memory pattern, which will significantly
    harm performance.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用程序拆分为 *合适的* 网格、块和线程大小通常是 CUDA 核心性能的关键因素之一。内存几乎是任何计算机设计中的瓶颈，GPU 也不例外。糟糕的线程布局选择通常会导致糟糕的内存模式，从而显著影响性能。
- en: Consider the first example, a 2 × 32 layout of threads ([Figure 9.30](#F0155))
    versus a 32 × 2 layout of threads. Think about how they would typically overlay
    memory if they were processing floating-point values. In the 2 × 32 example, thread
    0 cannot be coalesced with any other thread than thread 1\. In this case the hardware
    issues a total of 16 memory fetches. The warp cannot progress until at least the
    first half-warp has acquired all the data it needs. Therefore, at least eight
    of these very long memory transactions need to complete prior to any compute activity
    on the SM. As most warps will be following the same pattern, the SM will be swamped
    with issuing memory requests while the compute part of the SM is almost idle.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑第一个例子，一个 2 × 32 线程布局（[图 9.30](#F0155)）与 32 × 2 线程布局的区别。想想它们在处理浮点值时通常是如何覆盖内存的。在
    2 × 32 的例子中，线程 0 不能与除了线程 1 之外的任何线程进行合并。在这种情况下，硬件总共发出 16 次内存获取请求。直到至少第一半个 warp
    获取了它所需的所有数据，这个 warp 才能继续。因此，在任何计算活动开始之前，至少需要完成这八个非常长的内存事务。由于大多数 warp 会遵循相同的模式，SM
    将被大量内存请求所淹没，而 SM 的计算部分几乎处于空闲状态。
- en: '![image](../images/F000090f09-29-9780124159334.jpg)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-29-9780124159334.jpg)'
- en: FIGURE 9.29 I7 bandwidth from device.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.29 设备的 I7 带宽。
- en: '![image](../images/F000090f09-30-9780124159334.jpg)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-30-9780124159334.jpg)'
- en: FIGURE 9.30 2 × 32 thread grid layout.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.30 2 × 32 线程网格布局。
- en: We saw from the bandwidth analysis in the previous section that there is a limit
    to the number of memory requests the SM can push out from the warps. The SM services
    the data request for any single warp over two clock cycles. In our example, the
    request has to be broken into 16 × 8 byte memory transactions.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前一节的带宽分析中看出，SM 推送的内存请求数量是有限的。SM 为任何单一 warp 服务数据请求时需要两个时钟周期。在我们的例子中，请求必须分解为
    16 × 8 字节的内存事务。
- en: On Fermi, the first of these would cause a read miss in the L1 cache. The L1
    cache would request the minimum size of data possible, 128 bytes from the L2 cache,
    and some 16 times more data than the thread needs. Thus, when data is moved from
    the L2 cache to the L1 cache, just 3.125% of the data moved is consumed by thread
    0\. As thread 1 also wants the adjacent address, we can increase this to 6.25%,
    which is still terrible.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Fermi 上，第一个请求会导致 L1 缓存中的读取缺失。L1 缓存将请求可能的最小数据量，即从 L2 缓存请求 128 字节，而实际请求的数据量是线程所需数据的
    16 倍。因此，当数据从 L2 缓存移动到 L1 缓存时，线程 0 仅消耗了移动数据的 3.125%。由于线程 1 也需要相邻的地址，我们可以将这一比例提高到
    6.25%，但这仍然非常糟糕。
- en: On the first run through the code the L2 cache is unlikely to contain the data.
    It issues a 128-byte fetch also to slow global memory. This latency-expensive
    operation is finally performed and 128 bytes arrive at the L2 cache.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次运行代码时，L2 缓存不太可能包含数据。它还会发出一个 128 字节的请求来访问较慢的全局内存。这个延迟昂贵的操作最终完成，128 字节的数据到达
    L2 缓存。
- en: The L2 cache is 768 K in size on a 16 SM device. Assuming we’re using a GTX580,
    we have 16 SMs. That is just 48 KB per SM, the maximum size of the L1 cache. Using
    128-byte cache lines we have just 384 entries in the cache per SM. If we assume
    the SM is fully loaded with 48 warps (Kepler supports 64), each warp will issue
    16 separate reads, which is 768 reads in total. This means we’d need 768 cache
    lines, not the 384 we have, just to cache the data needed so each warp can hold
    a single block in memory.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个包含 16 个 SM 的设备上，L2 缓存的大小为 768 KB。假设我们使用的是 GTX580，它有 16 个 SM。每个 SM 的缓存大小仅为
    48 KB，这是 L1 缓存的最大大小。使用 128 字节的缓存行，我们每个 SM 只有 384 个缓存条目。如果假设 SM 完全加载，包含 48 个 warp（Kepler
    支持 64 个），那么每个 warp 将发出 16 次独立的读取请求，总共是 768 次读取。这意味着我们需要 768 个缓存行，而不是现有的 384 个，仅为了缓存每个
    warp 所需的数据，这样每个 warp 才能在内存中保留一个块。
- en: The cache is effectively far too small to be used for temporal locality in this
    example. By temporal locality we mean we expect the data to remain in the cache
    from one read to the next. Halfway through processing the warps in each SM, the
    cache is full and the hardware starts filling it with new data. Consequently,
    there is absolutely no data reuse with the L2 cache, but a significant overhead
    in having to fetch entire cache lines. In fact, the only saving grace is that
    Fermi, unlike previous generations, will now forward the data it fetched to the
    other thread in our example.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，缓存实质上小得不足以用于时间局部性。这里的时间局部性指的是我们期望数据在一次读取到下一次读取之间能保持在缓存中。在每个SM处理warps的过程中，缓存会很快被填满，硬件开始用新的数据填充缓存。因此，L2缓存没有任何数据重用，反而需要花费大量时间来获取整个缓存行。事实上，唯一的安慰是，与之前的世代不同，Fermi现在会将其获取的数据转发到我们示例中的另一个线程。
- en: The cache model is one that can cause problems in that it allows people to think
    the hardware will save them from poor programming. Let’s assume for a moment we
    have to use this thread pattern and we would have processed the element we fetched
    from memory a number of times. The thread pattern for fetching data does not have
    to be the same thread pattern for using the data. We can fetch data into shared
    memory in a 32 × 2 pattern, synchronize the threads, and then switch to a 2 ×
    32 usage pattern if we wish. Despite the shared memory bank conflicts this would
    then incur, it would still be an order of magnitude faster than doing the global
    memory fetches. We can also simply add a padding element to the shared memory
    by declaring it as 33 × 2 to ensure when we access it, these bank conflicts are
    removed.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存模型可能会带来问题，因为它让人们误以为硬件可以拯救他们免于糟糕的编程。假设我们必须使用这个线程模式，而我们已经从内存中获取的元素将被处理多次。获取数据的线程模式不必与使用数据的线程模式相同。如果需要，我们可以按照32
    × 2的模式将数据获取到共享内存中，同步线程，然后切换到2 × 32的使用模式。尽管这样会带来共享内存银行冲突，但仍然比执行全局内存读取快一个数量级。我们还可以通过将共享内存声明为33
    × 2，简单地添加一个填充元素，以确保访问时去除这些银行冲突。
- en: Consider for a moment the difference in handling of the memory system. We issue
    1 coalesced read for 128 bytes instead of 16 separate reads. There’s a factor
    of 16:1 improvement in both the number of memory transactions in flight and also
    bandwidth usage. Data can be moved from the L2 to the L1 cache in just one transaction,
    not 16.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想内存系统处理上的差异。我们为128字节发出1个合并读取，而不是16个单独的读取。无论是内存事务数量还是带宽使用，都提高了16:1。数据可以仅通过一个事务从L2移动到L1缓存，而不是16个事务。
- en: The LSUs in the SM have to issue only a single fetch transaction instead of
    16 separate fetches, taking just 2 clock cycles instead of 32 and freeing up the
    LSUs for other tasks from other warps.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: SM中的LSU只需要发出一个获取事务，而不是16个单独的获取事务，耗时仅为2个时钟周期，而不是32个，释放了LSU处理其他warps任务的时间。
- en: Each warp consumes a single cache line, 48 maximum per SM. Thus, of the 384
    cache lines we have per SM in the L2 cache, we’re using only 100, just 12.5% of
    the L2 cache instead of 200%. Thus, it’s absolutely critical that to get anywhere
    near the full performance, even in Fermi with its multilevel caches, you have
    to fetch data in coalesced blocks of 128 bytes across a warp.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 warp 消耗一个缓存行，每个 SM 最多 48 个。因此，在每个 SM 的 L2 缓存中，我们有 384 个缓存行，但只使用了 100 个，也就是只使用了
    12.5% 的 L2 缓存，而不是 200%。因此，要想接近完全性能，尤其是在拥有多级缓存的 Fermi 中，至关重要的是，你必须以 128 字节的合并块从全局内存中读取数据。
- en: Now we could configure the L2 cache to fetch only 32 bytes instead of 128 bytes
    using the `-Xptxas -dlcm=cg` compiler flag. However, this also disables global
    memory storage in the L1 cache. It’s an easy fix but a poor solution to the fact
    that you are not fetching data in large enough blocks from global memory. To get
    the best performance from a given device, you need to understand what’s going
    on down inside or use libraries that are coded by someone who does.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过使用`-Xptxas -dlcm=cg`编译器标志，将 L2 缓存配置为只获取 32 字节，而不是 128 字节。然而，这也会禁用 L1
    缓存中的全局内存存储。这是一个简单的修复方法，但对于你没有从全局内存中以足够大的块读取数据这一事实来说，解决方案并不理想。为了从给定的设备中获得最佳性能，你需要了解底层发生的情况，或者使用那些由懂得这些的人编写的库。
- en: We can see this quite clearly with the effects on memory bandwidth with Parallel
    Nsight if you select “Custom” experiment and then add in the L1 and L2 cache counters.
    The particular counters we are interested in are shown in [Table 9.6](#T0035).
    These can be set up in Parallel Nsight using the “Custom” experiment, shown in
    [Figure 9.31](#F0160).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择“自定义”实验，并添加 L1 和 L2 缓存计数器，你可以非常清楚地看到 Parallel Nsight 对内存带宽的影响。我们感兴趣的特定计数器显示在[表
    9.6](#T0035)中。这些可以通过 Parallel Nsight 中的“自定义”实验进行设置，如[图 9.31](#F0160)所示。
- en: Table 9.6 Parallel Nsight Cache Counters
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.6 Parallel Nsight 缓存计数器
- en: '| Nsight Counter | Usage |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| Nsight 计数器 | 用途 |'
- en: '| --- | --- |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| L1 global load hits | The number of global memory load requests met by the
    L1 cache. |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| L1 全局加载命中 | L1 缓存满足的全局内存加载请求次数。 |'
- en: '| L1 global load misses | The number of global memory load requests not met
    by the L1 cache. |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| L1 全局加载未命中 | L1 缓存未满足的全局内存加载请求次数。 |'
- en: '| L2 subpartition 0 read section misses | Half the number of L2 misses. |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| L2 子分区 0 读取部分未命中 | L2 未命中次数的一半。 |'
- en: '| L2 subpartition 1 read section misses | The other half of the number of L2
    misses. |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| L2 子分区 1 读取部分未命中 | L2 未命中次数的另一半。 |'
- en: '| L2 subpartition 0 read section queries | Half the number of L2 access attempts.
    |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| L2 子分区 0 读取部分查询 | L2 访问尝试次数的一半。 |'
- en: '| L2 subpartition 1 read section queries | The other half of the number of
    L2 access attempts. |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| L2 子分区 1 读取部分查询 | L2 访问尝试次数的另一半。 |'
- en: '![image](../images/F000090f09-31-9780124159334.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-31-9780124159334.jpg)'
- en: FIGURE 9.31 Setting up Parallel Nsight to capture cache statistics.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.31 设置 Parallel Nsight 捕获缓存统计信息。
- en: From these counters we can manually work out the L1 and L2 cache hit ratio.
    The hit ratio is the percentage of reads (or writes) that we cached. Every cached
    access saves us several hundreds of cycles of global memory latency.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些计数器，我们可以手动计算 L1 和 L2 缓存的命中率。命中率是我们缓存的读取（或写入）占总读取（或写入）的百分比。每次缓存访问都能为我们节省数百个周期的全局内存延迟。
- en: When we look at the results for the sample sort algorithm in [Table 9.7](#T0040),
    we can instantly see the L2 cache hit ratio drops off sharply as soon as the kernel
    exceeds 64 threads. Occupancy increases, but performance drops off. This is not
    at all surprising given the usage of the L2 cache a prefix sum array will generate.
    If each thread is processing one bin, as we extend the number of threads, the
    size of the memory area being cached increases. As soon as it exceeds the L2 cache
    size the hit ratio rapidly drops off.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看[表 9.7](#T0040)中样本排序算法的结果时，可以立刻看到当内核线程数超过 64 时，L2 缓存命中率急剧下降。占用率增加，但性能下降。考虑到
    L2 缓存前缀和求和数组的使用，这一点并不令人惊讶。如果每个线程处理一个桶，随着线程数的增加，缓存的内存区域的大小也随之增大。一旦超出 L2 缓存的大小，命中率就会迅速下降。
- en: Table 9.7 Cache Hit Ratio for Sample Sort
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.7 样本排序的缓存命中率
- en: '![Image](../images/T000090tabT0040.jpg)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0040.jpg)'
- en: The solution to the problem is to replace the existing algorithm that uses one
    thread per bin with one where the threads all work on a single bin at a time.
    This way we’d achieve coalesced memory accesses on each iteration and significantly
    better locality of memory access. An alternative solution would be to use shared
    memory to handle the transition between noncoalesced access by the threads and
    the necessary coalesced access when reading or writing to global memory.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是将现有的每个桶使用一个线程的算法，替换为所有线程同时处理一个桶的算法。这样我们可以在每次迭代中实现合并内存访问，并显著提高内存访问的局部性。另一种解决方案是使用共享内存来处理线程之间非合并访问与读取或写入全局内存时所需的合并访问之间的过渡。
- en: Inactive threads
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非活动线程
- en: Threads, despite there being many thousands of them, are not free, even if they
    are inactive. The problem with inactive threads is twofold. First, a warp will
    remain active, scheduled, and using resources if just one of its threads is active.
    There are a limited number of warps that can be dispatched in a dispatch period
    (two clock cycles). This is two on compute 2.0 hardware, four on compute 2.1 hardware
    and eight in compute 3.x hardware. There is no point in the hardware dispatching
    a warp with a single thread to a set of CUDA cores and having it use just a single
    CUDA core while the other 15 idle. However, this is exactly what the hardware
    has to do if there is divergence of execution flow within a warp down to just
    one thread being active.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管线程数量达到成千上万，但它们并不是免费的，即使它们处于不活跃状态。关于不活跃线程的问题有两个方面。首先，如果一个 warp 中的任意一个线程是活跃的，该
    warp 会保持活跃、被调度并使用资源。在一个调度周期（两个时钟周期）内，可以调度的 warp 数量是有限的。在 compute 2.0 硬件中为两个，在
    compute 2.1 硬件中为四个，在 compute 3.x 硬件中为八个。如果硬件将一个包含单个活跃线程的 warp 调度到一组 CUDA 核心上，结果只使用一个
    CUDA 核心而其他 15 个空闲，这是没有意义的。然而，如果 warp 内部的执行流出现分歧，导致只有一个线程活跃时，硬件就必须做这个操作。
- en: You sometimes see a parallel reduction–type operation that has been written
    by a programmer who does not understand the hardware well. They will perform the
    reduction operation within every warp, going from 32 to 16, to 8, to 4, to 2,
    and finally to 1 active thread. Regardless of whether you use 32 threads or 1
    thread the hardware still allocates 32 and simply masks out the inactive ones.
    Because the warps are still active, even if they have only one thread active,
    they still need to be scheduled onto the hardware.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你会看到一种并行归约类型的操作，它是由一个不了解硬件的程序员编写的。他们将在每个 warp 内进行归约操作，从 32 个线程减少到 16、8、4、2，最终到
    1 个活跃线程。不管你使用的是 32 个线程还是 1 个线程，硬件仍然会分配 32 个线程，只是将不活跃的线程屏蔽掉。因为这些 warps 仍然是活跃的，即使它们只有一个活跃线程，它们仍然需要被调度到硬件上。
- en: A much better approach to this is to have all 32 threads in every block compute
    a set of partial results. Let’s use the sum operation, as it’s easy to understand.
    With 32 threads per warp, you can compute 64 additions in one cycle. Now have
    each thread store its value into shared memory. Thus, the first warp stores to
    element 0..31, the second to 32..63, etc. Now divide *N*, the number of elements
    of the reduction, by 2\. Repeat the reduction using the threshold `if (threadIdx.x
    < (N/2))` until such time as *N* equals 2.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是让每个块中的所有 32 个线程计算一组部分结果。我们以求和操作为例，因为它容易理解。每个 warp 32 个线程，你可以在一个周期内计算 64
    次加法。然后让每个线程将其值存储到共享内存中。这样，第一组 warp 将数据存储到 0..31 元素，第二组存储到 32..63，依此类推。现在将 *N*（归约元素的数量）除以
    2。使用阈值 `if (threadIdx.x < (N/2))` 重复归约操作，直到 *N* 等于 2。
- en: Threads 0..255 read values 0..511 (eight active warps).
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 线程 0..255 读取值 0..511（八个活跃的 warps）。
- en: Threads 0..127 read values 0..256 (four active warps).
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 线程 0..127 读取值 0..256（四个活跃的 warps）。
- en: Threads 0..63 read values 0..127 (two active warps).
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 线程 0..63 读取值 0..127（两个活跃的 warps）。
- en: Threads 0..31 read values 0..63 (one active warp).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 线程0..31读取值0..63（一个激活的warp）。
- en: Threads 0..15 read values 0..31 (half an active warp).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 线程0..15读取值0..31（一个激活的warp的一半）。
- en: Etc.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。
- en: Warps with thread numbers greater than the threshold simply no longer get scheduled.
    The warps with values less than *N* are fully populated with work, until such
    time as *N* equals some value less than 32\. At this point we can simply do an
    addition or all remaining elements, or continue to iterate toward the final addition.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 线程数超过阈值的warp将不再被调度。线程数小于*N*的warp将被完全填充工作，直到*N*等于小于32的某个值。在这一点上，我们可以简单地对所有剩余元素进行加法操作，或者继续迭代直到最终的加法。
- en: Inactive warps are not in themselves free either. Although the SM internally
    cares about warps, not blocks, the external scheduler can *only schedule blocks*
    into an SM, not warps. Thus, if each block contains only one active warp, we can
    have as little as 6 to 8 warps for the SM to select from for scheduling. Usually
    we’d have up to 64 warps active in an SM, depending on the compute version and
    resource usage. This is a problem because the thread-level parallelism (TLP) model
    relies on having lots of threads to hide memory and instruction latency. As the
    number of active warps drops, the ability of the SM to hide latency using TLP
    also dramatically drops. As some point this will hurt the performance, especially
    if the warp is still making global memory accesses.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 非激活的warp本身也不是免费的。尽管SM内部关心的是warp，而不是块，外部调度器*只能调度块*进入SM，而不能调度warp。因此，如果每个块只包含一个激活的warp，我们可以从SM中选择的warp数量少至6到8个。通常，SM中最多会有64个warp处于激活状态，具体取决于计算版本和资源使用情况。这是一个问题，因为线程级并行性（TLP）模型依赖于大量线程来隐藏内存和指令延迟。随着激活warp数量的减少，SM利用TLP隐藏延迟的能力也会急剧下降。在某些情况下，这会影响性能，尤其是当warp仍在进行全局内存访问时。
- en: Therefore, at the last levels of such a reduction-type operation, or any operation
    where progressively larger numbers of warps will drop out, we need to introduce
    some instruction level parallelism (ILP). We want to terminate the last warp as
    soon as possible so the entire block can be retired and replaced with another
    block that will likely have a fresh set of active warps.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种归约操作的最后阶段，或者任何逐渐减少warp数量的操作中，我们需要引入一些指令级并行性（ILP）。我们希望尽快终止最后一个warp，以便整个块可以被退役，并替换为另一个可能包含新一组激活warp的块。
- en: We look at reduction in detail later in this chapter.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后详细讨论归约操作。
- en: Arithmetic density
  id: totrans-553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算术密度
- en: Arithmetic density is a term that measures the relative number of calculations
    per memory fetch. Thus, a kernel that fetches two values from memory, multiplies
    them, and stores the result back to memory has very low arithmetic density.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 算术密度是一个衡量每次内存获取的相对计算数量的术语。因此，一个从内存中获取两个值、相乘并将结果存回内存的内核，算术密度非常低。
- en: '[PRE91]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: The fetch and store operations may well involve some index calculations. The
    real work being done is the multiplication. However, with only one operation being
    performed per three memory transactions (two reads and one write), the kernel
    is very much memory bound.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 取值和存储操作可能涉及一些索引计算。实际执行的工作是乘法运算。然而，每三次内存事务（两次读取和一次写入）只执行一次操作，内核在很大程度上受到内存限制。
- en: The total execution time is
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 总执行时间是
- en: '![image](../images/F000090si1.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090si1.png)'
- en: or
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '![image](../images/F000090si2.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090si2.png)'
- en: Notice we use here *A* + *B* as opposed to multiplying *A*, the single memory
    fetch time, by 2\. The individual read times are not easy to predict. In fact
    neither *A*, *B*, or *C* are constant, as they are affected by the loads other
    SMs are making on the memory subsystem. Fetching of *A* may also bring into the
    cache *B*, so the access time for *B* is considerably less than *A*. Writing *C*
    may evict from the cache *A* or *B*. Changes to the resident lines in the L2 cache
    may be the result of the activity of an entirely different SM. Thus, we can see
    caching makes timing very unpredictable.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在此使用 *A* + *B*，而不是将 *A*（单次内存获取时间）乘以2。单独的读取时间不容易预测。事实上，*A*、*B* 或 *C* 都不是常量，因为它们会受到其他SM对内存子系统负载的影响。获取
    *A* 可能还会将 *B* 引入缓存，因此 *B* 的访问时间远小于 *A*。写入 *C* 可能会将 *A* 或 *B* 从缓存中驱逐。L2缓存中驻留行的变化可能是完全不同SM的活动结果。因此，我们可以看到，缓存使得时序变得非常不可预测。
- en: When looking at the arithmetic density, our goal is to increase the ratio of
    useful work done relative to memory fetches and other overhead operations. However,
    we have to consider what we define as a memory fetch. Clearly, a fetch from global
    memory would qualify for this, but what about a shared memory, or cache fetch?
    As the processor must physically move data from shared memory to a register to
    operate on it, we must consider this also as a memory operation. If the data comes
    from the L1, L2, or constant cache, it too has to be moved to a register before
    we can operate on it.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看算术密度时，我们的目标是增加相对于内存获取和其他开销操作的有效工作量比例。然而，我们必须考虑我们定义的内存获取。显然，从全局内存获取应视为一次内存操作，但如果是从共享内存或缓存获取呢？由于处理器必须将数据从共享内存移动到寄存器才能进行操作，因此我们必须将此也视为一次内存操作。如果数据来自L1、L2或常量缓存，则它也必须先移动到寄存器，然后才能进行操作。
- en: However, in the case of a shared memory or L1 cache access, the cost of such
    operations is reduced by an order of magnitude compared to global memory accesses.
    Thus, a global memory fetch should be weighted at 10× if a shared memory fetch
    equates to 1×.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在共享内存或L1缓存访问的情况下，与全局内存访问相比，这类操作的成本降低了一个数量级。因此，如果共享内存访问为1×，则全局内存获取应加权为10×。
- en: 'So how do we increase the arithmetic density of such instruction flows? First,
    we have to understand the underlying instruction set. The maximum operand size
    of an instruction is 128 bytes, a four-element vector load/store operation. This
    tells us the ideal chunk size for our data is four elements, assuming we’re using
    floats or integers, two if we’re using doubles. Thus, our operation should be
    in the first instance:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何提高这种指令流的算术密度呢？首先，我们需要理解底层的指令集。指令的最大操作数大小为128字节，即一个四元素的向量加载/存储操作。这告诉我们理想的数据块大小是四个元素，假设我们使用浮点数或整数，如果使用双精度浮点数则为两个。因此，我们的操作应该在第一次实例化时：
- en: '[PRE92]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: I’ve written this in long-hand form to make the operations clear. If you extend
    the vector-type class yourself and provide a multiplication operator that performs
    this expanded code, you can simply write
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我写下了这个长格式的代码，以便使操作更清晰。如果你自己扩展了向量类型类，并提供了执行这些扩展代码的乘法运算符，你可以直接写出
- en: '[PRE93]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Unfortunately, the GPU hardware currently doesn’t support such vector manipulations,
    only loads, stores, moves, and pack/unpack from scalar types.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当前GPU硬件并不支持此类向量操作，只支持加载、存储、移动以及标量类型的打包/解包。
- en: With such vector-based operations, we amortize the cost of the associated operations
    (load `A`, load `B`, write `C`, calculate `idx_A`, calculate `idx_B`, calculate
    `idx_C`) over four multiplies instead of one. The load and store operations take
    marginally longer as we have to introduce a pack and unpack operation that was
    not needed when accessing scalar parameters. We reduce the loop iterations by
    a factor of four with a consequential drop in the number of memory requests, issuing
    a much smaller number of larger requests to the memory system. This vastly improves
    performance (~20%), as we have seen with some examples in this book.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种基于向量的操作，我们可以将相关操作（加载`A`、加载`B`、写入`C`、计算`idx_A`、计算`idx_B`、计算`idx_C`）的成本摊销到四次乘法中，而不是一次。由于我们需要引入一个打包和解包操作，而在访问标量参数时不需要此操作，因此加载和存储操作会稍微慢一些。我们通过减少循环迭代次数四倍，从而显著减少了内存请求的数量，发出更少的、更大的请求到内存系统。这大大提高了性能（约20%），正如本书中的一些示例所展示的那样。
- en: Transcendental operations
  id: totrans-570
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超越运算
- en: 'The GPU hardware is aimed at speeding up gaming environments. Often these require
    the manipulation of hundreds of thousands of polygons, modeling the real world
    in some way. There are certain accelerators built into the GPU hardware. These
    are dedicated sections of hardware designed for a single purpose. GPUs have the
    following such accelerators:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: GPU硬件的目标是加速游戏环境。通常，这些环境需要处理成千上万的多边形，以某种方式模拟现实世界。GPU硬件中内置了某些加速器。这些加速器是专门为单一目的设计的硬件部分。GPU具备以下这些加速器：
- en: • Division
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: • 除法
- en: • Square root
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: • 平方根
- en: • Reciprocal square root
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: • 倒平方根
- en: • Sine
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: • 正弦
- en: • Cosine
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: • 余弦
- en: • Log²
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: • 对数²
- en: • Base 2 exponent Ex²
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: • 2为底的指数Ex²
- en: These various instructions perform operations to 24-bit accuracy, in line with
    the typical 24-bit RGB setup used in many game environments. None of these operations
    are enabled by default. Compute 1.x devices take various shortcuts that make single-precision
    math not IEEE 754 compliant. These will not be relevant to many applications,
    but be aware they are there. Fermi (compute 2.x) hardware brings IEEE compliance
    with regard to floating-point operations by default.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的指令执行的是24位精度的操作，符合许多游戏环境中常用的24位RGB设置。默认情况下，这些操作并未启用。Compute 1.x设备采取了多种捷径，使得单精度数学运算不符合IEEE
    754标准。这些对于许多应用程序可能无关紧要，但请注意它们的存在。Fermi（Compute 2.x）硬件默认带有符合IEEE标准的浮点运算。
- en: If you’d like the faster but less precise operation, you have to enable them
    using either the compile switch (`-use_fast_math`) or explicitly using intrinsic
    operations. The first step is simply to enable the option in the compiler and
    check the outcome of your existing application. The answer will be different,
    but by how much and how important this is, are the key questions. In the gaming
    industry it doesn’t matter if the flying globe projectile is one pixel off to
    the left or right of the target—no one will notice. In compute applications it
    can make a very real difference.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望更快但精度较低的操作，你需要通过编译开关（`-use_fast_math`）或显式使用内置操作来启用它们。第一步仅仅是启用编译器中的选项，并检查现有应用程序的结果。答案会有所不同，但差异有多大以及这个差异的重要性是关键问题。在游戏行业中，如果飞行中的球形投射物偏离目标左或右一个像素，没人会注意到。但在计算应用中，这个差异可能会产生非常实际的影响。
- en: Individual operations can also be selectively enabled in 24-bit math using an
    explicit compiler intrinsic such as `__logf(x)`, etc. For a complete list of these
    and an explanation of the drawbacks of using them, see Appendix C.2 of the CUDA
    C programming guide. They can considerably speed up your kernels so it’s worth
    investigating if this is an option for your particular code.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用显式的编译器内置函数，如`__logf(x)`等，选择性地启用24位数学中的单个操作。有关这些操作的完整列表以及使用它们的缺点的解释，请参见CUDA
    C编程指南的附录C.2。它们可以显著加速你的内核，因此如果这是你特定代码的一个可行选项，值得进一步调查。
- en: Approximation
  id: totrans-582
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 近似值
- en: Approximation is a useful technique in problems that explore a certain search
    space. Double-precision math is particularly expensive, in the order of at least
    twice as slow as floating-point math. Single-precision math uses 24 bits for the
    mantissa and 8 bits for the exponent. Thus, in the compute 1.x devices a fast
    24-bit integer approximation could be used to provide an additional computation
    path to the single- and double-precision math. Note in Fermi, the 24-bit native
    integer support was replaced with 32-bit integer support, so an integer approximation
    in 24-bit math is actually slower than if the same approximation was made in 32-bit
    math.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 近似是一种在探索特定搜索空间的问题中非常有用的技术。双精度运算尤其昂贵，速度至少是浮点运算的两倍慢。单精度运算使用24位来表示尾数，8位表示指数。因此，在计算1.x设备中，可以使用快速的24位整数近似来为单精度和双精度运算提供额外的计算路径。需要注意的是，在Fermi架构中，24位本地整数支持已被32位整数支持取代，因此在24位数学运算中进行整数近似实际上比在32位数学运算中进行相同的近似要慢。
- en: In all compute hardware versions that natively support double precision (compute
    onwards), approximation in single precision is at least twice the speed of double-precision
    math. Sometimes a much higher speedup can be achieved because the single-precision
    calculations require less registers and thus potentially more blocks can be loaded
    into the hardware. Memory fetches are also half the size, doubling the effective
    per-element memory bandwidth. Consumer-based GPUs also have less double-precision
    units enabled in the hardware than their Tesla counterparts, making single-precision
    approximation a far more attractive proposition for such hardware.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有原生支持双精度的计算硬件版本中（从计算开始），单精度近似的速度至少是双精度数学运算的两倍。有时可以实现更高的加速，因为单精度计算需要的寄存器较少，因此可以加载更多的块到硬件中。内存获取的大小也只有原来的一半，双倍有效的每元素内存带宽。消费级GPU的双精度单元启用比Tesla系列少，这使得在这类硬件上，单精度近似成为一个更具吸引力的选择。
- en: Clearly, with approximating you are performing a tradeoff between speed and
    accuracy and introducing additional complexity into the program. Often this is
    a tradeoff worth exploring, for it can bring a significant speedup.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，通过近似，你是在速度和准确性之间做出权衡，同时引入了额外的复杂性。通常，这是一个值得探索的权衡，因为它可以带来显著的加速。
- en: Once we have done the approximation, the kernel can test the result to see if
    it is within a certain range or meets some criteria by which further analysis
    is warranted. For this subset of the dataset, the single- or double-precision
    calculation is performed as necessary.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了近似，内核可以测试结果，看看它是否在某个范围内，或者是否满足某些标准，从而判断是否需要进一步分析。对于这部分数据集，将根据需要执行单精度或双精度计算。
- en: The initial pass simply acts as a filter on the data. For every data point that
    falls outside the criteria of interest, you have saved the expensive double-precision
    calculations. For every point that falls into it, you have added an additional
    24- or 32-bit filtering calculation. Thus, the benefit of this approach depends
    on the relative cost of the additional filtering calculation versus the cost of
    double-precision math required for the full calculation. If the filters remove
    90% of the double-precision calculations, you have a huge speedup. However, if
    90% of the calculations require a further double-precision calculation, then this
    strategy is not useful.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 初步的筛选仅仅充当数据的过滤器。对于每一个不符合兴趣标准的数据点，你已经避免了昂贵的双精度计算。对于每一个符合标准的数据点，你增加了一个额外的24位或32位过滤计算。因此，这种方法的好处取决于额外过滤计算的成本与全计算所需双精度数学运算的成本之间的相对关系。如果过滤器去除了90%的双精度计算，你就能获得巨大的加速。然而，如果90%的计算仍然需要进一步的双精度计算，那么这个策略就没有什么用处。
- en: NVIDIA claims Tesla Fermi has in the order of 8× faster double-precision math
    over the previous compute 1.3 implementations (GT200 series). However, consumer-level
    Fermi cards are artificially restricted to one-quarter the double-precision performance
    of Tesla cards. Therefore, if double precision is key to your application, clearly
    a Tesla is the easy-fix solution to the problem. However, some may prefer the
    alternative of using multiple consumer GPUs. Two 3 GB 580 GTXs would likely provide
    a faster solution than a single Fermi Tesla for considerably less money.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA声称Tesla Fermi的双精度计算比之前的Compute 1.3版本（GT200系列）快大约8倍。然而，消费级Fermi卡的双精度性能被人为限制为Tesla卡的四分之一。因此，如果双精度对你的应用至关重要，显然Tesla是解决问题的简便方案。然而，有些人可能更倾向于使用多个消费级GPU。两张3
    GB的580 GTX显然会比一张Fermi Tesla提供更快的解决方案，而且成本要低得多。
- en: If double precision is secondary or you simply wish to prototype a solution
    on commonly available hardware, then single precision of 24-bit filtered may be
    an attractive solution to this issue. Alternatively, if you have a mixture of
    GPUs, with an older card that is still good for single-precision usage, you can
    use the older card to scan the problem space for interesting sections, and the
    second card to investigate problem space in detail based on the likely candidates
    from the first card’s quick evaluation. Of course with a suitable Tesla card,
    you can perform both passes with just a single card.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 如果双精度不是关键，或者你只是想在常见硬件上快速原型一个解决方案，那么24位单精度的过滤可能是一个有吸引力的解决方案。或者，如果你有不同型号的GPU，配有一张仍适合单精度使用的老卡，你可以使用老卡扫描问题空间中的有趣部分，再用第二张卡基于第一张卡的快速评估来详细探查问题空间。当然，如果你有一张合适的Tesla卡，你可以只用一张卡完成两次计算。
- en: Lookup tables
  id: totrans-590
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查找表
- en: One common optimization technique used for complex algorithms is a lookup table.
    On CPUs where computation is quite expensive, these generally work reasonably
    well. The principle is that you calculate a number of representative points in
    the data space. You then apply an interpolation method between points based on
    the proportional distance to either edge point. This is typically used in modeling
    of the real world in that a linear interpolation method with a sufficient number
    of key sample points provides a good approximation of the actual signal.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的优化技术是查找表。对于计算代价较高的CPU，这些通常能合理地工作。其原理是，先计算出数据空间中的若干代表性点，然后根据与任一边界点的比例距离应用插值方法。这通常用于对现实世界的建模，因为具有足够数量关键样本点的线性插值方法能够很好地近似实际信号。
- en: A variation on this technique is used in brute-force attacks on ciphers. Passwords
    on most systems are stored as hashes, an apparently unintelligible series of digits.
    Hashes are designed so that it’s difficult to calculate the password from the
    hash by reversing the calculation. Otherwise, it would be trivial to calculate
    the original password based on a compromised hash table.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的变种被用于对密码进行暴力破解攻击。大多数系统上的密码以哈希值的形式存储，哈希值看似是一串无法理解的数字。哈希值的设计目的是使得从哈希值反向计算出密码变得困难。否则，如果能够通过已泄露的哈希表轻松计算出原始密码，那就毫无意义了。
- en: One method of attack on this type of system involves a CPU spending a considerable
    time generating all possible permutations based on the use of common and/or short
    passwords. The attacker then simply matches the precomputer hash against the target
    hash until such time as a match is made.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 一种针对这种系统的攻击方法是让CPU花费相当长的时间基于常见的和/或较短的密码生成所有可能的排列。攻击者然后简单地将预计算的哈希与目标哈希进行匹配，直到找到匹配项。
- en: In both cases, the lookup table method trades memory space for compute time.
    By simply storing the result, you have instant access to the answer. Many people
    will have learned multiplication tables in their heads as children. It’s the same
    principle; instead of tediously calculating *a* × *b*, for the most common set
    of values, we simply memorize the result.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，查找表方法通过用内存空间换取计算时间。只需存储结果，就能即时访问答案。许多人小时候都会在脑海里记住乘法表。原理是一样的；我们不再为最常见的数值集反复计算
    *a* × *b*，而是直接记住结果。
- en: This optimization technique works well on CPUs, especially older ones, where
    the compute time may be significant. However, as the compute resources have become
    faster and faster, it can be cheaper to calculate the results than to look them
    up from memory.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化技术在CPU上效果很好，特别是在较老的CPU上，因为那时计算时间可能相对较长。然而，随着计算资源越来越强大，计算结果的成本反而比从内存中查找结果更低。
- en: If you consider the average arithmetic instruction latency will be between 18
    to 24 cycles and the average memory fetch in the order of 400 to 600 cycles, you
    can clearly see we can do a lot of calculation work in the time it takes for the
    memory fetch to come back from global memory. This, however, assumes we have to
    go out to global memory for the result and that it’s not stored in shared memory
    or the cache. It also does not consider that the GPU, unlike the CPU, will not
    idle during this memory fetch time. In fact, the GPU will likely have switched
    to another thread and be performing some other operation. This, of course, depends
    on the number of available warps you have scheduled onto the device.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑到平均算术指令延迟在18到24个周期之间，而平均内存获取大约需要400到600个周期，你可以清楚地看到，在内存从全局内存返回的时间里，我们可以进行大量的计算工作。然而，这假设了我们需要从全局内存中获取结果，而不是存储在共享内存或缓存中。它也没有考虑到与CPU不同，GPU在内存获取期间不会处于空闲状态。事实上，GPU可能已经切换到另一个线程并执行其他操作。当然，这取决于你已调度到设备上的可用warp数量。
- en: In many cases the lookup may win over the calculation, especially where you
    are achieving a high level of GPU utilization. Where you have low utilization,
    the calculation method often wins out, depending of course on how complex the
    calculation really is. Let’s assume we have 20-cycle instruction latency for arithmetic
    operations and 600-cycle latency for memory operations. Clearly, if the calculation
    takes less than 30 operations it would be much faster than lookup in memory when
    we have low GPU utilization. In this case the SM is behaving like a serial processor,
    in that it has to wait for the memory fetch. With a reasonable utilization the
    memory fetch effectively becomes free, as the SM is simply executing other warps.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，查找操作可能会比计算操作更优，特别是在你达到较高的GPU利用率时。在低利用率的情况下，计算方法通常会更优，这当然取决于计算的复杂程度。假设算术操作的指令延迟是20个周期，而内存操作的延迟是600个周期。显然，如果计算需要的操作少于30次，那么在GPU利用率较低时，计算会比内存查找要快。在这种情况下，SM表现得像一个串行处理器，因为它必须等待内存获取。合理的利用率下，内存获取实际上是免费的，因为SM只是执行其他warp。
- en: It’s often a case of trying this and seeing how well it works. Also be prepared
    to take it back out again should you suddenly manage to increase utilization of
    the GPU through other means.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是一个尝试并观察效果的过程。同时，要准备好在通过其他手段突然提高GPU利用率时，将其移除。
- en: Note, in the case of linear interpolation, a low-precision floating point–based
    linear interpolation is available in the GPU hardware. This is a feature of the
    texture memory hardware, something we do not cover in this text. Texture memory
    was useful for its cache features (24 K per SM) in compute 1.x hardware, but this
    use has largely been made redundant by the L1/L2 cache introduced in Fermi. However,
    the linear interpolation in hardware may still be useful for some problems. See
    the “Texture and Surface Memory” chapter of the CUDA programming guide if this
    is of interest to you.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在线性插值的情况下，GPU 硬件中提供了基于低精度浮点数的线性插值。这是纹理内存硬件的一个特性，本文未涉及此内容。纹理内存在计算 1.x 硬件中因其缓存特性（每个
    SM 24K）而非常有用，但这一功能在 Fermi 中引入的 L1/L2 缓存中基本上被取代了。然而，硬件中的线性插值仍然可能对某些问题有用。如果你对此感兴趣，请参阅
    CUDA 编程指南中的“纹理和表面内存”章节。
- en: Some common compiler optimizations
  id: totrans-600
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些常见的编译器优化
- en: We’ll take a quick look at some compiler optimizations and how they affect GPUs.
    We cover these here to highlight cases where the optimizer may struggle and also
    to give you some understanding of how optimizations may be applied at the source
    code level where the automatic optimizations fail.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速浏览一些编译器优化以及它们如何影响 GPU。我们在这里讨论这些内容，旨在突出优化器可能会遇到困难的情况，同时也让你了解当自动优化失败时，优化如何在源代码层面应用。
- en: Some compilers are well known for producing efficient code on certain targets.
    Not surprisingly, the Intel ICC compiler produces extremely efficient code for
    the Intel platform. New features of the processor are incorporated rapidly to
    showcase the technology. Mainstream compilers often come from a code base that
    supports many targets. This allows for more efficient development, but means the
    compiler may not be so easy to customize for a single target.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 一些编译器因在某些目标上生成高效代码而广为人知。不出所料，Intel ICC 编译器为 Intel 平台生成的代码非常高效。处理器的新特性会迅速被融入进来，以展示这项技术。主流编译器通常来自一个支持多种目标的代码库，这使得开发更高效，但也意味着编译器可能不太容易为单一目标进行定制。
- en: As of the 4.1 SDK CUDA moved from using an Open64-based compiler to a more modern
    LLVM-based compiler. The most significant benefit from the user perspective is
    significantly faster compile times. NVIDIA also claims a 10% improvement in code
    speed. We saw noticeable improvements in code generation with this move. However,
    as with any new technology, there is room for improvement and I’m sure this will
    happen over time.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 从 4.1 SDK 开始，CUDA 从使用基于 Open64 的编译器转向了更现代的基于 LLVM 的编译器。从用户的角度来看，最显著的好处是编译时间大大缩短。NVIDIA
    还宣称代码速度提高了 10%。我们在代码生成方面确实看到了明显的改进。不过，像所有新技术一样，仍有改进的空间，我相信随着时间的推移，这些改进会逐步实现。
- en: The optimizations compilers apply are well documented. What we present here
    is a broad overview of some common ones. For most programmers, simply setting
    the optimization level is entirely sufficient. Others prefer to know what exactly
    is going on and check the output. This is of course a tradeoff of your programming
    time versus the potential gain and the relative costs of these.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器应用的优化有详细的文档记录。我们在这里展示的是一些常见优化的广泛概述。对于大多数程序员来说，仅仅设置优化级别就足够了。其他人则更喜欢了解具体发生了什么并检查输出。这当然是编程时间与潜在收益及相对成本之间的权衡。
- en: Strength reduction
  id: totrans-605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强度降低
- en: When accessing an array index, typically nonoptimized compiler code will use
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 当访问数组索引时，通常未优化的编译器代码将使用
- en: '[PRE94]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: This can be more efficiently replaced by one of two techniques. First, we must
    load the array base address (element 0) into a base register. Then we have the
    option of accessing an index as base + offset. We can also simply add a fixed
    offset, the size of an array element in bytes, to the base register after each
    loop iteration.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过两种技术中的一种更有效地替代。首先，我们必须将数组基地址（元素0）加载到基寄存器中。然后我们可以选择将索引作为基+偏移量进行访问。我们还可以在每次循环迭代后，简单地将数组元素的字节大小作为固定偏移量添加到基寄存器中。
- en: In terms of C this is the same as writing
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 在C语言中，这与写作是一样的
- en: '[PRE95]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: In terms of GPU usage, this optimization relies on the fact that certain instructions
    (multiply, divide) are computationally expensive and others (addition) are cheaper.
    It tries to replace the expensive operations with cheaper (or faster) ones. This
    technique works well on CPUs as well as on GPUs. This is especially the case with
    compute 2.1 devices where integer addition has three times the throughput of integer
    multiplication.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU使用方面，这种优化依赖于某些指令（乘法、除法）计算成本高，而其他指令（加法）成本较低。它试图用更便宜（或更快）的操作替换昂贵的操作。这种技术在CPU和GPU上都能很好地工作。尤其是在计算能力为2.1的设备上，整数加法的吞吐量是整数乘法的三倍。
- en: Notice also that the pointer version of the code creates a dependency between
    loop iterations. The value of `ptr` must be known to execute the assignment. The
    first example is much easier to parallelize because there is no dependency on
    the loop iteration and the address of `a[i]` can easily be statically calculated.
    In fact, simply adding the `#pragma unroll` directive would have caused the compiler
    to unroll the entire loop, as the boundaries in this simple example are literals.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，指针版本的代码在循环迭代之间创建了依赖关系。执行赋值时必须知道`ptr`的值。第一个示例更容易并行化，因为它不依赖于循环迭代，并且`a[i]`的地址可以轻松静态计算。实际上，仅仅添加`#pragma
    unroll`指令就会导致编译器展开整个循环，因为在这个简单示例中的边界是字面量。
- en: It’s a typical example of a CPU-based optimization that may have been applied
    and to parallelize the loop you need to reverse-engineer back to the original
    code. It’s shown here because it helps you understand how C code may have been
    changed in the past to provide faster execution time for a given target. Like
    most optimizations at the C source code level, it can lead to the purpose of the
    source code being obscured.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个典型的基于CPU的优化示例，可能已被应用，并且为了并行化该循环，你需要逆向工程回到原始代码。之所以在此展示，是因为它有助于你理解C代码如何在过去被修改，以便为给定目标提供更快的执行时间。像大多数C源代码级别的优化一样，它可能会导致源代码的原意被掩盖。
- en: Loop invariant analysis
  id: totrans-619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环不变式分析
- en: Loop invariant analysis looks for expressions that are constant within the loop
    body and moves them outside the loop body. Thus, for example,
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 循环不变式分析寻找循环体内常量表达式，并将其移到循环体外。因此，例如，
- en: '[PRE101]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'In this example, the parameter `j` is constant within the loop body for parameter
    `i`. Thus, the compiler can easily detect this and will move the calculation of
    `b` outside the inner loop and generate the following code:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，参数`j`对于参数`i`来说在循环体内是常量。因此，编译器可以轻松检测到这一点，并将`b`的计算移到内层循环外，并生成如下代码：
- en: '[PRE102]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'This optimized code removes thousands of unnecessary calculations of `b`, where
    `j`, and thus `b`, are constant in the inner loop. However, consider the case
    where `b` is an external to the function, a global variable, instead of a local
    variable. For example:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 这段优化后的代码去除了对`b`的数千次不必要的计算，其中`j`，因此`b`，在内层循环中是常量。然而，考虑到`b`是函数外部的全局变量，而非局部变量的情况。例如：
- en: '[PRE104]'
  id: totrans-626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '`}`'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: The compiler cannot safely make this optimization because the write to `q` may
    affect `b`. That is, the memory space of `q` and `b` may intersect. It cannot
    even safely reuse the result of `j ∗ 200` in the assignment to `q`, but must reload
    it from memory, as the contents of `b` may have changed since the assignment in
    the prior line.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器无法安全地进行此优化，因为对`q`的写入可能会影响`b`。也就是说，`q`和`b`的内存空间可能会交叉。它甚至不能安全地重用`j ∗ 200`在赋值给`q`时的结果，而必须重新从内存加载它，因为自从前一行赋值后，`b`的内容可能已经发生了变化。
- en: If you consider each line individually, then the issue becomes somewhat clearer.
    Any memory transaction, a read or write, will likely cause a switch to another
    warp, if that transaction involves accessing anything that is not immediately
    available. That area of global memory is accessible to any thread in any warp,
    on any active block in any SM. From one instruction to the next you get the very
    real possibility that any writable non register data could have changed.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你单独考虑每一行，那么问题会变得更清晰。任何内存操作，无论是读取还是写入，都会导致可能切换到另一个warp，如果该操作涉及访问任何未立即可用的内容。全局内存的该区域对任何warp中的任何线程都是可访问的，无论它是在任何活动块的任何SM中。从一条指令到下一条指令，你可能会遇到任何可写的非寄存器数据已经发生变化的情况。
- en: You might say, well I’ve split up the application into *N* tiles and the tiles
    do not intersect, so this is not necessary. As the programmer you may know this,
    but it is very difficult for the compiler to figure that out. Consequently, it
    opts for the safe route and does not perform such optimizations. Many programmers
    do not understand what the optimization stage of a compiler does, and thus when
    it does something that breaks the code, they blame the compiler. Consequently,
    compilers tend to be rather conservative in how they optimize code.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会说，嗯，我已经将应用程序拆分成了*N*个瓦片，并且这些瓦片互不重叠，所以不需要这样做。作为程序员，你可能知道这一点，但编译器很难推断出来。因此，编译器选择了安全的做法，并未进行这样的优化。许多程序员不了解编译器优化阶段的作用，因此当它做了一些破坏代码的操作时，他们会责怪编译器。因此，编译器在优化代码时通常非常保守。
- en: As the programmer, understanding this allows you to make such optimization at
    the source level. Remember to think of global memory as you might a slow I/O device.
    Read from it once and reuse the data.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 作为程序员，理解这一点可以让你在源代码级别进行优化。记住，要像对待一个慢速I/O设备那样考虑全局内存。读取一次并重用数据。
- en: Loop unrolling
  id: totrans-633
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环展开
- en: 'Loop unrolling is a technique that seeks to ensure you do a reasonable number
    of data operations for the overhead of running through a loop. Take the following
    code:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 循环展开是一种技术，旨在确保你在执行循环时进行合理数量的数据操作。以下是示例代码：
- en: '[PRE106]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'In terms of assembly code, this will generate:'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 就汇编代码而言，这将生成：
- en: • A load of a register with 0 for parameter `i`.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: • 为参数`i`加载一个值为0的寄存器。
- en: • A test of the register with 100.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: • 对寄存器进行100的测试。
- en: • A branch to either exit or execute the loop.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: • 分支跳转，可能退出或执行循环。
- en: • An increment of the register holding the loop counter.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: • 对持有循环计数器的寄存器进行递增。
- en: • An address calculation of array `q` indexed by `i`.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: • 计算数组`q`的地址，索引为`i`。
- en: • A store of `i` to the calculated address.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: • 将`i`存储到计算得到的地址。
- en: Only the last of these instructions actually does some *real* work. The rest
    of the instructions are overhead.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指令中只有最后一条实际做了一些*真正的*工作。其余的指令是开销。
- en: We can rewrite this C code as
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这段C代码重写为
- en: '[PRE107]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '`}`'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: Thus, the ratio of useful work to overhead of using the loop is much increased.
    However, the size of the C source code is somewhat increased and it’s now less
    obvious what exactly it was doing compared to the first loop.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用循环的有效工作与开销的比例大大增加。然而，C源代码的大小有所增加，相比第一个循环，代码到底在做什么变得不那么明显了。
- en: In terms of PTX code, we see each C statement translated into PTX. For every
    branch test, there are now four memory copy operations. Thus, the GPU is executing
    more instructions than before, but a higher percentage of the memory copy operations
    are doing useful work.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 就PTX代码而言，我们看到每个C语句都被翻译成了PTX代码。对于每个分支测试，现在有四个内存拷贝操作。因此，GPU执行的指令比以前更多，但更多的内存拷贝操作在进行有效的工作。
- en: In the CPU domain often there are limited registers, so the same registers will
    be reused in each step. This reduces register overhead, but means `q[i+1]` cannot
    start processing until `q[i]` has completed. We’d see the same overhead on the
    GPU with this approach. Each instruction has 20 cycles of latency. Therefore,
    the GPU assigns each address calculation to a separate register, so we have a
    set of four parallel instructions, rather than four sequential instructions executing.
    Each set is pushed into the pipelines and thus comes out one after another almost
    back to back.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU领域，通常寄存器有限，因此相同的寄存器会在每一步中被重复使用。这可以减少寄存器开销，但意味着`q[i+1]`在`q[i]`完成之前不能开始处理。采用这种方法，GPU也会遇到相同的开销。每条指令有20个周期的延迟。因此，GPU将每个地址计算分配给一个单独的寄存器，从而使我们有一组四个并行指令，而不是四个顺序执行的指令。每组指令被推入流水线，几乎是一个接一个地顺序输出。
- en: With this approach the limit is the number of registers. As the GPU has 64 (compute
    2.x,3.0) and 128 (compute 1.x) maximum, there is considerable scope for unrolling
    small loop bodies and achieving a good speedup.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法的限制是寄存器的数量。由于GPU最大有64个（计算2.x，3.0）和128个（计算1.x），因此有相当大的空间来展开小循环体并实现较好的加速。
- en: The NVCC compiler supports the `#pragma unroll` directive, which will automatically
    unroll fully such loops when the iteration count is constant or silently do nothing
    when it’s not. The latter is less than helpful, if the programmer has specified
    the loop should be unrolled. If the compiler is not able to, it should complain
    about this until the code is amended or the pragma removed.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: NVCC编译器支持`#pragma unroll`指令，当迭代次数为常数时，它会自动展开这些循环，或者当迭代次数不是常数时，什么也不做。如果程序员指定了循环应该展开，而编译器无法展开时，它应该给出警告，直到代码被修改或去除该指令。
- en: You can also specify `#pragma unroll 4` where four is replaced by any number
    the programmer wishes. Typically four or eight will work well, but beyond that
    too many registers will be used and this will result in register spilling. On
    compute 1.x hardware, this will cause a huge performance drop as registers are
    spilled to global memory. From compute 2.x hardware onwards, registers are spilled
    to the L1 cache and then to global memory if necessary. The best solution is to
    try it and see which value works best for each loop.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以指定`#pragma unroll 4`，其中4可以替换为程序员希望的任何数字。通常，四或八会表现良好，但超过这个数，过多的寄存器将被使用，这会导致寄存器溢出。在计算1.x硬件上，这会导致性能急剧下降，因为寄存器会溢出到全局内存。从计算2.x硬件开始，寄存器会先溢出到L1缓存，然后如果需要，再溢出到全局内存。最好的解决方案是尝试不同的值，看看哪一个在每个循环中效果最佳。
- en: Loop peeling
  id: totrans-653
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环剥离
- en: Loop peeling is an enhancement to the loop unrolling, when the number of iterations
    is not an exact multiple of the loop unrolling size. Here the last few iterations
    are peeled away and done separately, and then the main body of the loop is unrolled.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 循环剥离是对循环展开的增强，适用于迭代次数不是循环展开大小的精确倍数的情况。在这种情况下，最后几个迭代会被剥离出来单独处理，然后再展开主循环体。
- en: For example, if we have 101 loop iterations and plan to use four levels of loop
    unrolling, the first 100 iterations of the loop are unrolled and the final iteration
    is peeled away to allow the bulk of the code to operate on the unrolled code.
    The final few iterations are then handled as either a loop or explicitly.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，如果我们有101次循环迭代，并计划使用四级循环展开，那么循环的前100次迭代会被展开，最后一次迭代则被剥离出来，以便代码的大部分可以在展开后的代码上运行。最后几个迭代会作为循环或者显式地处理。
- en: Loop peeling can be equally applied to the start of a loop as to the end. It
    can be used in such cases to allow a nonaligned structure to be accessed as an
    aligned structure. For example, copying a byte-aligned memory section to another
    byte-aligned memory is slow because it has to be done one byte at a time. The
    first few iterations can be peeled away such that a 32-, 64-, or 128-byte alignment
    is achieved. Then the loop can switch to much faster word, double-, or quad-word
    based copies. At the end of the loop the byte-based copies can be used again.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 循环剥离同样可以应用于循环的开始部分，就像应用于结束部分一样。在这种情况下，它可以被用来允许非对齐的结构像对齐的结构那样被访问。例如，将一个字节对齐的内存区域复制到另一个字节对齐的内存区域很慢，因为它必须逐字节进行。可以剥离掉前几个迭代，使得32、64或128字节的对齐得到实现。然后，循环可以切换到基于字、双字或四字的更快复制方式。在循环结束时，可以再次使用基于字节的复制方式。
- en: When using the `#pragma loop unroll N` directive, the compiler will unroll the
    loop such that the number of iterations does not exceed the loop boundaries and
    insert the end of loop peeling code automatically.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`#pragma loop unroll N`指令时，编译器会展开循环，使得迭代次数不超过循环边界，并自动插入循环结束剥离代码。
- en: Peephole optimization
  id: totrans-658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 皮眼优化
- en: This optimization simple searches for combinations of instructions that can
    be replaced by more complex instructions with the same functionality. The classic
    example of this is multiply followed by an add instruction, as you might see in
    a gain and offset type calculation. This type of construct can be replaced with
    the more complex `madd` (multiply and add) instruction, reducing the number of
    instructions from two to one.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化简单地搜索可以被更复杂的指令替代的指令组合，而这些指令具有相同的功能。经典的例子是先乘法后加法指令，就像你在增益和偏移类型的计算中可能会看到的那样。这种结构可以用更复杂的`madd`（乘法加法）指令替代，将指令数量从两个减少到一个。
- en: Other types of peephole optimizations include simplification of flow of control,
    algebraic simplifications, and removal of unreachable code.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的皮眼优化包括简化控制流、代数简化和移除不可达代码。
- en: Common subexpressions and folding
  id: totrans-661
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 公共子表达式和折叠
- en: Many programmers write code that repeats some operation, for example,
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 许多程序员编写的代码会重复某些操作，例如，
- en: '[PRE108]'
  id: totrans-663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: or
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE109]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: In the first example, arrays `b` and `c` are indexed by the `base` and `i` parameters.
    Providing these parameters are within local scope, the compiler can simply calculate
    the index (`base + i`), and add this value to the start address of arrays `b`
    and `c` and to the work address for each parameter. However, if either of the
    index parameters are global variables, then the calculation must be repeated,
    since either could have changed once multiple threads are used. With a single
    thread it would be safe to eliminate the second calculation. With multiple threads
    it may also be safe to do so, but the compiler doesn’t know for sure, so will
    typically perform two calculations.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，数组 `b` 和 `c` 是通过 `base` 和 `i` 参数进行索引的。假设这些参数在局部作用域内，编译器可以直接计算索引（`base
    + i`），然后将这个值加到数组 `b` 和 `c` 的起始地址以及每个参数的工作地址上。然而，如果其中任何一个索引参数是全局变量，那么计算就必须重复，因为一旦使用多个线程，任意一个可能已经发生变化。在单线程下，消除第二次计算是安全的。而在多线程情况下，尽管这样做可能也是安全的，但编译器无法确定，因此通常会执行两次计算。
- en: In the second example, the term `NUM_ELEMENTS-1` is repeated. If we assume that
    `NUM_ELEMENTS` is a define, then the preprocessor will substitute the actual value,
    so we get `b[1024-1] ∗ c[1024-1]`. Clearly, 1024 − 1 can in both instances be
    replaced by 1023\. However, if `NUM_ELEMENTS` was actually a formal parameter,
    as it is in many kernel calls, this type of optimization is not available. In
    this case we have to drop back to common subexpression optimization.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，`NUM_ELEMENTS-1` 被重复使用。如果我们假设 `NUM_ELEMENTS` 是一个宏定义，那么预处理器会替换为实际的值，因此我们得到
    `b[1024-1] ∗ c[1024-1]`。显然，1024 − 1 在这两个实例中都可以替换为 1023。然而，如果 `NUM_ELEMENTS` 实际上是一个形式参数，正如许多内核调用中那样，这种优化就不可用。在这种情况下，我们需要回退到公共子表达式优化。
- en: Therefore, be aware that in making such constants parameters of a function,
    or by having such parameters in global memory, you may be limiting the compiler’s
    ability to optimize the code. You then have to ensure such common subexpressions
    are not present in the source code. Often eliminating the common subexpressions
    makes the code simpler to understand and improves the performance.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要注意的是，当将这样的常量作为函数的参数，或将其作为全局内存中的参数时，可能会限制编译器优化代码的能力。此时，你必须确保源代码中没有出现此类公共子表达式。通常，消除公共子表达式会使代码更容易理解，并提高性能。
- en: Divergence
  id: totrans-669
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分歧
- en: GPUs execute code in blocks, or warps. A single instruction is decoded once
    and dispatched to a warp scheduler. There it remains in a queue until the warp
    dispatcher dispatches it to a set of 32 execution units, which execute that instruction.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 执行代码时以块或波束（warp）为单位。单个指令只解码一次，然后分派到波束调度器。在那里，指令会一直排队，直到波束调度器将其分派到 32 个执行单元，后者会执行该指令。
- en: This approach amortizes the instruction fetch and decoding time over *N* execution
    units. This in itself is very similar to the old vector machines. However, the
    main difference is that CUDA does not require that every instruction execute in
    this way. If there is a branch in the code and only some instructions follow this
    branch, those instructions diverge while the others wait at the point of divergence.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将指令获取和解码的时间摊销到 *N* 个执行单元上。这本身与旧的向量机非常相似。然而，主要的区别在于，CUDA并不要求每条指令都以这种方式执行。如果代码中有分支，并且只有部分指令跟随该分支，那么这些指令会发生分歧，而其他指令则在分歧点等待。
- en: The single fetch/decode logic then fetches the instruction stream for the divergent
    threads and the other threads simply ignore it. In effect, each thread within
    the warp has a mask that enables its execution or not. Those threads not following
    the divergence have the mask cleared. Conversely, those following the branch have
    the bit set.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 单次获取/解码逻辑随后会获取分歧线程的指令流，其他线程则直接忽略它。实际上，warp 中的每个线程都有一个掩码，用以决定是否执行该线程。那些没有跟随分歧的线程会清除掩码，反之，跟随分支的线程会设置该位。
- en: This type of arrangement is called predication. A predicate is created, which
    results in a single bit being set for those threads within a warp that follow
    the branch. Most PTX op-codes support an optional predicate allowing selective
    threads to execute an instruction.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 这种安排被称为预处理。一个谓词被创建，它导致在warp中跟随分支的线程设置一个单独的位。大多数PTX操作码支持可选的谓词，允许选择性地执行指令的线程。
- en: 'Thus, for example, consider the following code:'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，举个例子，考虑以下代码：
- en: '[PRE110]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: In the first line of code the program eliminates all other warps in the current
    block except the first warp, the first 32 threads. This does not result in any
    divergence within the warp. The other warps in the block are simply not scheduled
    for this section of the code. They do not stall, but fall through the code and
    continue the execution of subsequent code.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的第一行中，程序会去除当前块中所有其他的warp，保留第一个warp，即前32个线程。这不会导致warp内部出现任何分歧。块中的其他warp仅仅是未被调度执行这一段代码。它们并不会停滞，而是跳过这段代码，继续执行后续的代码。
- en: The first warp then meets a test for `threadIdx.x < 16`, which splits the warp
    exactly in half. This is a special scenario where the warp does not actually diverge.
    Although the warp size is 32, the divergence criteria are actually a half-warp.
    If you noticed earlier, the CUDA cores are arranged in banks of 16 cores, not
    32 cores. The scheduler issues instructions to two or more sets of 16 cores per
    cycle. Thus both the true and false path of the conditional are executed.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个warp会遇到 `threadIdx.x < 16` 的测试条件，这将warp精确地分成两半。这是一个特殊的场景，其中warp并不会实际发生分歧。虽然warp的大小是32，但分歧的标准实际上是半个warp。如果你之前注意到过，CUDA核心是按16个核心为一组排列，而不是32个核心。调度程序每个周期向两个或更多的16核心组发出指令。因此，条件的真路径和假路径都会被执行。
- en: In the subsequent step, threads 16 to 31 call the function `func_b`. However,
    threads 0..15 hit another conditional. This time it’s not half-warp based, but
    quarter-warp based. The minimum scheduling quantity is 16 threads. Thus, the first
    set of eight threads jump off to call function `func_a1` while the second set
    of eight threads (8..15) stall.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的步骤中，线程16到31调用函数`func_b`。然而，线程0..15遇到了另一个条件。这次不是基于半warp，而是基于四分之一warp。最小调度单位是16个线程。因此，第一组八个线程跳到调用函数`func_a1`，而第二组八个线程（8..15）暂停。
- en: Functions `func_b` and `func_a1` will continue to independently fetch instructions
    and dispatch them to the two half-warps. This is somewhat less efficient than
    a single instruction fetch, but nonetheless better than sequential execution.
    Eventually `func_a1` will complete and `func_a2` will start, stalling the threads
    0..7\. In the meantime `func_b` may have also completed. We can write a short
    test program to demonstrate this.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`func_b`和`func_a1`将继续独立地获取指令并将它们分派到两个半warp。这比单一指令获取效率稍低，但仍然比顺序执行要好。最终，`func_a1`将完成，`func_a2`将开始，导致线程0..7暂停。与此同时，`func_b`可能也已经完成。我们可以编写一个简单的测试程序来演示这一点。
- en: '[PRE111]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '`  const u32 ∗ const b,`'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '`  const u32 ∗ const b,`'
- en: '[PRE112]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '`   {`'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '`   {`'
- en: '[PRE118]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '`      }`'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '`      }`'
- en: '[PRE121]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Here we have set up a number of kernels, each of which exhibit different levels
    of divergence. The first is the optimal with no divergence. The second diverges
    based on half-warps. These half-warps should execute in parallel. We then further
    subdivide the first half-warp into two groups. These should execute in series.
    We then subdivide again the first group into a total of four serial execution
    paths. The results we see are as follows:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置了多个内核，每个内核展示了不同级别的分歧。第一个是没有分歧的最优情况。第二个基于半warp发生分歧。这些半warp应该并行执行。接下来，我们将第一个半warp进一步细分为两个组，这些组应该按序列执行。然后，我们再次将第一组细分为总共四个串行执行路径。我们看到的结果如下：
- en: '[PRE122]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: We can see this somewhat better in a graphical format in [Figure 9.32](#F0165).
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图9.32](#F0165)中以图形格式更清楚地看到这一点。
- en: '![image](../images/F000090f09-32-9780124159334.jpg)'
  id: totrans-700
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-32-9780124159334.jpg)'
- en: FIGURE 9.32 How thread divergence affects execution time.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.32 线程分歧如何影响执行时间。
- en: Notice how the thread divergence is not such a significant problem on the compute
    1.x devices (the 9800 GT and GTX260). It has an effect, but takes the maximum
    time to just 145% of the optimal time. By comparison, the Fermi compute 2.x cards
    (GTX460, GTX470) suffer over a 4× slowdown when diverging significantly within
    a warp. The GTX460 seems especially sensitive to warp divergence. Notice the GTX470
    is almost 10× faster in absolute terms than the 9800 GT when there is no divergence,
    which is a massive improvement for just two generations of cards.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，线程分歧在计算1.x设备（9800 GT和GTX260）上并不是一个显著的问题。它确实有影响，但最大时间只会增加到最优时间的145%。相比之下，Fermi计算2.x卡（GTX460，GTX470）在warp内发生较大分歧时会遭遇超过4倍的性能下降。GTX460似乎特别容易受到warp分歧的影响。请注意，在没有分歧的情况下，GTX470在绝对性能上几乎比9800
    GT快了10倍，这对于仅仅两代显卡来说是一个巨大的提升。
- en: If you are curious to know how much a 32-way divergence costs, it leads to a
    27× slowdown on the compute 1.x cards and a massive 125× to 134× slowdown on the
    compute 2.x cards. Note that the code for this test was a simple switch statement
    based on the thread index, so it is not directly comparable to the code we’re
    using here. However, clearly such divergence needs to be avoided at all costs.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道一个32路分歧的代价，计算结果表明，在计算1.x卡上会导致27倍的性能下降，而在计算2.x卡上会出现高达125倍到134倍的性能下降。需要注意的是，这次测试的代码是基于线程索引的简单switch语句，因此无法与我们在这里使用的代码直接进行比较。然而，显然这种分歧必须不惜一切代价避免。
- en: The easiest method of avoiding divergence within a warp is to simply mask out
    the sections of the warp you don’t wish to contribute to the result. How can you
    do this? Just perform the same calculation on every thread in the warp, but select
    a value that does not contribute for the threads you wish to mask out.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 避免warp内分歧的最简单方法是简单地屏蔽掉你不希望参与计算的warp部分。如何做到这一点？只需在warp中的每个线程上执行相同的计算，但对于你希望屏蔽掉的线程，选择一个不会贡献结果的值。
- en: For example, for a `min` operation on 32-bit integers, select 0xFFFFFFFF as
    the value for threads that should not contribute. Conversely for `max`, `sum`,
    and many other arithmetic-type operations, just use 0 in the threads you do not
    wish to contribute. This will usually be much quicker than branching within a
    warp.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于32位整数的`min`操作，为那些不应参与计算的线程选择0xFFFFFFFF作为值。相反，对于`max`、`sum`和许多其他算术操作，只需在不希望参与的线程中使用0。与warp内的分支操作相比，这通常会更快。
- en: Understanding the low-level assembly code
  id: totrans-706
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解低级汇编代码
- en: The GPU compiles code into a virtual assembly system called PTX (Parallel Thread
    eXecution Instruction Set Architecture). This is a lot like Java byte-code in
    that it is a virtual assembly language. This can either be translated at compile
    time or runtime into the real code, which executes on the device. The compile
    time translation simply inserts a number of real binaries into the application,
    depending on which architectures you specify on the command line (the `–arch`
    switch).
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: GPU将代码编译成一种叫做PTX（Parallel Thread eXecution Instruction Set Architecture）的虚拟汇编系统。这与Java字节码非常相似，都是一种虚拟汇编语言。这些代码可以在编译时或运行时翻译成在设备上执行的真实代码。编译时翻译只是根据你在命令行上指定的架构（`–arch`开关）将一些真实的二进制文件插入到应用程序中。
- en: To look at the virtual assembly generated, you simply add the `–keep` flag to
    the compiler command line. For Visual Studio users, the default NVIDIA projects
    contain an option to keep the PTX files (`–keep`) ([Figure 9.33](#F0170)). You
    can also specify the place to store them if you prefer they do not clutter up
    the project directory using the `–keep-dir <directory>` option.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看生成的虚拟汇编代码，你只需要在编译器命令行中添加`–keep`标志。对于Visual Studio用户，默认的NVIDIA项目包含一个选项来保存PTX文件（`–keep`）（[图9.33](#F0170)）。如果你不希望它们占用项目目录的空间，还可以使用`–keep-dir
    <directory>`选项来指定存储位置。
- en: '![image](../images/F000090f09-33-9780124159334.jpg)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-33-9780124159334.jpg)'
- en: FIGURE 9.33 Visual C options—how to keep PTX files.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.33 Visual C选项—如何保存PTX文件。
- en: 'However, PTX is not what is really executed on the hardware, so it’s useful
    only to a certain degree. You can also see the actual binary post translation
    using the `cuobjdump` utility as follows:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，PTX并不是硬件上真正执行的代码，因此它只在一定程度上有用。你还可以通过以下方式使用`cuobjdump`工具查看实际的二进制代码：
- en: '`cuobjdump –sass global_mem_sample_sort.sm_20.cubin > out.txt`'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuobjdump –sass global_mem_sample_sort.sm_20.cubin > out.txt`'
- en: 'If we look at a small device function, this is what we see at the various levels:'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看一个小型设备函数，我们可以在不同的层次看到以下内容：
- en: '[PRE126]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'In PTX:'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 在PTX中：
- en: '[PRE128]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '`{`'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '`{`'
- en: '[PRE129]'
  id: totrans-719
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-720
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'And the actual generated code for a compute 2.0 device:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为计算2.0设备生成的实际代码：
- en: '[PRE131]'
  id: totrans-722
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-723
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: I’ve removed from the final generated code the actual raw hex codes, as they
    are not useful. Both PTX and the target assembler code use the format
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经从最终生成的代码中移除了实际的原始十六进制代码，因为它们没有用。PTX和目标汇编代码使用相同的格式。
- en: '[PRE133]'
  id: totrans-725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: The PTX code is extensively documented in the PTX ISA found in the “doc” directory
    of the NVIDIA GPU Computing Toolkit as the “ptx_isa_3.0.pdf” file for the CUDA
    4.1 SDK release. The binary instruction set is listed for GT200 and Fermi in the
    “cuobjdump.pdf” file found in the same directory. There is no detailed explanation
    of the actual instruction set as yet, as with the PTX, but it’s fairly easy to
    see which instructions map back to the PTX ISA.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: PTX代码在NVIDIA GPU计算工具包的“doc”目录中的“ptx_isa_3.0.pdf”文件中有详细文档，适用于CUDA 4.1 SDK发布。二进制指令集在同一目录中的“cuobjdump.pdf”文件中列出，适用于GT200和Fermi。与PTX一样，实际的指令集尚无详细解释，但很容易看到哪些指令映射回PTX
    ISA。
- en: While NVIDIA supports forward compatibility with the PTX ISA between revisions
    of hardware, that is, PTX for compute 1.x will run on compute 2.x, the binaries
    are not compatible. This support of older versions of PTX will usually involve
    the CUDA driver recompiling the code for the actual target hardware on-the-fly.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 NVIDIA 支持 PTX ISA 在硬件修订之间的向前兼容性，也就是说，1.x 版本的 PTX 可以在 2.x 版本的硬件上运行，但二进制文件并不兼容。对旧版本
    PTX 的支持通常意味着 CUDA 驱动程序会在运行时将代码重新编译为实际目标硬件的代码。
- en: You should read the PTX ISA document and understand it well. It refers to CTAs
    a lot, which are cooperative thread arrays. This is what is termed a “block” (of
    threads) at the CUDA runtime layer.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该阅读 PTX ISA 文档并深入理解它。文档中频繁提到 CTA，即合作线程数组。这就是 CUDA 运行时层中所称的“块”（线程块）。
- en: Changes in the C code will drastically affect the final assembly code generated.
    It’s always good practice to look at the code being generated and ensure it is
    doing what is expected. If the compiler is reloading something from memory or
    doing something you would not expect, there is usually a good reason. You can
    usually then identify the cause in the C source code and eliminate the problem.
    In certain instances, you can also create inline PTX to get the exact functionality
    you require, although a lot of the very low-level instructions have equivalent
    compiler intrinsic functions that can be used.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: C 代码的更改会显著影响最终生成的汇编代码。检查生成的代码并确保它按预期工作是一个好习惯。如果编译器从内存中重新加载某些内容或做了你不期望的事情，通常有其合理的原因。你通常可以在
    C 源代码中找到原因并消除问题。在某些情况下，你还可以创建内联 PTX 代码来获得所需的精确功能，尽管许多非常底层的指令都有等效的编译器内建函数可以使用。
- en: 'One of the easiest ways to look at and understand the low-level assembly functions
    is to view the interleaved source and assembly listing via the “View Disassembly”
    option from within Parallel Nsight. Simply set a breakpoint within the CUDA code,
    run the code from the Nsight menu (“Start CUDA Debugging”), and wait for the breakpoint
    to be hit. Then right-click near the breakpoint and the context menu will show
    “View Disassembly.” This brings up a new window showing the interleaved C, PTX,
    and SASS code. For example:'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 查看并理解低级汇编函数的最简单方法之一是通过 Parallel Nsight 中的“查看反汇编”选项查看交错的源代码和汇编列表。只需在 CUDA 代码中设置一个断点，通过
    Nsight 菜单（“开始 CUDA 调试”）运行代码，等待断点被触发。然后右键点击断点附近，弹出菜单中会显示“查看反汇编”。这将弹出一个新窗口，显示交错的
    C 代码、PTX 和 SASS 代码。例如：
- en: '[PRE134]'
  id: totrans-731
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '`0x0002caf0     MOV R11, R9;`'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '`0x0002caf0     MOV R11, R9;`'
- en: '[PRE135]'
  id: totrans-733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: Here you can easily see how the C source code, a test for `threadIdx.x < 128`,
    is translated into PTX and how each PTX instruction is itself translated into
    one or more SASS instructions.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以很容易地看到 C 源代码中 `threadIdx.x < 128` 的测试是如何被转换成 PTX，并且每条 PTX 指令又是如何转换成一个或多个
    SASS 指令的。
- en: Register usage
  id: totrans-735
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寄存器使用
- en: Registers are the fastest storage mechanism on the GPU. They are the only way
    of achieving anything like the peak performance of the device. However, they are
    limited in their availability.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 寄存器是GPU上最快的存储机制。它们是实现设备峰值性能的唯一途径。然而，寄存器的数量是有限的。
- en: To launch a block onto an SM, the CUDA runtime will look at the block’s usage
    of registers and shared memory. If there are sufficient resources, the block will
    be launched. If not, the block will not. The number of blocks that are resident
    in an SM will vary, but typically you can achieve up to six blocks with reasonably
    complex kernels, and up to eight with simple ones (up to 16 on Kepler). The number
    of blocks is not really the main concern. It’s the overall number of threads as
    a percentage of the maximum number supported, which is the key factor.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 要将一个块调度到SM上，CUDA运行时会查看该块对寄存器和共享内存的使用情况。如果资源足够，块将被启动。如果不够，则不会启动。驻留在SM中的块数会有所变化，但通常，对于相对复杂的内核，你可以实现最多六个块，对于简单的内核，可以达到八个块（在Kepler架构上最多16个）。块的数量其实并不是最主要的关注点。关键因素是线程的总数占支持的最大线程数的百分比。
- en: We listed a number of tables in [Chapter 5](CHP005.html) that gave an overview
    of how the number of registers per block affects the number of blocks that can
    be scheduled onto an SM, and consequentially the number of threads that the device
    will select from.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](CHP005.html)列出了一些表格，概述了每个块的寄存器数量如何影响能够调度到SM上的块数，从而影响设备将选择的线程数。
- en: 'The compiler provides a `–v` option, which provides some more detailed output
    of what is currently allocated. An example of a typical kernel is:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器提供了一个`–v`选项，它可以提供一些更详细的当前分配情况。一个典型内核的例子是：
- en: '[PRE136]'
  id: totrans-740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: The output is useful, but only if you understand what the compiler is telling
    you. The first item of interest is the `for sm_20` message, which tells you the
    code being created here is for the compute 2.x architecture (Fermi). If you’re
    using exclusively Fermi devices for your target deployment, then make sure your
    target is set correctly. By default you will generate compute 1.0 code unless
    you specify otherwise, which will restrict the available operations and generate
    code that is not as efficient as it could be for Fermi.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是有用的，但前提是你理解编译器给出的信息。第一个需要注意的项是`for sm_20`消息，它告诉你这里生成的代码是针对2.x架构（Fermi）。如果你仅使用Fermi设备进行目标部署，请确保正确设置目标。默认情况下，除非你特别指定，否则将生成计算1.0代码，这将限制可用操作，并生成对Fermi来说效率不高的代码。
- en: The next interesting point is `40 bytes of stack frame`, which generally means
    you have local variables you are taking the address of, or that you declared a
    local array. The term “local” in C refers to the scope of a variable, and in C++
    was replaced with the keyword “private,” which more accurately reflects what is
    meant.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个有趣的点是`40字节的栈帧`，通常意味着你有局部变量的地址，或者你声明了一个局部数组。在C语言中，“局部”一词指的是变量的作用域，而在C++中，这个术语被“private”关键词取代，这更准确地反映了其含义。
- en: In CUDA the term “local” refers to the scope of a variable for a given thread.
    Thus, the CUDA documentation also uses the term “local memory,” meaning thread
    private data. Unfortunately, “local” implies near or close, which in memory terms
    might imply the data is held close to the processor. In fact, “local data” is
    stored in either global memory for compute 1.x devices or in the L1 cache on Fermi
    devices. Thus, only on Fermi is it really “local” to the processor, and even in
    this case, its size is limited.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，“局部”一词指的是给定线程的变量作用域。因此，CUDA文档中也使用了“局部内存”一词，表示线程私有数据。不幸的是，“局部”一词暗示着接近或靠近，而在内存术语中，这可能意味着数据存储在处理器附近。实际上，“局部数据”存储在计算1.x设备的全局内存中，或者存储在Fermi设备的L1缓存中。因此，只有在Fermi设备上，它才真正是“局部的”处理器，而且即便如此，它的大小也受到限制。
- en: The stack frame is something you typically see with compute 2.x device code,
    especially if using atomic operations. The stack frame will also exist in the
    L1 cache unless it becomes too large. Where possible the CUDA compiler will simply
    inline calls to device functions, thereby removing the need to pass formal parameters
    to the called functions. If the stack frame is being created simply to pass values
    by reference (i.e., pointers) to the device function, it is often better to remove
    the call and manually inline the functions into the caller. This will eliminate
    the stack frame and generate a significant improvement in speed.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 栈帧通常出现在计算2.x设备的代码中，尤其是在使用原子操作时。栈帧也会存在于L1缓存中，除非它变得太大。在可能的情况下，CUDA编译器会直接内联对设备函数的调用，从而消除传递形式参数给被调用函数的需要。如果栈帧只是为了通过引用（即指针）传递值给设备函数，通常最好去掉调用并手动将函数内联到调用者中。这将消除栈帧并显著提高速度。
- en: The next section lists `8+0 bytes lmem`. By “lmem” the compiler is referring
    to local memory. Thus, for 8 bytes, probably a couple of floats or integers have
    been placed into local memory. Again this is typically not a good indication as,
    especially in compute 1.x devices, there will be implicit memory fetches/writes
    to and from slow global memory. It’s an indication you need to think about how
    you might rewrite the kernel, perhaps placing these values into shared memory
    or constant memory if possible.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分列出了`8+0字节的lmem`。通过“lmem”编译器指的是局部内存。因此，对于8字节，很可能有几个浮点数或整数被放入局部内存。再次强调，这通常不是一个好的指示，尤其是在计算1.x设备上，因为会有隐式的内存读取/写入到全局内存中。这表明你需要考虑如何重写内核，可能的话将这些值放入共享内存或常量内存中。
- en: Note the *a* + *b* notation used here denotes the total amount of variables
    declared in those sections (the first number), and then the amount used by the
    system (the second number). Also `smem` (shared memory) usages will be listed
    in addition to `lmem` if shared memory is used by the kernel.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里使用的*a* + *b* 表示法表示在这些部分中声明的变量总数（第一个数字），然后是系统使用的数量（第二个数字）。如果内核使用了共享内存，`smem`（共享内存）使用量也将列出，此外还有`lmem`。
- en: Next we see `80 bytes cmem[0]`. This says the compiler has used 80 bytes of
    constant memory. Constant memory is typically used for parameter passing, as most
    formal parameters do not change across calls. The value in the square brackets
    is the constant memory bank used and is not relevant. Simply add all the `cmem`
    figures to obtain the total constant memory usage.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们看到`80 bytes cmem[0]`。这表示编译器使用了80字节的常量内存。常量内存通常用于参数传递，因为大多数正式参数在调用之间不会改变。方括号中的值是使用的常量内存银行，这并不相关。只需将所有`cmem`数字相加即可获得总的常量内存使用量。
- en: Register usage can also be controlled, or forced, using the `–maxrregcount n`
    option in the compiler. You can use this to instruct the compiler to use more
    or less registers than it currently is. You may wish to have fewer registers to
    squeeze another block onto the SM. You may already be limited by some other criteria
    such as shared memory usage, so you may wish to allow the compiler to use more
    registers. By using more registers the compiler may be able to reuse more values
    in registers, rather than store/fetch them again. Conversely, asking for less
    registers usage will usually cause more memory accesses.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 寄存器的使用也可以通过编译器中的`–maxrregcount n`选项进行控制或强制。你可以使用这个选项来指示编译器使用比当前更多或更少的寄存器。你可能希望减少寄存器的使用，以便在SM上挤出另一个块。你可能已经受到其他标准的限制，例如共享内存的使用，因此你可能希望允许编译器使用更多的寄存器。通过使用更多的寄存器，编译器可能能够在寄存器中重用更多的值，而不是再次存储/获取它们。相反，请求减少寄存器的使用通常会导致更多的内存访问。
- en: Asking for less registers to get an additional block is a tradeoff exercise.
    The lower register count and the additional block may bring higher occupancy,
    but this does not necessarily make the code run faster. This is a concept most
    programmers starting with CUDA struggle with. The various analyzer tools try to
    get you to achieve higher occupancy rates. For the most part this is a good thing,
    as it allows the hardware scheduler to have a wider choice of warps to run. However,
    *only if* the scheduler actually runs out of warps at some point, and thus the
    SM stalls, does adding more available warps actually help. Fermi, due to its dual
    warp dispatcher and higher number of CUDA cores per SM, executes warps with a
    higher frequency than earlier models. The effect varies between applications,
    but generally asking for less register usage usually results in slower code. Try
    it for your particular application and see. We look at how you can see if the
    SMs are stalling in the later section on analysis tools.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 请求减少寄存器以获得更多的块是一种权衡的练习。较低的寄存器计数和额外的块可能会带来更高的占用率，但这不一定使代码运行更快。这是大多数刚开始学习 CUDA
    的程序员所面临的一个概念。各种分析工具尝试帮助你实现更高的占用率。大部分情况下，这是一件好事，因为它允许硬件调度器有更多的 warp 可以选择执行。然而，*只有当*调度器在某些时刻确实用尽了
    warp，导致 SM 阻塞时，增加更多可用的 warp 才会有帮助。由于 Fermi 具有双 warp 调度器和每个 SM 更多的 CUDA 核心，它比早期的模型更频繁地执行
    warp。效果因应用而异，但通常来说，请求减少寄存器使用通常会导致代码变慢。你可以为你的特定应用进行尝试并观察结果。我们将在后面的分析工具部分查看如何判断
    SM 是否发生了阻塞。
- en: A better approach to asking for less registers is to understand the register
    usage and allocation of variables. To do this, you need to look into the PTX code,
    using the `–keep` compiler flag. PTX, the virtual assembly language used by CUDA,
    defines a number of state spaces. A variable exists in one of these state spaces.
    These are shown in [Table 9.8](#T0045). Thus, you can always look into the PTX
    code to see where a variable has been placed.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 请求减少寄存器的更好方法是理解变量的寄存器使用和分配。为此，你需要查看 PTX 代码，并使用 `–keep` 编译器标志。PTX 是 CUDA 使用的虚拟汇编语言，定义了多个状态空间。变量存在于这些状态空间之一中。它们展示在[表
    9.8](#T0045)中。因此，你可以始终查看 PTX 代码，看看变量被放置在哪里。
- en: Table 9.8 PTX State Space
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.8 PTX 状态空间
- en: '![Image](../images/T000090tabT0045.jpg)'
  id: totrans-752
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000090tabT0045.jpg)'
- en: Reducing register usage from say 26 to 25 per kernel will have little effect.
    However, transitioning over a register boundary (16, 20, 24, and 32) will usually
    allow for more blocks to be scheduled. This will bring a greater selection of
    warps and will usually improve performance. This is not always the case. More
    blocks can mean more contention for shared resources (shared memory, L1/L2 caches).
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个内核的寄存器使用量从26减少到25几乎不会产生什么影响。然而，跨越寄存器边界（16、20、24 和 32）通常会允许更多的块被调度。这将带来更多的
    warp 选择，并且通常会提升性能。不过，这并不总是如此。更多的块可能意味着对共享资源（共享内存、L1/L2缓存）的更多争用。
- en: Register usage can often be reduced simply be rearranging the C source code.
    By bringing the assignment and usage of a variable closer together you enable
    the compiler to reuse registers. Thus, at the start of the kernel you might assign
    `a`, `b`, and `c`. If in fact they are used only later in the kernel, you’ll often
    find reduced register usage by moving the creation and assignment close to the
    usage. The compiler may then be able to use a single register for all three variables,
    as they exist in distinct and disjoint phases of the kernel.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 注册使用量通常可以通过重新排列C源代码来减少。通过将变量的赋值和使用靠近在一起，你可以使编译器重用寄存器。因此，在内核开始时，你可能会赋值`a`、`b`和`c`。如果它们实际上只在内核的后面使用，你通常会发现通过将创建和赋值移动到使用附近可以减少寄存器使用。编译器可能能够为这三个变量使用一个寄存器，因为它们存在于内核的不同且不重叠的阶段。
- en: Section summary
  id: totrans-755
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 段落总结
- en: • Understand how thread layout impacts memory and cache access patterns.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: • 了解线程布局如何影响内存和缓存访问模式。
- en: • Use only multiples of 32 when specifying the thread count for kernel launch.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: • 在指定内核启动的线程计数时，仅使用32的倍数。
- en: • Think about how to increase the amount of work performed per memory fetch.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: • 考虑如何增加每次内存获取所执行的工作量。
- en: • Understand at least a little of how compilers work when optimizing code and
    adapt your source code to aid the compiler.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: • 了解编译器在优化代码时的工作原理，并调整你的源代码以帮助编译器。
- en: • Consider how branching within a warp can be avoided.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: • 考虑如何避免在一个warp内的分支。
- en: • Look at the PTX and final target code to ensure the compiler is not generating
    inefficient code. If it is, understand why and make changes at the source level
    to address it.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: • 查看PTX和最终目标代码，以确保编译器没有生成低效的代码。如果有，了解原因并在源代码级别进行更改以解决问题。
- en: • Be aware and understand where data is being placed and what the compiler is
    telling you.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: • 注意并理解数据被放置的位置以及编译器告诉你的信息。
- en: 'Strategy 5: Algorithms'
  id: totrans-763
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略5：算法
- en: 'Selecting an efficient algorithm on the GPU can be challenging. The best algorithm
    in the CPU domain is not necessarily the best for the GPU. The GPU has its own
    unique challenges. To get the best performance you need to understand the hardware.
    Thus, when considering algorithms, we need to think about:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上选择高效的算法可能具有挑战性。CPU领域中最好的算法不一定是GPU上最好的。GPU有其独特的挑战。为了获得最佳性能，你需要了解硬件。因此，在考虑算法时，我们需要思考：
- en: • How to decompose the problem into blocks or tiles and then how to decompose
    those blocks into threads.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: • 如何将问题分解为块或瓦片，然后如何将这些块分解为线程。
- en: • How the threads will access the data and what sort of memory pattern this
    will generate.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: • 线程将如何访问数据，以及这将生成什么样的内存模式。
- en: • What data reuse opportunities are present and how these can be realized.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: • 存在哪些数据重用机会，以及如何实现这些机会。
- en: • How much work the algorithm will be performing in total and whether there
    is a significantly difference from a serial implementation.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: • 算法总共将执行多少工作，以及与串行实现是否存在显著差异。
- en: 'There is an 800-plus-page book published by Morgan Kaufman entitled *GPU Computing
    Gems* that covers in detail implementation of various algorithms for the following
    areas:'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 有一本由摩根·考夫曼出版的800多页书籍，名为*GPU计算宝典*，详细介绍了在以下领域中实现各种算法：
- en: • Scientific simulation
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: • 科学仿真
- en: • Life sciences
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: • 生命科学
- en: • Statistical modeling
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: • 统计建模
- en: • Data-intensive applications
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: • 数据密集型应用
- en: • Electronic design and automation
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: • 电子设计与自动化
- en: • Ray tracing and rendering
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: • 光线追踪与渲染
- en: • Computer vision
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: • 计算机视觉
- en: • Video and image processing
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: • 视频与图像处理
- en: • Medical imaging
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: • 医学成像
- en: The purpose of this section is not to look at algorithms that are specific to
    certain fields, as they are of limited general interest. Here we look at a few
    common algorithms that can be implemented, which in turn may form building blocks
    for more complex algorithms. This book is not about providing sets of examples
    you can copy and paste, but providing examples where you can learn the concepts
    of what makes good CUDA programs.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的是不是要研究特定领域的算法，因为这些算法的普遍兴趣较小。这里我们将研究一些常见的算法，这些算法可以实现，并且可能成为更复杂算法的构建模块。本书并非提供可以直接复制和粘贴的示例，而是提供可以帮助你理解哪些内容构成优秀CUDA程序的示例。
- en: Sorting
  id: totrans-780
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排序
- en: There are many, many sorting algorithms available, some of which can easily
    and efficiently be implemented on the GPU and many of which are not well suited.
    We’ve looked already in previous chapters at merge sort, radix sort, and the more
    exotic sample sort. We’ll look here at one more parallel sort that is useful in
    terms of looking at how algorithms are implemented in GPUs.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多排序算法可以使用，其中一些可以轻松高效地在GPU上实现，而许多则不太适合。我们在前几章中已经看过了归并排序、基数排序和更为特殊的样本排序。这里我们将讨论另外一种有用的并行排序算法，重点是如何在GPU上实现算法。
- en: Odd/even sort
  id: totrans-782
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奇偶排序
- en: An odd/even sort works by selecting every even array index and comparing it
    with the higher adjacent odd array index ([Figure 9.34](#F0175)). If the number
    at the even element is larger than the element at the odd index, the elements
    are swapped. The process is then repeated, starting with the odd indexes and comparing
    them with the higher adjacent even index. This is repeated until we make no swaps,
    at which point the list is sorted.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 奇偶排序的工作原理是选择每个偶数数组索引，并将其与更高的相邻奇数数组索引进行比较（[图 9.34](#F0175)）。如果偶数元素的值大于奇数索引处的元素值，则交换这两个元素。然后，过程会从奇数索引开始，并将其与更高的相邻偶数索引进行比较。这个过程会重复，直到不再发生交换，此时列表已排序。
- en: '![image](../images/F000090f09-34-9780124159334.jpg)'
  id: totrans-784
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-34-9780124159334.jpg)'
- en: FIGURE 9.34 Odd/even sort.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.34 奇偶排序。
- en: An odd/even sort is a variation of a bubble sort. A bubble sort works by selecting
    the number at the first index and comparing and swapping it with the index to
    the right until such time as it’s no longer larger than the number to its right.
    The odd/even sort simply extends this to use *P* independent threads to do this,
    where *P* is half the number of elements in the list.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 奇偶排序是冒泡排序的变种。冒泡排序通过选择第一个索引处的数字，并将其与右侧索引的数字进行比较并交换，直到它不再大于右侧的数字为止。奇偶排序只是将这一过程扩展为使用*P*个独立线程来执行，其中*P*是列表中元素数量的一半。
- en: If we define the number of elements in an array as *N*, then the ability to
    deploy half of *N* threads may be appealing. The sort is also quite easy to conceptualize,
    but raises some interesting problems when trying to implement on the GPU, so it
    is a good example to look at.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数组中的元素数量定义为*N*，那么部署*N*的一半线程可能是一个很有吸引力的选择。排序的概念也相当简单，但在尝试在GPU上实现时会引发一些有趣的问题，因此它是一个很好的例子可以参考。
- en: The first issue is that odd/even sort is designed for parallel systems where
    individual processor elements can exchange data with their immediate neighbor.
    It requires a connection to the left and right neighbor only. A connection for
    our purposes will be via shared memory.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是奇偶排序是为并行系统设计的，其中各个处理器元素可以与它们的直接邻居交换数据。它只需要连接到左右邻居。为了我们的目的，连接将通过共享内存进行。
- en: Having thread 0 access array elements zero *and* one and thread 1 access elements
    two *and* three causes a sequence issue for the coalescing hardware. It needs
    each thread to access a contiguous pattern for a coalesced access. Thus, on compute
    1.x hardware this access pattern is terrible, resulting in multiple 32-byte fetches.
    However, on compute 2.x hardware, the accesses fetch at most two cache lines.
    The additional data fetched from the even cycle will likely be available for the
    odd cycle and vice versa. There is also a significant amount of data reuse with
    a high degree of locality, suggesting cache and/or shared memory would be a good
    choice. Shared memory would likely be the only choice for compute 1.x devices
    due to the poor coalescing.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 让线程0访问数组元素0 *和*1，线程1访问元素2 *和*3，会导致共alescing硬件的序列问题。它需要每个线程访问一个连续的模式以实现合并访问。因此，在compute
    1.x硬件上，这种访问模式很差，导致多个32字节的提取。然而，在compute 2.x硬件上，这些访问最多会提取两个缓存行。来自偶数周期的额外数据可能会在奇数周期时可用，反之亦然。还有大量的数据重用，并具有较高的局部性，这表明缓存和/或共享内存可能是一个不错的选择。由于合并效果差，计算1.x设备可能只能选择共享内存。
- en: If we consider shared memory, we need to think about bank conflicts. Thread
    0 would need to read banks 0 and 1, plus write to bank 0\. Thread 1 would need
    to reads banks 2 and 3 and write to bank 2\. In a compute 1.x system with 16 banks,
    thread 8 would wrap around and start accessing banks 0 and 1\. On compute 2.0
    hardware, we’d see the same effect at thread 16\. Thus, we’d have four bank conflicts
    per thread on compute 1.x hardware and two bank conflicts per thread on compute
    2.x hardware with a shared memory implementation.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑共享内存，我们需要考虑银行冲突。线程0需要读取银行0和1，并写入银行0。线程1需要读取银行2和3，并写入银行2。在一个具有16个银行的计算1.x系统中，线程8将环绕并开始访问银行0和1。在计算2.0硬件上，我们将在线程16看到相同的效果。因此，在计算1.x硬件上，我们每个线程会有四个银行冲突，而在计算2.x硬件上，使用共享内存实现时每个线程会有两个银行冲突。
- en: 'The CPU code for odd/even sort is quite simple:'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 奇偶排序的CPU代码相当简单：
- en: '[PRE137]'
  id: totrans-792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-793
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '`  else`'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '`  else`'
- en: '[PRE143]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-800
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: The code iterates over the dataset from array element 0 to `num_elem-1` and
    then from element 1 to `num_elem-2`. The two data elements are read into local
    variables and compared. They are swapped if necessary and a counter `num_swaps`
    is used to keep track of the number of swaps done. When no swaps are necessary,
    the list is sorted.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 代码从数组元素0到`num_elem-1`遍历数据集，然后从元素1到`num_elem-2`。两个数据元素被读取到局部变量中并进行比较。必要时交换它们，并使用计数器`num_swaps`来跟踪交换次数。当不需要交换时，列表就已排序。
- en: For a mostly sorted list, such algorithms work well. The reverse sorted list
    is the worst case, where we have to move elements all through the list to the
    end. The output of a reverse sorted list is shown here. We can see in each stage
    how the values move between the cells.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数已经排序的列表，这类算法效果很好。逆序排序列表是最糟的情况，我们需要将元素从列表中移动到末尾。逆序排序列表的输出如图所示。我们可以看到在每个阶段，值是如何在单元格之间移动的。
- en: '[PRE145]'
  id: totrans-803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'For the GPU implementation, we’ll use global memory on a compute 2.x device.
    The GPU implementation is:'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU实现，我们将在计算2.x设备上使用全局内存。GPU实现如下：
- en: '[PRE146]'
  id: totrans-805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-806
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '`  u32 num_swaps;`'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '`  u32 num_swaps;`'
- en: '[PRE148]'
  id: totrans-808
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-810
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: Instead of the traditional `for` loop construct, the CPU code uses a `do..while`
    construct. The obvious choice for parallelism from the algorithm is the compare
    and swap operation, meaning we need *N*/2 threads where *N* is the number of elements
    in the array. Given that most lists we’d bother sorting on the GPU will be large,
    this gives us potential to make use of the maximum number of threads on a given
    device (24,576 threads on GTX580).
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的`for`循环结构不同，CPU代码使用了`do..while`结构。从算法中显而易见的并行化选择是比较与交换操作，这意味着我们需要*N*/2个线程，其中*N*是数组中的元素个数。考虑到我们在GPU上通常会排序的大多数列表都很大，这为我们提供了利用给定设备最大线程数的潜力（GTX580上为24,576个线程）。
- en: As each thread processes two elements we cannot simply use `tid` as the array
    index, so create a new local parameter `tid_idx`, which is used to index into
    the array. We also create a parameter `tid_idx_max`, which is set to the last
    value in the array, or the last value in the current block where there is more
    than one block.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个线程处理两个元素，我们不能简单地使用`tid`作为数组索引，因此我们创建一个新的局部参数`tid_idx`，用于索引数组。我们还创建了一个参数`tid_idx_max`，它被设置为数组中的最后一个值，或者在有多个块的情况下，当前块中的最后一个值。
- en: '[PRE156]'
  id: totrans-818
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: The end condition is somewhat problematic. The parameter `num_swap`s in the
    serial version is written to only once per iteration. In the parallel version
    we need to know if *any* thread did a swap. We could therefore use an atomic `add`,
    `increment`, `AND`, or `OR` operation for this, but this would represent a serial
    bottleneck, as every thread that did a write would have to be serialized.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 结束条件有些问题。串行版本中的参数`num_swap`每次迭代只写入一次。在并行版本中，我们需要知道*是否有*线程进行了交换。因此，我们可以使用原子`add`、`increment`、`AND`或`OR`操作来实现这一点，但这会造成串行瓶颈，因为每个进行写操作的线程都必须被串行化。
- en: We could mitigate the cost of the atomic operations somewhat by using a shared
    memory atomic operation. Note that shared memory atomics are supported only on
    compute 1.2 hardware or later (the GT200 series). For the older compute 1.1 hardware
    (the 9000 series) we’d need to use global memory atomics. The definition of the
    `num_swaps` variable would need to be changed accordingly.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用共享内存的原子操作来稍微减轻原子操作的成本。请注意，只有计算1.2硬件或更高版本（GT200系列）才支持共享内存原子操作。对于较旧的计算1.1硬件（9000系列），我们需要使用全局内存原子操作。`num_swaps`变量的定义需要相应地进行更改。
- en: For compute 2.x hardware, there is a much faster solution that we will use here.
    As we have to wait at the end of each round anyway, we can make use of the newly
    provided primitive, `__syncthreads_count`, to which we pass a predicate. If the
    predicate is nonzero in any of the threads, then the result to all threads is
    also nonzero. Thus, if just one thread does a swap, all threads again iterate
    around the loop.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算2.x硬件，我们将使用一种更快的解决方案。由于我们无论如何都必须在每轮结束时等待，我们可以利用新提供的原语`__syncthreads_count`，并向其传递一个谓词。如果任何线程中的谓词非零，那么所有线程的结果也将非零。因此，如果只有一个线程进行了交换，所有线程将再次迭代循环。
- en: '[PRE157]'
  id: totrans-822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: The host function to call the kernel is also shown here for completeness.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也展示了调用内核的主机函数，供完整性参考。
- en: '[PRE158]'
  id: totrans-824
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-826
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-827
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '`    %d threads (%u active)", num_blocks,`'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '`    %d 线程 (%u 活跃)", num_blocks,`'
- en: '[PRE163]'
  id: totrans-830
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-831
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-833
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-834
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-835
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'One question that should be in your mind about this code is what happens at
    the block boundaries. Let’s look at the results with one and two blocks with a
    dataset small enough to print here:'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 你在考虑这段代码时应该问的一个问题是块边界上会发生什么。让我们来看一下使用一个和两个块的结果，数据集足够小，可以在这里打印出来：
- en: '[PRE169]'
  id: totrans-837
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: and
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '[PRE170]'
  id: totrans-839
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: Notice in the second output the sort occurred only within the block. The values
    on the right needed to propagate to the left and vice versa. However, as the blocks
    do not overlap, they are not able to do this. The obvious solution would be to
    overlap the blocks, but this would not be an ideal solution.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在第二次输出中，排序仅发生在块内部。右侧的值需要传播到左侧，反之亦然。然而，由于这些块没有重叠，它们无法做到这一点。显然的解决方案是重叠块，但这并不是理想的解决方案。
- en: CUDA was designed to allow blocks to run in any order, without a means for cross-block
    synchronization within a single kernel run. It’s possible by issuing multiple
    kernels to synchronize between blocks, but this mechanism works well only where
    you have a small number of synchronization steps. In this kernel we’d need to
    overlap the blocks on every iteration. This would lose all locality, as now two
    SMs need to share the same dataset. We also need a global memory atomic or a reduction
    operation to keep track of whether any blocks performed a swap and have to continue
    issuing kernels until no swaps had taken place in any block—a lot of host interaction.
    That would not be a good route to go down.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA被设计为允许块以任何顺序运行，而在单个内核运行中没有跨块同步的手段。通过发出多个内核来同步块之间是可能的，但这种机制仅在同步步骤较少时效果良好。在这个内核中，我们需要在每次迭代时重叠块。这将失去所有的局部性，因为现在两个SM需要共享相同的数据集。我们还需要一个全局内存原子操作或减少操作来跟踪是否有任何块进行了交换，并且必须继续发出内核，直到没有块发生交换——这需要大量的主机交互。这条路并不是一个好的选择。
- en: So we’re left with the two choices found in most sorts that decompose into blocks.
    Either presort the input lists so the values in list *N*[−1] are less than *N*[0],
    which are larger than *N*[1], the solution we used with the sample sort, or merge
    *N* separate lists, the merge sort problem we also looked at earlier.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们只能选择大多数分解成块的排序方法中的两种选择之一。要么先对输入列表进行预排序，使得列表*N*[−1]中的值小于*N*[0]，而*N*[0]大于*N*[1]，这是我们在样本排序中使用的解决方案，或者合并*N*个单独的列表，这是我们之前讨论过的归并排序问题。
- en: Reduction
  id: totrans-843
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少
- en: Reduction is used significantly in parallel programming. We’ll look at some
    of the many ways we can perform a reduction to see which method produces the best
    results on the various compute platforms and to understand why.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 减少操作在并行编程中被广泛使用。我们将研究一些可以执行减少操作的方法，以便了解哪种方法在不同的计算平台上产生最佳结果，并理解其原因。
- en: We’ll look first at computing the sum of *N* 32-bit integers, some 48 million
    to give a reasonable sized dataset. With such a large number of values one of
    the first issues we need to consider is overflow. If we add 0xFFFFFFFF and 0x00000001
    then we have an overflow condition with a 32-bit number. Therefore, we need to
    accumulate into a 64-bit number. This presents some issues.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看计算*N*个32位整数的和，这里取48百万作为一个合理的大小数据集。对于这样大量的值，我们需要考虑的第一个问题是溢出。如果我们将0xFFFFFFFF和0x00000001相加，那么就会发生32位数字的溢出。因此，我们需要将其累加到一个64位数字中。这会带来一些问题。
- en: First, any atomic-based accumulation would require an atomic 64-bit integer
    add. Unfortunately, this is supported in shared memory only with compute 2.x hardware
    and in global memory only in compute 1.2 hardware onward.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，任何基于原子的累加都需要原子 64 位整数加法。不幸的是，只有计算能力 2.x 的硬件才支持在共享内存中进行此操作，而计算能力 1.2 及更高版本的硬件才支持在全局内存中进行此操作。
- en: Global atomic add
  id: totrans-847
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全局原子加法
- en: 'Let’s look first at the simplest form of reduction:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看最简单的归约形式：
- en: '[PRE171]'
  id: totrans-849
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-850
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: In this first example, each thread reads a single element from memory and adds
    it to a single result in global memory. This, although very simple, is probably
    one of the worst forms of a reduce operation. The interblock atomic operation
    means the value needs to be shared across all of the SMs.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个示例中，每个线程从内存中读取一个单独的元素，并将其添加到全局内存中的一个结果中。虽然这个过程非常简单，但它可能是最糟糕的归约操作之一。块间原子操作意味着该值需要在所有
    SM 之间共享。
- en: 'In the older hardware this means physically writing to global memory. In the
    compute 2.x hardware this means maintaining an L2 cache entry, shared among all
    the SMs, and eventually writing this to global memory. The results we see are
    as follows:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧的硬件上，这意味着物理写入全局内存。在计算能力 2.x 的硬件上，这意味着保持一个 L2 缓存条目，由所有 SM 共享，最终将其写入全局内存。我们得到的结果如下：
- en: '[PRE173]'
  id: totrans-853
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: We’ll look here at the compute 2.x devices, as these support 64-bit integer
    atomics.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将查看计算能力 2.x 的设备，因为这些设备支持 64 位整数原子操作。
- en: The issue with the atomic writes, even to L2 cache, is they force a serialization
    of the threads. We have six blocks in each SM, 256 threads per block, generating
    1536 threads per SM. On the GTX470 we have 14 SMs, so a total of 21, 504 active
    threads. On the GTX460 we have 7 SMs, so a total of 10,752 active threads. Performing
    an atomic operation on a single global memory cell means we create a lineup, or
    serialization, of 10 K to 21 K threads. Every thread has to queue, once for every
    single element it processes. Clearly a poor solution, even if it is a somewhat
    simple one.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是写入 L2 缓存的原子操作，也会强制线程序列化。每个 SM 中有六个块，每个块有 256 个线程，总共有 1536 个线程。GTX470 有 14
    个 SM，总共有 21,504 个活跃线程。GTX460 有 7 个 SM，总共有 10,752 个活跃线程。在单个全局内存单元上执行原子操作意味着我们会创建一个队列或序列化的线程排队，从
    10,000 到 21,000 个线程。每个线程必须排队，每次处理一个元素时都会排队。显然，这是一个很差的解决方案，尽管它是一个相对简单的解决方案。
- en: Reduction within the threads
  id: totrans-856
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线程内的归约
- en: We can improve this situation by performing some of the reduction within the
    thread. We can do this very simply by changing the data type and adjusting the
    kernel to ensure we don’t go out of bounds.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在线程内执行一部分归约来改善这种情况。我们可以通过简单地更改数据类型并调整内核来确保我们不会越界，从而轻松实现这一点。
- en: '[PRE174]'
  id: totrans-858
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '`// address in GMEM`'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: '`// 在 GMEM 中的地址`'
- en: '[PRE175]'
  id: totrans-860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-861
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-863
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-865
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-867
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'In the first example we process two elements per thread and four in the second
    using the built-in vector types `uint2` and `uint4`. This produces the following
    timings:'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们每个线程处理两个元素，而在第二个示例中使用内置的向量类型 `uint2` 和 `uint4` 处理四个元素。这产生了以下的时间：
- en: '[PRE183]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-870
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Although, a dramatic reduction, we’ve not really solved the problem. All we
    have done is to half or quarter the number of times each thread has to queue by
    performing a local reduction. This drops the overall time to approximately one-half
    and one-quarter of the original. However, there is still a 5 K thread lineup trying
    to write to the global memory.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管减少了很多，但我们并没有真正解决问题。我们所做的仅仅是通过执行本地归约，减少了每个线程排队的次数，约为原来的一半或四分之一。这使得总时间降到了原来的大约一半或四分之一。然而，仍然有5000个线程在排队等待写入全局内存。
- en: 'Note in performing the addition locally, we reduce the number of global writes
    by a factor equal to the level of ILP. However, we have to be careful about how
    the addition is performed. You could write:'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地执行加法时，我们通过将全局写入次数减少一个因子，该因子等于ILP的级别。然而，我们需要小心加法的执行方式。你可以这样写：
- en: '[PRE185]'
  id: totrans-873
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: In C, an expression is typically evaluated from left to right. A promotion of
    the left operator generates an implicit promotion of the right operator. Thus,
    you might expect `element.x` to be promoted to an unsigned 64-bit type, and as
    `element.y` is to be added to it, it will also be promoted. As `element.z` and
    `element.w` will subsequently be added, you might also expect these to be promoted.
    You are, however, thinking like a serial programmer. The `z` and `w` elements
    can be calculated independently of `x` and `y`. This is exactly what the PTX code
    does. As neither `z` nor `w` has been promoted to a 64-bit value, the addition
    is done as a 32-bit addition, which may result in an overflow.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 在C语言中，表达式通常是从左到右进行求值的。左侧操作数的提升会隐式地提升右侧操作数。因此，你可能期望`element.x`被提升为无符号64位类型，并且由于`element.y`将被加到它上面，它也会被提升。由于随后会对`element.z`和`element.w`进行加法操作，你可能还会期望它们也会被提升。然而，你的思维方式像一个串行程序员。`z`和`w`元素可以独立于`x`和`y`进行计算。这正是PTX代码所做的。由于`z`和`w`都没有被提升为64位值，加法操作是作为32位加法执行的，这可能会导致溢出。
- en: The problem lies in that C permits any order of evaluation where the operator
    is commutative. However, as you typically see a left to right evaluation, people
    assume this is how all compilers work. This is one of the portability issues between
    C compilers. When we move to a superscalar processor such as a GPU, it performs
    the two sets of additions independently to make the maximum use of the pipeline.
    We don’t want it to wait 18–22 plus cycles for the first addition to complete
    then make the subsequent additions in series.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于C语言允许操作数是可交换的情况下，采用任何顺序进行求值。然而，由于通常看到的是从左到右的求值，人们假设所有编译器都是这样工作的。这是C语言编译器之间的可移植性问题之一。当我们转向如GPU这样的超标量处理器时，它会独立执行两组加法，以最大限度地利用流水线。我们不希望它等待18到22个周期后，第一轮加法完成，再开始依次执行后续的加法操作。
- en: 'Thus, the correct way to write such additions is:'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正确的加法写法应该是：
- en: '[PRE186]'
  id: totrans-877
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: Here every value is converted to a 64-bit number before the addition takes place.
    Then any ordering of the addition is fine for integer values. Note for floating-point
    values simply converting to doubles is not enough. Due to the way floating-point
    numbers work adding a very tiny number to a very large number will result in the
    small number being discarded, as the floating-point notation does not have the
    required resolution to hold such values. The best approach to this type of problem
    is to first sort the floating-point values and work from the smallest number to
    the largest.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个值在加法操作之前都被转换为64位数字。然后，任何加法顺序对于整数值来说都是可以的。请注意，对于浮点值，仅仅将其转换为双精度浮点数是不够的。由于浮点数的工作方式，将一个非常小的数加到一个非常大的数上会导致小的数被丢弃，因为浮点表示无法提供足够的分辨率来保存这些值。解决此类问题的最佳方法是首先对浮点值进行排序，从最小的数字开始处理，直到最大的。
- en: We can take the ILP technique a little further by using multiple elements of
    `uint4` and adjusting the kernel accordingly.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用多个`uint4`元素并相应调整内核，进一步优化ILP技术。
- en: '[PRE187]'
  id: totrans-880
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-882
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '`  u64 value = ((u64)element.x) + `'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '`  u64 value = ((u64)element.x) + `'
- en: '[PRE190]'
  id: totrans-884
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-888
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-890
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '`     ((u64)element.y) + `'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: '`     ((u64)element.y) + `'
- en: '[PRE199]'
  id: totrans-894
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-895
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-897
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-899
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-901
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '`     ((u64)element.w);`'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: '`     ((u64)element.w);`'
- en: '[PRE208]'
  id: totrans-904
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-906
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-907
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: 'Notice that we’re mixing the loading of data with the addition. We could move
    all the loads to the start of the function. However, consider that each `uint4`
    type requires four registers. Thus, the `ILP32` example would require 32 registers
    just to hold the values from a single read iteration. In addition, some are needed
    for the addition and final write. If we use too many registers, the number of
    blocks that can be scheduled is reduced or the kernel spills registers to “local”
    memory. Such local memory is the L1 cache for compute 2.x devices and global memory
    for the compute 1.x devices. The results for these ILP kernels are shown here:'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将数据加载与加法操作混合在一起。我们可以将所有加载操作移到函数的开始部分。然而，考虑到每个`uint4`类型需要四个寄存器，`ILP32`示例将需要32个寄存器来保存单次读取迭代的值。此外，还需要一些寄存器用于加法和最终写入。如果我们使用过多的寄存器，则可以调度的块数量会减少，或者内核会将寄存器溢出到“本地”内存。这种本地内存对于计算2.x设备来说是L1缓存，对于计算1.x设备来说是全局内存。这里展示了这些ILP内核的结果：
- en: '[PRE212]'
  id: totrans-909
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-910
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-911
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: We can see that ILP significantly decreases the execution time, providing it’s
    not taken too far. Note the `ILP32` solution actually takes longer. Despite achieving
    a 20× speedup over the simplest version, we have still not solved the atomic write
    queuing problem, just reduced the overall number of elements. There are still
    too many active threads (10–21 K) all trying to write to the single accumulator.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，ILP显著减少了执行时间，前提是不要过度使用。请注意，`ILP32`方案实际上会花费更长的时间。尽管比最简单的版本实现了20倍的加速，我们仍然没有解决原子写入排队问题，只是减少了元素的总数。仍然有太多活动线程（10–21
    K）都在尝试写入同一个累加器。
- en: Reduction of the number of blocks
  id: totrans-913
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 减少块的数量
- en: Currently, we’re invoking *N* blocks where *N* is the problem size, 12 million
    elements (48 MB) divided by the number of threads per block multiplied by the
    number of elements processed per block. We finally get *N* atomic writes, all
    of which are serialized and cause a bottleneck.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，我们启动了*N*个块，其中*N*是问题的大小，1200万元素（48 MB）除以每个块的线程数，再乘以每个块处理的元素数量。最终，我们得到*N*次原子写操作，所有这些操作都被串行化并导致瓶颈。
- en: We can reduce the number of contentions if we create far, far less blocks and
    greatly increase the amount of work each block performs. However, we have to do
    this without increasing the register usage, something the `ILP32` example did.
    This, in turn, caused a slowdown due to local memory reads and writes.
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建的块数量大大减少，同时大幅增加每个块执行的工作量，就可以减少争用的次数。然而，我们必须在不增加寄存器使用量的情况下做到这一点，正如`ILP32`示例所做的那样。反过来，这导致了由于局部内存的读写而出现的性能下降。
- en: Currently, we launch 48 K blocks, but could reduce this to 16, 32, 64, 128,
    or 256 blocks. We can then have each thread march through memory, accumulating
    the result to a register, and only when the block is complete, write out the result.
    Depending on the number of blocks, this should generate quite good locality of
    memory references between the SMs, thus making good use of the memory bandwidth
    and L2 cache if present.
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，我们启动了48K个块，但可以将其减少到16、32、64、128或256个块。然后，每个线程可以依次遍历内存，将结果累积到寄存器中，只有当块完成时才写出结果。根据块的数量，这应该能够在SM之间产生良好的内存引用局部性，从而有效利用内存带宽和L2缓存（如果有的话）。
- en: '[PRE215]'
  id: totrans-917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-918
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-920
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-922
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-923
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '`}`'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: The first task is to work out how many iterations over the data each thread
    needs to make. The parameter `gridDim.x` holds the number of blocks launched.
    Each block consists of `blockDim.x` threads. Thus, we can work out how many elements
    of data each thread must accumulate. We then accumulate these in `local_result`,
    and only when the block is complete, do a single write to global memory.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项任务是计算每个线程需要遍历数据的次数。参数`gridDim.x`表示启动的块数。每个块由`blockDim.x`个线程组成。因此，我们可以计算每个线程必须累积多少个数据元素。然后，我们将这些元素累积到`local_result`中，只有当块完成时，才会一次性写入全局内存。
- en: This reduces the contention from a thread-level contention to a block-level
    contention. As we’re only launching a few hundred blocks, the probability of them
    all requiring the write at the same time is reasonably low. Clearly as we increase
    the number of blocks, the potential contention increases. Once we have loaded
    all the SMs with the maximum number of permitted blocks, there is little reason
    to increase the number of blocks further, other than for work balancing.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把争用从线程级争用减少到块级争用。由于我们只启动几百个块，它们同时都需要写操作的概率相对较低。显然，随着块的数量增加，潜在的争用也会增加。一旦所有SM被加载了允许的最大块数，除非是为了工作负载平衡，否则几乎没有理由再增加块数。
- en: 'The GTX460 is perhaps the worst example, as with only 7 SMs, each with 6 blocks,
    we should saturate the device at only 42 blocks. The GTX470 would need 90 blocks.
    We, therefore, try all number of blocks (49,152) down to 16 in powers of two,
    fewer blocks than would be necessary to fully populate the SMs. This generates
    the following results:'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: GTX460 可能是最糟糕的例子，因为只有 7 个 SM，每个 SM 有 6 个块，我们应该在仅 42 个块时就使设备饱和。GTX470 则需要 90
    个块。因此，我们尝试所有块数（49,152）到 16 的二次幂，块数少于完全填充 SM 所需的块数。这产生了以下结果：
- en: '[PRE222]'
  id: totrans-928
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-929
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: If we look at this first on the very large number of blocks we see a fairly
    linear drop as we halve the number of blocks for each run, for both the GTX470
    and GTX460 cards. We’re halving the number of blocks each cycle by increasing
    the amount of work done per thread, but without increasing the ILP (indicated
    here with `loop1`).
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们首先观察大量块数，我们会看到随着每次运行块数减半，GTX470 和 GTX460 显卡的性能呈现出相当线性的下降。我们通过增加每个线程的工作量来每个周期减半块数，但没有增加
    ILP（在这里用`loop1`表示）。
- en: Notice that the GTX460 has consistently outperformed the GTX470 in the previous
    examples. It does this until such time as we get down to a very small number of
    blocks ([Figure 9.35](#F0180)). At 384 blocks we see the GTX470 outperform the
    GTX460\. The GTX470’s larger number of smaller SMs (32 CUDA cores versus 48 CUDA
    cores each) and larger cache starts to impact performance.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在之前的例子中，GTX460 一直优于 GTX470。直到我们减少到非常少的块数时（[图 9.35](#F0180)），GTX470 才开始超越
    GTX460。在 384 个块时，我们看到 GTX470 超越了 GTX460。GTX470 较多的小 SM 数量（每个 32 个 CUDA 核心对比 48
    个 CUDA 核心）和更大的缓存开始影响性能。
- en: '![image](../images/F000090f09-35-9780124159334.jpg)'
  id: totrans-932
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-35-9780124159334.jpg)'
- en: FIGURE 9.35 Time (ms) versus number of blocks (large number of blocks).
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.35 时间（毫秒）与块数（大量块数）之间的关系。
- en: If you then look at the timing with a very small number of blocks, you can see
    that around 64 blocks is the minimum needed before the number of SM scheduling/occupancy
    issues come into play ([Figure 9.36](#F0185)). In the figure, we’ve split the
    graphs into one with a large number of blocks and one with a smaller number, so
    we can see the time at small block numbers.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看非常少量块数的时间，你会发现大约 64 个块是需要的最小值，之后 SM 调度/占用问题开始显现（[图 9.36](#F0185)）。在图中，我们将图表分为一个大量块数和一个较少块数的图，以便我们可以看到小块数时的时间。
- en: '![image](../images/F000090f09-36-9780124159334.jpg)'
  id: totrans-935
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-36-9780124159334.jpg)'
- en: FIGURE 9.36 Time (ms) versus number of blocks (small number of blocks).
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.36 时间（毫秒）与块数（少量块数）之间的关系。
- en: Note so far we’ve used no ILP (instruction-level parallelism). However, we know
    that introducing ILP allows us to achieve better timing. This is especially the
    case when we have a small number of blocks. The optimal timing is for 64 blocks.
    The GTX470 would have just over 4 blocks, 32 warps per SM. With 32-bit memory
    fetches we need a fully loaded SM, 48 warps, to achieve peak bandwidth from the
    global memory. We can achieve this only with ILP while maintaining this number
    of warps.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到目前为止我们没有使用ILP（指令级并行）。然而，我们知道引入ILP可以帮助我们获得更好的时序，尤其是在块数量较少时。最佳时序出现在64个块的情况下。GTX470每个SM有略多于4个块，32个warp。使用32位内存访问时，我们需要一个完全加载的SM，即48个warp，才能从全局内存中实现峰值带宽。我们只有在保持这个warp数量的同时，通过ILP才能实现这一点。
- en: '[PRE224]'
  id: totrans-938
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-939
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-940
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-942
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-943
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-944
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '` const uint4 ∗ const data,`'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '`const uint4 ∗ const data,`'
- en: '[PRE232]'
  id: totrans-947
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-948
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-949
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-950
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-951
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-952
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-953
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: 'The effect on introducing ILP has one additional benefit: The time spent performing
    the loop (overhead) is amortized over more useful instructions (memory fetch,
    add). We therefore see the following results:'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 引入ILP的影响还有一个额外的好处：执行循环的时间（开销）可以通过更多有用的指令（内存读取、加法）来摊销。因此，我们看到以下结果：
- en: '[PRE239]'
  id: totrans-955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: In `loop1` we use a single 32-bit element, in `loop2` we use two elements (`uint2`),
    and in `loop4` we use four elements (`uint4`). In each case we use 64 blocks,
    the best result from the previous test. You can see that moving from 32-bit elements
    per thread to 64-bit elements per thread we gain on the order of 20–25%. Moving
    from 64-bit reads to 128-bit reads gains us almost nothing on the GTX470, but
    on the order of an 8% gain on the GTX460\. This is entirely consistent with the
    bandwidth results we looked at earlier where the GTX460 (compute 2.1) device achieved
    a significantly higher bandwidth when using 128-bit reads instead of 64-bit reads.
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 在`loop1`中，我们使用一个32位元素，在`loop2`中，我们使用两个元素（`uint2`），而在`loop4`中，我们使用四个元素（`uint4`）。在每种情况下，我们使用64个块，这是前一个测试的最佳结果。你可以看到，从每个线程使用32位元素到每个线程使用64位元素，我们的性能提高了大约20-25%。从64位读取到128位读取在GTX470上几乎没有提升，但在GTX460上却有大约8%的提升。这与我们之前看到的带宽结果完全一致，其中GTX460（compute
    2.1）设备在使用128位读取而非64位读取时达到了显著更高的带宽。
- en: Reduction using shared memory
  id: totrans-957
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用共享内存的减少
- en: If we look at the last instruction of the kernel to date, we still have one
    issue, using an atomic add to write out the result. With 256 threads per block
    and 64 blocks resident, we have 16 K threads all trying to write to this final
    accumulated value. What we actually need is a reduction across the threads within
    the block. This would drop the number of writes from 16 K to just 64, the number
    of blocks. This should reduce the overall timing considerably, as we’re removing
    the serialization bottleneck.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看到目前为止内核的最后一条指令，我们仍然有一个问题，即使用原子加法来写出结果。每个块有256个线程，64个块驻留在内存中，总共有16 K个线程试图写入这个最终的累计值。我们真正需要的是在块内线程之间进行归约。这将把写入次数从16
    K减少到64次，即块的数量。这应该大大减少总体时序，因为我们移除了串行化的瓶颈。
- en: However, going back to the first section in this chapter, know when fast is
    fast enough and appreciate the additional effort required to squeeze that last
    few percent out of the problem. Notice as the speed has improved, the kernels
    become more and more complex.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，回到本章的第一部分，要知道何时“快”就已经足够，并且要理解为了挤出问题中的最后几个百分点需要付出额外的努力。请注意，随着速度的提升，内核变得越来越复杂。
- en: Shared memory is a bank-switched set of 32 banks (16 in compute 1.x). Providing
    each thread uses a unique bank index (0..31) the shared memory can process one
    element per clock, per thread. This is its peak performance, for a single warp.
    As we introduce more warps, if they too want to access shared memory, the ability
    of one warp to use the full bandwidth of shared memory is reduced as it must share
    the LSUs with other competing warps. Once the LSUs are running at 100% capacity,
    we’re limited by the bandwidth from the combined 64 K of L1 cache/shared memory
    on the SM.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存是一个银行切换的32个银行集（在计算1.x中是16个）。只要每个线程使用唯一的银行索引（0..31），共享内存就可以在每个时钟周期、每个线程处理一个元素。这是其单个warp的峰值性能。当我们引入更多warp时，如果它们也需要访问共享内存，一个warp使用共享内存的全部带宽的能力就会降低，因为它必须与其他竞争的warp共享LSU。
    一旦LSU运行在100%的容量下，我们就受限于SM上组合的64K L1缓存/共享内存带宽。
- en: We could simply perform a block-level reduction into a single shared memory
    value for each SM. Thus, with 256 threads we’d have a 256:1 reduction ratio. However,
    this proves not to be particularly effective, as each of the 256 threads is serialized.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地在每个SM中执行一个块级的归约，将其归约成一个共享内存值。这样，使用256个线程时，我们会得到一个256:1的归约比。然而，这证明并不特别有效，因为每个256个线程都是串行化的。
- en: The execution units within an SM can execute a half-warp, a group of 16 threads.
    Therefore, it makes sense to perform a reduction across half-warps. We could then
    either perform an additional reduction across the set of 16 half-warps, or we
    could simply write out the set of values to shared memory. It turns out there
    is almost no difference in execution time between the two approaches.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: SM中的执行单元可以执行一个半warp，即一组16个线程。因此，在半warp之间进行归约是有意义的。然后，我们可以在16个半warp之间执行一个额外的归约，或者我们可以简单地将一组值写入共享内存。事实证明，这两种方法在执行时间上几乎没有差别。
- en: The problem, however, with a subsequent intrablock reduction in shared memory
    is where to locate the shared memory parameter to perform the reduction. If you
    place it after the set of 64 bytes occupied by the intrawarp reduction parameters,
    it causes the next block of intrawarp not to be 64-byte aligned. The different
    blocks interact with one another to cause bank conflicts in the shared memory.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随后的在共享内存中的块内归约问题在于如何定位共享内存参数以执行归约。如果你将其放置在占据64字节的块内归约参数之后，这会导致下一个块的内warp不再是64字节对齐的。不同的块相互作用，导致共享内存中的银行冲突。
- en: We opted for the direct write to global memory, as this was the simpler solution
    and shows marginal if any difference in performance. Thus, instead of reducing
    the 16 K conflicting writes to 64 potentially conflicting writes, we have 512
    potentially conflicting writes, which is a factor of 32 reduction.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了直接写入全局内存，因为这是更简单的解决方案，并且性能几乎没有差异。因此，代替将16K冲突写操作减少为64个可能冲突的写操作，我们有512个可能冲突的写操作，这是32倍的减少。
- en: '[PRE240]'
  id: totrans-965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-966
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-967
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-968
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '[PRE244]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: '[PRE245]'
  id: totrans-970
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: '[PRE246]'
  id: totrans-971
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: '[PRE247]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: '[PRE248]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: '[PRE249]'
  id: totrans-974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '[PRE250]'
  id: totrans-975
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '[PRE251]'
  id: totrans-976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: '`}`'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: 'This results in the following:'
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下结果：
- en: '[PRE252]'
  id: totrans-979
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '[PRE253]'
  id: totrans-980
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: In this example, `loopB` has 512 atomic writes to global memory. The second
    kernel, `loopC`, performs an additional intrablock reduction before making 64
    atomic writes to global memory. As you can see, there is little if any difference
    in performance, demonstrating the additional reduction step gains us nothing and
    therefore was removed from the final solution. This is not really too surprising,
    as if the latency of the 512 memory writes is already hidden by the considerable
    computation workload, reducing this to just 64 writes would bring us nothing.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`loopB`有512个原子写操作到全局内存。第二个内核`loopC`在进行64个原子写操作到全局内存之前，进行了额外的块内减少操作。正如你所看到的，性能几乎没有差异，这表明额外的减少步骤并没有带来任何好处，因此被从最终解决方案中移除。这其实并不令人惊讶，因为如果512个内存写操作的延迟已经被相当大的计算工作负载隐藏，减少到仅64个写操作对我们来说没有任何意义。
- en: If we compare the best result from the previous section, using an accumulation
    into registers and then writing out the 16 K values we see on the GTX470 (compute
    2.0), this took 1.14 ms. By adding this further reduction step in shared memory
    we’ve reduced this to just 0.93 ms, a 19% saving in execution time. As the GTX470
    has 14 SMs, this intra-SM reduction step significantly reduces the number of final
    atomic global memory writes that must be coordinated between these SMs.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较上一节中的最佳结果，使用累积到寄存器，然后写出16K值，在GTX470（计算能力2.0）上执行时，这花费了1.14毫秒。通过在共享内存中添加进一步的减少步骤，我们将这个时间缩短到仅0.93毫秒，执行时间节省了19%。由于GTX470有14个SM，这个内部SM减少步骤显著减少了必须在这些SM之间协调的最终原子全局内存写操作的数量。
- en: By contrast, the GTX460 device (compute 2.1) reduced from 1.38 ms to 1.33 ms,
    just 4%. The absolute difference is of course clear in that the GTX470 has a 320-bit
    memory bus compared with the 256-bit memory bus on the GTX460\. It’s the relative
    speedup difference that is interesting.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，GTX460设备（计算能力2.1）从1.38毫秒减少到1.33毫秒，仅减少了4%。绝对差异当然是显而易见的，因为GTX470的内存总线宽度为320位，而GTX460的内存总线宽度为256位。更有趣的是相对加速差异。
- en: Such a small speedup would indicate that the multiple global memory atomic operations
    were not in fact the bottleneck as they were on the GTX470\. It could also indicate
    that perhaps we were already using the LSUs to their full capacity. The ratio
    of LSUs to CUDA cores is much less on the compute 2.1 devices than on the compute
    2.0 devices. Both global memory and shared memory accesses require the LSUs.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的小幅加速表明，多个全局内存原子操作实际上并不是瓶颈，就像在GTX470上那样。这也可能表明，我们可能已经将LSU（加载存储单元）用到了它们的最大容量。计算能力2.1设备上LSU与CUDA核心的比例远低于计算能力2.0设备。全局内存和共享内存访问都需要LSU。
- en: Thus, the shared memory–based reduction, based on half-warps, gains us a significant
    reduction over the purely atomic/global memory–based solution in the previous
    section.
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于共享内存的归约方法，基于半波段（half-warps），相比上一节中纯粹基于原子操作/全局内存的方法，取得了显著的性能提升。
- en: An alternative approach
  id: totrans-986
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一种替代方法
- en: As with any implementation, you should always look to what previous work has
    been done and how this could be used to improve existing designs. Mark Harris
    wrote an excellent study of parallel reduction^([3](#FN3)) back in the early GPU
    days based on the G80 device. Instead of performing a 512:16 reduction, it writes
    the entire set of values to shared memory and then uses shared memory to perform
    a series of partial reductions, always accumulating the result to shared memory.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何实现一样，你应该始终查看之前的工作成果，并思考如何利用这些成果改进现有设计。Mark Harris在早期GPU时代，基于G80设备写了一篇关于并行归约的优秀研究^([3](#FN3))。他没有进行512:16的归约，而是将整个值集写入共享内存，然后利用共享内存执行一系列部分归约，每次都将结果积累到共享内存中。
- en: The results are impressive. He used unsigned integer elements and achieved a
    total time of 0.268 ms on 4 million elements. Scaling this to the 12 million elements
    (48 MB data) we used in the example works out to 1.14 ms, a comparable number
    to the 0.93 ms we achieved on the GTX470.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人印象深刻。他使用无符号整数元素，在400万元素上达到了总时间0.268毫秒。将其扩展到我们在示例中使用的1200万元素（48MB数据），时间为1.14毫秒，这个数字与我们在GTX470上实现的0.93毫秒相当。
- en: However, the GTX470 has 448 CUDA cores, compared to the 128 CUDA cores of the
    G80, a factor of 3.5× improvement in arithmetic capacity. Memory bandwidth has
    increased from 86 GB/s to 134 GB/s, a factor of 1.5×. However, Mark’s kernel accumulates
    into 32-bit integers, whereas we accumulate into 64-bit integers to avoid the
    overflow problem. Therefore, the kernels are not directly comparable.
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GTX470拥有448个CUDA核心，而G80仅有128个CUDA核心，算力提升了3.5倍。内存带宽从86 GB/s增加到134 GB/s，提升了1.5倍。然而，Mark的内核使用32位整数进行累加，而我们使用64位整数来避免溢出问题。因此，这些内核并不能直接进行对比。
- en: Nonetheless the method proposed may produce good results. Accumulation into
    a register will clearly be faster than accumulation into shared memory. As the
    hardware does not support operations that directly operate on shared memory, to
    perform any operation we need to move the data to and from shared memory. One
    of the reasons for selecting register-based accumulation was the elimination of
    this overhead. However, that is not to say we have an optimum set of code for
    this part of reduction yet.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，所提出的方法可能会产生良好的结果。将数据累积到寄存器中显然比累积到共享内存中要快。由于硬件不支持直接操作共享内存的操作，因此要执行任何操作，我们需要将数据从共享内存中移动到寄存器中。选择基于寄存器的累积的原因之一是消除了这一开销。然而，这并不意味着我们已经为这部分归约代码找到了最佳的解决方案。
- en: Some time has passed since this chapter was originally written and this late
    addition comes after a transition from CUDA 4.0 to CUDA 4.1 SDK, which moved us
    from the Open64 compiler to an LLVM-based compiler. This should bring a performance
    boost, and indeed we find the more efficient compiler generates an execution time
    of 0.74 ms instead of our previous 0.93 ms, a huge improvement just from changing
    compilers.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 自本章最初编写以来已经过了一段时间，这一补充是在从CUDA 4.0到CUDA 4.1 SDK过渡后加入的，该过渡使我们从Open64编译器迁移到了基于LLVM的编译器。这个变化应该带来性能提升，事实上我们发现更高效的编译器生成的执行时间为0.74毫秒，而不是之前的0.93毫秒，这仅仅是通过更换编译器就带来了巨大的改进。
- en: However, of this time, how much is actually due to the reduction at the end
    of the code? We can find out simply by commenting out the final reduction. When
    we do this, the time drops to 0.58 ms, a drop of 0.16 ms or some 21%. Further
    investigation reveals that actually all but 0.1 ms of this time can be attributed
    to the atomic add operation.
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些时间中，实际上有多少是归因于代码末尾的归约操作呢？我们可以通过简单地注释掉最终的归约操作来找出答案。当我们这样做时，时间降至0.58毫秒，下降了0.16毫秒，约为21%的提升。进一步调查发现，实际上，除了0.1毫秒外，所有的时间都可以归因于原子加法操作。
- en: 'Using the 2.1 version of Parallel Nsight we can extract a number of useful
    facts from the data:'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Parallel Nsight 2.1版本，我们可以从数据中提取一些有用的信息：
- en: • Of the 48 scheduled warps, on average we get only 32 active warps.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: • 在48个计划的warp中，平均只有32个warp是活跃的。
- en: • The workload is unevenly distributed between the SMs.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: • 工作负载在SMs之间分布不均。
- en: • Most issue dependencies are the short class.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: • 大多数问题依赖性属于短类。
- en: • There is very little divergent branching.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: • 几乎没有分歧分支。
- en: • Around 8% of the time the SMs stalled. This was due mostly to either instruction
    fetch or instruction dependencies.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: • 大约8%的时间SMs处于停滞状态。 这主要是由于指令获取或指令依赖性问题。
- en: This occupancy issue is a somewhat misleading one, in that it is caused by the
    uneven distribution rather than some runtime issue. The problem is the number
    of blocks launched. With 14 SMs, we can have 84 blocks resident with 6 blocks
    per SM. Unfortunately we only launch 64, so in fact some of the SMs are not fully
    loaded with blocks. This drops the average executed warps per SM and means some
    SMs idle at the end of the workload.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 这个占用问题有些误导，因为它是由不均匀的分布造成的，而不是某种运行时问题。问题在于启动的块数。拥有 14 个 SM 时，我们可以在每个 SM 上分配 6
    个块，总共 84 个块驻留在 SM 上。不幸的是，我们只启动了 64 个块，因此实际上一些 SM 并没有完全加载块。这导致每个 SM 执行的 warps 平均数下降，并意味着在工作负载结束时一些
    SM 处于空闲状态。
- en: 'We ended up with a value of 64 due to it being identified as an ideal number
    from the earlier experiments. However, these were based on 16 K competing atomic
    writes to global memory. We’ve since reduced this to just 512 writes with most
    of the atomic writes being within the SM. Once we remove this global bottleneck,
    it would appear that 64 blocks in total is not the ideal number. Running a sample
    we see:'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 64 这个值，是因为它在早期实验中被认为是理想值。然而，这些实验基于 16 K 的竞争原子写入全局内存。我们已经将其减少为仅 512 次写入，并且大部分的原子写入都发生在
    SM 内部。一旦我们去除了全局瓶颈，看来 64 个块并不是理想的数量。运行一个示例后，我们发现：
- en: '[PRE254]'
  id: totrans-1001
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: '`ID:0 GeForce GTX 470:GMEM loopC 48 passed Time 0.82 ms`'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: '`ID:0 GeForce GTX 470:GMEM loopC 48 passed Time 0.82 ms`'
- en: '[PRE255]'
  id: totrans-1003
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: Notice the best number of blocks on the GTX470 is 384, while on the GTX460 it
    is 96\. A value of 192 works well on both devices. Clearly, however, a value of
    64 blocks does not work well.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 GTX470 上最佳的块数是 384，而在 GTX460 上是 96。192 这个值在两种设备上都表现良好。显然，64 个块的值并不适用。
- en: 'However, what about the last issue we noticed, that 8% of the time the SMs
    were idle? Well this improves to 7% when there are additional blocks, so this
    is helping. However, what is the cause of the problem? Looking to the kernel output
    gives us a clue to the issue:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们注意到的最后一个问题是，8% 的时间 SM 处于空闲状态。好消息是，当有更多块时，这一空闲时间降低到 7%，所以这种做法有所帮助。但是，问题的根源是什么呢？查看内核输出可以给我们提供一些线索：
- en: '[PRE256]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE256]'
- en: 'Notice, unlike the CUDA 4.0 SDK compiler, the 4.1 compiler places `uint4` types
    into local memory. This local memory on Fermi is the L1 cache, so should you care?
    We can rewrite the `uint4` access to use a `uint4` pointer. As the `uint4` types
    are 128-bit aligned (4 × 32 bit words), they are guaranteed to sit on a cache
    line and memory transaction boundary. Thus, an access to the first element of
    the `uint4` by any thread will pull the remaining three elements into the L1 cache.
    Consequently, we have L1 local memory access versus L1 direct cache access. There
    should be no difference, in theory. Let’s see:'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与 CUDA 4.0 SDK 编译器不同，4.1 编译器将 `uint4` 类型放入了局部内存中。Fermi 上的这个局部内存是 L1 缓存，那么我们需要关注这个问题吗？我们可以重写
    `uint4` 访问，以使用 `uint4` 指针。由于 `uint4` 类型是 128 位对齐的（4 × 32 位字），它们保证位于缓存行和内存事务边界上。因此，任何线程访问
    `uint4` 的第一个元素时，将把剩下的三个元素也加载到 L1 缓存中。因此，我们有了 L1 局部内存访问和 L1 直接缓存访问。在理论上，应该没有差别。我们来看一下：
- en: '[PRE257]'
  id: totrans-1008
  prefs: []
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[PRE258]'
  id: totrans-1009
  prefs: []
  type: TYPE_PRE
  zh: '[PRE258]'
- en: Both the GTX470 and GTX460 devices show a significant drop in the execution
    time. Looking to the cache utilization statistics, we can see the L1 cache hit
    rate has jumped from 61.1% to 74.5% as we have moved from the local memory version
    (`loopC`) to the pointer version (`loopD`). We also see the percentage of stalls
    in the SMs drops to 5%. Actually for this statistic, the difference on the GTX460
    is quite pronounced, as it started off at 9%, slightly higher than the GTX470\.
    This is likely to be because we’re now able to share the L1 cache data between
    threads as the data is no longer “thread private.”
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: GTX470和GTX460设备的执行时间都有显著下降。从缓存利用率统计来看，我们可以看到L1缓存命中率从61.1%跃升到74.5%，因为我们从本地内存版本（`loopC`）转到了指针版本（`loopD`）。我们还看到SM中的停顿百分比降到了5%。实际上，关于这个统计数据，GTX460的差异非常明显，因为它一开始的停顿率为9%，稍微高于GTX470。这可能是因为我们现在能够在线程之间共享L1缓存数据，因为数据不再是“线程私有的”。
- en: You may be wondering why we simply do not just use 84 blocks as we calculated
    earlier. The issue is one of rounding. The 12 million element dataset does not
    equally divide into 84 blocks. Thus, some blocks would need to process more than
    others. This means the logic would need to be more complex, but more complex for
    *every* block executed. Just running 84 blocks without solving this issue shows
    a time of 0.62 ms, a gain of 0.06 ms over the 384-block version. This demonstrates
    that the 384-block version introduces small enough blocks that the existing load-balancing
    mechanism handles it quite well. The value of making the code more complex significantly
    outweighs the benefits and is only necessary if we in fact do not know the size
    of the input dataset.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们不直接使用之前计算的84个block呢？问题在于四舍五入。1200万个元素的数据集无法平均划分到84个block中。因此，一些block需要处理更多的任务。这意味着逻辑需要更加复杂，但每个block的复杂度都会增加。仅仅运行84个block而不解决这个问题会导致0.62毫秒的时间，比384个block版本快了0.06毫秒。这表明，384个block的版本将任务划分得足够小，现有的负载均衡机制能够很好地处理。使代码更复杂的价值明显低于其带来的好处，除非我们实际上不知道输入数据集的大小。
- en: 'Coming back to the question of shared memory versus atomics, which is faster?
    We can replace the atomic-based reduction with the following code:'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 回到共享内存与原子操作的问题，哪一个更快？我们可以用以下代码替代基于原子的归约：
- en: '[PRE259]'
  id: totrans-1013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE259]'
- en: '[PRE260]'
  id: totrans-1014
  prefs: []
  type: TYPE_PRE
  zh: '[PRE260]'
- en: '[PRE261]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: '[PRE262]'
  id: totrans-1016
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: '[PRE263]'
  id: totrans-1017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: Notice how the code works. First, all 256 threads (warps 0..6) write out their
    current `local_result` to an array of 256 64-bit values in shared memory. Then
    those threads numbered 0 to 127 (warps 0..3) add to their result, the result from
    the upper set of warps. As the warps within a block are cooperating with one another,
    we need to ensure each warp runs to completion, so add the necessary `__syncthreads()`call.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 注意代码的工作方式。首先，所有256个线程（warps 0..6）将它们当前的`local_result`写入一个包含256个64位值的共享内存数组中。然后编号为0到127的线程（warps
    0..3）将上面一组warps的结果加到它们的结果中。由于一个block内的warps需要相互协作，我们需要确保每个warp都执行完毕，因此需要添加必要的`__syncthreads()`调用。
- en: We continue this reduction until the point at which we reach 32 threads, the
    size of a single warp. At this point all threads within the warp are synchronous.
    Thus, we no longer need to synchronize the threads, as the thread sync operation
    is really a warp sync operation within a single block.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续进行这种简化，直到达到32个线程的情况，即一个单一的warp大小。在这一点上，warp中的所有线程都是同步的。因此，我们不再需要同步线程，因为线程同步操作实际上是在单个block内的warp同步操作。
- en: We now have a couple of choices. We could continue the `if threadIdx.x < threshold`
    operation or we can simply ignore the fact that the redundant threads within the
    warp perform a useless operation. The additional test actually generates a considerable
    number of additional instructions, so we simply calculated all values within the
    warp. Note that this is different than running multiple warps, as in the case
    where we have the 128 and 64 test. Within a single warp, reducing the number of
    threads gains us nothing. By comparison, the prior tests eliminate entire warps.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有几个选择。我们可以继续执行`if threadIdx.x < threshold`操作，或者我们可以忽略warp中冗余线程执行无用操作的事实。额外的测试实际上会生成相当数量的附加指令，因此我们直接计算warp内的所有值。请注意，这与运行多个warps不同，像在128和64测试的情况下。在单个warp内，减少线程数并没有带来任何收益。相比之下，之前的测试是消除了整个warp。
- en: So does this gain us anything compared to the atomic reduction?
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这与原子操作相比有什么收益吗？
- en: '[PRE264]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: Compared with the last version, we moved from 0.68 ms to 0.64 ms on the GTX470
    and 0.8 ms to 0.79 ms on the GTX460\. Not a significant gain, but nonetheless
    a gain in execution speed. We can provide one last optimization to this code before
    we move on.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一个版本相比，我们在GTX470上从0.68毫秒下降到0.64毫秒，在GTX460上从0.8毫秒下降到0.79毫秒。虽然这不是一个显著的提升，但还是在执行速度上有所提高。在继续之前，我们可以对这段代码进行最后一次优化。
- en: Compilers typically generate less than optimal code for array indexing where
    the value of the array index is not a constant. The CUDA compiler is no exception.
    We can replace the array code with pointer code, which runs somewhat faster. We
    can also reduce the number of reads/writes to the shared memory area. However,
    as with most optimized solutions the code becomes more complex to understand and
    less easy to maintain and debug.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器通常会为数组索引生成不够优化的代码，尤其是当数组索引值不是常量时。CUDA编译器也不例外。我们可以将数组代码替换为指针代码，这样运行速度会稍微更快。我们还可以减少对共享内存区域的读写次数。然而，像大多数优化解决方案一样，代码变得更复杂，不易理解，也更难维护和调试。
- en: '[PRE265]'
  id: totrans-1025
  prefs: []
  type: TYPE_PRE
  zh: '[PRE265]'
- en: '[PRE266]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: '[PRE267]'
  id: totrans-1027
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: '[PRE268]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: '[PRE269]'
  id: totrans-1029
  prefs: []
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '` local_result += ∗(smem_ptr+64);`'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: '` local_result += ∗(smem_ptr+64);`'
- en: '[PRE270]'
  id: totrans-1031
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: '[PRE271]'
  id: totrans-1032
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: '[PRE272]'
  id: totrans-1033
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '[PRE273]'
  id: totrans-1034
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: '[PRE274]'
  id: totrans-1035
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '[PRE275]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: '[PRE276]'
  id: totrans-1037
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '[PRE277]'
  id: totrans-1038
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '[PRE278]'
  id: totrans-1039
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: 'The approach taken here is that, as we already have the current threads result
    stored in `local_result`, there is little point in accumulating into the shared
    memory. The only shared memory stores needed are those from the upper set of threads
    sending their data to the lower set. Thus, in each reduction step only the top
    set of threads write to shared memory. Once we get to a single warp, the code
    for this test takes longer than the reads/writes it saves from the shared memory,
    so we drop the test and write anyway. Also to avoid any address calculations,
    other than simple pointer addition, the address of the shared memory area is taken
    as a pointer at the start of the code section. The revised timings are:'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 这里采用的方法是，由于我们已经将当前线程的结果存储在 `local_result` 中，因此没有必要将结果累积到共享内存中。所需的唯一共享内存存储是来自上层线程将数据发送到下层线程的部分。因此，在每个缩减步骤中，只有顶部的线程会写入共享内存。一旦我们到达单个
    warp，这个测试的代码执行时间将超过它从共享内存中节省的读取/写入时间，因此我们会跳过测试并继续写入。此外，为了避免任何地址计算，除了简单的指针加法外，共享内存区域的地址在代码段开始时就被作为指针使用。修订后的时间是：
- en: '[PRE279]'
  id: totrans-1041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: Thus, we gained 0.02 ms on both the GTX470 and GTX460\. We have also largely
    eliminated the shared memory based atomic reduction operations, which in turn
    allows for implementation on older hardware. To remove the final reduction to
    global memory, you’d need to write to an array indexed by `blockIdx.x` and then
    run a further kernel to add up the individual results.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 GTX470 和 GTX460 上，我们分别获得了 0.02 毫秒的提升。我们还大大减少了基于共享内存的原子缩减操作，这也使得可以在较旧的硬件上实现。为了消除最后对全局内存的缩减，你需要将结果写入按
    `blockIdx.x` 索引的数组，然后运行一个进一步的内核将各个结果相加。
- en: An alternative CPU version
  id: totrans-1043
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 替代的 CPU 版本
- en: For reference, the CPU serial and parallel implementations are provided so we
    can see the same reduction on the CPU side.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，提供了 CPU 的串行和并行实现，以便我们可以在 CPU 端看到相同的缩减操作。
- en: '[PRE280]'
  id: totrans-1045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: '[PRE281]'
  id: totrans-1046
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: '[PRE282]'
  id: totrans-1047
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: '[PRE283]'
  id: totrans-1048
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: '[PRE284]'
  id: totrans-1049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: '[PRE285]'
  id: totrans-1050
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: On an AMD Phenom II X4 processor (four cores) running at 2.5 MHz, this resulted
    in a timing of 10.65 ms for the serial version and 5.25 ms for the parallel version.
    The parallel version was created using OpenMP and the “reduction” primitive. To
    enable these quite useful pragma in the NVCC compiler simply use the `-Xcompiler
    –openmp` flag and you can use any of the OpenMP directives for CPU-level thread
    parallelism.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 在一台运行 2.5 MHz 的 AMD Phenom II X4 处理器（四个核心）上，串行版本的时间为 10.65 毫秒，而并行版本的时间为 5.25
    毫秒。并行版本是使用 OpenMP 和 “reduction” 原语创建的。要在 NVCC 编译器中启用这些非常有用的 pragma，只需使用 `-Xcompiler
    –openmp` 标志，你就可以使用任何 OpenMP 指令来实现 CPU 级别的线程并行。
- en: This code spawns *N* threads where *N* is the number of cores. The threads are
    then free to run on any available core. The work is split into *N* chunks and
    finally the results are combined.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成 *N* 个线程，其中 *N* 是核心的数量。然后，这些线程可以自由地运行在任何可用的核心上。工作被分成 *N* 个块，最后将结果合并。
- en: As can often be the case with parallel programming on CPUs, we see sublinear
    scaling as the number of cores increases. We can see that the scaling works well
    from one core to two cores, with a 35% drop in time when using two cores and a
    50% drop when using three. However, the addition of the fourth core drops the
    execution time by just an additional 2% so is effectively noncontributing ([Figure
    9.37](#F0190)). You typically see a U shape as the number of cores is further
    increased.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 CPU 上并行编程常见的情况一样，随着核心数量的增加，我们会看到子线性扩展性。我们可以看到，从一个核心到两个核心的扩展效果很好，使用两个核心时执行时间下降了
    35%，使用三个核心时下降了 50%。然而，添加第四个核心后，执行时间仅减少了 2%，因此基本上没有贡献（[图 9.37](#F0190)）。通常随着核心数量的进一步增加，执行时间呈
    U 形变化。
- en: '![image](../images/F000090f09-37-9780124159334.jpg)'
  id: totrans-1054
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-37-9780124159334.jpg)'
- en: FIGURE 9.37 OpenMP scaling on four cores.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.37 四核上的 OpenMP 扩展性。
- en: The reason for this is, while the compute performance is being scaled by the
    introduction of more cores, the memory bandwidth to the socket is shared between
    all cores. Taking our test system as an example, the AMD 905e processor has a
    typical memory bandwidth of 12.5 MB/s. Just to read the 48 MB of data from memory
    without any compute operations would therefore take 3.8 seconds, a considerable
    chunk of the 5.25 ms execution time. Thus, the issue here is not OpenMP versus
    CUDA but one of memory bandwidth available *per core* on a CPU versus that of
    a GPU.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 造成这一现象的原因是，虽然通过引入更多的核心来提升计算性能，但内存带宽在所有核心之间共享。以我们的测试系统为例，AMD 905e 处理器的典型内存带宽为
    12.5 MB/s。仅仅从内存中读取 48 MB 数据而不进行任何计算操作，就需要 3.8 秒，这已经占据了 5.25 毫秒执行时间的相当一部分。因此，问题并不是
    OpenMP 对比 CUDA，而是 CPU 上每个核心可用的内存带宽与 GPU 的内存带宽的对比。
- en: Parallel reduction summary
  id: totrans-1057
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并行归约总结
- en: The original, very simplistic GPU implementation took 197 ms and 164 ms (GTX470
    and GTX460). Compared with the CPU parallel four-core result of 5.25 ms this is
    really terrible and an example of how an apparently fast device can be brought
    to its knees by poor programming practices.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的非常简化的 GPU 实现分别需要 197 毫秒和 164 毫秒（GTX470 和 GTX460）。与 CPU 四核并行结果 5.25 毫秒相比，这个性能非常差，说明了一个看似快速的设备如何因糟糕的编程实践而陷入瓶颈。
- en: The final GPU version uses atomic operations as little as possible outside of
    the SM. It achieves, in pure compute terms, a 6.8× (GTX460) or 8.4× (GTX470) speedup
    over a four-core CPU. However, 0.62 ms is very little compute time to hide any
    transfer time. At 5 GB/s to the device the PCI-E 2.0 bandwidth is around 40% of
    the bandwidth to main memory on our test platform (12 GB/s). A 5 GB per second
    transfer rate gives us around 5 MB per millisecond. Thus the transfer time of
    the 48 MB of data would be 9.6 ms alone. We’d be able to overlap less than 10%
    of compute time with this, which limits the overall execution to no faster than
    the PCI-E 2.0 transfer speed.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的GPU版本尽量减少在SM外部使用原子操作。从纯计算角度来看，它比四核CPU实现了6.8倍（GTX460）或8.4倍（GTX470）的加速。然而，0.62毫秒的计算时间对于隐藏任何传输时间来说非常短。在5
    GB/s的传输速度下，PCI-E 2.0带宽大约是我们测试平台主内存带宽（12 GB/s）的40%。5 GB每秒的传输速率大约相当于每毫秒5 MB的数据传输。因此，48
    MB数据的传输时间本身就需要9.6毫秒。我们只能将不到10%的计算时间与此重叠，这限制了整体执行速度不会超过PCI-E 2.0的传输速度。
- en: This is actually all too often a problem with GPUs in general. They need to
    have a sufficiently complex problem that the benefit of their huge compute power
    can be applied. In such cases, they can drastically outperform a CPU. A simple
    problem like performing a `sum`, `min`, `max`, or other simplistic task just doesn’t
    provide enough of a problem to justify the time for the PCI-E transfer, unless
    we can discount the transfer time by ensuring the data is already resident on
    the device and stays there. This is one of the reasons why the 6 GB Teslas are
    more attractive than the much cheaper consumer cards that have a maximum capacity
    of 4 GB.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是GPU普遍面临的一个问题。它们需要足够复杂的问题，才能充分发挥其巨大计算能力的优势。在这种情况下，GPU的性能可以远远超过CPU。而像执行`sum`、`min`、`max`或其他简单任务这样的简单问题，无法提供足够的挑战，无法为PCI-E传输时间提供合理的正当性，除非我们能通过确保数据已经驻留在设备上并保持在那里，来忽略传输时间。这也是为什么6
    GB的Tesla比那些最大容量为4 GB的便宜消费级显卡更具吸引力的原因之一。
- en: To increase the overall amount of data held in the GPU memory space, you can
    simply install multiple cards in a system, typically up to four per node, or more
    if you use exotic cooling methods. Thus, up to 24 GB in total data can be held
    on four Tesla class cards within a single node. The host memory space can be directly
    augmented with the GPU memory space using the UVA (universal virtual addressing)
    feature if this is available to you (requires a compute 2.x device onwards, a
    64-bit OS, Linux or the TCC driver under Windows, CUDA 4.x runtime). Inter-GPU
    communication (peer-to-peer, P2P) can also be performed without routing the data
    through the CPU, saving hugely on PCI-E bandwidth.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 要增加GPU内存空间中的总体数据量，你可以简单地在系统中安装多个显卡，通常每个节点最多安装四张显卡，若使用特殊冷却方法则可安装更多。因此，在单个节点内，最多可以容纳四张Tesla级显卡，总共可存储24
    GB的数据。如果你可以使用UVA（通用虚拟寻址）功能，还可以通过该功能直接扩展主机内存空间与GPU内存空间（要求设备为compute 2.x及以上版本，64位操作系统，Linux或Windows下的TCC驱动，CUDA
    4.x运行时）。GPU之间的通信（P2P，点对点通信）也可以在不通过CPU路由数据的情况下进行，从而大大节省PCI-E带宽。
- en: As we move from PCI-E 2.0 (5 GB/s) to PCI-E 3.0 the bandwidth per PCI-E slot
    should effectively double, significantly alleviating this problem for GPU devices
    supporting the new PCI-E 3.0 standard. As of the start of 2012 we saw motherboards
    start to support PCI-E 3.0 standard with the Ivybridge/Ivybridge-E processor.
    PCI-E graphics cards will start to appear through 2012 and beyond. In addition
    to increased PCI-E bandwidth came increased host memory bandwidth.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们从PCI-E 2.0（5 GB/s）升级到PCI-E 3.0，每个PCI-E插槽的带宽应该有效翻倍，这显著缓解了支持新PCI-E 3.0标准的GPU设备的带宽问题。从2012年初开始，我们看到主板开始支持PCI-E
    3.0标准，搭载Ivybridge/Ivybridge-E处理器的主板也开始出现。PCI-E显卡将陆续在2012年及以后开始推出。除了增加PCI-E带宽外，还提高了主机内存带宽。
- en: This also highlights another point that we’ve made throughout this book. The
    CPU can be a useful partner in dealing with all the simple problems in conjunction
    with a GPU. For example, where tiles of data need to communicate, it can process
    the halo cases where they need to share data while the GPU is processing the bulk
    of the data. Often such cases present a lot of branching, which is not efficient
    on the GPU and therefore can be better suited to a cooperative approach.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 这也突显了我们在本书中一直强调的另一个观点。CPU可以作为GPU的有用伙伴，处理所有简单的问题。例如，当数据块需要通信时，CPU可以处理它们需要共享数据的“边界”情况，而GPU则处理数据的主要部分。这类情况通常会涉及大量分支，而GPU处理分支不高效，因此这种合作方式可能更为合适。
- en: Section summary
  id: totrans-1064
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小节总结
- en: • There are now well-documented sources that detail algorithms for specific
    fields. Many are available in the form of plug-in libraries.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: • 现在有许多文献详细介绍了针对特定领域的算法。许多算法已经以插件库的形式提供。
- en: • Be aware that not all parallel algorithms have obvious implementations on
    GPUs. Consider factors such as coalescing and communications when thinking about
    how to implement such algorithms.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: • 需要注意的是，并非所有并行算法都能在GPU上有明显的实现方式。在考虑如何实现这些算法时，需要考虑如合并（coalescing）和通信等因素。
- en: • New functions such as `__syncthreads_count` may have been introduced to address
    certain types of problems as the API develops. Study carefully the various additions
    to the API and understand possible usage.
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: • 随着 API 的发展，可能引入了新的函数，如 `__syncthreads_count`，来解决某些类型的问题。仔细研究 API 中的各种新增功能，并理解其可能的使用方法。
- en: • Use multiple elements per thread wherever possible. However, using too many
    elements per thread may adversely affect performance.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: • 尽可能在每个线程中使用多个元素。然而，使用过多的元素可能会对性能产生不利影响。
- en: • As our reduction example shows, the simplest kernel is often the slowest.
    To achieve the absolute best performance often takes significant programming time
    and a good understanding of the underlying hardware.
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: • 正如我们的归约示例所示，最简单的内核往往是最慢的。要实现最佳性能通常需要大量的编程时间和对底层硬件的深刻理解。
- en: • A multicore CPU is more than a capable partner in calculating workloads, but
    will often be memory bandwidth constrained, which in turn may limit your ability
    to make effective use of all the cores.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: • 多核 CPU 在计算负载方面是一个非常有能力的伙伴，但通常会受到内存带宽的限制，这可能限制你有效利用所有核心的能力。
- en: • OpenMP can provide an easy-to-use multithreaded interface for threads on the
    CPU side and is included as part of the standard CUDA compiler SDK.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: • OpenMP 提供了一个易于使用的多线程接口，适用于 CPU 端的线程，并作为标准 CUDA 编译器 SDK 的一部分。
- en: 'Strategy 6: Resource Contentions'
  id: totrans-1072
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 6：资源竞争
- en: Identifying bottlenecks
  id: totrans-1073
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别瓶颈
- en: It’s often not clear to a programmer what, if anything, is wrong with a program.
    Most GPU programs, if they contain a reasonable amount of work for the GPU to
    do, show significant performance gains over their CPU counterparts. The question
    is how much is significant? The problem this question raises is that GPUs can
    be very good at some tasks, adequate at other tasks, and terrible with certain
    tasks. Anything that has a lot of arithmetic work and can be split into many independent
    problems works well.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 对程序员来说，通常不清楚程序是否有问题，或者问题出在哪里。大多数 GPU 程序，如果它们包含适量的 GPU 工作量，通常会比其 CPU 对应程序表现出显著的性能提升。问题是，这种性能提升的程度有多大才算显著？这个问题引出的难点是，GPU
    在某些任务上表现得非常出色，在其他任务上则仅仅足够，而在某些任务上表现得很差。任何有大量算术运算且可以分解成许多独立问题的任务通常都能很好地运行。
- en: Algorithms that have significant branching or are mostly sequential are not
    suited to GPU, or most parallel architectures for that matter. In going down the
    parallel route, you almost always see a tradeoff of single-thread performance
    versus multiple-thread performance. The GPUs are typically clocked at up to 1000
    MHz, one-third or less than that of a typical CPU. They contain none of the fancy
    branch prediction logic that is necessary for large pipelines.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 算法如果有显著的分支或大多数是顺序执行的，那么它们不适合 GPU，也不适合大多数并行架构。在采用并行路线时，你几乎总是会看到单线程性能和多线程性能之间的权衡。GPU
    通常的时钟频率最高可达 1000 MHz，只有典型 CPU 的三分之一或更低。它们没有为大型流水线所必需的分支预测逻辑。
- en: The CPU has had decades of development and we’re pretty much at the end game
    of any significant single-thread performance gains. Consequently, largely serial
    code performs terribly on a GPU compared to a CPU. This may change with future
    hybrid architectures, especially if we see them include the dedicated CPU as it
    is proposed with NVIDIA’s “Project Denver.” This aims to embed an ARM-based CPU
    core into the GPU fabric. We already see the inclusion of GPU elements onto common
    CPU platforms, so it’s fairly certain the future for both the CPU and GPU world
    is likely to be a hybrid, taking the most useful parts of each.
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: CPU经过数十年的发展，我们几乎已经达到了任何显著的单线程性能提升的终点。因此，与CPU相比，大部分串行代码在GPU上的表现非常糟糕。这种情况可能会随着未来混合架构的出现而改变，特别是如果我们看到它们包括专用CPU，就像NVIDIA的“丹佛项目”所提议的那样。该项目旨在将基于ARM的CPU核心嵌入GPU架构中。我们已经看到GPU元素被纳入常见的CPU平台，因此可以相当确定，CPU和GPU世界的未来可能是混合的，取各自最有用的部分。
- en: However, restricting ourselves to the data parallel problems that run well on
    current GPUs, what is a good baseline for your kernel? What should you compare
    it against? What is a realistic target?
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，限制我们自己只关注在当前GPU上运行良好的数据并行问题，你的内核的良好基准是什么？你应该与什么进行比较？什么是一个现实的目标？
- en: 'There are many fields now where CUDA is used to accelerate problems. One of
    the best resources to provide both some idea of what you can achieve and to see
    if there is already a solution that you can just buy in is [http://www.nvidia.com/object/cuda_app_tesla.html](http://www.nvidia.com/object/cuda_app_tesla.html).
    Here they list the following types of applications:'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有许多领域使用CUDA来加速问题。提供一些你可以实现的想法以及查看是否已经有可以直接购买的解决方案的最佳资源之一是 [http://www.nvidia.com/object/cuda_app_tesla.html](http://www.nvidia.com/object/cuda_app_tesla.html)。在这里，他们列出了以下类型的应用：
- en: • Government and Defense
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: • 政府和国防
- en: • Molecular Dynamic, Computation Chemistry
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: • 分子动力学、计算化学
- en: • Life Sciences, Bio-Informatics
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: • 生命科学、生物信息学
- en: • Electrodynamics and Electromagnetic
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: • 电动力学和电磁学
- en: • Medical Imagining, CR, MRI
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: • 医学成像、CR、MRI
- en: • Oil and Gas
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: • 石油和天然气
- en: • Financial Computing and Options Pricing
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: • 金融计算和期权定价
- en: • Matlab, Labview, Mathematica
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: • Matlab、Labview、Mathematica
- en: • Electronic Design Automation
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: • 电子设计自动化
- en: • Weather and Ocean Modeling
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: • 天气和海洋建模
- en: • Video, Imaging, and Vision Applications
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: • 视频、成像和视觉应用
- en: Thus, if your field is options pricing, you can go to the relevant section,
    browse through a few of the sites, and see that the Monte Carlo pricing model
    is somewhere from a 30× to 50× speedup over a single-core CPU according to the
    particular vendor’s analysis. Of course, you have to ask what CPU, what clock
    speed, how many cores were used, etc. to get a reasonable comparison. You also
    have to remember that any vendor-provided figures are trying to sell their product.
    Thus, any figures will be the best case and may well ignore certain difficult
    aspects of the problem to present a more compelling reason to purchase their product
    over their competitor’s product. However, a few hours of research can tell you
    what would be a reasonable target figure for your particular field. You will also
    get an appreciation of what other people have done and more importantly what still
    needs to be developed.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，如果你的领域是期权定价，你可以进入相关部分，浏览一些网站，看到根据特定供应商的分析，蒙特卡洛定价模型相较于单核 CPU 在速度上提高了30倍到50倍。当然，你需要问清楚使用的是哪种CPU，时钟频率是多少，使用了多少个核心等等，才能得到一个合理的比较。你还需要记住，任何供应商提供的数据都是为了推销他们的产品。因此，所有数据都是最佳情况，并且可能忽略了问题中的某些难点，以提供一个更具吸引力的理由，促使你购买他们的产品而不是竞争对手的产品。然而，几个小时的研究可以让你了解在你特定领域内什么样的数据是合理的目标。你还会了解其他人所做的工作，更重要的是，了解哪些方面仍然需要开发。  '
- en: However, don’t be disappointed with your initial GPU results in comparison with
    many of these applications. Often these arise from years of effort, which can
    be a great advantage, but can also mean they have to carry a lot of legacy code.
    A new approach to the problem, or a long forgotten approach used in the time of
    vector machines, may be the best approach today.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不要对你的初始GPU结果感到失望，尤其是在与许多这些应用进行比较时。通常这些应用是经过多年的努力开发出来的，这本身可能是一个巨大的优势，但也可能意味着它们需要承担大量的遗留代码。一个新的解决问题的方法，或者是曾经在向量机时代使用的被遗忘的方法，可能才是今天最好的方法。
- en: Also remember that many of these projects are from startup companies, although
    as CUDA has become more mainstream, there are now more and more corporate offerings.
    Often startups come from talented PhD students who want to continue their field
    of research or thesis into the commercial world. Thus, they often contain a small
    number of individuals who understand a particular problem domain well, but who
    may not come from a computing background. Thus, as someone with a detailed understanding
    of CUDA *and* a detailed understanding of the application field, you may well
    be able to do much better than the existing commercial or research offerings.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: '另外要记住，许多这些项目来自初创公司，尽管随着CUDA变得越来越主流，现在也有越来越多的企业产品提供。初创公司通常由有才华的博士生创办，他们希望将自己的研究领域或论文成果带入商业世界。因此，这些公司通常由少数几位对特定问题领域有深入了解的人组成，但这些人可能并非来自计算机背景。因此，作为一个既了解CUDA又对应用领域有详细理解的人，你完全有可能做得比现有的商业或研究产品更好。  '
- en: Analysis tools
  id: totrans-1093
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '分析工具  '
- en: Visual profiler
  id: totrans-1094
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '可视化分析器  '
- en: One of the first places to start, at least with existing code, is the analysis
    tools provided with the SDK. The first of these is the NVIDIA Visual Profiler
    tool. This is a multiplatform tool. It has the very useful feature of pointing
    out what it thinks is wrong with your kernel, at least pointing you toward what
    you need to do.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 至少对于现有的代码来说，首先可以从 SDK 提供的分析工具入手。其中第一个工具是 NVIDIA Visual Profiler。这是一个多平台工具，具有非常有用的功能，能够指出它认为你的内核有什么问题，至少能够指引你需要做的工作。
- en: To use this tool, you simply compile your CUDA kernel and then select File→New
    Session, selecting the executable you just created. You can also input any working
    directory and command line arguments if applicable. Finally, you have to tell
    the profiler how long the application run is, so it knows when the kernel has
    simply crashed and does not wait forever to start processing the results. Note
    with Windows, you need to disable the default Aero desktop and select the standard
    desktop theme.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个工具时，你只需要编译你的 CUDA 内核，然后选择 文件→新建会话，选择你刚刚创建的可执行文件。如果适用，你还可以输入任何工作目录和命令行参数。最后，你需要告诉分析器应用程序的运行时长，这样它就能知道内核何时崩溃，而不会永远等待处理结果。需要注意的是，在
    Windows 上，你需要禁用默认的 Aero 桌面并选择标准桌面主题。
- en: You are probably unlikely to be able to see the detail on the timeline in [Figure
    9.38](#F0195), but should be able to make out the major sections. The first thing
    that is striking about the timeline is how little compute is being performed (the
    green bar in the middle of the figure). This is a series of kernels using the
    default stream in sequence on a number of GPUs. We see that using the default
    stream causes implicit synchronization and the huge impact this has on overall
    timing.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能看不清 [图 9.38](#F0195) 中时间轴的细节，但应该能辨别出主要部分。时间轴的第一个显著特点是计算量非常小（图中间的绿色条）。这是一系列在多个
    GPU 上按顺序使用默认流的内核。我们看到，使用默认流会导致隐式同步，并且这一点对整体时序的影响巨大。
- en: '![image](../images/F000090f09-38-9780124159334.jpg)'
  id: totrans-1098
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-38-9780124159334.jpg)'
- en: FIGURE 9.38 Visual Profiler timeline.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.38 Visual Profiler 时间轴。
- en: Switching to a streaming example now, we get a different view. Here we can see
    a kernel pushed into a stream with a `memcpy to` and `memcpy from` device around
    it. Although we can see the two GPUs are being used together this time, the tool
    warns us that there is little kernel memory transfer overlap. This is entirely
    correct. It’s caused by the fact that a typical kernel will have some input data
    *and* some output data. Although on all Fermi devices there are two `memcpy` engines
    in the physical hardware, only one is enabled in consumer devices such as the
    GTX470 and GTX460 used here. Thus, all transfers must go into the same `memcpy`
    stream and be executed in order. As the kernel does a “copy to” followed by a
    “copy from” on the first stream, the subsequent stream’s “copy to” gets held up.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在切换到流式示例，我们会看到不同的视角。在这里，我们可以看到内核被推入一个流中，周围有`memcpy to`和`memcpy from`设备。虽然我们这次看到两个GPU一起使用，但工具提醒我们内核内存传输的重叠很少。这完全是正确的。其原因在于，典型的内核会有一些输入数据*和*一些输出数据。尽管在所有Fermi设备上，物理硬件中有两个`memcpy`引擎，但在如GTX470和GTX460等消费级设备中，只启用了一个。因此，所有传输必须进入同一个`memcpy`流并按顺序执行。由于内核先进行“复制到”操作，然后是“复制从”操作，随后的流中的“复制到”操作会被阻塞。
- en: Thus, on Tesla devices where both copy engines are present, we do not see such
    an issue. For consumer-level hardware, we need to adopt a different approach.
    We simply do not issue any copy back transfers into the streams, until such time
    as all the `memcpy to` and kernel invocations have been issued. At this point
    we then push a set of “copy back” commands into the streams and do the transfers.
    There may be some kernel overlap with the last kernel and transfer back, but this
    will be minimal.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在具有两个拷贝引擎的Tesla设备上，我们不会看到此类问题。对于消费级硬件，我们需要采取不同的方法。我们简单地不在流中发出任何拷贝回传输，直到所有的`memcpy
    to`和内核调用都已发出。此时，我们再将一组“拷贝回”命令推入流中并执行传输。可能会有一些内核与最后一个内核和回传输的重叠，但这将是最小的。
- en: The other issue the analysis presents is the bandwidth to and from the device
    is being underutilized (the “Low Memcpy/Compute Overlap” message). In this example,
    we’re using 32 MB chunks of data. If you look back to earlier sections of this
    chapter, you’ll see this is plenty enough to achieve the peak bandwidth of the
    PCI-E bus. However, this issue here is the compute part is taking up most of the
    time. Even if we were to overlap the transfer and kernel execution, the benefit
    would be marginal. Therefore, it’s important to understand the implications of
    what exactly the tools are telling you and if the associated effort will actually
    be worth the saving in execution time.
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 分析中提出的另一个问题是设备的带宽未得到充分利用（“低内存拷贝/计算重叠”消息）。在这个示例中，我们使用的是32 MB的数据块。如果回顾本章前面的部分，你会发现这已经足够达到PCI-E总线的峰值带宽。然而，这里存在的问题是计算部分占用了大部分时间。即使我们重叠传输和内核执行，收益也将是微乎其微的。因此，理解工具给出的提示以及相关的努力是否值得节省执行时间是非常重要的。
- en: Overall it’s a very useful tool and quite easy to set up and use. It produces
    reasonable results quite quickly and is supported on multiple platforms ([Figure
    9.39](#F0200)).
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，这是一个非常有用的工具，设置和使用都非常简单。它能够快速产生合理的结果，并且支持多种平台（[图 9.39](#F0200)）。
- en: '![image](../images/F000090f09-39-9780124159334.jpg)'
  id: totrans-1104
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-39-9780124159334.jpg)'
- en: FIGURE 9.39 Visual Profiler, multi-GPU.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.39 Visual Profiler，多个 GPU。
- en: Parallel Nsight
  id: totrans-1106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Parallel Nsight
- en: Visual Profiler can, unfortunately, only tell you so much. A much better level
    of detail can be found with the Parallel Nsight tool, which is a Windows-only
    visual analyzer and debugger. Even if Windows is not your primary development
    environment, it’s worth dedicating a spare PC to this tool for its analysis features
    alone.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Visual Profiler 能提供的信息是有限的。而通过 Parallel Nsight 工具，你可以获得更为详细的分析，Parallel
    Nsight 是一个仅适用于 Windows 的可视化分析和调试工具。即使 Windows 不是你的主要开发环境，仍然值得为这个工具专门配置一台备用 PC，仅仅为了它的分析功能。
- en: Parallel Nsight is a far more in-depth tool than Visual Profiler. It will tell
    you a lot more about the kernels and what they are doing. However, as with any
    more complex tool, it takes a little time to learn how to use it well. The Visual
    Profiler tool is far simpler to set up and use. It’s a beginner’s tool, whereas
    Parallel Nsight is more of an intermediate to advanced tool.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: Parallel Nsight 是一个比 Visual Profiler 更加深入的工具。它能告诉你更多关于内核及其执行情况的信息。然而，与任何复杂工具一样，它需要一些时间才能学会如何有效使用它。Visual
    Profiler 工具设置和使用更简单，适合初学者，而 Parallel Nsight 更适合中级到高级用户。
- en: Parallel Nsight is best set up with a single PC using one or more compute 2.x
    (Fermi) graphics cards. Parallel Nsight will also run remotely using two PCs,
    each of which has a NVIDIA graphics card. However, you’ll find it much easier
    to have one PC, rather than wait whilst data is copied to/from a remote machine.
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 最好在一台 PC 上使用一张或多张计算 2.x（Fermi）显卡来设置 Parallel Nsight。Parallel Nsight 也可以在两台 PC
    上远程运行，每台 PC 都需要安装 NVIDIA 显卡。不过，你会发现拥有一台 PC 会更加方便，而不是等待数据从远程机器复制过来。
- en: Parallel Nsight presents a number of options for debugging and profiling. The
    two main choices are “Application Trace” and “Profile.” The “Application Trace”
    feature allows you to generate a timeline as with Visual Profiler. This is particularly
    useful for seeing how the CPU interacts with the GPU and shows the times taken
    for host/device interaction. You should also use the timeline to verify correct
    operation of streams and overlapping kernel/memory copy operations.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: Parallel Nsight 提供了多种调试和性能分析的选项。主要有两个选择：“应用程序跟踪”和“性能分析”。“应用程序跟踪”功能允许你生成与 Visual
    Profiler 类似的时间线。这对于查看 CPU 如何与 GPU 互动特别有用，并展示主机/设备之间交互所花费的时间。你还应该使用时间线来验证流和内核/内存复制操作是否正确执行。
- en: Multiple concurrent GPU timelines are also supported. For example, the timeline
    in [Figure 9.40](#F0205) shows we’re failing to provide enough work to keep all
    GPUs busy. Only the computation parts are shown. The Fermi GPUs are shown in red
    as the first and last context, while the older GPUs are shown in green as the
    middle two bars. Each red square represents one kernel invocation on a given stream.
    You can see the first set of kernels end prior to the next set running. We have
    a huge time period where the first GPU is idle. It’s only through using tools
    such as Parallel Nsight you can see issues such as this. It’s difficult to see
    this using host/GPU timers alone.
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 还支持多个并发GPU时间线。例如，[图9.40](#F0205)中的时间线显示我们没有提供足够的工作来保持所有GPU忙碌。这里只显示了计算部分。Fermi
    GPU用红色表示，作为第一个和最后一个上下文，而较旧的GPU用绿色表示，作为中间的两个条形图。每个红色方块代表在给定流上的一个内核调用。你可以看到，第一组内核在下一组运行之前结束。我们有一个很长的时间段，第一个GPU处于空闲状态。只有通过使用像Parallel
    Nsight这样的工具，你才能看到类似的问题。仅使用主机/GPU定时器是很难看出这一点的。
- en: '![image](../images/F000090f09-40-9780124159334.jpg)'
  id: totrans-1112
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-40-9780124159334.jpg)'
- en: FIGURE 9.40 Parallel Nsight, multi-GPU timeline.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.40 Parallel Nsight，多GPU时间线。
- en: The next useful feature is the “Profile” option under the Activity Type menu
    ([Figure 9.41](#F0210)). This allows us to profile the CUDA kernels. However,
    as many of the experiments require multiple runs of the kernel, no timeline can
    be produced when selecting this option.
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个有用的功能是活动类型菜单下的“分析”选项（[图9.41](#F0210)）。这允许我们对CUDA内核进行分析。然而，由于许多实验需要多次运行内核，选择此选项时无法生成时间线。
- en: '![image](../images/F000090f09-41-9780124159334.jpg)'
  id: totrans-1115
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-41-9780124159334.jpg)'
- en: FIGURE 9.41 Parallel Nsight Activity Type selection.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.41 Parallel Nsight 活动类型选择。
- en: Selecting Experiments to Run as “All” from the dropdown box is the simplest
    option. As you can see from the list of experiments in [Figure 9.42](#F0215),
    they are quite extensive. To start acquiring data, simply press the “Launch” button
    in the application control panel ([Figure 9.43](#F0220)). Note the green Connection
    Status circle. This tells you the Parallel Nsight monitor has successfully connected
    with the target devices. This needs to be green before any other options work.
    See the help options for details about setting up the monitor.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 从下拉框中选择“所有”作为运行的实验是最简单的选项。如[图9.42](#F0215)所示，实验列表非常广泛。要开始采集数据，只需按下应用程序控制面板中的“启动”按钮（[图9.43](#F0220)）。注意绿色的连接状态圆圈。它告诉你Parallel
    Nsight监控器已成功连接到目标设备。在其他选项有效之前，这个圆圈需要是绿色的。有关设置监控器的详细信息，请参阅帮助选项。
- en: '![image](../images/F000090f09-42-9780124159334.jpg)'
  id: totrans-1118
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-42-9780124159334.jpg)'
- en: FIGURE 9.42 Parallel Nsight Experiments.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.42 Parallel Nsight 实验。
- en: '![image](../images/F000090f09-43-9780124159334.jpg)'
  id: totrans-1120
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-43-9780124159334.jpg)'
- en: FIGURE 9.43 Parallel Nsight application Launch control.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.43 Parallel Nsight 应用程序启动控制。
- en: Once you press the “Launch” button your application will run until such time
    as it exits. You then will have a number of options in a dropdown box on the top
    of the screen, the last of which is “GPU Devices” ([Figure 9.44](#F0225)). Select
    this and you will see an overview of the GPU devices in the system.
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你按下“启动”按钮，你的应用程序将运行直到退出。然后，你将在屏幕顶部的下拉框中看到多个选项，最后一个选项是“GPU 设备”([图 9.44](#F0225))。选择此项后，你将看到系统中
    GPU 设备的概览。
- en: '![image](../images/F000090f09-44-9780124159334.jpg)'
  id: totrans-1123
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-44-9780124159334.jpg)'
- en: FIGURE 9.44 Parallel Nsight, GPU devices present.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.44 并行 Nsight，存在的 GPU 设备。
- en: This is a useful dialog if you are not sure exactly what the properties of a
    particular device in your system are. Next, change the dropdown menu from “GPU
    Devices” to “CUDA Launches.” You’ll then see a list of kernels that were executed
    and various statistics. You’ll also find “Experimental Results” in the panel below
    the expandable list.
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定系统中特定设备的属性，这个对话框会很有用。接下来，将下拉菜单从“GPU 设备”更改为“CUDA 启动”。然后你将看到已执行的内核列表以及各种统计数据。你还将在可扩展列表下方的面板中找到“实验结果”。
- en: In this particular example, we have six kernels. We can see from the results
    a number of issues. First, none of the kernels achieve a theoretical occupancy
    above 33% ([Figure 9.45](#F0230)). In the case of the first kernel, this is caused
    by the block limit (8) being hit before we’ve achieved the maximum of 48 warps
    that can be resident on the device. Also note that the first kernel does not set
    the cache configuration and the CUDA runtime uses the `PREFER_SHARED` option,
    allocating 48 K to shared memory instead of the cache. As the kernel does not
    use shared memory, this is pointless. We’re missing a call in the host code to
    set to cache configuration to `PREFER_L1` prior to the first kernel call.
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个具体的示例中，我们有六个内核。从结果中我们可以看到一些问题。首先，没有一个内核的理论占用率超过 33%([图 9.45](#F0230))。对于第一个内核，这是由于在达到最大
    48 个 warp 的设备容量之前，块限制（8）已经达到了。还要注意，第一个内核没有设置缓存配置，CUDA 运行时使用了`PREFER_SHARED`选项，将
    48 K 分配给共享内存，而不是缓存。由于该内核没有使用共享内存，这样做毫无意义。在主机代码中，我们缺少在第一次内核调用之前设置缓存配置为`PREFER_L1`的调用。
- en: '![image](../images/F000090f09-45-9780124159334.jpg)'
  id: totrans-1127
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-45-9780124159334.jpg)'
- en: FIGURE 9.45 Parallel Nsight occupancy.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.45 并行 Nsight 占用率。
- en: The next experiment to look at is the “Instruction Statistics” ([Figure 9.46](#F0235)).
    Here we see a few issues. There is a very high level of instructions that are
    being issued but not executed. This is indicative of the SM having to serialize
    and thus reissue the same instructions. We also see a huge spike of activity on
    SM 2\. This is in fact very bad, as it means one of the blocks that were allocated
    to this SM performed a huge amount of additional work compared with the other
    blocks. This indicates the blocks are not equally distributed in terms of work
    per block, and this is something we need to solve at the algorithm level. Some
    balancing of the work per block is needed.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要查看的是“指令统计”实验（[图 9.46](#F0235)）。在这里，我们看到了一些问题。发出的指令数量非常高，但并没有被执行。这表明 SM 必须将指令序列化，从而重新发出相同的指令。我们还看到
    SM 2 上的活动出现了巨大峰值。这实际上是非常糟糕的，因为它意味着分配给该 SM 的某个块相比其他块进行了大量的额外工作。这表明块的工作量分配不均衡，这是我们需要在算法层面解决的问题。需要对每个块的工作量进行一定的平衡。
- en: '![image](../images/F000090f09-46-9780124159334.jpg)'
  id: totrans-1130
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-46-9780124159334.jpg)'
- en: FIGURE 9.46 Parallel Nsight “Instruction Statistics.”
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.46 并行 Nsight “指令统计”。
- en: The next experiment is the “Branch Statistics,” which tells us how much the
    execution within a warp diverges ([Figure 9.47](#F0240)). We ideally want a very
    small if not zero value for branch divergence. Here we see 16% of the branches
    diverge, which contributes to the reissuing of instructions we saw in the “Instruction
    Statistics” experiment. This too originates from the algorithm in that the amount
    of work per thread varies. It points to the need to balance the workload between
    the work blocks.
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个实验是“分支统计”，它告诉我们一个 Warp 内部的执行偏离了多少（[图 9.47](#F0240)）。我们理想情况下希望分支偏离值非常小，甚至为零。在这里，我们看到
    16% 的分支发生了偏离，这导致了我们在“指令统计”实验中看到的指令重新发出。这同样源自算法，因为每个线程的工作量不同。它指出了需要在工作块之间平衡工作负载。
- en: '![image](../images/F000090f09-47-9780124159334.jpg)'
  id: totrans-1133
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-47-9780124159334.jpg)'
- en: FIGURE 9.47 Parallel Nsight “Branch Statistics.”
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.47 并行 Nsight “分支统计”。
- en: The next experiment looks at the ability of the SM to issue and execute instructions.
    We’d expect to see a roughly equal distribute in terms of the “Active Warps per
    Cycle” chart. It shows that despite SM 2 taking a very long time to execute, it
    was actually only given a small number of warps to execute. This confirms that
    it was likely that one of the blocks given to it contained much more work than
    the other blocks. We also have a very low level of “Eligible Warps per Active
    Cycle,” which may in turn suggest the SMs are stalling at some point ([Figure
    9.48](#F0245)).
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个实验考察了 SM 发出和执行指令的能力。我们预计在“每周期活动 Warp 数量”图表中看到大致均衡的分布。图表显示，尽管 SM 2 执行的时间非常长，但它实际上只被分配了少量的
    Warp 来执行。这证实了它很可能是其中一个分配给它的块包含了比其他块更多的工作。我们还看到“每个活动周期的符合条件的 Warp 数量”非常低，这可能表明
    SM 在某个时刻发生了停滞（[图 9.48](#F0245)）。
- en: '![image](../images/F000090f09-48-9780124159334.jpg)'
  id: totrans-1136
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-48-9780124159334.jpg)'
- en: FIGURE 9.48 Parallel Nsight issue efficiency, eligible warps.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.48 并行Nsight问题效率，合适的warps。
- en: Looking at the next tab we see the distribution of instruction dependencies
    ([Figure 9.49](#F0250)). Instruction dependencies are caused by the output of
    one operation feeding into the input of the next. As the GPU uses a lazy evaluation
    model, the GPU operates best with long instruction dependencies.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 看下一个标签，我们看到指令依赖关系的分布（[图9.49](#F0250)）。指令依赖关系是由于一个操作的输出作为输入供下一个操作使用。由于GPU采用延迟求值模型，GPU在长指令依赖关系下运行效果最佳。
- en: '![image](../images/F000090f09-49-9780124159334.jpg)'
  id: totrans-1139
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-49-9780124159334.jpg)'
- en: FIGURE 9.49 Parallel Nsight issue efficiency, instruction dependencies.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.49 并行Nsight问题效率，指令依赖关系。
- en: The graph in [Figure 9.49](#F0250) shows there are too many immediate dependencies.
    The easiest method to solve this is by introducing some ILP on the thread level.
    As we in fact have very few blocks, we have a significant number of unused registers
    that could be used to introduce ILP. We could do this via the vector types or
    by expanding the loop to process *N* elements per iteration. We could also use
    one or more registers to prefetch the values from the next loop iteration.
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.49](#F0250)中的图表显示了有太多的直接依赖关系。解决这个问题最简单的方法是在线程级别引入一些ILP。由于我们实际上只有很少的块，我们有相当数量的未使用寄存器可以用来引入ILP。我们可以通过向量类型或者通过扩展循环，使每次迭代处理*N*个元素来实现。我们还可以使用一个或多个寄存器来预取下一个循环迭代的值。'
- en: The next tab confirms what we saw in the “Eligible Warps” tab, that the SMs
    are in fact hitting a stall condition. The first pie chart in [Figure 9.50](#F0255)
    shows that in 69% of the time, the SM has no eligible warp to execute, meaning
    it will stall or idle, which is of course not good. The second pie chart in [Figure
    9.50](#F0255) shows the reason for the stall, which we can see is 85% of the time
    related to execution dependencies.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个标签确认了我们在“合适的Warp”标签中看到的内容，即SM实际上遇到了停滞条件。[图9.50](#F0255)中的第一个饼图显示，在69%的时间里，SM没有合适的warp可以执行，这意味着它将停滞或空闲，这当然不好。[图9.50](#F0255)中的第二个饼图显示了停滞的原因，我们可以看到85%的停滞时间与执行依赖关系有关。
- en: '![image](../images/F000090f09-50-9780124159334.jpg)'
  id: totrans-1143
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-50-9780124159334.jpg)'
- en: FIGURE 9.50 Parallel Nsight issue efficiency, issue stalls.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.50 并行Nsight问题效率，问题停滞。
- en: This can be solved in one of two ways. Currently, we have only 64 threads per
    block, meaning we get too few warps that are resident (16 out of a possible 48).
    Increasing the number of threads per block will increase the number of resident
    warps. From this perspective only, we’d need to move from 64 to 192 threads per
    block. This in itself may well resolve the issue. However, the effect of this
    issue on the overall timing is significantly less than issues concerning memory.
    Increasing the number of resident blocks will affect cache usage, which may have
    a bigger impact on the overall timing.
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过两种方式解决。目前，我们每个块只有64个线程，这意味着我们获得的驻留warp太少（从48个可能warp中只有16个）。增加每个块的线程数将增加驻留warp的数量。仅从这个角度来看，我们需要将每个块的线程数从64增加到192。这本身可能会解决问题。然而，这个问题对整体时间的影响明显小于与内存相关的问题。增加驻留块的数量会影响缓存的使用，可能对整体时间产生更大的影响。
- en: We can see this in practice by looking at the total amount of data fetched from
    global memory by creating two versions, one that uses 128 threads per block and
    another that uses 64 threads per block. As we have registers to spare, we’ll also
    fetch 16 elements in the 64-register version and 12 elements in the 128-register
    version. This maximizes the register usage while still maintaining eight blocks
    per SM.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看从全局内存中提取的总数据量来实际验证这一点，通过创建两个版本，一个每个块使用128个线程，另一个每个块使用64个线程。由于我们有足够的寄存器空间，我们还将在64寄存器版本中提取16个元素，在128寄存器版本中提取12个元素。这最大化了寄存器的使用，同时仍保持每个SM八个块。
- en: Sure enough the “Warp Issue Efficiency” improves, reducing the “No Eligible”
    warps from 75% down to just 25%. The number of theoretical warps per SM also increases
    from 16 to 32 (13.25 versus 26.96 actual). The occupancy increases from 27% to
    56%. These are all improvements, but they are secondary effects. The kernel is
    performing a sort, so is likely, as with almost all sorts, to be memory bound.
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 果然，“warp效率问题”得到了改善，将“无合适warp”从75%降低到仅25%。每个SM的理论warp数量也从16增加到32（实际值为13.25与26.96）。占用率从27%增加到56%。这些都是改进，但它们是次要的影响。该内核正在执行排序操作，因此像几乎所有排序操作一样，很可能是内存瓶颈。
- en: In fact, when we compare the two kernels with the “CUDA Memory Statistics” experiment,
    there is a difference. The increased number of blocks per SM means that the ratio
    of L1 cache to each block is reduced. This in turn results in a doubling of the
    number of global memory fetch operations that are not cached in the L1 or L2 cache.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们通过“CUDA内存统计”实验对比这两个内核时，存在一些差异。每个SM的块数增加意味着每个块的L1缓存比例减少。这反过来导致了全局内存提取操作的数量翻倍，这些操作没有被缓存到L1或L2缓存中。
- en: In the first kernel, using 64 threads per block, we achieve a 93.7% cache hit
    rate, which is very good ([Figure 9.51](#F0260)). Of the 6.3% of the transactions
    the L1 cache misses, the L2 cache picks up 30%, or around one-third. Thus, very
    few read transactions actually make it to global memory and we stay mostly on
    chip.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个内核中，使用每个块64个线程时，我们达到了93.7%的缓存命中率，这是非常好的（[图 9.51](#F0260)）。在6.3%的事务中，L1缓存未命中，L2缓存拾取了其中的30%，即大约三分之一。因此，实际上很少有读取事务进入全局内存，我们大多数时间都停留在芯片上。
- en: '![image](../images/F000090f09-51-9780124159334.jpg)'
  id: totrans-1150
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-51-9780124159334.jpg)'
- en: FIGURE 9.51 Memory statistics, memory overview (256 blocks × 64 threads).
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.51 内存统计，内存概览（256个块 × 64个线程）。
- en: When we extend this to 128 threads per block, the overall number of blocks halves
    to 128 blocks in total ([Figure 9.52](#F0265)). However, this is not an issue,
    as with 14 SMs on the device and a maximum of eight resident blocks, we can only
    accommodate a maximum of 112 blocks at any given time anyway. Thus, we can increase
    the number of resident warps without any SMs running out of blocks.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将每个块的线程数扩展到128个时，块的总数量减半，变为128个块（[图 9.52](#F0265)）。然而，这并不成问题，因为设备上有14个SM，每个SM最多支持8个驻留块，因此在任何给定时间，我们最多只能容纳112个块。因此，我们可以增加驻留warp的数量，而不会导致任何SM用完块。
- en: '![image](../images/F000090f09-52-9780124159334.jpg)'
  id: totrans-1153
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-52-9780124159334.jpg)'
- en: FIGURE 9.52 Memory statistics, memory overview (128 blocks × 128 threads).
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.52 内存统计，内存概览（128个块 × 128个线程）。
- en: Notice the problem with the cache hit ratio. Both the L1 and L2 caches achieve
    a lower hit ratio than before. The amount of memory fetched from global memory
    approximately doubles from 272 MB to 449 MB. This takes the execution time from
    35 ms to 46 ms, despite the apparent improvements in utilization of the SMs. Note
    that due to the allocation of one thread to each sample block, these memory fetches
    are all uncoalesced, so they are in fact very expensive.
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意缓存命中率的问题。L1和L2缓存的命中率都低于之前的值。从全局内存中提取的内存量大约增加了一倍，从272 MB增加到449 MB。这使得执行时间从35毫秒增加到46毫秒，尽管SM的利用率看起来有所改善。注意，由于每个样本块分配一个线程，这些内存提取都是未合并的，因此实际上是非常昂贵的。
- en: Note that a design in which the threads from a thread block cooperated on sorting
    a single sample block would be far less sensitive to this effect. This analysis
    shows us this dependency. Through using a different mapping of threads to work
    in the sort stage, or by balancing or adjusting the bin boundaries, we may well
    be able to significantly improve the throughput.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果线程块中的线程协作对一个单一的样本块进行排序，设计会对这一效应的敏感度降低得多。这个分析向我们展示了这种依赖性。通过使用不同的线程映射来进行排序阶段的工作，或者通过平衡或调整桶边界，我们可能能够显著提高吞吐量。
- en: Resolving bottlenecks
  id: totrans-1157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决瓶颈
- en: 'It’s all very well knowing what the code you are running is doing, but it’s
    often another matter to both understand and fix the issue. The three types of
    bottlenecks you typically see, in order of importance, are:'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 知道您正在运行的代码在做什么非常好，但理解并修复问题往往是另一回事。您通常会看到的三种瓶颈类型，按重要性排序，分别是：
- en: • PCI-E transfer bottlenecks
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: • PCI-E传输瓶颈
- en: • Memory bandwidth bottlenecks
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: • 内存带宽瓶颈
- en: • Compute bottlenecks
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: • 计算瓶颈
- en: PCI-E transfer bottlenecks
  id: totrans-1162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCI-E传输瓶颈
- en: PCI-E transfer bottlenecks are often a key consideration. As we saw from the
    earlier sections, PCI-E bus bandwidth is limited and you can expect to achieve
    a peak of around 5 GB/s on PCI-E 2.0 depending on the host hardware. However,
    to achieve this peak you need to be using pinned memory and an appropriately sized
    transfer. Adding more GPUs to a node typically reduces the overall bandwidth,
    but allows the overall amount of GPU to be increased. If you can keep everything
    in the GPU memory space, be that a single Tesla GPU or multiple GPUs, then the
    transfer cost can be eliminated from the equation. The extent of the reduction
    in bandwidth by adding more cards is very much dependent on the host hardware.
    You therefore need to be aware of how much data you are transferring and its usage.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: PCI-E传输瓶颈通常是一个关键考虑因素。正如我们在前面的章节中看到的，PCI-E总线带宽是有限的，具体的峰值通常在PCI-E 2.0上为大约5 GB/s，具体取决于主机硬件。然而，要实现这一峰值，您需要使用固定内存，并且传输大小要适当。将更多GPU添加到节点中通常会减少整体带宽，但可以增加GPU的总体数量。如果您能够将所有数据保持在GPU内存空间内，无论是单个Tesla
    GPU还是多个GPU，那么传输成本就可以从方程中消除。增加更多卡片所导致的带宽减少程度在很大程度上取决于主机硬件。因此，您需要了解自己正在传输多少数据以及数据的使用方式。
- en: Compression techniques are one way to increase this apparently hard limit on
    PCI-E transfer rates. Do you really need to transfer all the data you are sending?
    For example, image data often contains an alpha channel that is used for transparency.
    If you are not using this on the GPU, then you can discard it and transfer from
    the host only the RGB (red, green, and blue) components, eliminating 25% of the
    data to be transferred. Although this may then mean you have 24 bits per pixel,
    the transfer time saving may significantly outweigh the nonaligned access pattern
    this might cause.
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩技术是一种增加PCI-E传输速率这一看似硬性限制的方法。您是否真的需要传输所有正在发送的数据？例如，图像数据通常包含一个用于透明度的alpha通道。如果您在GPU上不使用这个通道，那么您可以丢弃它，仅从主机传输RGB（红色、绿色和蓝色）分量，从而减少25%的传输数据量。虽然这可能意味着每像素仍然是24位，但传输时间的节省可能会大大超过这种非对齐访问模式可能带来的影响。
- en: The other question is can you infer some data from others? This is very much
    problem dependent, but you may be able to compress the data using a simple algorithm
    such as run-length encoding. A long series of the same numbers can be replaced
    with a value, count pair and reconstructed at the GPU end in very little time.
    You may have lots of activity from a sensor and then no “interesting” activity
    for quite a period of time. Clearly, you can transfer the “interesting” data in
    full and either throw away the “uninteresting” data at the host end, or transfer
    it in some compressed form.
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是你能否从其他数据中推断出一些信息？这很大程度上取决于问题的具体情况，但你可能可以使用简单的算法，如游程编码，来压缩数据。长时间序列的相同数字可以用一个值-计数对来替代，并在GPU端快速重建。你可能会从传感器获得大量数据，然后长时间没有“有趣”的活动。显然，你可以将“有趣”的数据完整传输，并在主机端丢弃“无趣”的数据，或者以某种压缩形式传输。
- en: Interleaving transfer with computation using streams or zero-copy memory is
    another essential technique we have already covered. In the situation where your
    PCI-E transfer time is in excess of your kernel time, you effectively have the
    computation time for free. Without overlapping, the two times must be added and
    you end up with large gaps where no computation is taking place. See [Chapter
    8](CHP008.html) for more information on using streams.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流或零拷贝内存将传输与计算交错进行是我们已经介绍过的另一项关键技术。在PCI-E传输时间超过内核时间的情况下，实际上你可以免费获得计算时间。如果不进行重叠，两个时间必须相加，这样就会导致大段时间没有进行计算。更多关于使用流的信息，请参见[第8章](CHP008.html)。
- en: PCI-E is not the only transfer bottleneck you need to consider. The host will
    have a limit on the amount of memory bandwidth there is. Hosts such as the Intel
    Sandybridge-E processors use quad-banked memory, meaning they can achieve much
    higher host memory bandwidth than other solutions. Host memory bandwidth can also
    be saved by using P2P (Peer to Peer) transfers if your problem allows for this.
    Unfortunately, at the time of writing, to use the P2P function you need to use
    an OS other than Windows 7\. With the exception of those using Tesla cards and
    thus the TCC (Tesla Compute Cluster) driver, Windows 7 is the only major OS not
    currently supported for this feature.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: PCI-E并不是你需要考虑的唯一传输瓶颈。主机的内存带宽也是有限制的。例如，英特尔Sandybridge-E处理器使用四通道内存，这意味着它们能够比其他解决方案实现更高的主机内存带宽。如果你的问题允许，使用P2P（点对点）传输还可以节省主机内存带宽。不幸的是，在写作本文时，使用P2P功能需要使用除Windows
    7以外的操作系统。除使用Tesla显卡和TCC（Tesla计算集群）驱动的用户外，Windows 7是唯一目前不支持此功能的主流操作系统。
- en: The speed at which the node can load and save data to storage devices, be they
    local devices or network devices, will also be a limiting factor. High-speed SSD
    drives connected in RAID 0 mode will help with this. These are all considerations
    for selecting host hardware. We look at a number of these in detail in [Chapter
    11](CHP011.html).
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 节点加载和保存数据到存储设备的速度，无论是本地设备还是网络设备，都将成为一个限制因素。连接在RAID 0模式下的高速SSD驱动器将有助于此。所有这些都是选择主机硬件时需要考虑的因素。在[第11章](CHP011.html)中，我们会详细讨论这些内容。
- en: Memory bottlenecks
  id: totrans-1169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存瓶颈
- en: Assuming you can get the data on and off the GPU, the next issue is memory bandwidth
    to or from global memory. Moving data is expensive in terms of time and power
    usage. Therefore, being able to efficiently fetch/store and reuse data are essential
    criteria for selecting an appropriate algorithm. The GPU has huge amounts of compute
    resources, so an inefficient algorithm with a memory pattern favorable to a GPU
    (coalesced, tiled, high locality) may outperform a more computationally intensive
    algorithm that exhibits less GPU-friendly memory pattern.
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你能够将数据从GPU传输进出，接下来的问题就是全局内存的带宽。数据传输在时间和能耗上是昂贵的。因此，能够高效地获取/存储和重用数据是选择合适算法的关键标准。GPU拥有大量的计算资源，因此即使是一个内存模式适合GPU（合并、瓦片化、高局部性）的低效算法，可能也会比一个计算密集型但内存模式不适合GPU的算法表现更好。
- en: When considering memory, think also about thread cooperation and appreciate
    the cooperation is best limited to a single block of threads. Generic algorithms
    that assume any thread can talk to any other thread are less useful than those
    that value locality of threads to one another. Algorithms designed for use on
    older vector machines are often far more efficient than those designed around
    distributing work over *N* independent processing nodes, as commonly found in
    today’s cluster machines.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑内存时，也要考虑线程间的协作，并理解协作最好限于单一线程块。假设任何线程都能与其他线程通信的通用算法不如那些重视线程局部性的算法有效。为老式向量机器设计的算法通常比为现代集群机器中分布式工作设计的算法要高效得多，后者通常围绕*N*个独立处理节点展开。
- en: On modern GPUs, the L1 and L2 caches can significantly affect the execution
    time of kernels in sometimes rather unpredictable ways. Shared memory should be
    used where you have data reuse, want a more predictable outcome, or are developing
    for compute 1.x hardware. Even with the full 48 K allocation to the L1 cache,
    there is still 16 K of local shared memory storage available on each SM.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代GPU上，L1和L2缓存可能会显著影响内核的执行时间，且有时这种影响是难以预测的。共享内存应该在数据重用的场景下使用，特别是在你希望结果更可预测，或是开发针对1.x计算硬件时。即使将L1缓存分配到48K，也每个SM（流多处理器）仍然有16K的本地共享内存可用。
- en: A fully populated Fermi GPU has 16 SMs, so this amounts to a total of 256 K
    of high-speed memory in addition to the 768 K of L1 cache. This can be swapped,
    giving 768 K of programmer-managed shared memory and 256 K of L1 cache. Data reuse
    through either or both mechanisms is critical to achieving high throughput. This
    is typically achieved by ensuring locality of the calculation. Instead of multiple
    passes over large datasets, break the data into tiny tiles, use multiple passes
    over individual tiles, and then repeat for the other tiles. This allows the data
    to remain on chip throughout whatever transformation is being made on it, without
    multiple read/writes to and from global memory.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全填充的Fermi GPU有16个SM，因此总共有256K的高速内存，外加768K的L1缓存。这个配置可以进行交换，提供768K的程序员管理共享内存和256K的L1缓存。通过这两种机制中的一种或两种进行数据重用，对于实现高吞吐量至关重要。这通常通过确保计算的局部性来实现。与其对大型数据集进行多次遍历，不如将数据划分为小块，对每个小块进行多次遍历，然后对其他小块重复此过程。这使得数据在进行任何变换时都能保持在芯片上，避免了频繁地读写全局内存。
- en: Memory coalescing is key to achieving high memory throughput, although a sufficiently
    high number of memory transactions is also required. On Fermi and Kepler devices,
    to achieve anything like the full bandwidth when using 32-bit values per thread
    (i.e., floats or integers), you need to have the GPU almost fully populated with
    threads (48 to 64 resident warps, 1536 to 2048 threads per SM). Increased transaction
    sizes through the use of the various vector types help improve both ILP and memory
    bandwidth. Having each thread process four values instead of one tends to work
    well for many applications.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 内存合并是实现高内存吞吐量的关键，尽管还需要足够高数量的内存事务。在Fermi和Kepler设备上，要实现接近满带宽的性能，使用32位值每线程（即浮点数或整数）时，需要几乎完全填充GPU的线程（每个SM有48到64个常驻warp，每个warp有1536到2048个线程）。通过使用各种向量类型增加事务大小，有助于提高指令级并行性（ILP）和内存带宽。让每个线程处理四个值而不是一个，对于许多应用程序来说效果很好。
- en: Compute bottlenecks
  id: totrans-1175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算瓶颈
- en: Complexity
  id: totrans-1176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 复杂性
- en: Surprisingly, despite the immense computing throughput of the GPU, there are
    still problems that are compute bound. These are usually problems where the overall
    amount of data is very large, such as the various forms of medical image scanning
    or data processing from devices that generate large amounts of sample data. These
    types of problems were previously processed on clusters. However, now due to the
    huge processing power available from a multi-GPU computer, many problems can be
    processed on a single standalone PC.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 出人意料的是，尽管GPU拥有强大的计算吞吐量，仍然存在一些计算受限的问题。这些通常是数据量非常大的问题，例如各种形式的医学图像扫描或来自生成大量样本数据的设备的数据处理。此前，这类问题通常是在集群上处理的。然而，现在由于多GPU计算机提供了巨大的处理能力，许多问题可以在单台独立的PC上进行处理。
- en: Algorithms that contain a lot of computations work really well on GPUs compared
    to their CPU counterparts. However, algorithms that also include a lot of control
    complexity do not. Take the example of boundary cells in a typical tiled algorithm.
    If the cells collect data from their immediate neighbors, then a cell at the corner
    of a tile needs to collect data from the corner points of three other tiles.
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CPU 相比，包含大量计算的算法在 GPU 上运行得非常好。然而，包含大量控制复杂性的算法则不然。以典型的瓦片算法中的边界单元格为例。如果单元格收集来自其邻居的数据，那么位于瓦片角落的单元格需要从三个其他瓦片的角点收集数据。
- en: In [Figure 9.53](#F0270) you can see there is a large block of green cells in
    the centre that have no boundary condition. They can safely calculate some value
    from the surrounding cells within the current block. Unfortunately, some programmers
    write programs that deal with the problem cases first. Thus, their kernel goes
    along the lines
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 9.53](#F0270) 中，您可以看到中央有一大块绿色单元格，它们没有边界条件。它们可以安全地从当前块中的周围单元格计算一些值。不幸的是，一些程序员编写的程序会先处理这些问题情况。因此，他们的内核大致如下：
- en: '![image](../images/F000090f09-53-9780124159334.jpg)'
  id: totrans-1180
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000090f09-53-9780124159334.jpg)'
- en: FIGURE 9.53 Halo cells needed.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.53 需要 Halo 单元格。
- en: '[PRE286]'
  id: totrans-1182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: '`else if (right row)`'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: '`else if (right row)`'
- en: '[PRE287]'
  id: totrans-1184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: Particularly, control complex algorithms are not well suited to GPUs. If each
    thread runs the same kernel, the center elements have nine conditions to test
    before the thread does any work on them. Reversing the order of the tests, so
    the center elements are tested first, means we need four boundary tests. This
    would be an improvement, but is still far from optimal. The solution is to write
    customized kernels for each special case or let the CPU handle these complex conditionals.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，控制复杂的算法不适合 GPU。如果每个线程都运行相同的内核，那么中心元素在线程对其进行任何操作之前需要测试九个条件。将测试顺序反转，使中心元素首先进行测试，则需要四个边界测试。这会有所改进，但仍然远非最佳解决方案。解决方案是为每个特殊情况编写定制的内核，或者让
    CPU 处理这些复杂的条件判断。
- en: The type of problem here is a stencil one, where cells *N* levels from the center
    contribute in some way to the result. In this simple example, *N* is 1, as the
    immediate neighbors are used. As *N* is increased, typically some factor is applied,
    as values that are a long way from the center often do not contribute as much
    to the result.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题类型是模板问题，其中距离中心 *N* 层的单元格以某种方式对结果做出贡献。在这个简单的例子中，*N* 是 1，因为只使用了直接的邻居。随着 *N*
    的增大，通常会应用某个因子，因为离中心较远的值往往对结果的贡献较小。
- en: As each cell will need values from the surrounding cells, each cell value will
    be read multiple times. Thus, a common approach to such problems is to use many
    threads to read a tile of data into shared memory. This allows for high-performance
    coalesced access to global memory, both when reading the data and also when writing
    it back. However, shared memory is not visible between blocks and there is no
    mechanism to pass shared data directly between blocks. This is due to the design
    of CUDA where there is only ever a subset of the total number of blocks executing.
    Thus, shared memory is reused as old blocks are retired and new blocks scheduled.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个单元需要来自周围单元的值，每个单元的值会被多次读取。因此，解决此类问题的常见方法是使用多个线程将一块数据读取到共享内存中。这允许高性能的全局内存合并访问，无论是读取数据还是写回数据。然而，共享内存在块之间是不可见的，并且没有机制可以直接在块之间传递共享数据。这是由于CUDA的设计，在该设计中，只有一部分块在执行。因此，共享内存在旧块退休并调度新块时被重用。
- en: Thus, to load the halo cells, the cells outside the boundary of our particular
    tile, you can either read them from global memory or also load these into shared
    memory. Reading the rows from global memory gives a nice coalesced memory pattern.
    However, the columns generate a number of separate memory transactions, one for
    each cell we load. As these cells may be read a number of times, reading the columns
    can be a memory-intensive operation that will limit performance. Thus, at least
    the columns are usually placed into shared memory.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了加载光环单元，即我们特定瓦片边界外的单元，你可以从全局内存读取它们，也可以将它们加载到共享内存中。从全局内存读取行可以得到一个很好的合并内存模式。然而，列会生成多个独立的内存事务，每个事务对应我们加载的一个单元。由于这些单元可能会被读取多次，读取列可能是一个内存密集型操作，会限制性能。因此，通常至少将列放入共享内存中。
- en: Thus, writing multiple kernels is usually a good solution to the problem of
    eliminating the control flow complexity. We can have one kernel that handles corner
    elements, another for rows, another for columns, and another for the center elements.
    If appropriate, each of these can call a common routine that processes the data
    as a series of values, and now the complexity of where the data came from has
    been removed.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编写多个内核通常是消除控制流复杂性问题的好方法。我们可以有一个内核处理角落元素，一个处理行，一个处理列，另一个处理中心元素。如果合适的话，这些内核中的每一个都可以调用一个通用例程，将数据作为一系列值进行处理，这样，数据来源的复杂性就被去除了。
- en: Note that for compute 1.x and compute 2.x different solutions are applicable.
    As compute 1.x hardware has no cache for global memory, each memory transaction
    would generate a considerable amount of latency. Thus, for these devices it can
    make sense to manually cache the necessary data from the surrounding tiles in
    shared memory or give the calculation to the CPU.
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于计算能力1.x和2.x，适用不同的解决方案。由于计算能力1.x的硬件没有全局内存缓存，每个内存事务都会产生相当大的延迟。因此，对于这些设备，将必要的数据从周围的瓦片手动缓存到共享内存中，或者将计算交给CPU处理是有意义的。
- en: However, compute 2.x devices have both an L1 and L2 cache. As each tile will
    have to process its own elements, it’s likely that the tiles above, above left,
    and left will have already been loaded into the cache by previous activity of
    other blocks. The tiles to the right, right bottom, and bottom will usually not
    be present unless there are multiple passes over quite a small dataset. Accessing
    these from global memory will bring them into the cache for the subsequent block.
    You can also explicitly request cache lines be brought into the cache using the
    prefetch PTX instruction (see PTX ISA).
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算2.x设备同时拥有L1和L2缓存。由于每个瓦片都需要处理自己的元素，因此上面、左上方和左边的瓦片可能已经通过其他块的先前活动被加载到缓存中。右侧、右下方和底部的瓦片通常不会出现在缓存中，除非对一个非常小的数据集进行多次遍历。从全局内存访问这些瓦片会将它们带入缓存以供后续块使用。你也可以使用预取PTX指令显式请求将缓存行带入缓存（参见PTX
    ISA）。
- en: As a consequence of the caching, we can eliminate a large amount of the control
    complexity necessary to manage shared memory by simply selecting a 48 K L1 cache
    and not using shared memory at all. Elimination of complexity is often useful
    in speeding up compute bound kernels.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缓存的原因，我们可以通过简单选择一个48 K的L1缓存并完全不使用共享内存，来消除大量管理共享内存所需的控制复杂性。消除复杂性通常对加速计算密集型内核非常有用。
- en: Instruction throughput
  id: totrans-1193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 指令吞吐量
- en: As with many processors, not all instructions take the same amount of time to
    execute on every device. Selecting the correct instruction mix for a given processor
    is something the compiler should be able to perform quite well, but it’s also
    something the programmer needs to be aware of.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 和许多处理器一样，并非所有指令在每个设备上执行的时间都相同。选择正确的指令组合是编译器应该能够很好地执行的任务，但程序员也需要意识到这一点。
- en: First of all, you need to ensure you are targeting the correct binaries for
    your hardware. Ideally, you should have one compute level specification for each
    target hardware platform. In Visual Studio this is done in the project options
    and is something we’ve already covered. For those people using command line it’s
    the `-arch` flag that specifies this.
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要确保你正在为你的硬件选择正确的二进制文件。理想情况下，你应该为每个目标硬件平台拥有一个计算级别的规格说明。在Visual Studio中，这是在项目选项中完成的，我们已经讨论过这个内容。对于使用命令行的人来说，是通过`-arch`标志来指定的。
- en: In terms of single-precision floating-point operations, all compute levels achieve
    a throughput of one instruction per clock, per thread. Remember, however, as this
    is per thread. In absolute terms we need to consider this is warp wide times the
    number of simultaneous warps per SM times the number of SMs on the GPU. Thus on
    Kepler GTX680 we have a 32 wide warp x 8 warp dispatch x 8 SMs = 2048 instructions
    per clock. Now throughput is not the same as instruction latency. It may take
    up to the order of 20 clock cycles for the result to become available to feed
    into a subsequent operation. A series of floating-point operations fed into the
    instruction pipeline would therefore appear 20 cycles later, one each cycle. The
    throughput would be one instruction per cycle, per thread but the latency would
    be 20 cycles.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 就单精度浮点操作而言，所有计算级别都能实现每个线程每时钟周期一条指令的吞吐量。然而，请记住，这指的是每个线程的吞吐量。换句话说，我们需要考虑这是每个 SM
    上的每个 warp 宽度乘以每个 SM 上的同时 warp 数，再乘以 GPU 上的 SM 数。因此，在 Kepler GTX680 上，我们有一个 32
    宽的 warp x 8 个 warp 分派 x 8 个 SMs = 每时钟周期 2048 条指令。现在吞吐量与指令延迟不同。结果可能需要大约 20 个时钟周期才能变得可用，并传递到后续操作中。因此，一系列浮点操作输入到指令流水线中时，会在
    20 个周期后逐个出现。吞吐量是每个线程每个周期一条指令，但延迟是 20 个周期。
- en: Double-precision floating-point hardware, however, does not achieve this. For
    compute 2.0 hardware, it’s half the speed of single precision. For compute 2.1
    hardware, it’s actually only one-third of the speed. Compute 2.1 hardware (GTX460/560)
    and compute 3.0 hardware (GTX680) was aimed more toward the gaming market, so
    it lacks the same level of double-precision floating-point performance.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，双精度浮点硬件并未实现这一点。对于计算 2.0 硬件，其速度仅为单精度的一半。对于计算 2.1 硬件，其速度实际上只有单精度的三分之一。计算 2.1
    硬件（GTX460/560）和计算 3.0 硬件（GTX680）更多是面向游戏市场，因此缺乏相同级别的双精度浮点性能。
- en: We see a similar issue with 32-bit integer values. Add and logical instructions
    only run at full speed. All other integer instructions (multiply, multiply-add,
    shift, compare, etc.) run at half speed on compute 2.0 hardware and one-third
    speed on compute 2.1 hardware. As usual, division and modulus operations are the
    exception. These are expensive on all compute levels, taking “tens of instructions”
    on compute 1.x hardware and “below 20 instructions” on compute 2.x hardware [NVIDIA
    CUDA C Programming Guide, v4.1, [chapter 5](CHP005.html)].
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 32 位整数值上看到类似的问题。加法和逻辑指令仅以全速运行。所有其他整数指令（乘法、乘法加法、移位、比较等）在计算 2.0 硬件上以半速运行，在计算
    2.1 硬件上以三分之一速度运行。像往常一样，除法和取模操作是例外。这些操作在所有计算级别上都是昂贵的，在计算 1.x 硬件上需要“几十条指令”，在计算 2.x
    硬件上则需要“不超过 20 条指令”【NVIDIA CUDA C 编程指南，v4.1，[第 5 章](CHP005.html)】。
- en: Type conversion instructions operate at half speed on compute 2.0 devices and
    one-third speed on compute 2.1 devices. These are necessary when 8- or 16-bit
    integer types are used, as the hardware supports only native integer types (32-bit
    on compute 2.x, 24-bit on compute 1.x). Thus, the addition of two byte values
    results in promotion of these values to two integer values. The subsequent result
    then again needs to be demoted to a byte value. Similarly, conversions to and
    from single-/double-precision floating-point values cause additional type conversion
    instructions to be inserted.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 类型转换指令在计算 2.0 设备上的运行速度为一半，在计算 2.1 设备上的运行速度为三分之一。当使用 8 位或 16 位整数类型时，这是必要的，因为硬件只支持原生整数类型（计算
    2.x 为 32 位，计算 1.x 为 24 位）。因此，将两个字节值相加会导致这些值提升为两个整数值。随后的结果则需要再次降级为字节值。同样，转换为单/双精度浮动点值会导致插入额外的类型转换指令。
- en: In C all whole numbers are by default signed integers. All numbers containing
    a decimal place are treated as double-precision floating-point values unless an
    `F` postfix is placed immediately after the number. Thus,
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C 语言中，所有整数默认都是有符号整数。所有包含小数点的数字除非紧跟一个 `F` 后缀，否则被视为双精度浮动点值。因此，
- en: '[PRE288]'
  id: totrans-1201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: creates a double-precision definition and
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: 创建双精度定义并
- en: '[PRE289]'
  id: totrans-1203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: creates a single-precision definition.
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 创建单精度定义。
- en: Using a non-postfixed constant in a floating-point expression causes an implicit
    conversion to double precision during the calculation. An implicit conversion
    to single precision is also performing when the result is assigned to a single-precision
    variable. Thus, forgetting to use the `F` postfix is a common cause of creating
    unnecessary conversion instructions.
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: 在浮动点表达式中使用未加后缀的常量会导致在计算过程中隐式转换为双精度。当结果赋值给单精度变量时，也会执行隐式转换为单精度。因此，忘记使用 `F` 后缀是导致产生不必要的转换指令的常见原因。
- en: Synchronization and atomics
  id: totrans-1206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 同步和原子操作
- en: Synchronization points are often necessary in many algorithms. Synchronization
    within a thread block is not costly, but does potentially impact performance.
    The CUDA scheduler will try to schedule up to sixteen blocks per SM, which it
    can do unless you start using larger numbers of threads (see [Chapter 5](CHP005.html)).
    As the number of threads increases, the number of blocks that can be scheduled
    decreases. This in itself is not too bad, but when combined with synchronization
    points it can lead to the SM stalling.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多算法中，通常需要同步点。线程块内的同步开销不大，但可能会影响性能。CUDA 调度器会尝试每个 SM 调度最多十六个块，除非你开始使用更多线程（见
    [第 5 章](CHP005.html)）。随着线程数量的增加，可以调度的块数会减少。虽然这本身不算太糟，但如果与同步点结合使用，则可能导致 SM 阻塞。
- en: When a block performs a synchronization, a number of warps out of the available
    set (24 on compute 1.x, 48 on compute 2.x, 64 on compute 3.x) effectively drop
    out of the scheduling availability, as all but the last warp hits the synchronization
    point. In the extreme case of 1024 threads per block (two blocks per SM), up to
    half of the resident warps would be at the synchronization barrier. Without any
    ILP, the ability of the SM to hide memory latency through running multiple threads
    then becomes insufficient. The SM stops running at peak efficiency. Clearly, we
    want maximum throughput from all the SMs for as much time as possible.
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个块执行同步时，一部分 warps 会从可用集合中“掉出”（在 compute 1.x 上为 24，在 compute 2.x 上为 48，在 compute
    3.x 上为 64），因为除了最后一个 warp 外，其他都达到了同步点。在每个块有 1024 个线程的极端情况下（每个 SM 有两个块），最多有一半的驻留
    warps 会停留在同步屏障处。如果没有任何指令级并行（ILP），SM 通过运行多个线程来隐藏内存延迟的能力就会变得不足。SM 无法继续以最佳效率运行。显然，我们希望尽可能长时间地从所有
    SM 中获得最大吞吐量。
- en: The solution to the synchronization issue is not to use large thread blocks.
    You should aim to fully populate the SM where possible, so 192 threads is an ideal
    number, which results in eight blocks per SM on compute 2.x hardware, 256 being
    better for compute 3.x hardware.
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: 解决同步问题的办法不是使用大的线程块。你应该尽量使 SM 完全填满，因此 192 个线程是理想的数量，这在 compute 2.x 硬件上会导致每个 SM
    有 8 个块，而在 compute 3.x 硬件上 256 个线程则更好。
- en: Unfortunately, if we’re using interthread synchronization it is likely we’ll
    also need interblock synchronization. It’s more efficient to synchronize data
    between threads than between blocks. For block-based synchronization we need to
    use global memory, whereas interthread synchronization can be performed with shared
    memory. Thus, it’s a tradeoff between the two scenarios best resolved by simply
    running both and seeing which is the fastest.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果我们使用线程间同步，那么很可能也需要块间同步。线程之间同步比块之间同步更高效。对于基于块的同步，我们需要使用全局内存，而线程间同步则可以通过共享内存来完成。因此，这是两种方案之间的权衡，最好的解决办法是同时运行两者，看看哪种速度更快。
- en: Atomic operations act very much like synchronization points in that all the
    threads in a warp have to line up one after another to perform the operation.
    It takes time for all the threads in a block to line up in groups of 32 to move
    through the atomic operation. However, unlike synchronization points, they are
    free to continue at full speed afterward. This helps in terms of increasing the
    availability of warps that can be run, but doesn’t help the overall execution
    time of the block. The block cannot be retired from the SM until all the threads
    have completed. Thus, a single atomic operation effectively serializes and spreads
    out, in terms of execution time, the warps in a given block. The block can’t finish
    until all the stragglers have completed.
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: 原子操作的行为很像同步点，因为一个 warp 中的所有线程必须依次排队才能执行该操作。所有线程在一个块中排成 32 组，以通过原子操作需要一定时间。然而，与同步点不同的是，执行完成后它们可以继续全速运行。这有助于提高可运行
    warp 的数量，但对整个块的执行时间没有帮助。直到所有线程都完成，块才能从 SM 中移除。因此，单个原子操作实际上是串行化并在执行时间上分散了给定块中的
    warp。直到所有滞后的线程完成，块才算完成。
- en: The effect of synchronization and atomics on your kernel can be seen using the
    “CUDA Issue Efficiency” experiment within Parallel Nsight.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在 Parallel Nsight 中使用“CUDA 问题效率”实验来观察同步和原子操作对内核的影响。
- en: Control flow
  id: totrans-1213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 控制流
- en: As we saw earlier, branch divergence can have a serious impact on execution
    time as both paths have to be executed separately. The compiler is aware of this
    and thus uses something called predication.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前看到的，分支分歧会对执行时间产生严重影响，因为两条路径必须分别执行。编译器知道这一点，因此使用了所谓的条件执行（predication）。
- en: Most of the PTX instructions can be predicated using the `.p` notation of the
    PTX ISA. For example,
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 PTX 指令可以使用 PTX ISA 的 `.p` 标记进行条件执行。例如，
- en: '[PRE290]'
  id: totrans-1216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: Here we set up a predicate register in each thread, testing virtual register
    295 for the value 1 and setting predicate register 16 accordingly. In the next
    instruction the predicate register 16 is used to predicate the `bra` (branch to)
    instruction. Thus, only those threads meeting the test condition of the earlier
    `setp.eq.s32` instruction follow the branch. We could replace the branch with
    a `mov` or similar instruction. Typically, you see the compiler generate this
    for small `if-else` constructs. For example,
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在每个线程中设置一个谓词寄存器，测试虚拟寄存器 295 的值是否为 1，并相应地设置谓词寄存器 16。在下一条指令中，谓词寄存器 16 用于对
    `bra`（跳转到）指令进行条件判断。因此，只有那些满足先前 `setp.eq.s32` 指令测试条件的线程才会跟随跳转。我们可以将分支指令替换为 `mov`
    或类似的指令。通常，编译器会为小型 `if-else` 结构生成这种代码。例如，
- en: '[PRE291]'
  id: totrans-1218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: will be translated to
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: 将被翻译为
- en: '[PRE292]'
  id: totrans-1220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE292]'
- en: 'This works well in avoiding branches, as in fact all threads in the warp execute
    the predicate instruction, but those threads without the predicate bit set simply
    ignore it. The compiler has a strong preference for predication, even when other
    approaches would be better. The criteria is simply based on the size of the body
    of the `if` statement. Consider the following example:'
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式在避免分支时效果良好，因为事实上所有线程都会执行谓词指令，但没有设置谓词位的线程会忽略它。即使其他方法可能更好，编译器仍然偏好使用谓词。其标准仅基于`if`语句体的大小。考虑以下示例：
- en: '[PRE293]'
  id: totrans-1222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '[PRE294]'
  id: totrans-1223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: '[PRE295]'
  id: totrans-1224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: '[PRE296]'
  id: totrans-1225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: This code simply selects one of *N* local variables (registers) based on an
    index. The local variables are individually named, as creating an array causes
    the compiler to place this into local memory. Unfortunately, the compiler implements
    a series of `if-else-if` type statements, which means at element 16 we have to
    perform 15 prior tests. I’d have expected it to implement a jump table, creating
    an assignment at the target of each jump. This would be two instructions, load
    `local_idx` into a register and then an indirect jump to some base address plus
    the value in the register. The jump table itself is set up at compile time.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码简单地根据索引选择一个*N*局部变量（寄存器）。局部变量有各自的名称，因为创建数组会导致编译器将其放入局部内存中。不幸的是，编译器实现了一系列`if-else-if`类型的语句，这意味着在元素
    16 时，我们需要先执行 15 次测试。我本希望它能实现跳转表，在每个跳转目标创建一个赋值。这样会是两条指令，首先将`local_idx`加载到寄存器中，然后执行间接跳转，跳转到某个基地址加上寄存器中的值。跳转表本身是在编译时设置的。
- en: Thus, you need to ensure the control flow you expect is the control flow the
    compiler generates. You can do this relatively easily by inspecting the PTX code
    and/or the actual target code if you are still unsure. Predication works well
    in many but not all instances.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要确保你预期的控制流与编译器生成的控制流一致。如果你仍然不确定，可以通过检查 PTX 代码和/或实际目标代码来相对容易地做到这一点。谓词在许多情况下都能很好地工作，但并非所有情况。
- en: Section summary
  id: totrans-1228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小节总结
- en: • Use profiling tools to really see into what is happening as opposed to what
    you *think* is happening.
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用性能分析工具深入了解实际发生的情况，而不是你*认为*发生的情况。
- en: • Avoid overly complex kernels by generating a general case and exception case
    kernel, or by using the caching features to eliminate the complex kernel altogether.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: • 避免使用过于复杂的内核，可以通过生成通用情况和例外情况内核，或者使用缓存功能来完全消除复杂内核。
- en: • Understand how predication works in control flow.
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: • 了解谓词在控制流中的工作原理。
- en: • Don’t assume the compiler will provide the same scope of optimizations found
    with more mature compilers. CUDA is still quite new and things will take time.
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: • 不要假设编译器会提供与更成熟的编译器相同范围的优化。CUDA 仍然相对较新，事情需要时间发展。
- en: 'Strategy 7: Self-Tuning Applications'
  id: totrans-1233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 7：自调节应用
- en: GPU optimization is not like CPU optimization. Many techniques overlap, while
    others have undesirable effects. I’ve tried to cover the major areas of optimization
    in the preceding sections. However, optimization is never an exact science, not
    when practiced by human programmers anyway. There are lots of factors that need
    to be considered when designing code for the GPU. Getting an optimal solution
    is not easy and it takes considerable time to become familiar with what works,
    try different solutions, and understand why one works when another doesn’t.
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 优化与 CPU 优化不同。许多技术是重叠的，而其他技术则可能产生不良影响。我在前面的章节中尝试涵盖了优化的主要领域。然而，优化从来都不是一门精确的科学，尤其是对于程序员来说。设计
    GPU 代码时需要考虑许多因素。获得最优解并不容易，而且需要相当长的时间才能熟悉哪些方法有效，尝试不同的解决方案，并理解为什么某些方法有效而其他方法无效。
- en: 'Consider some of the major factors:'
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一些主要因素：
- en: • Transfer to and from the host.
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: • 主机与设备之间的数据传输。
- en: • Memory coalescing.
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: • 内存合并。
- en: • Launch configuration.
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: • 启动配置。
- en: • Theoretical and achieved occupancy.
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: • 理论和实际的占用。
- en: • Cache utilization.
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: • 缓存利用率。
- en: • Shared memory usage/conflicts.
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: • 共享内存的使用/冲突。
- en: • Branch divergence.
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: • 分支分歧。
- en: • Instruction-level parallelism.
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: • 指令级并行性。
- en: • Device compute level.
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: • 设备计算层级。
- en: For someone starting out with CUDA, there is a lot to think about and it will
    take time to become proficient with each of these areas. However, the most challenging
    aspect of this is that what works on one device many not work on another. Throughout
    this book we’ve used the whole range of available devices and a number of different
    host platforms where necessary to highlight differences.
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于刚开始接触 CUDA 的人来说，需要考虑的因素很多，熟练掌握每个领域将需要时间。然而，最具挑战性的方面在于，在一台设备上有效的方案可能在另一台设备上无法奏效。在本书中，我们使用了各种可用的设备和不同的主机平台，以突出这些差异。
- en: In the same way as different CPUs provide different levels of performance and
    functionality, so do GPUs. The CPU world is largely stuck with an x86 architecture,
    which reflects design goals of a system designed to run serial programs. There
    have been many extensions to provide additional functionality, such as MMX, SSE,
    AVX, etc. The x86 instruction set is today translated within the hardware to micro-instructions,
    which can be really for any target hardware. Sandybridge is perhaps the best example
    of this, where the micro-instructions themselves are actually cached instead of
    the x86 assembly code instructions.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: 就像不同的 CPU 提供不同的性能和功能一样，GPU 也是如此。CPU 世界大多局限于 x86 架构，这一架构反映了设计目标，即运行串行程序的系统。为了提供额外的功能，出现了许多扩展，例如
    MMX、SSE、AVX 等。如今，x86 指令集在硬件内部被转换为微指令，这些微指令实际上可以应用于任何目标硬件。Sandybridge 也许是最好的例子，其中微指令本身实际上是缓存的，而不是
    x86 汇编代码指令。
- en: GPU hardware is also not fixed and has changed significantly since the first
    CUDA-enabled devices were released back in the GTX8800 times. CUDA compiles to
    PTX, a virtual assembly code, aimed at a parallel processor–like architecture.
    PTX can itself be compiled to many targets, including CPUs, as the cooperative
    thread array concept lends itself to implementation in most parallel hardware.
    However, as far as we’re concerned, it’s compiled to a specified compute level
    for various NVIDIA GPUs. Therefore, you need to be familiar with what a given
    compute level provides, that is you need to understand for what hardware you are
    writing code. This has always been the basis of good optimization. Trends toward
    abstraction, layering, and hiding the architecture are all aimed at programmer
    productivity, but often at the expense of performance.
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 硬件也不是固定的，自从 GTX8800 时代首批支持 CUDA 的设备发布以来，已经发生了显著变化。CUDA 编译成 PTX，一种虚拟汇编代码，旨在适用于并行处理器架构。PTX
    本身可以编译成多个目标，包括 CPU，因为协作线程数组的概念便于在大多数并行硬件中实现。然而，就我们而言，它会编译成适用于各种 NVIDIA GPU 的指定计算级别。因此，你需要了解一个给定的计算级别提供什么，即你需要理解你在为哪种硬件编写代码。这一直是良好优化的基础。抽象、分层和隐藏架构的趋势都旨在提高程序员的生产力，但往往以牺牲性能为代价。
- en: Not every programmer is interested in the intricate workings of the hardware.
    Even with the previous list of issues to consider you’re unlikely to get an optimal
    solution the first time, the second time, or the *N*th time without considerable
    thought and a lot of trial and error. Thus, one approach to this issue that works
    well is simply to ask the program to work out the best use of the hardware for
    a given problem. This can either be done on a small set of the problem or the
    real problem itself.
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个程序员都对硬件的复杂工作原理感兴趣。即使有了前面提到的需要考虑的问题清单，你也不太可能第一次、第二次，甚至是 *第 N 次* 就得到最优解，这需要经过深思熟虑和大量的反复尝试。因此，一个有效的解决方案是让程序自己计算在给定问题上如何最好地使用硬件。你可以在问题的一个小集合上，或直接在实际问题上进行此操作。
- en: Identifying the hardware
  id: totrans-1249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件识别
- en: The first step in any optimization process is to know what hardware is available
    and what it is. To find out how many GPUs we have, you simply call
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: 任何优化过程的第一步是了解可用的硬件以及它的具体情况。要找出我们有多少个 GPU，你只需调用
- en: '[PRE297]'
  id: totrans-1251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: This sets whatever parameter you pass as `count` to the number of devices available.
    If there is no CUDA hardware available the function returns `cudaErrorNoDevice`.
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把你传入的 `count` 参数设置为可用设备的数量。如果没有 CUDA 硬件可用，函数将返回 `cudaErrorNoDevice`。
- en: Then for each device found we need to know what its capabilities are. For this
    we call
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个找到的设备，我们需要知道它的能力。为此，我们调用
- en: '[PRE298]'
  id: totrans-1254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: 'We covered in detail the properties of a device in [Chapter 8](CHP008.html)
    so will not repeat this here. You should, however, be interested in at least the
    following:'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 8 章](CHP008.html)中详细介绍了设备的属性，因此这里不再重复。然而，你应该至少对以下内容感兴趣：
- en: • Members `major` and `minor` that, when combined, provide the compute level
    of the device.
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: • 成员 `major` 和 `minor`，结合起来可以提供设备的计算能力等级。
- en: • The `integrated` flag, especially when combined with the `canMapHostMemory`
    flag. This allows you to use zero-copy memory (covered in Strategy 3) and avoid
    memory copies to and from the device for devices of which the GPU memory is actually
    on the host.
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: • `integrated` 标志，特别是与 `canMapHostMemory` 标志结合使用时。这允许你使用零拷贝内存（详见策略 3），避免在设备和主机之间进行内存拷贝，适用于
    GPU 内存实际上位于主机上的设备。
- en: • The `totalGlobalMem` value so you can maximize the use of GPU memory and ensure
    you don’t try to allocate too much memory space on the GPU.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: • `totalGlobalMem` 值，以便你能够最大化 GPU 内存的使用，并确保不会试图在 GPU 上分配过多的内存空间。
- en: • The `sharedMemPerBlock` value so you know how much shared memory is available
    per SM.
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: • `sharedMemPerBlock` 值，让你了解每个 SM 可用的共享内存量。
- en: • The `multiProcessorCount`, which is the number of SMs present in the device.
    Multiply this number by the number of blocks you are able to run on an SM. The
    occupancy calculator, the Visual Profiler, and Parallel Nsight will all tell you
    the number of blocks you can run for a given kernel. It’s typically up to eight
    but can be as many as 16 on Kepler. This is the minimum number of blocks you need
    to schedule to this GPU.
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: • `multiProcessorCount`，即设备中存在的 SM 数量。将这个数字乘以你能够在一个 SM 上运行的块数。占用率计算器、Visual
    Profiler 和 Parallel Nsight 都会告诉你每个给定内核可以运行的块数。通常最多是 8，但在 Kepler 上最多可以达到 16。这是你需要调度到该
    GPU 的最小块数。
- en: 'This information gives us some bounds with which we can define the problem
    space. We then have two choices: either analyze offline the best solution or try
    to work it out at runtime. The offline approach generally leads to better results
    and can greatly increase your understanding of the issues involved and may cause
    you to redesign certain aspects of the program. The runtime approach is necessary
    for optimal performance, even after significant analysis has taken place.'
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息为我们提供了一些边界，通过这些边界我们可以定义问题空间。然后，我们有两个选择：要么离线分析最佳解决方案，要么尝试在运行时解决。离线方法通常会导致更好的结果，并且能够大大增加你对所涉及问题的理解，甚至可能促使你重新设计程序的某些方面。而运行时方法则是在经历了大量分析之后，仍然是实现最佳性能所必需的。
- en: Thus, the first part of the optimization takes place offline, during the development
    phase. If you are targeting multiple compute levels, you’ll need a suitable card
    to test your application on. For consumer cards as a whole the most popular NVIDIA
    cards have always been the 9800 (compute 1.1), 8800 (compute 1.0), GTX260 (compute
    1.3), and GTX460 (compute 2.1). For more modern DirectX 11 cards, the 460/560
    cards dominate, with a smaller number of power users opting for the more expensive
    470/570 cards. Our choice of hardware for this book pretty much reflects the market
    trends to make the figures presented as useful as possible for people developing
    mass-market applications.
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，优化的第一部分发生在离线阶段，即开发阶段。如果你针对多个计算级别进行优化，你将需要一块合适的卡来测试你的应用程序。对于消费者显卡，最受欢迎的NVIDIA卡包括9800（计算1.1）、8800（计算1.0）、GTX260（计算1.3）和GTX460（计算2.1）。对于现代的DirectX
    11显卡，460/560系列显卡占主导地位，而少数高端用户选择更昂贵的470/570显卡。本书选择的硬件基本上反映了市场趋势，以使所呈现的图表对于开发大众市场应用程序的人员尽可能有用。
- en: As we’ve been working with CUDA since it release on the 8800 series of cards,
    we have a number of consumer cards at hand. Clearly, many of these are no longer
    available for sale but can easily be purchased on eBay or elsewhere. All you need
    is a motherboard with four dual-spaced PCI-E connectors all running at the same
    speed when fully populated. The primary board used in the development of this
    book was the (AMD) MSI 790FX-GD70, although this has now been replaced with the
    MSI 890FXX-G70\. Note the newest 990FX board in the series no longer provides
    four double-spaced connectors.
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 自从CUDA在8800系列显卡上发布以来，我们一直在使用它，因此我们手头有许多消费者显卡。显然，其中许多显卡现在已经不再销售，但可以轻松在eBay或其他地方购买。你所需要的仅仅是一块具有四个双间距PCI-E连接器的主板，并且在完全安装时它们都能以相同的速度运行。本书开发过程中使用的主要主板是(AMD)
    MSI 790FX-GD70，尽管现在它已经被MSI 890FXX-G70所替代。请注意，系列中最新的990FX主板不再提供四个双间距的连接器。
- en: Device utilization
  id: totrans-1264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设备利用率
- en: Having identified what hardware we have, we then have to make use of it. If
    there are multiple GPUs in the system, as is often the case, then be sure to make
    use of them. Multi-GPU programming, as of the CUDA 4.x SDK, is now much easier
    than before, so be sure you are not leaving a 100% performance gain on the table
    because you’re only using a single GPU. See [Chapter 8](CHP008.html) for more
    information on this.
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了我们所拥有的硬件之后，我们还需要充分利用它。如果系统中有多个GPU，像常见的那样，那么务必加以利用。自CUDA 4.x SDK起，多GPU编程变得比以前容易得多，因此请确保不要因只使用单个GPU而错过100%的性能提升。有关此方面的更多信息，请参见[第8章](CHP008.html)。
- en: All applications are different, so the same primary performance factors may
    not always be the same. However, many will be. Primary among these is the launch
    configuration. The first part of this is ensuring you have multiple targets set
    up in the build process, one target for each compute level you plan on supporting.
    The target code will automatically be selected based on which GPU you are running
    the kernel on. Make sure also before running any performance tests you have the
    “Release” version selected as the build target, something in itself that can provide
    up to a 2× performance improvement. You’re not going to release the debug version,
    so don’t select this as your build target, other than for testing.
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: 所有应用程序都是不同的，因此相同的主要性能因素可能并不总是相同的。然而，许多因素是相似的。其中最主要的是启动配置。首先，确保在构建过程中设置了多个目标，每个目标对应你计划支持的每个计算级别。目标代码将根据你运行内核的GPU自动选择。在运行任何性能测试之前，确保选择了“Release”版本作为构建目标，这本身就能提供高达2倍的性能提升。你不会发布调试版本，因此除非是测试，否则不要将其选择为构建目标。
- en: Next we need some sort of check to ensure correctness. I suggest you run the
    GPU code back to back with the CPU code and then do a memory compare (`memcmp`)
    on the output of the two identical tests. Note this will detect any error, even
    if the error is not significant. This is especially the case with floating point,
    as the order in which the operations are combined will cause small rounding/precision
    errors. In such cases your check needs to iterate through both results and see
    if the answers differ by whatever you consider to be significant (0.01, 0.001,
    0.0001, etc.) for your particular problem.
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要某种检查来确保正确性。我建议你将GPU代码与CPU代码进行逐一对比，然后对两个相同测试的输出结果进行内存比较（`memcmp`）。注意，这将检测到任何错误，即使错误不显著。特别是在浮点运算中，操作的顺序会导致微小的舍入/精度误差。在这种情况下，你的检查需要遍历两个结果，并查看答案是否有差异，差异大小应根据你特定问题的需求来判断（如0.01、0.001、0.0001等）。
- en: 'In terms of launch configuration we’re trying to optimize for the following:'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: 就启动配置而言，我们的目标是优化以下几个方面：
- en: • Number of threads per block.
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个块的线程数。
- en: • Overall number of blocks.
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: • 总体块数。
- en: • Work performed per thread (ILP).
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个线程执行的工作量（ILP）。
- en: The answer for each of these will vary between compute levels. A simple `for`
    loop is all that is needed to iterate through all possible combinations and record
    the timings for each. Then at the end simply print a summary of the results.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些问题的答案，将根据计算级别的不同而有所变化。只需要一个简单的`for`循环，就能遍历所有可能的组合并记录每个组合的时间。最后，只需打印结果摘要即可。
- en: In terms of threads per block, start at 1 and increase in powers of two until
    you reach 16\. Then increase the thread count in 16-step intervals until you reach
    512 threads per block. Depending on the kernel resource usage (registers, shared
    memory) you may not be able to reach 512 threads on the earlier compute devices,
    so scale this back as necessary for these devices only.
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个块的线程数方面，从1开始，并按照2的幂次方递增，直到达到16。然后以16为步长增加线程数，直到达到每个块512个线程。根据内核资源的使用情况（寄存器、共享内存），在较早的计算设备上，可能无法达到512个线程，因此仅在这些设备上根据需要进行缩减。
- en: Note that we chose 16 here as the increment value, rather than 32, the warp
    size. This is because warp divergence is half-warp based. Certain devices such
    as the GTX460s are actually based on three sets of 16 CUDA cores, rather than
    two as found in other compute levels. Thus, a number of threads that is a multiple
    of 48 may work better on such devices.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里选择了16作为增量值，而不是32，即warp大小。这是因为warp分歧是基于半warp的。某些设备，如GTX460，实际上基于三组16个CUDA核心，而不是其他计算级别中常见的两组。因此，48的倍数线程数在这类设备上可能表现得更好。
- en: As a general rule, you’ll find well-written kernels work best with 128, 192,
    or 256 threads per block. You should use a consistent scaling from one thread
    per block up to a peak point where the performance will level off and then fall
    away. The plateau is usually hit when you achieve the maximum number of resident
    warps per SM and thus the instruction and memory latency hiding is working at
    its peak.
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你会发现写得好的内核在每个块128、192或256个线程时表现最佳。你应当采用从每块一个线程开始，逐步增加线程数，直到性能达到顶峰并开始平稳下降的方式。这个平台通常出现在你达到每个SM的最大驻留warp数时，从而指令和内存延迟隐藏工作达到顶峰。
- en: Using a slightly smaller number of threads (e.g., 192 instead of 256) is often
    desirable if this increases the number of resident blocks per SM. This usually
    provides for a better instruction mix, as more blocks increases the chance they
    will not all hit the same resource contention at the same time.
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用稍小的线程数（例如，使用192而非256个线程），通常是有利的，前提是这样可以增加每个SM上驻留的块数。这通常可以提供更好的指令混合，因为更多的块增加了它们不会同时遇到同一资源争用的机会。
- en: If you are hitting the maximum performance at 16, 32, or 64 threads then this
    usually indicates there is a contention issue, or that your kernel is highly geared
    toward ILP and you are using a lot of registers per thread.
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在16、32或64个线程时达到了最大性能，通常表示存在资源争用问题，或者内核高度依赖指令级并行（ILP），并且每个线程使用了大量寄存器。
- en: Once you have a baseline figure for the ideal number of threads per block, try
    increasing the amount of work done by each thread to two or four elements using
    the various `vector_N` types (e.g., `int2`, `int4`, `float2`, `float4`, etc.).
    You’ll typically see this will improve performance further. The easiest way of
    doing this is to create additional functions with the same name and simply overload
    the kernel function. CUDA will call the appropriate kernel depending on the type
    passed to it at runtime.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了理想的每个块线程数量的基准值，试着将每个线程的工作量增加到两个或四个元素，使用各种 `vector_N` 类型（例如，`int2`、`int4`、`float2`、`float4`
    等）。你通常会发现这会进一步提高性能。最简单的方式是创建具有相同名称的额外函数，直接重载内核函数。CUDA 会根据运行时传递的类型调用相应的内核。
- en: Using the vector types will increase register usage, which in turn may decrease
    the number of resident blocks per SM. This in turn may improve cache utilization.
    Memory throughput will also likely be increased as the overall number of memory
    transactions falls. However, kernels with synchronization points may suffer as
    the number of resident blocks drops and the SM has less choice of which warps
    are available to execute.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量类型会增加寄存器的使用，从而可能减少每个 SM 上的驻留块数。这反过来可能会提高缓存利用率。内存吞吐量也可能会提高，因为总体内存事务数减少。然而，带有同步点的内核可能会受到影响，因为驻留块数减少，SM
    可执行的 warp 选择变少。
- en: As with many optimizations, the outcome is difficult to predict with any degree
    of certainty, as some factors play in your favor while others don’t. The best
    solution is to try it and see. Then work backwards, to understand what factor(s)
    are the primary ones and which are secondary. Don’t waste your time worrying about
    secondary factors unless the primary ones are already addressed.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多优化一样，结果很难以任何程度的确定性来预测，因为一些因素有利于你，而另一些则不然。最好的方法是尝试并观察。然后反向推导，理解哪些因素是主要的，哪些是次要的。除非已经解决了主要因素，否则不要浪费时间担心次要因素。
- en: Sampling performance
  id: totrans-1281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能采样
- en: The final part of a self-tuning application is sampling. Although you can build
    a good performance model around compute level and number of SMs, there are many
    other factors. The same card model may be produced using GDD3 and GDD5 memory,
    the latter having significantly more global memory bandwidth. The same card may
    be clocked internally at 600 MHz yet also appear as a 900 MHz model. An optimization
    strategy that works well for a card with 16 SMs may not work well on one with
    half that number and vice versa. A mobile processor in a laptop may have been
    put on a PCI-E X1 link and may have dedicated or shared memory with the host.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 自调节应用程序的最后一个部分是采样。尽管你可以围绕计算级别和 SM 数量建立一个良好的性能模型，但还有许多其他因素。相同的卡片型号可能使用 GDD3 和
    GDD5 内存生产，后者具有显著更高的全局内存带宽。相同的卡片可能内部时钟为 600 MHz，但也可能表现为 900 MHz 型号。适用于 16 个 SM
    的优化策略可能不适用于只有一半 SM 数量的卡片，反之亦然。笔记本中的移动处理器可能已经通过 PCI-E X1 链接连接，并且可能与主机共享或独立内存。
- en: It’s impossible to collect every card and address every variation that your
    product might have to address. Even if you could do this, next week NVIDIA will
    release another card. This is of course mostly a problem for those people writing
    consumer applications, rather than the somewhat less diverse Tesla population
    of cards. Nonetheless, when a new card is released people first expect their existing
    applications to run on it, and second, if they have upgraded, to see a suitable
    performance boost.
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能收集每张卡片并处理你的产品可能需要解决的所有变种。即使你能做到这一点，下周NVIDIA也会发布另一张卡片。当然，这主要是那些编写消费级应用程序的人的问题，而不是较少多样化的Tesla卡片群体。然而，当新卡发布时，人们首先期望他们现有的应用程序能在新卡上运行，其次，如果他们进行了升级，还希望看到性能的提升。
- en: Sampling is the answer to this issue. Each card will have a peak value in terms
    of a launch configuration that works best for its particular setup. As we’ve seen
    in some of the tests run throughout this book, different cards prefer different
    setups. The Fermi cards work well with 192 or 256 threads per block, yet the prior
    GPUs work well with 128 and 192 threads per block. The compute 2.1 cards perform
    best with 64- or 128-byte memory fetches, mixed with ILP, instead of 32-byte memory
    fetches and a single element per thread. The earlier cards are hugely sensitive
    to thread/memory ordering when coalescing. Global memory bandwidth on these cards
    is a fraction of the newer models, yet they can perform to a similar level with
    some problems if shared memory is used well. The cache in Fermi can play a big
    part to the extent that very low thread numbers (32 or 64) can outperform higher
    occupancy rates if the data is then entirely contained in the cache.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 采样是解决这个问题的答案。每张卡片都会有一个最佳的启动配置，这个配置最适合其特定的设置。正如我们在本书的一些测试中看到的，不同的卡片倾向于不同的设置。Fermi卡片在每块192或256线程时效果最佳，而之前的GPU则在每块128和192线程时效果较好。Compute
    2.1卡片在64或128字节内存取回混合ILP时表现最佳，而不是32字节内存取回和每个线程单一元素。早期的卡片对线程/内存排序在合并时非常敏感。这些卡片的全局内存带宽是新型号的一个小部分，但如果充分利用共享内存，它们仍然可以在某些问题上达到相似的性能水平。Fermi中的缓存可以发挥重要作用，甚至在线程数非常低（32或64）时，如果数据完全存储在缓存中，也能超越更高占用率的情况。
- en: When the program is installed, run a short test suite as part of the installation
    procedure. Run a loop through all feasible numbers of threads. Try ILP values
    from one to four elements per thread. Enable and disable shared memory usage.
    Run a number of experiments, repeating each a number of times, and average the
    result. Store in a data file or program configuration file the ideal values and
    for which GPU these relate. If the user later upgrades the CPU or GPU, then rerun
    the experiments and update the configuration. As long as you don’t do this on
    every startup, the user will be happy you are tuning the application to make the
    best possible use of their hardware.
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: '当程序安装时，作为安装过程的一部分，运行一个简短的测试套件。循环运行所有可行的线程数量。尝试每个线程一个到四个元素的ILP值。启用和禁用共享内存使用。进行多次实验，每次重复实验，并取平均值。将理想值及其对应的GPU存储在数据文件或程序配置文件中。如果用户以后升级了CPU或GPU，则需要重新运行实验并更新配置。只要你不在每次启动时都这么做，用户会很高兴你正在调整应用程序以充分利用他们的硬件。  '
- en: Section summary
  id: totrans-1286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '小节总结  '
- en: • There are too many factors to say with certainty the effect of a change without
    actually trying it.
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: • 有太多因素，无法在不实际尝试的情况下确定一个更改的效果。
- en: • Some experimentation is often required during development to get the optimal
    solution.
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: '• 在开发过程中，通常需要进行一些实验来获得最优解。  '
- en: • The optimal solution will be different on different hardware platforms.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: '• 最优解会因不同的硬件平台而异。  '
- en: • Write your applications to be aware of the different hardware out there and
    what works best on each platform, either statically or dynamically.
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: '• 编写应用程序时，应该考虑到不同的硬件平台，并了解每个平台上最有效的方式，既可以静态实现，也可以动态实现。  '
- en: Conclusion
  id: totrans-1291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '结论  '
- en: We’ve looked in detail at a number of strategies for trying to improve the throughput
    of your kernels with various examples throughout this chapter. You should be aware
    of the factors that affect performance and their relative importance (primary
    ones are transfers, memory/data patterns, and finally SM utilization).
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: '本章中，我们详细探讨了多种提高内核吞吐量的策略，并通过各种示例进行了说明。你应该了解影响性能的因素及其相对重要性（主要因素包括数据传输、内存/数据模式，最后是SM利用率）。  '
- en: Correctness is a key issue in optimizing code. You cannot reliably optimize
    code without automatic regression testing. This doesn’t have to be hugely complex.
    A back-to-back run against a known working version with several known datasets
    is entirely sufficient. You should aim to spot 95% plus of the errors before any
    program leaves your desk. Testing is not the job of some test group, but your
    responsibility as a professional to produce reliable and working code. Optimization
    often breaks code and breaks it many times. The wrong answer in one minute instead
    of the correct answer in one hour is no use to anyone. Always test for correctness
    after every change and you’ll see the errors there and then, as and when they
    are introduced.
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: 正确性是优化代码时的一个关键问题。没有自动回归测试，你无法可靠地优化代码。这不需要非常复杂。与一个已知正常工作的版本进行对比测试，并使用几个已知的数据集就足够了。在任何程序离开你的桌面之前，你应该力求发现
    95% 以上的错误。测试不是某个测试小组的工作，而是你作为专业人员的责任，确保输出可靠且有效的代码。优化常常会破坏代码，而且会多次破坏。用一分钟得到错误答案，而不是用一小时得到正确答案，对任何人都没有帮助。每次修改后都要测试正确性，这样你就能即时发现引入的错误。
- en: You should also be aware that optimization is a time-consuming and iterative
    process that will grow your understanding of your code and how the hardware functions.
    This in turn will lead you to design and write better code from the outset as
    you become more familiar with what does and what does not work well on GPUs.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要意识到，优化是一个耗时且迭代的过程，它会加深你对代码和硬件工作原理的理解。这反过来会让你在开始时就能够设计和编写更好的代码，因为你会更加熟悉在
    GPU 上什么有效，什么无效。
- en: Questions on Optimization
  id: totrans-1295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化相关问题
- en: 1. Take an existing program that has one or more GPU kernels. Run the Visual
    Profiler and Parallel Nsight to analyze the kernels. What are the key indicators
    you need to look for? How would you optimize this program?
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 选择一个现有的程序，其中包含一个或多个 GPU 内核。运行 Visual Profiler 和 Parallel Nsight 来分析这些内核。你需要关注哪些关键指标？你会如何优化这个程序？
- en: 2. A colleague brings a printout of a GPU kernel to you and asks your advice
    about how to make it run faster. What would be your advice?
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 一位同事给你带来了一份 GPU 内核的打印输出，并请教你如何让它运行得更快。你会给出什么建议？
- en: 3. Another colleague proposes to implement a web server using CUDA. Do you think
    this is a good idea? What issues, if any, would you expect with such a program?
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 另一位同事提议使用 CUDA 实现一个 Web 服务器。你认为这是个好主意吗？你认为这种程序会遇到哪些问题（如果有的话）？
- en: 4. Implement a shared memory version of the odd–even sort, which produces a
    single sorted list. What issues might you expect to deal with?
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 实现一个共享内存版本的奇偶排序，它将生成一个排序后的单一列表。你预期会遇到哪些问题？
- en: Answers
  id: totrans-1300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 答案
- en: 1. You should be looking first to the execution time of each kernel. If one
    or more kernels dominate the timing, then, until these are optimized, trying to
    optimize the others is a waste of your time.
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 你首先应该查看每个内核的执行时间。如果一个或多个内核占用了大部分时间，那么在这些内核得到优化之前，尝试优化其他内核就是浪费时间。
- en: Second, you should be looking to the timeline, specifically concerning transfers.
    Are they overlapped with kernel operations and are they using pinned memory or
    not? Is the GPU busy all the time or only periodically given work by the host?
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，你应该关注时间线，特别是与数据传输相关的部分。它们是否与内核操作重叠？是否使用了固定内存？GPU是否一直忙碌，还是仅在主机定期分配任务时才工作？
- en: Of the two longest executing kernels, what is causing them to take this time?
    Is there a sufficient number of threads overall? Are there enough blocks to populate
    all the SMs? Are there any peaks on one SM, and if so, why? What is the thread
    to memory pattern and can this be coalesced by the hardware? Are there any serialization
    points, for example, shared memory bank conflicts, atomics, synchronization points?
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于执行时间最长的两个内核，是什么原因导致它们需要这么长时间？整体上线程数量是否足够？是否有足够的块来填充所有SMs？是否有某个SM的负载达到峰值？如果是，是什么原因？线程到内存的访问模式是怎样的？硬件是否能合并这些访问？是否存在任何串行化点，例如共享内存银行冲突、原子操作、同步点等？
- en: 2. First, you need to understand the problem before looking at specifics. The
    “look at the code” optimization strategy can be hit or miss. Sure you can probably
    optimize the code on the paper in some way, but you need much more information
    to provide a good answer to the question the person is really asking.
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 首先，你需要理解问题的根本原因，然后再看具体细节。单纯的“查看代码”优化策略往往事倍功半。当然，你可能能在纸面上优化代码的某些部分，但你需要更多的信息，才能提供一个有效的答案，解决对方真正想问的问题。
- en: Probably the best answer would be to tell your colleague to profile the application,
    including the host timeline, and then come back with the results. In doing this
    they will likely see what the problems are and these may well not even be related
    to the original kernel printout.
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最好的答案是让你的同事对应用程序进行分析，包括主机时间线，然后根据结果再做分析。通过这样做，他们很可能会发现问题所在，而这些问题可能与原始内核的输出并无直接关系。
- en: 3. Applications that are highly data parallel are well suited to GPUs. Applications
    that are highly task parallel with lots of divergence threads are not. The typical
    implementation of a web server on a CPU is to spawn one thread per *N* connections
    and to distribute connections dynamically over a cluster of servers to prevent
    overloading any single node.
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 高度数据并行的应用程序非常适合GPU。高度任务并行且有大量线程分歧的应用程序则不适合。CPU上实现的典型Web服务器是为每*N*个连接创建一个线程，并动态地将连接分配到服务器集群中，以防止单个节点的过载。
- en: GPUs execute code in groups of 32 warps, effectively a vector processor with
    the ability to follow single-thread control flow when necessary, but at a large
    performance penalty. Constructing in real time a dynamic web page is very expensive
    in terms of control flow, a significant amount of which will diverge on a per-user
    basis. PCI-E transfers would be small and not efficient.
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: GPU以32个warp为一组执行代码，实际上是一个向量处理器，在必要时能够遵循单线程控制流，但会产生较大的性能惩罚。在实时构建动态网页时，控制流的开销非常大，而且大部分会基于每个用户的情况发生分歧。PCI-E传输会很小并且效率低下。
- en: A GPU would not be a good choice, with the CPU host being a much better choice.
    However, the GPU may be able to be used in the back-end operations of the server,
    performing some analytical work, churning through the user-generated data to make
    sense of it, etc.
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 并不是一个好的选择，CPU 主机则是一个更好的选择。然而，GPU 可能会被用在服务器的后端操作中，执行一些分析工作，处理用户生成的数据以便理解等。
- en: 4. This is a useful exercise to think about how to solve some open-ended problems.
    First, the question does not specify how to combine the output of *N* blocks.
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 这是一个有用的练习，帮助思考如何解决一些开放性问题。首先，问题没有指定如何组合 *N* 个块的输出。
- en: The quickest solution for largest datasets should be the sample sort method
    as it completely eliminates the merge sort step. The framework for sample sort
    is provided in the text, but is nonetheless quite a complex sort. However, it
    suffers from a variable number of elements per bin. A prefix sum that padded the
    bins to 128-byte boundaries would help significantly.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最大数据集，最快的解决方案应该是样本排序方法，因为它完全消除了归并排序步骤。样本排序的框架在文中提供，但仍然是一个相当复杂的排序方法。然而，它面临着每个桶中元素数量不固定的问题。一个填充到
    128 字节边界的前缀和将显著帮助这一问题。
- en: Merge sort is much easier to implement, allows for fixed block sizes, and is
    what I’d expect most implementations to opt for.
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: 归并排序更容易实现，允许固定的块大小，这也是我预期大多数实现会选择的方式。
- en: In terms of the odd/even sort, the coalescing problems with global memory are
    largely hidden by the cache in Fermi due to the locality being extremely high.
    A compute 1.x implementation would need to use shared memory/registers for the
    sort. It would need to access the global memory in a coalesced manner in terms
    of loading and writing back.
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 就奇偶排序而言，Fermi 架构中的全局内存的合并问题大部分被缓存隐藏，因为局部性非常高。计算 1.x 实现需要使用共享内存/寄存器进行排序。在加载和写回时，它需要以合并的方式访问全局内存。
- en: '[¹](CHP009.html#CFN1)L1 cache is only available on Fermi architecture and is
    configurable between 16 KB and 48 KB. L1 cache on GT200/G80 is only via texture
    memory that is 24 KB in size.'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: '[¹](CHP009.html#CFN1)L1 缓存仅在 Fermi 架构中可用，大小可配置在 16 KB 到 48 KB 之间。GT200/G80
    上的 L1 缓存仅通过 24 KB 的纹理内存提供。'
- en: '[²](CHP009.html#CFN2)L2 cache is zero K on compute 1.x devices, up to 768 K
    on compute 2.x (Fermi) devices and up to 1536 K on compute 3.x (Kepler) devices.'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: '[²](CHP009.html#CFN2)L2 缓存对于计算 1.x 设备为零 K，对于计算 2.x（Fermi）设备为 768 K，对于计算 3.x（Kepler）设备为
    1536 K。'
- en: '[³](#CFN3)Mark Harris, NVIDIA Developer Technology, “Optimizing Parallel Reduction
    in CUDA,” 2007.'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: '[³](#CFN3)Mark Harris, NVIDIA 开发技术，“CUDA 中并行归约优化”，2007 年。'
