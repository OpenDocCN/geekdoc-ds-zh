- en: Chapter 9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizing Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we provide a detailed breakdown of the main areas that limit
    performance in CUDA. Each section contains small examples to illustrate the issues.
    They should be read in order. The previous chapters introduced you to CUDA and
    programming GPUs. The sections here assume you have read the previous chapters
    and are comfortable with the concepts introduced there, or are already familiar
    with CUDA and are specifically interested in techniques for improving execution
    speed of your programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is broken up into a number of strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 1: Understanding the problem and breaking it down correctly into serial
    and parallel workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 2: Understanding and optimizing for memory bandwidth, latency and
    cache usage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 3: Understanding the implications of needing to transfer data to or
    from the host. A look at the effects of pinned and zero-copy memory and bandwidth
    limits on a selection of hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 4: Understanding the threading and computational abilities in detail
    and how these impact performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 5: Where to look for algorithm implementations, with a couple of examples
    of optimization of some general-purpose algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 6: Focus on profiling and identifying where in your applications the
    bottlenecks are occurring and why.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 7: A look at how applications can tune themselves to the various hardware
    implementations out there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 1: Parallel/Serial GPU/CPU Problem Breakdown'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analyzing the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the first step in considering if trying to parallelize a problem is
    really the correct solution. Let’s look at some of the issues involved here.
  prefs: []
  type: TYPE_NORMAL
- en: Time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to define what an “acceptable” time period is for the execution
    time of the algorithm you have in mind. Now acceptable does not have to mean the
    best time humanly possible. When considering optimization, you have to realize
    as a software professional, your time costs money, and if you work in the western
    world, your time is not cheap. The faster a program needs to execute, the more
    effort is involved in making this happen [(Figure 9.1)](#F0010).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-01-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.1 Programmer time versus speedup achieved.
  prefs: []
  type: TYPE_NORMAL
- en: You will usually find with any optimization activity there is a certain amount
    of so-called “low-hanging fruit.” The changes required are easy and lead to a
    reasonable speedup. As these are removed, it becomes progressively harder to find
    optimizations and these require more complex restructuring, making them more costly
    in terms of time and the potential for errors they can introduce.
  prefs: []
  type: TYPE_NORMAL
- en: In most western countries, programming effort is quite expensive. Even if your
    programming time is free—for example, if you are student working on a project—time
    spent optimizing is still time that could be spent doing other activities. As
    engineers, we can sometimes get caught up in making things better than they need
    to be. Understand what is required and set a suitable goal.
  prefs: []
  type: TYPE_NORMAL
- en: In setting a suitable speedup goal, you have to be aware of what is reasonable,
    given a set of hardware. If you have 20 terabytes of data that needs to be processed
    in a few seconds, a single-GPU machine is just not going to be able to cope. You
    have exactly this sort of issue when you consider Internet search engines. They
    have to, within seconds, return a set of search results to the user. Yet at the
    same time, it used to be “acceptable” for their indexes to take several days to
    update—that is, the time taken for them to pick up new content. In this modern
    world, even this is considered slow. Thus, what is acceptable today may not be
    acceptable tomorrow, next month, or next year.
  prefs: []
  type: TYPE_NORMAL
- en: In considering what the acceptable time is, ask yourself how far away you currently
    are from this. If it’s a factor of two or less, often it will be worth spending
    time optimizing the CPU implementation, rather than creating an entirely new,
    parallel approach to the problem. Multiple threads introduce all sorts of problems
    of dependencies, deadlock, synchronization, debugging, etc. If you can live with
    the serial CPU version, this may be a better solution in the short term.
  prefs: []
  type: TYPE_NORMAL
- en: Consider also the easy-fix solution to problems used for the past 30 or so years.
    Simply buy some faster hardware. Use profiling to identify where the application
    is spending it time to determine where it’s bound. Is there an input/output (I/O)
    bottleneck, a memory bottleneck, or a processor bottleneck? Buy a high-speed PCI-E
    RAID card and use SATA 3/SAS SSD drives for I/O issues. Move to a socket 2011
    system with a high clock rate on the memory, if memory bandwidth is an issue.
    If it’s simply raw compute throughput, install an Extreme Edition or Black Edition
    processor with the highest clock rate you can buy. Purchase an out-of-the-box,
    liquid-cooled, Sandybridge K or X series overclocked processor solution. These
    solutions typically cost much less than $3,000–$6,000 USD, a budget you could
    easily spend on programming time to convert a program from a serial to a parallel
    program.
  prefs: []
  type: TYPE_NORMAL
- en: However, while this approach works well when you have a small amount of difference
    between where you are and where you want to be, it’s not always a good approach.
    A high clock rate means high power consumption. The processor manufacturers have
    already abandoned that route in favor of multicore as the only long-term solution
    to providing more compute power. While the “buy new hardware” approach may work
    in the short term, it’s not a long-term solution. Sometimes the hardware you have
    may not easily be changeable, because it’s provided by a restrictive IT department,
    or because you have insufficient funds to purchase new hardware but lots of “free”
    programming time.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to go down the GPU route, which for many problems is a very good
    solution, then you should typically set your design goal to be around a 10× (ten
    times) improvement in execution time of the program. The actual amount you achieve
    depends on the knowledge of the programmers and the time available, plus a huge
    contribution from the next issue we’ll talk about, how much parallelism there
    is in the application. At least a 2× or 3× speedup is a relatively easy goal,
    even for those new to CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Problem decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The fundamental question here is simply this: Can the problem you have be broken
    down into chunks that can run in parallel; that is, is there an opportunity to
    exploit concurrency in the problem? If the answer is no, then the GPU is not the
    answer for you. You instead have to look at optimization techniques for the CPU,
    such as cache optimizations, memory optimizations, SIMD optimizations, etc. At
    least some of these we have covered on the GPU side in previous chapters and others
    are covered in this chapter. Many of these optimization techniques work very well
    on serial CPU code.'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you are able to partition the problem into concurrent chunks, the question
    then is how many? One of the main limiting factors with CPU parallelization is
    that there is often just not enough large-granularity (or coarse-grained) parallel
    work to be done. GPUs run thousands of threads, so the problem needs to be decomposed
    into thousands of blocks, not just a handful of concurrent tasks as with the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The problem decomposition should always start with the data first and the tasks
    to be performed second. You should try to represent the problem in terms of the
    output dataset. Can you construct a formula that represents the value of a given
    output point in the dataset as a transformation of the input dataset *for that
    single point*? You may need more than one formula, for example, one for most data
    points and one for the data points around the edge of the problem space. If you
    can do this, then the transformation of a problem into the GPU space is relatively
    easy.
  prefs: []
  type: TYPE_NORMAL
- en: One of the issues with this type of approach is that you need to fully understand
    the problem for the best benefit. You can’t simply peek at the highest CPU “hogs”
    and try to make them parallel. The real benefit of this approach comes from making
    the chain from the input data points to the output data points completely parallel.
    There may be parts of this chain where you could use 100,000 processors if you
    had the hardware and points where you are reduced to a few hundred processors.
    Rarely are any problems truly single threaded. It’s just that as programmers,
    scientists, and engineers, this is the solution we may have learned many years
    ago at university. Thus, seeing the potential parallelism in a problem is often
    the first hurdle.
  prefs: []
  type: TYPE_NORMAL
- en: Now there are some problems where this single-output data point view is not
    practical—H264 video encoding, for example. In this particular problem, there
    are a number of stages defined, each of which defines a variable-length output
    data stream. However, there are aspects—filtering, in particular—within image
    encoding/processing that easily lend themselves to such approaches. Here the destination
    pixel is a function of *N* source pixels. This analogy works well in many scientific
    problems. The value of the forces of a given destination atom can be written as
    the sum of all the atoms that apply a force to the given destination atom. Where
    the input set is very large, simply apply a threshold or cutoff point such that
    those input data points that contribute very little are excluded from the dataset.
    This will contribute a small amount of error, but in some problems allows a huge
    section of the dataset to be eliminated from the calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization used to be about how to optimize the operations or functions being
    performed on the data. However, as compute capacity has increased hugely in comparison
    to memory bandwidth, it’s now the data that is the primary consideration. Despite
    the fact GPUs have on the order of 5 to 10 times the memory bandwidth of CPUs,
    you have to decompose the problem such that this bandwidth can be used. This is
    something we’ll talk about in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: One final consideration here, if you plan to use multiple GPUs or multiple GPU
    nodes, is how to decompose the problem and the dataset over the processor elements.
    Communication between nodes will be *very* expensive in terms of computation cycles
    so it needs to be minimized and overlapped with computation. This is something
    we’ll touch on later.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A dependency is where some calculation requires the result of a previous calculation,
    be that some calculation in the problem domain or simply an array index calculation.
    In either case, the dependency causes a problem in terms of parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies are seen in two main forms, where one element is dependent on one
    or more elements around it, or where there are multiple passes over a dataset
    and there exists a dependency from one pass to the next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: If you consider this example, you can see that both `a` and `c` have a dependency
    on `b`. You can also see that `d` has a dependency on both `a` and `c`. The calculation
    of `a` and `c` can be done in parallel, but the calculation of `d` requires the
    calculation of both `a` and `c` to have completed.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical superscalar CPU, there are multiple independent pipelines. The
    independent calculations of `a` and `c` would likely be dispatched to separate
    execution units that would perform the multiply. However, the results of those
    calculations would be needed prior to being able to compute the addition operation
    for `a` and `c`. The result of this addition operation would also need to be available
    before the final multiplication operation could be applied.
  prefs: []
  type: TYPE_NORMAL
- en: This type of code arrangement allows for little parallelism and causes a number
    of stalls in the pipeline, as the results from one instruction must feed into
    the next. While stalled, the CPU and GPU would otherwise be idle. Clearly this
    is a waste, and both CPUs and GPUs use multiple threads to cover this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the CPU side, instruction streams from other virtual CPU cores fill in the
    gaps in the instruction pipeline (e.g., hyperthreading). However, this requires
    that the CPU know from which thread the instruction in the pipeline belongs, which
    complicates the hardware. On the GPU, multiple threads are also used, but in a
    time-switching manner, so the latency of the arithmetic operations is hidden with
    little or no cost. In fact, on the GPU you need around 20 clocks to cover such
    latency. However, this latency need not come from another thread. Consider the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here the code has been rearranged and some new terms introduced. Notice if you
    insert some independent instructions between the calculation of `a` and `c` and
    their use in `d`, you allow these calculations more time to complete before the
    result is obtained. The calculations of `f`, `g`, and `h` in the example are also
    overlapped with the `d` calculation. In effect, you are hiding the arithmetic
    execution latency through overlapping nondependent instructions.
  prefs: []
  type: TYPE_NORMAL
- en: One way of handling dependencies and introducing additional nondependent instructions
    is through a technique called loop fusion, as shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have two independent calculations for results `a` and `d`.
    The number of iterations required in the second calculation is more than the first.
    However, the iteration space of the two calculations overlaps. You can, therefore,
    move part of the second calculation into the loop body of the first, as shown
    in function `loop_fusion_example_fused_01`. This has the effect of introducing
    additional, nondependent instructions, plus reducing the overall number of iterations,
    in this example, by one-third. Loop iterations are not free, as they need a loop
    iteration value and cause a branch. Thus, discarding a third of them brings us
    a significant benefit in terms of reducing the number of instructions executed.
  prefs: []
  type: TYPE_NORMAL
- en: In the `loop_fusion_example_fused_02` we can further fuse the two loops by eliminating
    the second loop and fusing the operation into the first, adjusting the loop index
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Now in the GPU it’s likely these loops would be unrolled into threads and a
    single kernel would calculate the value of `a` and `d`. There are a number of
    solutions, but the most likely is one block of 100 threads calculating `a` with
    an additional block of 200 threads calculating `d`. By combining the two calculations,
    you eliminate the need for an additional block to calculate `d`.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is one word of caution with this approach. By performing such
    operations, you are reducing the overall amount of parallelism available for thread/block-based
    scheduling. If this is already only a small amount, this will hurt the execution
    time. Also be aware that kernels, when fused, will usually consume more temporary
    registers. This may limit the amount of fusion you can practically achieve, as
    it will limit the number of blocks scheduled on an SM due to increased register
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should consider algorithms where there are multiple passes. These
    are typically implemented with a number of sequential kernel calls, one for each
    pass over the data. As each pass reads and writes global data, this is typically
    very inefficient. Many of these algorithms can be written as kernels that represent
    a single or small set of destination data point(s). This provides the opportunity
    to hold data in shared memory or registers and considerably increases the amount
    of work done by a given kernel, compared with the number of global memory accesses.
    This will vastly improve the execution times of most kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The size of the dataset makes a huge difference as to how a problem can be
    handled. These fall into a number of categories on a typical CPU implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within L1 cache (~16 KB to 32 KB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within L2 cache (~256 KB to 1 MB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within L3 cache (~512 K to 16 MB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within host memory on one machine (~1 GB to 128 GB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within host-persistent storage (~500 GB to ~20 TB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset distributed among many machines (>20 TB)
  prefs: []
  type: TYPE_NORMAL
- en: 'With a GPU the list looks slightly different:'
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within L1 cache (~16 KB to 48 KB)^([1](CHP009_a.html#FN1))
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within L2 cache (~512 KB to 1536 MB)^([2](CHP009_a.html#FN2))
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within GPU memory (~512 K to 6 GB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within host memory on one machine (~1 GB to 128 GB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset within host-persistent storage (~500 GB to ~20 TB)
  prefs: []
  type: TYPE_NORMAL
- en: • Dataset distributed among many machines (>20 TB)
  prefs: []
  type: TYPE_NORMAL
- en: For very small problem sets, adding more CPU cores to a particular problem can
    result in a superscalar speedup. This is where you get more than a linear speedup
    by adding more CPU cores. What is happening in practice is that the dataset each
    processor core is given is now smaller. With a 16-core CPU, the problem space
    is typically reduced by a factor of 16\. If this now moves the problem from memory
    to the L3 cache or the L3 cache to the L2 cache, you see a very impressive speedup,
    not due to parallelism, but due instead to the much higher-memory bandwidth of
    the associated cache. Obviously the same applies when you transition from the
    L2 cache to holding the problem entirely in the L1 cache.
  prefs: []
  type: TYPE_NORMAL
- en: The major question for GPUs is not so much about cache, but about how much data
    can you hold on a single card. Transferring data to and from the host system is
    expensive in terms of compute time. To hide this, you overlap computation with
    data transfers. On the more advanced cards, you can do a transfer in and a transfer
    out at the same time. However, for this to work you need to use pinned memory
    on the host. As pinned memory can’t be swapped out by the virtual memory management
    system, it has to be real DRAM memory on the host.
  prefs: []
  type: TYPE_NORMAL
- en: On a 6 GB Tesla system you might have allocated this as a 1 GB input buffer,
    a 1 GB output buffer, and 4 GB compute or working memory. On commodity hardware,
    you have up to 2 GB available, so much less to work with, although some commodity
    cards support up to 4 GB of global memory.
  prefs: []
  type: TYPE_NORMAL
- en: On the host side, you need at least as much memory as you pin for the input
    and output buffers. You typically have up to 24 GB available (6 DIMMs at 4 GB)
    on most I7 Nehalem platforms, 32 GB (8 DIMMs at 4 GBs) on Sandybridge–EP I7, and
    16 GB on AMD platforms (4 DIMMs at 4 GB). As you’d typically pin only 2 GB maximum,
    you easily have room to support multiple GPUs. Most systems have support for at
    least two GPU cards. Four physical cards is the practical limit for a top-end
    system in one box.
  prefs: []
  type: TYPE_NORMAL
- en: When the problem size is much larger than the host memory size, you have to
    consider the practical limits of the storage capacity on a single host. Multiterabyte
    disks can allow node storage into the tens of terabytes. Most motherboards are
    equipped with six or more SATA connectors and 4 TB-plus disks are readily available.
    Disks are easily transportable if the dataset is to be captured in some remote
    area. Next-day courier can often be the fastest way to transfer such data between
    sites.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when you cannot fit the dataset on a single machine, be it from compute,
    memory, storage, or power requirements, you have to look at multiple nodes. This
    brings you to the realm of internode communication. Internode communication is
    expensive in terms of time, at least an order of magnitude slower than any internal
    communication of data. You also have to learn another set of APIs, so this step
    is really best avoided if the problem can be contained to a single node.
  prefs: []
  type: TYPE_NORMAL
- en: Resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the question of what can be done with 10 times or 50 times as much
    processing power. An existing problem that previously took one hour to resolve
    can be done in just over a minute. How does this change the questions that can
    be asked with a given dataset? What can now be done in real time or near real
    time that was impossible in the past? The previous batch submission problem is
    now an interactive problem.
  prefs: []
  type: TYPE_NORMAL
- en: Such a change allows for a step back from the problem, to consider how else
    it might be approached. Are there algorithms that were discarded in the past because
    they were too computationally expensive? Can you now process far more data points,
    or data points to a higher resolution, to produce a more accurate result? If you
    were previously happy with a runtime of a few hours or a day because that let
    you get on with other tasks, does increasing the resolution of the problem appeal
    more than the speedup? What does a more accurate result gain in your problem domain?
  prefs: []
  type: TYPE_NORMAL
- en: In finance applications, if your mathematical model of events is running ahead
    of the main market players, then you can react to changes faster than others,
    which can directly translate into making a better return on trading activities.
  prefs: []
  type: TYPE_NORMAL
- en: In medical applications, being able to present the doctor with the result of
    a test before the patient has finished getting dressed and left allows much more
    efficient use of both the doctor’s and patient’s time as it avoids repeat appointments.
  prefs: []
  type: TYPE_NORMAL
- en: In simulation applications, not having to wait a long time allows a much larger
    problem space to be explored within a given timeframe. It also allows for speculative
    execution. This is where you ask the system to explore all values of *x* between
    *n* and *m* in a given dataset. Equally, you might explore variables in the 2D
    or 3D space. With complex problems or a nonlinear system it’s not always clear
    what the optimal solution is, especially when changing one parameter impacts many
    other parameters. It may be quicker to simply explore the problem space and observe
    the result than it is to have an expert try to sit down and work out the optimal
    solution. This brute-force approach is remarkably effective and will often come
    up with solutions the “experts” would not have considered.
  prefs: []
  type: TYPE_NORMAL
- en: As a student you can now kick off a problem between lectures on your personal
    desktop supercomputer, rather than submit a job to the university machine and
    wait a day for it to run, only to find out it crashed halfway through the job.
    You can prototype solutions and come up with answers far quicker than your non-CUDA-literate
    peers. Think what you could cover if their batch jobs take a day and yours are
    done locally in an hour.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the bottlenecks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amdahl’s Law
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Amdahl’s Law is often quoted in work on parallel architecture. It’s important
    because it tells us that, while serial elements of execution remain in the data
    flow, they will limit any speedup we can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the simple case where we have 50% of the program’s execution time spent
    on a section that could run in parallel and 50% that must be done serially. If
    you had an infinitely fast set of parallel processing units and you reduced the
    parallel aspect of the program down to zero time, you would still have the 50%
    serial code left to execute. The maximum possible speedup in this case is 2×,
    that is, the program executes in half the time period it did before. Not very
    impressive, really, given the huge amount of parallel processing power employed.
  prefs: []
  type: TYPE_NORMAL
- en: Even in the case where we have 90% of the program that could be parallelized,
    we still have the 10% serial code that remains. Thus, the maximum speedup is 9×,
    or nine times faster than the original, entirely serial, program.
  prefs: []
  type: TYPE_NORMAL
- en: The only way to scale a program infinitely is to eliminate all serial bottlenecks
    to program execution. Consider the diagram in [Figure 9.2](#F0015), where all
    the squares represent data items that need to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-02-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.2 Data flow bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, there are 10 threads, each processing one column of the data.
    In the center is a dependency, and thus all the threads must contribute their
    existing result to a single value before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine, for one moment, this is a field of crops, with each column a line of
    crops. Each thread is like a combine harvester, moving down the columns and collecting
    crops at each square. However, at the center of the field there is a wall with
    two gates.
  prefs: []
  type: TYPE_NORMAL
- en: With 1 or even 2 combine harvesters, the gates pose a small problem and each
    combine harvester passes from one field to another. With 10 combine harvesters,
    one per column, getting each one through the gate takes time and slows down everyone
    in the process. This is one of the reasons why it’s far more efficient to have
    large, open fields, rather than smaller, bounded ones.
  prefs: []
  type: TYPE_NORMAL
- en: So how is this relevant to software? Each gate is like a serial point in the
    code. The program is doing well, churning through the chunks of work, and then
    all of a sudden it hits a serial point or synchronization point and everything
    backs up. It’s the same as everyone trying to leave the parking lot at the same
    time through a limited number of exits.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this type of problem is to parallelize up the bottlenecks. If
    we have 10 gates in the field or 10 exits from the parking lot, there would be
    no bottleneck, just an orderly queue that would complete in *N* cycles.
  prefs: []
  type: TYPE_NORMAL
- en: When you consider algorithms like histograms, you see that having all threads
    add to the same set of bins forms exactly this sort of bottleneck. This is often
    done with atomic operations, which effectively introduce serial execution to a
    set of parallel threads. If, instead, you give every thread a set of its own bins
    and then add these sets together later, you remove the serialization bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Consider carefully in your code where you have such bottlenecks and how these
    might be eliminated. Often they will limit the maximum scaling available to your
    application. While this may not be an issue with two or even four CPU cores, with
    GPU code you need to think about tens of thousands of parallel threads.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Profiling is one of the most useful tasks in identifying where you are today
    and knowing where you should spend your time. Often people think they know where
    the bottlenecks are, then go off and optimize that routine, only to find it makes
    1% or 2% difference to the application’s overall execution time.
  prefs: []
  type: TYPE_NORMAL
- en: In modern software development, there are usually many teams working on various
    aspects of a software package. It may not be possible to keep in contact with
    everyone who touches the software, especially in larger teams. Often what you
    may think is the bottleneck is not really that important.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization should be based on hard numbers and facts, not speculation about
    what “might” be the best place to apply the software effort in terms of optimization.
    NVIDIA provides two good tools, CUDA Profiler and Parallel Nsight, that provide
    profiling information.
  prefs: []
  type: TYPE_NORMAL
- en: Profilers reveal, through looking at hardware counters, where the code spends
    it time, and also the occupancy level of the GPU. They provide useful counters
    such as the number of coalesced reads or writes, the cache hit/miss ratio, branch
    divergence, warp serialization, etc. The CUDA Memcheck tool is also very useful
    in identifying inefficient usage of memory bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Having done an initial run using the profiler, you should first look at the
    routine in which the code spends the most *total* time. Typical unoptimized programs
    spend 80% of their time in 20% of the code. Optimizing the 20% is the key to efficient
    use of your time and profiling is the key to identifying that 20% of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Of course once this has been optimized as best as it can be, it’s then progressively
    more and more time consuming to provide further speedups without a complete redesign.
    Measure the speedup and know when the time you’re spending is no longer providing
    a good return on that effort.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight is a very useful tool in this regard as it provides a number
    of default “experiments.” That shed light on what your kernels are actually doing.
    Some off the more useful information you can take from the experiments is shown
    in [Figure 9.3](#F0020).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-03-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.3 Parallel Nsight experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The first experiment is the CUDA Memory Statistics, which provides a nice graphical
    view of how the caches are laid out and the bandwidth being achieved in the different
    parts of the device.
  prefs: []
  type: TYPE_NORMAL
- en: This particular example (see [Figure 9.4](#F0025)) is taken from the odd/even
    sort we’ll look at a little later. What is interesting to note are the cache ratios.
    As we’re getting a 54% hit ratio in the L1 cache, we’re achieving an average throughput
    of 310 GB/s to global memory, in the order of double the actual bandwidth available
    from global memory. It also lists the number of transactions, which is important.
    If we can lower the number of transactions needed, through better coalescing and/or
    issuing larger reads/writes, we can significantly boost memory throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-04-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.4 Parallel Nsight memory overview.
  prefs: []
  type: TYPE_NORMAL
- en: The other important experiment is occupancy rates (see [Figure 9.5](#F0030)).
    In this experiment, notice the Achieved occupancy column and in particular the
    number of Active Warps. As this is a compute 2.0 device, we can have up to 48
    warps resident on a single SM. The achieved occupancy, as opposed to the theoretical
    occupancy, is the measured value of what was actually achieved. This will usually
    be significantly less than the theoretical maximum. Notice also that any limiting
    factor is highlighted in red, in this case the number of blocks per SM at six.
    The “occupancy” graphs tab allows you to understand this in somewhat more detail.
    It’s an extract from the occupancy calculation spreadsheet provided with the CUDA
    SDK.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-05-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.5 Parallel Nsight occupancy data.
  prefs: []
  type: TYPE_NORMAL
- en: The cause of this limit is actually the number of threads. Dropping this from
    256 to 192 would allow the hardware to schedule eight blocks. As this kernel has
    synchronization points, having more blocks available may introduce a better instruction
    mix. There will also be fewer warps that are unable to run due to the synchronization
    point.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, making this change helps quite significantly. It improves occupancy
    from 98.17% to 98.22%, which is marginal at best. However, the execution time
    drops from 14 ms to just 10 ms. The answer to this is in the memory usage. With
    192 threads per block, we’re accessing a smaller range of addresses which increases
    the locality of the accesses and consequently improves cache utilization. The
    total number of memory transactions needed by each SM drops by about one-quarter.
    Consequently, we see a proportional drop in execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping the tasks for CPU and GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dr. M. Fatica from NVIDIA gave a great talk at GTC2010 concerning how Linpack
    had been optimized for GPUs. Linpack is a benchmark based on linear algebra. It
    is used in the Top500 supercomputer benchmark ([*www.top500.org*](http://www.top500.org))
    to benchmark the various supercomputers around the world. One interesting fact
    from this talk was the GPU used at that time, a Fermi Tesla C2050 card, produced
    around 350 gigaflops of DGEMM (double-precision matrix multiply) performance.
    The CPU used produced around 80 gigaflops. The contribution of 80 gigaflops is
    a little under one-quarter of the GPU contribution, so not something that can
    be ignored. A quarter or so extra performance goes a long way to reducing execution
    time.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the best applications tend to be those that play to the strengths of
    both the CPU *and* the GPU and split the data accordingly. The CPU must be considered
    in any GPU-based optimization, because it’s the total application time that is
    important. If you have a four-, six-, or eight-core CPU and one core is busy handling
    a GPU application, why not use the other cores to also work on the problem? The
    more cores you have available, the higher the potential gain is by offloading
    some work to the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: If we say the CPU can handle work at one-tenth the rate of the GPU, then with
    just three CPU cores, you’re gaining a 30% additional throughput. If you had an
    eight-core device, potentially this is a 70% gain in performance, which is almost
    the same as having two GPUs working in tandem. In practice, however, often other
    constraints might limit the overall speed, such as memory, network, or I/O bandwidth.
    However, even so, you’re likely to see a significant speedup where the application
    is not already bound by one of these constraints on the host side.
  prefs: []
  type: TYPE_NORMAL
- en: Of these constraints, I/O is an interesting one, because introducing more CPU
    threads or processes can often significantly improve the overall I/O throughput.
    This may seem a strange statement, as surely the physical limits to and from an
    I/O device dictate the speed? On modern machines with large amounts of memory,
    most I/O is in fact cached. Therefore, I/O can be more about moving data in memory
    than it is about moving to or from devices. A decent RAID controller has its own
    processor to do the I/O operations. Multiple CPU cores allow for multiple independent
    memory transfers, which often provide a higher overall bandwidth than a single
    CPU core.
  prefs: []
  type: TYPE_NORMAL
- en: Separate CPU process or threads can create a separate GPU context and launch
    their own kernel onto the GPU. These additional kernels are then queued within
    the GPU for execution. When available resources become free the kernel is executed.
    If you look at the typical GPU usage you see that shown in [Figure 9.6](#F0035).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-06-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.6 CPU and GPU idle time.
  prefs: []
  type: TYPE_NORMAL
- en: Notice there is significant idle time on both the GPU and the CPU. Idle time
    on the GPU is more expensive, as it’s typically 10 times more useful than the
    CPU time. Tools such as Parallel Nsight allow you to display just such a timeline
    and you’ll be amazed to see just how much idle time certain kernels can create.
  prefs: []
  type: TYPE_NORMAL
- en: By placing multiple kernels onto a single GPU, these kernels then slot into
    the empty slots. This increases, marginally, the latency of the first set of kernels
    but greatly improves the overall throughput of the application. In a lot of applications,
    there can be as much as 30% idle time. Just consider what a typical application
    will do. First, fetch data from somewhere, typically a slow I/O device like a
    hard drive. Then transfer the data to the GPU and then sit and wait until the
    GPU kernel is complete. When it’s complete, the host transfers the data off the
    GPU. It then saves it somewhere, usually to slow I/O storage, fetches the next
    data block, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: While the GPU is executing the kernel, why not fetch the next data block from
    the slow I/O device, so it’s ready when the GPU kernel has completed? This is,
    in effect, what happens when you execute multiple processes. The I/O device blocks
    the second process, while fetching data for the first. When the first process
    is transferring data and invoking the kernel, the second process is accessing
    the I/O hardware. It then does a transfer, while process one is computing and
    the kernel invocation of the second process is queued. When the transfer back
    to the host for process one starts, the kernel from process two also starts executing.
    Thus, with the introduction of just a couple of processes, you have neatly overlapped
    the I/O, CPU, GPU, and transfer times, gaining a significant improvement in overall
    throughput. See the stream example in [Chapter 8](CHP008.html) for a detailed
    explanation of this.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can achieve the same results using threads or processes. Threads
    allow the application data to share a common data area and provide faster synchronization
    primitives. Processes allow for processor affinity, where you lock a process to
    a given CPU core, which can often improve performance because it allows for better
    core-specific cache reuse. The choice depends largely on how much, if any, synchronization
    is needed between the CPU tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The other aspect of the CPU/GPU decision is knowing how best to split the task.
    CPUs are great at serial problems, where the data is sparsely distributed, or
    where the dataset is small. However, with a typical 10:1 ratio of performance
    on the GPU to the CPU, you have to be careful that you will not be holding up
    the GPU. For this reason, many applications simply use the CPU to load and store
    data. This can sometimes fully load a single core on the CPU, depending on how
    much computation time is required on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: One usage you sometimes see a CPU being used for is the final stages of a reduction.
    A reduction operation typically reduces itself by a factor of two on every iteration
    of the reduction. If you start out with a million elements, within six iterations
    you are starting to hit the maximum number of schedulable threads on a GPU. Within
    a few more iterations, several of the SMs are idle. With the GT200 and prior generation
    of hardware, kernels were not overlapped, so the kernel had to continue to iterate
    down to the final elements before it freed up the idle SMs to do more work.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, one optimization when a certain threshold is reached, is to forward the
    remaining part of the computation to the CPU to complete. If the CPU was in fact
    idle anyway, and the remaining data being transferred is not huge, this strategy
    can show significant gains over waiting for the GPU to complete the entire reduction.
    With Fermi, NVIDIA addressed this issue, allowing those idle SMs to start work
    on the next queued kernel. However, for the SM to become idle, it’s necessary
    for all the thread blocks to have completed. Some nonoptimal kernels will have
    one or more active threads, even at the final levels of the reduction, which pins
    the kernel to the SM until the complete reduction is done. With algorithms like
    reduction, be sure you are reducing the number of active warps per iteration,
    not just the number of active threads.
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Understand the problem and define your speedup goal in the context of the
    programming time and skills available to you.
  prefs: []
  type: TYPE_NORMAL
- en: • Identify the parallelism in the problem and think about how to best to allocate
    this between the CPU and one or more GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: • Consider what is more important, a lower execution time or processing the
    data to a higher resolution.
  prefs: []
  type: TYPE_NORMAL
- en: • Understand the implication of any serial code sections and think about how
    these might best be handled.
  prefs: []
  type: TYPE_NORMAL
- en: • Profile your application to ensure your understanding reflects the actual
    reality. Repeat your earlier analysis if appropriate with your enhanced understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 2: Memory Considerations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Memory bandwidth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory bandwidth and latency are key considerations in almost all applications,
    but especially so for GPU applications. Bandwidth refers to the amount of data
    that can be moved to or from a given destination. In the GPU case we’re concerned
    primarily about the global memory bandwidth. Latency refers to the time the operation
    takes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Memory latency is designed to be hidden on GPUs by running threads from other
    warps. When a warp accesses a memory location that is not available, the hardware
    issues a read or write request to the memory. This request will be automatically
    combined or coalesced with requests from other threads in the same warp, provided
    the threads access adjacent memory locations and the start of the memory area
    is suitably aligned.
  prefs: []
  type: TYPE_NORMAL
- en: The size of memory transactions varies significantly between Fermi and the older
    versions. In compute 1.x devices (G80, GT200), the coalesced memory transaction
    size would start off at 128 bytes per memory access. This would then be reduced
    to 64 or 32 bytes if the total region being accessed by the coalesced threads
    was small enough and within the same 32-byte aligned block. This memory was not
    cached, so if threads did not access consecutive memory addresses, it led to a
    rapid drop off in memory bandwidth. Thus, if thread 0 reads addresses 0, 1, 2,
    3, 4, …, 31 and thread 1 reads addresses 32, 32, 34, …, 63, they will not be coalesced.
    In fact, the hardware will issue one read request of at least 32 bytes for each
    thread. The bytes not used will be fetched from memory and simply be discarded.
    Thus, without careful consideration of how memory is used, you can easily receive
    a tiny fraction of the actual bandwidth available on the device.
  prefs: []
  type: TYPE_NORMAL
- en: The situation in Fermi and Kepler is much improved from this perspective. Fermi,
    unlike compute 1.x devices, fetches memory in transactions of either 32 or 128
    bytes. A 64-byte fetch is not supported. By default every memory transaction is
    a 128-byte cache line fetch. Thus, one crucial difference is that access by a
    stride other than one, but within 128 bytes, now results in cached access instead
    of another memory fetch. This makes the GPU model from Fermi onwards considerably
    easier to program than previous generations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key areas to consider is in the number of memory transactions in
    flight. Each memory transaction feeds into a queue and is individually executed
    by the memory subsystem. There is a certain amount of overhead with this. It’s
    less expensive for a thread to issue a read of four floats or four integers in
    one pass than to issue four individual reads. In fact, if you look at some of
    the graphs NVIDIA has produced, you see that to get anywhere near the peak bandwidth
    on Fermi and Kepler you need to adopt one of two approaches. First, fully load
    the processor with warps and achieve near 100% occupancy. Second, use the 64-/128-bit
    reads via the `float2`/`int2` or `float4`/`int4` vector types and your occupancy
    can be much less but still allow near 100% of peak memory bandwidth. In effect,
    by using the vector types you are issuing a smaller number of larger transactions
    that the hardware can more efficiently process. You also introduce a certain amount
    of instruction-level parallelism through processing more than one element per
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: However, be aware that the vector types (`int2`, `int4`, etc.) introduce an
    implicit alignment of 8 and 16 bytes, respectively. The data must support this,
    so for example, you cannot cast a pointer to `int` from array element `int[5]`
    to `int2∗` and expect it to work correctly. In such cases you’re better off performing
    back-to-back 32-bit reads or adding some padding to the data structure to allow
    aligned access. As we saw when optimizing the sample sort example, a value of
    four elements per thread often provides the optimal balance between additional
    register usage, providing increased memory throughput and opportunity for the
    processor to exploit instruction-level parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Source of limit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernels are typically limited by two key factors, memory latency/bandwidth and
    instruction latency/bandwidth. Optimizing for one when the other is the key limiter
    will result in a lot of effort and very little return on that effort. Therefore,
    being able to understand which of these two key factors is limiting performance
    is critical to knowing where to direct your efforts.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way in which you can see where the balance of the code lies is
    to simply comment out all the arithmetic instructions and replace them with a
    straight assignment to the result. Arithmetic instructions include any calculations,
    branches, loops, etc. If you have a one-to-one mapping of input values to calculated
    outputs, this is very simple and a one-to-one assignment works well. Where you
    have a reduction operation of one form or another, simply replace it with a sum
    operation. Be sure to include all the parameters read from memory into the final
    output or the compiler will remove the apparently redundant memory reads/writes.
    Retime the execution of the kernel and you will see the approximate percentage
    of time that was spent on the arithmetic or algorithmic part. If this percentage
    is very high, you are arithmetically bound. Conversely, if very little changed
    on the overall timing, you are memory bound.
  prefs: []
  type: TYPE_NORMAL
- en: With the arithmetic code still commented out, run the kernel using Parallel
    Nsight, using the Analysis function and the Profile setting. Examine the instruction
    statistics it produces ([Figure 9.7](#F0040)). If the bar graph contains a significant
    amount of blue, then the kernel memory pattern is displaying poor coalescing and
    the GPU has to serialize the instruction stream to support scattered memory reads
    or writes.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-07-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.7 High instruction reissue rate.
  prefs: []
  type: TYPE_NORMAL
- en: If this is the case, is it possible to rearrange the memory pattern so the GPU
    can coalesce the memory access pattern by thread? Remember, to do this, thread
    0 has to access address 0, thread 1 address 1, thread 2 address 2, and so on.
    Ideally, your data pattern should generate a column-based access pattern by thread,
    not a row-based access. If you can’t easily rearrange the data pattern, can you
    rearrange the thread pattern such that you can use them to load the data into
    shared memory before accessing the data? If so, you don’t have to worry about
    coalescing the reads when accessing them from shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Is it possible to expand the number of elements of the output dataset that are
    processed by a single thread? This will often help both memory- and arithmetic-bound
    kernels. If you do this, do it without introducing a loop into the thread, but
    by duplicating the code. If the code is nontrivial, this can also be done as a
    device function or a macro. Be sure to hoist the read operations up to the start
    of the kernel, so that the read operations have finished fetching data before
    they are needed. This will increase register usage, so be sure to monitor the
    number of warps being scheduled to see it does not suddenly drop off.
  prefs: []
  type: TYPE_NORMAL
- en: With arithmetic-bound kernels, look at the source code and think about how this
    would be translated into assembly (PTX) code. Don’t be afraid to have a look at
    the actual PTX code being generated. Array indexes can often be replaced with
    pointer-based code, replacing slow multiplies with much faster additions. Divide
    or multiply instructions that use a power of 2 can be replaced with much faster
    right and left shift operations, respectively. Anything that is constant within
    a loop body, an invariant, should be moved outside the loop body. If the thread
    contains a loop, does unrolling the loop speed up things (it usually does)? What
    loop unrolling factor works best? We look at these optimization strategies in
    detail a little later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Are you using single- or double-precision floats in reality, and what did you
    want to use? Look out for floating-point constants without an `F` postfix, which
    the compiler will treat as double precision. Do you really need 32 bits of precision
    in all of the calculations? Try the `-use_fast_math` compiler switch and see if
    the results are still accurate enough for your needs. This switch enables 24-bit
    floating-point arithmetic, which can be significantly quicker than the standard
    IEEE 32-bit floating-point math logic.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, are you testing speed with the “release” version of the code? As we
    saw in some of the examples earlier, this alone can increase performance by 15%
    or more.
  prefs: []
  type: TYPE_NORMAL
- en: Memory organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Getting the memory pattern correct for a GPU is often the key consideration
    in many applications. CPU programs typically arrange the data in rows within memory.
    While Fermi and Kepler will tolerate noncoalesced reads and writes, as we mentioned
    earlier, compute 1.x devices will not. You have to try and arrange the memory
    pattern such that access to it by consecutive threads will be in columns. This
    is true of both global memory and shared memory. This means for a given warp (32
    threads) thread 0 should access address offset 0, thread 1 address offset 1, thread
    2 address offset 2, etc. Think about the fetch to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: However, assuming you have an aligned access, 128 bytes of data will come in
    from global memory at a time. With a single float or integer per thread, all 32
    threads in the warp will be given exactly one element of data each.
  prefs: []
  type: TYPE_NORMAL
- en: Note the `cudaMalloc` function will allocate memory in 128-byte aligned blocks,
    so for the most part alignment is not an issue. However, if using a structure
    that would straddle such a boundary, then there are two approaches. First, you
    can either add padding bytes/words explicitly to the structure. Alternatively,
    you can use the `cudaMallocPitch` function we covered in [Chapter 6](CHP006.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that alignment is a key criteria as to whether one or two memory transactions,
    or cache lines, need to be fetched. Suppose thread 0 accesses address offset 2
    instead of 0\. Perhaps you’re accessing some data structure that has a header
    at the start, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If the kernel processes `msg_data`, then threads 30 and 31 of the warp cannot
    be served by the single memory fetch. In fact, they generate an additional 128-byte
    memory transaction as shown in [Figure 9.8](#F0045). Any subsequent warps suffer
    from the same issue. You are halving your memory bandwidth, just by having a 2-byte
    header at the start of the data structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-08-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.8 Cache line/memory transaction usage within structures.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see this most acutely on compute 1.x devices where the additional fetch
    generated for threads 30/31 isn’t even used to prefill the cache, but just discarded.
    Loading the header into a separate chunk of memory somewhere else allows for aligned
    access to the data block. If you are unable to do this, then manually insert padding
    bytes into the structure definition to ensure that `msg_data` is aligned to a
    128-byte boundary. Note that simply reordering the structure elements to move
    ‘header’ after `msg_data` will also work, providing the structure is not subsequently
    used to create an array of structures. All of a sudden your threads match the
    memory organization and your memory throughput when working with the `msg_data`
    part of the structure will double.
  prefs: []
  type: TYPE_NORMAL
- en: Consider also the case where prefix sum is used. Prefix sum allows for multiple
    independent processes or threads to read or write to independent areas of memory
    without interfering with one another. Multiple reads from the same address are
    actually hugely beneficial, in that the GPU will simply forward the value to whatever
    additional threads within the warp need it without additional memory fetches.
    Multiple writes are of course an issue, in that they need to be sequenced.
  prefs: []
  type: TYPE_NORMAL
- en: If we assume integers or floats for now, the size of each entry in the data
    array is 4 bytes. If the distribution of the prefix array is exactly equal then
    we don’t need prefix arrays to access the data anyway, as you could simply use
    a fixed offset per thread. Therefore, if you’re using a prefix sum to calculate
    an offset into the dataset, it’s highly likely there are a variable number of
    elements per bin. If you know the upper bounds of the number of elements per bin
    and you have a sufficient memory available, then just pad each bin to the alignment
    boundary. Use an additional array that holds the number of elements in the bin
    or calculate this value from the prefix sum index. In this way we can achieve
    aligned access to memory at the expense of unused cells at the end of most bins.
  prefs: []
  type: TYPE_NORMAL
- en: One very simple solution to the alignment problem is to use a padding value
    that has no effect on the calculated result. For example, if you’re performing
    a sum over the values in each bin, padding with zero will mean no change to the
    end result, but will give a uniform memory pattern and execution path for all
    elements in the warp. For a `min` operation, you can use a padding value of 0xFFFFFFFF,
    and conversely 0 for a `max` operation. It is usually not hard to come up with
    a padding value that can be processed, yet contributes nothing to the result.
  prefs: []
  type: TYPE_NORMAL
- en: Once you move to fixed-sized bins, it’s also relatively simple to ensure the
    dataset is generated and accessed in columns, rather than rows. It’s often desirable
    to use shared memory as a staging buffer because of the lack of coalescing requirements.
    This can then be used to allow coalesced reads/writes to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: Memory accesses to computation ratio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One question that you should often ask is what is the ratio of memory operations
    to arithmetic operations? You ideally want a ratio of at least 10:1\. That is,
    for every memory fetch the kernel makes from global memory it does 10 or more
    other instructions. These can be array index calculations, loop calculations,
    branches, or conditional evaluations. Every instruction should contribute to useful
    output. Loops, in particular, especially when not unrolled, often simply contribute
    toward instruction overhead and not to any useful work.
  prefs: []
  type: TYPE_NORMAL
- en: If we look inside an SM, architecturally, we see that warps are dispatched to
    sets of CUDA cores based on even and odd instruction dispatchers. Compute 1.x
    devices have a single warp dispatcher and compute 2.x devices have two. In the
    GF100/GF110 chipset (Fermi GTX480/GTX580) there are 32 CUDA cores and four SFUs
    (special-function units) per SM ([Figure 9.9](#F0050)). In the GF104/GF114-based
    devices (GTX460/GTX560) there are 48 CUDA cores and eight SFUs per SM ([Figure
    9.10](#F0055)). Each SM for both compute 2.0 and compute 2.1 devices has a single
    set of 16 LSUs (load store units) that are used to load values to and from memory
    (global, constant, shared, local, and cache).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-09-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.9 Dispatching of CUDA warps (GF100/GF110, compute 2.0).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-10-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.10 Dispatching of CUDA warps (GF104/GF114).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in a single cycle, the warp dispatchers issue (or dispatch) a total of
    two (compute 2.0) or four (compute 2.1) instructions, one set from each dispatcher.
    As these come from different warps, the instructions are entirely independent
    of one another. These are then pushed into the pipeline of the execution units
    (CUDA cores, SFUs, and LSUs).
  prefs: []
  type: TYPE_NORMAL
- en: There are a few implications to this design. First, the absolute minimum number
    of warps that must be present is two for the GF100 series (compute 2.0) hardware
    and four for the GF104 series (compute 2.1) hardware. This in turn implies an
    absolute minimum of 64 or 128 threads per SM, respectively. Having less than this
    means that one or more of the instruction dispatch units will remain idle, effectively
    halving (GF100) the instruction dispatch speed. Using a number of threads other
    than a multiple of 32 will mean some elements of the CUDA cores will idle, again
    undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: Having this minimum number of resident warps provides absolutely no hiding of
    latency, either memory or instruction, based on the ability to switch to another
    warp. A stall in the instruction stream will actually stall the CUDA cores, which
    is highly undesirable. In practice, multiple blocks are allocated to an SM to
    try to ensure this problem never occurs and, more importantly, a variable mix
    of instructions is generated.
  prefs: []
  type: TYPE_NORMAL
- en: The second implication is the shared resources limit the ability to continuously
    perform the same operation. Both the CUDA cores and the LSUs are pipelined, but
    are only 16 units wide. Thus, to dispatch an entire warp to either unit takes
    two cycles. On compute 2.0 hardware, only one instruction per dispatcher can be
    dispatched. Thus, to push an operation into the LSUs, one slot in the pipeline
    of one of the CUDA cores must be left empty. There are four possible receivers
    for the dispatch (CUDA, CUDA, SFUs and LSUs), yet only two suppliers per cycle.
  prefs: []
  type: TYPE_NORMAL
- en: The situation is drastically improved in compute 2.1 hardware, in that the two
    dispatchers dispatch two instructions each, for a total of four per clock. With
    three sets of CUDA cores it would be possible to supply three arithmetic instructions
    plus a load/save instruction without creating holes in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: However, if all warps want to issue an instruction to the same execution unit,
    for example the LSU or SFU, there is a problem. Only a single warp can use the
    LSU per two clock cycles. As the SFU has just eight units, four on compute 2.0
    hardware, a warp can take up to eight cycles to be fully consumed by the SFUs.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the bandwidth available to and from the LSUs on a compute 2.1 device is
    50% less than a compute 2.0 device with the same number of CUDA cores. Consequently,
    the LSUs or SFUs can become a bottleneck. There need to be other instructions
    in the stream such that the CUDA cores can do some useful work while the memory
    and transcendental instructions progress through the LSU or SFU pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The Kepler GK104 device (GTX680/Tesla K10) further extends the GF104/114 (GTX460/560)
    design by extending the number of CUDA cores from 48 to 96, and then putting two
    of these within an SM. Thus there are four warp schedulers, eight dispatch units,
    two LSUs and two SFUs per SM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s expand a little on the example we looked at earlier. Consider the case
    of a typical kernel. At the start of the kernel, all threads in all warps fetch
    a 32-bit value from memory. The addresses are such that they can be coalesced.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This would break down into a multiply and add (MADD) integer instruction, to
    calculate the value to put into the register for the variable `tid`. Variables
    `data`, `b`, and `c` are arrays somewhere in global memory. The variables `data`,
    `a`, and `b` are indexed by `tid` so the address to write to needs to be calculated
    by multiplying `tid` by the size of the elements making up the array. Let’s assume
    they all are integer arrays, so the size is 4 bytes per entry.
  prefs: []
  type: TYPE_NORMAL
- en: We very quickly hit the first dependency in the calculation of `tid` ([Figure
    9.11](#F0060)). The warp dispatches the multiply of `blockIdx.x` and `blockDim.x`
    to the integer MADD units in the CUDA cores. Until the multiply and add instruction
    to calculate `tid` has completed we can continue no further, so the warp is marked
    as blocked and suspended.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-11-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.11 Data flow dependency.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the next warp is selected, which does the same operation and
    is again suspended at the calculation of `tid`. After all warps have progressed
    to this point, enough clocks have passed such that the value of `tid` in warp
    0 is now known and can be fed into the multiply for the destination address calculations.
    Thus, three additional MADD instructions are dispatched to the CUDA cores, to
    calculate the address offsets. The next instruction would be a couple of loads,
    but for this we need the address of `a` and `b` from the multiply instructions.
    At this point we again suspend the warp and the other warps execute.
  prefs: []
  type: TYPE_NORMAL
- en: Once the address calculation of `a` is available, the load instruction can be
    dispatched. It’s likely, due to the address calculation of `b` being issued back
    to back with that of `a`, that the address calculation of `b` will be retired
    by the time the load for `a` has been dispatched. Thus, we immediately issue the
    load for the ‘`b`’. The next instruction in the stream would be a multiply of
    ‘`a`’ and ‘`b`’, neither of which will be available for some time yet as they
    have to be fetched from main memory to the SM. Thus, the warp is suspended and
    the subsequent warps execute to the same point.
  prefs: []
  type: TYPE_NORMAL
- en: As memory fetches take a long time, all warps dispatch the necessary load instructions
    to the LSU and are suspended. If there is no other work to do from other blocks,
    the SM will idle pending the memory transactions completing.
  prefs: []
  type: TYPE_NORMAL
- en: Sometime later `a` finally arrives from the memory subsystem as a coalesced
    read of 128 bytes, a single cache line, or a memory transaction. The 16 LSUs distributes
    64 of the 128 bytes to the registers used by the first half-warp of warp 0\. In
    the next cycle, the 16 LSUs distribute the remaining 64 bytes to the register
    used by the other half-warp. However, warp 0 still can not progress as it has
    only one of the two operands it needs for the multiply. It thus does not execute
    and the subsequent bytes arriving from the coalesced read of `a` for the other
    warps are distributed to the relevant registers for those warps.
  prefs: []
  type: TYPE_NORMAL
- en: By the time all of the data from the coalesced read for `a` has been distributed
    to the registers of all the other warps, the data for `b` will likely have arrived
    in the L1 cache. Again, the 16 LSUs distribute the first 64 bytes to the registers
    of the first half-warp of warp 0\. In the subsequent cycle they distribute the
    second 64 bytes to the second half-warp.
  prefs: []
  type: TYPE_NORMAL
- en: At the start of this second cycle, the first half-warp is able to progress the
    multiply instruction for `a[tid] ∗ b[tid]`. In the third cycle the LSUs start
    providing data to the first half-warp of warp 0\. Meanwhile, the second half-warp
    of warp 0 starts the execution of the multiply. As the next instruction in warp
    0 would be a store and is dependent on the multiply, warp 0 is suspended.
  prefs: []
  type: TYPE_NORMAL
- en: Providing there are on the order of 18–22 warps resident, by the time the last
    warp has dispatched the final multiply, the multiply will have completed for warp
    0\. It can then dispatch the store instructions to the 16 LSUs and complete its
    execution. The other warps then do exactly the same and the kernel is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Now consider the case of (see [Figure 9.12](#F0065)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-12-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.12 Dual data flow dependency.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: By halving the number of blocks, we can process two elements per thread. Notice
    this introduces an independent execution stream into each thread of the warp.
    Thus, the arithmetic operations start to overlap with the load operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as the example C code is written, this will not help. This is because
    the code contains dependencies that are not immediately obvious. The write operation
    to the first element of `data` could affect the value in either the `a` or the
    `b` array. That is, the address space of `data` may overlap with `a` or `b`. Where
    you have a write in the data flow to global memory, you need to lift out the reads
    to the start of the kernel. Use the following code instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`data[tid] = a_vect ∗ b_vect;`'
  prefs: []
  type: TYPE_NORMAL
- en: We have two choices, a scalar approach or a vector approach. The GPU supports
    only vector loads and saves, not vector operations, in hardware. Thus, the multiplication
    is actually done as an overloaded operator in C++ and simply multiplies the two
    integers independently of one another. However, the vector loads and saves two
    64-bit loads and a single 64-bit save, respectively, instead of the four separate
    32-bit loads and a single 32-bit save with the nonvector version. Thus, 40% of
    the memory transactions are eliminated. The memory bandwidth usage is the same,
    but less memory transactions mean less memory latency, and therefore any stall
    time waiting for memory is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: To use the vector types, simply declare all arrays as type `int2`, which is
    an in-built vector type of two integers. Supported types are `int2`, `int3`, `int4`,
    `float2`, `float3`, and `float4`. You can of course create your own types, such
    as `uchar4`, and define your own operators. Each vector type is actually just
    an aligned structure with *N* named member elements of the base type.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, I hope you can actually see that a balance is therefore required between
    the different types of instructions. This becomes somewhat more critical with
    the compute 2.1 devices (GF104 series) where there are three sets of CUDA cores
    sharing the same resources within the SM. The change in compute 2.0 to compute
    2.1 devices added significantly more arithmetic capacity within the SM without
    providing additional data transport capacity. The compute 2.0 devices have up
    to 512 CUDA cores on a bus of up to 384 bits wide, giving a ratio of 1:3 of cores
    to memory bandwidth. The compute 2.1 devices have up to 384 CUDA cores on a bus
    of up to 256 bits, giving a ratio of 1:5 cores to memory bandwidth. Thus, compute
    2.0 devices are more suited to applications that are memory bound, whereas compute
    2.1 devices are more suited to applications that are compute bound.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this is balanced in the compute 2.0 devices by having up to 33%
    more CUDA cores. The compute 2.1 devices, however, typically also run at somewhat
    higher clock rates, both in terms of the internal clock speed and also the external
    memory bus speed. This helps significantly in rebalancing the smaller memory bus
    width but is generally not sufficient to allow compute 2.1 devices to outperform
    their 2.0 counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: What is important to realize, especially with compute 2.1 devices, is that there
    needs to be sufficient arithmetic density to the instruction stream to make good
    use of the CUDA cores present on the SMs. A kernel that simply does loads or stores
    and little else will not achieve anything like the peak performance available
    from these devices. Expand such kernels to also include independent instruction
    flow via processing two, four, or eight elements per thread. Use vector operations
    where possible.
  prefs: []
  type: TYPE_NORMAL
- en: Loop and kernel fusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another area where we can significantly save on memory bandwidth is a technique
    based on loop fusion we looked at in the last section. Loop fusion is where two
    apparently independent loops run over an intersecting range. For example, loop
    1 runs from 0 to 100 and loop 2 from 0 to 200\. The code for loop 2 can be fused
    with the code for loop 1, for at least the first 100 iterations. This increases
    the level of instruction-level parallelism, but also decreases the overall number
    of iterations by a third.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel fusion is a variation on loop fusion. If you have a number of kernels
    that are run in sequence, one after the other, are there elements of these kernels
    that can be fused? Be careful doing this with kernels you did not write or do
    not fully understand. Invoking two kernels in series generates an implicit synchronization
    between them. This may have been intended by design and, as it’s implicit, probably
    only the original designer is aware of it.
  prefs: []
  type: TYPE_NORMAL
- en: In developing kernels it’s quite common to break down the operation into a number
    of phases or passes. For example, in the first pass you might calculate the results
    over the whole dataset. On the second pass you may filter data for certain criteria
    and perform some further processing on certain points. If the second pass can
    be localized to a block, the first and second pass can usually be combined into
    a single kernel. This eliminates the write to main memory of the first kernel
    and the subsequent read of the second, as well as the overhead of invoking an
    additional kernel. If the first kernel is able to write the results to shared
    memory, and you only need those results for the second pass, you eliminate the
    read/write to global memory entirely. Reduction operations often fall into this
    category and can benefit significantly from such an optimization, as the output
    of the second phase is usually many times smaller than the first phase, so it
    saves considerably on memory bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Part of the reason why kernel fusion works so well is because of the data reuse
    it allows. Fetching data from global memory is slow, on the order of 400–600 clock
    cycles. Think of memory access like reading something from disk. If you’ve ever
    done any disk I/O, you’ll know that reading a file by fetching one character at
    time is very slow and using `fread` to read large blocks is far more efficient
    than repeatedly calling read character functions like `fgetch`. Having read the
    data in, you keep it in memory. Apply the same approach to accessing global memory.
    Fetch data in chunks of up to 16 bytes per thread (`float4`, `int4`), not in single
    bytes or words. Once you have each thread successfully processing a single element,
    switch to `int2` or `float2` and process two. Moving to four may or may not help,
    but moving from one to two often does. Once you have the data, store it in shared
    memory, or keep it in the register set and reuse it as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Use of shared memory and cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using shared memory can provide a 10:1 increase in speed over global memory,
    but is limited in size—48 K on Fermi/Kepler devices and 16 K on all the previous
    devices. This may not sound like a great deal of space, especially with multigigabyte
    memory systems found on the host, but this is actually per SM. Thus, a GTX580
    or Tesla M2090 has 16 SMs active per GPU, each of which provides 48 K of shared
    memory, a total of 768 K. This is memory that runs at L1 cache speed. In addition,
    you have 768 K of L2 cache memory (on 16 SM devices) that is shared between all
    the SMs. This allows for an order of magnitude faster, global memory, atomic operations
    than in previous generation GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: When you consider that a GTX580 comes with 1.5 GB of memory, 768 K means just
    a tiny fraction of that memory space can be held in cache at any one point in
    time. The equivalent Tesla card comes with 6 GB of memory. Thus, kernels that
    iterate over datasets need to be aware that they may be using either the cache
    or shared memory in an ineffective manner, if they are not reusing data.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than a number of passes over a large dataset, techniques such as kernel
    fusion can be used to move through the data as opposed to passing over it multiple
    times. Think of the problem in terms of the output data and not the input data.
    Construct the problem such that you assign threads to output data items, not input
    data items. Create a fan in and not a fan out in terms of data flow. Have a preference
    for gather (collecting data) primitives, rather than scatter (distributing data)
    primitives. The GPU will broadcast data, both from global memory and the L2 cache,
    directly to each SM. This supports high-speed gather-type operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Fermi and Kepler we have a very interesting choice, to configure the shared
    memory to either prefer L1 cache (48 K L1 cache, 16 K shared) or to prefer shared
    (48 K shared, 16 K cache). By default the device will prefer shared memory, and
    thus you’ll have 48 K of shared memory available. This decision is not fixed,
    but set at runtime, and thus can be set per kernel call. Kernels that do not make
    use of shared memory, or keep to the 16 K limit to ensure compatibility with earlier
    GPUs, usually benefit significantly (10% to 20% performance gain) by enabling
    the additional 32 K of cache, disabled by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: where the `cache_prefer` parameter is `cudaFuncCachePreferShared` for 48 K of
    shared memory and 16 K of L1 cache, or `cudaFuncCachePreferL1` for 48 K of cache
    memory and 16 K of shared memory. Note, Kepler also allows a 32 K/32 K split.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, some areas where the cache causes Fermi and Kepler to operate
    slower than previous generation GPUs. On compute 1.x devices, memory transactions
    would be progressively reduced in size to as little as 32 bytes per access if
    the data item was small. Thus, a kernel that accesses one data element from a
    widely dispersed area in memory will perform poorly on any cache-based architecture,
    CPU, or GPU. The reason for this is that a single-element read will drag in 128
    bytes of data. For most programs, the data brought into the cache will then allow
    a cache hit on the next loop iteration. This is because programs typically access
    data close in memory to where they previously accessed data. Thus, for most programs
    this is a significant benefit. However, for programs that only need one data element,
    the other 124 bytes are wasted. For such kernels, you have to configure the memory
    subsystem to fetch only the memory transactions it needs, not one that is cache
    line sized. You can do this only at compile time via the `-Xptxas –dlcm=cg` flag.
    This reduces all access to 32 bytes per transaction and disables the L1 cache.
    For read only data consider also using either texture or constant memory.
  prefs: []
  type: TYPE_NORMAL
- en: With G80/GT200, compute 1.x hardware, it’s essential that you make use of shared
    memory as an integral part of the kernel design. Without cached accessed to data,
    be it explicitly via shared memory or implicitly via a hardware-managed cache,
    memory latency times are just huge. The arrival of cache on GPUs via the Fermi
    architecture has made it much, much easier to write at a program, or kernel, that
    performs at least reasonably well on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of the obstacles to using shared memory. The first is the
    size available—16 K on compute 1.x hardware and up to 48 K on compute 2.x hardware.
    It can be allocated statically at compile time via the `__shared__` prefix for
    variables. It is also one of the optional parameters in a kernel call, that is,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With runtime allocation, you additionally need a pointer to the start of the
    memory. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that L2 cache size in Fermi is not always 768 K as stated in the CUDA C
    programmer guide. In fact, the L2 cache is based on the type of device being used
    and the number of SMs present. Compute 2.1 devices may have less L2 cache than
    compute 2.0 devices. Even compute 2.0 devices without all the SMs enabled (GTX470,
    GTX480, GTX570) have less than 768 K of L2 cache. The GTX460 device we’re using
    for testing has 512 K of L2 cache and the GTX470 device has 640 K.
  prefs: []
  type: TYPE_NORMAL
- en: The size of the L2 cache is returned from a call to `cudaGetDeviceProperties`
    API as `l2CacheSize` member.
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Think carefully about the data your kernel processes and how best to arrange
    this in memory.
  prefs: []
  type: TYPE_NORMAL
- en: • Optimize memory access patterns for coalesced 128-byte access, aligning with
    the 128-byte memory fetch and L1 cache line size.
  prefs: []
  type: TYPE_NORMAL
- en: • Consider the single-/double-precision tradeoff and how this impacts memory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: • Fuse multiple kernels to single kernels where appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: • Make optimal use of shared memory and cache, ensuring you’re making full use
    of the expanded size on later compute levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 3: Transfers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pinned memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To work on a dataset you need to transfer the data from the host to the device,
    work on the dataset, and transfer the results back to the host. Performed in a
    purely serial manner, this causes periods where both the host and GPU are inactive,
    both in terms of unused transfer capacity and compute capacity.
  prefs: []
  type: TYPE_NORMAL
- en: We looked in detail in the chapter on multi-GPU usage at how to use streams
    to ensure the GPU always has some work to do. With a simple double-buffering technique,
    while the GPU is transferring back the results and acquiring a new work packet,
    the other buffer is being used by the compute engine to process the next data
    block.
  prefs: []
  type: TYPE_NORMAL
- en: The host processor supports a virtual memory system where a physical memory
    page can be marked as swapped out. It can then be paged to disk. Upon an access
    by the host processor to that page, the processor loads the page back in from
    disk. It allows the programmer to use a much larger virtual address space than
    is actually present on the hardware. Given that the programs typically exhibit
    quite good locality, this allows the total memory space to be much larger than
    the physical limits allow. However, if the program really does need 8 GB and the
    host only has 4 GB, the performance will typically be poor.
  prefs: []
  type: TYPE_NORMAL
- en: Arguably the use of virtual memory is a hangover from a time when memory capacities
    were very limited. Today you can purchase 16 GB of memory for a little over 100
    euros/dollars/pounds, meaning the host’s need to use virtual memory is almost
    eliminated for most applications.
  prefs: []
  type: TYPE_NORMAL
- en: Most programs, except for big data problems, will generally fit within the host
    memory space. If not, then there are special server solutions that can hold up
    to 128 GB of memory per node. Such solutions are often preferable, as they allow
    you to keep the data within one node rather than add the complexity of a multinode
    solution. Of course, loading the dataset in chunks is perfectly feasible, but
    then you are ultimately limited by the throughput of the I/O hardware.
  prefs: []
  type: TYPE_NORMAL
- en: You should always be using page-locked memory on a system that has a reasonable
    amount of host memory. Page-locked memory allows the DMA (direct memory access)
    controller on the GPU to request a transfer to and from host memory without the
    involvement of the CPU host processor. Thus, no load is placed onto the host processor
    in terms of managing a transfer or having to bring back from disk any pages that
    have been swapped out.
  prefs: []
  type: TYPE_NORMAL
- en: The PCI-E transfers in practice can only be performed using DMA-based transfer.
    The driver does this in the background when you don’t use page-locked memory directly.
    Thus, the driver has to allocate (or malloc) a block of paged-locked memory, do
    a host copy from the regular memory to the page-locked memory, initiate the transfer,
    wait for the transfer to complete, and then free the page-locked memory. All of
    this takes time and consumes precious CPU cycles that could be used more productively.
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocated on the GPU is by default allocated as page locked simply because
    the GPU does not support swapping memory to disk. It’s the memory allocated on
    the host processor we’re concerned with. To allocate page-locked memory we need
    to either allocate it using the special `cudaHostMalloc` function or allocate
    it with the regular `malloc` function and register it as page-locked memory.
  prefs: []
  type: TYPE_NORMAL
- en: Registering memory simply sets some internal flags to ensure the memory is never
    swapped out and also tells the CUDA driver that this memory is page-locked memory
    so it is able to use it directly rather than using a staging buffer.
  prefs: []
  type: TYPE_NORMAL
- en: As with `malloc`, if you use `cudaHostAlloc` you need to use the `cudaFreeHost`
    function to free this memory. Do not call the regular C free function with pointers
    allocated from `cudaHostAlloc` or you will likely get a crash, some undefined
    behavior, or a strange error later in your program.
  prefs: []
  type: TYPE_NORMAL
- en: The prototype for `cudaHostAlloc` is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The flags consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaHostAllocDefault`—Use for most cases. Simply specifies the default behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaHostAllocWriteCombined`—Use for memory regions that will be transferred
    *to the device only.* Do not use this flag when the host will read from this memory
    area. This turns off the caching of the memory region on the host processor, which
    means it completely ignores the memory region during transfers. This speeds up
    transfer to the device with certain hardware configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaHostAllocPortable`—The page-locked memory becomes page locked and visible
    in all CUDA contexts. By default the allocation belongs to the context creating
    it. You must use this flag if you plan to pass the pointer between CUDA contexts
    or threads on the host processor.'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaHostAllocMapped`—We’ll look at this shortly. It allocates host memory
    into device memory space, allowing the GPU kernel to directly read and write with
    all transfers being implicitly handled.'
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the effect of paged memory versus nonpaged memory, we wrote a
    short program. This simply does a number of transfers, varied by size to and from
    a device, and invokes a dummy kernel to ensure the transfers actually take place.
    The results are shown in [Figure 9.13](#F0070).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-13-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.13 Transfer speed to and from the device (AMD Phenom II X4 905e, PCI-E
    2.0 X8 link).
  prefs: []
  type: TYPE_NORMAL
- en: On the Y axis we have MB/second to or from the device and the transfer size
    in bytes along the X axis. What we can see from the chart is that there is a considerable
    difference between using paged memory and nonpaged memory, the page-locked (pinned)
    memory being 1.4× faster for writes and 1.8× faster for reads. It took 194 ms
    to send out 512 MB of data to the card using page-locked memory, as opposed to
    278 ms to do this with nonpaged memory. Timings to transfer data from the device,
    for comparison, were 295 ms for paged memory versus 159 ms for pinned memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the input side, we see a strange issue: With page-locked memory, the bandwidth
    *from* the device is 20% higher than *to* the device. Given that PCI-E provides
    for a full duplex connection of the same speed to and from the device, you’d expect
    to see a similar transfer speed for both reads and writes. This variation, as
    you will see in subsequent tests, is very hardware dependent. All the systems
    tested except the Intel Nehalem I7 system exhibiting it to varying degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer rates to and from the four devices were almost identical, which is
    to be expected given the bandwidth of global memory on all of the cards is at
    least an order of magnitude greater than the PCI-E bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: What is also very noticable is that to get near-peak bandwidth, even with pinned
    memory, the transfer size needs to be on the order of 2 MB of data. In fact, we
    don’t achieve the absolute peak until the transfer size is 16 MB or beyond.
  prefs: []
  type: TYPE_NORMAL
- en: For comparison, the results are also shown in [Figures 9.14](#F0075), [9.15](#F0080)
    and [9.16](#F0085) for a number of systems we tested.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-14-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.14 Transfer speed to and from the device (Intel Atom D525, PCI-E 2.0
    X1 link).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-15-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.15 Transfer speed to and from the device (Intel I3 540, PCI-E X16 link).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-16-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.16 Transfer speed to and from the device (Intel I7 920, PCI-E X16 link).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9.14](#F0075) shows a small netbook based on Intel’s low-power ATOM
    device, equipped with a dedicated GT218 NVIDIA ION graphics card. The peak PCI-E
    bandwidth you can typically see is up to 5 GB/s when using a 2.0 X16 link. As
    this netbook uses an X1 link, we could expect a maximum of 320 MB/s and we see
    in the order of 200 MB/s.'
  prefs: []
  type: TYPE_NORMAL
- en: However, we see a very similar pattern to the AMD system, in that we need around
    2 MB plus transfer sizes before we start to achieve anything like the peak transfer
    rate. The only difference we see is there is a noticable difference between transfers
    to the device and transfers from the device.
  prefs: []
  type: TYPE_NORMAL
- en: A midrange system quite common in the consumer enviroment is the i3/i5 system
    from Intel. This particular one is the i3 540 running with a H55 chipset. As this
    device has a single GPU only, it’s running at X16 the peak speed PCI-E 2.0 ([Figure
    9.15](#F0080)).
  prefs: []
  type: TYPE_NORMAL
- en: Again we can see the very large difference between pinned and nonpinned transfers,
    in excess of 2×. However, notice the absolute speed difference, approximately
    a 2× increase over the AMD system. This is largely due to the AMD system using
    an X8 PCI-E link, whereas the Intel system here uses an X16 PCI-E link.
  prefs: []
  type: TYPE_NORMAL
- en: The Intel I3 is a typical consumer processor. Anyone writing consumer-based
    applications should be very much aware by now that they need to be using pinned
    memory transfers, as we can see the huge difference it makes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we look at one further system, this time from the server arena, using
    the Intel I7 920 Nehalem processor and the ASUS supercomputer socket 1366 motherboard.
    This is a common motherboard for very high-end GPUs, as it allows up to four PCI-E
    slots. This particular one is equipped with 3× GTX290 GPUs each using an PCI-E
    2.0 X16 connection.
  prefs: []
  type: TYPE_NORMAL
- en: What we see from the diagram is again interesting. Pinned and paged memory transfers
    are equal until transfer sizes larger than 512 KB, after which the pinned memory
    transfers lead by up to 1.8× over the paged memory–based transfers. Unlike the
    Nehalem I3 system, notice the Nehalem I7 system is more consistent and there is
    not a huge variation between inbound and outbound transfer speeds. However, also
    note the peak transfer speed, despite both devices being on a X16 PCI-E 2.0 link,
    is only 5400 MB/s as opposed to the I3, which achieved a peak of 6300 MB/s ([Figure
    9.16](#F0085)).
  prefs: []
  type: TYPE_NORMAL
- en: So in summary, we can say that across a selection of today’s computing hardware,
    pinned memory transfers are approximately twice as fast as nonpinned transfers.
    Also we see there can be a considerable variance in performance between read and
    write speeds from and to the various devices. We can also see that we need to
    use larger, rather than smaller, block sizes, perhaps combining multiple transfers
    to increase the overall bandwidth utilization of the bus.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-copy memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Zero-copy memory is a special form of memory mapping that allows you to map
    host memory into the memory space of the GPU directly. Thus, when you dereference
    memory on the GPU, if it’s GPU based, then you get high-speed (180 GB/s) bandwidth
    to global memory. If the GPU code reads a host-mapped variable it issues a PCI-E
    read transaction, and a (very) long time later the host will return the data over
    the PCI-E bus.
  prefs: []
  type: TYPE_NORMAL
- en: After looking at the PCI-E bus bandwidth in the previous section, this doesn’t,
    at first glance, make a lot of sense. Big transfers are efficient and small transfers
    inefficient. If we rerun the test program we used for the previous examples, we
    see that the median transfer time is 0.06 ms on our sample AMD Phenom X4 platform.
    However, these are explicit, individual transfers, so it’s possible the zero-copy
    implementation may be more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: If you think about what happens with access to global memory, an entire cache
    line is brought in from memory on compute 2.x hardware. Even on compute 1.x hardware
    the same 128 bytes, potentially reduced to 64 or 32, is fetched from global memory.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA does not publish the size of the PCI-E transfers it uses, or details
    on how zero copy is actually implemented. However, the coalescing approach used
    for global memory could be used with PCI-E transfer. The warp memory latency hiding
    model can equally be applied to PCI-E transfers, providing there is enough arithmetic
    density to hide the latency of the PCI-E transfers. This is, in fact, the key
    to getting this to work. If you do very little for each global memory fetch and
    your application is already memory bound, this approach is unlikely to help you.
  prefs: []
  type: TYPE_NORMAL
- en: However, if your application is arithmetically bound, zero-copy memory can be
    a very useful technique. It saves you the explicit transfer time to and from the
    device. In effect, you are overlapping computation with data transfers without
    having to do explicit stream management. The catch, of course, is that you have
    to be efficient with your data usage. If you fetch or write the same data point
    more than once, this will create multiple PCI-E transactions. As each and every
    one of these is expensive in terms of latency, the fewer there are the better.
  prefs: []
  type: TYPE_NORMAL
- en: This can also be used very effectively on systems where the host and GPU share
    the same memory space, such as on the low-end NVIDIA ION-based netbooks. Here
    a malloc of global memory on the GPU actually results in a malloc of memory on
    the host. Clearly it doesn’t make sense to copy from one memory area on the host
    to another memory area on the host. Zero-copy memory can eliminate the need to
    perform these copies in such systems, without the impact of a PCI-E bus transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-copy memory also has one very useful use case. This is during the phase
    where you are initially porting a CPU application to a GPU. During this development
    phase there will often be sections of code that exist on the host that have not
    yet been ported over to the GPU. By declaring such data references as zero-copy
    memory regions, it allows the code to be ported in sections and still have it
    work. The performance will be generally poor until all the intended parts are
    present on the GPU. It simply allows this to be done in smaller steps so it’s
    not an “everything or nothing” problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by taking the existing `memcpy` program and expanding the kernel
    so it does the read of the data instead of relying on an explicit copy. For this
    we absolutely must coalesce accesses to memory, which when reading a simple one-dimensional
    array is easy. Thus, our kernel becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the kernel we simply make the `x` and `y` grid dimensions into a single linear
    array and assign one element from the source dataset to the destination dataset.
    Next we have to do three critical things to use zero-copy or host-mapped memory—that
    is, first to enable it, second to allocate memory using it, and finally to convert
    the regular host pointer to the device memory space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior to any creation of a CUDA context, we need to make the following call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When the CUDA context is created the driver will know it also has to support
    host-mapped memory. Without this the host-mapped (zero-copy) memory will not work.
    This will not work if it’s done after the CUDA context has been created. Be aware
    that calls to functions like `cudaHostAlloc`, despite operating on host memory,
    still create a GPU context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although most devices support zero-copy memory, some earlier devices do not.
    It’s not part of the compute level, so it has to be checked for explicitly as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The next stage is to allocate memory on the host such that it can be mapped
    into device memory. This is done with an additional flag `cudaHostAllocMapped`
    to the `cudaHostAlloc` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to convert the host pointer to a device pointer, which is
    done with the `cudaHostGetDevicePointer` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In this call we convert the `host_data_to_device` previously allocated in the
    host memory space to an equvalent pointer, but within the GPU memory space. Do
    not confuse the pointers. Use the converted pointer only with GPU kernels and
    the original pointer only in code that executes on the host. Thus, for example,
    to free the memory later, an operation performed on the host, the existing call
    remains the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As we’re using memory blocks up to 512 MB in size, to access one element per
    thread no matter how many threads we allocate per block means the number of blocks
    will exceed 64 K. This is the hard limit on the number of blocks in any single
    dimension. Thus, we have to introduce another dimension. This introduces grids,
    which we covered in [Chapter 5](CHP005.html). We can do this relatively simply
    by fixing the number of grids at some value that will be large enough to allow
    sufficient flexibility in selecting the number of threads per block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`dim3 blocks(num_grid, num_blocks_per_grid);`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dim3` operation simply assigns the regular scalar values we calculated
    to a structure type holding a triplet that can be used as a single parameter in
    the kernel launch. It causes the kernel to launch 64 grids of *N* blocks. This
    simply ensures that for a given block index we do not exceed the 64 K limit. Thus,
    on the kernel launch, we replace `num blocks`, a scalar type, with `blocks`, a
    `dim3` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: What we see for transfers *to the device* is that the overall figures are identical
    to the transfers using explicit memory copies. This has significant implications.
    Most applications that do not already use the stream API simply copy memory to
    the GPU at the start and copy back once the kernel is complete. We can shrink
    that time drastically using pinned memory copies, but the time is still cumulative
    because it’s a serial operation.
  prefs: []
  type: TYPE_NORMAL
- en: In effect, what happens with the zero-copy memory is we break both the transfer
    and the kernel operation into much smaller blocks, which execute them in a pipeline
    ([Figure 9.17](#F0090)). The overall time is reduced quite significantly.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-17-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.17 Serial versus overlapped transfer/kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Notice we did not perform the same optimization with the copy from device. The
    reason for this is because consumer GPUs have only one copy engine enabled. Thus,
    they support only a single memory stream. When you do a read-kernel-write operation,
    if the write is pushed into the stream ahead of subsequent reads, it will block
    the read operations until the pending write has completed. Note this is not the
    case for Tesla devices, as both copy engines are enabled and thus Tesla cards
    are able to support independent to and from streams. Prior to Fermi, there was
    only ever one copy engine on any card.
  prefs: []
  type: TYPE_NORMAL
- en: However, with zero-copy memory the transfers are actually quite small. The PCI-E
    bus has the same bandwidth in both directions. Due to the high latency of the
    PCI-E-based memory reads, actually most of the reads should have been pushed into
    the read queue ahead of any writes. We may be able to achieve significant execution
    time savings over the explicit memory copy version.
  prefs: []
  type: TYPE_NORMAL
- en: Note the diagram in [Figure 9.18](#F0095) is simplified in that it lists a single
    “Pinned To & From Device” line, yet we show the zero device copy times explicitly
    for the devices. The pinned memory time was effectively the same for all devices,
    so it was not shown per device.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-18-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.18 Zero-copy time versus explicit pinned copy time over different GPU
    generations.
  prefs: []
  type: TYPE_NORMAL
- en: We have listed the entire execution time of a single memory copy to device,
    kernel execution, and memory copy from device. Thus, there is some overhead that
    is not present when purely measuring the transfer to/from the device. As we’re
    using zero copy, the memory transactions and the kernel time cannot be pulled
    apart. However, as the kernel is doing very little, the overall execution time
    represents a fair comparison between the zero copy and explicit copy versions.
  prefs: []
  type: TYPE_NORMAL
- en: There is a considerable amount of variability. What we can see, however, is
    that for small transfer amounts, less than 512 KB, zero copy is faster than using
    explicit copies. Let’s now look at sizes larger than 512 KB in [Table 9.1](#T0010)
    and [Figure 9.19](#F0100).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.1 Zero-Copy Results (execution time in ms)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000090f09-19-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.19 Zero-copy graph (time in ms versus transfer size).
  prefs: []
  type: TYPE_NORMAL
- en: What is very interesting to see here is a considerable drop in execution time.
    On the Fermi hardware the overlapping of the kernel operation with the memory
    copies drops the execution time from 182 ms to 104 ms, a 1.75× speedup. The results
    are less impressive in the earlier devices, but still represent a significant
    speedup.
  prefs: []
  type: TYPE_NORMAL
- en: You can of course achieve this using streams and asynchronous memory copies,
    as demonstrated in [Chapter 8](CHP008.html). Zero copy simply presents an alternative,
    and somewhat simpler, interface you can work with.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some caveats. Beware of exactly how many times the data is
    being fetched from memory. Re-reading data from global memory will usually exclude
    the use of zero-copy memory.
  prefs: []
  type: TYPE_NORMAL
- en: If we modify the program to read the value from host memory twice instead of
    once, then the performance drops by half on the 9800 GT and GTX260 platforms,
    the compute 1.x devices. This is because each and every fetch from global memory
    on these platforms is not cached. Thus, the number of PCI-E transactions issued
    is doubled, as we double the amount of times the GPU accesses the zero-copy memory
    area.
  prefs: []
  type: TYPE_NORMAL
- en: On Fermi the situation is somewhat different. It has an L1 and L2 cache and
    it’s highly likely the data fetched earlier in the kernel will still be in the
    cache when the latter access hits the same memory address. To be sure, you have
    to explicitly copy the data you plan to reuse to the shared memory. So in Fermi,
    depending on the data pattern, you typically do not see the device issuing multiple
    PCI-E transactions, as many of these hit the internal caches and therefore never
    create a global memory transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, zero-copy memory presents a relatively easy way to speed up your existing
    serial code without having to explicitly learn the stream API, providing you are
    careful about data reuse and have a reasonable amount of work to do with each
    data item.
  prefs: []
  type: TYPE_NORMAL
- en: However, be aware that the bandwidth of the PCI-E bus is nowhere near the bandwidth
    available on a CPU. The latest Sandybridge I7 processor (Socket 2011) achieves
    some 37 GB/s of memory bandwidth, from a theoretical peak of 51 GB/s. We’re achieving
    5–6 GB/s from a theoretical peak of 8 GB/s on the PCI-E 2.0 bus. You must have
    enough work in your application to justify the cost of moving the data over the
    PCI-E bus. Consider that the CPU can be a better alternative in situations where
    very little work is being done per element.
  prefs: []
  type: TYPE_NORMAL
- en: The program used for these measurements is shown here for reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '`         size_in_bytes));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '`   {`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '`  // Adjust for doing a copy to and back`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Bandwidth limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ultimate bandwidth limitation of a significant number of applications is
    the I/O speed of whatever devices the input and output data have to be acquired
    from and written to. This is often the limitation on the speedup of any application.
    If your application takes 20 minutes to run on a serial CPU implementation and
    can express enough parallelism, it’s quite feasible for that application to run
    on a GPU in less time than it takes to load and save the data from the storage
    device you are using.
  prefs: []
  type: TYPE_NORMAL
- en: The first problem we have in terms of bandwidth is simply getting the data in
    and out of the machine. If you are using network-attached storage, the limit to
    this will be the speed of the network link. The best solution to this problem
    is a high-speed SATA3 RAID controller using many high-speed SSD drives. However,
    this will not solve your bandwidth issues unless you are using the drive efficiently.
    Each drive will have a peak transfer rate into host memory, which is actually
    a function of the transfer rate of the drive, the controller, and the route to
    host memory.
  prefs: []
  type: TYPE_NORMAL
- en: Running a benchmark on a drive, such as the commonly used ATTO benchmark, can
    show you the effect of using different size blocks (see [Figure 9.20](#F0105)).
    This benchmark simulates access to drives based on a certain size of reads and
    writes. Thus, it reads and writes a 2 GB file in blocks of 1 K, 2 K, 4 K, etc.
    to see the effect of changing the block size.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-20-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.20 Bandwidth (MB/s) for a single SSD versus five hard disks in RAID
    0.
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the results that only when we read data in 64 K chunks or more
    do we achieve the peak bandwidth from the single SSD drive. For the RAID 0 hard
    drive system we need at least 1 MB blocks to make use of the multiple disks. Thus,
    you need to make sure you’re using the `fread` function in C to read suitable
    sized blocks of data from the disk subsystem. If we fetch data in 1 K chunks,
    we get just 24 MB/s from the drive, less than 10% of its peak read bandwidth.
    The more drives you add to a RAID system, the larger the minimum block size becomes.
    If you are processing compressed music or image files, the size of a single file
    may only be a few megabytes.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that whether the data is compressible or not has a big impact on drive
    performance. The server level drives, such as the OCZ Vertex 3, provide both higher
    peak values and sustained bandwidth with uncompressible data. Thus, if your dataset
    is in an already compressed format (MP3, MP4, WMV, H.264, JPG, etc.), then you
    need to make sure you use server drives. The bandwidth on many consumer-level
    SSD drives can fall to half of quoted peak when using uncompressible data streams.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is the use of synchronous NAND memory in the high-end server
    SSDs versus the cheaper and much lower-performing asynchronous NAND memory used
    in consumer SSDs. Even with noncompressed data, synchronous NAND-based drives
    still outperform their asynchronous cousins, especially once the drive starts
    to contain some data. OCZ also provides the RevoDrive R4 PCI-E-based product,
    which claims speeds in the order of 2 GB/s plus at the expense of a PCI-E slot.
  prefs: []
  type: TYPE_NORMAL
- en: The next bandwidth limit you hit is that of host memory speed. This is typically
    not an issue until you introduce multiple GPUs per node, if you consider that
    you can fetch data at 6 GB/s off the PCI-E bus from a very high-speed SSD RAID
    system. We then have to send out data at 6 GB/s to and from the host memory to
    the GPU. Potentially you could also write data again at 6 GB/s to the RAID controller.
    That’s a potential 24 GB/s of pure data movement without the CPU actually doing
    anything useful except moving data. We’re already hitting the bandwidth limits
    of most modern processor designs and have already surpassed that available from
    the older-generation CPUs. In fact, only the latest quad channel I7 Sandybridge-E
    CPU has anything like the bandwidth we could start moving around, if we were to
    solve the slow I/O device issue.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA 4.0 SDK introduced Peer2Peer GPU communication. The CUDA 4.1 SDK also introduced
    Peer2Peer communication with non-NVIDIA hardware. Thus, with the correct hardware,
    GPUs can talk to any supported device. This is mostly limited to a small number
    of InfiniBand and other highspeed network cards. However, in principle, any PCI-E
    device can talk with the GPU. Thus, a RAID controller could send data directly
    to and from a GPU. There is a huge potential for such devices, as no host memory
    bandwidth, PCI-E, or memory is consumed. As data is not having to flow to a CPU
    and then back out again, latency is dropped considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been moved to the GPU, there is a bandwidth limit of up to
    190 GB/s on GeForce cards and 177 GB for Tesla, to and from the global memory
    on the device. To achieve this you need to ensure coalescing of the data reads
    from the threads and ensure your application makes use of 100% of the data moved
    from memory to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have shared memory. Even if you partition data into tiles, move
    it into shared memory, and access it in a bank conflict–free manner, the bandwidth
    limit is on the order of 1.3 TB/s. For comparison the AMD Phenom II and Nehalem
    I7 CPUs for a 64 KB L1 cache block, the same capacity as the GPU L1 cache and
    shared memory, has around 330 GB/s bandwidth, some 25% of that of the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: If we take a typical float or integer parameter, it’s 4 bytes wide. Thus, the
    bandwidth to global memory is a maximum of 47.5 giga-elements per second (190
    GB/s ÷ 4). Assuming you read and write just one value, we can halve this figure
    to 23.75 giga-elements per second. Thus, with no data reuse, this is the maximum
    upper throughput of your application.
  prefs: []
  type: TYPE_NORMAL
- en: The Fermi device is rated in excess of 1 teraflop, that is, it can process on
    the order of 1000 giga floating-point operations per second. Kepler is rated at
    in excess of 3 teraflops. The actual available flops depend on how you measure
    flops. The fastest measure is the FMADD instruction (floating-point multiply and
    add) instruction. This multiplies two floating-point numbers together and adds
    another number to it. As such, this counts as two flops, not one. Real instruction
    streams intermix memory loads, integer calculations, loops, branches, etc. Thus,
    in practice, kernels never get near to this peak figure.
  prefs: []
  type: TYPE_NORMAL
- en: We can measure the real speed achievable by simply using the program we previously
    developed to visualize the PCI-E bandwidth. Simply performing a memory copy from
    global memory to global memory will show us the maximum possible read and write
    speed a kernel can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Note the values in the parentheses shows grids × blocks × threads. The above
    figures are plotted in [Figure 9.21](#F0110).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-21-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.21 Global memory bandwidth across devices.
  prefs: []
  type: TYPE_NORMAL
- en: These results are created by pushing 16 kernels into an asynchronous stream,
    with each call surrounded by a stop and start event. Each kernel performs a single-element
    copy from the source to the destination for every memory location. The execution
    time of the first kernel in each batch is ignored. The remaining kernels contribute
    to the total time, which is then averaged over the kernels. The quoted bandwidth
    for the GTX470 is 134 GB/s, so we’re falling short of this, despite having a simple
    kernel and obviously hitting the peak at the larger transfer sizes.
  prefs: []
  type: TYPE_NORMAL
- en: What we see from this chart is that to achieve anywhere near the peak memory
    performance you need to have enough threads. We start off by using 32 threads
    per block until we launch a total of 64 blocks. This ensures that all the SMs
    are given work, rather than one SM getting a large number of threads and therefore
    most of the work. We then increase the thread count per block up to 256 threads
    once there is a reasonable distribution of blocks to the SMs.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the element type from `uint1` to `uint2`, `uint3`, and `uint4` produces
    some interesting results. As you increase the size of a single element, the total
    number of transactions issued to the memory subsystem is reduced. On the GTX470,
    going from the 4-byte read (single-element integer or float) to an 8-byte read
    (dual-element integer, float, or single-element double) resulted in up to a peak
    23% increase in measured bandwidth to and from global memory ([Figure 9.22](#F0115)).
    The average improvement was somewhat lower at just 7%, but this still represents
    a reasonable improvement in execution time by simply switching from `int1`/`float1`
    to `int2`/`float2` vector types. The GTX460 presents a similar, but more pronounced
    pattern ([Figure 9.23](#F0120)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-22-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.22 Global memory bandwidth GTX470/compute 2.0 (transaction size in
    bytes).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-23-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.23 Global memory bandwidth GTX460/compute 2.1 (transaction size in
    bytes).
  prefs: []
  type: TYPE_NORMAL
- en: To achieve optimum bandwidth, the CUDA code was compiled specifically for compute
    2.1 devices. We also found that thread blocks that were a multiple of 48 threads
    worked best. This is not surprising given that there are three sets of 16 cores
    per SM instead of the usual two. When moving from 4 bytes per element to 8 or
    16 bytes per element, the bandwidth was increased by an average of 19%, but a
    best case of 38%.
  prefs: []
  type: TYPE_NORMAL
- en: A single warp transaction for 8 bytes per thread would result in a total of
    256 bytes moving over the memory bus. The GTX460 we are using has a 256-bit-wide
    bus to the global memory. This would clearly indicate that, regardless of any
    occupancy considerations, on such devices you should always be processing either
    8 or 16 bytes (two or four elements) per thread. This is most likely due to the
    higher ratio of CUDA codes within the SM causing some contention for the single
    set of LSUs (load/store units).
  prefs: []
  type: TYPE_NORMAL
- en: The GTX260 for comparison, a compute 1.3 device similar to the Tesla C2050 device,
    gained, on average, 5% by moving from 4 to 8 bytes per element. However, its performance
    was drastically reduced when moving beyond this. The 9800 GT did not show any
    significant improvement, suggesting this device is already achieving the peak
    when using 4 bytes per element.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that Fermi-based Tesla devices implement an ECC (error checking
    and correction) based memory protocol. Disabling this can boost transfer speeds
    by around 10% at the expense of the error detection and correction ability. In
    a single machine versus a server room, this may be an acceptable tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: GPU timing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Single GPU timing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Timing data on the GPU is not particularly straightforward. Using a timer that
    is CPU based is not a good solution, as the best way to use the GPU and CPU is
    to operate asynchronously. That is, both the GPU and CPU are running at the same
    time. CPU timing is only semi-accurate when you force sequential operation of
    the GPU and CPU. As this is not what we want in practice, it’s a poor solution.
  prefs: []
  type: TYPE_NORMAL
- en: The GPU, by default, operates in a synchronous mode in that the `memcpy` operations
    implicitly synchronize. The programmer expects to copy to the device, run the
    kernel, copy back from the device, and have the results in CPU memory to save
    to disk or for further processing. While this is an easy model to understand,
    it’s also a slow model. It’s one aimed at getting kernels to work, but not one
    aimed at performance.
  prefs: []
  type: TYPE_NORMAL
- en: We examined the use of streams, in detail, in [Chapter 8](CHP008.html). A stream
    is effectively a work queue. Stream 0 is used as the default work queue when you
    do not specify a stream to the CUDA API. However, stream 0 has many operations
    that implicitly synchronize with the host. You might be expecting an asynchronous
    operation, but in practice certain API calls have implicit synchronization when
    using stream 0.
  prefs: []
  type: TYPE_NORMAL
- en: To use asynchronous operations, we need to first create a stream such as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: For the bandwidth test, we created an array of events.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The GPU provides events that can be time-stamped by the GPU hardware ([Figure
    9.24](#F0125)). Thus, to time a particular action on the GPU, you need to push
    a start event into the queue, then the action you wish to time, and finally a
    stop event. Streams are simply a FIFO (first in, first out) queue of operations
    for the GPU to perform. Each stream represents an independent queue of operations.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-24-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.24 Timing an action on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-25-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.25 Multi-GPU timeline.
  prefs: []
  type: TYPE_NORMAL
- en: Having created a stream, you need to create one or more events.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Here we have a simple loop creating `MAX_NUM_TESTS` events—a start event and
    a stop event. We then need to push the events into the stream on either side of
    the action to measure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '`// Run the kernel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: To calculate the time, either per CUDA call or in total, call the CUDA function
    `cudaEventElapsedTime` to get the time difference between two time-stamped events.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: You should realize that in performing such a timed event, there is no guarantee
    of ordering of events between streams. The CUDA runtime could execute your start
    event in stream 0 and then switch to a previously suspended kernel execution in
    stream 5, sometime later come back to stream 0, kick off the kernel, jump to another
    stream to process a number of other start events, and finally come back to stream
    0 and timestamp the stop event. The delta time is the time from the start period
    to the end period.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, notice we have created only a single stream. We have multiple
    events, but they all execute from the same stream. With only a single stream the
    runtime can only execute events in order, so we guarantee achieving the correct
    timing.
  prefs: []
  type: TYPE_NORMAL
- en: Notice the call to the `cudaEventSynchronize` API. This call causes the CPU
    thread to block should it be called when the event has not completed. As we’re
    doing nothing useful on the CPU, this is perfectly fine for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the host program we must ensure that with any resources we allocated
    are freed up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Destroying an event before it’s actually been used will result in undefined
    runtime errors when executing the kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you should be aware that events are not free. It takes some resources
    to handle the events at runtime. In this example we specifically wanted to time
    each kernel to ensure there was not significant variability. In most cases a single
    start and stop event at the start and end of the work queue will be entirely sufficient
    for the overall timing.
  prefs: []
  type: TYPE_NORMAL
- en: Multi GPU timing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi GPU timing is a little more complex, but based on the same principles.
    Again, we create a number of streams and push events into the streams.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no function provided in the API to obtain the absolute
    timestamp from an event. You can only obtain the delta between two events. However,
    by pushing an event into the start of the stream, you can use this as time point
    zero and thus obtain the time relative to the start of the stream. However, asking
    for the delta time between events on different GPUs causes the API to return an
    error. This complicates creating a timeline when using multiple GPUs, as you may
    need to adjust the time based on when the start events actually happened. We can
    see in [Figure 9.29](#F0150) a copy to the device, a kernel execution, a copy
    from the device, a copy to the device, a second kernel invocation, and finally
    a copy from the device.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that with different devices, the copy times are largely similar but the
    kernels’ time will vary considerably. In the second-to-last copy from device operation
    for the GTX470 device (CFD 2), notice the bar is somewhat smaller (258 ms versus
    290 ms). This is because the GTX470 starts its transfer first and only toward
    the tail end of the transfer do the other devices also initiate a transfer. The
    GT9800, being a much slower device, still has its kernel being executed while
    GTX470 has in fact completed its transfer. With different device generations,
    you will get such a pattern. The transfer rates are largely similar, but the kernel
    times cause shifts in the points where the transfers are initiated.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9.25](#F0130) was generated using timers, but tools such as Parallel
    Nsight and the Visual Profiler will draw the timeline for you automatically, along
    with the CPU timeline so you can clearly see what has happened and when.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that it’s possible with `cudaEventQuery` API to simply query if the event
    has completed without causing a blocking call as with `cudaEventSynchronize`.
    Thus, the CPU can continue to do useful work, or simply move onto the next stream
    to see if it has completed yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: In this particular example, taken from another program, we have an array of
    events, `memcpy_to_stop`, indexed by device number and test number. We check if
    the event has completed by a call to `cudaEventQuery`, which returns `cudaSuccess`
    if the event has already completed. If so, we get the delta time between this
    event and the start event `memcpy_to_start` from the same device, but for test
    0, we get the start event for the whole kernel stream on that GPU. To obtain the
    delta time we simply call the `cudaEventElapsedTime` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note as this will generate an error if the event has not yet completed, it is
    guarded by the check with `cudaEventQuery`. We could equally call `cudaEventSynchronize`
    if we simply wanted a blocking call that would wait for the event to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we’re particularly interested in the absolute time, the GPU does provide
    access to the low-level timers with the help of some embedded PTX code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: This section of code loads the raw clock value into a C variable that can then
    later be stored in a history buffer and transferred back to the host. The special
    `%clock` value is simply a 32-bit counter that wraps at max(u32). Compute 2.x
    hardware provides a 64-bit clock, thus allowing a wider time range over which
    values can be timed. Note, the CUDA API provides functions to access these register
    values through the use of the `clock` and `clock64` functions.
  prefs: []
  type: TYPE_NORMAL
- en: You can use this to measure the times of device functions within kernel calls
    or sections of code. Such measurements are not shown with either the Visual Profiler
    or Parallel Nsight, as their resolution onto the timing stops at the global-level
    kernel functions. You can also use this to store the times warps arrive at a barrier
    point. Simply create a store on a per-warp basis prior to a call to a barrier
    primitive such as `syncthreads`. You can then see the distribution of the warps
    to the synchronization point.
  prefs: []
  type: TYPE_NORMAL
- en: However, one very important caveat here is you must understand that a given
    warp in a kernel will not be running all the time. Thus, as with timing multiple
    streams, a warp may store a start time, get suspended, sometime later get resumed,
    and meet the next timer store event. The delta is only the overall real time difference,
    not the time the SM spent executing code from the given warp.
  prefs: []
  type: TYPE_NORMAL
- en: You should also realize that instrumenting code in this way may well affect
    its timing and execution order relative to other warps. You will be making global
    memory stores to later transfer this data back to the host where it can be analyzed.
    Consequently, your instrumentation impacts not only execution flow, but memory
    accesses. The effect of this can be minimized by running a single block of 32
    threads, that is, a single warp. However, this entirely discounts the quite necessary
    effects of running with other warps present on the SM and across multiple SMs
    within the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Overlapping GPU transfers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two strategies for trying to overlap transfer; first, to overlap transfer
    times with the compute time. We’ve look at this in detail in the last section,
    explicitly with the use of streams and implicitly with the use of zero-copy memory.
  prefs: []
  type: TYPE_NORMAL
- en: Streams are a very useful feature of GPU computing. By building independent
    work queues we can drive the GPU device in an asynchronous manner. That is, the
    CPU can push a number of work elements into a queue and then go off and do something
    else before having to service the GPU again.
  prefs: []
  type: TYPE_NORMAL
- en: To some extent, operating the GPU synchronously with stream 0 is like polling
    a serial device with a single character buffer. Such devices were used in the
    original serial port implementations for devices like modems that operated over
    the RS232 interface. These are now obsolete and have been replaced with USB1,
    USB2, and USB3 interfaces. The original serial controller, a UART, would raise
    an interrupt request to the processor to say it had received enough bits to decode
    one character and its single character buffer was full. Only once the CPU serviced
    the interrupt could the communications continue. One character at a time communication
    was never very fast, and highly CPU intensive. Such devices were rapidly replaced
    with UARTs that had a 16-character buffer in them. Thus, the frequency of the
    device raising an interrupt to the CPU was reduced by a factor of 16\. It could
    process the incoming characters and accumulate them to create a reasonably sized
    transfer to the CPU’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: By creating a stream of work for the GPU we’re effectively doing something similar.
    Instead of the GPU working in a synchronous manner with the CPU, and the CPU having
    to poll the GPU all the time to find out if it’s ready, we just give it a chunk
    of work to be getting on with. We then only periodically have to check if it’s
    now out of work, and if so, push some more work into the stream or work queue.
  prefs: []
  type: TYPE_NORMAL
- en: Through the CUDA stream interface we can also drive multiple GPU devices, providing
    you remember to switch the desired device before trying to access it. For asynchronous
    operation, pinned or page-locked memory is required for any transfers to and from
    the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: On a single-processor system, all the GPUs will be connected to a single PCI-E
    switch. The purpose of a PCI-E switch is to connect the various high-speed components
    to the PCI-E bus. It also functions as a means for PCI-E cards to talk to one
    another without having to go to host memory.
  prefs: []
  type: TYPE_NORMAL
- en: Although we may have multiple PCI-E devices, in the case of our test machine,
    four GPUs on four separate X8 PCI-E 2.0 links, they are still connected to a *single*
    PCI-E controller. In addition, depending on the implementation, this controller
    may actually be on the CPU itself. Thus, if we perform a set of transfers to multiple
    GPUs at any one point in time, although the individual bandwidth to each device
    may be in the order of 5 GB/s in each direction, can the PCI-E switch, the CPU,
    the memory, and other components work at that speed if all devices become active?
  prefs: []
  type: TYPE_NORMAL
- en: With four GPUs present on a system, what scaling can be expected? With our I7
    920 Nehalem system, we measured around 5 GB/s to a single card using a PCI-E 2.0
    X16 link. With the AMD system, we have around 2.5–3 GB/s on the PCI-E 2.0 X8 link.
    As the number of PCI-E lanes are half that of the I7 system, these sorts of numbers
    are around what you might expect to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: We modified the bandwidth test program we used earlier for measuring the PCI-E
    bandwidth to measure the bandwidth as we introduce more cards and more concurrent
    transfers. Any number of things can affect the transfers once we start introducing
    concurrent transfers to different GPUs. Anyone familiar with the multi-GPU scaling
    within the games industry will appreciate that simply inserting a second GPU does
    not guarantee twice the performance. Many benchmarks show that most commercial
    games benefit significantly from two GPU cards. Adding a third card often introduces
    some noticeable benefit, but nothing like the almost times two scaling that is
    often seen with a second card. Adding a fourth card will often cause the performance
    to drop.
  prefs: []
  type: TYPE_NORMAL
- en: Now this may not seem very intuitive, adding more hardware equals lower speed.
    However, it’s the same issue we see on CPUs when the core count becomes too high
    for the surrounding components. A typical high-end motherboard/CPU solution will
    dedicate at most 32 PCI-E lands to the PCI-E bus. This means only two cards can
    run at full X16 PCI-E 2.0 speed. Anything more than this is achieved by the use
    of PCI-E switch chips, which multiplex the PCI-E lines. This works well until
    the two cards on the PCI-E multiplexer both need to do a transfer at the same
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The AMD system we’ve run most of these tests in this book on does not use a
    multiplexer, but drops the speed of each connected GPU to an X8 link when four
    GPUs are present. Thus, at 2.5–3 GB/s per device, we could achieve a theoretical
    maximum of 10–12.5 GB/s. In addition, being an AMD solution, the PCI-E controller
    is built into the processor, which also sits between the PCI-E system and main
    memory. The bandwidth to main memory is approximately 12.5 GB/s. Therefore, you
    can see this system would be unlikely to achieve the full potential of four GPUs.
    See [Tables 9.2](#T0015) and [9.3](#T0020) and [Figures 9.26](#F0135) and [9.27](#F0140).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.2 Bandwidth Effects of Multiple PCI-E Transfers to the Device
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 9.3 Bandwidth Effects of Multiple PCI-E Transfers from the Device
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000090f09-26-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.26 Multi-GPU PCI-E bandwidth to device AMD 905e Phenom II.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-27-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.27 Multi-GPU PCI-E bandwidth from device AMD 905e Phenom II.
  prefs: []
  type: TYPE_NORMAL
- en: What you can see from [Tables 9.2](#T0015) and [9.3](#T0020) is that transfers
    scale quite nicely to three GPUs. We’re seeing approximately linear scaling. However,
    when the four GPUs compete for the available resources (CPU, memory bandwidth,
    and PCI-E switch bandwidth) the overall rate is slower.
  prefs: []
  type: TYPE_NORMAL
- en: The other multi-GPU platform we have to work with is a six-GPU system based
    on the Nehalem I7 platform and the ASUS supercomputer motherboard (P6T7WS) with
    3 GTX295 Dual GPU cards. This uses dual NF200 PCI-E switch chips allowing each
    PCI-E card to work with a full X16 link. While this might be useful for inter-GPU
    communication, the P2P (peer-to-peer) model supported in CUDA 4.x, it does not
    extend the bandwidth available to and from the host if both cards are simultaneously
    using the bus. We are using GTX290 cards, which are a dual-GPU device. Internally,
    each GPU has to share the X16 PCI-E 2.0 link. [Table 9.4](#T0025) and [Figure
    9.28](#F0145) show what effect this has.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.4 I7 Bandwidth to Device
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000090f09-28-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.28 I7 bandwidth to device.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from [Table 9.4](#T0025), we see an approximate linear increase
    in total bandwidth to the device. We achieve a peak of just over 10 GB/s, 20%
    or so higher than our AMD-based system.
  prefs: []
  type: TYPE_NORMAL
- en: We can see the bandwidth from the device is a different story ([Table 9.5](#T0030)
    and [Figure 9.29](#F0150)). Bandwidth peaks with two devices, and is not significantly
    higher than our AMD system. This is not altogether unexpected if you consider
    the design in most GPU systems is based around gaming. In a game, most of the
    data is being sent *to* the GPU with very little if any coming back to the CPU
    host. Thus, we see a near linear scaling of up to three cards, which coincides
    with the top-end triple SLI (scalable link interface) gaming platforms. Vendors
    have little incentive to provide PCI-E bandwidth beyond this setup. As the GTX290
    is actually a dual-GPU card, we may also be seeing that the internal SLI interface
    is not really able to push the limits of the card. We’re clearly seeing some resource
    contention.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.5 I7 Bandwidth from Device
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Section summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Understand and plan for the fact you will have limited PCI-E bandwidth capability.
  prefs: []
  type: TYPE_NORMAL
- en: • Always use pinned memory where possible.
  prefs: []
  type: TYPE_NORMAL
- en: • Use transfer sizes of at least 2 MB.
  prefs: []
  type: TYPE_NORMAL
- en: • Understand the use of zero-copy memory as an alternative to the stream API.
  prefs: []
  type: TYPE_NORMAL
- en: • Think about how to overlap transfer time with kernel execution time.
  prefs: []
  type: TYPE_NORMAL
- en: • Do not expect a linear scaling of bandwidth when using multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 4: Thread Usage, Calculations, and Divergence'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thread memory patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Breaking down the application into *suitably* sized grids, blocks, and threads
    is often one of the key aspects of performance of CUDA kernels. Memory is the
    bottleneck in almost any computer design, the GPU included. A bad choice of thread
    layout typically also leads to a bad memory pattern, which will significantly
    harm performance.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the first example, a 2 × 32 layout of threads ([Figure 9.30](#F0155))
    versus a 32 × 2 layout of threads. Think about how they would typically overlay
    memory if they were processing floating-point values. In the 2 × 32 example, thread
    0 cannot be coalesced with any other thread than thread 1\. In this case the hardware
    issues a total of 16 memory fetches. The warp cannot progress until at least the
    first half-warp has acquired all the data it needs. Therefore, at least eight
    of these very long memory transactions need to complete prior to any compute activity
    on the SM. As most warps will be following the same pattern, the SM will be swamped
    with issuing memory requests while the compute part of the SM is almost idle.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-29-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.29 I7 bandwidth from device.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-30-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.30 2 × 32 thread grid layout.
  prefs: []
  type: TYPE_NORMAL
- en: We saw from the bandwidth analysis in the previous section that there is a limit
    to the number of memory requests the SM can push out from the warps. The SM services
    the data request for any single warp over two clock cycles. In our example, the
    request has to be broken into 16 × 8 byte memory transactions.
  prefs: []
  type: TYPE_NORMAL
- en: On Fermi, the first of these would cause a read miss in the L1 cache. The L1
    cache would request the minimum size of data possible, 128 bytes from the L2 cache,
    and some 16 times more data than the thread needs. Thus, when data is moved from
    the L2 cache to the L1 cache, just 3.125% of the data moved is consumed by thread
    0\. As thread 1 also wants the adjacent address, we can increase this to 6.25%,
    which is still terrible.
  prefs: []
  type: TYPE_NORMAL
- en: On the first run through the code the L2 cache is unlikely to contain the data.
    It issues a 128-byte fetch also to slow global memory. This latency-expensive
    operation is finally performed and 128 bytes arrive at the L2 cache.
  prefs: []
  type: TYPE_NORMAL
- en: The L2 cache is 768 K in size on a 16 SM device. Assuming we’re using a GTX580,
    we have 16 SMs. That is just 48 KB per SM, the maximum size of the L1 cache. Using
    128-byte cache lines we have just 384 entries in the cache per SM. If we assume
    the SM is fully loaded with 48 warps (Kepler supports 64), each warp will issue
    16 separate reads, which is 768 reads in total. This means we’d need 768 cache
    lines, not the 384 we have, just to cache the data needed so each warp can hold
    a single block in memory.
  prefs: []
  type: TYPE_NORMAL
- en: The cache is effectively far too small to be used for temporal locality in this
    example. By temporal locality we mean we expect the data to remain in the cache
    from one read to the next. Halfway through processing the warps in each SM, the
    cache is full and the hardware starts filling it with new data. Consequently,
    there is absolutely no data reuse with the L2 cache, but a significant overhead
    in having to fetch entire cache lines. In fact, the only saving grace is that
    Fermi, unlike previous generations, will now forward the data it fetched to the
    other thread in our example.
  prefs: []
  type: TYPE_NORMAL
- en: The cache model is one that can cause problems in that it allows people to think
    the hardware will save them from poor programming. Let’s assume for a moment we
    have to use this thread pattern and we would have processed the element we fetched
    from memory a number of times. The thread pattern for fetching data does not have
    to be the same thread pattern for using the data. We can fetch data into shared
    memory in a 32 × 2 pattern, synchronize the threads, and then switch to a 2 ×
    32 usage pattern if we wish. Despite the shared memory bank conflicts this would
    then incur, it would still be an order of magnitude faster than doing the global
    memory fetches. We can also simply add a padding element to the shared memory
    by declaring it as 33 × 2 to ensure when we access it, these bank conflicts are
    removed.
  prefs: []
  type: TYPE_NORMAL
- en: Consider for a moment the difference in handling of the memory system. We issue
    1 coalesced read for 128 bytes instead of 16 separate reads. There’s a factor
    of 16:1 improvement in both the number of memory transactions in flight and also
    bandwidth usage. Data can be moved from the L2 to the L1 cache in just one transaction,
    not 16.
  prefs: []
  type: TYPE_NORMAL
- en: The LSUs in the SM have to issue only a single fetch transaction instead of
    16 separate fetches, taking just 2 clock cycles instead of 32 and freeing up the
    LSUs for other tasks from other warps.
  prefs: []
  type: TYPE_NORMAL
- en: Each warp consumes a single cache line, 48 maximum per SM. Thus, of the 384
    cache lines we have per SM in the L2 cache, we’re using only 100, just 12.5% of
    the L2 cache instead of 200%. Thus, it’s absolutely critical that to get anywhere
    near the full performance, even in Fermi with its multilevel caches, you have
    to fetch data in coalesced blocks of 128 bytes across a warp.
  prefs: []
  type: TYPE_NORMAL
- en: Now we could configure the L2 cache to fetch only 32 bytes instead of 128 bytes
    using the `-Xptxas -dlcm=cg` compiler flag. However, this also disables global
    memory storage in the L1 cache. It’s an easy fix but a poor solution to the fact
    that you are not fetching data in large enough blocks from global memory. To get
    the best performance from a given device, you need to understand what’s going
    on down inside or use libraries that are coded by someone who does.
  prefs: []
  type: TYPE_NORMAL
- en: We can see this quite clearly with the effects on memory bandwidth with Parallel
    Nsight if you select “Custom” experiment and then add in the L1 and L2 cache counters.
    The particular counters we are interested in are shown in [Table 9.6](#T0035).
    These can be set up in Parallel Nsight using the “Custom” experiment, shown in
    [Figure 9.31](#F0160).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.6 Parallel Nsight Cache Counters
  prefs: []
  type: TYPE_NORMAL
- en: '| Nsight Counter | Usage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| L1 global load hits | The number of global memory load requests met by the
    L1 cache. |'
  prefs: []
  type: TYPE_TB
- en: '| L1 global load misses | The number of global memory load requests not met
    by the L1 cache. |'
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 0 read section misses | Half the number of L2 misses. |'
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 1 read section misses | The other half of the number of L2
    misses. |'
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 0 read section queries | Half the number of L2 access attempts.
    |'
  prefs: []
  type: TYPE_TB
- en: '| L2 subpartition 1 read section queries | The other half of the number of
    L2 access attempts. |'
  prefs: []
  type: TYPE_TB
- en: '![image](../images/F000090f09-31-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.31 Setting up Parallel Nsight to capture cache statistics.
  prefs: []
  type: TYPE_NORMAL
- en: From these counters we can manually work out the L1 and L2 cache hit ratio.
    The hit ratio is the percentage of reads (or writes) that we cached. Every cached
    access saves us several hundreds of cycles of global memory latency.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at the results for the sample sort algorithm in [Table 9.7](#T0040),
    we can instantly see the L2 cache hit ratio drops off sharply as soon as the kernel
    exceeds 64 threads. Occupancy increases, but performance drops off. This is not
    at all surprising given the usage of the L2 cache a prefix sum array will generate.
    If each thread is processing one bin, as we extend the number of threads, the
    size of the memory area being cached increases. As soon as it exceeds the L2 cache
    size the hit ratio rapidly drops off.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.7 Cache Hit Ratio for Sample Sort
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The solution to the problem is to replace the existing algorithm that uses one
    thread per bin with one where the threads all work on a single bin at a time.
    This way we’d achieve coalesced memory accesses on each iteration and significantly
    better locality of memory access. An alternative solution would be to use shared
    memory to handle the transition between noncoalesced access by the threads and
    the necessary coalesced access when reading or writing to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: Inactive threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Threads, despite there being many thousands of them, are not free, even if they
    are inactive. The problem with inactive threads is twofold. First, a warp will
    remain active, scheduled, and using resources if just one of its threads is active.
    There are a limited number of warps that can be dispatched in a dispatch period
    (two clock cycles). This is two on compute 2.0 hardware, four on compute 2.1 hardware
    and eight in compute 3.x hardware. There is no point in the hardware dispatching
    a warp with a single thread to a set of CUDA cores and having it use just a single
    CUDA core while the other 15 idle. However, this is exactly what the hardware
    has to do if there is divergence of execution flow within a warp down to just
    one thread being active.
  prefs: []
  type: TYPE_NORMAL
- en: You sometimes see a parallel reduction–type operation that has been written
    by a programmer who does not understand the hardware well. They will perform the
    reduction operation within every warp, going from 32 to 16, to 8, to 4, to 2,
    and finally to 1 active thread. Regardless of whether you use 32 threads or 1
    thread the hardware still allocates 32 and simply masks out the inactive ones.
    Because the warps are still active, even if they have only one thread active,
    they still need to be scheduled onto the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: A much better approach to this is to have all 32 threads in every block compute
    a set of partial results. Let’s use the sum operation, as it’s easy to understand.
    With 32 threads per warp, you can compute 64 additions in one cycle. Now have
    each thread store its value into shared memory. Thus, the first warp stores to
    element 0..31, the second to 32..63, etc. Now divide *N*, the number of elements
    of the reduction, by 2\. Repeat the reduction using the threshold `if (threadIdx.x
    < (N/2))` until such time as *N* equals 2.
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..255 read values 0..511 (eight active warps).
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..127 read values 0..256 (four active warps).
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..63 read values 0..127 (two active warps).
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..31 read values 0..63 (one active warp).
  prefs: []
  type: TYPE_NORMAL
- en: Threads 0..15 read values 0..31 (half an active warp).
  prefs: []
  type: TYPE_NORMAL
- en: Etc.
  prefs: []
  type: TYPE_NORMAL
- en: Warps with thread numbers greater than the threshold simply no longer get scheduled.
    The warps with values less than *N* are fully populated with work, until such
    time as *N* equals some value less than 32\. At this point we can simply do an
    addition or all remaining elements, or continue to iterate toward the final addition.
  prefs: []
  type: TYPE_NORMAL
- en: Inactive warps are not in themselves free either. Although the SM internally
    cares about warps, not blocks, the external scheduler can *only schedule blocks*
    into an SM, not warps. Thus, if each block contains only one active warp, we can
    have as little as 6 to 8 warps for the SM to select from for scheduling. Usually
    we’d have up to 64 warps active in an SM, depending on the compute version and
    resource usage. This is a problem because the thread-level parallelism (TLP) model
    relies on having lots of threads to hide memory and instruction latency. As the
    number of active warps drops, the ability of the SM to hide latency using TLP
    also dramatically drops. As some point this will hurt the performance, especially
    if the warp is still making global memory accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, at the last levels of such a reduction-type operation, or any operation
    where progressively larger numbers of warps will drop out, we need to introduce
    some instruction level parallelism (ILP). We want to terminate the last warp as
    soon as possible so the entire block can be retired and replaced with another
    block that will likely have a fresh set of active warps.
  prefs: []
  type: TYPE_NORMAL
- en: We look at reduction in detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Arithmetic density
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Arithmetic density is a term that measures the relative number of calculations
    per memory fetch. Thus, a kernel that fetches two values from memory, multiplies
    them, and stores the result back to memory has very low arithmetic density.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: The fetch and store operations may well involve some index calculations. The
    real work being done is the multiplication. However, with only one operation being
    performed per three memory transactions (two reads and one write), the kernel
    is very much memory bound.
  prefs: []
  type: TYPE_NORMAL
- en: The total execution time is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090si1.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090si2.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice we use here *A* + *B* as opposed to multiplying *A*, the single memory
    fetch time, by 2\. The individual read times are not easy to predict. In fact
    neither *A*, *B*, or *C* are constant, as they are affected by the loads other
    SMs are making on the memory subsystem. Fetching of *A* may also bring into the
    cache *B*, so the access time for *B* is considerably less than *A*. Writing *C*
    may evict from the cache *A* or *B*. Changes to the resident lines in the L2 cache
    may be the result of the activity of an entirely different SM. Thus, we can see
    caching makes timing very unpredictable.
  prefs: []
  type: TYPE_NORMAL
- en: When looking at the arithmetic density, our goal is to increase the ratio of
    useful work done relative to memory fetches and other overhead operations. However,
    we have to consider what we define as a memory fetch. Clearly, a fetch from global
    memory would qualify for this, but what about a shared memory, or cache fetch?
    As the processor must physically move data from shared memory to a register to
    operate on it, we must consider this also as a memory operation. If the data comes
    from the L1, L2, or constant cache, it too has to be moved to a register before
    we can operate on it.
  prefs: []
  type: TYPE_NORMAL
- en: However, in the case of a shared memory or L1 cache access, the cost of such
    operations is reduced by an order of magnitude compared to global memory accesses.
    Thus, a global memory fetch should be weighted at 10× if a shared memory fetch
    equates to 1×.
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do we increase the arithmetic density of such instruction flows? First,
    we have to understand the underlying instruction set. The maximum operand size
    of an instruction is 128 bytes, a four-element vector load/store operation. This
    tells us the ideal chunk size for our data is four elements, assuming we’re using
    floats or integers, two if we’re using doubles. Thus, our operation should be
    in the first instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: I’ve written this in long-hand form to make the operations clear. If you extend
    the vector-type class yourself and provide a multiplication operator that performs
    this expanded code, you can simply write
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, the GPU hardware currently doesn’t support such vector manipulations,
    only loads, stores, moves, and pack/unpack from scalar types.
  prefs: []
  type: TYPE_NORMAL
- en: With such vector-based operations, we amortize the cost of the associated operations
    (load `A`, load `B`, write `C`, calculate `idx_A`, calculate `idx_B`, calculate
    `idx_C`) over four multiplies instead of one. The load and store operations take
    marginally longer as we have to introduce a pack and unpack operation that was
    not needed when accessing scalar parameters. We reduce the loop iterations by
    a factor of four with a consequential drop in the number of memory requests, issuing
    a much smaller number of larger requests to the memory system. This vastly improves
    performance (~20%), as we have seen with some examples in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Transcendental operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The GPU hardware is aimed at speeding up gaming environments. Often these require
    the manipulation of hundreds of thousands of polygons, modeling the real world
    in some way. There are certain accelerators built into the GPU hardware. These
    are dedicated sections of hardware designed for a single purpose. GPUs have the
    following such accelerators:'
  prefs: []
  type: TYPE_NORMAL
- en: • Division
  prefs: []
  type: TYPE_NORMAL
- en: • Square root
  prefs: []
  type: TYPE_NORMAL
- en: • Reciprocal square root
  prefs: []
  type: TYPE_NORMAL
- en: • Sine
  prefs: []
  type: TYPE_NORMAL
- en: • Cosine
  prefs: []
  type: TYPE_NORMAL
- en: • Log²
  prefs: []
  type: TYPE_NORMAL
- en: • Base 2 exponent Ex²
  prefs: []
  type: TYPE_NORMAL
- en: These various instructions perform operations to 24-bit accuracy, in line with
    the typical 24-bit RGB setup used in many game environments. None of these operations
    are enabled by default. Compute 1.x devices take various shortcuts that make single-precision
    math not IEEE 754 compliant. These will not be relevant to many applications,
    but be aware they are there. Fermi (compute 2.x) hardware brings IEEE compliance
    with regard to floating-point operations by default.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like the faster but less precise operation, you have to enable them
    using either the compile switch (`-use_fast_math`) or explicitly using intrinsic
    operations. The first step is simply to enable the option in the compiler and
    check the outcome of your existing application. The answer will be different,
    but by how much and how important this is, are the key questions. In the gaming
    industry it doesn’t matter if the flying globe projectile is one pixel off to
    the left or right of the target—no one will notice. In compute applications it
    can make a very real difference.
  prefs: []
  type: TYPE_NORMAL
- en: Individual operations can also be selectively enabled in 24-bit math using an
    explicit compiler intrinsic such as `__logf(x)`, etc. For a complete list of these
    and an explanation of the drawbacks of using them, see Appendix C.2 of the CUDA
    C programming guide. They can considerably speed up your kernels so it’s worth
    investigating if this is an option for your particular code.
  prefs: []
  type: TYPE_NORMAL
- en: Approximation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Approximation is a useful technique in problems that explore a certain search
    space. Double-precision math is particularly expensive, in the order of at least
    twice as slow as floating-point math. Single-precision math uses 24 bits for the
    mantissa and 8 bits for the exponent. Thus, in the compute 1.x devices a fast
    24-bit integer approximation could be used to provide an additional computation
    path to the single- and double-precision math. Note in Fermi, the 24-bit native
    integer support was replaced with 32-bit integer support, so an integer approximation
    in 24-bit math is actually slower than if the same approximation was made in 32-bit
    math.
  prefs: []
  type: TYPE_NORMAL
- en: In all compute hardware versions that natively support double precision (compute
    onwards), approximation in single precision is at least twice the speed of double-precision
    math. Sometimes a much higher speedup can be achieved because the single-precision
    calculations require less registers and thus potentially more blocks can be loaded
    into the hardware. Memory fetches are also half the size, doubling the effective
    per-element memory bandwidth. Consumer-based GPUs also have less double-precision
    units enabled in the hardware than their Tesla counterparts, making single-precision
    approximation a far more attractive proposition for such hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, with approximating you are performing a tradeoff between speed and
    accuracy and introducing additional complexity into the program. Often this is
    a tradeoff worth exploring, for it can bring a significant speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have done the approximation, the kernel can test the result to see if
    it is within a certain range or meets some criteria by which further analysis
    is warranted. For this subset of the dataset, the single- or double-precision
    calculation is performed as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The initial pass simply acts as a filter on the data. For every data point that
    falls outside the criteria of interest, you have saved the expensive double-precision
    calculations. For every point that falls into it, you have added an additional
    24- or 32-bit filtering calculation. Thus, the benefit of this approach depends
    on the relative cost of the additional filtering calculation versus the cost of
    double-precision math required for the full calculation. If the filters remove
    90% of the double-precision calculations, you have a huge speedup. However, if
    90% of the calculations require a further double-precision calculation, then this
    strategy is not useful.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA claims Tesla Fermi has in the order of 8× faster double-precision math
    over the previous compute 1.3 implementations (GT200 series). However, consumer-level
    Fermi cards are artificially restricted to one-quarter the double-precision performance
    of Tesla cards. Therefore, if double precision is key to your application, clearly
    a Tesla is the easy-fix solution to the problem. However, some may prefer the
    alternative of using multiple consumer GPUs. Two 3 GB 580 GTXs would likely provide
    a faster solution than a single Fermi Tesla for considerably less money.
  prefs: []
  type: TYPE_NORMAL
- en: If double precision is secondary or you simply wish to prototype a solution
    on commonly available hardware, then single precision of 24-bit filtered may be
    an attractive solution to this issue. Alternatively, if you have a mixture of
    GPUs, with an older card that is still good for single-precision usage, you can
    use the older card to scan the problem space for interesting sections, and the
    second card to investigate problem space in detail based on the likely candidates
    from the first card’s quick evaluation. Of course with a suitable Tesla card,
    you can perform both passes with just a single card.
  prefs: []
  type: TYPE_NORMAL
- en: Lookup tables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One common optimization technique used for complex algorithms is a lookup table.
    On CPUs where computation is quite expensive, these generally work reasonably
    well. The principle is that you calculate a number of representative points in
    the data space. You then apply an interpolation method between points based on
    the proportional distance to either edge point. This is typically used in modeling
    of the real world in that a linear interpolation method with a sufficient number
    of key sample points provides a good approximation of the actual signal.
  prefs: []
  type: TYPE_NORMAL
- en: A variation on this technique is used in brute-force attacks on ciphers. Passwords
    on most systems are stored as hashes, an apparently unintelligible series of digits.
    Hashes are designed so that it’s difficult to calculate the password from the
    hash by reversing the calculation. Otherwise, it would be trivial to calculate
    the original password based on a compromised hash table.
  prefs: []
  type: TYPE_NORMAL
- en: One method of attack on this type of system involves a CPU spending a considerable
    time generating all possible permutations based on the use of common and/or short
    passwords. The attacker then simply matches the precomputer hash against the target
    hash until such time as a match is made.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the lookup table method trades memory space for compute time.
    By simply storing the result, you have instant access to the answer. Many people
    will have learned multiplication tables in their heads as children. It’s the same
    principle; instead of tediously calculating *a* × *b*, for the most common set
    of values, we simply memorize the result.
  prefs: []
  type: TYPE_NORMAL
- en: This optimization technique works well on CPUs, especially older ones, where
    the compute time may be significant. However, as the compute resources have become
    faster and faster, it can be cheaper to calculate the results than to look them
    up from memory.
  prefs: []
  type: TYPE_NORMAL
- en: If you consider the average arithmetic instruction latency will be between 18
    to 24 cycles and the average memory fetch in the order of 400 to 600 cycles, you
    can clearly see we can do a lot of calculation work in the time it takes for the
    memory fetch to come back from global memory. This, however, assumes we have to
    go out to global memory for the result and that it’s not stored in shared memory
    or the cache. It also does not consider that the GPU, unlike the CPU, will not
    idle during this memory fetch time. In fact, the GPU will likely have switched
    to another thread and be performing some other operation. This, of course, depends
    on the number of available warps you have scheduled onto the device.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases the lookup may win over the calculation, especially where you
    are achieving a high level of GPU utilization. Where you have low utilization,
    the calculation method often wins out, depending of course on how complex the
    calculation really is. Let’s assume we have 20-cycle instruction latency for arithmetic
    operations and 600-cycle latency for memory operations. Clearly, if the calculation
    takes less than 30 operations it would be much faster than lookup in memory when
    we have low GPU utilization. In this case the SM is behaving like a serial processor,
    in that it has to wait for the memory fetch. With a reasonable utilization the
    memory fetch effectively becomes free, as the SM is simply executing other warps.
  prefs: []
  type: TYPE_NORMAL
- en: It’s often a case of trying this and seeing how well it works. Also be prepared
    to take it back out again should you suddenly manage to increase utilization of
    the GPU through other means.
  prefs: []
  type: TYPE_NORMAL
- en: Note, in the case of linear interpolation, a low-precision floating point–based
    linear interpolation is available in the GPU hardware. This is a feature of the
    texture memory hardware, something we do not cover in this text. Texture memory
    was useful for its cache features (24 K per SM) in compute 1.x hardware, but this
    use has largely been made redundant by the L1/L2 cache introduced in Fermi. However,
    the linear interpolation in hardware may still be useful for some problems. See
    the “Texture and Surface Memory” chapter of the CUDA programming guide if this
    is of interest to you.
  prefs: []
  type: TYPE_NORMAL
- en: Some common compiler optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll take a quick look at some compiler optimizations and how they affect GPUs.
    We cover these here to highlight cases where the optimizer may struggle and also
    to give you some understanding of how optimizations may be applied at the source
    code level where the automatic optimizations fail.
  prefs: []
  type: TYPE_NORMAL
- en: Some compilers are well known for producing efficient code on certain targets.
    Not surprisingly, the Intel ICC compiler produces extremely efficient code for
    the Intel platform. New features of the processor are incorporated rapidly to
    showcase the technology. Mainstream compilers often come from a code base that
    supports many targets. This allows for more efficient development, but means the
    compiler may not be so easy to customize for a single target.
  prefs: []
  type: TYPE_NORMAL
- en: As of the 4.1 SDK CUDA moved from using an Open64-based compiler to a more modern
    LLVM-based compiler. The most significant benefit from the user perspective is
    significantly faster compile times. NVIDIA also claims a 10% improvement in code
    speed. We saw noticeable improvements in code generation with this move. However,
    as with any new technology, there is room for improvement and I’m sure this will
    happen over time.
  prefs: []
  type: TYPE_NORMAL
- en: The optimizations compilers apply are well documented. What we present here
    is a broad overview of some common ones. For most programmers, simply setting
    the optimization level is entirely sufficient. Others prefer to know what exactly
    is going on and check the output. This is of course a tradeoff of your programming
    time versus the potential gain and the relative costs of these.
  prefs: []
  type: TYPE_NORMAL
- en: Strength reduction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When accessing an array index, typically nonoptimized compiler code will use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: This can be more efficiently replaced by one of two techniques. First, we must
    load the array base address (element 0) into a base register. Then we have the
    option of accessing an index as base + offset. We can also simply add a fixed
    offset, the size of an array element in bytes, to the base register after each
    loop iteration.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of C this is the same as writing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: In terms of GPU usage, this optimization relies on the fact that certain instructions
    (multiply, divide) are computationally expensive and others (addition) are cheaper.
    It tries to replace the expensive operations with cheaper (or faster) ones. This
    technique works well on CPUs as well as on GPUs. This is especially the case with
    compute 2.1 devices where integer addition has three times the throughput of integer
    multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Notice also that the pointer version of the code creates a dependency between
    loop iterations. The value of `ptr` must be known to execute the assignment. The
    first example is much easier to parallelize because there is no dependency on
    the loop iteration and the address of `a[i]` can easily be statically calculated.
    In fact, simply adding the `#pragma unroll` directive would have caused the compiler
    to unroll the entire loop, as the boundaries in this simple example are literals.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a typical example of a CPU-based optimization that may have been applied
    and to parallelize the loop you need to reverse-engineer back to the original
    code. It’s shown here because it helps you understand how C code may have been
    changed in the past to provide faster execution time for a given target. Like
    most optimizations at the C source code level, it can lead to the purpose of the
    source code being obscured.
  prefs: []
  type: TYPE_NORMAL
- en: Loop invariant analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loop invariant analysis looks for expressions that are constant within the loop
    body and moves them outside the loop body. Thus, for example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the parameter `j` is constant within the loop body for parameter
    `i`. Thus, the compiler can easily detect this and will move the calculation of
    `b` outside the inner loop and generate the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'This optimized code removes thousands of unnecessary calculations of `b`, where
    `j`, and thus `b`, are constant in the inner loop. However, consider the case
    where `b` is an external to the function, a global variable, instead of a local
    variable. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: The compiler cannot safely make this optimization because the write to `q` may
    affect `b`. That is, the memory space of `q` and `b` may intersect. It cannot
    even safely reuse the result of `j ∗ 200` in the assignment to `q`, but must reload
    it from memory, as the contents of `b` may have changed since the assignment in
    the prior line.
  prefs: []
  type: TYPE_NORMAL
- en: If you consider each line individually, then the issue becomes somewhat clearer.
    Any memory transaction, a read or write, will likely cause a switch to another
    warp, if that transaction involves accessing anything that is not immediately
    available. That area of global memory is accessible to any thread in any warp,
    on any active block in any SM. From one instruction to the next you get the very
    real possibility that any writable non register data could have changed.
  prefs: []
  type: TYPE_NORMAL
- en: You might say, well I’ve split up the application into *N* tiles and the tiles
    do not intersect, so this is not necessary. As the programmer you may know this,
    but it is very difficult for the compiler to figure that out. Consequently, it
    opts for the safe route and does not perform such optimizations. Many programmers
    do not understand what the optimization stage of a compiler does, and thus when
    it does something that breaks the code, they blame the compiler. Consequently,
    compilers tend to be rather conservative in how they optimize code.
  prefs: []
  type: TYPE_NORMAL
- en: As the programmer, understanding this allows you to make such optimization at
    the source level. Remember to think of global memory as you might a slow I/O device.
    Read from it once and reuse the data.
  prefs: []
  type: TYPE_NORMAL
- en: Loop unrolling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Loop unrolling is a technique that seeks to ensure you do a reasonable number
    of data operations for the overhead of running through a loop. Take the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of assembly code, this will generate:'
  prefs: []
  type: TYPE_NORMAL
- en: • A load of a register with 0 for parameter `i`.
  prefs: []
  type: TYPE_NORMAL
- en: • A test of the register with 100.
  prefs: []
  type: TYPE_NORMAL
- en: • A branch to either exit or execute the loop.
  prefs: []
  type: TYPE_NORMAL
- en: • An increment of the register holding the loop counter.
  prefs: []
  type: TYPE_NORMAL
- en: • An address calculation of array `q` indexed by `i`.
  prefs: []
  type: TYPE_NORMAL
- en: • A store of `i` to the calculated address.
  prefs: []
  type: TYPE_NORMAL
- en: Only the last of these instructions actually does some *real* work. The rest
    of the instructions are overhead.
  prefs: []
  type: TYPE_NORMAL
- en: We can rewrite this C code as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the ratio of useful work to overhead of using the loop is much increased.
    However, the size of the C source code is somewhat increased and it’s now less
    obvious what exactly it was doing compared to the first loop.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of PTX code, we see each C statement translated into PTX. For every
    branch test, there are now four memory copy operations. Thus, the GPU is executing
    more instructions than before, but a higher percentage of the memory copy operations
    are doing useful work.
  prefs: []
  type: TYPE_NORMAL
- en: In the CPU domain often there are limited registers, so the same registers will
    be reused in each step. This reduces register overhead, but means `q[i+1]` cannot
    start processing until `q[i]` has completed. We’d see the same overhead on the
    GPU with this approach. Each instruction has 20 cycles of latency. Therefore,
    the GPU assigns each address calculation to a separate register, so we have a
    set of four parallel instructions, rather than four sequential instructions executing.
    Each set is pushed into the pipelines and thus comes out one after another almost
    back to back.
  prefs: []
  type: TYPE_NORMAL
- en: With this approach the limit is the number of registers. As the GPU has 64 (compute
    2.x,3.0) and 128 (compute 1.x) maximum, there is considerable scope for unrolling
    small loop bodies and achieving a good speedup.
  prefs: []
  type: TYPE_NORMAL
- en: The NVCC compiler supports the `#pragma unroll` directive, which will automatically
    unroll fully such loops when the iteration count is constant or silently do nothing
    when it’s not. The latter is less than helpful, if the programmer has specified
    the loop should be unrolled. If the compiler is not able to, it should complain
    about this until the code is amended or the pragma removed.
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify `#pragma unroll 4` where four is replaced by any number
    the programmer wishes. Typically four or eight will work well, but beyond that
    too many registers will be used and this will result in register spilling. On
    compute 1.x hardware, this will cause a huge performance drop as registers are
    spilled to global memory. From compute 2.x hardware onwards, registers are spilled
    to the L1 cache and then to global memory if necessary. The best solution is to
    try it and see which value works best for each loop.
  prefs: []
  type: TYPE_NORMAL
- en: Loop peeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loop peeling is an enhancement to the loop unrolling, when the number of iterations
    is not an exact multiple of the loop unrolling size. Here the last few iterations
    are peeled away and done separately, and then the main body of the loop is unrolled.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have 101 loop iterations and plan to use four levels of loop
    unrolling, the first 100 iterations of the loop are unrolled and the final iteration
    is peeled away to allow the bulk of the code to operate on the unrolled code.
    The final few iterations are then handled as either a loop or explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Loop peeling can be equally applied to the start of a loop as to the end. It
    can be used in such cases to allow a nonaligned structure to be accessed as an
    aligned structure. For example, copying a byte-aligned memory section to another
    byte-aligned memory is slow because it has to be done one byte at a time. The
    first few iterations can be peeled away such that a 32-, 64-, or 128-byte alignment
    is achieved. Then the loop can switch to much faster word, double-, or quad-word
    based copies. At the end of the loop the byte-based copies can be used again.
  prefs: []
  type: TYPE_NORMAL
- en: When using the `#pragma loop unroll N` directive, the compiler will unroll the
    loop such that the number of iterations does not exceed the loop boundaries and
    insert the end of loop peeling code automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Peephole optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This optimization simple searches for combinations of instructions that can
    be replaced by more complex instructions with the same functionality. The classic
    example of this is multiply followed by an add instruction, as you might see in
    a gain and offset type calculation. This type of construct can be replaced with
    the more complex `madd` (multiply and add) instruction, reducing the number of
    instructions from two to one.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of peephole optimizations include simplification of flow of control,
    algebraic simplifications, and removal of unreachable code.
  prefs: []
  type: TYPE_NORMAL
- en: Common subexpressions and folding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many programmers write code that repeats some operation, for example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: In the first example, arrays `b` and `c` are indexed by the `base` and `i` parameters.
    Providing these parameters are within local scope, the compiler can simply calculate
    the index (`base + i`), and add this value to the start address of arrays `b`
    and `c` and to the work address for each parameter. However, if either of the
    index parameters are global variables, then the calculation must be repeated,
    since either could have changed once multiple threads are used. With a single
    thread it would be safe to eliminate the second calculation. With multiple threads
    it may also be safe to do so, but the compiler doesn’t know for sure, so will
    typically perform two calculations.
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, the term `NUM_ELEMENTS-1` is repeated. If we assume that
    `NUM_ELEMENTS` is a define, then the preprocessor will substitute the actual value,
    so we get `b[1024-1] ∗ c[1024-1]`. Clearly, 1024 − 1 can in both instances be
    replaced by 1023\. However, if `NUM_ELEMENTS` was actually a formal parameter,
    as it is in many kernel calls, this type of optimization is not available. In
    this case we have to drop back to common subexpression optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, be aware that in making such constants parameters of a function,
    or by having such parameters in global memory, you may be limiting the compiler’s
    ability to optimize the code. You then have to ensure such common subexpressions
    are not present in the source code. Often eliminating the common subexpressions
    makes the code simpler to understand and improves the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Divergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPUs execute code in blocks, or warps. A single instruction is decoded once
    and dispatched to a warp scheduler. There it remains in a queue until the warp
    dispatcher dispatches it to a set of 32 execution units, which execute that instruction.
  prefs: []
  type: TYPE_NORMAL
- en: This approach amortizes the instruction fetch and decoding time over *N* execution
    units. This in itself is very similar to the old vector machines. However, the
    main difference is that CUDA does not require that every instruction execute in
    this way. If there is a branch in the code and only some instructions follow this
    branch, those instructions diverge while the others wait at the point of divergence.
  prefs: []
  type: TYPE_NORMAL
- en: The single fetch/decode logic then fetches the instruction stream for the divergent
    threads and the other threads simply ignore it. In effect, each thread within
    the warp has a mask that enables its execution or not. Those threads not following
    the divergence have the mask cleared. Conversely, those following the branch have
    the bit set.
  prefs: []
  type: TYPE_NORMAL
- en: This type of arrangement is called predication. A predicate is created, which
    results in a single bit being set for those threads within a warp that follow
    the branch. Most PTX op-codes support an optional predicate allowing selective
    threads to execute an instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, for example, consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: In the first line of code the program eliminates all other warps in the current
    block except the first warp, the first 32 threads. This does not result in any
    divergence within the warp. The other warps in the block are simply not scheduled
    for this section of the code. They do not stall, but fall through the code and
    continue the execution of subsequent code.
  prefs: []
  type: TYPE_NORMAL
- en: The first warp then meets a test for `threadIdx.x < 16`, which splits the warp
    exactly in half. This is a special scenario where the warp does not actually diverge.
    Although the warp size is 32, the divergence criteria are actually a half-warp.
    If you noticed earlier, the CUDA cores are arranged in banks of 16 cores, not
    32 cores. The scheduler issues instructions to two or more sets of 16 cores per
    cycle. Thus both the true and false path of the conditional are executed.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent step, threads 16 to 31 call the function `func_b`. However,
    threads 0..15 hit another conditional. This time it’s not half-warp based, but
    quarter-warp based. The minimum scheduling quantity is 16 threads. Thus, the first
    set of eight threads jump off to call function `func_a1` while the second set
    of eight threads (8..15) stall.
  prefs: []
  type: TYPE_NORMAL
- en: Functions `func_b` and `func_a1` will continue to independently fetch instructions
    and dispatch them to the two half-warps. This is somewhat less efficient than
    a single instruction fetch, but nonetheless better than sequential execution.
    Eventually `func_a1` will complete and `func_a2` will start, stalling the threads
    0..7\. In the meantime `func_b` may have also completed. We can write a short
    test program to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '`  const u32 ∗ const b,`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '`   {`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '`      }`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have set up a number of kernels, each of which exhibit different levels
    of divergence. The first is the optimal with no divergence. The second diverges
    based on half-warps. These half-warps should execute in parallel. We then further
    subdivide the first half-warp into two groups. These should execute in series.
    We then subdivide again the first group into a total of four serial execution
    paths. The results we see are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: We can see this somewhat better in a graphical format in [Figure 9.32](#F0165).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-32-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.32 How thread divergence affects execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the thread divergence is not such a significant problem on the compute
    1.x devices (the 9800 GT and GTX260). It has an effect, but takes the maximum
    time to just 145% of the optimal time. By comparison, the Fermi compute 2.x cards
    (GTX460, GTX470) suffer over a 4× slowdown when diverging significantly within
    a warp. The GTX460 seems especially sensitive to warp divergence. Notice the GTX470
    is almost 10× faster in absolute terms than the 9800 GT when there is no divergence,
    which is a massive improvement for just two generations of cards.
  prefs: []
  type: TYPE_NORMAL
- en: If you are curious to know how much a 32-way divergence costs, it leads to a
    27× slowdown on the compute 1.x cards and a massive 125× to 134× slowdown on the
    compute 2.x cards. Note that the code for this test was a simple switch statement
    based on the thread index, so it is not directly comparable to the code we’re
    using here. However, clearly such divergence needs to be avoided at all costs.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest method of avoiding divergence within a warp is to simply mask out
    the sections of the warp you don’t wish to contribute to the result. How can you
    do this? Just perform the same calculation on every thread in the warp, but select
    a value that does not contribute for the threads you wish to mask out.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for a `min` operation on 32-bit integers, select 0xFFFFFFFF as
    the value for threads that should not contribute. Conversely for `max`, `sum`,
    and many other arithmetic-type operations, just use 0 in the threads you do not
    wish to contribute. This will usually be much quicker than branching within a
    warp.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the low-level assembly code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GPU compiles code into a virtual assembly system called PTX (Parallel Thread
    eXecution Instruction Set Architecture). This is a lot like Java byte-code in
    that it is a virtual assembly language. This can either be translated at compile
    time or runtime into the real code, which executes on the device. The compile
    time translation simply inserts a number of real binaries into the application,
    depending on which architectures you specify on the command line (the `–arch`
    switch).
  prefs: []
  type: TYPE_NORMAL
- en: To look at the virtual assembly generated, you simply add the `–keep` flag to
    the compiler command line. For Visual Studio users, the default NVIDIA projects
    contain an option to keep the PTX files (`–keep`) ([Figure 9.33](#F0170)). You
    can also specify the place to store them if you prefer they do not clutter up
    the project directory using the `–keep-dir <directory>` option.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-33-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.33 Visual C options—how to keep PTX files.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, PTX is not what is really executed on the hardware, so it’s useful
    only to a certain degree. You can also see the actual binary post translation
    using the `cuobjdump` utility as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cuobjdump –sass global_mem_sample_sort.sm_20.cubin > out.txt`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at a small device function, this is what we see at the various levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'In PTX:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '`{`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'And the actual generated code for a compute 2.0 device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: I’ve removed from the final generated code the actual raw hex codes, as they
    are not useful. Both PTX and the target assembler code use the format
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: The PTX code is extensively documented in the PTX ISA found in the “doc” directory
    of the NVIDIA GPU Computing Toolkit as the “ptx_isa_3.0.pdf” file for the CUDA
    4.1 SDK release. The binary instruction set is listed for GT200 and Fermi in the
    “cuobjdump.pdf” file found in the same directory. There is no detailed explanation
    of the actual instruction set as yet, as with the PTX, but it’s fairly easy to
    see which instructions map back to the PTX ISA.
  prefs: []
  type: TYPE_NORMAL
- en: While NVIDIA supports forward compatibility with the PTX ISA between revisions
    of hardware, that is, PTX for compute 1.x will run on compute 2.x, the binaries
    are not compatible. This support of older versions of PTX will usually involve
    the CUDA driver recompiling the code for the actual target hardware on-the-fly.
  prefs: []
  type: TYPE_NORMAL
- en: You should read the PTX ISA document and understand it well. It refers to CTAs
    a lot, which are cooperative thread arrays. This is what is termed a “block” (of
    threads) at the CUDA runtime layer.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the C code will drastically affect the final assembly code generated.
    It’s always good practice to look at the code being generated and ensure it is
    doing what is expected. If the compiler is reloading something from memory or
    doing something you would not expect, there is usually a good reason. You can
    usually then identify the cause in the C source code and eliminate the problem.
    In certain instances, you can also create inline PTX to get the exact functionality
    you require, although a lot of the very low-level instructions have equivalent
    compiler intrinsic functions that can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the easiest ways to look at and understand the low-level assembly functions
    is to view the interleaved source and assembly listing via the “View Disassembly”
    option from within Parallel Nsight. Simply set a breakpoint within the CUDA code,
    run the code from the Nsight menu (“Start CUDA Debugging”), and wait for the breakpoint
    to be hit. Then right-click near the breakpoint and the context menu will show
    “View Disassembly.” This brings up a new window showing the interleaved C, PTX,
    and SASS code. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '`0x0002caf0     MOV R11, R9;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: Here you can easily see how the C source code, a test for `threadIdx.x < 128`,
    is translated into PTX and how each PTX instruction is itself translated into
    one or more SASS instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Register usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Registers are the fastest storage mechanism on the GPU. They are the only way
    of achieving anything like the peak performance of the device. However, they are
    limited in their availability.
  prefs: []
  type: TYPE_NORMAL
- en: To launch a block onto an SM, the CUDA runtime will look at the block’s usage
    of registers and shared memory. If there are sufficient resources, the block will
    be launched. If not, the block will not. The number of blocks that are resident
    in an SM will vary, but typically you can achieve up to six blocks with reasonably
    complex kernels, and up to eight with simple ones (up to 16 on Kepler). The number
    of blocks is not really the main concern. It’s the overall number of threads as
    a percentage of the maximum number supported, which is the key factor.
  prefs: []
  type: TYPE_NORMAL
- en: We listed a number of tables in [Chapter 5](CHP005.html) that gave an overview
    of how the number of registers per block affects the number of blocks that can
    be scheduled onto an SM, and consequentially the number of threads that the device
    will select from.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compiler provides a `–v` option, which provides some more detailed output
    of what is currently allocated. An example of a typical kernel is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: The output is useful, but only if you understand what the compiler is telling
    you. The first item of interest is the `for sm_20` message, which tells you the
    code being created here is for the compute 2.x architecture (Fermi). If you’re
    using exclusively Fermi devices for your target deployment, then make sure your
    target is set correctly. By default you will generate compute 1.0 code unless
    you specify otherwise, which will restrict the available operations and generate
    code that is not as efficient as it could be for Fermi.
  prefs: []
  type: TYPE_NORMAL
- en: The next interesting point is `40 bytes of stack frame`, which generally means
    you have local variables you are taking the address of, or that you declared a
    local array. The term “local” in C refers to the scope of a variable, and in C++
    was replaced with the keyword “private,” which more accurately reflects what is
    meant.
  prefs: []
  type: TYPE_NORMAL
- en: In CUDA the term “local” refers to the scope of a variable for a given thread.
    Thus, the CUDA documentation also uses the term “local memory,” meaning thread
    private data. Unfortunately, “local” implies near or close, which in memory terms
    might imply the data is held close to the processor. In fact, “local data” is
    stored in either global memory for compute 1.x devices or in the L1 cache on Fermi
    devices. Thus, only on Fermi is it really “local” to the processor, and even in
    this case, its size is limited.
  prefs: []
  type: TYPE_NORMAL
- en: The stack frame is something you typically see with compute 2.x device code,
    especially if using atomic operations. The stack frame will also exist in the
    L1 cache unless it becomes too large. Where possible the CUDA compiler will simply
    inline calls to device functions, thereby removing the need to pass formal parameters
    to the called functions. If the stack frame is being created simply to pass values
    by reference (i.e., pointers) to the device function, it is often better to remove
    the call and manually inline the functions into the caller. This will eliminate
    the stack frame and generate a significant improvement in speed.
  prefs: []
  type: TYPE_NORMAL
- en: The next section lists `8+0 bytes lmem`. By “lmem” the compiler is referring
    to local memory. Thus, for 8 bytes, probably a couple of floats or integers have
    been placed into local memory. Again this is typically not a good indication as,
    especially in compute 1.x devices, there will be implicit memory fetches/writes
    to and from slow global memory. It’s an indication you need to think about how
    you might rewrite the kernel, perhaps placing these values into shared memory
    or constant memory if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Note the *a* + *b* notation used here denotes the total amount of variables
    declared in those sections (the first number), and then the amount used by the
    system (the second number). Also `smem` (shared memory) usages will be listed
    in addition to `lmem` if shared memory is used by the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Next we see `80 bytes cmem[0]`. This says the compiler has used 80 bytes of
    constant memory. Constant memory is typically used for parameter passing, as most
    formal parameters do not change across calls. The value in the square brackets
    is the constant memory bank used and is not relevant. Simply add all the `cmem`
    figures to obtain the total constant memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Register usage can also be controlled, or forced, using the `–maxrregcount n`
    option in the compiler. You can use this to instruct the compiler to use more
    or less registers than it currently is. You may wish to have fewer registers to
    squeeze another block onto the SM. You may already be limited by some other criteria
    such as shared memory usage, so you may wish to allow the compiler to use more
    registers. By using more registers the compiler may be able to reuse more values
    in registers, rather than store/fetch them again. Conversely, asking for less
    registers usage will usually cause more memory accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Asking for less registers to get an additional block is a tradeoff exercise.
    The lower register count and the additional block may bring higher occupancy,
    but this does not necessarily make the code run faster. This is a concept most
    programmers starting with CUDA struggle with. The various analyzer tools try to
    get you to achieve higher occupancy rates. For the most part this is a good thing,
    as it allows the hardware scheduler to have a wider choice of warps to run. However,
    *only if* the scheduler actually runs out of warps at some point, and thus the
    SM stalls, does adding more available warps actually help. Fermi, due to its dual
    warp dispatcher and higher number of CUDA cores per SM, executes warps with a
    higher frequency than earlier models. The effect varies between applications,
    but generally asking for less register usage usually results in slower code. Try
    it for your particular application and see. We look at how you can see if the
    SMs are stalling in the later section on analysis tools.
  prefs: []
  type: TYPE_NORMAL
- en: A better approach to asking for less registers is to understand the register
    usage and allocation of variables. To do this, you need to look into the PTX code,
    using the `–keep` compiler flag. PTX, the virtual assembly language used by CUDA,
    defines a number of state spaces. A variable exists in one of these state spaces.
    These are shown in [Table 9.8](#T0045). Thus, you can always look into the PTX
    code to see where a variable has been placed.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.8 PTX State Space
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000090tabT0045.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Reducing register usage from say 26 to 25 per kernel will have little effect.
    However, transitioning over a register boundary (16, 20, 24, and 32) will usually
    allow for more blocks to be scheduled. This will bring a greater selection of
    warps and will usually improve performance. This is not always the case. More
    blocks can mean more contention for shared resources (shared memory, L1/L2 caches).
  prefs: []
  type: TYPE_NORMAL
- en: Register usage can often be reduced simply be rearranging the C source code.
    By bringing the assignment and usage of a variable closer together you enable
    the compiler to reuse registers. Thus, at the start of the kernel you might assign
    `a`, `b`, and `c`. If in fact they are used only later in the kernel, you’ll often
    find reduced register usage by moving the creation and assignment close to the
    usage. The compiler may then be able to use a single register for all three variables,
    as they exist in distinct and disjoint phases of the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Understand how thread layout impacts memory and cache access patterns.
  prefs: []
  type: TYPE_NORMAL
- en: • Use only multiples of 32 when specifying the thread count for kernel launch.
  prefs: []
  type: TYPE_NORMAL
- en: • Think about how to increase the amount of work performed per memory fetch.
  prefs: []
  type: TYPE_NORMAL
- en: • Understand at least a little of how compilers work when optimizing code and
    adapt your source code to aid the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: • Consider how branching within a warp can be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: • Look at the PTX and final target code to ensure the compiler is not generating
    inefficient code. If it is, understand why and make changes at the source level
    to address it.
  prefs: []
  type: TYPE_NORMAL
- en: • Be aware and understand where data is being placed and what the compiler is
    telling you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 5: Algorithms'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Selecting an efficient algorithm on the GPU can be challenging. The best algorithm
    in the CPU domain is not necessarily the best for the GPU. The GPU has its own
    unique challenges. To get the best performance you need to understand the hardware.
    Thus, when considering algorithms, we need to think about:'
  prefs: []
  type: TYPE_NORMAL
- en: • How to decompose the problem into blocks or tiles and then how to decompose
    those blocks into threads.
  prefs: []
  type: TYPE_NORMAL
- en: • How the threads will access the data and what sort of memory pattern this
    will generate.
  prefs: []
  type: TYPE_NORMAL
- en: • What data reuse opportunities are present and how these can be realized.
  prefs: []
  type: TYPE_NORMAL
- en: • How much work the algorithm will be performing in total and whether there
    is a significantly difference from a serial implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an 800-plus-page book published by Morgan Kaufman entitled *GPU Computing
    Gems* that covers in detail implementation of various algorithms for the following
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: • Scientific simulation
  prefs: []
  type: TYPE_NORMAL
- en: • Life sciences
  prefs: []
  type: TYPE_NORMAL
- en: • Statistical modeling
  prefs: []
  type: TYPE_NORMAL
- en: • Data-intensive applications
  prefs: []
  type: TYPE_NORMAL
- en: • Electronic design and automation
  prefs: []
  type: TYPE_NORMAL
- en: • Ray tracing and rendering
  prefs: []
  type: TYPE_NORMAL
- en: • Computer vision
  prefs: []
  type: TYPE_NORMAL
- en: • Video and image processing
  prefs: []
  type: TYPE_NORMAL
- en: • Medical imaging
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this section is not to look at algorithms that are specific to
    certain fields, as they are of limited general interest. Here we look at a few
    common algorithms that can be implemented, which in turn may form building blocks
    for more complex algorithms. This book is not about providing sets of examples
    you can copy and paste, but providing examples where you can learn the concepts
    of what makes good CUDA programs.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many, many sorting algorithms available, some of which can easily
    and efficiently be implemented on the GPU and many of which are not well suited.
    We’ve looked already in previous chapters at merge sort, radix sort, and the more
    exotic sample sort. We’ll look here at one more parallel sort that is useful in
    terms of looking at how algorithms are implemented in GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Odd/even sort
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An odd/even sort works by selecting every even array index and comparing it
    with the higher adjacent odd array index ([Figure 9.34](#F0175)). If the number
    at the even element is larger than the element at the odd index, the elements
    are swapped. The process is then repeated, starting with the odd indexes and comparing
    them with the higher adjacent even index. This is repeated until we make no swaps,
    at which point the list is sorted.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-34-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.34 Odd/even sort.
  prefs: []
  type: TYPE_NORMAL
- en: An odd/even sort is a variation of a bubble sort. A bubble sort works by selecting
    the number at the first index and comparing and swapping it with the index to
    the right until such time as it’s no longer larger than the number to its right.
    The odd/even sort simply extends this to use *P* independent threads to do this,
    where *P* is half the number of elements in the list.
  prefs: []
  type: TYPE_NORMAL
- en: If we define the number of elements in an array as *N*, then the ability to
    deploy half of *N* threads may be appealing. The sort is also quite easy to conceptualize,
    but raises some interesting problems when trying to implement on the GPU, so it
    is a good example to look at.
  prefs: []
  type: TYPE_NORMAL
- en: The first issue is that odd/even sort is designed for parallel systems where
    individual processor elements can exchange data with their immediate neighbor.
    It requires a connection to the left and right neighbor only. A connection for
    our purposes will be via shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Having thread 0 access array elements zero *and* one and thread 1 access elements
    two *and* three causes a sequence issue for the coalescing hardware. It needs
    each thread to access a contiguous pattern for a coalesced access. Thus, on compute
    1.x hardware this access pattern is terrible, resulting in multiple 32-byte fetches.
    However, on compute 2.x hardware, the accesses fetch at most two cache lines.
    The additional data fetched from the even cycle will likely be available for the
    odd cycle and vice versa. There is also a significant amount of data reuse with
    a high degree of locality, suggesting cache and/or shared memory would be a good
    choice. Shared memory would likely be the only choice for compute 1.x devices
    due to the poor coalescing.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider shared memory, we need to think about bank conflicts. Thread
    0 would need to read banks 0 and 1, plus write to bank 0\. Thread 1 would need
    to reads banks 2 and 3 and write to bank 2\. In a compute 1.x system with 16 banks,
    thread 8 would wrap around and start accessing banks 0 and 1\. On compute 2.0
    hardware, we’d see the same effect at thread 16\. Thus, we’d have four bank conflicts
    per thread on compute 1.x hardware and two bank conflicts per thread on compute
    2.x hardware with a shared memory implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CPU code for odd/even sort is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '`  else`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: The code iterates over the dataset from array element 0 to `num_elem-1` and
    then from element 1 to `num_elem-2`. The two data elements are read into local
    variables and compared. They are swapped if necessary and a counter `num_swaps`
    is used to keep track of the number of swaps done. When no swaps are necessary,
    the list is sorted.
  prefs: []
  type: TYPE_NORMAL
- en: For a mostly sorted list, such algorithms work well. The reverse sorted list
    is the worst case, where we have to move elements all through the list to the
    end. The output of a reverse sorted list is shown here. We can see in each stage
    how the values move between the cells.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'For the GPU implementation, we’ll use global memory on a compute 2.x device.
    The GPU implementation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '`  u32 num_swaps;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: Instead of the traditional `for` loop construct, the CPU code uses a `do..while`
    construct. The obvious choice for parallelism from the algorithm is the compare
    and swap operation, meaning we need *N*/2 threads where *N* is the number of elements
    in the array. Given that most lists we’d bother sorting on the GPU will be large,
    this gives us potential to make use of the maximum number of threads on a given
    device (24,576 threads on GTX580).
  prefs: []
  type: TYPE_NORMAL
- en: As each thread processes two elements we cannot simply use `tid` as the array
    index, so create a new local parameter `tid_idx`, which is used to index into
    the array. We also create a parameter `tid_idx_max`, which is set to the last
    value in the array, or the last value in the current block where there is more
    than one block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: The end condition is somewhat problematic. The parameter `num_swap`s in the
    serial version is written to only once per iteration. In the parallel version
    we need to know if *any* thread did a swap. We could therefore use an atomic `add`,
    `increment`, `AND`, or `OR` operation for this, but this would represent a serial
    bottleneck, as every thread that did a write would have to be serialized.
  prefs: []
  type: TYPE_NORMAL
- en: We could mitigate the cost of the atomic operations somewhat by using a shared
    memory atomic operation. Note that shared memory atomics are supported only on
    compute 1.2 hardware or later (the GT200 series). For the older compute 1.1 hardware
    (the 9000 series) we’d need to use global memory atomics. The definition of the
    `num_swaps` variable would need to be changed accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: For compute 2.x hardware, there is a much faster solution that we will use here.
    As we have to wait at the end of each round anyway, we can make use of the newly
    provided primitive, `__syncthreads_count`, to which we pass a predicate. If the
    predicate is nonzero in any of the threads, then the result to all threads is
    also nonzero. Thus, if just one thread does a swap, all threads again iterate
    around the loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: The host function to call the kernel is also shown here for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '`    %d threads (%u active)", num_blocks,`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: 'One question that should be in your mind about this code is what happens at
    the block boundaries. Let’s look at the results with one and two blocks with a
    dataset small enough to print here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: Notice in the second output the sort occurred only within the block. The values
    on the right needed to propagate to the left and vice versa. However, as the blocks
    do not overlap, they are not able to do this. The obvious solution would be to
    overlap the blocks, but this would not be an ideal solution.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA was designed to allow blocks to run in any order, without a means for cross-block
    synchronization within a single kernel run. It’s possible by issuing multiple
    kernels to synchronize between blocks, but this mechanism works well only where
    you have a small number of synchronization steps. In this kernel we’d need to
    overlap the blocks on every iteration. This would lose all locality, as now two
    SMs need to share the same dataset. We also need a global memory atomic or a reduction
    operation to keep track of whether any blocks performed a swap and have to continue
    issuing kernels until no swaps had taken place in any block—a lot of host interaction.
    That would not be a good route to go down.
  prefs: []
  type: TYPE_NORMAL
- en: So we’re left with the two choices found in most sorts that decompose into blocks.
    Either presort the input lists so the values in list *N*[−1] are less than *N*[0],
    which are larger than *N*[1], the solution we used with the sample sort, or merge
    *N* separate lists, the merge sort problem we also looked at earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reduction is used significantly in parallel programming. We’ll look at some
    of the many ways we can perform a reduction to see which method produces the best
    results on the various compute platforms and to understand why.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look first at computing the sum of *N* 32-bit integers, some 48 million
    to give a reasonable sized dataset. With such a large number of values one of
    the first issues we need to consider is overflow. If we add 0xFFFFFFFF and 0x00000001
    then we have an overflow condition with a 32-bit number. Therefore, we need to
    accumulate into a 64-bit number. This presents some issues.
  prefs: []
  type: TYPE_NORMAL
- en: First, any atomic-based accumulation would require an atomic 64-bit integer
    add. Unfortunately, this is supported in shared memory only with compute 2.x hardware
    and in global memory only in compute 1.2 hardware onward.
  prefs: []
  type: TYPE_NORMAL
- en: Global atomic add
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s look first at the simplest form of reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: In this first example, each thread reads a single element from memory and adds
    it to a single result in global memory. This, although very simple, is probably
    one of the worst forms of a reduce operation. The interblock atomic operation
    means the value needs to be shared across all of the SMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the older hardware this means physically writing to global memory. In the
    compute 2.x hardware this means maintaining an L2 cache entry, shared among all
    the SMs, and eventually writing this to global memory. The results we see are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: We’ll look here at the compute 2.x devices, as these support 64-bit integer
    atomics.
  prefs: []
  type: TYPE_NORMAL
- en: The issue with the atomic writes, even to L2 cache, is they force a serialization
    of the threads. We have six blocks in each SM, 256 threads per block, generating
    1536 threads per SM. On the GTX470 we have 14 SMs, so a total of 21, 504 active
    threads. On the GTX460 we have 7 SMs, so a total of 10,752 active threads. Performing
    an atomic operation on a single global memory cell means we create a lineup, or
    serialization, of 10 K to 21 K threads. Every thread has to queue, once for every
    single element it processes. Clearly a poor solution, even if it is a somewhat
    simple one.
  prefs: []
  type: TYPE_NORMAL
- en: Reduction within the threads
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can improve this situation by performing some of the reduction within the
    thread. We can do this very simply by changing the data type and adjusting the
    kernel to ensure we don’t go out of bounds.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '`// address in GMEM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first example we process two elements per thread and four in the second
    using the built-in vector types `uint2` and `uint4`. This produces the following
    timings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: Although, a dramatic reduction, we’ve not really solved the problem. All we
    have done is to half or quarter the number of times each thread has to queue by
    performing a local reduction. This drops the overall time to approximately one-half
    and one-quarter of the original. However, there is still a 5 K thread lineup trying
    to write to the global memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note in performing the addition locally, we reduce the number of global writes
    by a factor equal to the level of ILP. However, we have to be careful about how
    the addition is performed. You could write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: In C, an expression is typically evaluated from left to right. A promotion of
    the left operator generates an implicit promotion of the right operator. Thus,
    you might expect `element.x` to be promoted to an unsigned 64-bit type, and as
    `element.y` is to be added to it, it will also be promoted. As `element.z` and
    `element.w` will subsequently be added, you might also expect these to be promoted.
    You are, however, thinking like a serial programmer. The `z` and `w` elements
    can be calculated independently of `x` and `y`. This is exactly what the PTX code
    does. As neither `z` nor `w` has been promoted to a 64-bit value, the addition
    is done as a 32-bit addition, which may result in an overflow.
  prefs: []
  type: TYPE_NORMAL
- en: The problem lies in that C permits any order of evaluation where the operator
    is commutative. However, as you typically see a left to right evaluation, people
    assume this is how all compilers work. This is one of the portability issues between
    C compilers. When we move to a superscalar processor such as a GPU, it performs
    the two sets of additions independently to make the maximum use of the pipeline.
    We don’t want it to wait 18–22 plus cycles for the first addition to complete
    then make the subsequent additions in series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the correct way to write such additions is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: Here every value is converted to a 64-bit number before the addition takes place.
    Then any ordering of the addition is fine for integer values. Note for floating-point
    values simply converting to doubles is not enough. Due to the way floating-point
    numbers work adding a very tiny number to a very large number will result in the
    small number being discarded, as the floating-point notation does not have the
    required resolution to hold such values. The best approach to this type of problem
    is to first sort the floating-point values and work from the smallest number to
    the largest.
  prefs: []
  type: TYPE_NORMAL
- en: We can take the ILP technique a little further by using multiple elements of
    `uint4` and adjusting the kernel accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '`  u64 value = ((u64)element.x) + `'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '`     ((u64)element.y) + `'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: '`     ((u64)element.w);`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we’re mixing the loading of data with the addition. We could move
    all the loads to the start of the function. However, consider that each `uint4`
    type requires four registers. Thus, the `ILP32` example would require 32 registers
    just to hold the values from a single read iteration. In addition, some are needed
    for the addition and final write. If we use too many registers, the number of
    blocks that can be scheduled is reduced or the kernel spills registers to “local”
    memory. Such local memory is the L1 cache for compute 2.x devices and global memory
    for the compute 1.x devices. The results for these ILP kernels are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: We can see that ILP significantly decreases the execution time, providing it’s
    not taken too far. Note the `ILP32` solution actually takes longer. Despite achieving
    a 20× speedup over the simplest version, we have still not solved the atomic write
    queuing problem, just reduced the overall number of elements. There are still
    too many active threads (10–21 K) all trying to write to the single accumulator.
  prefs: []
  type: TYPE_NORMAL
- en: Reduction of the number of blocks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Currently, we’re invoking *N* blocks where *N* is the problem size, 12 million
    elements (48 MB) divided by the number of threads per block multiplied by the
    number of elements processed per block. We finally get *N* atomic writes, all
    of which are serialized and cause a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: We can reduce the number of contentions if we create far, far less blocks and
    greatly increase the amount of work each block performs. However, we have to do
    this without increasing the register usage, something the `ILP32` example did.
    This, in turn, caused a slowdown due to local memory reads and writes.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, we launch 48 K blocks, but could reduce this to 16, 32, 64, 128,
    or 256 blocks. We can then have each thread march through memory, accumulating
    the result to a register, and only when the block is complete, write out the result.
    Depending on the number of blocks, this should generate quite good locality of
    memory references between the SMs, thus making good use of the memory bandwidth
    and L2 cache if present.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: The first task is to work out how many iterations over the data each thread
    needs to make. The parameter `gridDim.x` holds the number of blocks launched.
    Each block consists of `blockDim.x` threads. Thus, we can work out how many elements
    of data each thread must accumulate. We then accumulate these in `local_result`,
    and only when the block is complete, do a single write to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: This reduces the contention from a thread-level contention to a block-level
    contention. As we’re only launching a few hundred blocks, the probability of them
    all requiring the write at the same time is reasonably low. Clearly as we increase
    the number of blocks, the potential contention increases. Once we have loaded
    all the SMs with the maximum number of permitted blocks, there is little reason
    to increase the number of blocks further, other than for work balancing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GTX460 is perhaps the worst example, as with only 7 SMs, each with 6 blocks,
    we should saturate the device at only 42 blocks. The GTX470 would need 90 blocks.
    We, therefore, try all number of blocks (49,152) down to 16 in powers of two,
    fewer blocks than would be necessary to fully populate the SMs. This generates
    the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: If we look at this first on the very large number of blocks we see a fairly
    linear drop as we halve the number of blocks for each run, for both the GTX470
    and GTX460 cards. We’re halving the number of blocks each cycle by increasing
    the amount of work done per thread, but without increasing the ILP (indicated
    here with `loop1`).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the GTX460 has consistently outperformed the GTX470 in the previous
    examples. It does this until such time as we get down to a very small number of
    blocks ([Figure 9.35](#F0180)). At 384 blocks we see the GTX470 outperform the
    GTX460\. The GTX470’s larger number of smaller SMs (32 CUDA cores versus 48 CUDA
    cores each) and larger cache starts to impact performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-35-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.35 Time (ms) versus number of blocks (large number of blocks).
  prefs: []
  type: TYPE_NORMAL
- en: If you then look at the timing with a very small number of blocks, you can see
    that around 64 blocks is the minimum needed before the number of SM scheduling/occupancy
    issues come into play ([Figure 9.36](#F0185)). In the figure, we’ve split the
    graphs into one with a large number of blocks and one with a smaller number, so
    we can see the time at small block numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-36-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.36 Time (ms) versus number of blocks (small number of blocks).
  prefs: []
  type: TYPE_NORMAL
- en: Note so far we’ve used no ILP (instruction-level parallelism). However, we know
    that introducing ILP allows us to achieve better timing. This is especially the
    case when we have a small number of blocks. The optimal timing is for 64 blocks.
    The GTX470 would have just over 4 blocks, 32 warps per SM. With 32-bit memory
    fetches we need a fully loaded SM, 48 warps, to achieve peak bandwidth from the
    global memory. We can achieve this only with ILP while maintaining this number
    of warps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: '` const uint4 ∗ const data,`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: 'The effect on introducing ILP has one additional benefit: The time spent performing
    the loop (overhead) is amortized over more useful instructions (memory fetch,
    add). We therefore see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: In `loop1` we use a single 32-bit element, in `loop2` we use two elements (`uint2`),
    and in `loop4` we use four elements (`uint4`). In each case we use 64 blocks,
    the best result from the previous test. You can see that moving from 32-bit elements
    per thread to 64-bit elements per thread we gain on the order of 20–25%. Moving
    from 64-bit reads to 128-bit reads gains us almost nothing on the GTX470, but
    on the order of an 8% gain on the GTX460\. This is entirely consistent with the
    bandwidth results we looked at earlier where the GTX460 (compute 2.1) device achieved
    a significantly higher bandwidth when using 128-bit reads instead of 64-bit reads.
  prefs: []
  type: TYPE_NORMAL
- en: Reduction using shared memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we look at the last instruction of the kernel to date, we still have one
    issue, using an atomic add to write out the result. With 256 threads per block
    and 64 blocks resident, we have 16 K threads all trying to write to this final
    accumulated value. What we actually need is a reduction across the threads within
    the block. This would drop the number of writes from 16 K to just 64, the number
    of blocks. This should reduce the overall timing considerably, as we’re removing
    the serialization bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: However, going back to the first section in this chapter, know when fast is
    fast enough and appreciate the additional effort required to squeeze that last
    few percent out of the problem. Notice as the speed has improved, the kernels
    become more and more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory is a bank-switched set of 32 banks (16 in compute 1.x). Providing
    each thread uses a unique bank index (0..31) the shared memory can process one
    element per clock, per thread. This is its peak performance, for a single warp.
    As we introduce more warps, if they too want to access shared memory, the ability
    of one warp to use the full bandwidth of shared memory is reduced as it must share
    the LSUs with other competing warps. Once the LSUs are running at 100% capacity,
    we’re limited by the bandwidth from the combined 64 K of L1 cache/shared memory
    on the SM.
  prefs: []
  type: TYPE_NORMAL
- en: We could simply perform a block-level reduction into a single shared memory
    value for each SM. Thus, with 256 threads we’d have a 256:1 reduction ratio. However,
    this proves not to be particularly effective, as each of the 256 threads is serialized.
  prefs: []
  type: TYPE_NORMAL
- en: The execution units within an SM can execute a half-warp, a group of 16 threads.
    Therefore, it makes sense to perform a reduction across half-warps. We could then
    either perform an additional reduction across the set of 16 half-warps, or we
    could simply write out the set of values to shared memory. It turns out there
    is almost no difference in execution time between the two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The problem, however, with a subsequent intrablock reduction in shared memory
    is where to locate the shared memory parameter to perform the reduction. If you
    place it after the set of 64 bytes occupied by the intrawarp reduction parameters,
    it causes the next block of intrawarp not to be 64-byte aligned. The different
    blocks interact with one another to cause bank conflicts in the shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: We opted for the direct write to global memory, as this was the simpler solution
    and shows marginal if any difference in performance. Thus, instead of reducing
    the 16 K conflicting writes to 64 potentially conflicting writes, we have 512
    potentially conflicting writes, which is a factor of 32 reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `loopB` has 512 atomic writes to global memory. The second
    kernel, `loopC`, performs an additional intrablock reduction before making 64
    atomic writes to global memory. As you can see, there is little if any difference
    in performance, demonstrating the additional reduction step gains us nothing and
    therefore was removed from the final solution. This is not really too surprising,
    as if the latency of the 512 memory writes is already hidden by the considerable
    computation workload, reducing this to just 64 writes would bring us nothing.
  prefs: []
  type: TYPE_NORMAL
- en: If we compare the best result from the previous section, using an accumulation
    into registers and then writing out the 16 K values we see on the GTX470 (compute
    2.0), this took 1.14 ms. By adding this further reduction step in shared memory
    we’ve reduced this to just 0.93 ms, a 19% saving in execution time. As the GTX470
    has 14 SMs, this intra-SM reduction step significantly reduces the number of final
    atomic global memory writes that must be coordinated between these SMs.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, the GTX460 device (compute 2.1) reduced from 1.38 ms to 1.33 ms,
    just 4%. The absolute difference is of course clear in that the GTX470 has a 320-bit
    memory bus compared with the 256-bit memory bus on the GTX460\. It’s the relative
    speedup difference that is interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Such a small speedup would indicate that the multiple global memory atomic operations
    were not in fact the bottleneck as they were on the GTX470\. It could also indicate
    that perhaps we were already using the LSUs to their full capacity. The ratio
    of LSUs to CUDA cores is much less on the compute 2.1 devices than on the compute
    2.0 devices. Both global memory and shared memory accesses require the LSUs.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the shared memory–based reduction, based on half-warps, gains us a significant
    reduction over the purely atomic/global memory–based solution in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As with any implementation, you should always look to what previous work has
    been done and how this could be used to improve existing designs. Mark Harris
    wrote an excellent study of parallel reduction^([3](#FN3)) back in the early GPU
    days based on the G80 device. Instead of performing a 512:16 reduction, it writes
    the entire set of values to shared memory and then uses shared memory to perform
    a series of partial reductions, always accumulating the result to shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: The results are impressive. He used unsigned integer elements and achieved a
    total time of 0.268 ms on 4 million elements. Scaling this to the 12 million elements
    (48 MB data) we used in the example works out to 1.14 ms, a comparable number
    to the 0.93 ms we achieved on the GTX470.
  prefs: []
  type: TYPE_NORMAL
- en: However, the GTX470 has 448 CUDA cores, compared to the 128 CUDA cores of the
    G80, a factor of 3.5× improvement in arithmetic capacity. Memory bandwidth has
    increased from 86 GB/s to 134 GB/s, a factor of 1.5×. However, Mark’s kernel accumulates
    into 32-bit integers, whereas we accumulate into 64-bit integers to avoid the
    overflow problem. Therefore, the kernels are not directly comparable.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless the method proposed may produce good results. Accumulation into
    a register will clearly be faster than accumulation into shared memory. As the
    hardware does not support operations that directly operate on shared memory, to
    perform any operation we need to move the data to and from shared memory. One
    of the reasons for selecting register-based accumulation was the elimination of
    this overhead. However, that is not to say we have an optimum set of code for
    this part of reduction yet.
  prefs: []
  type: TYPE_NORMAL
- en: Some time has passed since this chapter was originally written and this late
    addition comes after a transition from CUDA 4.0 to CUDA 4.1 SDK, which moved us
    from the Open64 compiler to an LLVM-based compiler. This should bring a performance
    boost, and indeed we find the more efficient compiler generates an execution time
    of 0.74 ms instead of our previous 0.93 ms, a huge improvement just from changing
    compilers.
  prefs: []
  type: TYPE_NORMAL
- en: However, of this time, how much is actually due to the reduction at the end
    of the code? We can find out simply by commenting out the final reduction. When
    we do this, the time drops to 0.58 ms, a drop of 0.16 ms or some 21%. Further
    investigation reveals that actually all but 0.1 ms of this time can be attributed
    to the atomic add operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the 2.1 version of Parallel Nsight we can extract a number of useful
    facts from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: • Of the 48 scheduled warps, on average we get only 32 active warps.
  prefs: []
  type: TYPE_NORMAL
- en: • The workload is unevenly distributed between the SMs.
  prefs: []
  type: TYPE_NORMAL
- en: • Most issue dependencies are the short class.
  prefs: []
  type: TYPE_NORMAL
- en: • There is very little divergent branching.
  prefs: []
  type: TYPE_NORMAL
- en: • Around 8% of the time the SMs stalled. This was due mostly to either instruction
    fetch or instruction dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: This occupancy issue is a somewhat misleading one, in that it is caused by the
    uneven distribution rather than some runtime issue. The problem is the number
    of blocks launched. With 14 SMs, we can have 84 blocks resident with 6 blocks
    per SM. Unfortunately we only launch 64, so in fact some of the SMs are not fully
    loaded with blocks. This drops the average executed warps per SM and means some
    SMs idle at the end of the workload.
  prefs: []
  type: TYPE_NORMAL
- en: 'We ended up with a value of 64 due to it being identified as an ideal number
    from the earlier experiments. However, these were based on 16 K competing atomic
    writes to global memory. We’ve since reduced this to just 512 writes with most
    of the atomic writes being within the SM. Once we remove this global bottleneck,
    it would appear that 64 blocks in total is not the ideal number. Running a sample
    we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: '`ID:0 GeForce GTX 470:GMEM loopC 48 passed Time 0.82 ms`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: Notice the best number of blocks on the GTX470 is 384, while on the GTX460 it
    is 96\. A value of 192 works well on both devices. Clearly, however, a value of
    64 blocks does not work well.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, what about the last issue we noticed, that 8% of the time the SMs
    were idle? Well this improves to 7% when there are additional blocks, so this
    is helping. However, what is the cause of the problem? Looking to the kernel output
    gives us a clue to the issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice, unlike the CUDA 4.0 SDK compiler, the 4.1 compiler places `uint4` types
    into local memory. This local memory on Fermi is the L1 cache, so should you care?
    We can rewrite the `uint4` access to use a `uint4` pointer. As the `uint4` types
    are 128-bit aligned (4 × 32 bit words), they are guaranteed to sit on a cache
    line and memory transaction boundary. Thus, an access to the first element of
    the `uint4` by any thread will pull the remaining three elements into the L1 cache.
    Consequently, we have L1 local memory access versus L1 direct cache access. There
    should be no difference, in theory. Let’s see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: Both the GTX470 and GTX460 devices show a significant drop in the execution
    time. Looking to the cache utilization statistics, we can see the L1 cache hit
    rate has jumped from 61.1% to 74.5% as we have moved from the local memory version
    (`loopC`) to the pointer version (`loopD`). We also see the percentage of stalls
    in the SMs drops to 5%. Actually for this statistic, the difference on the GTX460
    is quite pronounced, as it started off at 9%, slightly higher than the GTX470\.
    This is likely to be because we’re now able to share the L1 cache data between
    threads as the data is no longer “thread private.”
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why we simply do not just use 84 blocks as we calculated
    earlier. The issue is one of rounding. The 12 million element dataset does not
    equally divide into 84 blocks. Thus, some blocks would need to process more than
    others. This means the logic would need to be more complex, but more complex for
    *every* block executed. Just running 84 blocks without solving this issue shows
    a time of 0.62 ms, a gain of 0.06 ms over the 384-block version. This demonstrates
    that the 384-block version introduces small enough blocks that the existing load-balancing
    mechanism handles it quite well. The value of making the code more complex significantly
    outweighs the benefits and is only necessary if we in fact do not know the size
    of the input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the question of shared memory versus atomics, which is faster?
    We can replace the atomic-based reduction with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the code works. First, all 256 threads (warps 0..6) write out their
    current `local_result` to an array of 256 64-bit values in shared memory. Then
    those threads numbered 0 to 127 (warps 0..3) add to their result, the result from
    the upper set of warps. As the warps within a block are cooperating with one another,
    we need to ensure each warp runs to completion, so add the necessary `__syncthreads()`call.
  prefs: []
  type: TYPE_NORMAL
- en: We continue this reduction until the point at which we reach 32 threads, the
    size of a single warp. At this point all threads within the warp are synchronous.
    Thus, we no longer need to synchronize the threads, as the thread sync operation
    is really a warp sync operation within a single block.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a couple of choices. We could continue the `if threadIdx.x < threshold`
    operation or we can simply ignore the fact that the redundant threads within the
    warp perform a useless operation. The additional test actually generates a considerable
    number of additional instructions, so we simply calculated all values within the
    warp. Note that this is different than running multiple warps, as in the case
    where we have the 128 and 64 test. Within a single warp, reducing the number of
    threads gains us nothing. By comparison, the prior tests eliminate entire warps.
  prefs: []
  type: TYPE_NORMAL
- en: So does this gain us anything compared to the atomic reduction?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: Compared with the last version, we moved from 0.68 ms to 0.64 ms on the GTX470
    and 0.8 ms to 0.79 ms on the GTX460\. Not a significant gain, but nonetheless
    a gain in execution speed. We can provide one last optimization to this code before
    we move on.
  prefs: []
  type: TYPE_NORMAL
- en: Compilers typically generate less than optimal code for array indexing where
    the value of the array index is not a constant. The CUDA compiler is no exception.
    We can replace the array code with pointer code, which runs somewhat faster. We
    can also reduce the number of reads/writes to the shared memory area. However,
    as with most optimized solutions the code becomes more complex to understand and
    less easy to maintain and debug.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: '` local_result += ∗(smem_ptr+64);`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: 'The approach taken here is that, as we already have the current threads result
    stored in `local_result`, there is little point in accumulating into the shared
    memory. The only shared memory stores needed are those from the upper set of threads
    sending their data to the lower set. Thus, in each reduction step only the top
    set of threads write to shared memory. Once we get to a single warp, the code
    for this test takes longer than the reads/writes it saves from the shared memory,
    so we drop the test and write anyway. Also to avoid any address calculations,
    other than simple pointer addition, the address of the shared memory area is taken
    as a pointer at the start of the code section. The revised timings are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we gained 0.02 ms on both the GTX470 and GTX460\. We have also largely
    eliminated the shared memory based atomic reduction operations, which in turn
    allows for implementation on older hardware. To remove the final reduction to
    global memory, you’d need to write to an array indexed by `blockIdx.x` and then
    run a further kernel to add up the individual results.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative CPU version
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For reference, the CPU serial and parallel implementations are provided so we
    can see the same reduction on the CPU side.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: On an AMD Phenom II X4 processor (four cores) running at 2.5 MHz, this resulted
    in a timing of 10.65 ms for the serial version and 5.25 ms for the parallel version.
    The parallel version was created using OpenMP and the “reduction” primitive. To
    enable these quite useful pragma in the NVCC compiler simply use the `-Xcompiler
    –openmp` flag and you can use any of the OpenMP directives for CPU-level thread
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: This code spawns *N* threads where *N* is the number of cores. The threads are
    then free to run on any available core. The work is split into *N* chunks and
    finally the results are combined.
  prefs: []
  type: TYPE_NORMAL
- en: As can often be the case with parallel programming on CPUs, we see sublinear
    scaling as the number of cores increases. We can see that the scaling works well
    from one core to two cores, with a 35% drop in time when using two cores and a
    50% drop when using three. However, the addition of the fourth core drops the
    execution time by just an additional 2% so is effectively noncontributing ([Figure
    9.37](#F0190)). You typically see a U shape as the number of cores is further
    increased.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-37-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.37 OpenMP scaling on four cores.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is, while the compute performance is being scaled by the
    introduction of more cores, the memory bandwidth to the socket is shared between
    all cores. Taking our test system as an example, the AMD 905e processor has a
    typical memory bandwidth of 12.5 MB/s. Just to read the 48 MB of data from memory
    without any compute operations would therefore take 3.8 seconds, a considerable
    chunk of the 5.25 ms execution time. Thus, the issue here is not OpenMP versus
    CUDA but one of memory bandwidth available *per core* on a CPU versus that of
    a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel reduction summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The original, very simplistic GPU implementation took 197 ms and 164 ms (GTX470
    and GTX460). Compared with the CPU parallel four-core result of 5.25 ms this is
    really terrible and an example of how an apparently fast device can be brought
    to its knees by poor programming practices.
  prefs: []
  type: TYPE_NORMAL
- en: The final GPU version uses atomic operations as little as possible outside of
    the SM. It achieves, in pure compute terms, a 6.8× (GTX460) or 8.4× (GTX470) speedup
    over a four-core CPU. However, 0.62 ms is very little compute time to hide any
    transfer time. At 5 GB/s to the device the PCI-E 2.0 bandwidth is around 40% of
    the bandwidth to main memory on our test platform (12 GB/s). A 5 GB per second
    transfer rate gives us around 5 MB per millisecond. Thus the transfer time of
    the 48 MB of data would be 9.6 ms alone. We’d be able to overlap less than 10%
    of compute time with this, which limits the overall execution to no faster than
    the PCI-E 2.0 transfer speed.
  prefs: []
  type: TYPE_NORMAL
- en: This is actually all too often a problem with GPUs in general. They need to
    have a sufficiently complex problem that the benefit of their huge compute power
    can be applied. In such cases, they can drastically outperform a CPU. A simple
    problem like performing a `sum`, `min`, `max`, or other simplistic task just doesn’t
    provide enough of a problem to justify the time for the PCI-E transfer, unless
    we can discount the transfer time by ensuring the data is already resident on
    the device and stays there. This is one of the reasons why the 6 GB Teslas are
    more attractive than the much cheaper consumer cards that have a maximum capacity
    of 4 GB.
  prefs: []
  type: TYPE_NORMAL
- en: To increase the overall amount of data held in the GPU memory space, you can
    simply install multiple cards in a system, typically up to four per node, or more
    if you use exotic cooling methods. Thus, up to 24 GB in total data can be held
    on four Tesla class cards within a single node. The host memory space can be directly
    augmented with the GPU memory space using the UVA (universal virtual addressing)
    feature if this is available to you (requires a compute 2.x device onwards, a
    64-bit OS, Linux or the TCC driver under Windows, CUDA 4.x runtime). Inter-GPU
    communication (peer-to-peer, P2P) can also be performed without routing the data
    through the CPU, saving hugely on PCI-E bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: As we move from PCI-E 2.0 (5 GB/s) to PCI-E 3.0 the bandwidth per PCI-E slot
    should effectively double, significantly alleviating this problem for GPU devices
    supporting the new PCI-E 3.0 standard. As of the start of 2012 we saw motherboards
    start to support PCI-E 3.0 standard with the Ivybridge/Ivybridge-E processor.
    PCI-E graphics cards will start to appear through 2012 and beyond. In addition
    to increased PCI-E bandwidth came increased host memory bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: This also highlights another point that we’ve made throughout this book. The
    CPU can be a useful partner in dealing with all the simple problems in conjunction
    with a GPU. For example, where tiles of data need to communicate, it can process
    the halo cases where they need to share data while the GPU is processing the bulk
    of the data. Often such cases present a lot of branching, which is not efficient
    on the GPU and therefore can be better suited to a cooperative approach.
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • There are now well-documented sources that detail algorithms for specific
    fields. Many are available in the form of plug-in libraries.
  prefs: []
  type: TYPE_NORMAL
- en: • Be aware that not all parallel algorithms have obvious implementations on
    GPUs. Consider factors such as coalescing and communications when thinking about
    how to implement such algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: • New functions such as `__syncthreads_count` may have been introduced to address
    certain types of problems as the API develops. Study carefully the various additions
    to the API and understand possible usage.
  prefs: []
  type: TYPE_NORMAL
- en: • Use multiple elements per thread wherever possible. However, using too many
    elements per thread may adversely affect performance.
  prefs: []
  type: TYPE_NORMAL
- en: • As our reduction example shows, the simplest kernel is often the slowest.
    To achieve the absolute best performance often takes significant programming time
    and a good understanding of the underlying hardware.
  prefs: []
  type: TYPE_NORMAL
- en: • A multicore CPU is more than a capable partner in calculating workloads, but
    will often be memory bandwidth constrained, which in turn may limit your ability
    to make effective use of all the cores.
  prefs: []
  type: TYPE_NORMAL
- en: • OpenMP can provide an easy-to-use multithreaded interface for threads on the
    CPU side and is included as part of the standard CUDA compiler SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 6: Resource Contentions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Identifying bottlenecks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s often not clear to a programmer what, if anything, is wrong with a program.
    Most GPU programs, if they contain a reasonable amount of work for the GPU to
    do, show significant performance gains over their CPU counterparts. The question
    is how much is significant? The problem this question raises is that GPUs can
    be very good at some tasks, adequate at other tasks, and terrible with certain
    tasks. Anything that has a lot of arithmetic work and can be split into many independent
    problems works well.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that have significant branching or are mostly sequential are not
    suited to GPU, or most parallel architectures for that matter. In going down the
    parallel route, you almost always see a tradeoff of single-thread performance
    versus multiple-thread performance. The GPUs are typically clocked at up to 1000
    MHz, one-third or less than that of a typical CPU. They contain none of the fancy
    branch prediction logic that is necessary for large pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU has had decades of development and we’re pretty much at the end game
    of any significant single-thread performance gains. Consequently, largely serial
    code performs terribly on a GPU compared to a CPU. This may change with future
    hybrid architectures, especially if we see them include the dedicated CPU as it
    is proposed with NVIDIA’s “Project Denver.” This aims to embed an ARM-based CPU
    core into the GPU fabric. We already see the inclusion of GPU elements onto common
    CPU platforms, so it’s fairly certain the future for both the CPU and GPU world
    is likely to be a hybrid, taking the most useful parts of each.
  prefs: []
  type: TYPE_NORMAL
- en: However, restricting ourselves to the data parallel problems that run well on
    current GPUs, what is a good baseline for your kernel? What should you compare
    it against? What is a realistic target?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many fields now where CUDA is used to accelerate problems. One of
    the best resources to provide both some idea of what you can achieve and to see
    if there is already a solution that you can just buy in is [http://www.nvidia.com/object/cuda_app_tesla.html](http://www.nvidia.com/object/cuda_app_tesla.html).
    Here they list the following types of applications:'
  prefs: []
  type: TYPE_NORMAL
- en: • Government and Defense
  prefs: []
  type: TYPE_NORMAL
- en: • Molecular Dynamic, Computation Chemistry
  prefs: []
  type: TYPE_NORMAL
- en: • Life Sciences, Bio-Informatics
  prefs: []
  type: TYPE_NORMAL
- en: • Electrodynamics and Electromagnetic
  prefs: []
  type: TYPE_NORMAL
- en: • Medical Imagining, CR, MRI
  prefs: []
  type: TYPE_NORMAL
- en: • Oil and Gas
  prefs: []
  type: TYPE_NORMAL
- en: • Financial Computing and Options Pricing
  prefs: []
  type: TYPE_NORMAL
- en: • Matlab, Labview, Mathematica
  prefs: []
  type: TYPE_NORMAL
- en: • Electronic Design Automation
  prefs: []
  type: TYPE_NORMAL
- en: • Weather and Ocean Modeling
  prefs: []
  type: TYPE_NORMAL
- en: • Video, Imaging, and Vision Applications
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if your field is options pricing, you can go to the relevant section,
    browse through a few of the sites, and see that the Monte Carlo pricing model
    is somewhere from a 30× to 50× speedup over a single-core CPU according to the
    particular vendor’s analysis. Of course, you have to ask what CPU, what clock
    speed, how many cores were used, etc. to get a reasonable comparison. You also
    have to remember that any vendor-provided figures are trying to sell their product.
    Thus, any figures will be the best case and may well ignore certain difficult
    aspects of the problem to present a more compelling reason to purchase their product
    over their competitor’s product. However, a few hours of research can tell you
    what would be a reasonable target figure for your particular field. You will also
    get an appreciation of what other people have done and more importantly what still
    needs to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: However, don’t be disappointed with your initial GPU results in comparison with
    many of these applications. Often these arise from years of effort, which can
    be a great advantage, but can also mean they have to carry a lot of legacy code.
    A new approach to the problem, or a long forgotten approach used in the time of
    vector machines, may be the best approach today.
  prefs: []
  type: TYPE_NORMAL
- en: Also remember that many of these projects are from startup companies, although
    as CUDA has become more mainstream, there are now more and more corporate offerings.
    Often startups come from talented PhD students who want to continue their field
    of research or thesis into the commercial world. Thus, they often contain a small
    number of individuals who understand a particular problem domain well, but who
    may not come from a computing background. Thus, as someone with a detailed understanding
    of CUDA *and* a detailed understanding of the application field, you may well
    be able to do much better than the existing commercial or research offerings.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Visual profiler
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One of the first places to start, at least with existing code, is the analysis
    tools provided with the SDK. The first of these is the NVIDIA Visual Profiler
    tool. This is a multiplatform tool. It has the very useful feature of pointing
    out what it thinks is wrong with your kernel, at least pointing you toward what
    you need to do.
  prefs: []
  type: TYPE_NORMAL
- en: To use this tool, you simply compile your CUDA kernel and then select File→New
    Session, selecting the executable you just created. You can also input any working
    directory and command line arguments if applicable. Finally, you have to tell
    the profiler how long the application run is, so it knows when the kernel has
    simply crashed and does not wait forever to start processing the results. Note
    with Windows, you need to disable the default Aero desktop and select the standard
    desktop theme.
  prefs: []
  type: TYPE_NORMAL
- en: You are probably unlikely to be able to see the detail on the timeline in [Figure
    9.38](#F0195), but should be able to make out the major sections. The first thing
    that is striking about the timeline is how little compute is being performed (the
    green bar in the middle of the figure). This is a series of kernels using the
    default stream in sequence on a number of GPUs. We see that using the default
    stream causes implicit synchronization and the huge impact this has on overall
    timing.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-38-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.38 Visual Profiler timeline.
  prefs: []
  type: TYPE_NORMAL
- en: Switching to a streaming example now, we get a different view. Here we can see
    a kernel pushed into a stream with a `memcpy to` and `memcpy from` device around
    it. Although we can see the two GPUs are being used together this time, the tool
    warns us that there is little kernel memory transfer overlap. This is entirely
    correct. It’s caused by the fact that a typical kernel will have some input data
    *and* some output data. Although on all Fermi devices there are two `memcpy` engines
    in the physical hardware, only one is enabled in consumer devices such as the
    GTX470 and GTX460 used here. Thus, all transfers must go into the same `memcpy`
    stream and be executed in order. As the kernel does a “copy to” followed by a
    “copy from” on the first stream, the subsequent stream’s “copy to” gets held up.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, on Tesla devices where both copy engines are present, we do not see such
    an issue. For consumer-level hardware, we need to adopt a different approach.
    We simply do not issue any copy back transfers into the streams, until such time
    as all the `memcpy to` and kernel invocations have been issued. At this point
    we then push a set of “copy back” commands into the streams and do the transfers.
    There may be some kernel overlap with the last kernel and transfer back, but this
    will be minimal.
  prefs: []
  type: TYPE_NORMAL
- en: The other issue the analysis presents is the bandwidth to and from the device
    is being underutilized (the “Low Memcpy/Compute Overlap” message). In this example,
    we’re using 32 MB chunks of data. If you look back to earlier sections of this
    chapter, you’ll see this is plenty enough to achieve the peak bandwidth of the
    PCI-E bus. However, this issue here is the compute part is taking up most of the
    time. Even if we were to overlap the transfer and kernel execution, the benefit
    would be marginal. Therefore, it’s important to understand the implications of
    what exactly the tools are telling you and if the associated effort will actually
    be worth the saving in execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Overall it’s a very useful tool and quite easy to set up and use. It produces
    reasonable results quite quickly and is supported on multiple platforms ([Figure
    9.39](#F0200)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-39-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.39 Visual Profiler, multi-GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Visual Profiler can, unfortunately, only tell you so much. A much better level
    of detail can be found with the Parallel Nsight tool, which is a Windows-only
    visual analyzer and debugger. Even if Windows is not your primary development
    environment, it’s worth dedicating a spare PC to this tool for its analysis features
    alone.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight is a far more in-depth tool than Visual Profiler. It will tell
    you a lot more about the kernels and what they are doing. However, as with any
    more complex tool, it takes a little time to learn how to use it well. The Visual
    Profiler tool is far simpler to set up and use. It’s a beginner’s tool, whereas
    Parallel Nsight is more of an intermediate to advanced tool.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight is best set up with a single PC using one or more compute 2.x
    (Fermi) graphics cards. Parallel Nsight will also run remotely using two PCs,
    each of which has a NVIDIA graphics card. However, you’ll find it much easier
    to have one PC, rather than wait whilst data is copied to/from a remote machine.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight presents a number of options for debugging and profiling. The
    two main choices are “Application Trace” and “Profile.” The “Application Trace”
    feature allows you to generate a timeline as with Visual Profiler. This is particularly
    useful for seeing how the CPU interacts with the GPU and shows the times taken
    for host/device interaction. You should also use the timeline to verify correct
    operation of streams and overlapping kernel/memory copy operations.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple concurrent GPU timelines are also supported. For example, the timeline
    in [Figure 9.40](#F0205) shows we’re failing to provide enough work to keep all
    GPUs busy. Only the computation parts are shown. The Fermi GPUs are shown in red
    as the first and last context, while the older GPUs are shown in green as the
    middle two bars. Each red square represents one kernel invocation on a given stream.
    You can see the first set of kernels end prior to the next set running. We have
    a huge time period where the first GPU is idle. It’s only through using tools
    such as Parallel Nsight you can see issues such as this. It’s difficult to see
    this using host/GPU timers alone.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-40-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.40 Parallel Nsight, multi-GPU timeline.
  prefs: []
  type: TYPE_NORMAL
- en: The next useful feature is the “Profile” option under the Activity Type menu
    ([Figure 9.41](#F0210)). This allows us to profile the CUDA kernels. However,
    as many of the experiments require multiple runs of the kernel, no timeline can
    be produced when selecting this option.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-41-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.41 Parallel Nsight Activity Type selection.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Experiments to Run as “All” from the dropdown box is the simplest
    option. As you can see from the list of experiments in [Figure 9.42](#F0215),
    they are quite extensive. To start acquiring data, simply press the “Launch” button
    in the application control panel ([Figure 9.43](#F0220)). Note the green Connection
    Status circle. This tells you the Parallel Nsight monitor has successfully connected
    with the target devices. This needs to be green before any other options work.
    See the help options for details about setting up the monitor.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-42-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.42 Parallel Nsight Experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-43-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.43 Parallel Nsight application Launch control.
  prefs: []
  type: TYPE_NORMAL
- en: Once you press the “Launch” button your application will run until such time
    as it exits. You then will have a number of options in a dropdown box on the top
    of the screen, the last of which is “GPU Devices” ([Figure 9.44](#F0225)). Select
    this and you will see an overview of the GPU devices in the system.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-44-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.44 Parallel Nsight, GPU devices present.
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful dialog if you are not sure exactly what the properties of a
    particular device in your system are. Next, change the dropdown menu from “GPU
    Devices” to “CUDA Launches.” You’ll then see a list of kernels that were executed
    and various statistics. You’ll also find “Experimental Results” in the panel below
    the expandable list.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, we have six kernels. We can see from the results
    a number of issues. First, none of the kernels achieve a theoretical occupancy
    above 33% ([Figure 9.45](#F0230)). In the case of the first kernel, this is caused
    by the block limit (8) being hit before we’ve achieved the maximum of 48 warps
    that can be resident on the device. Also note that the first kernel does not set
    the cache configuration and the CUDA runtime uses the `PREFER_SHARED` option,
    allocating 48 K to shared memory instead of the cache. As the kernel does not
    use shared memory, this is pointless. We’re missing a call in the host code to
    set to cache configuration to `PREFER_L1` prior to the first kernel call.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-45-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.45 Parallel Nsight occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: The next experiment to look at is the “Instruction Statistics” ([Figure 9.46](#F0235)).
    Here we see a few issues. There is a very high level of instructions that are
    being issued but not executed. This is indicative of the SM having to serialize
    and thus reissue the same instructions. We also see a huge spike of activity on
    SM 2\. This is in fact very bad, as it means one of the blocks that were allocated
    to this SM performed a huge amount of additional work compared with the other
    blocks. This indicates the blocks are not equally distributed in terms of work
    per block, and this is something we need to solve at the algorithm level. Some
    balancing of the work per block is needed.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-46-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.46 Parallel Nsight “Instruction Statistics.”
  prefs: []
  type: TYPE_NORMAL
- en: The next experiment is the “Branch Statistics,” which tells us how much the
    execution within a warp diverges ([Figure 9.47](#F0240)). We ideally want a very
    small if not zero value for branch divergence. Here we see 16% of the branches
    diverge, which contributes to the reissuing of instructions we saw in the “Instruction
    Statistics” experiment. This too originates from the algorithm in that the amount
    of work per thread varies. It points to the need to balance the workload between
    the work blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-47-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.47 Parallel Nsight “Branch Statistics.”
  prefs: []
  type: TYPE_NORMAL
- en: The next experiment looks at the ability of the SM to issue and execute instructions.
    We’d expect to see a roughly equal distribute in terms of the “Active Warps per
    Cycle” chart. It shows that despite SM 2 taking a very long time to execute, it
    was actually only given a small number of warps to execute. This confirms that
    it was likely that one of the blocks given to it contained much more work than
    the other blocks. We also have a very low level of “Eligible Warps per Active
    Cycle,” which may in turn suggest the SMs are stalling at some point ([Figure
    9.48](#F0245)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-48-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.48 Parallel Nsight issue efficiency, eligible warps.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the next tab we see the distribution of instruction dependencies
    ([Figure 9.49](#F0250)). Instruction dependencies are caused by the output of
    one operation feeding into the input of the next. As the GPU uses a lazy evaluation
    model, the GPU operates best with long instruction dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-49-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.49 Parallel Nsight issue efficiency, instruction dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The graph in [Figure 9.49](#F0250) shows there are too many immediate dependencies.
    The easiest method to solve this is by introducing some ILP on the thread level.
    As we in fact have very few blocks, we have a significant number of unused registers
    that could be used to introduce ILP. We could do this via the vector types or
    by expanding the loop to process *N* elements per iteration. We could also use
    one or more registers to prefetch the values from the next loop iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The next tab confirms what we saw in the “Eligible Warps” tab, that the SMs
    are in fact hitting a stall condition. The first pie chart in [Figure 9.50](#F0255)
    shows that in 69% of the time, the SM has no eligible warp to execute, meaning
    it will stall or idle, which is of course not good. The second pie chart in [Figure
    9.50](#F0255) shows the reason for the stall, which we can see is 85% of the time
    related to execution dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-50-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.50 Parallel Nsight issue efficiency, issue stalls.
  prefs: []
  type: TYPE_NORMAL
- en: This can be solved in one of two ways. Currently, we have only 64 threads per
    block, meaning we get too few warps that are resident (16 out of a possible 48).
    Increasing the number of threads per block will increase the number of resident
    warps. From this perspective only, we’d need to move from 64 to 192 threads per
    block. This in itself may well resolve the issue. However, the effect of this
    issue on the overall timing is significantly less than issues concerning memory.
    Increasing the number of resident blocks will affect cache usage, which may have
    a bigger impact on the overall timing.
  prefs: []
  type: TYPE_NORMAL
- en: We can see this in practice by looking at the total amount of data fetched from
    global memory by creating two versions, one that uses 128 threads per block and
    another that uses 64 threads per block. As we have registers to spare, we’ll also
    fetch 16 elements in the 64-register version and 12 elements in the 128-register
    version. This maximizes the register usage while still maintaining eight blocks
    per SM.
  prefs: []
  type: TYPE_NORMAL
- en: Sure enough the “Warp Issue Efficiency” improves, reducing the “No Eligible”
    warps from 75% down to just 25%. The number of theoretical warps per SM also increases
    from 16 to 32 (13.25 versus 26.96 actual). The occupancy increases from 27% to
    56%. These are all improvements, but they are secondary effects. The kernel is
    performing a sort, so is likely, as with almost all sorts, to be memory bound.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, when we compare the two kernels with the “CUDA Memory Statistics” experiment,
    there is a difference. The increased number of blocks per SM means that the ratio
    of L1 cache to each block is reduced. This in turn results in a doubling of the
    number of global memory fetch operations that are not cached in the L1 or L2 cache.
  prefs: []
  type: TYPE_NORMAL
- en: In the first kernel, using 64 threads per block, we achieve a 93.7% cache hit
    rate, which is very good ([Figure 9.51](#F0260)). Of the 6.3% of the transactions
    the L1 cache misses, the L2 cache picks up 30%, or around one-third. Thus, very
    few read transactions actually make it to global memory and we stay mostly on
    chip.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-51-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.51 Memory statistics, memory overview (256 blocks × 64 threads).
  prefs: []
  type: TYPE_NORMAL
- en: When we extend this to 128 threads per block, the overall number of blocks halves
    to 128 blocks in total ([Figure 9.52](#F0265)). However, this is not an issue,
    as with 14 SMs on the device and a maximum of eight resident blocks, we can only
    accommodate a maximum of 112 blocks at any given time anyway. Thus, we can increase
    the number of resident warps without any SMs running out of blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-52-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.52 Memory statistics, memory overview (128 blocks × 128 threads).
  prefs: []
  type: TYPE_NORMAL
- en: Notice the problem with the cache hit ratio. Both the L1 and L2 caches achieve
    a lower hit ratio than before. The amount of memory fetched from global memory
    approximately doubles from 272 MB to 449 MB. This takes the execution time from
    35 ms to 46 ms, despite the apparent improvements in utilization of the SMs. Note
    that due to the allocation of one thread to each sample block, these memory fetches
    are all uncoalesced, so they are in fact very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a design in which the threads from a thread block cooperated on sorting
    a single sample block would be far less sensitive to this effect. This analysis
    shows us this dependency. Through using a different mapping of threads to work
    in the sort stage, or by balancing or adjusting the bin boundaries, we may well
    be able to significantly improve the throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Resolving bottlenecks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s all very well knowing what the code you are running is doing, but it’s
    often another matter to both understand and fix the issue. The three types of
    bottlenecks you typically see, in order of importance, are:'
  prefs: []
  type: TYPE_NORMAL
- en: • PCI-E transfer bottlenecks
  prefs: []
  type: TYPE_NORMAL
- en: • Memory bandwidth bottlenecks
  prefs: []
  type: TYPE_NORMAL
- en: • Compute bottlenecks
  prefs: []
  type: TYPE_NORMAL
- en: PCI-E transfer bottlenecks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCI-E transfer bottlenecks are often a key consideration. As we saw from the
    earlier sections, PCI-E bus bandwidth is limited and you can expect to achieve
    a peak of around 5 GB/s on PCI-E 2.0 depending on the host hardware. However,
    to achieve this peak you need to be using pinned memory and an appropriately sized
    transfer. Adding more GPUs to a node typically reduces the overall bandwidth,
    but allows the overall amount of GPU to be increased. If you can keep everything
    in the GPU memory space, be that a single Tesla GPU or multiple GPUs, then the
    transfer cost can be eliminated from the equation. The extent of the reduction
    in bandwidth by adding more cards is very much dependent on the host hardware.
    You therefore need to be aware of how much data you are transferring and its usage.
  prefs: []
  type: TYPE_NORMAL
- en: Compression techniques are one way to increase this apparently hard limit on
    PCI-E transfer rates. Do you really need to transfer all the data you are sending?
    For example, image data often contains an alpha channel that is used for transparency.
    If you are not using this on the GPU, then you can discard it and transfer from
    the host only the RGB (red, green, and blue) components, eliminating 25% of the
    data to be transferred. Although this may then mean you have 24 bits per pixel,
    the transfer time saving may significantly outweigh the nonaligned access pattern
    this might cause.
  prefs: []
  type: TYPE_NORMAL
- en: The other question is can you infer some data from others? This is very much
    problem dependent, but you may be able to compress the data using a simple algorithm
    such as run-length encoding. A long series of the same numbers can be replaced
    with a value, count pair and reconstructed at the GPU end in very little time.
    You may have lots of activity from a sensor and then no “interesting” activity
    for quite a period of time. Clearly, you can transfer the “interesting” data in
    full and either throw away the “uninteresting” data at the host end, or transfer
    it in some compressed form.
  prefs: []
  type: TYPE_NORMAL
- en: Interleaving transfer with computation using streams or zero-copy memory is
    another essential technique we have already covered. In the situation where your
    PCI-E transfer time is in excess of your kernel time, you effectively have the
    computation time for free. Without overlapping, the two times must be added and
    you end up with large gaps where no computation is taking place. See [Chapter
    8](CHP008.html) for more information on using streams.
  prefs: []
  type: TYPE_NORMAL
- en: PCI-E is not the only transfer bottleneck you need to consider. The host will
    have a limit on the amount of memory bandwidth there is. Hosts such as the Intel
    Sandybridge-E processors use quad-banked memory, meaning they can achieve much
    higher host memory bandwidth than other solutions. Host memory bandwidth can also
    be saved by using P2P (Peer to Peer) transfers if your problem allows for this.
    Unfortunately, at the time of writing, to use the P2P function you need to use
    an OS other than Windows 7\. With the exception of those using Tesla cards and
    thus the TCC (Tesla Compute Cluster) driver, Windows 7 is the only major OS not
    currently supported for this feature.
  prefs: []
  type: TYPE_NORMAL
- en: The speed at which the node can load and save data to storage devices, be they
    local devices or network devices, will also be a limiting factor. High-speed SSD
    drives connected in RAID 0 mode will help with this. These are all considerations
    for selecting host hardware. We look at a number of these in detail in [Chapter
    11](CHP011.html).
  prefs: []
  type: TYPE_NORMAL
- en: Memory bottlenecks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assuming you can get the data on and off the GPU, the next issue is memory bandwidth
    to or from global memory. Moving data is expensive in terms of time and power
    usage. Therefore, being able to efficiently fetch/store and reuse data are essential
    criteria for selecting an appropriate algorithm. The GPU has huge amounts of compute
    resources, so an inefficient algorithm with a memory pattern favorable to a GPU
    (coalesced, tiled, high locality) may outperform a more computationally intensive
    algorithm that exhibits less GPU-friendly memory pattern.
  prefs: []
  type: TYPE_NORMAL
- en: When considering memory, think also about thread cooperation and appreciate
    the cooperation is best limited to a single block of threads. Generic algorithms
    that assume any thread can talk to any other thread are less useful than those
    that value locality of threads to one another. Algorithms designed for use on
    older vector machines are often far more efficient than those designed around
    distributing work over *N* independent processing nodes, as commonly found in
    today’s cluster machines.
  prefs: []
  type: TYPE_NORMAL
- en: On modern GPUs, the L1 and L2 caches can significantly affect the execution
    time of kernels in sometimes rather unpredictable ways. Shared memory should be
    used where you have data reuse, want a more predictable outcome, or are developing
    for compute 1.x hardware. Even with the full 48 K allocation to the L1 cache,
    there is still 16 K of local shared memory storage available on each SM.
  prefs: []
  type: TYPE_NORMAL
- en: A fully populated Fermi GPU has 16 SMs, so this amounts to a total of 256 K
    of high-speed memory in addition to the 768 K of L1 cache. This can be swapped,
    giving 768 K of programmer-managed shared memory and 256 K of L1 cache. Data reuse
    through either or both mechanisms is critical to achieving high throughput. This
    is typically achieved by ensuring locality of the calculation. Instead of multiple
    passes over large datasets, break the data into tiny tiles, use multiple passes
    over individual tiles, and then repeat for the other tiles. This allows the data
    to remain on chip throughout whatever transformation is being made on it, without
    multiple read/writes to and from global memory.
  prefs: []
  type: TYPE_NORMAL
- en: Memory coalescing is key to achieving high memory throughput, although a sufficiently
    high number of memory transactions is also required. On Fermi and Kepler devices,
    to achieve anything like the full bandwidth when using 32-bit values per thread
    (i.e., floats or integers), you need to have the GPU almost fully populated with
    threads (48 to 64 resident warps, 1536 to 2048 threads per SM). Increased transaction
    sizes through the use of the various vector types help improve both ILP and memory
    bandwidth. Having each thread process four values instead of one tends to work
    well for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: Compute bottlenecks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Complexity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Surprisingly, despite the immense computing throughput of the GPU, there are
    still problems that are compute bound. These are usually problems where the overall
    amount of data is very large, such as the various forms of medical image scanning
    or data processing from devices that generate large amounts of sample data. These
    types of problems were previously processed on clusters. However, now due to the
    huge processing power available from a multi-GPU computer, many problems can be
    processed on a single standalone PC.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that contain a lot of computations work really well on GPUs compared
    to their CPU counterparts. However, algorithms that also include a lot of control
    complexity do not. Take the example of boundary cells in a typical tiled algorithm.
    If the cells collect data from their immediate neighbors, then a cell at the corner
    of a tile needs to collect data from the corner points of three other tiles.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 9.53](#F0270) you can see there is a large block of green cells in
    the centre that have no boundary condition. They can safely calculate some value
    from the surrounding cells within the current block. Unfortunately, some programmers
    write programs that deal with the problem cases first. Thus, their kernel goes
    along the lines
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000090f09-53-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 9.53 Halo cells needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: '`else if (right row)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: Particularly, control complex algorithms are not well suited to GPUs. If each
    thread runs the same kernel, the center elements have nine conditions to test
    before the thread does any work on them. Reversing the order of the tests, so
    the center elements are tested first, means we need four boundary tests. This
    would be an improvement, but is still far from optimal. The solution is to write
    customized kernels for each special case or let the CPU handle these complex conditionals.
  prefs: []
  type: TYPE_NORMAL
- en: The type of problem here is a stencil one, where cells *N* levels from the center
    contribute in some way to the result. In this simple example, *N* is 1, as the
    immediate neighbors are used. As *N* is increased, typically some factor is applied,
    as values that are a long way from the center often do not contribute as much
    to the result.
  prefs: []
  type: TYPE_NORMAL
- en: As each cell will need values from the surrounding cells, each cell value will
    be read multiple times. Thus, a common approach to such problems is to use many
    threads to read a tile of data into shared memory. This allows for high-performance
    coalesced access to global memory, both when reading the data and also when writing
    it back. However, shared memory is not visible between blocks and there is no
    mechanism to pass shared data directly between blocks. This is due to the design
    of CUDA where there is only ever a subset of the total number of blocks executing.
    Thus, shared memory is reused as old blocks are retired and new blocks scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to load the halo cells, the cells outside the boundary of our particular
    tile, you can either read them from global memory or also load these into shared
    memory. Reading the rows from global memory gives a nice coalesced memory pattern.
    However, the columns generate a number of separate memory transactions, one for
    each cell we load. As these cells may be read a number of times, reading the columns
    can be a memory-intensive operation that will limit performance. Thus, at least
    the columns are usually placed into shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, writing multiple kernels is usually a good solution to the problem of
    eliminating the control flow complexity. We can have one kernel that handles corner
    elements, another for rows, another for columns, and another for the center elements.
    If appropriate, each of these can call a common routine that processes the data
    as a series of values, and now the complexity of where the data came from has
    been removed.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for compute 1.x and compute 2.x different solutions are applicable.
    As compute 1.x hardware has no cache for global memory, each memory transaction
    would generate a considerable amount of latency. Thus, for these devices it can
    make sense to manually cache the necessary data from the surrounding tiles in
    shared memory or give the calculation to the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: However, compute 2.x devices have both an L1 and L2 cache. As each tile will
    have to process its own elements, it’s likely that the tiles above, above left,
    and left will have already been loaded into the cache by previous activity of
    other blocks. The tiles to the right, right bottom, and bottom will usually not
    be present unless there are multiple passes over quite a small dataset. Accessing
    these from global memory will bring them into the cache for the subsequent block.
    You can also explicitly request cache lines be brought into the cache using the
    prefetch PTX instruction (see PTX ISA).
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence of the caching, we can eliminate a large amount of the control
    complexity necessary to manage shared memory by simply selecting a 48 K L1 cache
    and not using shared memory at all. Elimination of complexity is often useful
    in speeding up compute bound kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Instruction throughput
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As with many processors, not all instructions take the same amount of time to
    execute on every device. Selecting the correct instruction mix for a given processor
    is something the compiler should be able to perform quite well, but it’s also
    something the programmer needs to be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, you need to ensure you are targeting the correct binaries for
    your hardware. Ideally, you should have one compute level specification for each
    target hardware platform. In Visual Studio this is done in the project options
    and is something we’ve already covered. For those people using command line it’s
    the `-arch` flag that specifies this.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of single-precision floating-point operations, all compute levels achieve
    a throughput of one instruction per clock, per thread. Remember, however, as this
    is per thread. In absolute terms we need to consider this is warp wide times the
    number of simultaneous warps per SM times the number of SMs on the GPU. Thus on
    Kepler GTX680 we have a 32 wide warp x 8 warp dispatch x 8 SMs = 2048 instructions
    per clock. Now throughput is not the same as instruction latency. It may take
    up to the order of 20 clock cycles for the result to become available to feed
    into a subsequent operation. A series of floating-point operations fed into the
    instruction pipeline would therefore appear 20 cycles later, one each cycle. The
    throughput would be one instruction per cycle, per thread but the latency would
    be 20 cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Double-precision floating-point hardware, however, does not achieve this. For
    compute 2.0 hardware, it’s half the speed of single precision. For compute 2.1
    hardware, it’s actually only one-third of the speed. Compute 2.1 hardware (GTX460/560)
    and compute 3.0 hardware (GTX680) was aimed more toward the gaming market, so
    it lacks the same level of double-precision floating-point performance.
  prefs: []
  type: TYPE_NORMAL
- en: We see a similar issue with 32-bit integer values. Add and logical instructions
    only run at full speed. All other integer instructions (multiply, multiply-add,
    shift, compare, etc.) run at half speed on compute 2.0 hardware and one-third
    speed on compute 2.1 hardware. As usual, division and modulus operations are the
    exception. These are expensive on all compute levels, taking “tens of instructions”
    on compute 1.x hardware and “below 20 instructions” on compute 2.x hardware [NVIDIA
    CUDA C Programming Guide, v4.1, [chapter 5](CHP005.html)].
  prefs: []
  type: TYPE_NORMAL
- en: Type conversion instructions operate at half speed on compute 2.0 devices and
    one-third speed on compute 2.1 devices. These are necessary when 8- or 16-bit
    integer types are used, as the hardware supports only native integer types (32-bit
    on compute 2.x, 24-bit on compute 1.x). Thus, the addition of two byte values
    results in promotion of these values to two integer values. The subsequent result
    then again needs to be demoted to a byte value. Similarly, conversions to and
    from single-/double-precision floating-point values cause additional type conversion
    instructions to be inserted.
  prefs: []
  type: TYPE_NORMAL
- en: In C all whole numbers are by default signed integers. All numbers containing
    a decimal place are treated as double-precision floating-point values unless an
    `F` postfix is placed immediately after the number. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: creates a double-precision definition and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: creates a single-precision definition.
  prefs: []
  type: TYPE_NORMAL
- en: Using a non-postfixed constant in a floating-point expression causes an implicit
    conversion to double precision during the calculation. An implicit conversion
    to single precision is also performing when the result is assigned to a single-precision
    variable. Thus, forgetting to use the `F` postfix is a common cause of creating
    unnecessary conversion instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization and atomics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Synchronization points are often necessary in many algorithms. Synchronization
    within a thread block is not costly, but does potentially impact performance.
    The CUDA scheduler will try to schedule up to sixteen blocks per SM, which it
    can do unless you start using larger numbers of threads (see [Chapter 5](CHP005.html)).
    As the number of threads increases, the number of blocks that can be scheduled
    decreases. This in itself is not too bad, but when combined with synchronization
    points it can lead to the SM stalling.
  prefs: []
  type: TYPE_NORMAL
- en: When a block performs a synchronization, a number of warps out of the available
    set (24 on compute 1.x, 48 on compute 2.x, 64 on compute 3.x) effectively drop
    out of the scheduling availability, as all but the last warp hits the synchronization
    point. In the extreme case of 1024 threads per block (two blocks per SM), up to
    half of the resident warps would be at the synchronization barrier. Without any
    ILP, the ability of the SM to hide memory latency through running multiple threads
    then becomes insufficient. The SM stops running at peak efficiency. Clearly, we
    want maximum throughput from all the SMs for as much time as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to the synchronization issue is not to use large thread blocks.
    You should aim to fully populate the SM where possible, so 192 threads is an ideal
    number, which results in eight blocks per SM on compute 2.x hardware, 256 being
    better for compute 3.x hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, if we’re using interthread synchronization it is likely we’ll
    also need interblock synchronization. It’s more efficient to synchronize data
    between threads than between blocks. For block-based synchronization we need to
    use global memory, whereas interthread synchronization can be performed with shared
    memory. Thus, it’s a tradeoff between the two scenarios best resolved by simply
    running both and seeing which is the fastest.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations act very much like synchronization points in that all the
    threads in a warp have to line up one after another to perform the operation.
    It takes time for all the threads in a block to line up in groups of 32 to move
    through the atomic operation. However, unlike synchronization points, they are
    free to continue at full speed afterward. This helps in terms of increasing the
    availability of warps that can be run, but doesn’t help the overall execution
    time of the block. The block cannot be retired from the SM until all the threads
    have completed. Thus, a single atomic operation effectively serializes and spreads
    out, in terms of execution time, the warps in a given block. The block can’t finish
    until all the stragglers have completed.
  prefs: []
  type: TYPE_NORMAL
- en: The effect of synchronization and atomics on your kernel can be seen using the
    “CUDA Issue Efficiency” experiment within Parallel Nsight.
  prefs: []
  type: TYPE_NORMAL
- en: Control flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As we saw earlier, branch divergence can have a serious impact on execution
    time as both paths have to be executed separately. The compiler is aware of this
    and thus uses something called predication.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the PTX instructions can be predicated using the `.p` notation of the
    PTX ISA. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: Here we set up a predicate register in each thread, testing virtual register
    295 for the value 1 and setting predicate register 16 accordingly. In the next
    instruction the predicate register 16 is used to predicate the `bra` (branch to)
    instruction. Thus, only those threads meeting the test condition of the earlier
    `setp.eq.s32` instruction follow the branch. We could replace the branch with
    a `mov` or similar instruction. Typically, you see the compiler generate this
    for small `if-else` constructs. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: will be translated to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE292]'
  prefs: []
  type: TYPE_PRE
- en: 'This works well in avoiding branches, as in fact all threads in the warp execute
    the predicate instruction, but those threads without the predicate bit set simply
    ignore it. The compiler has a strong preference for predication, even when other
    approaches would be better. The criteria is simply based on the size of the body
    of the `if` statement. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE296]'
  prefs: []
  type: TYPE_PRE
- en: This code simply selects one of *N* local variables (registers) based on an
    index. The local variables are individually named, as creating an array causes
    the compiler to place this into local memory. Unfortunately, the compiler implements
    a series of `if-else-if` type statements, which means at element 16 we have to
    perform 15 prior tests. I’d have expected it to implement a jump table, creating
    an assignment at the target of each jump. This would be two instructions, load
    `local_idx` into a register and then an indirect jump to some base address plus
    the value in the register. The jump table itself is set up at compile time.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, you need to ensure the control flow you expect is the control flow the
    compiler generates. You can do this relatively easily by inspecting the PTX code
    and/or the actual target code if you are still unsure. Predication works well
    in many but not all instances.
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • Use profiling tools to really see into what is happening as opposed to what
    you *think* is happening.
  prefs: []
  type: TYPE_NORMAL
- en: • Avoid overly complex kernels by generating a general case and exception case
    kernel, or by using the caching features to eliminate the complex kernel altogether.
  prefs: []
  type: TYPE_NORMAL
- en: • Understand how predication works in control flow.
  prefs: []
  type: TYPE_NORMAL
- en: • Don’t assume the compiler will provide the same scope of optimizations found
    with more mature compilers. CUDA is still quite new and things will take time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 7: Self-Tuning Applications'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPU optimization is not like CPU optimization. Many techniques overlap, while
    others have undesirable effects. I’ve tried to cover the major areas of optimization
    in the preceding sections. However, optimization is never an exact science, not
    when practiced by human programmers anyway. There are lots of factors that need
    to be considered when designing code for the GPU. Getting an optimal solution
    is not easy and it takes considerable time to become familiar with what works,
    try different solutions, and understand why one works when another doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider some of the major factors:'
  prefs: []
  type: TYPE_NORMAL
- en: • Transfer to and from the host.
  prefs: []
  type: TYPE_NORMAL
- en: • Memory coalescing.
  prefs: []
  type: TYPE_NORMAL
- en: • Launch configuration.
  prefs: []
  type: TYPE_NORMAL
- en: • Theoretical and achieved occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: • Cache utilization.
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory usage/conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: • Branch divergence.
  prefs: []
  type: TYPE_NORMAL
- en: • Instruction-level parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: • Device compute level.
  prefs: []
  type: TYPE_NORMAL
- en: For someone starting out with CUDA, there is a lot to think about and it will
    take time to become proficient with each of these areas. However, the most challenging
    aspect of this is that what works on one device many not work on another. Throughout
    this book we’ve used the whole range of available devices and a number of different
    host platforms where necessary to highlight differences.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way as different CPUs provide different levels of performance and
    functionality, so do GPUs. The CPU world is largely stuck with an x86 architecture,
    which reflects design goals of a system designed to run serial programs. There
    have been many extensions to provide additional functionality, such as MMX, SSE,
    AVX, etc. The x86 instruction set is today translated within the hardware to micro-instructions,
    which can be really for any target hardware. Sandybridge is perhaps the best example
    of this, where the micro-instructions themselves are actually cached instead of
    the x86 assembly code instructions.
  prefs: []
  type: TYPE_NORMAL
- en: GPU hardware is also not fixed and has changed significantly since the first
    CUDA-enabled devices were released back in the GTX8800 times. CUDA compiles to
    PTX, a virtual assembly code, aimed at a parallel processor–like architecture.
    PTX can itself be compiled to many targets, including CPUs, as the cooperative
    thread array concept lends itself to implementation in most parallel hardware.
    However, as far as we’re concerned, it’s compiled to a specified compute level
    for various NVIDIA GPUs. Therefore, you need to be familiar with what a given
    compute level provides, that is you need to understand for what hardware you are
    writing code. This has always been the basis of good optimization. Trends toward
    abstraction, layering, and hiding the architecture are all aimed at programmer
    productivity, but often at the expense of performance.
  prefs: []
  type: TYPE_NORMAL
- en: Not every programmer is interested in the intricate workings of the hardware.
    Even with the previous list of issues to consider you’re unlikely to get an optimal
    solution the first time, the second time, or the *N*th time without considerable
    thought and a lot of trial and error. Thus, one approach to this issue that works
    well is simply to ask the program to work out the best use of the hardware for
    a given problem. This can either be done on a small set of the problem or the
    real problem itself.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in any optimization process is to know what hardware is available
    and what it is. To find out how many GPUs we have, you simply call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE297]'
  prefs: []
  type: TYPE_PRE
- en: This sets whatever parameter you pass as `count` to the number of devices available.
    If there is no CUDA hardware available the function returns `cudaErrorNoDevice`.
  prefs: []
  type: TYPE_NORMAL
- en: Then for each device found we need to know what its capabilities are. For this
    we call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE298]'
  prefs: []
  type: TYPE_PRE
- en: 'We covered in detail the properties of a device in [Chapter 8](CHP008.html)
    so will not repeat this here. You should, however, be interested in at least the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: • Members `major` and `minor` that, when combined, provide the compute level
    of the device.
  prefs: []
  type: TYPE_NORMAL
- en: • The `integrated` flag, especially when combined with the `canMapHostMemory`
    flag. This allows you to use zero-copy memory (covered in Strategy 3) and avoid
    memory copies to and from the device for devices of which the GPU memory is actually
    on the host.
  prefs: []
  type: TYPE_NORMAL
- en: • The `totalGlobalMem` value so you can maximize the use of GPU memory and ensure
    you don’t try to allocate too much memory space on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: • The `sharedMemPerBlock` value so you know how much shared memory is available
    per SM.
  prefs: []
  type: TYPE_NORMAL
- en: • The `multiProcessorCount`, which is the number of SMs present in the device.
    Multiply this number by the number of blocks you are able to run on an SM. The
    occupancy calculator, the Visual Profiler, and Parallel Nsight will all tell you
    the number of blocks you can run for a given kernel. It’s typically up to eight
    but can be as many as 16 on Kepler. This is the minimum number of blocks you need
    to schedule to this GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'This information gives us some bounds with which we can define the problem
    space. We then have two choices: either analyze offline the best solution or try
    to work it out at runtime. The offline approach generally leads to better results
    and can greatly increase your understanding of the issues involved and may cause
    you to redesign certain aspects of the program. The runtime approach is necessary
    for optimal performance, even after significant analysis has taken place.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the first part of the optimization takes place offline, during the development
    phase. If you are targeting multiple compute levels, you’ll need a suitable card
    to test your application on. For consumer cards as a whole the most popular NVIDIA
    cards have always been the 9800 (compute 1.1), 8800 (compute 1.0), GTX260 (compute
    1.3), and GTX460 (compute 2.1). For more modern DirectX 11 cards, the 460/560
    cards dominate, with a smaller number of power users opting for the more expensive
    470/570 cards. Our choice of hardware for this book pretty much reflects the market
    trends to make the figures presented as useful as possible for people developing
    mass-market applications.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve been working with CUDA since it release on the 8800 series of cards,
    we have a number of consumer cards at hand. Clearly, many of these are no longer
    available for sale but can easily be purchased on eBay or elsewhere. All you need
    is a motherboard with four dual-spaced PCI-E connectors all running at the same
    speed when fully populated. The primary board used in the development of this
    book was the (AMD) MSI 790FX-GD70, although this has now been replaced with the
    MSI 890FXX-G70\. Note the newest 990FX board in the series no longer provides
    four double-spaced connectors.
  prefs: []
  type: TYPE_NORMAL
- en: Device utilization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having identified what hardware we have, we then have to make use of it. If
    there are multiple GPUs in the system, as is often the case, then be sure to make
    use of them. Multi-GPU programming, as of the CUDA 4.x SDK, is now much easier
    than before, so be sure you are not leaving a 100% performance gain on the table
    because you’re only using a single GPU. See [Chapter 8](CHP008.html) for more
    information on this.
  prefs: []
  type: TYPE_NORMAL
- en: All applications are different, so the same primary performance factors may
    not always be the same. However, many will be. Primary among these is the launch
    configuration. The first part of this is ensuring you have multiple targets set
    up in the build process, one target for each compute level you plan on supporting.
    The target code will automatically be selected based on which GPU you are running
    the kernel on. Make sure also before running any performance tests you have the
    “Release” version selected as the build target, something in itself that can provide
    up to a 2× performance improvement. You’re not going to release the debug version,
    so don’t select this as your build target, other than for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Next we need some sort of check to ensure correctness. I suggest you run the
    GPU code back to back with the CPU code and then do a memory compare (`memcmp`)
    on the output of the two identical tests. Note this will detect any error, even
    if the error is not significant. This is especially the case with floating point,
    as the order in which the operations are combined will cause small rounding/precision
    errors. In such cases your check needs to iterate through both results and see
    if the answers differ by whatever you consider to be significant (0.01, 0.001,
    0.0001, etc.) for your particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of launch configuration we’re trying to optimize for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: • Number of threads per block.
  prefs: []
  type: TYPE_NORMAL
- en: • Overall number of blocks.
  prefs: []
  type: TYPE_NORMAL
- en: • Work performed per thread (ILP).
  prefs: []
  type: TYPE_NORMAL
- en: The answer for each of these will vary between compute levels. A simple `for`
    loop is all that is needed to iterate through all possible combinations and record
    the timings for each. Then at the end simply print a summary of the results.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of threads per block, start at 1 and increase in powers of two until
    you reach 16\. Then increase the thread count in 16-step intervals until you reach
    512 threads per block. Depending on the kernel resource usage (registers, shared
    memory) you may not be able to reach 512 threads on the earlier compute devices,
    so scale this back as necessary for these devices only.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we chose 16 here as the increment value, rather than 32, the warp
    size. This is because warp divergence is half-warp based. Certain devices such
    as the GTX460s are actually based on three sets of 16 CUDA cores, rather than
    two as found in other compute levels. Thus, a number of threads that is a multiple
    of 48 may work better on such devices.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, you’ll find well-written kernels work best with 128, 192,
    or 256 threads per block. You should use a consistent scaling from one thread
    per block up to a peak point where the performance will level off and then fall
    away. The plateau is usually hit when you achieve the maximum number of resident
    warps per SM and thus the instruction and memory latency hiding is working at
    its peak.
  prefs: []
  type: TYPE_NORMAL
- en: Using a slightly smaller number of threads (e.g., 192 instead of 256) is often
    desirable if this increases the number of resident blocks per SM. This usually
    provides for a better instruction mix, as more blocks increases the chance they
    will not all hit the same resource contention at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: If you are hitting the maximum performance at 16, 32, or 64 threads then this
    usually indicates there is a contention issue, or that your kernel is highly geared
    toward ILP and you are using a lot of registers per thread.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a baseline figure for the ideal number of threads per block, try
    increasing the amount of work done by each thread to two or four elements using
    the various `vector_N` types (e.g., `int2`, `int4`, `float2`, `float4`, etc.).
    You’ll typically see this will improve performance further. The easiest way of
    doing this is to create additional functions with the same name and simply overload
    the kernel function. CUDA will call the appropriate kernel depending on the type
    passed to it at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Using the vector types will increase register usage, which in turn may decrease
    the number of resident blocks per SM. This in turn may improve cache utilization.
    Memory throughput will also likely be increased as the overall number of memory
    transactions falls. However, kernels with synchronization points may suffer as
    the number of resident blocks drops and the SM has less choice of which warps
    are available to execute.
  prefs: []
  type: TYPE_NORMAL
- en: As with many optimizations, the outcome is difficult to predict with any degree
    of certainty, as some factors play in your favor while others don’t. The best
    solution is to try it and see. Then work backwards, to understand what factor(s)
    are the primary ones and which are secondary. Don’t waste your time worrying about
    secondary factors unless the primary ones are already addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final part of a self-tuning application is sampling. Although you can build
    a good performance model around compute level and number of SMs, there are many
    other factors. The same card model may be produced using GDD3 and GDD5 memory,
    the latter having significantly more global memory bandwidth. The same card may
    be clocked internally at 600 MHz yet also appear as a 900 MHz model. An optimization
    strategy that works well for a card with 16 SMs may not work well on one with
    half that number and vice versa. A mobile processor in a laptop may have been
    put on a PCI-E X1 link and may have dedicated or shared memory with the host.
  prefs: []
  type: TYPE_NORMAL
- en: It’s impossible to collect every card and address every variation that your
    product might have to address. Even if you could do this, next week NVIDIA will
    release another card. This is of course mostly a problem for those people writing
    consumer applications, rather than the somewhat less diverse Tesla population
    of cards. Nonetheless, when a new card is released people first expect their existing
    applications to run on it, and second, if they have upgraded, to see a suitable
    performance boost.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling is the answer to this issue. Each card will have a peak value in terms
    of a launch configuration that works best for its particular setup. As we’ve seen
    in some of the tests run throughout this book, different cards prefer different
    setups. The Fermi cards work well with 192 or 256 threads per block, yet the prior
    GPUs work well with 128 and 192 threads per block. The compute 2.1 cards perform
    best with 64- or 128-byte memory fetches, mixed with ILP, instead of 32-byte memory
    fetches and a single element per thread. The earlier cards are hugely sensitive
    to thread/memory ordering when coalescing. Global memory bandwidth on these cards
    is a fraction of the newer models, yet they can perform to a similar level with
    some problems if shared memory is used well. The cache in Fermi can play a big
    part to the extent that very low thread numbers (32 or 64) can outperform higher
    occupancy rates if the data is then entirely contained in the cache.
  prefs: []
  type: TYPE_NORMAL
- en: When the program is installed, run a short test suite as part of the installation
    procedure. Run a loop through all feasible numbers of threads. Try ILP values
    from one to four elements per thread. Enable and disable shared memory usage.
    Run a number of experiments, repeating each a number of times, and average the
    result. Store in a data file or program configuration file the ideal values and
    for which GPU these relate. If the user later upgrades the CPU or GPU, then rerun
    the experiments and update the configuration. As long as you don’t do this on
    every startup, the user will be happy you are tuning the application to make the
    best possible use of their hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Section summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: • There are too many factors to say with certainty the effect of a change without
    actually trying it.
  prefs: []
  type: TYPE_NORMAL
- en: • Some experimentation is often required during development to get the optimal
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: • The optimal solution will be different on different hardware platforms.
  prefs: []
  type: TYPE_NORMAL
- en: • Write your applications to be aware of the different hardware out there and
    what works best on each platform, either statically or dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve looked in detail at a number of strategies for trying to improve the throughput
    of your kernels with various examples throughout this chapter. You should be aware
    of the factors that affect performance and their relative importance (primary
    ones are transfers, memory/data patterns, and finally SM utilization).
  prefs: []
  type: TYPE_NORMAL
- en: Correctness is a key issue in optimizing code. You cannot reliably optimize
    code without automatic regression testing. This doesn’t have to be hugely complex.
    A back-to-back run against a known working version with several known datasets
    is entirely sufficient. You should aim to spot 95% plus of the errors before any
    program leaves your desk. Testing is not the job of some test group, but your
    responsibility as a professional to produce reliable and working code. Optimization
    often breaks code and breaks it many times. The wrong answer in one minute instead
    of the correct answer in one hour is no use to anyone. Always test for correctness
    after every change and you’ll see the errors there and then, as and when they
    are introduced.
  prefs: []
  type: TYPE_NORMAL
- en: You should also be aware that optimization is a time-consuming and iterative
    process that will grow your understanding of your code and how the hardware functions.
    This in turn will lead you to design and write better code from the outset as
    you become more familiar with what does and what does not work well on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Questions on Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1. Take an existing program that has one or more GPU kernels. Run the Visual
    Profiler and Parallel Nsight to analyze the kernels. What are the key indicators
    you need to look for? How would you optimize this program?
  prefs: []
  type: TYPE_NORMAL
- en: 2. A colleague brings a printout of a GPU kernel to you and asks your advice
    about how to make it run faster. What would be your advice?
  prefs: []
  type: TYPE_NORMAL
- en: 3. Another colleague proposes to implement a web server using CUDA. Do you think
    this is a good idea? What issues, if any, would you expect with such a program?
  prefs: []
  type: TYPE_NORMAL
- en: 4. Implement a shared memory version of the odd–even sort, which produces a
    single sorted list. What issues might you expect to deal with?
  prefs: []
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1. You should be looking first to the execution time of each kernel. If one
    or more kernels dominate the timing, then, until these are optimized, trying to
    optimize the others is a waste of your time.
  prefs: []
  type: TYPE_NORMAL
- en: Second, you should be looking to the timeline, specifically concerning transfers.
    Are they overlapped with kernel operations and are they using pinned memory or
    not? Is the GPU busy all the time or only periodically given work by the host?
  prefs: []
  type: TYPE_NORMAL
- en: Of the two longest executing kernels, what is causing them to take this time?
    Is there a sufficient number of threads overall? Are there enough blocks to populate
    all the SMs? Are there any peaks on one SM, and if so, why? What is the thread
    to memory pattern and can this be coalesced by the hardware? Are there any serialization
    points, for example, shared memory bank conflicts, atomics, synchronization points?
  prefs: []
  type: TYPE_NORMAL
- en: 2. First, you need to understand the problem before looking at specifics. The
    “look at the code” optimization strategy can be hit or miss. Sure you can probably
    optimize the code on the paper in some way, but you need much more information
    to provide a good answer to the question the person is really asking.
  prefs: []
  type: TYPE_NORMAL
- en: Probably the best answer would be to tell your colleague to profile the application,
    including the host timeline, and then come back with the results. In doing this
    they will likely see what the problems are and these may well not even be related
    to the original kernel printout.
  prefs: []
  type: TYPE_NORMAL
- en: 3. Applications that are highly data parallel are well suited to GPUs. Applications
    that are highly task parallel with lots of divergence threads are not. The typical
    implementation of a web server on a CPU is to spawn one thread per *N* connections
    and to distribute connections dynamically over a cluster of servers to prevent
    overloading any single node.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs execute code in groups of 32 warps, effectively a vector processor with
    the ability to follow single-thread control flow when necessary, but at a large
    performance penalty. Constructing in real time a dynamic web page is very expensive
    in terms of control flow, a significant amount of which will diverge on a per-user
    basis. PCI-E transfers would be small and not efficient.
  prefs: []
  type: TYPE_NORMAL
- en: A GPU would not be a good choice, with the CPU host being a much better choice.
    However, the GPU may be able to be used in the back-end operations of the server,
    performing some analytical work, churning through the user-generated data to make
    sense of it, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 4. This is a useful exercise to think about how to solve some open-ended problems.
    First, the question does not specify how to combine the output of *N* blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The quickest solution for largest datasets should be the sample sort method
    as it completely eliminates the merge sort step. The framework for sample sort
    is provided in the text, but is nonetheless quite a complex sort. However, it
    suffers from a variable number of elements per bin. A prefix sum that padded the
    bins to 128-byte boundaries would help significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Merge sort is much easier to implement, allows for fixed block sizes, and is
    what I’d expect most implementations to opt for.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the odd/even sort, the coalescing problems with global memory are
    largely hidden by the cache in Fermi due to the locality being extremely high.
    A compute 1.x implementation would need to use shared memory/registers for the
    sort. It would need to access the global memory in a coalesced manner in terms
    of loading and writing back.
  prefs: []
  type: TYPE_NORMAL
- en: '[¹](CHP009.html#CFN1)L1 cache is only available on Fermi architecture and is
    configurable between 16 KB and 48 KB. L1 cache on GT200/G80 is only via texture
    memory that is 24 KB in size.'
  prefs: []
  type: TYPE_NORMAL
- en: '[²](CHP009.html#CFN2)L2 cache is zero K on compute 1.x devices, up to 768 K
    on compute 2.x (Fermi) devices and up to 1536 K on compute 3.x (Kepler) devices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[³](#CFN3)Mark Harris, NVIDIA Developer Technology, “Optimizing Parallel Reduction
    in CUDA,” 2007.'
  prefs: []
  type: TYPE_NORMAL
