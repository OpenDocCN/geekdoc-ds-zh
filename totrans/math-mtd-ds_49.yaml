- en: '6.5\. Application: linear-Gaussian models and Kalman filtering#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap06_prob/05_kalman/roch-mmids-prob-kalman.html](https://mmids-textbook.github.io/chap06_prob/05_kalman/roch-mmids-prob-kalman.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this section, we illustrate the use of linear-Gaussian models for object
    tracking. We first give some background.
  prefs: []
  type: TYPE_NORMAL
- en: '6.5.1\. Multivariate Gaussians: marginals and conditionals[#](#multivariate-gaussians-marginals-and-conditionals
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need the marginal and conditional densities of a multivariate Gaussian.
    For this purpose, we require various formulas and results about block matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties of block matrices** Recall that block matrices have a convenient
    algebra that mimics the usual matrix algebra. Consider a square block matrix with
    the same partitioning of the rows and columns, that is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} A_{11} & A_{12}\\ A_{21} & A_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A \in \mathbb{R}^{n \times n}\), \(A_{ij} \in \mathbb{R}^{n_i \times
    n_j}\) for \(i, j = 1, 2\) with the condition \(n_1 + n_2 = n\). Then it is straightforward
    to check (try it!) that the transpose can be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^T = \begin{pmatrix} A_{11}^T & A_{21}^T\\ A_{12}^T & A_{22}^T
    \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, if \(A\) is symmetric then \(A_{11} = A_{11}^T\), \(A_{22} =
    A_{22}^T\) and \(A_{21} = A_{12}^T\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** For instance, consider the \(\mathbf{z} = (\mathbf{z}_1, \mathbf{z}_2)\),
    where \(\mathbf{z}_1 \in \mathbb{R}^{n_1}\) and \(\mathbf{z}_2 \in \mathbb{R}^{n_2}\).
    We want to compute the quadratic form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{z}^T \begin{pmatrix} A_{11} & A_{12}\\ A_{12}^T & A_{22}
    \end{pmatrix} \mathbf{z} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A_{ij} \in \mathbb{R}^{n_i \times n_j}\) for \(i,j = 1, 2\) with the
    conditions \(n_1 + n_2 = n\), and \(A_{11}^T = A_{11}\) and \(A_{22}^T = A_{22}\).
  prefs: []
  type: TYPE_NORMAL
- en: We apply the block matrix product formula twice to get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\mathbf{z}_1, \mathbf{z}_2)^T \begin{pmatrix} A_{11} & A_{12}\\
    A_{12}^T & A_{22} \end{pmatrix} (\mathbf{z}_1, \mathbf{z}_2)\\ &= (\mathbf{z}_1,
    \mathbf{z}_2)^T \begin{pmatrix} A_{11} \mathbf{z}_1 + A_{12} \mathbf{z}_2\\ A_{12}^T
    \mathbf{z}_1 + A_{22} \mathbf{z}_2 \end{pmatrix}\\ &= \mathbf{z}_1^T A_{11} \mathbf{z}_1
    + \mathbf{z}_1^T A_{12} \mathbf{z}_2 + \mathbf{z}_2^T A_{12}^T \mathbf{z}_1 +
    \mathbf{z}_2^T A_{22} \mathbf{z}_2\\ &= \mathbf{z}_1^T A_{11} \mathbf{z}_1 + 2
    \mathbf{z}_1^T A_{12} \mathbf{z}_2 + \mathbf{z}_2^T A_{22} \mathbf{z}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A_{ii} \in \mathbb{R}^{n_i \times n_i}\) for \(i = 1, 2\)
    be invertible. We claim that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} A_{11} & \mathbf{0}\\ \mathbf{0} & A_{22} \end{pmatrix}^{-1}
    = \begin{pmatrix} A_{11}^{-1} & \mathbf{0}\\ \mathbf{0} & A_{22}^{-1} \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The matrix on the right-hand side is well-defined by the invertibility of \(A_{11}\)
    and \(A_{22}\). We check the claim using the formula for matrix products of block
    matrices. The matrices above are block diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} A_{11} & \mathbf{0}\\ \mathbf{0} & A_{22}
    \end{pmatrix} \begin{pmatrix} A_{11}^{-1} & \mathbf{0}\\ \mathbf{0} & A_{22}^{-1}
    \end{pmatrix}\\ &= \begin{pmatrix} A_{11} A_{11}^{-1} + \mathbf{0} & \mathbf{0}
    + \mathbf{0}\\ \mathbf{0} + \mathbf{0} & \mathbf{0} + A_{22} A_{22}^{-1} \end{pmatrix}
    = \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ \mathbf{0} & I_{n_2 \times
    n_2} \end{pmatrix} = I_{n \times n} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly for the other direction. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A_{21} \in \mathbb{R}^{n_2 \times n_1}\). Then we claim
    that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ A_{21} & I_{n_2
    \times n_2} \end{pmatrix}^{-1} = \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    - A_{21} & I_{n_2 \times n_2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: A similar formula holds for the block upper triangular case. In particular,
    such matrices are invertible (which can be proved in other ways, for instance
    through determinants).
  prefs: []
  type: TYPE_NORMAL
- en: 'It suffices to check:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ A_{21} &
    I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    -A_{21} & I_{n_2 \times n_2} \end{pmatrix}\\ &= \begin{pmatrix} I_{n_1 \times
    n_1} I_{n_1 \times n_1} + \mathbf{0} & \mathbf{0} + \mathbf{0}\\ A_{21} I_{n_1
    \times n_1} + (-A_{21}) I_{n_1 \times n_1} & \mathbf{0} + I_{n_2 \times n_2} I_{n_2
    \times n_2} \end{pmatrix} = \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix} = I_{n \times n} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a transpose gives a similar formula for the block upper triangular case.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverting a block matrix** We will need a classical formula for inverting
    a block matrix. We start with the concept of a Schur complement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Schur Complement)** \(\idx{Schur complement}\xdi\) Consider
    the matrix \(B \in \mathbb{R}^{n \times n}\) in block form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11} \in \mathbb{R}^{n_1 \times n_1}\), \(B_{22} \in \mathbb{R}^{n
    - n_1 \times n - n_1}\), \(B_{12} \in \mathbb{R}^{n_1 \times n - n_1}\), and \(B_{21}
    \in \mathbb{R}^{n - n_1 \times n_1}\). Then, provided \(B_{22}\) is invertible,
    the Schur complement of the block \(B_{22}\) is defined as the matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ B/B_{22} := B_{11} - B_{12} B_{22}^{-1} B_{21}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, provided \(B_{11}\) is invertible,
  prefs: []
  type: TYPE_NORMAL
- en: \[ B/B_{11} := B_{22} - B_{21} B_{11}^{-1} B_{12}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Inverting a Block Matrix)** \(\idx{inverting a block matrix lemma}\xdi\)
    Consider the matrix \(B \in \mathbb{R}^{n \times n}\) in block form as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11} \in \mathbb{R}^{n_1 \times n_1}\), \(B_{22} \in \mathbb{R}^{n
    - n_1 \times n - n_1}\), \(B_{12} \in \mathbb{R}^{n_1 \times n - n_1}\), and \(B_{21}
    \in \mathbb{R}^{n - n_1 \times n_1}\). Then, provided \(B_{22}\) is invertible,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^{-1} = \begin{pmatrix} (B/B_{22})^{-1} & - (B/B_{22})^{-1}
    B_{12} B_{22}^{-1}\\ - B_{22}^{-1} B_{21} (B/B_{22})^{-1} & B_{22}^{-1} B_{21}
    (B/B_{22})^{-1} B_{12} B_{22}^{-1} + B_{22}^{-1} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, provided \(B_{11}\) is invertible,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^{-1} = \begin{pmatrix} B_{11}^{-1} B_{12} (B/B_{11})^{-1}
    B_{21} B_{11}^{-1} + B_{11}^{-1} & - B_{11}^{-1} B_{12} (B/B_{11})^{-1}\\ - (B/B_{11})^{-1}
    B_{21} B_{11}^{-1} & (B/B_{11})^{-1} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* One way to prove this is to multiply \(B\) and \(B^{-1}\) and
    check that the identity matrix comes out (try it!). We give a longer proof that
    provides more insight into where the formula is coming from.'
  prefs: []
  type: TYPE_NORMAL
- en: The trick is to multiply \(B\) on the left and right by carefully chosen block
    triangular matrices with identity matrices on the diagonal (which we know are
    invertible by a previous example) to produce a block diagonal matrix with invertible
    matrices on the diagonal (which we know how to invert by a previous example).
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We only prove the first formula. The proof is a calculation based
    on the formula for matrix products of block matrices. We will need that, if \(C\),
    \(D\), and \(E\) are invertible and of the same size, then \((CDE)^{-1} = E^{-1}
    D^{-1} C^{-1}\) (check it!).'
  prefs: []
  type: TYPE_NORMAL
- en: We make a series of observations.
  prefs: []
  type: TYPE_NORMAL
- en: 1- Our first step is to get a zero block in the upper right corner using an
    invertible matrix. Note that (recall that the order of multiplication matters
    here!)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} B_{11} & B_{12}\\
    B_{21} & B_{22} \end{pmatrix}\\ &= \begin{pmatrix} B_{11} - B_{12} B_{22}^{-1}
    B_{21} & B_{12} - B_{12} B_{22}^{-1} B_{22}\\ \mathbf{0} + B_{21} & \mathbf{0}
    + B_{22} \end{pmatrix} = \begin{pmatrix} B/B_{22} & \mathbf{0}\\ B_{21} & B_{22}
    \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 2- Next we get a zero block in the bottom left corner. Starting from the final
    matrix in the last display,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} & \begin{pmatrix} B/B_{22} & \mathbf{0}\\ B_{21} & B_{22} \end{pmatrix}
    \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2
    \times n_2} \end{pmatrix}\\ &= \begin{pmatrix} B/B_{22} + \mathbf{0} & \mathbf{0}
    + \mathbf{0} \\ B_{21} - B_{22} B_{22}^{-1} B_{21} & \mathbf{0} + B_{22} \end{pmatrix}
    = \begin{pmatrix} B/B_{22} & \mathbf{0}\\ \mathbf{0} & B_{22} \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 3- Combining the last two steps, we have shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} B_{11} & B_{12}\\
    B_{21} & B_{22} \end{pmatrix} \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    - B_{22}^{-1} B_{21} & I_{n_2 \times n_2} \end{pmatrix} = \begin{pmatrix} B/B_{22}
    & \mathbf{0}\\ \mathbf{0} & B_{22} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Using the formula for the inverse of a product of three invertible matrices,
    we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1}
    B_{21} & I_{n_2 \times n_2} \end{pmatrix}^{-1} \begin{pmatrix} B_{11} & B_{12}\\
    B_{21} & B_{22} \end{pmatrix}^{-1} \begin{pmatrix} I_{n_1 \times n_1} & - B_{12}
    B_{22}^{-1}\\ \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix}^{-1}\\ &= \begin{pmatrix}
    B/B_{22} & \mathbf{0}\\ \mathbf{0} & B_{22} \end{pmatrix}^{-1}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 4- Rearranging and using the formula for the inverse of a block diagonal matrix,
    we finally get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix}^{-1}\\
    &= \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2
    \times n_2} \end{pmatrix} \begin{pmatrix} B/B_{22} & \mathbf{0}\\ \mathbf{0} &
    B_{22} \end{pmatrix}^{-1} \begin{pmatrix} I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix}\\ &= \begin{pmatrix} I_{n_1 \times
    n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix}
    (B/B_{22})^{-1} & \mathbf{0}\\ \mathbf{0} & B_{22}^{-1} \end{pmatrix} \begin{pmatrix}
    I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\ \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix}\\
    &= \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2
    \times n_2} \end{pmatrix} \begin{pmatrix} (B/B_{22})^{-1} + \mathbf{0}& - (B/B_{22})^{-1}
    B_{12} B_{22}^{-1} + \mathbf{0}\\ \mathbf{0} + \mathbf{0} & \mathbf{0} + B_{22}^{-1}
    \end{pmatrix}\\ &= \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1}
    B_{21} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} (B/B_{22})^{-1} & -
    (B/B_{22})^{-1} B_{12} B_{22}^{-1} \\ \mathbf{0} & B_{22}^{-1} \end{pmatrix}\\
    &= \begin{pmatrix} (B/B_{22})^{-1} & - (B/B_{22})^{-1} B_{12} B_{22}^{-1}\\ -
    B_{22}^{-1} B_{21} (B/B_{22})^{-1} & B_{22}^{-1} B_{21} (B/B_{22})^{-1} B_{12}
    B_{22}^{-1} + B_{22}^{-1} \end{pmatrix}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**The positive definite case** In applying the inversion formula, it will be
    enough to restrict ourselves to the positive definite case, where the lemmas that
    follow guarantee the required invertibility conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Invertibility of Positive Definite Matrices)** \(\idx{invertibility
    of positive definite matrices}\xdi\) Let \(B \in \mathbb{R}^{n \times n}\) be
    symmetric, positive definite. Then \(B\) is invertible. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For any \(\mathbf{x} \neq \mathbf{0}\), it holds by positive definiteness
    that \(\mathbf{x}^T B \mathbf{x} > 0\). In particular, it must be that \(\mathbf{x}^T
    B \mathbf{x} \neq 0\) and therefore, by contradiction, \(B \mathbf{x} \neq \mathbf{0}\)
    (since for any \(\mathbf{z}\), it holds that \(\mathbf{z}^T \mathbf{0} = 0\)).
    The claim follows from the *Equivalent Definition of Linear Independence*. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: A principal submatrix is a square submatrix obtained by removing some rows and
    columns. Moreover we require that the set of row indices that remain is the same
    as the set of column indices that remain.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Principal Submatrices)** \(\idx{principal submatrices lemma}\xdi\)
    Let \(B \in \mathbb{R}^{n \times n}\) be positive definite and let \(Z \in \mathbb{R}^{n
    \times p}\) have full column rank. Then \(Z^T B Z\) is positive definite. In particular
    all principal submatrices of positive definite matrices are positive definite.
    \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* If \(\mathbf{x} \neq \mathbf{0}\), then \(\mathbf{x}^T (Z^T B Z) \mathbf{x}
    = \mathbf{y}^T B \mathbf{y}\), where we defined \(\mathbf{y} = Z \mathbf{x}\).
    Because \(Z\) has full column rank and \(\mathbf{x} \neq \mathbf{0}\), it follows
    that \(\mathbf{y} \neq \mathbf{0}\) by the *Equivalent Definition of Linear Independence*.
    Hence, since \(B \succ 0\), we have \(\mathbf{y}^T B \mathbf{y} > 0\) which proves
    the first claim. For the second claim, take \(Z\) of the form \((\mathbf{e}_{m_1}\
    \mathbf{e}_{m_2}\ \ldots\ \mathbf{e}_{m_p})\), where the indices \(m_1, \ldots,
    m_p\) are distinct and increasing. The columns of \(Z\) are then linearly independent
    since they are distinct basis vectors. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the last claim in the proof, note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ (Z^T B Z)_{i,j} = (Z^T)_{i,\cdot} B Z_{\cdot,j} = (Z_{\cdot,i})^T B Z_{\cdot,j}
    = \sum_{k=1}^n \sum_{\ell=1}^n Z_{k,i} B_{k,\ell} Z_{\ell,j}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So if the \(i\)-th column of \(Z\) is \(\mathbf{e}_{m_i}\) and the \(j\)-th
    column of \(Z\) is \(\mathbf{e}_{m_j}\), then the rightmost summation picks up
    only one element, \(B_{m_i, m_j}\). In other words, \(Z^T B Z\) is the principal
    submatrix of \(B\) corresponding to rows and columns \(m_1, \ldots, m_p\).
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Consider the matrices'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 3 & 0 & 2\\ 2 & 8 & 4 & 0\\ 6 & 1 &
    1 & 4\\ 3 & 2 & 0 & 1 \end{pmatrix} \qquad\text{and}\qquad Z = \begin{pmatrix}
    0 & 0\\ 1 & 0\\ 0 & 0\\ 0 & 1 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following matrices is \(Z^T A Z\)?
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\begin{pmatrix} 1 & 0\\ 6 & 1 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\begin{pmatrix} 8 & 0\\ 2 & 1 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\begin{pmatrix} 2 & 8 & 4 & 0\\ 3 & 2 & 0 & 1 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\begin{pmatrix} 1 & 0\\ 2 & 4\\ 6 & 1\\ 3 & 0 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Schur Complement)** \(\idx{Schur complement lemma}\xdi\) Let \(B
    \in \mathbb{R}^{n \times n}\) be positive definite and write it in block form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} B_{11} & B_{12}\\ B_{12}^T & B_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11} \in \mathbb{R}^{n_1 \times n_1}\) and \(B_{22} \in \mathbb{R}^{n
    - n_1 \times n - n_1}\) are symmetric, and \(B_{12} \in \mathbb{R}^{n_1 \times
    n - n_1}\) . Then the Schur complement of the block \(B_{11}\), i.e., the matrix
    \(B/B_{11} := B_{22} - B_{12}^T B_{11}^{-1} B_{12}\), is well-defined, symmetric,
    and positive definite. The same holds for \(B/B_{22} := B_{11} - B_{12} B_{22}^{-1}
    B_{12}^T\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Principal Submatrices Lemma*, \(B_{11}\) is positive definite.
    By the *Invertibility of Positive Definite Matrices*, \(B_{11}\) is therefore
    invertible. Hence the Schur complement is well defined. Moreover, it is symmetric
    since'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (B/B_{11})^T = B_{22}^T - (B_{12}^T B_{11}^{-1} B_{12})^T = B_{22} - B_{12}^T
    B_{11}^{-1} B_{12} = B/B_{11}, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the symmetry of \(B_{11}\), \(B_{22}\), and \(B_{11}^{-1}\) (prove that last
    one!).
  prefs: []
  type: TYPE_NORMAL
- en: For a non-zero \(\mathbf{x} \in \mathbb{R}^{n_2}\), let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{z} = \begin{pmatrix} \mathbf{z}_1\\ \mathbf{z}_2 \end{pmatrix}
    = \begin{pmatrix} B_{11}^{-1} B_{12} \mathbf{x}\\ - \mathbf{x} \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The result then follows from the observation that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\mathbf{z}^T \begin{pmatrix} B_{11} & B_{12}\\ B_{12}^T &
    B_{22} \end{pmatrix} \mathbf{z}\\ &= \mathbf{z}_1^T B_{11} \mathbf{z}_1 + 2 \mathbf{z}_1^T
    B_{12} \mathbf{z}_2 + \mathbf{z}_2^T B_{22} \mathbf{z}_2\\ &= (B_{11}^{-1} B_{12}
    \mathbf{x})^T B_{11} B_{11}^{-1} B_{12} \mathbf{x} + 2 (B_{11}^{-1} B_{12} \mathbf{x})^T
    B_{12} (- \mathbf{x}) + (- \mathbf{x})^T B_{22} (- \mathbf{x})\\ &= \mathbf{x}^T
    B_{12}^T B_{11}^{-1} B_{12}\,\mathbf{x} - 2 \mathbf{x}^T B_{12}^T B_{11}^{-1}
    B_{12}\,\mathbf{x} + \mathbf{x}^T B_{22}\,\mathbf{x}\\ &= \mathbf{x}^T(B_{22}
    - B_{12}^T B_{11}^{-1} B_{12})\,\mathbf{x}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Marginals and conditionals** We are now ready to derive the distribution
    of marginals and conditionals of multivariate Gaussians\(\idx{multivariate Gaussian}\xdi\).
    Recall that a multivariate Gaussian\(\idx{multivariate normal}\xdi\) vector \(\mathbf{X}
    = (X_1,\ldots,X_d)\) on \(\mathbb{R}^d\) with mean \(\bmu \in \mathbb{R}^d\) and
    positive definite covariance matrix \(\bSigma \in \mathbb{R}^{d \times d}\) has
    probability density function'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that one way to compute \(|\bSigma|\) is as the product of all eigenvalues
    of \(\bSigma\) (with repeats). The matrix \(\bLambda = \bSigma^{-1}\) is called
    the precision matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Partition \(\mathbf{X}\) as the column vector \((\mathbf{X}_1, \mathbf{X}_2)\)
    where \(\mathbf{X}_i \in \mathbb{R}^{d_i}\), \(i=1,2\), with \(d_1 + d_2 = d\).
    Similarly, consider the corresponding block vectors and matrices
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bmu = \begin{pmatrix} \bmu_1\\ \bmu_2 \end{pmatrix} \qquad
    \bSigma = \begin{pmatrix} \bSigma_{11} & \bSigma_{12}\\ \bSigma_{21} & \bSigma_{22}
    \end{pmatrix} \qquad \bLambda = \begin{pmatrix} \bLambda_{11} & \bLambda_{12}\\
    \bLambda_{21} & \bLambda_{22} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that by the symmetry of \(\bSigma\), we have \(\bSigma_{21} = \bSigma_{12}^T\).
    Furthemore, it can be proved that a symmetric, invertible matrix has a symmetric
    inverse (try it!) so that \(\bLambda_{21} = \bLambda_{12}^T\).
  prefs: []
  type: TYPE_NORMAL
- en: We seek to compute the marginals\(\idx{marginal}\xdi\) \(f_{\mathbf{X}_1}(\mathbf{x}_1)\)
    and \(f_{\mathbf{X}_2}(\mathbf{x}_2)\), as well as the conditional density of
    \(\mathbf{X}_1\) given \(\mathbf{X}_2\), which we denote as \(f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1|\mathbf{x}_2)\),
    and similarly \(f_{\mathbf{X}_2|\mathbf{X}_1}(\mathbf{x}_2|\mathbf{x}_1)\).
  prefs: []
  type: TYPE_NORMAL
- en: By the multiplication rule, the joint density \(f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2)\)
    can be decomposed as
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2) = f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1|\mathbf{x}_2)
    f_{\mathbf{X}_2}(\mathbf{x}_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: We use the *Inverting a Block Matrix* lemma to rewrite \(f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2)\)
    in this form and “reveal” the marginal and conditional\(\idx{conditional}\xdi\)
    involved. Indeed, once the joint density is in this form, by integrating over
    \(\mathbf{x}_1\) we obtain that the marginal density of \(\mathbf{X}_2\) is \(f_{\mathbf{X}_2}\)
    and the conditional density of \(\mathbf{X}_1\) given \(\mathbf{X}_2\) is obtained
    by taking the ratio of the joint and the marginal.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it will be easier to work with an expression derived in the proof of
    that lemma, specifically
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} \bSigma_{11} & \bSigma_{12}\\ \bSigma_{21}
    & \bSigma_{22} \end{pmatrix}^{-1}\\ &= \begin{pmatrix} I_{d_1 \times d_1} & \mathbf{0}\\
    - \bSigma_{22}^{-1} \bSigma_{12}^T & I_{d_2 \times d_2} \end{pmatrix} \begin{pmatrix}
    (\bSigma/\bSigma_{22})^{-1} & \mathbf{0}\\ \mathbf{0} & \bSigma_{22}^{-1} \end{pmatrix}
    \begin{pmatrix} I_{d_1 \times d_1} & - \bSigma_{12} \bSigma_{22}^{-1}\\ \mathbf{0}
    & I_{d_2 \times d_2} \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We will use the fact that the first matrix on the last line is the transpose
    of the third one (check it!).
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the joint density, we need to expand the quadratic function \((\mathbf{x}
    - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\) appearing in the exponential. We
    break this up in a few steps.
  prefs: []
  type: TYPE_NORMAL
- en: 1- Note first that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} I_{d_1 \times d_1} & - \bSigma_{12} \bSigma_{22}^{-1}\\
    \mathbf{0} & I_{d_2 \times d_2} \end{pmatrix} \begin{pmatrix} \mathbf{x}_1 - \bmu_1\\
    \mathbf{x}_2 - \bmu_2 \end{pmatrix} = \begin{pmatrix} (\mathbf{x}_1 - \bmu_1)
    - \bSigma_{12} \bSigma_{22}^{-1} (\mathbf{x}_2 - \bmu_2) \\ \mathbf{x}_2 - \bmu_2
    \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly for its transpose. We define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu_{1|2}(\mathbf{x}_2) := \bmu_1 + \bSigma_{12} \bSigma_{22}^{-1} (\mathbf{x}_2
    - \bmu_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: 2- Plugging this back in the quadratic function, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\\ &=
    \begin{pmatrix} \mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2) \\ \mathbf{x}_2 - \bmu_2
    \end{pmatrix}^T \begin{pmatrix} (\bSigma/\bSigma_{22})^{-1} & \mathbf{0}\\ \mathbf{0}
    & \bSigma_{22}^{-1} \end{pmatrix} \begin{pmatrix} \mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2)
    \\ \mathbf{x}_2 - \bmu_2 \end{pmatrix}\\ &= (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))^T
    (\bSigma/\bSigma_{22})^{-1} (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2)) + (\mathbf{x}_2
    - \bmu_2)^T \bSigma_{22}^{-1} (\mathbf{x}_2 - \bmu_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that both terms have the same form as the original quadratic function.
  prefs: []
  type: TYPE_NORMAL
- en: 3- Going back to the density, we use \(\propto\) to indicate that the expression
    holds up to a constant not depending on \(\mathbf{x}\). Using that the exponential
    of a sum is the product of exponentials, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2)\\
    &\propto \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x}
    - \bmu)\right)\\ &\propto \exp\left(-\frac{1}{2}(\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))^T
    (\bSigma/\bSigma_{22})^{-1} (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))\right)\\
    & \qquad \times \exp\left(-\frac{1}{2}(\mathbf{x}_2 - \bmu_2)^T \bSigma_{22}^{-1}
    (\mathbf{x}_2 - \bmu_2)\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 4- We have shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1|\mathbf{x}_2) \propto \exp\left(-\frac{1}{2}(\mathbf{x}_1
    - \bmu_{1|2}(\mathbf{x}_2))^T (\bSigma/\bSigma_{22})^{-1} (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\mathbf{X}_2}(\mathbf{x}_2) \propto \exp\left(-\frac{1}{2}(\mathbf{x}_2
    - \bmu_2)^T \bSigma_{22}^{-1} (\mathbf{x}_2 - \bmu_2)\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the marginal density of \(\mathbf{X}_2\) is multivariate Gaussian
    with mean \(\bmu_2\) and covariance \(\bSigma_{22}\). The conditional density
    of \(\mathbf{X}_1\) given \(\mathbf{X}_2\) is multivariate Gaussian with mean
    \(\bmu_{1|2}(\mathbf{X}_2)\) and covariance \(\bSigma/\bSigma_{22} = \bSigma_{11}
    - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{12}^T\). We write this as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X}_1|\mathbf{X}_2 \sim N_{d_1}(\bmu_{1|2}(\mathbf{X}_2), \bSigma/\bSigma_{22})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X}_2 \sim N_{d_2}(\bmu_2, \bSigma_{22}). \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, by exchanging the roles of \(\mathbf{X}_1\) and \(\mathbf{X}_2\),
    we see that the marginal density of \(\mathbf{X}_1\) is multivariate Gaussian
    with mean \(\bmu_1\) and covariance \(\bSigma_{11}\). The conditional density
    of \(\mathbf{X}_2\) given \(\mathbf{X}_1\) is multivariate Gaussian with mean
    \(\bmu_{2|1}(\mathbf{X}_1) = \bmu_2 + \bSigma_{21} \bSigma_{11}^{-1} (\mathbf{X}_1
    - \bmu_1)\) and covariance \(\bSigma/\bSigma_{11} = \bSigma_{22} - \bSigma_{12}^T
    \bSigma_{11}^{-1} \bSigma_{12}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Suppose \((X_1, X_2)\) is a bivariate Gaussian with mean
    \((0,0)\), variances \(2\) and correlation coefficient \(-1/2\). Conditioned on
    \(X_2 = 1\), what is the mean of \(X_1\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(1/2\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(-1/2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(1\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(-1\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2\. Kalman filter[#](#kalman-filter "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider a stochastic process\(\idx{stochastic process}\xdi\) \(\{\bX_t\}_{t=0}^T\)
    (i.e., a collection of random vectors – often indexed by time) with state space
    \(\S = \mathbb{R}^{d_0}\) of the following form
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_{t+1} = F \,\bX_t + \bW_t \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bW_t\)s are i.i.d. \(N_{d_0}(\mathbf{0}, Q)\) and \(F\) and \(Q\)
    are known \(d_0 \times d_0\) matrices. We denote the initial state by \(\bX_0
    \sim N_{d_0}(\bmu_0, \bSigma_0)\). We assume that the process \(\{\bX_t\}_{t=1}^T\)
    is not observed, but rather that an auxiliary observed process \(\{\bY_t\}_{t=1}^T\)
    with state space \(\S = \mathbb{R}^{d}\) satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bY_t = H\,\bX_t + \bV_t \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bV_t\)s are i.i.d. \(N_d(\mathbf{0}, R)\) and \(H \in \mathbb{R}^{d
    \times d_0}\) and \(R \in \mathbb{R}^{d \times d}\) are known matrices. This is
    an example of a linear-Gaussian system\(\idx{linear-Gaussian system}\xdi\) (also
    known as linear-Gaussian state space model).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to infer the unobserved states given the observed process. Specifically,
    we look at the filtering problem. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Hidden_Markov_model#Filtering):'
  prefs: []
  type: TYPE_NORMAL
- en: The task is to compute, given the model’s parameters and a sequence of observations,
    the distribution over hidden states of the last latent variable at the end of
    the sequence, i.e. to compute P(x(t)|y(1),…,y(t)). This task is normally used
    when the sequence of latent variables is thought of as the underlying states that
    a process moves through at a sequence of points of time, with corresponding observations
    at each point in time. Then, it is natural to ask about the state of the process
    at the end.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Key lemma** Given the structure of the linear-Gaussian model, the following
    lemma will play a key role.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Linear-Gaussian System)** \(\idx{linear-Gaussian system lemma}\xdi\)
    Let \(\bW \sim N_{d}(\bmu, \bSigma)\) and \(\bW''|\bW \sim N_{d''}(A \,\bW, \bSigma'')\)
    where \(A \in \mathbb{R}^{d'' \times d}\) is a deterministic matrix, and \(\bSigma
    \in \mathbb{R}^{d \times d}\) and \(\bSigma'' \in \mathbb{R}^{d'' \times d''}\)
    are positive definite. Then, \((\bW, \bW'')\) is multivariate Gaussian with mean
    vector'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bmu'' = \begin{pmatrix} \bmu \\ A\bmu \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and positive definite covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bSigma'' = \begin{pmatrix} \bSigma & \bSigma A^T\\ A \bSigma
    & A \bSigma A^T + \bSigma' \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** What are the dimensions of each block of \(\bSigma''''\)?
    \(\checkmark\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We have'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &f_{\bW, \bW'}(\bw, \bw')\\ &= f_{\bW}(\bw) \, f_{\bW'|\bW}(\bw'|\bw)\\
    &\propto \exp\left(-\frac{1}{2}(\bw - \bmu)^T \bSigma^{-1} (\bw - \bmu)\right)\\
    & \qquad \times \exp\left(-\frac{1}{2}(\bw' - A \bw)^T (\bSigma')^{-1} (\bw' -
    A \bw)\right)\\ &\propto \exp\left(-\frac{1}{2}\left[(\bw - \bmu)^T \bSigma^{-1}
    (\bw - \bmu) + (\bw' - A \bw)^T (\bSigma')^{-1} (\bw' - A \bw)\right]\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We re-write the quadratic function in square brackets in the exponent as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\bw - \bmu)^T \bSigma^{-1} (\bw - \bmu) + (\bw' - A \bw)^T
    (\bSigma')^{-1} (\bw' - A \bw)\\ &=(\bw - \bmu)^T \bSigma^{-1} (\bw - \bmu)\\
    & \qquad + ([\bw' - A\bmu] - A[\bw-\bmu])^T (\bSigma')^{-1} ([\bw' - A\bmu] -
    A[\bw-\bmu])\\ &=(\bw - \bmu)^T (A^T (\bSigma')^{-1} A + \bSigma^{-1}) (\bw -
    \bmu)\\ & \qquad - 2 (\bw-\bmu)^T A^T (\bSigma')^{-1} (\bw' - A\bmu) + (\bw' -
    A\bmu)^T (\bSigma')^{-1} (\bw' - A\bmu)\\ &= \begin{pmatrix} \bw - \bmu \\ \bw'
    - A\bmu \end{pmatrix}^T \begin{pmatrix} A^T (\bSigma')^{-1} A + \bSigma^{-1} &
    - A^T (\bSigma')^{-1}\\ - (\bSigma')^{-1} A & (\bSigma')^{-1} \end{pmatrix} \begin{pmatrix}
    \bw - \bmu \\ \bw' - A\bmu \end{pmatrix}\\ &= \begin{pmatrix} \bw - \bmu \\ \bw'
    - A\bmu \end{pmatrix}^T \bLambda'' \begin{pmatrix} \bw - \bmu \\ \bw' - A\bmu
    \end{pmatrix}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the last line defines \(\bLambda''\) and the second line shows that it
    is positive definite (why?). We have shown that \((\bW, \bW')\) is multivariate
    Gaussian with mean \((\bmu, A\bmu)\).
  prefs: []
  type: TYPE_NORMAL
- en: We use the *Inverting a Block Matrix Lemma* to invert \(\bLambda''\) and reveal
    the covariance matrix. (One could also compute the covariance directly; try it!)
    We break up \(\bLambda''\) into blocks
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bLambda'' = \begin{pmatrix} \bLambda''_{11} & \bLambda''_{12}\\
    (\bLambda''_{12})^T & \bLambda''_{22} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bLambda''_{11} \in \mathbb{R}^{d \times d}\), \(\bLambda''_{22} \in
    \mathbb{R}^{d' \times d'}\), and \(\bLambda''_{12} \in \mathbb{R}^{d \times d'}\).
    Recall that the inverse is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^{-1} = \begin{pmatrix} (B/B_{22})^{-1} & -(B/B_{22})^{-1}
    B_{12} B_{22}^{-1}\\ -B_{22}^{-1} B_{12}^T (B/B_{22})^{-1} & B_{22}^{-1} B_{12}^T
    (B/B_{22})^{-1} B_{12} B_{22}^{-1} + B_{22}^{-1} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where here \(B = \bLambda''\).
  prefs: []
  type: TYPE_NORMAL
- en: The Schur complement is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bLambda'' / \bLambda''_{22} &= \bLambda''_{11} - \bLambda''_{12}
    (\bLambda''_{22})^{-1} (\bLambda''_{12})^T\\ &= A^T (\bSigma')^{-1} A + \bSigma^{-1}
    - (- A^T (\bSigma')^{-1}) ((\bSigma')^{-1})^{-1} (- A^T (\bSigma')^{-1})^T\\ &=
    A^T (\bSigma')^{-1} A + \bSigma^{-1} - A^T (\bSigma')^{-1} \bSigma' (\bSigma')^{-1}
    A\\ &= A^T (\bSigma')^{-1} A + \bSigma^{-1} - A^T (\bSigma')^{-1} A\\ &= \bSigma^{-1}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence \((\bLambda'' / \bLambda''_{22})^{-1} = \bSigma\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} - (\bLambda''/\bLambda''_{22})^{-1} \bLambda''_{12} (\bLambda''_{22})^{-1}
    &= - \bSigma (- A^T (\bSigma')^{-1}) ((\bSigma')^{-1})^{-1}\\ &= \bSigma A^T (\bSigma')^{-1}
    \bSigma'\\ &= \bSigma A^T \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\bLambda''_{22})^{-1} (\bLambda''_{12})^T (\bLambda''/\bLambda''_{22})^{-1}
    \bLambda''_{12} (\bLambda''_{22})^{-1} + (\bLambda''_{22})^{-1}\\ &= ((\bSigma')^{-1})^{-1}
    (- A^T (\bSigma')^{-1})^T \bSigma (-A^T (\bSigma')^{-1}) ((\bSigma')^{-1})^{-1}
    + ((\bSigma')^{-1})^{-1}\\ &= \bSigma' (\bSigma')^{-1} A \bSigma A^T (\bSigma')^{-1}
    \bSigma' + \bSigma'\\ &= A \bSigma A^T + \bSigma'. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Plugging back into the formula for the inverse of a block matrix completes the
    proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint distribution** We extend our previous notation as follows: for two
    disjoint, finite collections of random vectors \(\{\mathbf{U}_i\}_i\) and \(\{\mathbf{W}_j\}_j\)
    defined on the same probability space, we let \(f_{\{\mathbf{U}_i\}_i, \{\mathbf{W}_j\}_j}\)
    be their joint density, \(f_{\{\mathbf{U}_i\}_i | \{\mathbf{W}_j\}_j}\) be the
    conditional density of \(\{\mathbf{U}_i\}_i\) given \(\{\mathbf{W}_j\}_j\), and
    \(f_{\{\mathbf{U}_i\}_i}\) be the marginal density of \(\{\mathbf{U}_i\}_i\).
    Formally, our goal is to compute'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_{t}|\bY_{1:t}} := f_{\{\bX_{t}\}|\{\bY_1,\ldots,\bY_{t}\}} \]
  prefs: []
  type: TYPE_NORMAL
- en: recursively in \(t\), where we define \(\bY_{1:t} = \{\bY_1,\ldots,\bY_{t}\}\).
    We will see that all densities appearing in this calculation are multivariate
    Gaussians, and therefore keeping track of the means and covariances will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we posit that the density of the full process (with both observed
    and unobserved parts) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_0}(\bx_0) \prod_{t=1}^{T} f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})
    f_{\bY_{t}|\bX_{t}}(\by_{t}|\bx_{t}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the description of the model stipulates that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_{t}|\bX_{t-1} \sim N_{d_0}(F \,\bX_{t-1}, Q). \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bY_t|\bX_t \sim N_d(H\,\bX_t, R) \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(t \geq 1\) and \(\bX_0 \sim N_{d_0}(\bmu_0, \bSigma_0)\). We assume
    that \(\bmu_0\) and \(\bSigma_0\) are known. Applying the *Linear-Gaussian System
    Lemma* inductively shows that the full process \((\bX_{0:T}, \bY_{1:T})\) is jointly
    multivariate Gaussian (try it!). In particular, all marginals and conditionals
    are multivariate Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, it can be represented as follows, where as before each variable
    is a node and its conditional distribution depends only on its parent nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![HMM (with help from Claude)](../Images/dff6cc66efbd6ca33102a19a1e83e4b7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Conditional independence** The stipulated form of the joint density of the
    full process implies many conditional independence relations. We will need the
    following two:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- \(\bX_t\) is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: 2- \(\bY_t\) is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'We prove the first one and leave the second one as an exercise. In fact, we
    prove something stronger: \(\bX_t\) is conditionally independent of \(\bX_{0:t-2},
    \bY_{1:t-1}\) given \(\bX_{t-1}\).'
  prefs: []
  type: TYPE_NORMAL
- en: First, by integrating over \(\by_T\), then \(\bx_{T}\), then \(\by_{T-1}\),
    then \(\bx_{T-1}\), … , then \(\by_t\), we see that the joint density of \((\bX_{0:t},
    \bY_{1:t-1})\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the joint density of \((\bX_{0:t-1}, \bY_{1:t-1})\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: The conditional density given \(\bX_{t-1}\) is then obtained by dividing the
    first expression by the marginal density of \(\bX_{t-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\frac{f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})}{f_{\bX_{t-1}}(\bx_{t-1})}\\
    &= \frac{f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})}{f_{\bX_{t-1}}(\bx_{t-1})}\\
    &= \frac{f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) }{f_{\bX_{t-1}}(\bx_{t-1})}f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})\\
    &= f_{\bX_{0:t-2}, \bY_{1:t-1}|\bX_{t-1}}(\bx_{0:t-2}, \by_{1:t-1}|\bx_{t-1})
    f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})\\ \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm** \(\idx{Kalman filter}\xdi\) We give a recursive algorithm for
    solving the filtering problem, that is, for computing the mean vector and covariance
    matrix of the conditional density \(f_{\bX_{t}|\bY_{1:t}}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Initial step:* The first compute \(f_{\bX_1|\bY_1}\). We do this through a
    series of observations which will generalize straightforwardly.'
  prefs: []
  type: TYPE_NORMAL
- en: 1- We have \(\bX_0 \sim N_{d_0}(\bmu_0, \bSigma_0)\) and \(\bX_{1}|\bX_{0} \sim
    N_{d_0}(F \,\bX_0, Q)\). So, by the *Linear-Gaussian System Lemma*, the joint
    vector \((\bX_0, \bX_{1})\) is multivariate Gaussian with mean vector \((\bmu_0,
    F \bmu_0)\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \bSigma_0 & \bSigma_0 F^T\\ F \bSigma_0 & F
    \bSigma_0 F^T + Q \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the marginal density of \(\bX_{1}\) is multivariate Gaussian with mean
    vector \(F \bmu_0\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_0 := F \bSigma_0 F^T + Q. \]
  prefs: []
  type: TYPE_NORMAL
- en: 2- Combining the previous observation about the marginal density of \(\bX_{1}\)
    with the fact that \(\bY_1|\bX_1 \sim N_d(H\,\bX_1, R)\), the *Linear-Gaussian
    System Lemma* says that \((\bX_1, \bY_{1})\) is multivariate Gaussian with mean
    vector \((F \bmu_0, H F \bmu_0)\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} P_0 & P_0 H^T\\ H P_0 & H P_0 H^T + R \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, define \(K_1 := P_0 H^T (H P_0 H^T + R)^{-1}\). This new observation
    and the conditional density formula give that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_1|\bY_1 \sim N_d(\bmu_1, \bSigma_1) \]
  prefs: []
  type: TYPE_NORMAL
- en: where we define
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu_1 &:= F \bmu_0 + P_0 H^T (H P_0 H^T + R)^{-1} (\mathbf{Y}_1
    - H F \bmu_0)\\ &= F \bmu_0 + K_1 (\mathbf{Y}_1 - H F \bmu_0) \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bSigma_1 &:= P_0 - P_0 H^T (H P_0 H^T + R)^{-1} H P_0\\ &=
    (I_{d_0 \times d_0} - K_1 H) P_0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*General step:* Assuming by induction that \(\bX_{t-1}|\bY_{1:t-1} \sim N_{d_0}(\bmu_{t-1},
    \bSigma_{t-1})\) where \(\bmu_{t-1}\) depends implicitly on \(\bY_{1:t-1}\) (but
    \(\bSigma_{t-1}\) does not), we deduce the next step. It mimics closely the initial
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: '1- Predict: We first “predict” \(\bX_{t}\) given \(\bY_{1:t-1}\). We use the
    fact that \(\bX_{t-1}|\bY_{1:t-1} \sim N_{d_0}(\bmu_{t-1}, \bSigma_{t-1})\). Moreover,
    we have that \(\bX_{t}|\bX_{t-1} \sim N_{d_0}(F \,\bX_{t-1}, Q)\) and that \(\bX_t\)
    is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t-1}\). So, \(\bX_{t}|\{\bX_{t-1},
    \bY_{1:t-1}\} \sim N_{d_0}(F \,\bX_{t-1}, Q)\) by the *Role of Independence Lemma*.
    By the *Linear-Gaussian System Lemma*, the joint vector \((\bX_{t-1}, \bX_{t})\)
    conditioned on \(\bY_{1:t-1}\) is multivariate Gaussian with mean vector \((\bmu_{t-1},
    F \bmu_{t-1})\) and covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \bSigma_{t-1} & \bSigma_{t-1} F^T\\ F \bSigma_{t-1}
    & F \bSigma_{t-1} F^T + Q \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, the conditional marginal density of \(\bX_{t}\) given \(\bY_{1:t-1}\)
    is multivariate Gaussian with mean vector \(F \bmu_{t-1}\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{t-1} := F \bSigma_{t-1} F^T + Q. \]
  prefs: []
  type: TYPE_NORMAL
- en: '2- Update: Next we “update” our prediction of \(\bX_{t}\) using the new observation
    \(\bY_{t}\). We have that \(\bY_{t}|\bX_{t} \sim N_d(H\,\bX_{t}, R)\) and that
    \(\bY_t\) is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t}\). So
    \(\bY_{t}|\{\bX_{t}, \bY_{1:t-1}\} \sim N_d(H\,\bX_{t}, R)\) by the *Role of Independence*
    lemma. Combining this with the previous observation, the *Linear-Gaussian System*
    lemma says that \((\bX_{t}, \bY_{t})\) given \(\bY_{1:t-1}\) is multivariate Gaussian
    with mean vector \((F \bmu_{t-1}, H F \bmu_{t-1})\) and covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} P_{t-1} & P_{t-1} H^T\\ H P_{t-1} & H P_{t-1}
    H^T + R \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, define \(K_{t} := P_{t-1} H^T (H P_{t-1} H^T + R)^{-1}\). This new
    observation and the conditional density formula give that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_{t}|\{\bY_t, \bY_{1:t-1}\} = \bX_{t}|\bY_{1:t} \sim N_d(\bmu_{t}, \bSigma_{t})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where we define
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu_{t} &:= F \bmu_{t-1} + K_{t} (\mathbf{Y}_{t} - H F \bmu_{t-1})
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bSigma_{t} &= (I_{d_0 \times d_0} - K_{t} H) P_{t-1}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Summary:* Let \(\bmu_t\) and \(\bSigma_t\) be the mean and covariance matrix
    of \(\bX_t\) conditioned on \(\bY_{1:t}\). The recursions for these quantities
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu_t &= F\,\bmu_{t-1} + K_{t} (\bY_{t} - H F \bmu_{t-1})\\
    \bSigma_t &= (I_{d_0 \times d_0} - K_t H) P_{t-1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P_{t-1} &= F \,\bSigma_{t-1} F^T + Q\\ K_t &= P_{t-1} H^T (H
    P_{t-1} H^T + R)^{-1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: This last matrix is known as the Kalman gain matrix. The vector \(\bY_{t} -
    H F \bmu_{t-1}\) is referred to as innovation; it compares the new observation
    \(\bY_{t}\) to its predicted expectation \(H F \bmu_{t-1}\) based on the previous
    observations. Hence, in some sense, the Kalman gain matrix\(\idx{Kalman gain matrix}\xdi\)
    represents the “weight” given to the observation at time \(t\) when updating the
    state estimate \(\bmu_t\). The solution above is known as Kalman filtering.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Explore the concept of sequential Monte Carlo methods, also
    known as particle filters, as an alternative to the Kalman filter. Ask your favorite
    AI chatbot for an explanation and implementation of a particle filter. Try it
    on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3\. Back to location tracking[#](#back-to-location-tracking "Link to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We apply Kalman filtering to location tracking. Returning to our cyborg corgi
    example, we imagine that we get noisy observations about its successive positions
    in a park. (Think of GPS measurements.) We seek to get a better estimate of its
    location using the method above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Cyborg corgi (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cyborg corgi](../Images/348998a4b7d1b53beeec4c9913f351c0.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: We model the true location as a linear-Gaussian system over the 2d position
    \((z_{1,t}, z_{2,t})_t\) and velocity \((\dot{z}_{1,t}, \dot{z}_{2,t})_t\) sampled
    at \(\Delta\) intervals of time. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bX_t = (z_{1,t}, z_{2,t}, \dot{z}_{1,t}, \dot{z}_{2,t}), \quad
    F = \begin{pmatrix} 1 & 0 & \Delta & 0\\ 0 & 1 & 0 & \Delta\\ 0 & 0 & 1 & 0\\
    0 & 0 & 0 & 1 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so the unobserved dynamics are
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} z_{1,t+1}\\ z_{2,t+1}\\ \dot{z}_{1,t+1}\\ \dot{z}_{2,t+1}
    \end{pmatrix} = \bX_{t+1} = F \,\bX_t + \bW_t = \begin{pmatrix} z_{1,t} + \Delta
    \dot{z}_{1,t} + W_{1,t}\\ z_{2,t} + \Delta \dot{z}_{2,t} + W_{2,t}\\ \dot{z}_{1,t}
    + \dot{W}_{1,t}\\ \dot{z}_{2,t} + \dot{W}_{2,t} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bW_t = (W_{1,t}, W_{2,t}, \dot{W}_{1,t}, \dot{W}_{2,t}) \sim N_{d_0}(\mathbf{0},
    Q)\) with \(Q\) known.
  prefs: []
  type: TYPE_NORMAL
- en: In words, the velocity is unchanged, up to Gaussian perturbation. The position
    changes proportionally to the velocity in the corresponding dimension, again up
    to Gaussian perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: The observations \((\tilde{z}_{1,t}, \tilde{z}_{2,t})_t\) are modeled as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bY_t = (\tilde{z}_{1,t}, \tilde{z}_{2,t}), \quad H = \begin{pmatrix}
    1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so the observed process satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \tilde{z}_{1,t}\\ \tilde{z}_{2,t} \end{pmatrix}
    = \bY_t = H\,\bX_t + \bV_t = \begin{pmatrix} z_{1,t} + V_{1,t}\\ z_{2,t} + V_{2,t}
    \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bV_t = (V_{1,t}, V_{2,t}) \sim N_d(\mathbf{0}, R)\) with \(R\)
    known.
  prefs: []
  type: TYPE_NORMAL
- en: In words, we only observe the positions, up to Gaussian noise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing the Kalman filter** We implement the Kalman filter as described
    above with known covariance matrices. We take \(\Delta = 1\) for simplicity. The
    code is adapted from [[Mur0](https://github.com/probml/pmtk3/blob/master/demos/kalmanTrackingDemo.m)].'
  prefs: []
  type: TYPE_NORMAL
- en: We will test Kalman filtering on a simulated path drawn from the linear-Gaussian
    model above. The following function creates such a path and its noisy observations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Here is an example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the next plot (and throughout this section), the dots are the noisy observations.
    The unobserved true path is also shown as a dotted line.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]</details> ![../../_images/7887a8a25f3bbd2d4c1cc1b809a24f2aa031c84a10f4cd5ed5f0ae71eff0253c.png](../Images/c8b3c6c67639d980e6620091452f12a9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The following function implements the Kalman filter. The full recursion is broken
    up into several steps. We use [`numpy.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)
    to compute the Kalman gain matrix. Below, `mu_pred` is \(F \bmu_{t-1}\) and `Sig_pred`
    is \(P_{t-1} = F \bSigma_{t-1} F^T + Q\), which are the mean vector and covariance
    matrix of \(\bX_{t}\) given \(\bY_{1:t-1}\) as computed in the *Predict* step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We apply this to the location tracking example. The inferred
    states, or more precisely their estimated mean, are in blue. Note that we also
    inferred the velocity at each time point, but we are not plotting that information.'
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]</details> ![../../_images/046beb4acdf4c61da2a5de4c6dbb7d32ca2d7b4e8416651030b3f49747fcac9a.png](../Images/959069dbefd76cfab2d54229507451d1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: To quantify the improvement in the inferred means compared to the observations,
    we compute the mean squared error in both cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We indeed observe a substantial reduction.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The Kalman filter assumes that the parameters of the state
    evolution and observation models are known. Ask your favorite AI chatbot about
    methods for estimating these parameters from data, such as the expectation-maximization
    algorithm or the variational Bayes approach. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is the Schur complement of the block \(B_{11}\)
    in the positive definite matrix \(B = \begin{pmatrix} B_{11} & B_{12} \\ B_{12}^T
    & B_{22} \end{pmatrix}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(B_{22} - B_{12}^T B_{11}^{-1} B_{12}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(B_{11} - B_{12} B_{22}^{-1} B_{12}^T\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(B_{22}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(B_{11}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is true about the Schur complement \(B/B_{11}\)
    of the block \(B_{11}\) in a positive definite matrix \(B\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is always symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is always positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: c) Both a and b.
  prefs: []
  type: TYPE_NORMAL
- en: d) Neither a nor b.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** What is the conditional distribution of \(\mathbf{X}_1\) given \(\mathbf{X}_2\)
    in a multivariate Gaussian distribution?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 + \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} - \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 - \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} + \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 + \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} + \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 - \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} - \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In a linear-Gaussian system, which of the following is true about the
    conditional independence relationships?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{X}_t\) is conditionally independent of \(\mathbf{Y}_{1:t-1}\) given
    \(\mathbf{X}_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{Y}_t\) is conditionally independent of \(\mathbf{Y}_{1:t-1}\) given
    \(\mathbf{X}_t\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{X}_t\) is conditionally independent of \(\mathbf{X}_{t-2}\) given
    \(\mathbf{X}_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the above.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In the Kalman filter, what does the Kalman gain matrix \(K_t\) represent?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The covariance matrix of the state estimate at time \(t\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The covariance matrix of the observation at time \(t\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The weight given to the observation at time \(t\) when updating the state
    estimate.
  prefs: []
  type: TYPE_NORMAL
- en: d) The weight given to the previous state estimate when predicting the current
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: a. Justification: The text defines the Schur complement of \(B_{11}\)
    as \(B / B_{11} := B_{22} - B_{12}^T B_{11}^{-1} B_{12}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: The text states “the Schur complement of the
    block \(B_{11}\), i.e., the matrix \(B/B_{11} := B_{22} - B_{12}^T B_{11}^{-1}
    B_{12}\), is symmetric and positive definite.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The conditional distribution of \(\mathbf{X}_1\)
    given \(\mathbf{X}_2\) is derived in the text as \(\mathbf{X}_1 | \mathbf{X}_2
    \sim N_{d_1}(\boldsymbol{\mu}_1 + \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2 -
    \boldsymbol{\mu}_2), \bSigma_{11} - \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text explicitly states these conditional
    independence relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: The text defines \(K_t := P_{t-1} H^T (H P_{t-1}
    H^T + R)^{-1}\) as the Kalman gain matrix, which is used to update the state estimate
    as \(\boldsymbol{\mu}_t := F \boldsymbol{\mu}_{t-1} + K_t (\mathbf{Y}_t - H F
    \boldsymbol{\mu}_{t-1})\), where \(K_t\) weighs the innovation \((\mathbf{Y}_t
    - H F \boldsymbol{\mu}_{t-1})\).'
  prefs: []
  type: TYPE_NORMAL
- en: '6.5.1\. Multivariate Gaussians: marginals and conditionals[#](#multivariate-gaussians-marginals-and-conditionals
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need the marginal and conditional densities of a multivariate Gaussian.
    For this purpose, we require various formulas and results about block matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '**Properties of block matrices** Recall that block matrices have a convenient
    algebra that mimics the usual matrix algebra. Consider a square block matrix with
    the same partitioning of the rows and columns, that is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} A_{11} & A_{12}\\ A_{21} & A_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A \in \mathbb{R}^{n \times n}\), \(A_{ij} \in \mathbb{R}^{n_i \times
    n_j}\) for \(i, j = 1, 2\) with the condition \(n_1 + n_2 = n\). Then it is straightforward
    to check (try it!) that the transpose can be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^T = \begin{pmatrix} A_{11}^T & A_{21}^T\\ A_{12}^T & A_{22}^T
    \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, if \(A\) is symmetric then \(A_{11} = A_{11}^T\), \(A_{22} =
    A_{22}^T\) and \(A_{21} = A_{12}^T\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** For instance, consider the \(\mathbf{z} = (\mathbf{z}_1, \mathbf{z}_2)\),
    where \(\mathbf{z}_1 \in \mathbb{R}^{n_1}\) and \(\mathbf{z}_2 \in \mathbb{R}^{n_2}\).
    We want to compute the quadratic form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{z}^T \begin{pmatrix} A_{11} & A_{12}\\ A_{12}^T & A_{22}
    \end{pmatrix} \mathbf{z} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A_{ij} \in \mathbb{R}^{n_i \times n_j}\) for \(i,j = 1, 2\) with the
    conditions \(n_1 + n_2 = n\), and \(A_{11}^T = A_{11}\) and \(A_{22}^T = A_{22}\).
  prefs: []
  type: TYPE_NORMAL
- en: We apply the block matrix product formula twice to get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\mathbf{z}_1, \mathbf{z}_2)^T \begin{pmatrix} A_{11} & A_{12}\\
    A_{12}^T & A_{22} \end{pmatrix} (\mathbf{z}_1, \mathbf{z}_2)\\ &= (\mathbf{z}_1,
    \mathbf{z}_2)^T \begin{pmatrix} A_{11} \mathbf{z}_1 + A_{12} \mathbf{z}_2\\ A_{12}^T
    \mathbf{z}_1 + A_{22} \mathbf{z}_2 \end{pmatrix}\\ &= \mathbf{z}_1^T A_{11} \mathbf{z}_1
    + \mathbf{z}_1^T A_{12} \mathbf{z}_2 + \mathbf{z}_2^T A_{12}^T \mathbf{z}_1 +
    \mathbf{z}_2^T A_{22} \mathbf{z}_2\\ &= \mathbf{z}_1^T A_{11} \mathbf{z}_1 + 2
    \mathbf{z}_1^T A_{12} \mathbf{z}_2 + \mathbf{z}_2^T A_{22} \mathbf{z}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A_{ii} \in \mathbb{R}^{n_i \times n_i}\) for \(i = 1, 2\)
    be invertible. We claim that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} A_{11} & \mathbf{0}\\ \mathbf{0} & A_{22} \end{pmatrix}^{-1}
    = \begin{pmatrix} A_{11}^{-1} & \mathbf{0}\\ \mathbf{0} & A_{22}^{-1} \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The matrix on the right-hand side is well-defined by the invertibility of \(A_{11}\)
    and \(A_{22}\). We check the claim using the formula for matrix products of block
    matrices. The matrices above are block diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} A_{11} & \mathbf{0}\\ \mathbf{0} & A_{22}
    \end{pmatrix} \begin{pmatrix} A_{11}^{-1} & \mathbf{0}\\ \mathbf{0} & A_{22}^{-1}
    \end{pmatrix}\\ &= \begin{pmatrix} A_{11} A_{11}^{-1} + \mathbf{0} & \mathbf{0}
    + \mathbf{0}\\ \mathbf{0} + \mathbf{0} & \mathbf{0} + A_{22} A_{22}^{-1} \end{pmatrix}
    = \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ \mathbf{0} & I_{n_2 \times
    n_2} \end{pmatrix} = I_{n \times n} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly for the other direction. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A_{21} \in \mathbb{R}^{n_2 \times n_1}\). Then we claim
    that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ A_{21} & I_{n_2
    \times n_2} \end{pmatrix}^{-1} = \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    - A_{21} & I_{n_2 \times n_2} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: A similar formula holds for the block upper triangular case. In particular,
    such matrices are invertible (which can be proved in other ways, for instance
    through determinants).
  prefs: []
  type: TYPE_NORMAL
- en: 'It suffices to check:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ A_{21} &
    I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    -A_{21} & I_{n_2 \times n_2} \end{pmatrix}\\ &= \begin{pmatrix} I_{n_1 \times
    n_1} I_{n_1 \times n_1} + \mathbf{0} & \mathbf{0} + \mathbf{0}\\ A_{21} I_{n_1
    \times n_1} + (-A_{21}) I_{n_1 \times n_1} & \mathbf{0} + I_{n_2 \times n_2} I_{n_2
    \times n_2} \end{pmatrix} = \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix} = I_{n \times n} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a transpose gives a similar formula for the block upper triangular case.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverting a block matrix** We will need a classical formula for inverting
    a block matrix. We start with the concept of a Schur complement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Schur Complement)** \(\idx{Schur complement}\xdi\) Consider
    the matrix \(B \in \mathbb{R}^{n \times n}\) in block form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11} \in \mathbb{R}^{n_1 \times n_1}\), \(B_{22} \in \mathbb{R}^{n
    - n_1 \times n - n_1}\), \(B_{12} \in \mathbb{R}^{n_1 \times n - n_1}\), and \(B_{21}
    \in \mathbb{R}^{n - n_1 \times n_1}\). Then, provided \(B_{22}\) is invertible,
    the Schur complement of the block \(B_{22}\) is defined as the matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ B/B_{22} := B_{11} - B_{12} B_{22}^{-1} B_{21}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, provided \(B_{11}\) is invertible,
  prefs: []
  type: TYPE_NORMAL
- en: \[ B/B_{11} := B_{22} - B_{21} B_{11}^{-1} B_{12}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Inverting a Block Matrix)** \(\idx{inverting a block matrix lemma}\xdi\)
    Consider the matrix \(B \in \mathbb{R}^{n \times n}\) in block form as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11} \in \mathbb{R}^{n_1 \times n_1}\), \(B_{22} \in \mathbb{R}^{n
    - n_1 \times n - n_1}\), \(B_{12} \in \mathbb{R}^{n_1 \times n - n_1}\), and \(B_{21}
    \in \mathbb{R}^{n - n_1 \times n_1}\). Then, provided \(B_{22}\) is invertible,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^{-1} = \begin{pmatrix} (B/B_{22})^{-1} & - (B/B_{22})^{-1}
    B_{12} B_{22}^{-1}\\ - B_{22}^{-1} B_{21} (B/B_{22})^{-1} & B_{22}^{-1} B_{21}
    (B/B_{22})^{-1} B_{12} B_{22}^{-1} + B_{22}^{-1} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, provided \(B_{11}\) is invertible,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^{-1} = \begin{pmatrix} B_{11}^{-1} B_{12} (B/B_{11})^{-1}
    B_{21} B_{11}^{-1} + B_{11}^{-1} & - B_{11}^{-1} B_{12} (B/B_{11})^{-1}\\ - (B/B_{11})^{-1}
    B_{21} B_{11}^{-1} & (B/B_{11})^{-1} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* One way to prove this is to multiply \(B\) and \(B^{-1}\) and
    check that the identity matrix comes out (try it!). We give a longer proof that
    provides more insight into where the formula is coming from.'
  prefs: []
  type: TYPE_NORMAL
- en: The trick is to multiply \(B\) on the left and right by carefully chosen block
    triangular matrices with identity matrices on the diagonal (which we know are
    invertible by a previous example) to produce a block diagonal matrix with invertible
    matrices on the diagonal (which we know how to invert by a previous example).
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We only prove the first formula. The proof is a calculation based
    on the formula for matrix products of block matrices. We will need that, if \(C\),
    \(D\), and \(E\) are invertible and of the same size, then \((CDE)^{-1} = E^{-1}
    D^{-1} C^{-1}\) (check it!).'
  prefs: []
  type: TYPE_NORMAL
- en: We make a series of observations.
  prefs: []
  type: TYPE_NORMAL
- en: 1- Our first step is to get a zero block in the upper right corner using an
    invertible matrix. Note that (recall that the order of multiplication matters
    here!)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} B_{11} & B_{12}\\
    B_{21} & B_{22} \end{pmatrix}\\ &= \begin{pmatrix} B_{11} - B_{12} B_{22}^{-1}
    B_{21} & B_{12} - B_{12} B_{22}^{-1} B_{22}\\ \mathbf{0} + B_{21} & \mathbf{0}
    + B_{22} \end{pmatrix} = \begin{pmatrix} B/B_{22} & \mathbf{0}\\ B_{21} & B_{22}
    \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 2- Next we get a zero block in the bottom left corner. Starting from the final
    matrix in the last display,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} & \begin{pmatrix} B/B_{22} & \mathbf{0}\\ B_{21} & B_{22} \end{pmatrix}
    \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2
    \times n_2} \end{pmatrix}\\ &= \begin{pmatrix} B/B_{22} + \mathbf{0} & \mathbf{0}
    + \mathbf{0} \\ B_{21} - B_{22} B_{22}^{-1} B_{21} & \mathbf{0} + B_{22} \end{pmatrix}
    = \begin{pmatrix} B/B_{22} & \mathbf{0}\\ \mathbf{0} & B_{22} \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 3- Combining the last two steps, we have shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} B_{11} & B_{12}\\
    B_{21} & B_{22} \end{pmatrix} \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\
    - B_{22}^{-1} B_{21} & I_{n_2 \times n_2} \end{pmatrix} = \begin{pmatrix} B/B_{22}
    & \mathbf{0}\\ \mathbf{0} & B_{22} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Using the formula for the inverse of a product of three invertible matrices,
    we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1}
    B_{21} & I_{n_2 \times n_2} \end{pmatrix}^{-1} \begin{pmatrix} B_{11} & B_{12}\\
    B_{21} & B_{22} \end{pmatrix}^{-1} \begin{pmatrix} I_{n_1 \times n_1} & - B_{12}
    B_{22}^{-1}\\ \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix}^{-1}\\ &= \begin{pmatrix}
    B/B_{22} & \mathbf{0}\\ \mathbf{0} & B_{22} \end{pmatrix}^{-1}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 4- Rearranging and using the formula for the inverse of a block diagonal matrix,
    we finally get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix}^{-1}\\
    &= \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2
    \times n_2} \end{pmatrix} \begin{pmatrix} B/B_{22} & \mathbf{0}\\ \mathbf{0} &
    B_{22} \end{pmatrix}^{-1} \begin{pmatrix} I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\
    \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix}\\ &= \begin{pmatrix} I_{n_1 \times
    n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix}
    (B/B_{22})^{-1} & \mathbf{0}\\ \mathbf{0} & B_{22}^{-1} \end{pmatrix} \begin{pmatrix}
    I_{n_1 \times n_1} & - B_{12} B_{22}^{-1}\\ \mathbf{0} & I_{n_2 \times n_2} \end{pmatrix}\\
    &= \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1} B_{21} & I_{n_2
    \times n_2} \end{pmatrix} \begin{pmatrix} (B/B_{22})^{-1} + \mathbf{0}& - (B/B_{22})^{-1}
    B_{12} B_{22}^{-1} + \mathbf{0}\\ \mathbf{0} + \mathbf{0} & \mathbf{0} + B_{22}^{-1}
    \end{pmatrix}\\ &= \begin{pmatrix} I_{n_1 \times n_1} & \mathbf{0}\\ - B_{22}^{-1}
    B_{21} & I_{n_2 \times n_2} \end{pmatrix} \begin{pmatrix} (B/B_{22})^{-1} & -
    (B/B_{22})^{-1} B_{12} B_{22}^{-1} \\ \mathbf{0} & B_{22}^{-1} \end{pmatrix}\\
    &= \begin{pmatrix} (B/B_{22})^{-1} & - (B/B_{22})^{-1} B_{12} B_{22}^{-1}\\ -
    B_{22}^{-1} B_{21} (B/B_{22})^{-1} & B_{22}^{-1} B_{21} (B/B_{22})^{-1} B_{12}
    B_{22}^{-1} + B_{22}^{-1} \end{pmatrix}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**The positive definite case** In applying the inversion formula, it will be
    enough to restrict ourselves to the positive definite case, where the lemmas that
    follow guarantee the required invertibility conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Invertibility of Positive Definite Matrices)** \(\idx{invertibility
    of positive definite matrices}\xdi\) Let \(B \in \mathbb{R}^{n \times n}\) be
    symmetric, positive definite. Then \(B\) is invertible. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For any \(\mathbf{x} \neq \mathbf{0}\), it holds by positive definiteness
    that \(\mathbf{x}^T B \mathbf{x} > 0\). In particular, it must be that \(\mathbf{x}^T
    B \mathbf{x} \neq 0\) and therefore, by contradiction, \(B \mathbf{x} \neq \mathbf{0}\)
    (since for any \(\mathbf{z}\), it holds that \(\mathbf{z}^T \mathbf{0} = 0\)).
    The claim follows from the *Equivalent Definition of Linear Independence*. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: A principal submatrix is a square submatrix obtained by removing some rows and
    columns. Moreover we require that the set of row indices that remain is the same
    as the set of column indices that remain.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Principal Submatrices)** \(\idx{principal submatrices lemma}\xdi\)
    Let \(B \in \mathbb{R}^{n \times n}\) be positive definite and let \(Z \in \mathbb{R}^{n
    \times p}\) have full column rank. Then \(Z^T B Z\) is positive definite. In particular
    all principal submatrices of positive definite matrices are positive definite.
    \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* If \(\mathbf{x} \neq \mathbf{0}\), then \(\mathbf{x}^T (Z^T B Z) \mathbf{x}
    = \mathbf{y}^T B \mathbf{y}\), where we defined \(\mathbf{y} = Z \mathbf{x}\).
    Because \(Z\) has full column rank and \(\mathbf{x} \neq \mathbf{0}\), it follows
    that \(\mathbf{y} \neq \mathbf{0}\) by the *Equivalent Definition of Linear Independence*.
    Hence, since \(B \succ 0\), we have \(\mathbf{y}^T B \mathbf{y} > 0\) which proves
    the first claim. For the second claim, take \(Z\) of the form \((\mathbf{e}_{m_1}\
    \mathbf{e}_{m_2}\ \ldots\ \mathbf{e}_{m_p})\), where the indices \(m_1, \ldots,
    m_p\) are distinct and increasing. The columns of \(Z\) are then linearly independent
    since they are distinct basis vectors. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the last claim in the proof, note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ (Z^T B Z)_{i,j} = (Z^T)_{i,\cdot} B Z_{\cdot,j} = (Z_{\cdot,i})^T B Z_{\cdot,j}
    = \sum_{k=1}^n \sum_{\ell=1}^n Z_{k,i} B_{k,\ell} Z_{\ell,j}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So if the \(i\)-th column of \(Z\) is \(\mathbf{e}_{m_i}\) and the \(j\)-th
    column of \(Z\) is \(\mathbf{e}_{m_j}\), then the rightmost summation picks up
    only one element, \(B_{m_i, m_j}\). In other words, \(Z^T B Z\) is the principal
    submatrix of \(B\) corresponding to rows and columns \(m_1, \ldots, m_p\).
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Consider the matrices'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 3 & 0 & 2\\ 2 & 8 & 4 & 0\\ 6 & 1 &
    1 & 4\\ 3 & 2 & 0 & 1 \end{pmatrix} \qquad\text{and}\qquad Z = \begin{pmatrix}
    0 & 0\\ 1 & 0\\ 0 & 0\\ 0 & 1 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following matrices is \(Z^T A Z\)?
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\begin{pmatrix} 1 & 0\\ 6 & 1 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\begin{pmatrix} 8 & 0\\ 2 & 1 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\begin{pmatrix} 2 & 8 & 4 & 0\\ 3 & 2 & 0 & 1 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\begin{pmatrix} 1 & 0\\ 2 & 4\\ 6 & 1\\ 3 & 0 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Schur Complement)** \(\idx{Schur complement lemma}\xdi\) Let \(B
    \in \mathbb{R}^{n \times n}\) be positive definite and write it in block form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} B_{11} & B_{12}\\ B_{12}^T & B_{22} \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11} \in \mathbb{R}^{n_1 \times n_1}\) and \(B_{22} \in \mathbb{R}^{n
    - n_1 \times n - n_1}\) are symmetric, and \(B_{12} \in \mathbb{R}^{n_1 \times
    n - n_1}\) . Then the Schur complement of the block \(B_{11}\), i.e., the matrix
    \(B/B_{11} := B_{22} - B_{12}^T B_{11}^{-1} B_{12}\), is well-defined, symmetric,
    and positive definite. The same holds for \(B/B_{22} := B_{11} - B_{12} B_{22}^{-1}
    B_{12}^T\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Principal Submatrices Lemma*, \(B_{11}\) is positive definite.
    By the *Invertibility of Positive Definite Matrices*, \(B_{11}\) is therefore
    invertible. Hence the Schur complement is well defined. Moreover, it is symmetric
    since'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (B/B_{11})^T = B_{22}^T - (B_{12}^T B_{11}^{-1} B_{12})^T = B_{22} - B_{12}^T
    B_{11}^{-1} B_{12} = B/B_{11}, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the symmetry of \(B_{11}\), \(B_{22}\), and \(B_{11}^{-1}\) (prove that last
    one!).
  prefs: []
  type: TYPE_NORMAL
- en: For a non-zero \(\mathbf{x} \in \mathbb{R}^{n_2}\), let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{z} = \begin{pmatrix} \mathbf{z}_1\\ \mathbf{z}_2 \end{pmatrix}
    = \begin{pmatrix} B_{11}^{-1} B_{12} \mathbf{x}\\ - \mathbf{x} \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The result then follows from the observation that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\mathbf{z}^T \begin{pmatrix} B_{11} & B_{12}\\ B_{12}^T &
    B_{22} \end{pmatrix} \mathbf{z}\\ &= \mathbf{z}_1^T B_{11} \mathbf{z}_1 + 2 \mathbf{z}_1^T
    B_{12} \mathbf{z}_2 + \mathbf{z}_2^T B_{22} \mathbf{z}_2\\ &= (B_{11}^{-1} B_{12}
    \mathbf{x})^T B_{11} B_{11}^{-1} B_{12} \mathbf{x} + 2 (B_{11}^{-1} B_{12} \mathbf{x})^T
    B_{12} (- \mathbf{x}) + (- \mathbf{x})^T B_{22} (- \mathbf{x})\\ &= \mathbf{x}^T
    B_{12}^T B_{11}^{-1} B_{12}\,\mathbf{x} - 2 \mathbf{x}^T B_{12}^T B_{11}^{-1}
    B_{12}\,\mathbf{x} + \mathbf{x}^T B_{22}\,\mathbf{x}\\ &= \mathbf{x}^T(B_{22}
    - B_{12}^T B_{11}^{-1} B_{12})\,\mathbf{x}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Marginals and conditionals** We are now ready to derive the distribution
    of marginals and conditionals of multivariate Gaussians\(\idx{multivariate Gaussian}\xdi\).
    Recall that a multivariate Gaussian\(\idx{multivariate normal}\xdi\) vector \(\mathbf{X}
    = (X_1,\ldots,X_d)\) on \(\mathbb{R}^d\) with mean \(\bmu \in \mathbb{R}^d\) and
    positive definite covariance matrix \(\bSigma \in \mathbb{R}^{d \times d}\) has
    probability density function'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that one way to compute \(|\bSigma|\) is as the product of all eigenvalues
    of \(\bSigma\) (with repeats). The matrix \(\bLambda = \bSigma^{-1}\) is called
    the precision matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Partition \(\mathbf{X}\) as the column vector \((\mathbf{X}_1, \mathbf{X}_2)\)
    where \(\mathbf{X}_i \in \mathbb{R}^{d_i}\), \(i=1,2\), with \(d_1 + d_2 = d\).
    Similarly, consider the corresponding block vectors and matrices
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bmu = \begin{pmatrix} \bmu_1\\ \bmu_2 \end{pmatrix} \qquad
    \bSigma = \begin{pmatrix} \bSigma_{11} & \bSigma_{12}\\ \bSigma_{21} & \bSigma_{22}
    \end{pmatrix} \qquad \bLambda = \begin{pmatrix} \bLambda_{11} & \bLambda_{12}\\
    \bLambda_{21} & \bLambda_{22} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that by the symmetry of \(\bSigma\), we have \(\bSigma_{21} = \bSigma_{12}^T\).
    Furthemore, it can be proved that a symmetric, invertible matrix has a symmetric
    inverse (try it!) so that \(\bLambda_{21} = \bLambda_{12}^T\).
  prefs: []
  type: TYPE_NORMAL
- en: We seek to compute the marginals\(\idx{marginal}\xdi\) \(f_{\mathbf{X}_1}(\mathbf{x}_1)\)
    and \(f_{\mathbf{X}_2}(\mathbf{x}_2)\), as well as the conditional density of
    \(\mathbf{X}_1\) given \(\mathbf{X}_2\), which we denote as \(f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1|\mathbf{x}_2)\),
    and similarly \(f_{\mathbf{X}_2|\mathbf{X}_1}(\mathbf{x}_2|\mathbf{x}_1)\).
  prefs: []
  type: TYPE_NORMAL
- en: By the multiplication rule, the joint density \(f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2)\)
    can be decomposed as
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2) = f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1|\mathbf{x}_2)
    f_{\mathbf{X}_2}(\mathbf{x}_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: We use the *Inverting a Block Matrix* lemma to rewrite \(f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2)\)
    in this form and “reveal” the marginal and conditional\(\idx{conditional}\xdi\)
    involved. Indeed, once the joint density is in this form, by integrating over
    \(\mathbf{x}_1\) we obtain that the marginal density of \(\mathbf{X}_2\) is \(f_{\mathbf{X}_2}\)
    and the conditional density of \(\mathbf{X}_1\) given \(\mathbf{X}_2\) is obtained
    by taking the ratio of the joint and the marginal.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it will be easier to work with an expression derived in the proof of
    that lemma, specifically
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix} \bSigma_{11} & \bSigma_{12}\\ \bSigma_{21}
    & \bSigma_{22} \end{pmatrix}^{-1}\\ &= \begin{pmatrix} I_{d_1 \times d_1} & \mathbf{0}\\
    - \bSigma_{22}^{-1} \bSigma_{12}^T & I_{d_2 \times d_2} \end{pmatrix} \begin{pmatrix}
    (\bSigma/\bSigma_{22})^{-1} & \mathbf{0}\\ \mathbf{0} & \bSigma_{22}^{-1} \end{pmatrix}
    \begin{pmatrix} I_{d_1 \times d_1} & - \bSigma_{12} \bSigma_{22}^{-1}\\ \mathbf{0}
    & I_{d_2 \times d_2} \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We will use the fact that the first matrix on the last line is the transpose
    of the third one (check it!).
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the joint density, we need to expand the quadratic function \((\mathbf{x}
    - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\) appearing in the exponential. We
    break this up in a few steps.
  prefs: []
  type: TYPE_NORMAL
- en: 1- Note first that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} I_{d_1 \times d_1} & - \bSigma_{12} \bSigma_{22}^{-1}\\
    \mathbf{0} & I_{d_2 \times d_2} \end{pmatrix} \begin{pmatrix} \mathbf{x}_1 - \bmu_1\\
    \mathbf{x}_2 - \bmu_2 \end{pmatrix} = \begin{pmatrix} (\mathbf{x}_1 - \bmu_1)
    - \bSigma_{12} \bSigma_{22}^{-1} (\mathbf{x}_2 - \bmu_2) \\ \mathbf{x}_2 - \bmu_2
    \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly for its transpose. We define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu_{1|2}(\mathbf{x}_2) := \bmu_1 + \bSigma_{12} \bSigma_{22}^{-1} (\mathbf{x}_2
    - \bmu_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: 2- Plugging this back in the quadratic function, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\\ &=
    \begin{pmatrix} \mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2) \\ \mathbf{x}_2 - \bmu_2
    \end{pmatrix}^T \begin{pmatrix} (\bSigma/\bSigma_{22})^{-1} & \mathbf{0}\\ \mathbf{0}
    & \bSigma_{22}^{-1} \end{pmatrix} \begin{pmatrix} \mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2)
    \\ \mathbf{x}_2 - \bmu_2 \end{pmatrix}\\ &= (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))^T
    (\bSigma/\bSigma_{22})^{-1} (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2)) + (\mathbf{x}_2
    - \bmu_2)^T \bSigma_{22}^{-1} (\mathbf{x}_2 - \bmu_2). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that both terms have the same form as the original quadratic function.
  prefs: []
  type: TYPE_NORMAL
- en: 3- Going back to the density, we use \(\propto\) to indicate that the expression
    holds up to a constant not depending on \(\mathbf{x}\). Using that the exponential
    of a sum is the product of exponentials, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &f_{\mathbf{X}_1,\mathbf{X}_2}(\mathbf{x}_1,\mathbf{x}_2)\\
    &\propto \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x}
    - \bmu)\right)\\ &\propto \exp\left(-\frac{1}{2}(\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))^T
    (\bSigma/\bSigma_{22})^{-1} (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))\right)\\
    & \qquad \times \exp\left(-\frac{1}{2}(\mathbf{x}_2 - \bmu_2)^T \bSigma_{22}^{-1}
    (\mathbf{x}_2 - \bmu_2)\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 4- We have shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1|\mathbf{x}_2) \propto \exp\left(-\frac{1}{2}(\mathbf{x}_1
    - \bmu_{1|2}(\mathbf{x}_2))^T (\bSigma/\bSigma_{22})^{-1} (\mathbf{x}_1 - \bmu_{1|2}(\mathbf{x}_2))\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\mathbf{X}_2}(\mathbf{x}_2) \propto \exp\left(-\frac{1}{2}(\mathbf{x}_2
    - \bmu_2)^T \bSigma_{22}^{-1} (\mathbf{x}_2 - \bmu_2)\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the marginal density of \(\mathbf{X}_2\) is multivariate Gaussian
    with mean \(\bmu_2\) and covariance \(\bSigma_{22}\). The conditional density
    of \(\mathbf{X}_1\) given \(\mathbf{X}_2\) is multivariate Gaussian with mean
    \(\bmu_{1|2}(\mathbf{X}_2)\) and covariance \(\bSigma/\bSigma_{22} = \bSigma_{11}
    - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{12}^T\). We write this as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X}_1|\mathbf{X}_2 \sim N_{d_1}(\bmu_{1|2}(\mathbf{X}_2), \bSigma/\bSigma_{22})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X}_2 \sim N_{d_2}(\bmu_2, \bSigma_{22}). \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, by exchanging the roles of \(\mathbf{X}_1\) and \(\mathbf{X}_2\),
    we see that the marginal density of \(\mathbf{X}_1\) is multivariate Gaussian
    with mean \(\bmu_1\) and covariance \(\bSigma_{11}\). The conditional density
    of \(\mathbf{X}_2\) given \(\mathbf{X}_1\) is multivariate Gaussian with mean
    \(\bmu_{2|1}(\mathbf{X}_1) = \bmu_2 + \bSigma_{21} \bSigma_{11}^{-1} (\mathbf{X}_1
    - \bmu_1)\) and covariance \(\bSigma/\bSigma_{11} = \bSigma_{22} - \bSigma_{12}^T
    \bSigma_{11}^{-1} \bSigma_{12}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Suppose \((X_1, X_2)\) is a bivariate Gaussian with mean
    \((0,0)\), variances \(2\) and correlation coefficient \(-1/2\). Conditioned on
    \(X_2 = 1\), what is the mean of \(X_1\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(1/2\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(-1/2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(1\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(-1\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2\. Kalman filter[#](#kalman-filter "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider a stochastic process\(\idx{stochastic process}\xdi\) \(\{\bX_t\}_{t=0}^T\)
    (i.e., a collection of random vectors – often indexed by time) with state space
    \(\S = \mathbb{R}^{d_0}\) of the following form
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_{t+1} = F \,\bX_t + \bW_t \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bW_t\)s are i.i.d. \(N_{d_0}(\mathbf{0}, Q)\) and \(F\) and \(Q\)
    are known \(d_0 \times d_0\) matrices. We denote the initial state by \(\bX_0
    \sim N_{d_0}(\bmu_0, \bSigma_0)\). We assume that the process \(\{\bX_t\}_{t=1}^T\)
    is not observed, but rather that an auxiliary observed process \(\{\bY_t\}_{t=1}^T\)
    with state space \(\S = \mathbb{R}^{d}\) satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bY_t = H\,\bX_t + \bV_t \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bV_t\)s are i.i.d. \(N_d(\mathbf{0}, R)\) and \(H \in \mathbb{R}^{d
    \times d_0}\) and \(R \in \mathbb{R}^{d \times d}\) are known matrices. This is
    an example of a linear-Gaussian system\(\idx{linear-Gaussian system}\xdi\) (also
    known as linear-Gaussian state space model).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to infer the unobserved states given the observed process. Specifically,
    we look at the filtering problem. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Hidden_Markov_model#Filtering):'
  prefs: []
  type: TYPE_NORMAL
- en: The task is to compute, given the model’s parameters and a sequence of observations,
    the distribution over hidden states of the last latent variable at the end of
    the sequence, i.e. to compute P(x(t)|y(1),…,y(t)). This task is normally used
    when the sequence of latent variables is thought of as the underlying states that
    a process moves through at a sequence of points of time, with corresponding observations
    at each point in time. Then, it is natural to ask about the state of the process
    at the end.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Key lemma** Given the structure of the linear-Gaussian model, the following
    lemma will play a key role.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Linear-Gaussian System)** \(\idx{linear-Gaussian system lemma}\xdi\)
    Let \(\bW \sim N_{d}(\bmu, \bSigma)\) and \(\bW''|\bW \sim N_{d''}(A \,\bW, \bSigma'')\)
    where \(A \in \mathbb{R}^{d'' \times d}\) is a deterministic matrix, and \(\bSigma
    \in \mathbb{R}^{d \times d}\) and \(\bSigma'' \in \mathbb{R}^{d'' \times d''}\)
    are positive definite. Then, \((\bW, \bW'')\) is multivariate Gaussian with mean
    vector'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bmu'' = \begin{pmatrix} \bmu \\ A\bmu \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and positive definite covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bSigma'' = \begin{pmatrix} \bSigma & \bSigma A^T\\ A \bSigma
    & A \bSigma A^T + \bSigma' \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** What are the dimensions of each block of \(\bSigma''''\)?
    \(\checkmark\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We have'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &f_{\bW, \bW'}(\bw, \bw')\\ &= f_{\bW}(\bw) \, f_{\bW'|\bW}(\bw'|\bw)\\
    &\propto \exp\left(-\frac{1}{2}(\bw - \bmu)^T \bSigma^{-1} (\bw - \bmu)\right)\\
    & \qquad \times \exp\left(-\frac{1}{2}(\bw' - A \bw)^T (\bSigma')^{-1} (\bw' -
    A \bw)\right)\\ &\propto \exp\left(-\frac{1}{2}\left[(\bw - \bmu)^T \bSigma^{-1}
    (\bw - \bmu) + (\bw' - A \bw)^T (\bSigma')^{-1} (\bw' - A \bw)\right]\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We re-write the quadratic function in square brackets in the exponent as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\bw - \bmu)^T \bSigma^{-1} (\bw - \bmu) + (\bw' - A \bw)^T
    (\bSigma')^{-1} (\bw' - A \bw)\\ &=(\bw - \bmu)^T \bSigma^{-1} (\bw - \bmu)\\
    & \qquad + ([\bw' - A\bmu] - A[\bw-\bmu])^T (\bSigma')^{-1} ([\bw' - A\bmu] -
    A[\bw-\bmu])\\ &=(\bw - \bmu)^T (A^T (\bSigma')^{-1} A + \bSigma^{-1}) (\bw -
    \bmu)\\ & \qquad - 2 (\bw-\bmu)^T A^T (\bSigma')^{-1} (\bw' - A\bmu) + (\bw' -
    A\bmu)^T (\bSigma')^{-1} (\bw' - A\bmu)\\ &= \begin{pmatrix} \bw - \bmu \\ \bw'
    - A\bmu \end{pmatrix}^T \begin{pmatrix} A^T (\bSigma')^{-1} A + \bSigma^{-1} &
    - A^T (\bSigma')^{-1}\\ - (\bSigma')^{-1} A & (\bSigma')^{-1} \end{pmatrix} \begin{pmatrix}
    \bw - \bmu \\ \bw' - A\bmu \end{pmatrix}\\ &= \begin{pmatrix} \bw - \bmu \\ \bw'
    - A\bmu \end{pmatrix}^T \bLambda'' \begin{pmatrix} \bw - \bmu \\ \bw' - A\bmu
    \end{pmatrix}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the last line defines \(\bLambda''\) and the second line shows that it
    is positive definite (why?). We have shown that \((\bW, \bW')\) is multivariate
    Gaussian with mean \((\bmu, A\bmu)\).
  prefs: []
  type: TYPE_NORMAL
- en: We use the *Inverting a Block Matrix Lemma* to invert \(\bLambda''\) and reveal
    the covariance matrix. (One could also compute the covariance directly; try it!)
    We break up \(\bLambda''\) into blocks
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bLambda'' = \begin{pmatrix} \bLambda''_{11} & \bLambda''_{12}\\
    (\bLambda''_{12})^T & \bLambda''_{22} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bLambda''_{11} \in \mathbb{R}^{d \times d}\), \(\bLambda''_{22} \in
    \mathbb{R}^{d' \times d'}\), and \(\bLambda''_{12} \in \mathbb{R}^{d \times d'}\).
    Recall that the inverse is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^{-1} = \begin{pmatrix} (B/B_{22})^{-1} & -(B/B_{22})^{-1}
    B_{12} B_{22}^{-1}\\ -B_{22}^{-1} B_{12}^T (B/B_{22})^{-1} & B_{22}^{-1} B_{12}^T
    (B/B_{22})^{-1} B_{12} B_{22}^{-1} + B_{22}^{-1} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where here \(B = \bLambda''\).
  prefs: []
  type: TYPE_NORMAL
- en: The Schur complement is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bLambda'' / \bLambda''_{22} &= \bLambda''_{11} - \bLambda''_{12}
    (\bLambda''_{22})^{-1} (\bLambda''_{12})^T\\ &= A^T (\bSigma')^{-1} A + \bSigma^{-1}
    - (- A^T (\bSigma')^{-1}) ((\bSigma')^{-1})^{-1} (- A^T (\bSigma')^{-1})^T\\ &=
    A^T (\bSigma')^{-1} A + \bSigma^{-1} - A^T (\bSigma')^{-1} \bSigma' (\bSigma')^{-1}
    A\\ &= A^T (\bSigma')^{-1} A + \bSigma^{-1} - A^T (\bSigma')^{-1} A\\ &= \bSigma^{-1}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence \((\bLambda'' / \bLambda''_{22})^{-1} = \bSigma\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} - (\bLambda''/\bLambda''_{22})^{-1} \bLambda''_{12} (\bLambda''_{22})^{-1}
    &= - \bSigma (- A^T (\bSigma')^{-1}) ((\bSigma')^{-1})^{-1}\\ &= \bSigma A^T (\bSigma')^{-1}
    \bSigma'\\ &= \bSigma A^T \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &(\bLambda''_{22})^{-1} (\bLambda''_{12})^T (\bLambda''/\bLambda''_{22})^{-1}
    \bLambda''_{12} (\bLambda''_{22})^{-1} + (\bLambda''_{22})^{-1}\\ &= ((\bSigma')^{-1})^{-1}
    (- A^T (\bSigma')^{-1})^T \bSigma (-A^T (\bSigma')^{-1}) ((\bSigma')^{-1})^{-1}
    + ((\bSigma')^{-1})^{-1}\\ &= \bSigma' (\bSigma')^{-1} A \bSigma A^T (\bSigma')^{-1}
    \bSigma' + \bSigma'\\ &= A \bSigma A^T + \bSigma'. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Plugging back into the formula for the inverse of a block matrix completes the
    proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint distribution** We extend our previous notation as follows: for two
    disjoint, finite collections of random vectors \(\{\mathbf{U}_i\}_i\) and \(\{\mathbf{W}_j\}_j\)
    defined on the same probability space, we let \(f_{\{\mathbf{U}_i\}_i, \{\mathbf{W}_j\}_j}\)
    be their joint density, \(f_{\{\mathbf{U}_i\}_i | \{\mathbf{W}_j\}_j}\) be the
    conditional density of \(\{\mathbf{U}_i\}_i\) given \(\{\mathbf{W}_j\}_j\), and
    \(f_{\{\mathbf{U}_i\}_i}\) be the marginal density of \(\{\mathbf{U}_i\}_i\).
    Formally, our goal is to compute'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_{t}|\bY_{1:t}} := f_{\{\bX_{t}\}|\{\bY_1,\ldots,\bY_{t}\}} \]
  prefs: []
  type: TYPE_NORMAL
- en: recursively in \(t\), where we define \(\bY_{1:t} = \{\bY_1,\ldots,\bY_{t}\}\).
    We will see that all densities appearing in this calculation are multivariate
    Gaussians, and therefore keeping track of the means and covariances will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we posit that the density of the full process (with both observed
    and unobserved parts) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_0}(\bx_0) \prod_{t=1}^{T} f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})
    f_{\bY_{t}|\bX_{t}}(\by_{t}|\bx_{t}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the description of the model stipulates that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_{t}|\bX_{t-1} \sim N_{d_0}(F \,\bX_{t-1}, Q). \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bY_t|\bX_t \sim N_d(H\,\bX_t, R) \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(t \geq 1\) and \(\bX_0 \sim N_{d_0}(\bmu_0, \bSigma_0)\). We assume
    that \(\bmu_0\) and \(\bSigma_0\) are known. Applying the *Linear-Gaussian System
    Lemma* inductively shows that the full process \((\bX_{0:T}, \bY_{1:T})\) is jointly
    multivariate Gaussian (try it!). In particular, all marginals and conditionals
    are multivariate Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, it can be represented as follows, where as before each variable
    is a node and its conditional distribution depends only on its parent nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![HMM (with help from Claude)](../Images/dff6cc66efbd6ca33102a19a1e83e4b7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Conditional independence** The stipulated form of the joint density of the
    full process implies many conditional independence relations. We will need the
    following two:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- \(\bX_t\) is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: 2- \(\bY_t\) is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'We prove the first one and leave the second one as an exercise. In fact, we
    prove something stronger: \(\bX_t\) is conditionally independent of \(\bX_{0:t-2},
    \bY_{1:t-1}\) given \(\bX_{t-1}\).'
  prefs: []
  type: TYPE_NORMAL
- en: First, by integrating over \(\by_T\), then \(\bx_{T}\), then \(\by_{T-1}\),
    then \(\bx_{T-1}\), … , then \(\by_t\), we see that the joint density of \((\bX_{0:t},
    \bY_{1:t-1})\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the joint density of \((\bX_{0:t-1}, \bY_{1:t-1})\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: The conditional density given \(\bX_{t-1}\) is then obtained by dividing the
    first expression by the marginal density of \(\bX_{t-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\frac{f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})}{f_{\bX_{t-1}}(\bx_{t-1})}\\
    &= \frac{f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})}{f_{\bX_{t-1}}(\bx_{t-1})}\\
    &= \frac{f_{\bX_0}(\bx_0) \left(\prod_{s=1}^{t-1} f_{\bX_{s}|\bX_{s-1}}(\bx_{s}|\bx_{s-1})
    f_{\bY_{s}|\bX_{s}}(\by_{s}|\bx_{s})\right) }{f_{\bX_{t-1}}(\bx_{t-1})}f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})\\
    &= f_{\bX_{0:t-2}, \bY_{1:t-1}|\bX_{t-1}}(\bx_{0:t-2}, \by_{1:t-1}|\bx_{t-1})
    f_{\bX_{t}|\bX_{t-1}}(\bx_{t}|\bx_{t-1})\\ \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm** \(\idx{Kalman filter}\xdi\) We give a recursive algorithm for
    solving the filtering problem, that is, for computing the mean vector and covariance
    matrix of the conditional density \(f_{\bX_{t}|\bY_{1:t}}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Initial step:* The first compute \(f_{\bX_1|\bY_1}\). We do this through a
    series of observations which will generalize straightforwardly.'
  prefs: []
  type: TYPE_NORMAL
- en: 1- We have \(\bX_0 \sim N_{d_0}(\bmu_0, \bSigma_0)\) and \(\bX_{1}|\bX_{0} \sim
    N_{d_0}(F \,\bX_0, Q)\). So, by the *Linear-Gaussian System Lemma*, the joint
    vector \((\bX_0, \bX_{1})\) is multivariate Gaussian with mean vector \((\bmu_0,
    F \bmu_0)\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \bSigma_0 & \bSigma_0 F^T\\ F \bSigma_0 & F
    \bSigma_0 F^T + Q \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the marginal density of \(\bX_{1}\) is multivariate Gaussian with mean
    vector \(F \bmu_0\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_0 := F \bSigma_0 F^T + Q. \]
  prefs: []
  type: TYPE_NORMAL
- en: 2- Combining the previous observation about the marginal density of \(\bX_{1}\)
    with the fact that \(\bY_1|\bX_1 \sim N_d(H\,\bX_1, R)\), the *Linear-Gaussian
    System Lemma* says that \((\bX_1, \bY_{1})\) is multivariate Gaussian with mean
    vector \((F \bmu_0, H F \bmu_0)\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} P_0 & P_0 H^T\\ H P_0 & H P_0 H^T + R \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, define \(K_1 := P_0 H^T (H P_0 H^T + R)^{-1}\). This new observation
    and the conditional density formula give that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_1|\bY_1 \sim N_d(\bmu_1, \bSigma_1) \]
  prefs: []
  type: TYPE_NORMAL
- en: where we define
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu_1 &:= F \bmu_0 + P_0 H^T (H P_0 H^T + R)^{-1} (\mathbf{Y}_1
    - H F \bmu_0)\\ &= F \bmu_0 + K_1 (\mathbf{Y}_1 - H F \bmu_0) \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bSigma_1 &:= P_0 - P_0 H^T (H P_0 H^T + R)^{-1} H P_0\\ &=
    (I_{d_0 \times d_0} - K_1 H) P_0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*General step:* Assuming by induction that \(\bX_{t-1}|\bY_{1:t-1} \sim N_{d_0}(\bmu_{t-1},
    \bSigma_{t-1})\) where \(\bmu_{t-1}\) depends implicitly on \(\bY_{1:t-1}\) (but
    \(\bSigma_{t-1}\) does not), we deduce the next step. It mimics closely the initial
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: '1- Predict: We first “predict” \(\bX_{t}\) given \(\bY_{1:t-1}\). We use the
    fact that \(\bX_{t-1}|\bY_{1:t-1} \sim N_{d_0}(\bmu_{t-1}, \bSigma_{t-1})\). Moreover,
    we have that \(\bX_{t}|\bX_{t-1} \sim N_{d_0}(F \,\bX_{t-1}, Q)\) and that \(\bX_t\)
    is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t-1}\). So, \(\bX_{t}|\{\bX_{t-1},
    \bY_{1:t-1}\} \sim N_{d_0}(F \,\bX_{t-1}, Q)\) by the *Role of Independence Lemma*.
    By the *Linear-Gaussian System Lemma*, the joint vector \((\bX_{t-1}, \bX_{t})\)
    conditioned on \(\bY_{1:t-1}\) is multivariate Gaussian with mean vector \((\bmu_{t-1},
    F \bmu_{t-1})\) and covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \bSigma_{t-1} & \bSigma_{t-1} F^T\\ F \bSigma_{t-1}
    & F \bSigma_{t-1} F^T + Q \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, the conditional marginal density of \(\bX_{t}\) given \(\bY_{1:t-1}\)
    is multivariate Gaussian with mean vector \(F \bmu_{t-1}\) and covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{t-1} := F \bSigma_{t-1} F^T + Q. \]
  prefs: []
  type: TYPE_NORMAL
- en: '2- Update: Next we “update” our prediction of \(\bX_{t}\) using the new observation
    \(\bY_{t}\). We have that \(\bY_{t}|\bX_{t} \sim N_d(H\,\bX_{t}, R)\) and that
    \(\bY_t\) is conditionally independent of \(\bY_{1:t-1}\) given \(\bX_{t}\). So
    \(\bY_{t}|\{\bX_{t}, \bY_{1:t-1}\} \sim N_d(H\,\bX_{t}, R)\) by the *Role of Independence*
    lemma. Combining this with the previous observation, the *Linear-Gaussian System*
    lemma says that \((\bX_{t}, \bY_{t})\) given \(\bY_{1:t-1}\) is multivariate Gaussian
    with mean vector \((F \bmu_{t-1}, H F \bmu_{t-1})\) and covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} P_{t-1} & P_{t-1} H^T\\ H P_{t-1} & H P_{t-1}
    H^T + R \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, define \(K_{t} := P_{t-1} H^T (H P_{t-1} H^T + R)^{-1}\). This new
    observation and the conditional density formula give that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX_{t}|\{\bY_t, \bY_{1:t-1}\} = \bX_{t}|\bY_{1:t} \sim N_d(\bmu_{t}, \bSigma_{t})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where we define
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu_{t} &:= F \bmu_{t-1} + K_{t} (\mathbf{Y}_{t} - H F \bmu_{t-1})
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bSigma_{t} &= (I_{d_0 \times d_0} - K_{t} H) P_{t-1}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Summary:* Let \(\bmu_t\) and \(\bSigma_t\) be the mean and covariance matrix
    of \(\bX_t\) conditioned on \(\bY_{1:t}\). The recursions for these quantities
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu_t &= F\,\bmu_{t-1} + K_{t} (\bY_{t} - H F \bmu_{t-1})\\
    \bSigma_t &= (I_{d_0 \times d_0} - K_t H) P_{t-1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P_{t-1} &= F \,\bSigma_{t-1} F^T + Q\\ K_t &= P_{t-1} H^T (H
    P_{t-1} H^T + R)^{-1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: This last matrix is known as the Kalman gain matrix. The vector \(\bY_{t} -
    H F \bmu_{t-1}\) is referred to as innovation; it compares the new observation
    \(\bY_{t}\) to its predicted expectation \(H F \bmu_{t-1}\) based on the previous
    observations. Hence, in some sense, the Kalman gain matrix\(\idx{Kalman gain matrix}\xdi\)
    represents the “weight” given to the observation at time \(t\) when updating the
    state estimate \(\bmu_t\). The solution above is known as Kalman filtering.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Explore the concept of sequential Monte Carlo methods, also
    known as particle filters, as an alternative to the Kalman filter. Ask your favorite
    AI chatbot for an explanation and implementation of a particle filter. Try it
    on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3\. Back to location tracking[#](#back-to-location-tracking "Link to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We apply Kalman filtering to location tracking. Returning to our cyborg corgi
    example, we imagine that we get noisy observations about its successive positions
    in a park. (Think of GPS measurements.) We seek to get a better estimate of its
    location using the method above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Cyborg corgi (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cyborg corgi](../Images/348998a4b7d1b53beeec4c9913f351c0.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: We model the true location as a linear-Gaussian system over the 2d position
    \((z_{1,t}, z_{2,t})_t\) and velocity \((\dot{z}_{1,t}, \dot{z}_{2,t})_t\) sampled
    at \(\Delta\) intervals of time. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bX_t = (z_{1,t}, z_{2,t}, \dot{z}_{1,t}, \dot{z}_{2,t}), \quad
    F = \begin{pmatrix} 1 & 0 & \Delta & 0\\ 0 & 1 & 0 & \Delta\\ 0 & 0 & 1 & 0\\
    0 & 0 & 0 & 1 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so the unobserved dynamics are
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} z_{1,t+1}\\ z_{2,t+1}\\ \dot{z}_{1,t+1}\\ \dot{z}_{2,t+1}
    \end{pmatrix} = \bX_{t+1} = F \,\bX_t + \bW_t = \begin{pmatrix} z_{1,t} + \Delta
    \dot{z}_{1,t} + W_{1,t}\\ z_{2,t} + \Delta \dot{z}_{2,t} + W_{2,t}\\ \dot{z}_{1,t}
    + \dot{W}_{1,t}\\ \dot{z}_{2,t} + \dot{W}_{2,t} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bW_t = (W_{1,t}, W_{2,t}, \dot{W}_{1,t}, \dot{W}_{2,t}) \sim N_{d_0}(\mathbf{0},
    Q)\) with \(Q\) known.
  prefs: []
  type: TYPE_NORMAL
- en: In words, the velocity is unchanged, up to Gaussian perturbation. The position
    changes proportionally to the velocity in the corresponding dimension, again up
    to Gaussian perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: The observations \((\tilde{z}_{1,t}, \tilde{z}_{2,t})_t\) are modeled as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bY_t = (\tilde{z}_{1,t}, \tilde{z}_{2,t}), \quad H = \begin{pmatrix}
    1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so the observed process satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \tilde{z}_{1,t}\\ \tilde{z}_{2,t} \end{pmatrix}
    = \bY_t = H\,\bX_t + \bV_t = \begin{pmatrix} z_{1,t} + V_{1,t}\\ z_{2,t} + V_{2,t}
    \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\bV_t = (V_{1,t}, V_{2,t}) \sim N_d(\mathbf{0}, R)\) with \(R\)
    known.
  prefs: []
  type: TYPE_NORMAL
- en: In words, we only observe the positions, up to Gaussian noise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing the Kalman filter** We implement the Kalman filter as described
    above with known covariance matrices. We take \(\Delta = 1\) for simplicity. The
    code is adapted from [[Mur0](https://github.com/probml/pmtk3/blob/master/demos/kalmanTrackingDemo.m)].'
  prefs: []
  type: TYPE_NORMAL
- en: We will test Kalman filtering on a simulated path drawn from the linear-Gaussian
    model above. The following function creates such a path and its noisy observations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Here is an example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the next plot (and throughout this section), the dots are the noisy observations.
    The unobserved true path is also shown as a dotted line.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]</details> ![../../_images/7887a8a25f3bbd2d4c1cc1b809a24f2aa031c84a10f4cd5ed5f0ae71eff0253c.png](../Images/c8b3c6c67639d980e6620091452f12a9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The following function implements the Kalman filter. The full recursion is broken
    up into several steps. We use [`numpy.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)
    to compute the Kalman gain matrix. Below, `mu_pred` is \(F \bmu_{t-1}\) and `Sig_pred`
    is \(P_{t-1} = F \bSigma_{t-1} F^T + Q\), which are the mean vector and covariance
    matrix of \(\bX_{t}\) given \(\bY_{1:t-1}\) as computed in the *Predict* step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We apply this to the location tracking example. The inferred
    states, or more precisely their estimated mean, are in blue. Note that we also
    inferred the velocity at each time point, but we are not plotting that information.'
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]</details> ![../../_images/046beb4acdf4c61da2a5de4c6dbb7d32ca2d7b4e8416651030b3f49747fcac9a.png](../Images/959069dbefd76cfab2d54229507451d1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: To quantify the improvement in the inferred means compared to the observations,
    we compute the mean squared error in both cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We indeed observe a substantial reduction.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The Kalman filter assumes that the parameters of the state
    evolution and observation models are known. Ask your favorite AI chatbot about
    methods for estimating these parameters from data, such as the expectation-maximization
    algorithm or the variational Bayes approach. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is the Schur complement of the block \(B_{11}\)
    in the positive definite matrix \(B = \begin{pmatrix} B_{11} & B_{12} \\ B_{12}^T
    & B_{22} \end{pmatrix}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(B_{22} - B_{12}^T B_{11}^{-1} B_{12}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(B_{11} - B_{12} B_{22}^{-1} B_{12}^T\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(B_{22}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(B_{11}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is true about the Schur complement \(B/B_{11}\)
    of the block \(B_{11}\) in a positive definite matrix \(B\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is always symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is always positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: c) Both a and b.
  prefs: []
  type: TYPE_NORMAL
- en: d) Neither a nor b.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** What is the conditional distribution of \(\mathbf{X}_1\) given \(\mathbf{X}_2\)
    in a multivariate Gaussian distribution?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 + \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} - \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 - \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} + \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 + \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} + \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{X}_1 | \mathbf{X}_2 \sim N_{d_1}(\boldsymbol{\mu}_1 - \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2
    - \boldsymbol{\mu}_2), \bSigma_{11} - \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In a linear-Gaussian system, which of the following is true about the
    conditional independence relationships?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{X}_t\) is conditionally independent of \(\mathbf{Y}_{1:t-1}\) given
    \(\mathbf{X}_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{Y}_t\) is conditionally independent of \(\mathbf{Y}_{1:t-1}\) given
    \(\mathbf{X}_t\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{X}_t\) is conditionally independent of \(\mathbf{X}_{t-2}\) given
    \(\mathbf{X}_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the above.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In the Kalman filter, what does the Kalman gain matrix \(K_t\) represent?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The covariance matrix of the state estimate at time \(t\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The covariance matrix of the observation at time \(t\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The weight given to the observation at time \(t\) when updating the state
    estimate.
  prefs: []
  type: TYPE_NORMAL
- en: d) The weight given to the previous state estimate when predicting the current
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: a. Justification: The text defines the Schur complement of \(B_{11}\)
    as \(B / B_{11} := B_{22} - B_{12}^T B_{11}^{-1} B_{12}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: The text states “the Schur complement of the
    block \(B_{11}\), i.e., the matrix \(B/B_{11} := B_{22} - B_{12}^T B_{11}^{-1}
    B_{12}\), is symmetric and positive definite.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The conditional distribution of \(\mathbf{X}_1\)
    given \(\mathbf{X}_2\) is derived in the text as \(\mathbf{X}_1 | \mathbf{X}_2
    \sim N_{d_1}(\boldsymbol{\mu}_1 + \bSigma_{12}\bSigma_{22}^{-1}(\mathbf{X}_2 -
    \boldsymbol{\mu}_2), \bSigma_{11} - \bSigma_{12}\bSigma_{22}^{-1}\bSigma_{12}^T)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text explicitly states these conditional
    independence relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: The text defines \(K_t := P_{t-1} H^T (H P_{t-1}
    H^T + R)^{-1}\) as the Kalman gain matrix, which is used to update the state estimate
    as \(\boldsymbol{\mu}_t := F \boldsymbol{\mu}_{t-1} + K_t (\mathbf{Y}_t - H F
    \boldsymbol{\mu}_{t-1})\), where \(K_t\) weighs the innovation \((\mathbf{Y}_t
    - H F \boldsymbol{\mu}_{t-1})\).'
  prefs: []
  type: TYPE_NORMAL
