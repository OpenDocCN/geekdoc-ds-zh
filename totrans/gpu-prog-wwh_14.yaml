- en: Preparing code for GPU porting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://enccs.github.io/gpu-programming/11-gpu-porting/](https://enccs.github.io/gpu-programming/11-gpu-porting/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*[GPU programming: why, when and how?](../)* **   Preparing code for GPU porting'
  prefs: []
  type: TYPE_NORMAL
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/11-gpu-porting.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs: []
  type: TYPE_NORMAL
- en: What are the key steps involved in porting code to take advantage of GPU parallel
    processing capability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I identify the computationally intensive parts of my code that can benefit
    from GPU acceleration?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the considerations for refactoring loops to suit the GPU architecture
    and improve memory access patterns?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any tools that can translate automatically between different frameworks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiarized the steps involved in porting code to GPUs to take advantage
    of parallel processing capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Giving some idea about refactoring loops and modifying operations to suit the
    GPU architecture and improve memory access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to use automatic translation tools to port from CUDA to HIP and from OpenACC
    to OpenMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 30 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting from CPU to GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When porting code to take advantage of the parallel processing capability of
    GPUs, several steps need to be followed and some additional work is required before
    writing actual parallel code to be executed on the GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify Targeted Parts**: Begin by identifying the parts of the code that
    contribute significantly to the execution time. These are often computationally
    intensive sections such as loops or matrix operations. The Pareto principle suggests
    that roughly 10-20% of the code accounts for 80-90% of the execution time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equivalent GPU Libraries**: If the original code uses CPU libraries like
    BLAS, FFT, etc, it’s crucial to identify the equivalent GPU libraries. For example,
    cuBLAS or hipBLAS can replace CPU-based BLAS libraries. Utilizing GPU-specific
    libraries ensures efficient GPU utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Refactor Loops**: When porting loops directly to GPUs, some refactoring is
    necessary to suit the GPU architecture. This typically involves splitting the
    loop into multiple steps or modifying operations to exploit the independence between
    iterations and improve memory access patterns. Each step of the original loop
    can be mapped to a kernel, executed by multiple GPU threads, with each thread
    corresponding to an iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Access Optimization**: Consider the memory access patterns in the
    code. GPUs perform best when memory access is coalesced and aligned. Minimizing
    global memory accesses and maximizing utilization of shared memory or registers
    can significantly enhance performance. Review the code to ensure optimal memory
    access for GPU execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How would this be ported? (n_soap ≈ 100, n_sites ⩾ 10000, k_max ≈ 20*n_sites
    )
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Inspect the following Fortran code (if you don’t read Fortran: do-loops ==
    for-loops)'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some steps at first glance:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: the code could (has to) be splitted in 3-4 kernels. Why?
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: check if there are any variables that could lead to false dependencies between
    iterations, like the index k2
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it efficient for GPUs to split the work over the index i? What about the
    memory access? Note the arrays are 2D in Fortran
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it possible to collapse some loops? Combining nested loops can reduce overhead
    and improve memory access patterns, leading to better GPU performance.
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: what is the best memory access in a GPU? Review memory access patterns in the
    code. Minimize global memory access by utilizing shared memory or registers where
    appropriate. Ensure memory access is coalesced and aligned, maximizing GPU memory
    throughput
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactored code!
  prefs: []
  type: TYPE_NORMAL
- en: Registers are limited and the larger the kernel use more registers registers
    resulting in less active threads (small occupancy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_rad_der(is,k2) the CUDA thread needs access to all
    the previous values soap_rad_der(1:nsoap,k2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_cart_der(1, 1:n_soap, k3) it is required to have access
    to all values (k3+1:k2+n_neigh(i)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note the indices in the first part. The matrices are transposed for better access
    patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Identify equivalent GPU libraries for CPU-based libraries and utilizing them
    to ensure efficient GPU utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of identifying the computationally intensive parts of the code that
    contribute significantly to the execution time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need to refactor loops to suit the GPU architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significance of memory access optimization for efficient GPU execution, including
    coalesced and aligned memory access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting between different GPU frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might also find yourself in a situation where you need to port a code from
    one particular GPU framework to another. This section gives an overview of different
    tools that enable converting CUDA and OpenACC codes to HIP and OpenMP, respectively.
    This conversion process enables an application to target various GPU architectures,
    specifically, NVIDIA and AMD GPUs. Here we focus on [hipify](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html)
    and [clacc](https://csmd.ornl.gov/project/clacc) tools. This guide is adapted
    from the [NRIS documentation](https://documentation.sigma2.no/code_development/guides/cuda_translating-tools.html).
  prefs: []
  type: TYPE_NORMAL
- en: Translating CUDA to HIP with Hipify
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we cover the use of `hipify-perl` and `hipify-clang` tools
    to translate a CUDA code to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-perl
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `hipify-perl` tool is a script based on perl that translates CUDA syntax
    into HIP syntax (see .e.g. [here](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl)
    for more details). For instance, in a CUDA code that incorporates the CUDA functions
    `` cudaMalloc` `` and `cudaDeviceSynchronize`, the tool will substitute `cudaMalloc`
    with the HIP function `hipMalloc`. Similarly the CUDA function `cudaDeviceSynchronize`
    will be substituted with the HIP function `hipDeviceSynchronize`. We list below
    the basic steps to run `hipify-perl` on LUMI-G.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Generating `hipify-perl` script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Running the generated `hipify-perl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 3**: Compiling with `hipcc` the generated HIP code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Despite the simplicity of the use of `hipify-perl`, the tool might not be suitable
    for large applications, as it relies heavily on substituting CUDA strings with
    HIP strings (e.g. it substitutes `*cuda*` with `*hip*`). In addition, `hipify-perl`
    lacks the ability of [distinguishing device/host function calls](https://docs.amd.com/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl).
    The alternative here is to use the `hipify-clang` tool as will be described in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-clang
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As described in the [HIPIFY documentation](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl),
    the `hipify-clang` tool is based on clang for translating CUDA sources into HIP
    sources. The tool is more robust for translating CUDA codes compared to the `hipify-perl`
    tool. Furthermore, it facilitates the analysis of the code by providing assistance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, `hipify-clang` requires `LLVM+CLANG` and `CUDA`. Details about building
    `hipify-clang` can be found [here](https://github.com/ROCm/HIPIFY). Note that
    `hipify-clang` is available on LUMI-G. The issue however might be related to the
    installation of CUDA-toolkit. To avoid any eventual issues with the installation
    procedure we opt for CUDA singularity container. Here we present a step-by-step
    guide for running `hipify-clang`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Pulling a CUDA singularity container e.g.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Loading a rocm module and launching the CUDA singularity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: where the current directory `$PWD` in the host is mounted to that of the container,
    and the directory `/opt` in the host is mounted to the that inside the container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 3**: Setting the environment variable `$PATH`. In order to run `hipify-clang`
    from inside the container, one can set the environment variable `$PATH` that defines
    the path to look for the binary `hipify-clang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the rocm version we used is `rocm-6.0.3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4**: Running `hipify-clang` from inside the singularity container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the cuda path and the path to the `*includes*` and `*defines*` files should
    be specified. The CUDA source code and the generated output code are program.cu
    and hip_program.cu.hip, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The syntax for the compilation process of the generated hip code is similar
    to the one described in the previous section (see the **Step 3** in the hipify-perl
    section).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Code examples for the `Hipify` exercises can be accessed in the content/examples/exercise_hipify
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Exercise I : Translate an CUDA code to HIP with `hipify-perl`'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Generate the `hipify-perl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_perl`
    with the `Hipify-perl` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II : Translate an CUDA code to HIP with `hipify-clang`'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_clang`
    with the `Hipify-clang` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Translating OpenACC to OpenMP with Clacc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main) is a
    tool to translate an OpenACC application to OpenMP offloading with the Clang/LLVM
    compiler environment. Note that the tool is specific to OpenACC C, while OpenACC
    Fortran is already supported on AMD GPU. As indicated in the [GitHub repository](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main)
    the compiler `Clacc` is the `Clang`’s executable in the subdirectory `\bin` of
    the `\install` directory as described below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following we present a step-by-step guide for building and using Clacc:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Building and installing [Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Setting up environment variables to be able to work from the `/install`
    directory, which is the simplest way. We assume that the `/install` directory
    is located in the path `/project/project_xxxxxx/Clacc/llvm-project`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more advanced usage, which includes for instance modifying `Clacc`, we refer
    readers to [“Usage from Build directory”](https://github.com/llvm-doe-org/llvm-project/blob/clacc/main/README.md)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '**Step 3**: Source to source conversion of the openACC_code.c code to be printed
    out to the file openMP_code.c:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the flag `-fopenacc-structured-ref-count-omp=no-ompx-hold` is introduced
    to disable the `ompx_hold` map type modifier, which is used by the OpenACC `copy`
    clause translation. The `ompx_hold` is an OpenMP extension that might not be supported
    yet by other compilers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4** Compiling the code with the [cc compiler wrapper](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Access exercise material
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for the `Clacc` exercise can be accessed in the content/examples/exercise_clacc
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise : Translate an OpenACC code to OpenMP'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the OpenACC code `openACC_code.c` located in `/exercise_clacc` with
    the `Clacc` compiler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the generated OpenMP code with the `cc` compiler wrapper and run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translating CUDA to SYCL/DPC++ with SYCLomatic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel offers a tool for CUDA-to-SYCL code migration, included in the Intel oneAPI
    Basekit.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not installed on LUMI, but the general workflow is similar to the HIPify
    Clang and also requires an existing CUDA installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: SYCLomatic can migrate larger projects by using `-in-root` and `-out-root` flags
    to process directories recursively. It can also use compilation database (supported
    by CMake and other build systems) to deal with more complex project layouts.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the code generated by SYCLomatic relies on oneAPI-specific
    extensions, and thus cannot be directly used with other SYCL implementations,
    such as AdaptiveCpp (hipSYCL). The `--no-incremental-migration` flag can be added
    to `dpct` command to minimize, but not completely avoid, the use of this compatibility
    layer. That would require manual effort, since some CUDA concepts cannot be directly
    mapped to SYCL.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, CUDA applications might assume certain hardware behavior, such
    as 32-wide warps. If the target hardware is different (e.g., AMD MI250 GPUs, used
    in LUMI, have warp size of 64), the algorithms might need to be adjusted manually.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This concludes a brief overview of the usage of available tools to convert CUDA
    codes to HIP and SYCL, and OpenACC codes to OpenMP offloading. In general the
    translation process for large applications might be incomplete and thus requires
    manual modification to complete the porting process. It is however worth noting
    that the accuracy of the translation process requires that applications are written
    correctly according to the CUDA and OpenACC syntaxes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hipify GitHub](https://github.com/ROCm/HIPIFY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HIPify Reference Guide v5.1](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HIP example](https://github.com/olcf-tutorials/simple_HIP_examples/tree/master/vector_addition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Porting CUDA to HIP](https://www.admin-magazine.com/HPC/Articles/Porting-CUDA-to-HIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clacc Main repository README](https://github.com/llvm-doe-org/llvm-project/blob/clacc/main/README.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SYCLomatic main mage](https://www.intel.com/content/www/us/en/developer/articles/technical/syclomatic-new-cuda-to-sycl-code-migration-tool.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SYCLomatic documentation](https://oneapi-src.github.io/SYCLomatic/get_started/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Useful tools exist to automatically translate tools from CUDA to HIP and SYCL
    and from OpenACC to OpenMP, but they may require manual modifications. [Previous](../10-multiple_gpu/
    "Multiple GPU programming with MPI") [Next](../12-recommendations/ "Recommendations")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2023-2024, The contributors.
  prefs: []
  type: TYPE_NORMAL
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  prefs: []
  type: TYPE_NORMAL
- en: What are the key steps involved in porting code to take advantage of GPU parallel
    processing capability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I identify the computationally intensive parts of my code that can benefit
    from GPU acceleration?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the considerations for refactoring loops to suit the GPU architecture
    and improve memory access patterns?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any tools that can translate automatically between different frameworks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiarized the steps involved in porting code to GPUs to take advantage
    of parallel processing capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Giving some idea about refactoring loops and modifying operations to suit the
    GPU architecture and improve memory access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to use automatic translation tools to port from CUDA to HIP and from OpenACC
    to OpenMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 30 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting from CPU to GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When porting code to take advantage of the parallel processing capability of
    GPUs, several steps need to be followed and some additional work is required before
    writing actual parallel code to be executed on the GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify Targeted Parts**: Begin by identifying the parts of the code that
    contribute significantly to the execution time. These are often computationally
    intensive sections such as loops or matrix operations. The Pareto principle suggests
    that roughly 10-20% of the code accounts for 80-90% of the execution time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equivalent GPU Libraries**: If the original code uses CPU libraries like
    BLAS, FFT, etc, it’s crucial to identify the equivalent GPU libraries. For example,
    cuBLAS or hipBLAS can replace CPU-based BLAS libraries. Utilizing GPU-specific
    libraries ensures efficient GPU utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Refactor Loops**: When porting loops directly to GPUs, some refactoring is
    necessary to suit the GPU architecture. This typically involves splitting the
    loop into multiple steps or modifying operations to exploit the independence between
    iterations and improve memory access patterns. Each step of the original loop
    can be mapped to a kernel, executed by multiple GPU threads, with each thread
    corresponding to an iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Access Optimization**: Consider the memory access patterns in the
    code. GPUs perform best when memory access is coalesced and aligned. Minimizing
    global memory accesses and maximizing utilization of shared memory or registers
    can significantly enhance performance. Review the code to ensure optimal memory
    access for GPU execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How would this be ported? (n_soap ≈ 100, n_sites ⩾ 10000, k_max ≈ 20*n_sites
    )
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Inspect the following Fortran code (if you don’t read Fortran: do-loops ==
    for-loops)'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some steps at first glance:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: the code could (has to) be splitted in 3-4 kernels. Why?
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: check if there are any variables that could lead to false dependencies between
    iterations, like the index k2
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it efficient for GPUs to split the work over the index i? What about the
    memory access? Note the arrays are 2D in Fortran
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it possible to collapse some loops? Combining nested loops can reduce overhead
    and improve memory access patterns, leading to better GPU performance.
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: what is the best memory access in a GPU? Review memory access patterns in the
    code. Minimize global memory access by utilizing shared memory or registers where
    appropriate. Ensure memory access is coalesced and aligned, maximizing GPU memory
    throughput
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactored code!
  prefs: []
  type: TYPE_NORMAL
- en: Registers are limited and the larger the kernel use more registers registers
    resulting in less active threads (small occupancy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_rad_der(is,k2) the CUDA thread needs access to all
    the previous values soap_rad_der(1:nsoap,k2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_cart_der(1, 1:n_soap, k3) it is required to have access
    to all values (k3+1:k2+n_neigh(i)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note the indices in the first part. The matrices are transposed for better access
    patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Identify equivalent GPU libraries for CPU-based libraries and utilizing them
    to ensure efficient GPU utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of identifying the computationally intensive parts of the code that
    contribute significantly to the execution time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need to refactor loops to suit the GPU architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significance of memory access optimization for efficient GPU execution, including
    coalesced and aligned memory access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting between different GPU frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might also find yourself in a situation where you need to port a code from
    one particular GPU framework to another. This section gives an overview of different
    tools that enable converting CUDA and OpenACC codes to HIP and OpenMP, respectively.
    This conversion process enables an application to target various GPU architectures,
    specifically, NVIDIA and AMD GPUs. Here we focus on [hipify](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html)
    and [clacc](https://csmd.ornl.gov/project/clacc) tools. This guide is adapted
    from the [NRIS documentation](https://documentation.sigma2.no/code_development/guides/cuda_translating-tools.html).
  prefs: []
  type: TYPE_NORMAL
- en: Translating CUDA to HIP with Hipify
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we cover the use of `hipify-perl` and `hipify-clang` tools
    to translate a CUDA code to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-perl
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `hipify-perl` tool is a script based on perl that translates CUDA syntax
    into HIP syntax (see .e.g. [here](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl)
    for more details). For instance, in a CUDA code that incorporates the CUDA functions
    `` cudaMalloc` `` and `cudaDeviceSynchronize`, the tool will substitute `cudaMalloc`
    with the HIP function `hipMalloc`. Similarly the CUDA function `cudaDeviceSynchronize`
    will be substituted with the HIP function `hipDeviceSynchronize`. We list below
    the basic steps to run `hipify-perl` on LUMI-G.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Generating `hipify-perl` script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Running the generated `hipify-perl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 3**: Compiling with `hipcc` the generated HIP code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Despite the simplicity of the use of `hipify-perl`, the tool might not be suitable
    for large applications, as it relies heavily on substituting CUDA strings with
    HIP strings (e.g. it substitutes `*cuda*` with `*hip*`). In addition, `hipify-perl`
    lacks the ability of [distinguishing device/host function calls](https://docs.amd.com/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl).
    The alternative here is to use the `hipify-clang` tool as will be described in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-clang
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As described in the [HIPIFY documentation](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl),
    the `hipify-clang` tool is based on clang for translating CUDA sources into HIP
    sources. The tool is more robust for translating CUDA codes compared to the `hipify-perl`
    tool. Furthermore, it facilitates the analysis of the code by providing assistance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, `hipify-clang` requires `LLVM+CLANG` and `CUDA`. Details about building
    `hipify-clang` can be found [here](https://github.com/ROCm/HIPIFY). Note that
    `hipify-clang` is available on LUMI-G. The issue however might be related to the
    installation of CUDA-toolkit. To avoid any eventual issues with the installation
    procedure we opt for CUDA singularity container. Here we present a step-by-step
    guide for running `hipify-clang`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Pulling a CUDA singularity container e.g.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Loading a rocm module and launching the CUDA singularity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: where the current directory `$PWD` in the host is mounted to that of the container,
    and the directory `/opt` in the host is mounted to the that inside the container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 3**: Setting the environment variable `$PATH`. In order to run `hipify-clang`
    from inside the container, one can set the environment variable `$PATH` that defines
    the path to look for the binary `hipify-clang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the rocm version we used is `rocm-6.0.3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4**: Running `hipify-clang` from inside the singularity container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the cuda path and the path to the `*includes*` and `*defines*` files should
    be specified. The CUDA source code and the generated output code are program.cu
    and hip_program.cu.hip, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The syntax for the compilation process of the generated hip code is similar
    to the one described in the previous section (see the **Step 3** in the hipify-perl
    section).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Code examples for the `Hipify` exercises can be accessed in the content/examples/exercise_hipify
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Exercise I : Translate an CUDA code to HIP with `hipify-perl`'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Generate the `hipify-perl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_perl`
    with the `Hipify-perl` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II : Translate an CUDA code to HIP with `hipify-clang`'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_clang`
    with the `Hipify-clang` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Translating OpenACC to OpenMP with Clacc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main) is a
    tool to translate an OpenACC application to OpenMP offloading with the Clang/LLVM
    compiler environment. Note that the tool is specific to OpenACC C, while OpenACC
    Fortran is already supported on AMD GPU. As indicated in the [GitHub repository](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main)
    the compiler `Clacc` is the `Clang`’s executable in the subdirectory `\bin` of
    the `\install` directory as described below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following we present a step-by-step guide for building and using Clacc:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Building and installing [Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Setting up environment variables to be able to work from the `/install`
    directory, which is the simplest way. We assume that the `/install` directory
    is located in the path `/project/project_xxxxxx/Clacc/llvm-project`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more advanced usage, which includes for instance modifying `Clacc`, we refer
    readers to [“Usage from Build directory”](https://github.com/llvm-doe-org/llvm-project/blob/clacc/main/README.md)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '**Step 3**: Source to source conversion of the openACC_code.c code to be printed
    out to the file openMP_code.c:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the flag `-fopenacc-structured-ref-count-omp=no-ompx-hold` is introduced
    to disable the `ompx_hold` map type modifier, which is used by the OpenACC `copy`
    clause translation. The `ompx_hold` is an OpenMP extension that might not be supported
    yet by other compilers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4** Compiling the code with the [cc compiler wrapper](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Access exercise material
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for the `Clacc` exercise can be accessed in the content/examples/exercise_clacc
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise : Translate an OpenACC code to OpenMP'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the OpenACC code `openACC_code.c` located in `/exercise_clacc` with
    the `Clacc` compiler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the generated OpenMP code with the `cc` compiler wrapper and run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translating CUDA to SYCL/DPC++ with SYCLomatic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel offers a tool for CUDA-to-SYCL code migration, included in the Intel oneAPI
    Basekit.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not installed on LUMI, but the general workflow is similar to the HIPify
    Clang and also requires an existing CUDA installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: SYCLomatic can migrate larger projects by using `-in-root` and `-out-root` flags
    to process directories recursively. It can also use compilation database (supported
    by CMake and other build systems) to deal with more complex project layouts.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the code generated by SYCLomatic relies on oneAPI-specific
    extensions, and thus cannot be directly used with other SYCL implementations,
    such as AdaptiveCpp (hipSYCL). The `--no-incremental-migration` flag can be added
    to `dpct` command to minimize, but not completely avoid, the use of this compatibility
    layer. That would require manual effort, since some CUDA concepts cannot be directly
    mapped to SYCL.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, CUDA applications might assume certain hardware behavior, such
    as 32-wide warps. If the target hardware is different (e.g., AMD MI250 GPUs, used
    in LUMI, have warp size of 64), the algorithms might need to be adjusted manually.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This concludes a brief overview of the usage of available tools to convert CUDA
    codes to HIP and SYCL, and OpenACC codes to OpenMP offloading. In general the
    translation process for large applications might be incomplete and thus requires
    manual modification to complete the porting process. It is however worth noting
    that the accuracy of the translation process requires that applications are written
    correctly according to the CUDA and OpenACC syntaxes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hipify GitHub](https://github.com/ROCm/HIPIFY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HIPify Reference Guide v5.1](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HIP example](https://github.com/olcf-tutorials/simple_HIP_examples/tree/master/vector_addition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Porting CUDA to HIP](https://www.admin-magazine.com/HPC/Articles/Porting-CUDA-to-HIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clacc Main repository README](https://github.com/llvm-doe-org/llvm-project/blob/clacc/main/README.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SYCLomatic main mage](https://www.intel.com/content/www/us/en/developer/articles/technical/syclomatic-new-cuda-to-sycl-code-migration-tool.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SYCLomatic documentation](https://oneapi-src.github.io/SYCLomatic/get_started/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Useful tools exist to automatically translate tools from CUDA to HIP and SYCL
    and from OpenACC to OpenMP, but they may require manual modifications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting from CPU to GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When porting code to take advantage of the parallel processing capability of
    GPUs, several steps need to be followed and some additional work is required before
    writing actual parallel code to be executed on the GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify Targeted Parts**: Begin by identifying the parts of the code that
    contribute significantly to the execution time. These are often computationally
    intensive sections such as loops or matrix operations. The Pareto principle suggests
    that roughly 10-20% of the code accounts for 80-90% of the execution time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equivalent GPU Libraries**: If the original code uses CPU libraries like
    BLAS, FFT, etc, it’s crucial to identify the equivalent GPU libraries. For example,
    cuBLAS or hipBLAS can replace CPU-based BLAS libraries. Utilizing GPU-specific
    libraries ensures efficient GPU utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Refactor Loops**: When porting loops directly to GPUs, some refactoring is
    necessary to suit the GPU architecture. This typically involves splitting the
    loop into multiple steps or modifying operations to exploit the independence between
    iterations and improve memory access patterns. Each step of the original loop
    can be mapped to a kernel, executed by multiple GPU threads, with each thread
    corresponding to an iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Access Optimization**: Consider the memory access patterns in the
    code. GPUs perform best when memory access is coalesced and aligned. Minimizing
    global memory accesses and maximizing utilization of shared memory or registers
    can significantly enhance performance. Review the code to ensure optimal memory
    access for GPU execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How would this be ported? (n_soap ≈ 100, n_sites ⩾ 10000, k_max ≈ 20*n_sites
    )
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Inspect the following Fortran code (if you don’t read Fortran: do-loops ==
    for-loops)'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some steps at first glance:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: the code could (has to) be splitted in 3-4 kernels. Why?
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: check if there are any variables that could lead to false dependencies between
    iterations, like the index k2
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it efficient for GPUs to split the work over the index i? What about the
    memory access? Note the arrays are 2D in Fortran
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it possible to collapse some loops? Combining nested loops can reduce overhead
    and improve memory access patterns, leading to better GPU performance.
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: what is the best memory access in a GPU? Review memory access patterns in the
    code. Minimize global memory access by utilizing shared memory or registers where
    appropriate. Ensure memory access is coalesced and aligned, maximizing GPU memory
    throughput
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactored code!
  prefs: []
  type: TYPE_NORMAL
- en: Registers are limited and the larger the kernel use more registers registers
    resulting in less active threads (small occupancy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_rad_der(is,k2) the CUDA thread needs access to all
    the previous values soap_rad_der(1:nsoap,k2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_cart_der(1, 1:n_soap, k3) it is required to have access
    to all values (k3+1:k2+n_neigh(i)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note the indices in the first part. The matrices are transposed for better access
    patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Identify equivalent GPU libraries for CPU-based libraries and utilizing them
    to ensure efficient GPU utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of identifying the computationally intensive parts of the code that
    contribute significantly to the execution time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need to refactor loops to suit the GPU architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significance of memory access optimization for efficient GPU execution, including
    coalesced and aligned memory access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How would this be ported? (n_soap ≈ 100, n_sites ⩾ 10000, k_max ≈ 20*n_sites
    )
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Inspect the following Fortran code (if you don’t read Fortran: do-loops ==
    for-loops)'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some steps at first glance:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: the code could (has to) be splitted in 3-4 kernels. Why?
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: check if there are any variables that could lead to false dependencies between
    iterations, like the index k2
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it efficient for GPUs to split the work over the index i? What about the
    memory access? Note the arrays are 2D in Fortran
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: is it possible to collapse some loops? Combining nested loops can reduce overhead
    and improve memory access patterns, leading to better GPU performance.
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: what is the best memory access in a GPU? Review memory access patterns in the
    code. Minimize global memory access by utilizing shared memory or registers where
    appropriate. Ensure memory access is coalesced and aligned, maximizing GPU memory
    throughput
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactored code!
  prefs: []
  type: TYPE_NORMAL
- en: Registers are limited and the larger the kernel use more registers registers
    resulting in less active threads (small occupancy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_rad_der(is,k2) the CUDA thread needs access to all
    the previous values soap_rad_der(1:nsoap,k2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to compute soap_cart_der(1, 1:n_soap, k3) it is required to have access
    to all values (k3+1:k2+n_neigh(i)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note the indices in the first part. The matrices are transposed for better access
    patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Identify equivalent GPU libraries for CPU-based libraries and utilizing them
    to ensure efficient GPU utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of identifying the computationally intensive parts of the code that
    contribute significantly to the execution time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need to refactor loops to suit the GPU architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significance of memory access optimization for efficient GPU execution, including
    coalesced and aligned memory access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting between different GPU frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might also find yourself in a situation where you need to port a code from
    one particular GPU framework to another. This section gives an overview of different
    tools that enable converting CUDA and OpenACC codes to HIP and OpenMP, respectively.
    This conversion process enables an application to target various GPU architectures,
    specifically, NVIDIA and AMD GPUs. Here we focus on [hipify](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html)
    and [clacc](https://csmd.ornl.gov/project/clacc) tools. This guide is adapted
    from the [NRIS documentation](https://documentation.sigma2.no/code_development/guides/cuda_translating-tools.html).
  prefs: []
  type: TYPE_NORMAL
- en: Translating CUDA to HIP with Hipify
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we cover the use of `hipify-perl` and `hipify-clang` tools
    to translate a CUDA code to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-perl
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `hipify-perl` tool is a script based on perl that translates CUDA syntax
    into HIP syntax (see .e.g. [here](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl)
    for more details). For instance, in a CUDA code that incorporates the CUDA functions
    `` cudaMalloc` `` and `cudaDeviceSynchronize`, the tool will substitute `cudaMalloc`
    with the HIP function `hipMalloc`. Similarly the CUDA function `cudaDeviceSynchronize`
    will be substituted with the HIP function `hipDeviceSynchronize`. We list below
    the basic steps to run `hipify-perl` on LUMI-G.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Generating `hipify-perl` script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Running the generated `hipify-perl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 3**: Compiling with `hipcc` the generated HIP code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Despite the simplicity of the use of `hipify-perl`, the tool might not be suitable
    for large applications, as it relies heavily on substituting CUDA strings with
    HIP strings (e.g. it substitutes `*cuda*` with `*hip*`). In addition, `hipify-perl`
    lacks the ability of [distinguishing device/host function calls](https://docs.amd.com/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl).
    The alternative here is to use the `hipify-clang` tool as will be described in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-clang
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As described in the [HIPIFY documentation](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl),
    the `hipify-clang` tool is based on clang for translating CUDA sources into HIP
    sources. The tool is more robust for translating CUDA codes compared to the `hipify-perl`
    tool. Furthermore, it facilitates the analysis of the code by providing assistance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, `hipify-clang` requires `LLVM+CLANG` and `CUDA`. Details about building
    `hipify-clang` can be found [here](https://github.com/ROCm/HIPIFY). Note that
    `hipify-clang` is available on LUMI-G. The issue however might be related to the
    installation of CUDA-toolkit. To avoid any eventual issues with the installation
    procedure we opt for CUDA singularity container. Here we present a step-by-step
    guide for running `hipify-clang`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Pulling a CUDA singularity container e.g.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Loading a rocm module and launching the CUDA singularity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: where the current directory `$PWD` in the host is mounted to that of the container,
    and the directory `/opt` in the host is mounted to the that inside the container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 3**: Setting the environment variable `$PATH`. In order to run `hipify-clang`
    from inside the container, one can set the environment variable `$PATH` that defines
    the path to look for the binary `hipify-clang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the rocm version we used is `rocm-6.0.3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4**: Running `hipify-clang` from inside the singularity container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the cuda path and the path to the `*includes*` and `*defines*` files should
    be specified. The CUDA source code and the generated output code are program.cu
    and hip_program.cu.hip, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The syntax for the compilation process of the generated hip code is similar
    to the one described in the previous section (see the **Step 3** in the hipify-perl
    section).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Code examples for the `Hipify` exercises can be accessed in the content/examples/exercise_hipify
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Exercise I : Translate an CUDA code to HIP with `hipify-perl`'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Generate the `hipify-perl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_perl`
    with the `Hipify-perl` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II : Translate an CUDA code to HIP with `hipify-clang`'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_clang`
    with the `Hipify-clang` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Translating OpenACC to OpenMP with Clacc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main) is a
    tool to translate an OpenACC application to OpenMP offloading with the Clang/LLVM
    compiler environment. Note that the tool is specific to OpenACC C, while OpenACC
    Fortran is already supported on AMD GPU. As indicated in the [GitHub repository](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main)
    the compiler `Clacc` is the `Clang`’s executable in the subdirectory `\bin` of
    the `\install` directory as described below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following we present a step-by-step guide for building and using Clacc:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Building and installing [Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Setting up environment variables to be able to work from the `/install`
    directory, which is the simplest way. We assume that the `/install` directory
    is located in the path `/project/project_xxxxxx/Clacc/llvm-project`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more advanced usage, which includes for instance modifying `Clacc`, we refer
    readers to [“Usage from Build directory”](https://github.com/llvm-doe-org/llvm-project/blob/clacc/main/README.md)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '**Step 3**: Source to source conversion of the openACC_code.c code to be printed
    out to the file openMP_code.c:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the flag `-fopenacc-structured-ref-count-omp=no-ompx-hold` is introduced
    to disable the `ompx_hold` map type modifier, which is used by the OpenACC `copy`
    clause translation. The `ompx_hold` is an OpenMP extension that might not be supported
    yet by other compilers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4** Compiling the code with the [cc compiler wrapper](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Access exercise material
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for the `Clacc` exercise can be accessed in the content/examples/exercise_clacc
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise : Translate an OpenACC code to OpenMP'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the OpenACC code `openACC_code.c` located in `/exercise_clacc` with
    the `Clacc` compiler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the generated OpenMP code with the `cc` compiler wrapper and run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translating CUDA to SYCL/DPC++ with SYCLomatic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel offers a tool for CUDA-to-SYCL code migration, included in the Intel oneAPI
    Basekit.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not installed on LUMI, but the general workflow is similar to the HIPify
    Clang and also requires an existing CUDA installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: SYCLomatic can migrate larger projects by using `-in-root` and `-out-root` flags
    to process directories recursively. It can also use compilation database (supported
    by CMake and other build systems) to deal with more complex project layouts.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the code generated by SYCLomatic relies on oneAPI-specific
    extensions, and thus cannot be directly used with other SYCL implementations,
    such as AdaptiveCpp (hipSYCL). The `--no-incremental-migration` flag can be added
    to `dpct` command to minimize, but not completely avoid, the use of this compatibility
    layer. That would require manual effort, since some CUDA concepts cannot be directly
    mapped to SYCL.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, CUDA applications might assume certain hardware behavior, such
    as 32-wide warps. If the target hardware is different (e.g., AMD MI250 GPUs, used
    in LUMI, have warp size of 64), the algorithms might need to be adjusted manually.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This concludes a brief overview of the usage of available tools to convert CUDA
    codes to HIP and SYCL, and OpenACC codes to OpenMP offloading. In general the
    translation process for large applications might be incomplete and thus requires
    manual modification to complete the porting process. It is however worth noting
    that the accuracy of the translation process requires that applications are written
    correctly according to the CUDA and OpenACC syntaxes.
  prefs: []
  type: TYPE_NORMAL
- en: Translating CUDA to HIP with Hipify
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we cover the use of `hipify-perl` and `hipify-clang` tools
    to translate a CUDA code to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-perl
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `hipify-perl` tool is a script based on perl that translates CUDA syntax
    into HIP syntax (see .e.g. [here](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl)
    for more details). For instance, in a CUDA code that incorporates the CUDA functions
    `` cudaMalloc` `` and `cudaDeviceSynchronize`, the tool will substitute `cudaMalloc`
    with the HIP function `hipMalloc`. Similarly the CUDA function `cudaDeviceSynchronize`
    will be substituted with the HIP function `hipDeviceSynchronize`. We list below
    the basic steps to run `hipify-perl` on LUMI-G.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Generating `hipify-perl` script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Running the generated `hipify-perl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 3**: Compiling with `hipcc` the generated HIP code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Despite the simplicity of the use of `hipify-perl`, the tool might not be suitable
    for large applications, as it relies heavily on substituting CUDA strings with
    HIP strings (e.g. it substitutes `*cuda*` with `*hip*`). In addition, `hipify-perl`
    lacks the ability of [distinguishing device/host function calls](https://docs.amd.com/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl).
    The alternative here is to use the `hipify-clang` tool as will be described in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-clang
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As described in the [HIPIFY documentation](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl),
    the `hipify-clang` tool is based on clang for translating CUDA sources into HIP
    sources. The tool is more robust for translating CUDA codes compared to the `hipify-perl`
    tool. Furthermore, it facilitates the analysis of the code by providing assistance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, `hipify-clang` requires `LLVM+CLANG` and `CUDA`. Details about building
    `hipify-clang` can be found [here](https://github.com/ROCm/HIPIFY). Note that
    `hipify-clang` is available on LUMI-G. The issue however might be related to the
    installation of CUDA-toolkit. To avoid any eventual issues with the installation
    procedure we opt for CUDA singularity container. Here we present a step-by-step
    guide for running `hipify-clang`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Pulling a CUDA singularity container e.g.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Loading a rocm module and launching the CUDA singularity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: where the current directory `$PWD` in the host is mounted to that of the container,
    and the directory `/opt` in the host is mounted to the that inside the container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 3**: Setting the environment variable `$PATH`. In order to run `hipify-clang`
    from inside the container, one can set the environment variable `$PATH` that defines
    the path to look for the binary `hipify-clang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the rocm version we used is `rocm-6.0.3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4**: Running `hipify-clang` from inside the singularity container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the cuda path and the path to the `*includes*` and `*defines*` files should
    be specified. The CUDA source code and the generated output code are program.cu
    and hip_program.cu.hip, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The syntax for the compilation process of the generated hip code is similar
    to the one described in the previous section (see the **Step 3** in the hipify-perl
    section).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Code examples for the `Hipify` exercises can be accessed in the content/examples/exercise_hipify
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Exercise I : Translate an CUDA code to HIP with `hipify-perl`'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Generate the `hipify-perl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_perl`
    with the `Hipify-perl` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II : Translate an CUDA code to HIP with `hipify-clang`'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_clang`
    with the `Hipify-clang` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-perl
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `hipify-perl` tool is a script based on perl that translates CUDA syntax
    into HIP syntax (see .e.g. [here](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl)
    for more details). For instance, in a CUDA code that incorporates the CUDA functions
    `` cudaMalloc` `` and `cudaDeviceSynchronize`, the tool will substitute `cudaMalloc`
    with the HIP function `hipMalloc`. Similarly the CUDA function `cudaDeviceSynchronize`
    will be substituted with the HIP function `hipDeviceSynchronize`. We list below
    the basic steps to run `hipify-perl` on LUMI-G.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Generating `hipify-perl` script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Running the generated `hipify-perl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 3**: Compiling with `hipcc` the generated HIP code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Despite the simplicity of the use of `hipify-perl`, the tool might not be suitable
    for large applications, as it relies heavily on substituting CUDA strings with
    HIP strings (e.g. it substitutes `*cuda*` with `*hip*`). In addition, `hipify-perl`
    lacks the ability of [distinguishing device/host function calls](https://docs.amd.com/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl).
    The alternative here is to use the `hipify-clang` tool as will be described in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hipify-clang
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As described in the [HIPIFY documentation](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html#perl),
    the `hipify-clang` tool is based on clang for translating CUDA sources into HIP
    sources. The tool is more robust for translating CUDA codes compared to the `hipify-perl`
    tool. Furthermore, it facilitates the analysis of the code by providing assistance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, `hipify-clang` requires `LLVM+CLANG` and `CUDA`. Details about building
    `hipify-clang` can be found [here](https://github.com/ROCm/HIPIFY). Note that
    `hipify-clang` is available on LUMI-G. The issue however might be related to the
    installation of CUDA-toolkit. To avoid any eventual issues with the installation
    procedure we opt for CUDA singularity container. Here we present a step-by-step
    guide for running `hipify-clang`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Pulling a CUDA singularity container e.g.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Loading a rocm module and launching the CUDA singularity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: where the current directory `$PWD` in the host is mounted to that of the container,
    and the directory `/opt` in the host is mounted to the that inside the container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 3**: Setting the environment variable `$PATH`. In order to run `hipify-clang`
    from inside the container, one can set the environment variable `$PATH` that defines
    the path to look for the binary `hipify-clang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the rocm version we used is `rocm-6.0.3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4**: Running `hipify-clang` from inside the singularity container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the cuda path and the path to the `*includes*` and `*defines*` files should
    be specified. The CUDA source code and the generated output code are program.cu
    and hip_program.cu.hip, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The syntax for the compilation process of the generated hip code is similar
    to the one described in the previous section (see the **Step 3** in the hipify-perl
    section).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Code examples for the `Hipify` exercises can be accessed in the content/examples/exercise_hipify
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Exercise I : Translate an CUDA code to HIP with `hipify-perl`'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Generate the `hipify-perl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_perl`
    with the `Hipify-perl` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II : Translate an CUDA code to HIP with `hipify-clang`'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Convert the CUDA code `vec_add_cuda.cu` located in `/exercise_hipify/Hipify_clang`
    with the `Hipify-clang` tool to HIP.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Compile the generated HIP code with the `hipcc` compiler wrapper and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Translating OpenACC to OpenMP with Clacc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main) is a
    tool to translate an OpenACC application to OpenMP offloading with the Clang/LLVM
    compiler environment. Note that the tool is specific to OpenACC C, while OpenACC
    Fortran is already supported on AMD GPU. As indicated in the [GitHub repository](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main)
    the compiler `Clacc` is the `Clang`’s executable in the subdirectory `\bin` of
    the `\install` directory as described below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following we present a step-by-step guide for building and using Clacc:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Building and installing [Clacc](https://github.com/llvm-doe-org/llvm-project/tree/clacc/main).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Step 2**: Setting up environment variables to be able to work from the `/install`
    directory, which is the simplest way. We assume that the `/install` directory
    is located in the path `/project/project_xxxxxx/Clacc/llvm-project`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more advanced usage, which includes for instance modifying `Clacc`, we refer
    readers to [“Usage from Build directory”](https://github.com/llvm-doe-org/llvm-project/blob/clacc/main/README.md)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: '**Step 3**: Source to source conversion of the openACC_code.c code to be printed
    out to the file openMP_code.c:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the flag `-fopenacc-structured-ref-count-omp=no-ompx-hold` is introduced
    to disable the `ompx_hold` map type modifier, which is used by the OpenACC `copy`
    clause translation. The `ompx_hold` is an OpenMP extension that might not be supported
    yet by other compilers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4** Compiling the code with the [cc compiler wrapper](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Access exercise material
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for the `Clacc` exercise can be accessed in the content/examples/exercise_clacc
    subdirectory by cloning this repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise : Translate an OpenACC code to OpenMP'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the OpenACC code `openACC_code.c` located in `/exercise_clacc` with
    the `Clacc` compiler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the generated OpenMP code with the `cc` compiler wrapper and run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translating CUDA to SYCL/DPC++ with SYCLomatic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel offers a tool for CUDA-to-SYCL code migration, included in the Intel oneAPI
    Basekit.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not installed on LUMI, but the general workflow is similar to the HIPify
    Clang and also requires an existing CUDA installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: SYCLomatic can migrate larger projects by using `-in-root` and `-out-root` flags
    to process directories recursively. It can also use compilation database (supported
    by CMake and other build systems) to deal with more complex project layouts.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the code generated by SYCLomatic relies on oneAPI-specific
    extensions, and thus cannot be directly used with other SYCL implementations,
    such as AdaptiveCpp (hipSYCL). The `--no-incremental-migration` flag can be added
    to `dpct` command to minimize, but not completely avoid, the use of this compatibility
    layer. That would require manual effort, since some CUDA concepts cannot be directly
    mapped to SYCL.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, CUDA applications might assume certain hardware behavior, such
    as 32-wide warps. If the target hardware is different (e.g., AMD MI250 GPUs, used
    in LUMI, have warp size of 64), the algorithms might need to be adjusted manually.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This concludes a brief overview of the usage of available tools to convert CUDA
    codes to HIP and SYCL, and OpenACC codes to OpenMP offloading. In general the
    translation process for large applications might be incomplete and thus requires
    manual modification to complete the porting process. It is however worth noting
    that the accuracy of the translation process requires that applications are written
    correctly according to the CUDA and OpenACC syntaxes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hipify GitHub](https://github.com/ROCm/HIPIFY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HIPify Reference Guide v5.1](https://docs.amd.com/en-US/bundle/HIPify-Reference-Guide-v5.1/page/HIPify.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HIP example](https://github.com/olcf-tutorials/simple_HIP_examples/tree/master/vector_addition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Porting CUDA to HIP](https://www.admin-magazine.com/HPC/Articles/Porting-CUDA-to-HIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clacc Main repository README](https://github.com/llvm-doe-org/llvm-project/blob/clacc/main/README.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SYCLomatic main mage](https://www.intel.com/content/www/us/en/developer/articles/technical/syclomatic-new-cuda-to-sycl-code-migration-tool.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SYCLomatic documentation](https://oneapi-src.github.io/SYCLomatic/get_started/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Useful tools exist to automatically translate tools from CUDA to HIP and SYCL
    and from OpenACC to OpenMP, but they may require manual modifications.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
