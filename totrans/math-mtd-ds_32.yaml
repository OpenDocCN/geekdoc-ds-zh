- en: '4.6\. Further applications of the SVD: low-rank approximations and ridge regression#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html](https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this section, we discuss further properties of the SVD. We first introduce
    additional matrix norms.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1\. Matrix norms[#](#matrix-norms "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that the Frobenius norm\(\idx{Frobenius norm}\xdi\) of an \(n \times
    m\) matrix \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here we introduce a different notion of matrix norm that has many uses in data
    science (and beyond).
  prefs: []
  type: TYPE_NORMAL
- en: '**Induced norm** The Frobenius norm does not directly relate to \(A\) as a
    representation of a [linear map](https://en.wikipedia.org/wiki/Linear_map). In
    particular, it is desirable in many contexts to quantify how two matrices differ
    in terms of how they act on vectors. For instance, one is often interested in
    bounding quantities of the following form. Let \(B, B'' \in \mathbb{R}^{n \times
    m}\) and let \(\mathbf{x} \in \mathbb{R}^m\) be of unit norm. What can be said
    about \(\|B \mathbf{x} - B'' \mathbf{x}\|\)? Intuitively, what we would like is
    this: if the norm of \(B - B''\) is small then \(B\) is close to \(B''\) as a
    linear map, that is, the vector norm \(\|B \mathbf{x} - B'' \mathbf{x}\|\) is
    small for any unit vector \(\mathbf{x}\). The following definition provides us
    with such a notion. Define the unit sphere \(\mathbb{S}^{m-1} = \{\mathbf{x} \in
    \mathbb{R}^m\,:\,\|\mathbf{x}\| = 1\}\) in \(m\) dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(\(2\)-Norm)** The \(2\)-norm of a matrix\(\idx{2-norm}\xdi\)
    \(A \in \mathbb{R}^{n \times m}\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_2 := \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|}
    = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: The equality in the definition uses the absolute homogeneity of the vector norm.
    Also the definition implicitly uses the *Extreme Value Theorem*. In this case,
    we use the fact that the function \(f(\mathbf{x}) = \|A \mathbf{x}\|\) is continuous
    and the set \(\mathbb{S}^{m-1}\) is closed and bounded to conclude that there
    exists \(\mathbf{x}^* \in \mathbb{S}^{m-1}\) such that \(f(\mathbf{x}^*) \geq
    f(\mathbf{x})\) for all \(\mathbf{x} \in \mathbb{S}^{m-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: The \(2\)-norm of a matrix has many other useful properties. The first four
    below are what makes it a [norm](https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Properties of the \(2\)-Norm)** Let \(A, B \in \mathbb{R}^{n \times
    m}\) and \(\alpha \in \mathbb{R}\). The following hold:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\|A\|_2 \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\|A\|_2 = 0\) if and only if \(A = 0\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\|\alpha A\|_2 = |\alpha| \|A\|_2\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)
  prefs: []
  type: TYPE_NORMAL
- en: e) \(\|A B \|_2 \leq \|A\|_2 \|B\|_2\).
  prefs: []
  type: TYPE_NORMAL
- en: f) \(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\), \(\forall \mathbf{0} \neq
    \mathbf{x} \in \mathbb{R}^m\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* These properties all follow from the definition of the \(2\)-norm
    and the corresponding properties for the vector norm:'
  prefs: []
  type: TYPE_NORMAL
- en: Claims a) and f) are immediate from the definition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For b) note that \(\|A\|_2 = 0\) implies \(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\), so that \(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\). In particular, \(a_{ij} = \mathbf{e}_i^T A \mathbf{e}_j
    = 0, \forall i,j\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For c), d), e), observe that for all \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|, \]\[\|(A+B)\mathbf{x}\|
    = \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\| \leq \|A\|_2
    + \|B\|_2 \]\[ \|(AB)\mathbf{x}\| = \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
    \leq \|A\|_2 \|B\|_2.\]
  prefs: []
  type: TYPE_NORMAL
- en: Then apply the definition of \(2\)-norm. For example, for ©,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\alpha A\|_2 &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha
    A \mathbf{x}\|\\ &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
    &= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\ &= |\alpha|
    \|A\|_2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(|\alpha|\) does not depend on \(\mathbf{x}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In NumPy, the Frobenius norm of a matrix can be computed
    using the default of the function `numpy.linalg.norm` while the induced norm can
    be computed using the same function with [`ord` parameter set to `2`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix norms and SVD** As it turns out, the two notions of matrix norms we
    have introduced admit simple expressions in terms of the singular values of the
    matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Matrix Norms and Singular Values)** \(\idx{matrix norms and singular
    values lemma}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with compact
    SVD'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\). Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|^2_2 = \sigma_{1}^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We will use the notation \(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\).
    Using that the squared Frobenius norm of \(A\) is the sum of the squared norms
    of its columns, we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|^2_F = \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
    = \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell \right\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\mathbf{u}_\ell\)’s are orthonormal, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^m \sum_{\ell=1}^r \sigma_\ell^2 v_{\ell,j}^2 = \sum_{\ell=1}^r
    \sigma_\ell^2 \left(\sum_{j=1}^m v_{\ell,j}^2\right) = \sum_{\ell=1}^r \sigma_\ell^2
    \|\mathbf{v}_{\ell}\|^2 = \sum_{\ell=1}^r \sigma_\ell^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the \(\mathbf{v}_\ell\)’s are also orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: For the second claim, recall that the \(2\)-norm is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_2^2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: We have shown previously that \(\mathbf{v}_1\) solves this problem. Hence \(\|A\|_2^2
    = \|A \mathbf{v}_1\|^2 = \sigma_1^2\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2\. Low-rank approximation[#](#low-rank-approximation "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have defined a notion of distance between matrices, we will consider
    the problem of finding a good approximation to a matrix \(A\) among all matrices
    of rank at most \(k\). We will start with the Frobenius norm, which is easier
    to work with, and we will show later on that the solution is the same under the
    induced norm. The solution to this problem will be familiar. In essence, we will
    re-interpret our solution to the best approximating subspace as a low-rank approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-rank approximation in the Frobenius norm** \(\idx{low-rank approximation}\xdi\)
    From the proof of the *Row Rank Equals Column Rank Lemma*, it follows that a rank-\(r\)
    matrix \(A\) can be written as a sum of \(r\) rank-\(1\) matrices'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: We will now consider the problem of finding a “simpler” approximation to \(A\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(k < r\). Here we measure the quality of this approximation using a matrix
    norm.
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to state our key observation. In words, the best rank-\(k\) approximation
    to \(A\) in Frobenius norm is obtained by projecting the rows of \(A\) onto a
    linear subspace of dimension \(k\). We will come back to how one finds the best
    such subspace below. (*Hint:* We have already solved this problem.)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Projection and Rank-\(k\) Approximation)** \(\idx{projection and
    rank-k approximation lemma}\xdi\) Let \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n
    \times m}\). For any matrix \(B = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)
    of rank \(k \leq \min\{n,m\}\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - B_{\perp}\|_F \leq \|A - B\|_F \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{\perp} \in \mathbb{R}^{n \times m}\) is the matrix of rank at most
    \(k\) obtained as follows. Denote row \(i\) of \(A\), \(B\) and \(B_{\perp}\)
    respectively by \(\boldsymbol{\alpha}_i^T\), \(\mathbf{b}_{i}^T\) and \(\mathbf{b}_{\perp,i}^T\),
    \(i=1,\ldots, n\). Set \(\mathbf{b}_{\perp,i}\) to be the orthogonal projection
    of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z} = \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\).
    \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* The square of the Frobenius norm decomposes as a sum of squared
    row norms. Each term in the sum is minimized by the orthogonal projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By definition of the Frobenius norm, we note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 = \sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'and similarly for \(\|A - B_{\perp}\|_F\). We make two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Because the orthogonal projection of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z}\)
    minimizes the distance to \(\mathcal{Z}\), it follows that term by term \(\|\boldsymbol{\alpha}_i
    - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|\) so
    that
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \|A - B_\perp\|_F^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
    \leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 = \|A - B\|_F^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, because the projections satisfy \(\mathbf{b}_{\perp,i} \in \mathcal{Z}\)
    for all \(i\), \(\mathrm{row}(B_\perp) \subseteq \mathrm{row}(B)\) and, hence,
    the rank of \(B_\perp\) is at most the rank of \(B\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Recall the approximating subspace problem. That is, think of the rows \(\boldsymbol{\alpha}_i^T\)
    of \(A \in \mathbb{R}^{n \times m}\) as a collection of \(n\) data points in \(\mathbb{R}^m\).
    We are looking for a linear subspace \(\mathcal{Z}\) that minimizes \(\sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)
    over all linear subspaces of \(\mathbb{R}^m\) of dimension at most \(k\). By the
    *Projection and Rank-\(k\) Approximation Lemma*, this problem is equivalent to
    finding a matrix \(B\) that minimizes \(\|A - B\|_F\) among all matrices in \(\mathbb{R}^{n
    \times m}\) of rank at most \(k\). Of course we have solved this problem before.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). For \(k < r\), truncate the sum at the
    \(k\)-th term \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). The
    rank of \(A_k\) is exactly \(k\). Indeed, by construction,
  prefs: []
  type: TYPE_NORMAL
- en: the vectors \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) are orthonormal, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: since \(\sigma_j > 0\) for \(j=1,\ldots,k\) and the vectors \(\{\mathbf{v}_j\,:\,j
    = 1,\ldots,k\}\) are orthonormal, \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) spans
    the column space of \(A_k\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have shown before that \(A_k\) is the best approximation to \(A\) among matrices
    of rank at most \(k\) in Frobenius norm. Specifically, the *Greedy Finds Best
    Fit Theorem* implies that, for any matrix \(B \in \mathbb{R}^{n \times m}\) of
    rank at most \(k\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|_F \leq \|A - B\|_F. \]
  prefs: []
  type: TYPE_NORMAL
- en: This result is known as the *Eckart-Young Theorem*\(\idx{Eckart-Young theorem}\xdi\).
    It also holds in the induced \(2\)-norm, as we show next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-rank approximation in the induced norm** We show in this section that
    the same holds in the induced norm. First, some observations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Matrix Norms and Singular Values: Truncation)** Let \(A \in \mathbb{R}^{n
    \times m}\) be a matrix with SVD'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\) and let
    \(A_k\) be the truncation defined above. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|^2_2 = \sigma_{k+1}^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For the first claim, by definition, summing over the columns of \(A
    - A_k\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|^2_F = \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
    = \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j \right\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\mathbf{u}_j\)’s are orthonormal, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^m \sum_{j=k+1}^r \sigma_j^2 v_{j,i}^2 = \sum_{j=k+1}^r \sigma_j^2
    \left(\sum_{i=1}^m v_{j,i}^2\right) = \sum_{j=k+1}^r \sigma_j^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the \(\mathbf{v}_j\)’s are also orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: For the second claim, recall that the induced norm is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|B\|_2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: For any \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\|(A - A_k)\mathbf{x}\right\|^2 = \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j
    (\mathbf{v}_j^T \mathbf{x}) \right\|^2 = \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j,
    \mathbf{x}\rangle^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\sigma_j\)’s are in decreasing order, this is maximized when \(\langle
    \mathbf{v}_j, \mathbf{x}\rangle = 1\) if \(j=k+1\) and \(0\) otherwise. That is,
    we take \(\mathbf{x} = \mathbf{v}_{k+1}\) and the norm is then \(\sigma_{k+1}^2\),
    as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Low-Rank Approximation in the Induced Norm)** \(\idx{low-rank
    approximation in the induced norm theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times
    m}\) be a matrix with SVD'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  prefs: []
  type: TYPE_NORMAL
- en: and let \(A_k\) be the truncation defined above with \(k < r\). For any matrix
    \(B \in \mathbb{R}^{n \times m}\) of rank at most \(k\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|_2 \leq \|A - B\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We know that \(\|A - A_k\|_2^2 = \sigma_{k+1}^2\). So we want
    to lower bound \(\|A - B\|_2^2\) by \(\sigma_{k+1}^2\). For that, we have to find
    an appropriate \(\mathbf{z}\) for any given \(B\) of rank at most \(k\). The idea
    is to take a vector \(\mathbf{z}\) in the intersection of the null space of \(B\)
    and the span of the singular vectors \(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\).
    By the former, the squared norm of \((A - B)\mathbf{z}\) is equal to the squared
    norm of \(A\mathbf{z}\) which lower bounds \(\|A\|_2^2\). By the latter, \(\|A
    \mathbf{z}\|^2\) is at least \(\sigma_{k+1}^2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Rank-Nullity Theorem*, the dimension of \(\mathrm{null}(B)\)
    is at least \(m-k\) so there is a unit vector \(\mathbf{z}\) in the intersection'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: (Prove it!) Then \((A-B)\mathbf{z} = A\mathbf{z}\) since \(\mathbf{z} \in \mathrm{null}(B)\).
    Also since \(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\),
    and therefore orthogonal to \(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\), we have
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|(A-B)\mathbf{z}\|^2 &= \|A\mathbf{z}\|^2\\ &= \left\|\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \left\|\sum_{j=1}^{k+1}
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \sum_{j=1}^{k+1}
    \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1}
    \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &= \sigma_{k+1}^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the previous lemma, \(\sigma_{k+1}^2 = \|A - A_k\|_2\) and we are done. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**An application: Why project?** We return to \(k\)-means clustering and why
    projecting to a lower-dimensional subspace can produce better results. We prove
    a simple inequality that provides some insight. Quoting [BHK, Section 7.5.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To elaborate, suppose we have \(n\) data points in \(d\) dimensions in the form
    of the rows \(\boldsymbol{\alpha}_i^T\), \(i=1\ldots, n\), of matrix \(A \in \mathbb{A}^{n
    \times d}\), where we assume that \(n > d\) and that \(A\) has full column rank.
    Imagine these data points come from an unknown ground-truth \(k\)-clustering assignment
    \(g(i) \in [k]\), \(i = 1,\ldots, n\), with corresponding unknown centers \(\mathbf{c}_j\),
    \(j = 1,\ldots, k\). Let \(C \in \mathbb{R}^{n \times d}\) be the corresponding
    matrix, that is, row \(i\) of \(C\) is \(\mathbf{c}_j^T\) if \(g(i) = j\). The
    \(k\)-means objective of the true clustering is then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
    &= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
    &= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\ &= \|A - C\|_F^2.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(A\) has an SVD \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)
    and for \(k < r\) we have the truncation \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\). It corresponds to projecting each row of \(A\) onto the linear
    subspace spanned by the first \(k\) right singular vectors \(\mathbf{v}_1, \ldots,
    \mathbf{v}_k\). To see this, note that the \(i\)-th row of \(A\) is \(\boldsymbol{\alpha}_i^T
    = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\) and that, because the \(\mathbf{v}_j\)’s
    are linearly independent and in particular \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    is an orthonormal basis of its span, the projection of \(\boldsymbol{\alpha}_i\)
    onto \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\ell=1}^k \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i}
    \mathbf{v}_j,\mathbf{v}_\ell\right\rangle = \sum_{\ell=1}^k \sigma_\ell u_{\ell,i}
    \mathbf{v}_\ell \]
  prefs: []
  type: TYPE_NORMAL
- en: which is the \(i\)-th row of \(A_k\). The \(k\)-means objective of \(A_k\) with
    respect to the ground-truth centers \(\mathbf{c}_j\), \(j=1,\ldots,k\), is \(\|A_k
    - C\|_F^2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'One more observation: the rank of \(C\) is at most \(k\). Indeed, there are
    \(k\) different rows in \(C\) so its row rank is \(k\) if these different rows
    are linearly independent and less than \(k\) otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Why Project)** \(\idx{why project theorem}\xdi\) Let \(A \in
    \mathbb{A}^{n \times d}\) be a matrix and let \(A_k\) be the truncation above.
    For any matrix \(C \in \mathbb{R}^{n \times d}\) of rank \(\leq k\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: Observe that we used different matrix norms on the different sides of the inequality.
    The content of this inequality is the following. The quantity \(\|A_k - C\|_F^2\)
    is the \(k\)-means objective of the projection \(A_k\) with respect to the true
    centers, that is, the sum of the squared distances to the centers. By the *Matrix
    Norms and Singular Values Lemma*, the inequality above gives that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma_j(A - C)\) is the \(j\)-th singular value of \(A - C\). On the
    other hand, by the same lemma, the \(k\)-means objective of the un-projected data
    is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: If the rank of \(A-C\) is much larger than \(k\) and the singular values of
    \(A-C\) decay slowly, then the latter quantity may be much larger. In other words,
    projecting may bring the data points closer to their true centers, potentially
    making it easier to cluster them.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Why Project)* We have shown previously that, for any matrices \(A,
    B \in \mathbb{R}^{n \times m}\), the rank of their sum is less or equal than the
    sum of their ranks, that is, \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
    So the rank of the difference \(A_k - C\) is at most the sum of the ranks'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(A_k - C) \leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the rank of \(A_k\) is \(k\) and the rank of \(C\) is \(\leq
    k\) since it has \(k\) distinct rows. By the *Matrix Norms and Singular Values
    Lemma*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2\. \]
  prefs: []
  type: TYPE_NORMAL
- en: By the triangle inequality for matrix norms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_2 \leq \|A_k - A\|_2 + \|A - C\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Low-Rank Approximation in the Induced Norm Theorem*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|_2 \leq \|A - C\|_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: since \(C\) has rank at most \(k\). Putting these three inequalities together,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 2k (2 \|A - C\|_2)^2 = 8k \|A - C\|_2^2\. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We return to our example with the two Gaussian clusters.
    We use function producing two separate clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We first generate the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In reality, we cannot compute the matrix norms of \(X-C\) and \(X_k-C\) as
    the true centers are not known. But, because this is simulated data, we happen
    to know the truth and we can check the validity of our results in this case. The
    centers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We use [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function to compute the norms from the formulas in the *Matrix Norms and Singular
    Values Lemma*. First, we observe that the singular values of \(X-C\) are decaying
    slowly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png](../Images/23b4327417d560c241971d818c1583cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The \(k\)-means objective with respect to the true centers under the full-dimensional
    data is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'while the square of the top singular value of \(X-C\) is only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we compute the \(k\)-means objective with respect to the true centers
    under the projected one-dimensional data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the applications of SVD
    in recommendation systems. How is it used to predict user preferences? \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about nonnegative matrix factorization
    (NMF) and how it compares to SVD. What are the key differences in their constraints
    and applications? How does NMF handle interpretability in topics like text analysis
    or image processing? Explore some algorithms used to compute NMF. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.3\. Ridge regression[#](#ridge-regression "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we consider what is called Tikhonov regularization\(\idx{Tikhonov regularization}\xdi\),
    an idea that turns out to be useful in overdetermined linear systems, particularly
    when the columns of the matrix \(A\) are linearly dependent or close to linearly
    dependent (which is sometimes referred to as [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\(\idx{multicollinearity}\xdi\)
    in statistics). It trades off minimizing the fit to the data versus minimizing
    the norm of the solution. More precisely, for a parameter \(\lambda > 0\) to be
    chosen, we solve\(\idx{ridge regression}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: The second term is referred to as an \(L_2\)-regularizer\(\idx{L2-regularization}\xdi\).
    Here \(A \in \mathbb{R}^{n\times m}\) with \(n \geq m\) and \(\mathbf{b} \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: To solve this optimization problem, we show that the objective function is strongly
    convex. We then find its unique stationary point. Rewriting the objective in quadratic
    function form
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}) &= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2\\ &= \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x}
    + \mathbf{b}^T \mathbf{b} + \lambda \mathbf{x}^T \mathbf{x}\\ &= \mathbf{x}^T
    (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T
    \mathbf{b}\\ &= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(P = 2 (A^T A + \lambda I_{m \times m})\) is symmetric, \(\mathbf{q}
    = - 2 A^T \mathbf{b}\), and \(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\).
  prefs: []
  type: TYPE_NORMAL
- en: As we previously computed, the Hessian of \(f\) is \(H_f(\mathbf{x})= P\). Now
    comes a key observation. The matrix \(P\) is positive definite whenever \(\lambda
    > 0\). Indeed, for any \(\mathbf{z} \in \mathbb{R}^m\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z} = 2 \|A \mathbf{z}\|_2^2
    + 2 \lambda \|\mathbf{z}\|_2^2 > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mu = 2 \lambda > 0\). Then \(f\) is \(\mu\)-strongly convex. This holds
    whether or not the columns of \(A\) are linearly independent.
  prefs: []
  type: TYPE_NORMAL
- en: The stationary points are easily characterized. Recall that the gradient is
    \(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\). Equating to \(\mathbf{0}\)
    leads to the system
  prefs: []
  type: TYPE_NORMAL
- en: \[ 2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: that is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The matrix in parenthesis is invertible as it is \(1/2\) of the Hessian, which
    is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: '**Connection to SVD** Expressing the solution in terms of a compact SVD \(A
    = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) provides
    some insights into how ridge regression works. Suppose that \(A\) has full column
    rank. That implies that \(V V^T = I_{m \times m}\). Then observe that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (A^T A + \lambda I_{m \times m})^{-1} &= (V \Sigma U^T U \Sigma
    V^T + \lambda I_{m \times m})^{-1}\\ &= (V \Sigma^2 V^T + \lambda I_{m \times
    m})^{-1}\\ &= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\ &= (V [\Sigma^2
    + \lambda I_{m \times m}] V^T)^{-1}\\ &= V (\Sigma^2 + \lambda I_{m \times m})^{-1}
    V^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b} =
    V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b} = V (\Sigma^2
    + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Our predictions are
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{x}^{**} &= U \Sigma V^T V (\Sigma^2 + \lambda I_{m
    \times m})^{-1} \Sigma U^T \mathbf{b}\\ &= U \Sigma (\Sigma^2 + \lambda I_{m \times
    m})^{-1} \Sigma U^T \mathbf{b}\\ &= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2
    + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that the terms in curly brackets are \(< 1\) when \(\lambda > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: Compare this to the unregularized least squares solution, which is obtained
    simply by setting \(\lambda = 0\) above,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{x}^* &= \sum_{j=1}^r \mathbf{u}_j \mathbf{u}_j^T
    \mathbf{b}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The difference is that the regularized solution reduces the contributions from
    the left singular vectors corresponding to small singular values.
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following best describes the Frobenius norm of a matrix
    \(A \in \mathbb{R}^{n \times m}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The maximum singular value of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The square root of the sum of the squares of all entries in \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The maximum absolute row sum of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) The maximum absolute column sum of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) and let \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) be the truncated SVD with \(k < r\). Which of the following is
    true about the Frobenius norm of \(A - A_k\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\|A - A_k\|_F^2 = \sigma_k^2\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** The ridge regression problem is formulated as \(\min_{\mathbf{x} \in
    \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\).
    What is the role of the parameter \(\lambda\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It controls the trade-off between fitting the data and minimizing the norm
    of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: b) It determines the rank of the matrix \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: c) It is the smallest singular value of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) It is the largest singular value of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Let \(A\) be an \(n \times m\) matrix with compact SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). How does the ridge regression solution
    \(\mathbf{x}^{**}\) compare to the least squares solution \(\mathbf{x}^*\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{x}^{**}\) has larger components along the left singular vectors
    corresponding to small singular values.
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{x}^{**}\) has smaller components along the left singular vectors
    corresponding to small singular values.
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{x}^{**}\) is identical to \(\mathbf{x}^*\).
  prefs: []
  type: TYPE_NORMAL
- en: d) None of the above.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** (*Note:* Refers to online supplementary materials.) Let \(A \in \mathbb{R}^{n
    \times n}\) be a square nonsingular matrix with compact SVD \(A = \sum_{j=1}^n
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). Which of the following is true about the
    induced 2-norm of the inverse \(A^{-1}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\|A^{-1}\|_2 = \sigma_1\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\|A^{-1}\|_2 = \sigma_n\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\|A^{-1}\|_2 = \sigma_1^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\|A^{-1}\|_2 = \sigma_n^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text defines the Frobenius norm of an \(n
    \times m\) matrix \(A\) as \(\|A\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{m} a_{ij}^2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text proves that \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r
    \sigma_j^2\) in the Matrix Norms and Singular Values: Truncation Lemma.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The text explains that ridge regression “trades
    off minimizing the fit to the data versus minimizing the norm of the solution,”
    and \(\lambda\) is the parameter that controls this trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The text notes that the ridge regression solution
    “reduces the contributions from the left singular vectors corresponding to small
    singular values.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: d. Justification: The text shows in an example that for a square
    nonsingular matrix \(A\), \(\|A^{-1}\|_2 = \sigma_n^{-1}\), where \(\sigma_n\)
    is the smallest singular value of \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1\. Matrix norms[#](#matrix-norms "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that the Frobenius norm\(\idx{Frobenius norm}\xdi\) of an \(n \times
    m\) matrix \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here we introduce a different notion of matrix norm that has many uses in data
    science (and beyond).
  prefs: []
  type: TYPE_NORMAL
- en: '**Induced norm** The Frobenius norm does not directly relate to \(A\) as a
    representation of a [linear map](https://en.wikipedia.org/wiki/Linear_map). In
    particular, it is desirable in many contexts to quantify how two matrices differ
    in terms of how they act on vectors. For instance, one is often interested in
    bounding quantities of the following form. Let \(B, B'' \in \mathbb{R}^{n \times
    m}\) and let \(\mathbf{x} \in \mathbb{R}^m\) be of unit norm. What can be said
    about \(\|B \mathbf{x} - B'' \mathbf{x}\|\)? Intuitively, what we would like is
    this: if the norm of \(B - B''\) is small then \(B\) is close to \(B''\) as a
    linear map, that is, the vector norm \(\|B \mathbf{x} - B'' \mathbf{x}\|\) is
    small for any unit vector \(\mathbf{x}\). The following definition provides us
    with such a notion. Define the unit sphere \(\mathbb{S}^{m-1} = \{\mathbf{x} \in
    \mathbb{R}^m\,:\,\|\mathbf{x}\| = 1\}\) in \(m\) dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(\(2\)-Norm)** The \(2\)-norm of a matrix\(\idx{2-norm}\xdi\)
    \(A \in \mathbb{R}^{n \times m}\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_2 := \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|}
    = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: The equality in the definition uses the absolute homogeneity of the vector norm.
    Also the definition implicitly uses the *Extreme Value Theorem*. In this case,
    we use the fact that the function \(f(\mathbf{x}) = \|A \mathbf{x}\|\) is continuous
    and the set \(\mathbb{S}^{m-1}\) is closed and bounded to conclude that there
    exists \(\mathbf{x}^* \in \mathbb{S}^{m-1}\) such that \(f(\mathbf{x}^*) \geq
    f(\mathbf{x})\) for all \(\mathbf{x} \in \mathbb{S}^{m-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: The \(2\)-norm of a matrix has many other useful properties. The first four
    below are what makes it a [norm](https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Properties of the \(2\)-Norm)** Let \(A, B \in \mathbb{R}^{n \times
    m}\) and \(\alpha \in \mathbb{R}\). The following hold:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\|A\|_2 \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\|A\|_2 = 0\) if and only if \(A = 0\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\|\alpha A\|_2 = |\alpha| \|A\|_2\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)
  prefs: []
  type: TYPE_NORMAL
- en: e) \(\|A B \|_2 \leq \|A\|_2 \|B\|_2\).
  prefs: []
  type: TYPE_NORMAL
- en: f) \(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\), \(\forall \mathbf{0} \neq
    \mathbf{x} \in \mathbb{R}^m\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* These properties all follow from the definition of the \(2\)-norm
    and the corresponding properties for the vector norm:'
  prefs: []
  type: TYPE_NORMAL
- en: Claims a) and f) are immediate from the definition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For b) note that \(\|A\|_2 = 0\) implies \(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\), so that \(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\). In particular, \(a_{ij} = \mathbf{e}_i^T A \mathbf{e}_j
    = 0, \forall i,j\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For c), d), e), observe that for all \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|, \]\[\|(A+B)\mathbf{x}\|
    = \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\| \leq \|A\|_2
    + \|B\|_2 \]\[ \|(AB)\mathbf{x}\| = \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
    \leq \|A\|_2 \|B\|_2.\]
  prefs: []
  type: TYPE_NORMAL
- en: Then apply the definition of \(2\)-norm. For example, for ©,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\alpha A\|_2 &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha
    A \mathbf{x}\|\\ &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
    &= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\ &= |\alpha|
    \|A\|_2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(|\alpha|\) does not depend on \(\mathbf{x}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In NumPy, the Frobenius norm of a matrix can be computed
    using the default of the function `numpy.linalg.norm` while the induced norm can
    be computed using the same function with [`ord` parameter set to `2`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix norms and SVD** As it turns out, the two notions of matrix norms we
    have introduced admit simple expressions in terms of the singular values of the
    matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Matrix Norms and Singular Values)** \(\idx{matrix norms and singular
    values lemma}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with compact
    SVD'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\). Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|^2_2 = \sigma_{1}^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We will use the notation \(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\).
    Using that the squared Frobenius norm of \(A\) is the sum of the squared norms
    of its columns, we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|^2_F = \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
    = \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell \right\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\mathbf{u}_\ell\)’s are orthonormal, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^m \sum_{\ell=1}^r \sigma_\ell^2 v_{\ell,j}^2 = \sum_{\ell=1}^r
    \sigma_\ell^2 \left(\sum_{j=1}^m v_{\ell,j}^2\right) = \sum_{\ell=1}^r \sigma_\ell^2
    \|\mathbf{v}_{\ell}\|^2 = \sum_{\ell=1}^r \sigma_\ell^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the \(\mathbf{v}_\ell\)’s are also orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: For the second claim, recall that the \(2\)-norm is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_2^2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: We have shown previously that \(\mathbf{v}_1\) solves this problem. Hence \(\|A\|_2^2
    = \|A \mathbf{v}_1\|^2 = \sigma_1^2\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2\. Low-rank approximation[#](#low-rank-approximation "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have defined a notion of distance between matrices, we will consider
    the problem of finding a good approximation to a matrix \(A\) among all matrices
    of rank at most \(k\). We will start with the Frobenius norm, which is easier
    to work with, and we will show later on that the solution is the same under the
    induced norm. The solution to this problem will be familiar. In essence, we will
    re-interpret our solution to the best approximating subspace as a low-rank approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-rank approximation in the Frobenius norm** \(\idx{low-rank approximation}\xdi\)
    From the proof of the *Row Rank Equals Column Rank Lemma*, it follows that a rank-\(r\)
    matrix \(A\) can be written as a sum of \(r\) rank-\(1\) matrices'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: We will now consider the problem of finding a “simpler” approximation to \(A\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(k < r\). Here we measure the quality of this approximation using a matrix
    norm.
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to state our key observation. In words, the best rank-\(k\) approximation
    to \(A\) in Frobenius norm is obtained by projecting the rows of \(A\) onto a
    linear subspace of dimension \(k\). We will come back to how one finds the best
    such subspace below. (*Hint:* We have already solved this problem.)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Projection and Rank-\(k\) Approximation)** \(\idx{projection and
    rank-k approximation lemma}\xdi\) Let \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n
    \times m}\). For any matrix \(B = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)
    of rank \(k \leq \min\{n,m\}\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - B_{\perp}\|_F \leq \|A - B\|_F \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{\perp} \in \mathbb{R}^{n \times m}\) is the matrix of rank at most
    \(k\) obtained as follows. Denote row \(i\) of \(A\), \(B\) and \(B_{\perp}\)
    respectively by \(\boldsymbol{\alpha}_i^T\), \(\mathbf{b}_{i}^T\) and \(\mathbf{b}_{\perp,i}^T\),
    \(i=1,\ldots, n\). Set \(\mathbf{b}_{\perp,i}\) to be the orthogonal projection
    of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z} = \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\).
    \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* The square of the Frobenius norm decomposes as a sum of squared
    row norms. Each term in the sum is minimized by the orthogonal projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By definition of the Frobenius norm, we note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 = \sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'and similarly for \(\|A - B_{\perp}\|_F\). We make two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Because the orthogonal projection of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z}\)
    minimizes the distance to \(\mathcal{Z}\), it follows that term by term \(\|\boldsymbol{\alpha}_i
    - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|\) so
    that
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \|A - B_\perp\|_F^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
    \leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 = \|A - B\|_F^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, because the projections satisfy \(\mathbf{b}_{\perp,i} \in \mathcal{Z}\)
    for all \(i\), \(\mathrm{row}(B_\perp) \subseteq \mathrm{row}(B)\) and, hence,
    the rank of \(B_\perp\) is at most the rank of \(B\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Recall the approximating subspace problem. That is, think of the rows \(\boldsymbol{\alpha}_i^T\)
    of \(A \in \mathbb{R}^{n \times m}\) as a collection of \(n\) data points in \(\mathbb{R}^m\).
    We are looking for a linear subspace \(\mathcal{Z}\) that minimizes \(\sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)
    over all linear subspaces of \(\mathbb{R}^m\) of dimension at most \(k\). By the
    *Projection and Rank-\(k\) Approximation Lemma*, this problem is equivalent to
    finding a matrix \(B\) that minimizes \(\|A - B\|_F\) among all matrices in \(\mathbb{R}^{n
    \times m}\) of rank at most \(k\). Of course we have solved this problem before.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). For \(k < r\), truncate the sum at the
    \(k\)-th term \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). The
    rank of \(A_k\) is exactly \(k\). Indeed, by construction,
  prefs: []
  type: TYPE_NORMAL
- en: the vectors \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) are orthonormal, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: since \(\sigma_j > 0\) for \(j=1,\ldots,k\) and the vectors \(\{\mathbf{v}_j\,:\,j
    = 1,\ldots,k\}\) are orthonormal, \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) spans
    the column space of \(A_k\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have shown before that \(A_k\) is the best approximation to \(A\) among matrices
    of rank at most \(k\) in Frobenius norm. Specifically, the *Greedy Finds Best
    Fit Theorem* implies that, for any matrix \(B \in \mathbb{R}^{n \times m}\) of
    rank at most \(k\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|_F \leq \|A - B\|_F. \]
  prefs: []
  type: TYPE_NORMAL
- en: This result is known as the *Eckart-Young Theorem*\(\idx{Eckart-Young theorem}\xdi\).
    It also holds in the induced \(2\)-norm, as we show next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-rank approximation in the induced norm** We show in this section that
    the same holds in the induced norm. First, some observations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Matrix Norms and Singular Values: Truncation)** Let \(A \in \mathbb{R}^{n
    \times m}\) be a matrix with SVD'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\) and let
    \(A_k\) be the truncation defined above. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|^2_2 = \sigma_{k+1}^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For the first claim, by definition, summing over the columns of \(A
    - A_k\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|^2_F = \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
    = \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j \right\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\mathbf{u}_j\)’s are orthonormal, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^m \sum_{j=k+1}^r \sigma_j^2 v_{j,i}^2 = \sum_{j=k+1}^r \sigma_j^2
    \left(\sum_{i=1}^m v_{j,i}^2\right) = \sum_{j=k+1}^r \sigma_j^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the \(\mathbf{v}_j\)’s are also orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: For the second claim, recall that the induced norm is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|B\|_2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: For any \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\|(A - A_k)\mathbf{x}\right\|^2 = \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j
    (\mathbf{v}_j^T \mathbf{x}) \right\|^2 = \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j,
    \mathbf{x}\rangle^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\sigma_j\)’s are in decreasing order, this is maximized when \(\langle
    \mathbf{v}_j, \mathbf{x}\rangle = 1\) if \(j=k+1\) and \(0\) otherwise. That is,
    we take \(\mathbf{x} = \mathbf{v}_{k+1}\) and the norm is then \(\sigma_{k+1}^2\),
    as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Low-Rank Approximation in the Induced Norm)** \(\idx{low-rank
    approximation in the induced norm theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times
    m}\) be a matrix with SVD'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  prefs: []
  type: TYPE_NORMAL
- en: and let \(A_k\) be the truncation defined above with \(k < r\). For any matrix
    \(B \in \mathbb{R}^{n \times m}\) of rank at most \(k\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|_2 \leq \|A - B\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We know that \(\|A - A_k\|_2^2 = \sigma_{k+1}^2\). So we want
    to lower bound \(\|A - B\|_2^2\) by \(\sigma_{k+1}^2\). For that, we have to find
    an appropriate \(\mathbf{z}\) for any given \(B\) of rank at most \(k\). The idea
    is to take a vector \(\mathbf{z}\) in the intersection of the null space of \(B\)
    and the span of the singular vectors \(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\).
    By the former, the squared norm of \((A - B)\mathbf{z}\) is equal to the squared
    norm of \(A\mathbf{z}\) which lower bounds \(\|A\|_2^2\). By the latter, \(\|A
    \mathbf{z}\|^2\) is at least \(\sigma_{k+1}^2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Rank-Nullity Theorem*, the dimension of \(\mathrm{null}(B)\)
    is at least \(m-k\) so there is a unit vector \(\mathbf{z}\) in the intersection'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: (Prove it!) Then \((A-B)\mathbf{z} = A\mathbf{z}\) since \(\mathbf{z} \in \mathrm{null}(B)\).
    Also since \(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\),
    and therefore orthogonal to \(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\), we have
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|(A-B)\mathbf{z}\|^2 &= \|A\mathbf{z}\|^2\\ &= \left\|\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \left\|\sum_{j=1}^{k+1}
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \sum_{j=1}^{k+1}
    \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1}
    \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &= \sigma_{k+1}^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the previous lemma, \(\sigma_{k+1}^2 = \|A - A_k\|_2\) and we are done. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**An application: Why project?** We return to \(k\)-means clustering and why
    projecting to a lower-dimensional subspace can produce better results. We prove
    a simple inequality that provides some insight. Quoting [BHK, Section 7.5.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To elaborate, suppose we have \(n\) data points in \(d\) dimensions in the form
    of the rows \(\boldsymbol{\alpha}_i^T\), \(i=1\ldots, n\), of matrix \(A \in \mathbb{A}^{n
    \times d}\), where we assume that \(n > d\) and that \(A\) has full column rank.
    Imagine these data points come from an unknown ground-truth \(k\)-clustering assignment
    \(g(i) \in [k]\), \(i = 1,\ldots, n\), with corresponding unknown centers \(\mathbf{c}_j\),
    \(j = 1,\ldots, k\). Let \(C \in \mathbb{R}^{n \times d}\) be the corresponding
    matrix, that is, row \(i\) of \(C\) is \(\mathbf{c}_j^T\) if \(g(i) = j\). The
    \(k\)-means objective of the true clustering is then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
    &= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
    &= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\ &= \|A - C\|_F^2.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(A\) has an SVD \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)
    and for \(k < r\) we have the truncation \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\). It corresponds to projecting each row of \(A\) onto the linear
    subspace spanned by the first \(k\) right singular vectors \(\mathbf{v}_1, \ldots,
    \mathbf{v}_k\). To see this, note that the \(i\)-th row of \(A\) is \(\boldsymbol{\alpha}_i^T
    = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\) and that, because the \(\mathbf{v}_j\)’s
    are linearly independent and in particular \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    is an orthonormal basis of its span, the projection of \(\boldsymbol{\alpha}_i\)
    onto \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\ell=1}^k \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i}
    \mathbf{v}_j,\mathbf{v}_\ell\right\rangle = \sum_{\ell=1}^k \sigma_\ell u_{\ell,i}
    \mathbf{v}_\ell \]
  prefs: []
  type: TYPE_NORMAL
- en: which is the \(i\)-th row of \(A_k\). The \(k\)-means objective of \(A_k\) with
    respect to the ground-truth centers \(\mathbf{c}_j\), \(j=1,\ldots,k\), is \(\|A_k
    - C\|_F^2\).
  prefs: []
  type: TYPE_NORMAL
- en: 'One more observation: the rank of \(C\) is at most \(k\). Indeed, there are
    \(k\) different rows in \(C\) so its row rank is \(k\) if these different rows
    are linearly independent and less than \(k\) otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Why Project)** \(\idx{why project theorem}\xdi\) Let \(A \in
    \mathbb{A}^{n \times d}\) be a matrix and let \(A_k\) be the truncation above.
    For any matrix \(C \in \mathbb{R}^{n \times d}\) of rank \(\leq k\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: Observe that we used different matrix norms on the different sides of the inequality.
    The content of this inequality is the following. The quantity \(\|A_k - C\|_F^2\)
    is the \(k\)-means objective of the projection \(A_k\) with respect to the true
    centers, that is, the sum of the squared distances to the centers. By the *Matrix
    Norms and Singular Values Lemma*, the inequality above gives that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma_j(A - C)\) is the \(j\)-th singular value of \(A - C\). On the
    other hand, by the same lemma, the \(k\)-means objective of the un-projected data
    is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: If the rank of \(A-C\) is much larger than \(k\) and the singular values of
    \(A-C\) decay slowly, then the latter quantity may be much larger. In other words,
    projecting may bring the data points closer to their true centers, potentially
    making it easier to cluster them.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Why Project)* We have shown previously that, for any matrices \(A,
    B \in \mathbb{R}^{n \times m}\), the rank of their sum is less or equal than the
    sum of their ranks, that is, \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
    So the rank of the difference \(A_k - C\) is at most the sum of the ranks'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(A_k - C) \leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the rank of \(A_k\) is \(k\) and the rank of \(C\) is \(\leq
    k\) since it has \(k\) distinct rows. By the *Matrix Norms and Singular Values
    Lemma*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2\. \]
  prefs: []
  type: TYPE_NORMAL
- en: By the triangle inequality for matrix norms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_2 \leq \|A_k - A\|_2 + \|A - C\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Low-Rank Approximation in the Induced Norm Theorem*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A - A_k\|_2 \leq \|A - C\|_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: since \(C\) has rank at most \(k\). Putting these three inequalities together,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A_k - C\|_F^2 \leq 2k (2 \|A - C\|_2)^2 = 8k \|A - C\|_2^2\. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We return to our example with the two Gaussian clusters.
    We use function producing two separate clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We first generate the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In reality, we cannot compute the matrix norms of \(X-C\) and \(X_k-C\) as
    the true centers are not known. But, because this is simulated data, we happen
    to know the truth and we can check the validity of our results in this case. The
    centers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We use [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function to compute the norms from the formulas in the *Matrix Norms and Singular
    Values Lemma*. First, we observe that the singular values of \(X-C\) are decaying
    slowly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png](../Images/23b4327417d560c241971d818c1583cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The \(k\)-means objective with respect to the true centers under the full-dimensional
    data is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'while the square of the top singular value of \(X-C\) is only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we compute the \(k\)-means objective with respect to the true centers
    under the projected one-dimensional data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the applications of SVD
    in recommendation systems. How is it used to predict user preferences? \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about nonnegative matrix factorization
    (NMF) and how it compares to SVD. What are the key differences in their constraints
    and applications? How does NMF handle interpretability in topics like text analysis
    or image processing? Explore some algorithms used to compute NMF. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.3\. Ridge regression[#](#ridge-regression "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we consider what is called Tikhonov regularization\(\idx{Tikhonov regularization}\xdi\),
    an idea that turns out to be useful in overdetermined linear systems, particularly
    when the columns of the matrix \(A\) are linearly dependent or close to linearly
    dependent (which is sometimes referred to as [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\(\idx{multicollinearity}\xdi\)
    in statistics). It trades off minimizing the fit to the data versus minimizing
    the norm of the solution. More precisely, for a parameter \(\lambda > 0\) to be
    chosen, we solve\(\idx{ridge regression}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: The second term is referred to as an \(L_2\)-regularizer\(\idx{L2-regularization}\xdi\).
    Here \(A \in \mathbb{R}^{n\times m}\) with \(n \geq m\) and \(\mathbf{b} \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: To solve this optimization problem, we show that the objective function is strongly
    convex. We then find its unique stationary point. Rewriting the objective in quadratic
    function form
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}) &= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2\\ &= \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x}
    + \mathbf{b}^T \mathbf{b} + \lambda \mathbf{x}^T \mathbf{x}\\ &= \mathbf{x}^T
    (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T
    \mathbf{b}\\ &= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(P = 2 (A^T A + \lambda I_{m \times m})\) is symmetric, \(\mathbf{q}
    = - 2 A^T \mathbf{b}\), and \(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\).
  prefs: []
  type: TYPE_NORMAL
- en: As we previously computed, the Hessian of \(f\) is \(H_f(\mathbf{x})= P\). Now
    comes a key observation. The matrix \(P\) is positive definite whenever \(\lambda
    > 0\). Indeed, for any \(\mathbf{z} \in \mathbb{R}^m\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z} = 2 \|A \mathbf{z}\|_2^2
    + 2 \lambda \|\mathbf{z}\|_2^2 > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mu = 2 \lambda > 0\). Then \(f\) is \(\mu\)-strongly convex. This holds
    whether or not the columns of \(A\) are linearly independent.
  prefs: []
  type: TYPE_NORMAL
- en: The stationary points are easily characterized. Recall that the gradient is
    \(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\). Equating to \(\mathbf{0}\)
    leads to the system
  prefs: []
  type: TYPE_NORMAL
- en: \[ 2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: that is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The matrix in parenthesis is invertible as it is \(1/2\) of the Hessian, which
    is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: '**Connection to SVD** Expressing the solution in terms of a compact SVD \(A
    = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) provides
    some insights into how ridge regression works. Suppose that \(A\) has full column
    rank. That implies that \(V V^T = I_{m \times m}\). Then observe that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (A^T A + \lambda I_{m \times m})^{-1} &= (V \Sigma U^T U \Sigma
    V^T + \lambda I_{m \times m})^{-1}\\ &= (V \Sigma^2 V^T + \lambda I_{m \times
    m})^{-1}\\ &= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\ &= (V [\Sigma^2
    + \lambda I_{m \times m}] V^T)^{-1}\\ &= V (\Sigma^2 + \lambda I_{m \times m})^{-1}
    V^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b} =
    V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b} = V (\Sigma^2
    + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Our predictions are
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{x}^{**} &= U \Sigma V^T V (\Sigma^2 + \lambda I_{m
    \times m})^{-1} \Sigma U^T \mathbf{b}\\ &= U \Sigma (\Sigma^2 + \lambda I_{m \times
    m})^{-1} \Sigma U^T \mathbf{b}\\ &= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2
    + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that the terms in curly brackets are \(< 1\) when \(\lambda > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: Compare this to the unregularized least squares solution, which is obtained
    simply by setting \(\lambda = 0\) above,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{x}^* &= \sum_{j=1}^r \mathbf{u}_j \mathbf{u}_j^T
    \mathbf{b}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The difference is that the regularized solution reduces the contributions from
    the left singular vectors corresponding to small singular values.
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following best describes the Frobenius norm of a matrix
    \(A \in \mathbb{R}^{n \times m}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The maximum singular value of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The square root of the sum of the squares of all entries in \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The maximum absolute row sum of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) The maximum absolute column sum of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) and let \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) be the truncated SVD with \(k < r\). Which of the following is
    true about the Frobenius norm of \(A - A_k\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\|A - A_k\|_F^2 = \sigma_k^2\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** The ridge regression problem is formulated as \(\min_{\mathbf{x} \in
    \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\).
    What is the role of the parameter \(\lambda\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It controls the trade-off between fitting the data and minimizing the norm
    of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: b) It determines the rank of the matrix \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: c) It is the smallest singular value of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) It is the largest singular value of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Let \(A\) be an \(n \times m\) matrix with compact SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). How does the ridge regression solution
    \(\mathbf{x}^{**}\) compare to the least squares solution \(\mathbf{x}^*\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{x}^{**}\) has larger components along the left singular vectors
    corresponding to small singular values.
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{x}^{**}\) has smaller components along the left singular vectors
    corresponding to small singular values.
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{x}^{**}\) is identical to \(\mathbf{x}^*\).
  prefs: []
  type: TYPE_NORMAL
- en: d) None of the above.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** (*Note:* Refers to online supplementary materials.) Let \(A \in \mathbb{R}^{n
    \times n}\) be a square nonsingular matrix with compact SVD \(A = \sum_{j=1}^n
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). Which of the following is true about the
    induced 2-norm of the inverse \(A^{-1}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\|A^{-1}\|_2 = \sigma_1\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\|A^{-1}\|_2 = \sigma_n\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\|A^{-1}\|_2 = \sigma_1^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\|A^{-1}\|_2 = \sigma_n^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text defines the Frobenius norm of an \(n
    \times m\) matrix \(A\) as \(\|A\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{m} a_{ij}^2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text proves that \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r
    \sigma_j^2\) in the Matrix Norms and Singular Values: Truncation Lemma.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The text explains that ridge regression “trades
    off minimizing the fit to the data versus minimizing the norm of the solution,”
    and \(\lambda\) is the parameter that controls this trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The text notes that the ridge regression solution
    “reduces the contributions from the left singular vectors corresponding to small
    singular values.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: d. Justification: The text shows in an example that for a square
    nonsingular matrix \(A\), \(\|A^{-1}\|_2 = \sigma_n^{-1}\), where \(\sigma_n\)
    is the smallest singular value of \(A\).'
  prefs: []
  type: TYPE_NORMAL
