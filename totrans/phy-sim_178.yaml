- en: Conjugate Gradient Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://phys-sim-book.github.io/lec33.3-conjugate_gradient.html](https://phys-sim-book.github.io/lec33.3-conjugate_gradient.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
  prefs: []
  type: TYPE_NORMAL
- en: The Conjugate Gradient (CG) method is a powerful iterative algorithm for solving
    large, sparse linear systems of the form Ax=b, where A is symmetric positive definite
    (SPD). Unlike general iterative methods such as Jacobi and Gauss-Seidel, CG is
    specifically designed for SPD matrices and has become fundamental in scientific
    computing and numerical simulation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Formulation as Quadratic Optimization](#formulation-as-quadratic-optimization)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The conjugate gradient method can be elegantly understood as an optimization
    algorithm for minimizing the quadratic function: f(x)=21​xTAx−bTx,(34.3.1) where
    A is SPD. The unique global minimizer of f(x) is precisely the solution to Ax=b.'
  prefs: []
  type: TYPE_NORMAL
- en: The classical steepest descent method updates x along the negative gradient
    direction −∇f(x)=b−Ax, but this approach can suffer from slow convergence, particularly
    when A is ill-conditioned. The CG method overcomes this limitation by searching
    along a carefully chosen sequence of directions p(k) that are A-conjugate (or
    A-orthogonal), meaning p(i)TAp(j)=0 for i=j. This conjugacy property ensures
    that progress made along one direction is never undone by subsequent steps, and
    the minimization along each direction becomes independent of the others.
  prefs: []
  type: TYPE_NORMAL
- en: '[Line Search](#line-search)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a sequence of conjugate directions p(0),p(1),p(2),…, the problem of minimizing
    the quadratic function reduces to finding optimal step sizes α(i) such that ∑i=0n−1​α(i)p(i)
    closely approximates the solution x∗.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward approach is **greedy line search**: starting from
    an initial point x(0), we select a search direction p(0) and then minimize f(x(0)+α(0)p(0))
    with respect to α(0). For quadratic functions, this optimization has a simple
    closed-form solution that avoids matrix inversion: α(0)=p(0)TAp(0)p(0)T(b−Ax(0))​.
    The intuition is clear: we start at x(0), choose a direction p(0), and move along
    this direction until the objective function is minimized. While this may not reach
    the global minimum in one step, it guarantees progress toward the optimal solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This procedure is then repeated iteratively: at the new point x(1)=x(0)+α(0)p(0),
    we select the next direction p(1), compute the corresponding step size α(1), and
    continue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The general iteration process can be summarized as: α(i)x(i+1)r(i+1)​=p(i)TAp(i)p(i)Tr(i)​,=x(i)+α(i)p(i),=r(i)−α(i)Ap(i)​(34.3.2)
    where p(0),p(1),p(2),… are the search directions and r(i)=b−Ax(i) is the residual
    at step i. Note that the residual can be updated without recomputing Ax(i+1) from
    scratch: r(i+1)=b−Ax(i+1)=b−A(x(i)+α(i)p(i))=r(i)−α(i)Ap(i). In practice, when
    ∥r(i+1)∥ becomes sufficiently small (typically below a prescribed tolerance),
    the iteration process can be terminated early to achieve better computational
    efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Conjugate Directions](#conjugate-directions)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The choice of search directions is crucial for the method's performance. If
    the directions p(0),p(1),p(2),… are poorly chosen, convergence will be slow. In
    particular, gradient descent (which uses the steepest descent directions) exhibits
    slow convergence for ill-conditioned matrices A.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, if we choose the directions to be mutually A-conjugate: p(i)TAp(j)=0,∀i=j,(34.3.3)
    the algorithm achieves remarkable efficiency: there is no "zigzagging" behavior,
    and we obtain the exact solution in at most n steps, where n is the dimension
    of the system:'
  prefs: []
  type: TYPE_NORMAL
- en: Without loss of generality, assume x(0)=0 and set p(0) to be the initial residual.
    Since x(0)=0, the gradient at x(0) is Ax(0)−b=−b, so we set p(0)=b. The remaining
    directions will be constructed to be A-conjugate to all previous directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let r(k) denote the residual at the k-th step: r(k)=b−Ax(k).(34.3.4) Note that
    r(k) equals the negative gradient of f at x(k), so standard gradient descent would
    move in the direction r(k).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To construct conjugate directions, we require that each new search direction
    p(k) be built from the current residual r(k) while being A-conjugate to all previous
    search directions. We start with the negative gradient r(k) and orthogonalize
    it w.r.t. A against all previous search directions p(0),p(1),…,p(k−1) using the
    Gram–Schmidt process: p(k)=r(k)−i=0∑k−1​p(i)TAp(i)p(i)TAr(k)​p(i).(34.3.5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Algorithmic Simplification](#algorithmic-simplification)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The expressions above can be significantly simplified, leading to the elegant
    final form of the conjugate gradient algorithm. The key insight lies in proving
    two fundamental orthogonality relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '**Orthogonality Properties.** We first establish that the following orthogonality
    relations hold: p(i)Tr(j)=0,∀i<j,(34.3.6) r(i)Tr(j)=0,∀i=j.(34.3.7)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof Sketch.** From the residual update formula (Equation [(34.3.4)](#eq:lec31:residual)),
    we can write recursively: rj​=r(j−1)−α(j−1)Ap(j−1)=r(i)−k=i∑j−1​α(k)Ap(k). Taking
    the dot product with p(i) on both sides: p(i)Tr(j)=p(i)Tr(i)−k=i∑j−1​αk​pi​TApk​.
    By the A-conjugacy property (Equation [(34.3.3)](#eq:lec31:A-conjugate)) and the
    step size formula α(i)=p(i)TAp(i)p(i)Tr(i)​, we can verify that p(i)Tr(j)=0 for
    i<j.'
  prefs: []
  type: TYPE_NORMAL
- en: The second orthogonality relation r(i)Tr(j)=0 follows from the fact that each
    residual r(j) lies in the span of the search directions {p(0),p(1),…,p(j−1)} by
    construction, and these directions are built from previous residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified Formulas.** Using these orthogonality properties, the conjugate
    direction formula (Equation [(34.3.5)](#eq:lec31:conjugate_direction)) simplifies
    dramatically. Since p(i)Tr(k)=0 for i<k, we have: p(k)Tr(k)=r(k)Tr(k).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows us to simplify the step size calculation: α(k)=p(k)TAp(k)r(k)Tr(k)​.(34.3.8)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the conjugate direction update, we can show that only the most recent direction
    matters. Using the residual update formula and the orthogonality properties: p(k+1)​=r(k+1)−i=0∑k​p(i)TAp(i)p(i)TAr(k+1)​p(i)=r(k+1)−i=0∑k​p(i)TAp(i)r(k+1)TAp(i)​p(i)=r(k+1)−i=0∑k​α(i)p(i)TAp(i)r(k+1)T(r(i)−r(i+1))​p(i).​'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the orthogonality relations (Equations [(34.3.7)](#eq:lec31:lemma2)
    and [(34.3.8)](#eq:lec31:simple_step_size)), this simplifies to: p(k+1)​=r(k+1)+i=0∑k​α(i)p(i)TAp(i)r(k+1)Tr(i+1)​p(i)=r(k+1)+i=0∑k​r(i)Tr(i)r(k+1)Tr(i+1)​p(i)=r(k+1)+r(k)Tr(k)r(k+1)Tr(k+1)​p(k).​'
  prefs: []
  type: TYPE_NORMAL
- en: This remarkable simplification shows that we only need to retain the most recent
    search direction p(k) to compute the next one p(k+1).
  prefs: []
  type: TYPE_NORMAL
- en: '**Final Algorithm.** The simplified CG algorithm is summarized in [Algorithm
    34.3.1](#alg:lec31:cg):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm 34.3.1 (The Conjugate Gradient Method).** ![](../Images/bc2dc8215cbe71e4a5fbcb076232da21.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Preconditioning](#preconditioning)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The convergence rate of the conjugate gradient method is fundamentally determined
    by the condition number of matrix A. When A is ill-conditioned (i.e., has a large
    condition number), CG may converge slowly, requiring many iterations to reach
    acceptable accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Preconditioning** is a crucial technique for accelerating convergence by
    transforming the original system into an equivalent one with more favorable spectral
    properties. The idea is to solve a modified system: M−1Ax=M−1b, where M is a carefully
    chosen **preconditioner** matrix that approximates A but is much easier to invert.'
  prefs: []
  type: TYPE_NORMAL
- en: A simple yet effective choice is the **diagonal (Jacobi) preconditioner**, where
    M=diag(A) contains only the diagonal entries of A. The preconditioned CG algorithm
    then solves the transformed system while maintaining the essential structure of
    the original algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, preconditioning amounts to scaling the residuals at each iteration
    by M−1, which is computationally inexpensive when M is diagonal. The preconditioned
    CG algorithm follows the same iterative structure as standard CG, but operates
    on the preconditioned residuals, often achieving significantly faster convergence
    for ill-conditioned problems.
  prefs: []
  type: TYPE_NORMAL
