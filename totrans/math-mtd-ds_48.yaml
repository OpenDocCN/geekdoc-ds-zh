- en: '6.4\. Modeling more complex dependencies 2: marginalizing out an unobserved
    variable#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html](https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this section, we move on to the second technique for constructing joint
    distributions from simpler building blocks: marginalizing out an unobserved random
    variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1\. Mixtures[#](#mixtures "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mixtures\(\idx{mixture}\xdi\) are a natural way to define probability distributions.
    The basic idea is to consider a pair of random vectors \((\bX,\bY)\) and assume
    that \(\bY\) is unobserved. The effect on the observed vector \(\bX\) is that
    \(\bY\) is marginalized out. Indeed, by the law of total probability, for any
    \(\bx \in \S_\bX\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} p_\bX(\bx) &= \P[\bX = \bx]\\ &= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by]
    \,\P[\bY=\by]\\ &= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by) \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'where we used that the events \(\{\bY=\by\}\), \(\by \in \S_\bY\), form a partition
    of the probability space. We interpret this equation as defining \(p_\bX(\bx)\)
    as a convex combination – a mixture – of the distributions \(p_{\bX|\bY}(\bx|\by)\),
    \(\by \in \S_\bY\), with mixing weights \(p_\bY(\by)\). In general, we need to
    specify the full conditional probability distribution (CPD): \(p_{\bX|\bY}(\bx|\by),
    \forall \bx \in \S_{\bX}, \by \in \S_\bY\). But assuming that the mixing weights
    and/or CPD come from parametric families can help reduce the complexity of the
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: That can be represented in a digraph with a directed edge from a vertex for
    \(\mathbf{Y}\) to a vertex for \(\mathbf{X}\). Further, we let the vertex for
    \(\mathbf{X}\) be shaded to indicate that it is observed, while the vertex for
    \(\mathbf{Y}\) is not shaded to indicate that it is not. Mathematically, that
    corresponds to applying the law of total probability as we did previously.
  prefs: []
  type: TYPE_NORMAL
- en: '![A mixture](../Images/d7c8f87d42f669398ac57f92c0a43388.png)'
  prefs: []
  type: TYPE_IMG
- en: In the parametric context, this gives rise to a fruitful approach to expanding
    distribution families. Suppose \(\{p_{\btheta}:\btheta \in \Theta\}\) is a parametric
    family of distributions. Let \(K \geq 2\), \(\btheta_1, \ldots, \btheta_K \in
    \Theta\) and \(\bpi = (\pi_1,\ldots,\pi_K) \in \Delta_K\). Suppose \(Y \sim \mathrm{Cat}(\bpi)\)
    and that the conditional distributions satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{\bX|Y}(\bx|i) = p_{\btheta_i}(\bx). \]
  prefs: []
  type: TYPE_NORMAL
- en: We write this as \(\bX|\{Y=i\} \sim p_{\btheta_i}\). Then we obtain the mixture
    model
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{\bX}(\bx) = \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i) = \sum_{i=1}^K \pi_i
    p_{\btheta_i}(\bx). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Mixture of Multinomials)** Let \(n, m , K \geq 1\), \(\bpi
    \in \Delta_K\) and, for \(i=1,\ldots,K\), \(\mathbf{p}_i = (p_{i1},\ldots,p_{im})
    \in \Delta_m\). Suppose that \(Y \sim \mathrm{Cat}(\bpi)\) and that the conditional
    distributions are'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i). \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(\bX\) is a mixture of multinomials. Its distribution is then
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m
    p_{ij}^{x_j}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Next is an important continuous example.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Gaussian mixture model)** \(\idx{Gaussian mixture model}\xdi\)
    For \(i=1,\ldots,K\), let \(\bmu_i\) and \(\bSigma_i\) be the mean and covariance
    matrix of a multivariate Gaussian. Let \(\bpi \in \Delta_K\). A Gaussian Mixture
    Model (GMM) is obtained as follows: take \(Y \sim \mathrm{Cat}(\bpi)\) and'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i). \]
  prefs: []
  type: TYPE_NORMAL
- en: Its probability density function (PDF) takes the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We plot the density for means \(\bmu_1 = (-2,-2)\) and
    \(\bmu_2 = (2,2)\) and covariance matrices'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{and} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1
    \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma_1 = 1.5\), \(\sigma_2 = 0.5\) and \(\rho = -0.75\). The mixing
    weights are \(\pi_1 = 0.25\) and \(\pi_2 = 0.75\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]</details> ![../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png](../Images/810f47a1332c13f0595b65b28e44ab63.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In NumPy, as we have seen before, the module [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html)
    also provides a way to sample from mixture models by using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we consider mixtures of multivariate Gaussians. We chage the notation
    slightly to track Python’s indexing. For \(i=0,1\), we have a mean \(\bmu_i \in
    \mathbb{R}^d\) and a positive definite covariance matrix \(\bSigma_i \in \mathbb{R}^{d
    \times d}\). We also have mixture weights \(\phi_0, \phi_1 \in (0,1)\) such that
    \(\phi_0 + \phi_1 = 1\). Suppose we want to generate a total of \(n\) samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each sample \(j=1,\ldots, n\), independently from everything else:'
  prefs: []
  type: TYPE_NORMAL
- en: We first pick a component \(i \in \{0,1\}\) at random according to the mixture
    weights, that is, \(i=0\) is chosen with probability \(\phi_0\) and \(i=1\) is
    chosen with probability \(\phi_1\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generate a sample \(\bX_j = (X_{j,1},\ldots,X_{j,d})\) according to a multivariate
    Gaussian with mean \(\bmu_i\) and covariance \(\bSigma_i\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is straightforward to implement by using again [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html)
    to choose the component of each sample and [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html)
    to generate multivariate Gaussians. For convenience, we will stack the means and
    covariances into one array with a new dimension. So, for instance, the covariance
    matrices will now be in a 3d-array, that is, an array with \(3\) indices. The
    first index corresponds to the component (here \(0\) or \(1\)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Three matrices ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Three matrices](../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Three matrices stacked into a 3d-array ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Three matrices stacked into a tensor](../Images/a61cc745a3c58639bf7340b2af821416.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: The code is the following. It returns an `d` by `n` array `X`, where each row
    is a sample from a 2-component Gaussian mixture.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Let us try it with following parameters. We first define
    the covariance matrices and show what happens when they are stacked into a 3d
    array (as is done within `gmm2`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then we define the rest of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png](../Images/22ec3b142976b6b20a0185e9e997fea9.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '6.4.2\. Example: Mixtures of multivariate Bernoullis and the EM algorithm[#](#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let \(\mathcal{C} = \{1, \ldots, K\}\) be a collection of classes. Let \(C\)
    be a random variable taking values in \(\mathcal{C}\) and, for \(m=1, \ldots,
    M\), let \(X_i\) take values in \(\{0,1\}\). Define \(\pi_k = \P[C = k]\) and
    \(p_{k,m} = \P[X_m = 1|C = k]\) for \(m = 1,\ldots, M\). We denote by \(\bX =
    (X_1, \ldots, X_M)\) the corresponding vector of \(X_i\)’s and assume that the
    entries are conditionally independent given \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: However, we assume this time that \(C\) itself is *not observed*. So the resulting
    joint distribution is the mixture
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[\bX = \bx] &= \sum_{k=1}^K \P[C = k, \bX = \bx]\\ &= \sum_{k=1}^K
    \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, this is the same are the Naive Bayes model, except that \(C\) is
    not observed and therefore is not shaded.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mixture of multivariate Bernoullis](../Images/c51352d9d6ab19d2f9e73f898c25b648.png)'
  prefs: []
  type: TYPE_IMG
- en: This type of model is useful in particular for clustering tasks, where the \(c_k\)s
    can be thought of as different clusters. Similarly to what we did in the previous
    section, our goal is to infer the parameters from samples and then predict the
    class of an old or new sample given its features. The main – substantial – difference
    is that the true labels of the samples are not observed. As we will see, that
    complicates the task considerably.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model fitting** We first fit the model from training data \(\{\bx_i\}_{i=1}^n\).
    Recall that the corresponding class labels \(c_i\)s are not observed. In this
    type of model, they are referred to as hidden or latent variables and we will
    come back to their inference below.'
  prefs: []
  type: TYPE_NORMAL
- en: We would like to use maximum likelihood estimation, that is, maximize the probability
    of observing the data
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\}) = \prod_{i=1}^n \left( \sum_{k=1}^K
    \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we assume that the samples are independent and identically distributed.
    Consider the negative log-likelihood (NLL)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Already, we see that things are potentially more difficult than they were in
    the supervised (or fully observed) case. The NLL does not decompose into a sum
    of terms depending on different sets of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, one could fall back on the field of optimization and use a gradient-based
    method to minimize the NLL. Indeed that is an option, although note that one must
    be careful to account for the constrained nature of the problem (i.e., the parameters
    sum to one). There is a vast array of constrained optimization techniques suited
    for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Instead a more popular approach in this context, the EM algorithm, is based
    on the general principle of majorization-minimization, which we have encountered
    implicitly in the \(k\)-means algorithm and the convergence proof of gradient
    descent in the smooth case. We detail this important principle in the next subsection
    before returning to model fitting in mixtures.
  prefs: []
  type: TYPE_NORMAL
- en: '**Majorization-minimization** \(\idx{majorization-minimization}\xdi\) Here
    is a deceptively simple, yet powerful observation. Suppose we want to minimize
    a function \(f : \mathbb{R}^d \to \mathbb{R}\). Finding a local minimum of \(f\)
    may not be easy. But imagine that for each \(\mathbf{x} \in \mathbb{R}^d\) we
    have a surrogate function \(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\) that
    (1) dominates \(f\) in the following sense'
  prefs: []
  type: TYPE_NORMAL
- en: \[ U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in
    \mathbb{R}^d \]
  prefs: []
  type: TYPE_NORMAL
- en: and (2) equals \(f\) at \(\mathbf{x}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}). \]
  prefs: []
  type: TYPE_NORMAL
- en: We say that \(U_\mathbf{x}\) majorizes \(f\) at \(\mathbf{x}\). Then we prove
    in the next lemma that \(U_\mathbf{x}\) can be used to make progress towards minimizing
    \(f\), that is, find a point \(\mathbf{x}'\) such that \(f(\mathbf{x}') \leq f(\mathbf{x})\).
    If in addition \(U_\mathbf{x}\) is easier to minimize than \(f\) itself, say because
    an explicit minimum can be computed, then this observation naturally leads to
    an iterative algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![A majorizing function (with help from ChatGPT; inspired by Source)](../Images/c05a4e1add7058b646f07a53f61281a2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**LEMMA** **(Majorization-Minimization)** \(\idx{majorization-minimization
    lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) and suppose \(U_{\mathbf{x}}\)
    majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\) be a global minimum of
    \(U_\mathbf{x}\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}') \leq f(\mathbf{x}). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x})
    = f(\mathbf{x}), \]
  prefs: []
  type: TYPE_NORMAL
- en: where the first inequality follows from the domination property of \(U_\mathbf{x}\),
    the second inequality follows from the fact that \(\mathbf{x}'\) is a global minimum
    of \(U_\mathbf{x}\) and the equality follows from the fact that \(U_{\mathbf{x}}\)
    equals \(f\) at \(\mathbf{x}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We have already encountered this idea.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Minimizing a smooth function)** Let \(f : \mathbb{R}^d \to
    \mathbb{R}\) be \(L\)-smooth. By the *Quadratic Bound for Smooth Functions*, for
    all \(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\) it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{z}) \leq U_{\mathbf{x}}(\mathbf{z}) := f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{z}
    - \mathbf{x}) + \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: By showing that \(U_{\mathbf{x}}\) is minimized at \(\mathbf{z} = \mathbf{x}
    - (1/L)\nabla f(\mathbf{x})\), we previously obtained the descent guarantee
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x} - (1/L)\nabla f(\mathbf{x})) \leq f(\mathbf{x}) - \frac{1}{2
    L} \|\nabla f(\mathbf{x})\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for gradient descent, which played a central role in the analysis of its convergence\(\idx{convergence
    analysis}\xdi\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(\(k\)-means)** \(\idx{Lloyd''s algorithm}\xdi\) Let \(\mathbf{x}_1,\ldots,\mathbf{x}_n\)
    be \(n\) vectors in \(\mathbb{R}^d\). One way to formulate the \(k\)-means clustering
    problem is as the minimization of'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: over the centers \(\bmu_1,\ldots,\bmu_K\), where recall that \([K] = \{1,\ldots,K\}\).
    For fixed \(\bmu_1,\ldots,\bmu_K\) and \(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\),
    define
  prefs: []
  type: TYPE_NORMAL
- en: \[ c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in
    [K]\right\}, \quad i=1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \|\mathbf{x}_i
    - \blambda_{c_\mathbf{m}(i)}\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\). That is, we fix the optimal
    cluster assignments under \(\bmu_1,\ldots,\bmu_K\) and then vary the centers.
  prefs: []
  type: TYPE_NORMAL
- en: We claim that \(U_\mathbf{m}\) is majorizing \(f\) at \(\bmu_1,\ldots,\bmu_K\).
    Indeed
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i
    - \blambda_j\|^2 \leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
    = U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 = \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2 = U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover \(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\) is easy to minimize.
    We showed previously that the optimal representatives are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(C_j = \{i : c_\mathbf{m}(i) = j\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: The *Majorization-Minimization Lemma* implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\bmu_1',\ldots,\bmu_K') \leq f(\bmu_1,\ldots,\bmu_K). \]
  prefs: []
  type: TYPE_NORMAL
- en: This argument is equivalent to our previous analysis of the \(k\)-means algorithm.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model assumes a fixed
    number of clusters. Ask your favorite AI chatbot to discuss Bayesian nonparametric
    extensions of this model, such as the Dirichlet process mixture model, which can
    automatically infer the number of clusters from the data. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**EM algorithm** The [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\(\idx{EM
    algorithm}\xdi\) is an instantiation of the majorization-minimization principle
    that applies widely to parameter estimation of mixtures. Here we focus on the
    mixture of multivariate Bernoullis.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the objective to be minimized is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the notation and highlight the general idea, we let \(\btheta =
    (\bpi, \{\bp_k\})\), denote by \(\Theta\) the set of allowed values for \(\btheta\),
    and use \(\P_{\btheta}\) to indicate that probabilities are computed under the
    parameters \(\btheta\). We also return to the description of the model in terms
    of the unobserved latent variables \(\{C_i\}\). That is, we write the NLL as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i
    = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\ &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To derive a majorizing function, we use the convexity of the negative logarithm.
    Indeed
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial z}[- \log z] = - \frac{1}{z} \quad \text{and} \quad
    \frac{\partial^2}{\partial^2 z}[- \log z] = \frac{1}{z^2} > 0, \quad \forall z
    > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The first step of the construction is not obvious – it just works. For each
    \(i=1,\ldots,n\), we let \(r_{k,i}^{\btheta}\), \(k=1,\ldots,K\), be a strictly
    positive probability distribution over \([K]\). In other words, it defines a convex
    combination for every \(i\). Then we use convexity to obtain the upper bound
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\tilde\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K
    r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right), \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: which holds for any \(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\).
  prefs: []
  type: TYPE_NORMAL
- en: We choose
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i] \]
  prefs: []
  type: TYPE_NORMAL
- en: (which for the time being we assume is strictly positive) and we denote the
    right-hand side of the inequality by \(Q_{n}(\tilde\btheta|\btheta)\) (as a function
    of \(\tilde\btheta\)).
  prefs: []
  type: TYPE_NORMAL
- en: We make two observations.
  prefs: []
  type: TYPE_NORMAL
- en: '1- *Dominating property*: For any \(\tilde\btheta \in \Theta\), the inequality
    above implies immediately that \(L_n(\tilde\btheta) \leq Q_n(\tilde\btheta|\btheta)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '2- *Equality at \(\btheta\)*: At \(\tilde\btheta = \btheta\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Q_n(\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i
    = k | \bX_i = \bx_i] \P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\btheta}[\bX_i = \bx_i]\\
    &= - \sum_{i=1}^n \log \P_{\btheta}[\bX_i = \bx_i]\\ &= L_n(\btheta). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The two properties above show that \(Q_n(\tilde\btheta|\btheta)\), as a function
    of \(\tilde\btheta\), majorizes \(L_n\) at \(\btheta\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(EM Guarantee)** \(\idx{EM guarantee}\xdi\) Let \(\btheta^*\) be
    a global minimizer of \(Q_n(\tilde\btheta|\btheta)\) as a function of \(\tilde\btheta\),
    provided it exists. Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_n(\btheta^*) \leq L_n(\btheta). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The result follows directly from the *Majorization-Minimization Lemma*.
    \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: What have we gained from this? As we mentioned before, using the *Majorization-Minimization
    Lemma* makes sense if \(Q_n\) is easier to minimize than \(L_n\) itself. Let us
    see why that is the case here.
  prefs: []
  type: TYPE_NORMAL
- en: '*E Step:* The function \(Q_n\) naturally decomposes into two terms'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Q_n(\tilde\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k] + \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(r_{k,i}^{\btheta}\) depends on \(\btheta\) *but not \(\tilde\btheta\)*,
    the second term is irrelevant to the opimization with respect to \(\tilde\btheta\).
  prefs: []
  type: TYPE_NORMAL
- en: The first term above can be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} & - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k}
    \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
    &= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M
    [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta})
    \log (1-\tilde{p}_{k,m})], \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we defined, for \(k=1,\ldots,K\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_{k,m}^{\btheta} = \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta} \quad \text{and}
    \quad \eta_k^{\btheta} = \sum_{i=1}^n r_{k,i}^{\btheta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here comes the key observation: this last expression is essentially the same
    as the NLL for the fully observed Naive Bayes model, except that the terms \(\mathbf{1}_{\{c_i
    = k\}}\) are replaced by \(r_{k,i}^{\btheta}\). If \(\btheta\) is our current
    estimate of the parameters, then the quantity \(r_{k,i}^{\btheta} = \P_{\btheta}[C_i
    = k|\bX_i = \bx_i]\) is our estimate – under the current parameter \(\btheta\)
    – of the probability that the sample \(\bx_i\) comes from cluster \(k\). We have
    previously computed \(r_{k,i}^{\btheta}\) for prediction under the Naive Bayes
    model. We showed there that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^{\btheta} = \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'which in this new context is referred to as the responsibility that cluster
    \(k\) takes for data point \(i\). So we can interpret the expression above as
    follows: the variables \(\mathbf{1}_{\{c_i = k\}}\) are not observed here, but
    we have estimated their conditional probability distribution given the observed
    data \(\{\bx_i\}\), and we are taking an expectation with respect to that distribution
    instead.'
  prefs: []
  type: TYPE_NORMAL
- en: The “E” in E Step (and EM) stands for “expectation”, which refers to using a
    surrogate function that is essentially an expected NLL.
  prefs: []
  type: TYPE_NORMAL
- en: '*M Step:* In any case, from a practical point of view, minimizing \(Q_n(\tilde\btheta|\btheta)\)
    over \(\tilde\btheta\) turns out to be a variant of fitting a Naive Bayes model
    – and the upshot to all this is that there is a straightforward formula for that!
    Recall that this happens because, the NLL in the Naive Bayes model decomposes:
    it naturally breaks up into terms that depend on separate sets of parameters,
    each of which can be optimized with a closed-form expression. The same happens
    with \(Q_n\) as should be clear from the derivation.'
  prefs: []
  type: TYPE_NORMAL
- en: Adapting our previous calculations for fitting a Naive Bayes model, we get that
    \(Q_n(\tilde\btheta|\btheta)\) is minimized at
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_k^* = \frac{\eta_k^{\btheta}}{n} \quad \text{and} \quad p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
    \quad \forall k \in [K], m \in [M]. \]
  prefs: []
  type: TYPE_NORMAL
- en: We used the fact that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sum_{k=1}^K \eta_k^{\btheta} &= \sum_{k=1}^K \sum_{i=1}^n
    r_{k,i}^{\btheta}\\ &= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i =
    \bx_i]\\ &= \sum_{i=1}^n 1\\ &= n, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: since the conditional probability \(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\) adds
    up to one when summed over \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: The “M” in M Step (and EM) stands for maximization, which here turns into minimization
    because of the use of the NLL.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the EM algorithm works as follows in this case. Assume we have
    data points \(\{\bx_i\}_{i=1}^n\), that we have fixed \(K\) and that we have some
    initial parameter estimate \(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\) with
    strictly positive \(\pi_k^0\)s and \(p_{k,m}^0\)s. For \(t = 0,1,\ldots, T-1\)
    we compute for all \(i \in [n]\), \(k \in [K]\), and \(m \in [M]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \quad \text{(E Step)} \]\[ \eta_{k,m}^t = \sum_{i=1}^n x_{i,m} r_{k,i}^t \quad
    \text{and} \quad \eta_k^t = \sum_{i=1}^n r_{k,i}^t, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_k^{t+1} = \frac{\eta_k^t}{n} \quad \text{and} \quad p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
    \quad \text{(M Step)} \]
  prefs: []
  type: TYPE_NORMAL
- en: Provided \(\sum_{i=1}^n x_{i,m} > 0\) for all \(m\), the \(\eta_{k,m}^t\)s and
    \(\eta_k^t\)s remain positive for all \(t\) and the algorithm is well-defined.
    The *EM Guarantee* stipulates that the NLL cannot deteriorate, although note that
    it does not guarantee convergence to a global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: We implement the EM algorithm for mixtures of multivariate Bernoullis. For this
    purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility
    of using Laplace smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We implement the E and M Step next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We test the algorithm on a very simple dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We compute the probability that the vector \((0, 0, 1)\) is in each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**CHAT & LEARN** The EM algorithm can sometimes get stuck in local optima.
    Ask your favorite AI chatbot to discuss strategies for initializing the EM algorithm
    to avoid this issue, such as using multiple random restarts or using the k-means
    algorithm for initialization. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3\. Clustering handwritten digits[#](#clustering-handwritten-digits "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give a more involved example, we use the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database) again:'
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database (Modified National Institute of Standards and Technology
    database) is a large database of handwritten digits that is commonly used for
    training various image processing systems. The database is also widely used for
    training and testing in the field of machine learning. It was created by “re-mixing”
    the samples from NIST’s original datasets. The creators felt that since NIST’s
    training dataset was taken from American Census Bureau employees, while the testing
    dataset was taken from American high school students, it was not well-suited for
    machine learning experiments. Furthermore, the black and white images from NIST
    were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which
    introduced grayscale levels. The MNIST database contains 60,000 training images
    and 10,000 testing images. Half of the training set and half of the test set were
    taken from NIST’s training dataset, while the other half of the training set and
    the other half of the test set were taken from NIST’s testing dataset.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Figure:** MNIST sample images ([Source](https://commons.wikimedia.org/wiki/File:MnistExamples.png))'
  prefs: []
  type: TYPE_NORMAL
- en: '![MNIST sample images](../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We load it from PyTorch. The data can be accessed with
    [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html).
    The [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html)
    below removes the color dimension in the image, which is grayscale. The [`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    converts the PyTorch tensors into NumPy arrays. See [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    for details on the data loading. We will say more about PyTorch in a later chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: We turn the grayscale images into a black-and-white images by rounding the pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There are two common ways to write a \(2\). Let’s see if a mixture of multivariate
    Bernoullis can find them. We extract the images labelled \(2\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first image is the following.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]</details> ![../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png](../Images/e8eea00af5868fb166365334b2072575.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we transform the images into vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We run the algorithm with \(2\) clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Uh-oh. Something went wrong. We encountered a numerical issue, underflow, which
    we discussed briefly previously. To confirm this, we run the code again but ask
    Python to warn us about it using [`numpy.seterr`](https://numpy.org/doc/stable/reference/generated/numpy.seterr.html).
    (By default, warnings are turned off in the book, but they can be reactivated
    using [`warnings.resetwarnings`](https://docs.python.org/3/library/warnings.html#warnings.resetwarnings).)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: When we compute the responsibilities
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: we first compute the negative logarithm of each term in the numerator as we
    did in the Naive Bayes case. But then we apply the function \(e^{-x}\), because
    this time we are not simply computing an optimal score. When all scores are high,
    this last step may result in underflow, that is, produces numbers so small that
    they get rounded down to zero by NumPy. Then the ratio defining `r_k` is not well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with this, we introduce a technique called the log-sum-exp trick\(\idx{log-sum-exp
    trick}\xdi\) (with some help from ChatGPT). Consider the computation of a function
    of \(\mathbf{a} = (a_1, \ldots, a_n)\) of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right). \]
  prefs: []
  type: TYPE_NORMAL
- en: When the \(a_i\) values are large positive numbers, the terms \(e^{-a_i}\) can
    be so small that they underflow to zero. To counter this, the log-sum-exp trick
    involves a shift to bring these terms into a more favorable numerical range.
  prefs: []
  type: TYPE_NORMAL
- en: 'It proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the minimum value \(M\) among the \(a_i\)s
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ M = \min\{a_1, a_2, \ldots, a_n\}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Subtract \(M\) from each \(a_i\) before exponentiation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \log \left( \sum_{i=1}^{n} e^{-a_i} \right) = \log \left( e^{-M} \sum_{i=1}^{n}
    e^{- (a_i - M)} \right). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rewrite using log properties
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Why does this help with underflow? By subtracting \(M\), the smallest value
    in the set, from each \(a_i\): (i) the largest term in \(\{e^{-(a_i - M)} : i
    = 1,\ldots,n\}\) becomes \(e^0 = 1\); and (ii) all other terms are between 0 and
    1, as they are exponentiations of nonpositive numbers. This manipulation avoids
    terms underflowing to zero because even very large values, when shifted by \(M\),
    are less likely to hit the underflow threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example. Imagine you have \(\mathbf{a} = (1000, 1001, 1002)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Direct computation: \(e^{-1000}\), \(e^{-1001}\), and \(e^{-1002}\) might all
    underflow to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the log-sum-exp trick: Subtract \(M = 1000\), leading to \(e^{0}\), \(e^{-1}\),
    and \(e^{-2}\), all meaningful, non-zero results that accurately contribute to
    the sum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implement in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We try it on a simple example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We first attempt a direct computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Predictly, we get an underflow error and a useless output.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we try the log-sum-exp trick.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This time we get an output which seems reasonable, something slightly larger
    than \(-1000\) as expected (Why?).
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: After this long – but important! – parenthesis, we return to the EM algorithm.
    We modify it by implementing the log-sum-exp trick in the subroutine `responsibility`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We go back to the MNIST example with only the 2s.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here are the parameters for one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png](../Images/4e3661e0a1a8051341921aba94079c7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here is the other one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png](../Images/372533fd8fad14df01f1cf71749a0141.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that the model is trained, we compute the probability that an example image
    is in each cluster. We use the first image in the dataset that we plotted earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: It indeed identifies the second cluster as significantly more likely.
  prefs: []
  type: TYPE_NORMAL
- en: '**TRY IT!** In the MNIST example, as we have seen, the probabilities involved
    are extremely small and the responsibilities are close to \(0\) or \(1\). Implement
    a variant of EM, called hard EM, which replaces responsibilities with the one-hot
    encoding of the largest responsibility. Test it on the MNIST example again. ([Open
    In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model is a simple example
    of a latent variable model. Ask your favorite AI chatbot to discuss more complex
    latent variable models, such as the variational autoencoder or the Gaussian process
    latent variable model, and their applications in unsupervised learning. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In the mixture of multivariate Bernoullis model, the joint distribution
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X}
    = \mathbf{x}]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C =
    k, \mathbf{X} = \mathbf{x}]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** The majorization-minimization principle states that:'
  prefs: []
  type: TYPE_NORMAL
- en: a) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: b) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: c) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: d) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: '**3** In the EM algorithm for mixtures of multivariate Bernoullis, the M-step
    involves:'
  prefs: []
  type: TYPE_NORMAL
- en: a) Updating the parameters \(\pi_k\) and \(p_{k,m}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) Computing the responsibilities \(r_{k,i}^t\)
  prefs: []
  type: TYPE_NORMAL
- en: c) Minimizing the negative log-likelihood
  prefs: []
  type: TYPE_NORMAL
- en: d) Applying the log-sum-exp trick
  prefs: []
  type: TYPE_NORMAL
- en: '**4** The mixture of multivariate Bernoullis model is represented by the following
    graphical model:'
  prefs: []
  type: TYPE_NORMAL
- en: a)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: b)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: c)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: d)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '**5** In the context of clustering, what is the interpretation of the responsibilities
    computed in the E-step of the EM algorithm?'
  prefs: []
  type: TYPE_NORMAL
- en: a) They represent the distance of each data point to the cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: b) They indicate the probability of each data point belonging to each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: c) They determine the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: d) They are used to initialize the cluster centers in the M-step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states, “\(\mathbb{P}[\mathbf{X} =
    \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K
    \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: “Let \(f: \mathbb{R}^d \to \mathbb{R}\) and
    suppose \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\)
    be a global minimizer of \(U_{\mathbf{x}}(\mathbf{z})\) as a function of \(\mathbf{z}\),
    provided it exists. Then \(f(\mathbf{x}'') \leq f(\mathbf{x})\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: In the summary of the EM algorithm, the M-step
    is described as updating the parameters: “\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\)
    and \(p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}\),” which require the responsibilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The text states, “Mathematically, that corresponds
    to applying the law of total probability as we did previously. Further, we let
    the vertex for \(X\) be shaded to indicate that it is observed, while the vertex
    for \(Y\) is not shaded to indicate that it is not.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text refers to responsibilities as “our
    estimate – under the current parameter – of the probability that the sample comes
    from cluster \(k\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1\. Mixtures[#](#mixtures "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mixtures\(\idx{mixture}\xdi\) are a natural way to define probability distributions.
    The basic idea is to consider a pair of random vectors \((\bX,\bY)\) and assume
    that \(\bY\) is unobserved. The effect on the observed vector \(\bX\) is that
    \(\bY\) is marginalized out. Indeed, by the law of total probability, for any
    \(\bx \in \S_\bX\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} p_\bX(\bx) &= \P[\bX = \bx]\\ &= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by]
    \,\P[\bY=\by]\\ &= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by) \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'where we used that the events \(\{\bY=\by\}\), \(\by \in \S_\bY\), form a partition
    of the probability space. We interpret this equation as defining \(p_\bX(\bx)\)
    as a convex combination – a mixture – of the distributions \(p_{\bX|\bY}(\bx|\by)\),
    \(\by \in \S_\bY\), with mixing weights \(p_\bY(\by)\). In general, we need to
    specify the full conditional probability distribution (CPD): \(p_{\bX|\bY}(\bx|\by),
    \forall \bx \in \S_{\bX}, \by \in \S_\bY\). But assuming that the mixing weights
    and/or CPD come from parametric families can help reduce the complexity of the
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: That can be represented in a digraph with a directed edge from a vertex for
    \(\mathbf{Y}\) to a vertex for \(\mathbf{X}\). Further, we let the vertex for
    \(\mathbf{X}\) be shaded to indicate that it is observed, while the vertex for
    \(\mathbf{Y}\) is not shaded to indicate that it is not. Mathematically, that
    corresponds to applying the law of total probability as we did previously.
  prefs: []
  type: TYPE_NORMAL
- en: '![A mixture](../Images/d7c8f87d42f669398ac57f92c0a43388.png)'
  prefs: []
  type: TYPE_IMG
- en: In the parametric context, this gives rise to a fruitful approach to expanding
    distribution families. Suppose \(\{p_{\btheta}:\btheta \in \Theta\}\) is a parametric
    family of distributions. Let \(K \geq 2\), \(\btheta_1, \ldots, \btheta_K \in
    \Theta\) and \(\bpi = (\pi_1,\ldots,\pi_K) \in \Delta_K\). Suppose \(Y \sim \mathrm{Cat}(\bpi)\)
    and that the conditional distributions satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{\bX|Y}(\bx|i) = p_{\btheta_i}(\bx). \]
  prefs: []
  type: TYPE_NORMAL
- en: We write this as \(\bX|\{Y=i\} \sim p_{\btheta_i}\). Then we obtain the mixture
    model
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{\bX}(\bx) = \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i) = \sum_{i=1}^K \pi_i
    p_{\btheta_i}(\bx). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Mixture of Multinomials)** Let \(n, m , K \geq 1\), \(\bpi
    \in \Delta_K\) and, for \(i=1,\ldots,K\), \(\mathbf{p}_i = (p_{i1},\ldots,p_{im})
    \in \Delta_m\). Suppose that \(Y \sim \mathrm{Cat}(\bpi)\) and that the conditional
    distributions are'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i). \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(\bX\) is a mixture of multinomials. Its distribution is then
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m
    p_{ij}^{x_j}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Next is an important continuous example.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Gaussian mixture model)** \(\idx{Gaussian mixture model}\xdi\)
    For \(i=1,\ldots,K\), let \(\bmu_i\) and \(\bSigma_i\) be the mean and covariance
    matrix of a multivariate Gaussian. Let \(\bpi \in \Delta_K\). A Gaussian Mixture
    Model (GMM) is obtained as follows: take \(Y \sim \mathrm{Cat}(\bpi)\) and'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i). \]
  prefs: []
  type: TYPE_NORMAL
- en: Its probability density function (PDF) takes the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We plot the density for means \(\bmu_1 = (-2,-2)\) and
    \(\bmu_2 = (2,2)\) and covariance matrices'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{and} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1
    \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma_1 = 1.5\), \(\sigma_2 = 0.5\) and \(\rho = -0.75\). The mixing
    weights are \(\pi_1 = 0.25\) and \(\pi_2 = 0.75\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]</details> ![../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png](../Images/810f47a1332c13f0595b65b28e44ab63.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In NumPy, as we have seen before, the module [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html)
    also provides a way to sample from mixture models by using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we consider mixtures of multivariate Gaussians. We chage the notation
    slightly to track Python’s indexing. For \(i=0,1\), we have a mean \(\bmu_i \in
    \mathbb{R}^d\) and a positive definite covariance matrix \(\bSigma_i \in \mathbb{R}^{d
    \times d}\). We also have mixture weights \(\phi_0, \phi_1 \in (0,1)\) such that
    \(\phi_0 + \phi_1 = 1\). Suppose we want to generate a total of \(n\) samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each sample \(j=1,\ldots, n\), independently from everything else:'
  prefs: []
  type: TYPE_NORMAL
- en: We first pick a component \(i \in \{0,1\}\) at random according to the mixture
    weights, that is, \(i=0\) is chosen with probability \(\phi_0\) and \(i=1\) is
    chosen with probability \(\phi_1\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generate a sample \(\bX_j = (X_{j,1},\ldots,X_{j,d})\) according to a multivariate
    Gaussian with mean \(\bmu_i\) and covariance \(\bSigma_i\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is straightforward to implement by using again [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html)
    to choose the component of each sample and [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html)
    to generate multivariate Gaussians. For convenience, we will stack the means and
    covariances into one array with a new dimension. So, for instance, the covariance
    matrices will now be in a 3d-array, that is, an array with \(3\) indices. The
    first index corresponds to the component (here \(0\) or \(1\)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Three matrices ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Three matrices](../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Three matrices stacked into a 3d-array ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Three matrices stacked into a tensor](../Images/a61cc745a3c58639bf7340b2af821416.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: The code is the following. It returns an `d` by `n` array `X`, where each row
    is a sample from a 2-component Gaussian mixture.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Let us try it with following parameters. We first define
    the covariance matrices and show what happens when they are stacked into a 3d
    array (as is done within `gmm2`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Then we define the rest of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png](../Images/22ec3b142976b6b20a0185e9e997fea9.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '6.4.2\. Example: Mixtures of multivariate Bernoullis and the EM algorithm[#](#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let \(\mathcal{C} = \{1, \ldots, K\}\) be a collection of classes. Let \(C\)
    be a random variable taking values in \(\mathcal{C}\) and, for \(m=1, \ldots,
    M\), let \(X_i\) take values in \(\{0,1\}\). Define \(\pi_k = \P[C = k]\) and
    \(p_{k,m} = \P[X_m = 1|C = k]\) for \(m = 1,\ldots, M\). We denote by \(\bX =
    (X_1, \ldots, X_M)\) the corresponding vector of \(X_i\)’s and assume that the
    entries are conditionally independent given \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: However, we assume this time that \(C\) itself is *not observed*. So the resulting
    joint distribution is the mixture
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[\bX = \bx] &= \sum_{k=1}^K \P[C = k, \bX = \bx]\\ &= \sum_{k=1}^K
    \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, this is the same are the Naive Bayes model, except that \(C\) is
    not observed and therefore is not shaded.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mixture of multivariate Bernoullis](../Images/c51352d9d6ab19d2f9e73f898c25b648.png)'
  prefs: []
  type: TYPE_IMG
- en: This type of model is useful in particular for clustering tasks, where the \(c_k\)s
    can be thought of as different clusters. Similarly to what we did in the previous
    section, our goal is to infer the parameters from samples and then predict the
    class of an old or new sample given its features. The main – substantial – difference
    is that the true labels of the samples are not observed. As we will see, that
    complicates the task considerably.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model fitting** We first fit the model from training data \(\{\bx_i\}_{i=1}^n\).
    Recall that the corresponding class labels \(c_i\)s are not observed. In this
    type of model, they are referred to as hidden or latent variables and we will
    come back to their inference below.'
  prefs: []
  type: TYPE_NORMAL
- en: We would like to use maximum likelihood estimation, that is, maximize the probability
    of observing the data
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\}) = \prod_{i=1}^n \left( \sum_{k=1}^K
    \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we assume that the samples are independent and identically distributed.
    Consider the negative log-likelihood (NLL)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Already, we see that things are potentially more difficult than they were in
    the supervised (or fully observed) case. The NLL does not decompose into a sum
    of terms depending on different sets of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, one could fall back on the field of optimization and use a gradient-based
    method to minimize the NLL. Indeed that is an option, although note that one must
    be careful to account for the constrained nature of the problem (i.e., the parameters
    sum to one). There is a vast array of constrained optimization techniques suited
    for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Instead a more popular approach in this context, the EM algorithm, is based
    on the general principle of majorization-minimization, which we have encountered
    implicitly in the \(k\)-means algorithm and the convergence proof of gradient
    descent in the smooth case. We detail this important principle in the next subsection
    before returning to model fitting in mixtures.
  prefs: []
  type: TYPE_NORMAL
- en: '**Majorization-minimization** \(\idx{majorization-minimization}\xdi\) Here
    is a deceptively simple, yet powerful observation. Suppose we want to minimize
    a function \(f : \mathbb{R}^d \to \mathbb{R}\). Finding a local minimum of \(f\)
    may not be easy. But imagine that for each \(\mathbf{x} \in \mathbb{R}^d\) we
    have a surrogate function \(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\) that
    (1) dominates \(f\) in the following sense'
  prefs: []
  type: TYPE_NORMAL
- en: \[ U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in
    \mathbb{R}^d \]
  prefs: []
  type: TYPE_NORMAL
- en: and (2) equals \(f\) at \(\mathbf{x}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}). \]
  prefs: []
  type: TYPE_NORMAL
- en: We say that \(U_\mathbf{x}\) majorizes \(f\) at \(\mathbf{x}\). Then we prove
    in the next lemma that \(U_\mathbf{x}\) can be used to make progress towards minimizing
    \(f\), that is, find a point \(\mathbf{x}'\) such that \(f(\mathbf{x}') \leq f(\mathbf{x})\).
    If in addition \(U_\mathbf{x}\) is easier to minimize than \(f\) itself, say because
    an explicit minimum can be computed, then this observation naturally leads to
    an iterative algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![A majorizing function (with help from ChatGPT; inspired by Source)](../Images/c05a4e1add7058b646f07a53f61281a2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**LEMMA** **(Majorization-Minimization)** \(\idx{majorization-minimization
    lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) and suppose \(U_{\mathbf{x}}\)
    majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\) be a global minimum of
    \(U_\mathbf{x}\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}') \leq f(\mathbf{x}). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x})
    = f(\mathbf{x}), \]
  prefs: []
  type: TYPE_NORMAL
- en: where the first inequality follows from the domination property of \(U_\mathbf{x}\),
    the second inequality follows from the fact that \(\mathbf{x}'\) is a global minimum
    of \(U_\mathbf{x}\) and the equality follows from the fact that \(U_{\mathbf{x}}\)
    equals \(f\) at \(\mathbf{x}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We have already encountered this idea.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Minimizing a smooth function)** Let \(f : \mathbb{R}^d \to
    \mathbb{R}\) be \(L\)-smooth. By the *Quadratic Bound for Smooth Functions*, for
    all \(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\) it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{z}) \leq U_{\mathbf{x}}(\mathbf{z}) := f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{z}
    - \mathbf{x}) + \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: By showing that \(U_{\mathbf{x}}\) is minimized at \(\mathbf{z} = \mathbf{x}
    - (1/L)\nabla f(\mathbf{x})\), we previously obtained the descent guarantee
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x} - (1/L)\nabla f(\mathbf{x})) \leq f(\mathbf{x}) - \frac{1}{2
    L} \|\nabla f(\mathbf{x})\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for gradient descent, which played a central role in the analysis of its convergence\(\idx{convergence
    analysis}\xdi\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(\(k\)-means)** \(\idx{Lloyd''s algorithm}\xdi\) Let \(\mathbf{x}_1,\ldots,\mathbf{x}_n\)
    be \(n\) vectors in \(\mathbb{R}^d\). One way to formulate the \(k\)-means clustering
    problem is as the minimization of'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: over the centers \(\bmu_1,\ldots,\bmu_K\), where recall that \([K] = \{1,\ldots,K\}\).
    For fixed \(\bmu_1,\ldots,\bmu_K\) and \(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\),
    define
  prefs: []
  type: TYPE_NORMAL
- en: \[ c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in
    [K]\right\}, \quad i=1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \|\mathbf{x}_i
    - \blambda_{c_\mathbf{m}(i)}\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\). That is, we fix the optimal
    cluster assignments under \(\bmu_1,\ldots,\bmu_K\) and then vary the centers.
  prefs: []
  type: TYPE_NORMAL
- en: We claim that \(U_\mathbf{m}\) is majorizing \(f\) at \(\bmu_1,\ldots,\bmu_K\).
    Indeed
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i
    - \blambda_j\|^2 \leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
    = U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 = \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2 = U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover \(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\) is easy to minimize.
    We showed previously that the optimal representatives are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(C_j = \{i : c_\mathbf{m}(i) = j\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: The *Majorization-Minimization Lemma* implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\bmu_1',\ldots,\bmu_K') \leq f(\bmu_1,\ldots,\bmu_K). \]
  prefs: []
  type: TYPE_NORMAL
- en: This argument is equivalent to our previous analysis of the \(k\)-means algorithm.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model assumes a fixed
    number of clusters. Ask your favorite AI chatbot to discuss Bayesian nonparametric
    extensions of this model, such as the Dirichlet process mixture model, which can
    automatically infer the number of clusters from the data. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**EM algorithm** The [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\(\idx{EM
    algorithm}\xdi\) is an instantiation of the majorization-minimization principle
    that applies widely to parameter estimation of mixtures. Here we focus on the
    mixture of multivariate Bernoullis.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the objective to be minimized is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the notation and highlight the general idea, we let \(\btheta =
    (\bpi, \{\bp_k\})\), denote by \(\Theta\) the set of allowed values for \(\btheta\),
    and use \(\P_{\btheta}\) to indicate that probabilities are computed under the
    parameters \(\btheta\). We also return to the description of the model in terms
    of the unobserved latent variables \(\{C_i\}\). That is, we write the NLL as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i
    = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\ &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To derive a majorizing function, we use the convexity of the negative logarithm.
    Indeed
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial z}[- \log z] = - \frac{1}{z} \quad \text{and} \quad
    \frac{\partial^2}{\partial^2 z}[- \log z] = \frac{1}{z^2} > 0, \quad \forall z
    > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The first step of the construction is not obvious – it just works. For each
    \(i=1,\ldots,n\), we let \(r_{k,i}^{\btheta}\), \(k=1,\ldots,K\), be a strictly
    positive probability distribution over \([K]\). In other words, it defines a convex
    combination for every \(i\). Then we use convexity to obtain the upper bound
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} L_n(\tilde\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K
    r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right), \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: which holds for any \(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\).
  prefs: []
  type: TYPE_NORMAL
- en: We choose
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i] \]
  prefs: []
  type: TYPE_NORMAL
- en: (which for the time being we assume is strictly positive) and we denote the
    right-hand side of the inequality by \(Q_{n}(\tilde\btheta|\btheta)\) (as a function
    of \(\tilde\btheta\)).
  prefs: []
  type: TYPE_NORMAL
- en: We make two observations.
  prefs: []
  type: TYPE_NORMAL
- en: '1- *Dominating property*: For any \(\tilde\btheta \in \Theta\), the inequality
    above implies immediately that \(L_n(\tilde\btheta) \leq Q_n(\tilde\btheta|\btheta)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '2- *Equality at \(\btheta\)*: At \(\tilde\btheta = \btheta\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Q_n(\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i
    = k | \bX_i = \bx_i] \P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\btheta}[\bX_i = \bx_i]\\
    &= - \sum_{i=1}^n \log \P_{\btheta}[\bX_i = \bx_i]\\ &= L_n(\btheta). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The two properties above show that \(Q_n(\tilde\btheta|\btheta)\), as a function
    of \(\tilde\btheta\), majorizes \(L_n\) at \(\btheta\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(EM Guarantee)** \(\idx{EM guarantee}\xdi\) Let \(\btheta^*\) be
    a global minimizer of \(Q_n(\tilde\btheta|\btheta)\) as a function of \(\tilde\btheta\),
    provided it exists. Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_n(\btheta^*) \leq L_n(\btheta). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The result follows directly from the *Majorization-Minimization Lemma*.
    \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: What have we gained from this? As we mentioned before, using the *Majorization-Minimization
    Lemma* makes sense if \(Q_n\) is easier to minimize than \(L_n\) itself. Let us
    see why that is the case here.
  prefs: []
  type: TYPE_NORMAL
- en: '*E Step:* The function \(Q_n\) naturally decomposes into two terms'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Q_n(\tilde\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k] + \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(r_{k,i}^{\btheta}\) depends on \(\btheta\) *but not \(\tilde\btheta\)*,
    the second term is irrelevant to the opimization with respect to \(\tilde\btheta\).
  prefs: []
  type: TYPE_NORMAL
- en: The first term above can be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} & - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k}
    \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
    &= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M
    [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta})
    \log (1-\tilde{p}_{k,m})], \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we defined, for \(k=1,\ldots,K\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \eta_{k,m}^{\btheta} = \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta} \quad \text{and}
    \quad \eta_k^{\btheta} = \sum_{i=1}^n r_{k,i}^{\btheta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here comes the key observation: this last expression is essentially the same
    as the NLL for the fully observed Naive Bayes model, except that the terms \(\mathbf{1}_{\{c_i
    = k\}}\) are replaced by \(r_{k,i}^{\btheta}\). If \(\btheta\) is our current
    estimate of the parameters, then the quantity \(r_{k,i}^{\btheta} = \P_{\btheta}[C_i
    = k|\bX_i = \bx_i]\) is our estimate – under the current parameter \(\btheta\)
    – of the probability that the sample \(\bx_i\) comes from cluster \(k\). We have
    previously computed \(r_{k,i}^{\btheta}\) for prediction under the Naive Bayes
    model. We showed there that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^{\btheta} = \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'which in this new context is referred to as the responsibility that cluster
    \(k\) takes for data point \(i\). So we can interpret the expression above as
    follows: the variables \(\mathbf{1}_{\{c_i = k\}}\) are not observed here, but
    we have estimated their conditional probability distribution given the observed
    data \(\{\bx_i\}\), and we are taking an expectation with respect to that distribution
    instead.'
  prefs: []
  type: TYPE_NORMAL
- en: The “E” in E Step (and EM) stands for “expectation”, which refers to using a
    surrogate function that is essentially an expected NLL.
  prefs: []
  type: TYPE_NORMAL
- en: '*M Step:* In any case, from a practical point of view, minimizing \(Q_n(\tilde\btheta|\btheta)\)
    over \(\tilde\btheta\) turns out to be a variant of fitting a Naive Bayes model
    – and the upshot to all this is that there is a straightforward formula for that!
    Recall that this happens because, the NLL in the Naive Bayes model decomposes:
    it naturally breaks up into terms that depend on separate sets of parameters,
    each of which can be optimized with a closed-form expression. The same happens
    with \(Q_n\) as should be clear from the derivation.'
  prefs: []
  type: TYPE_NORMAL
- en: Adapting our previous calculations for fitting a Naive Bayes model, we get that
    \(Q_n(\tilde\btheta|\btheta)\) is minimized at
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_k^* = \frac{\eta_k^{\btheta}}{n} \quad \text{and} \quad p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
    \quad \forall k \in [K], m \in [M]. \]
  prefs: []
  type: TYPE_NORMAL
- en: We used the fact that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sum_{k=1}^K \eta_k^{\btheta} &= \sum_{k=1}^K \sum_{i=1}^n
    r_{k,i}^{\btheta}\\ &= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i =
    \bx_i]\\ &= \sum_{i=1}^n 1\\ &= n, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: since the conditional probability \(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\) adds
    up to one when summed over \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: The “M” in M Step (and EM) stands for maximization, which here turns into minimization
    because of the use of the NLL.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the EM algorithm works as follows in this case. Assume we have
    data points \(\{\bx_i\}_{i=1}^n\), that we have fixed \(K\) and that we have some
    initial parameter estimate \(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\) with
    strictly positive \(\pi_k^0\)s and \(p_{k,m}^0\)s. For \(t = 0,1,\ldots, T-1\)
    we compute for all \(i \in [n]\), \(k \in [K]\), and \(m \in [M]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \quad \text{(E Step)} \]\[ \eta_{k,m}^t = \sum_{i=1}^n x_{i,m} r_{k,i}^t \quad
    \text{and} \quad \eta_k^t = \sum_{i=1}^n r_{k,i}^t, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_k^{t+1} = \frac{\eta_k^t}{n} \quad \text{and} \quad p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
    \quad \text{(M Step)} \]
  prefs: []
  type: TYPE_NORMAL
- en: Provided \(\sum_{i=1}^n x_{i,m} > 0\) for all \(m\), the \(\eta_{k,m}^t\)s and
    \(\eta_k^t\)s remain positive for all \(t\) and the algorithm is well-defined.
    The *EM Guarantee* stipulates that the NLL cannot deteriorate, although note that
    it does not guarantee convergence to a global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: We implement the EM algorithm for mixtures of multivariate Bernoullis. For this
    purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility
    of using Laplace smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We implement the E and M Step next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We test the algorithm on a very simple dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We compute the probability that the vector \((0, 0, 1)\) is in each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '**CHAT & LEARN** The EM algorithm can sometimes get stuck in local optima.
    Ask your favorite AI chatbot to discuss strategies for initializing the EM algorithm
    to avoid this issue, such as using multiple random restarts or using the k-means
    algorithm for initialization. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3\. Clustering handwritten digits[#](#clustering-handwritten-digits "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give a more involved example, we use the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database) again:'
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database (Modified National Institute of Standards and Technology
    database) is a large database of handwritten digits that is commonly used for
    training various image processing systems. The database is also widely used for
    training and testing in the field of machine learning. It was created by “re-mixing”
    the samples from NIST’s original datasets. The creators felt that since NIST’s
    training dataset was taken from American Census Bureau employees, while the testing
    dataset was taken from American high school students, it was not well-suited for
    machine learning experiments. Furthermore, the black and white images from NIST
    were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which
    introduced grayscale levels. The MNIST database contains 60,000 training images
    and 10,000 testing images. Half of the training set and half of the test set were
    taken from NIST’s training dataset, while the other half of the training set and
    the other half of the test set were taken from NIST’s testing dataset.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Figure:** MNIST sample images ([Source](https://commons.wikimedia.org/wiki/File:MnistExamples.png))'
  prefs: []
  type: TYPE_NORMAL
- en: '![MNIST sample images](../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We load it from PyTorch. The data can be accessed with
    [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html).
    The [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html)
    below removes the color dimension in the image, which is grayscale. The [`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    converts the PyTorch tensors into NumPy arrays. See [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    for details on the data loading. We will say more about PyTorch in a later chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: We turn the grayscale images into a black-and-white images by rounding the pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: There are two common ways to write a \(2\). Let’s see if a mixture of multivariate
    Bernoullis can find them. We extract the images labelled \(2\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The first image is the following.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]</details> ![../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png](../Images/e8eea00af5868fb166365334b2072575.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we transform the images into vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We run the algorithm with \(2\) clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Uh-oh. Something went wrong. We encountered a numerical issue, underflow, which
    we discussed briefly previously. To confirm this, we run the code again but ask
    Python to warn us about it using [`numpy.seterr`](https://numpy.org/doc/stable/reference/generated/numpy.seterr.html).
    (By default, warnings are turned off in the book, but they can be reactivated
    using [`warnings.resetwarnings`](https://docs.python.org/3/library/warnings.html#warnings.resetwarnings).)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: When we compute the responsibilities
  prefs: []
  type: TYPE_NORMAL
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: we first compute the negative logarithm of each term in the numerator as we
    did in the Naive Bayes case. But then we apply the function \(e^{-x}\), because
    this time we are not simply computing an optimal score. When all scores are high,
    this last step may result in underflow, that is, produces numbers so small that
    they get rounded down to zero by NumPy. Then the ratio defining `r_k` is not well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with this, we introduce a technique called the log-sum-exp trick\(\idx{log-sum-exp
    trick}\xdi\) (with some help from ChatGPT). Consider the computation of a function
    of \(\mathbf{a} = (a_1, \ldots, a_n)\) of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right). \]
  prefs: []
  type: TYPE_NORMAL
- en: When the \(a_i\) values are large positive numbers, the terms \(e^{-a_i}\) can
    be so small that they underflow to zero. To counter this, the log-sum-exp trick
    involves a shift to bring these terms into a more favorable numerical range.
  prefs: []
  type: TYPE_NORMAL
- en: 'It proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the minimum value \(M\) among the \(a_i\)s
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ M = \min\{a_1, a_2, \ldots, a_n\}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Subtract \(M\) from each \(a_i\) before exponentiation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \log \left( \sum_{i=1}^{n} e^{-a_i} \right) = \log \left( e^{-M} \sum_{i=1}^{n}
    e^{- (a_i - M)} \right). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rewrite using log properties
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Why does this help with underflow? By subtracting \(M\), the smallest value
    in the set, from each \(a_i\): (i) the largest term in \(\{e^{-(a_i - M)} : i
    = 1,\ldots,n\}\) becomes \(e^0 = 1\); and (ii) all other terms are between 0 and
    1, as they are exponentiations of nonpositive numbers. This manipulation avoids
    terms underflowing to zero because even very large values, when shifted by \(M\),
    are less likely to hit the underflow threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example. Imagine you have \(\mathbf{a} = (1000, 1001, 1002)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Direct computation: \(e^{-1000}\), \(e^{-1001}\), and \(e^{-1002}\) might all
    underflow to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the log-sum-exp trick: Subtract \(M = 1000\), leading to \(e^{0}\), \(e^{-1}\),
    and \(e^{-2}\), all meaningful, non-zero results that accurately contribute to
    the sum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implement in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We try it on a simple example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We first attempt a direct computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Predictly, we get an underflow error and a useless output.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we try the log-sum-exp trick.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This time we get an output which seems reasonable, something slightly larger
    than \(-1000\) as expected (Why?).
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: After this long – but important! – parenthesis, we return to the EM algorithm.
    We modify it by implementing the log-sum-exp trick in the subroutine `responsibility`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We go back to the MNIST example with only the 2s.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Here are the parameters for one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png](../Images/4e3661e0a1a8051341921aba94079c7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here is the other one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png](../Images/372533fd8fad14df01f1cf71749a0141.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that the model is trained, we compute the probability that an example image
    is in each cluster. We use the first image in the dataset that we plotted earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: It indeed identifies the second cluster as significantly more likely.
  prefs: []
  type: TYPE_NORMAL
- en: '**TRY IT!** In the MNIST example, as we have seen, the probabilities involved
    are extremely small and the responsibilities are close to \(0\) or \(1\). Implement
    a variant of EM, called hard EM, which replaces responsibilities with the one-hot
    encoding of the largest responsibility. Test it on the MNIST example again. ([Open
    In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model is a simple example
    of a latent variable model. Ask your favorite AI chatbot to discuss more complex
    latent variable models, such as the variational autoencoder or the Gaussian process
    latent variable model, and their applications in unsupervised learning. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In the mixture of multivariate Bernoullis model, the joint distribution
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X}
    = \mathbf{x}]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C =
    k, \mathbf{X} = \mathbf{x}]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** The majorization-minimization principle states that:'
  prefs: []
  type: TYPE_NORMAL
- en: a) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: b) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: c) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: d) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: '**3** In the EM algorithm for mixtures of multivariate Bernoullis, the M-step
    involves:'
  prefs: []
  type: TYPE_NORMAL
- en: a) Updating the parameters \(\pi_k\) and \(p_{k,m}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) Computing the responsibilities \(r_{k,i}^t\)
  prefs: []
  type: TYPE_NORMAL
- en: c) Minimizing the negative log-likelihood
  prefs: []
  type: TYPE_NORMAL
- en: d) Applying the log-sum-exp trick
  prefs: []
  type: TYPE_NORMAL
- en: '**4** The mixture of multivariate Bernoullis model is represented by the following
    graphical model:'
  prefs: []
  type: TYPE_NORMAL
- en: a)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: b)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: c)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: d)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '**5** In the context of clustering, what is the interpretation of the responsibilities
    computed in the E-step of the EM algorithm?'
  prefs: []
  type: TYPE_NORMAL
- en: a) They represent the distance of each data point to the cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: b) They indicate the probability of each data point belonging to each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: c) They determine the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: d) They are used to initialize the cluster centers in the M-step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states, “\(\mathbb{P}[\mathbf{X} =
    \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K
    \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: “Let \(f: \mathbb{R}^d \to \mathbb{R}\) and
    suppose \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\)
    be a global minimizer of \(U_{\mathbf{x}}(\mathbf{z})\) as a function of \(\mathbf{z}\),
    provided it exists. Then \(f(\mathbf{x}'') \leq f(\mathbf{x})\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: In the summary of the EM algorithm, the M-step
    is described as updating the parameters: “\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\)
    and \(p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}\),” which require the responsibilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The text states, “Mathematically, that corresponds
    to applying the law of total probability as we did previously. Further, we let
    the vertex for \(X\) be shaded to indicate that it is observed, while the vertex
    for \(Y\) is not shaded to indicate that it is not.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text refers to responsibilities as “our
    estimate – under the current parameter – of the probability that the sample comes
    from cluster \(k\).”'
  prefs: []
  type: TYPE_NORMAL
