<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>5.4. Spectral properties of the Laplacian matrix#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>5.4. Spectral properties of the Laplacian matrix#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html">https://mmids-textbook.github.io/chap05_specgraph/04_laplacian/roch-mmids-specgraph-laplacian.html</a></blockquote>

<p>In this section, we look at the spectral properties of the Laplacian of a graph.</p>
<section id="eigenvalues-of-the-laplacian-matrix-first-observations">
<h2><span class="section-number">5.4.1. </span>Eigenvalues of the Laplacian matrix: first observations<a class="headerlink" href="#eigenvalues-of-the-laplacian-matrix-first-observations" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices. Two observations:</p>
<p>1- Since the Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> of <span class="math notranslate nohighlight">\(G\)</span> is symmetric, by the <em>Spectral Theorem</em>, it has a spectral decomposition</p>
<div class="math notranslate nohighlight">
\[
L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T
\]</div>
<p>where the <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>’s form an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>.</p>
<p>2- Further, because <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite, the eigenvalues are nonnegative. By convention, we assume</p>
<div class="math notranslate nohighlight">
\[
0 \leq \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n.
\]</div>
<p>Note that this is the opposite order to we used in the previous section.</p>
<p>Another observation:</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices and Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span>. The constant unit vector</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y}_1 
= 
\frac{1}{\sqrt{n}} (1, \ldots, 1)
\]</div>
<p>is an eigenvector of <span class="math notranslate nohighlight">\(L\)</span> with eigenvalue <span class="math notranslate nohighlight">\(0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span> recall that <span class="math notranslate nohighlight">\(L = B B^T\)</span>. By construction <span class="math notranslate nohighlight">\(B^T \mathbf{y}_1 = \mathbf{0}\)</span> since each column of <span class="math notranslate nohighlight">\(B\)</span> has exactly one <span class="math notranslate nohighlight">\(1\)</span> and one <span class="math notranslate nohighlight">\(-1\)</span>. So <span class="math notranslate nohighlight">\(L \mathbf{y}_1 = B B^T \mathbf{y}_1 = \mathbf{0}\)</span> as claimed. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>In general, the constant vector may not be the only eigenvector with eigenvalue one.</p>
<p><strong>NUMERICAL CORNER:</strong> One use of the spectral decomposition of the Laplacian matrix is in graph drawing<span class="math notranslate nohighlight">\(\idx{graph drawing}\xdi\)</span>. We illustrate this next. Given a graph <span class="math notranslate nohighlight">\(G = (V, E)\)</span>, it is not clear a priori how to draw it in the plane since the only information available are adjacencies of vertices. One approach is just to position the vertices at random. The function <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw.html"><code class="docutils literal notranslate"><span class="pre">networkx.draw</span></code></a> or <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html#networkx.drawing.nx_pylab.draw_networkx"><code class="docutils literal notranslate"><span class="pre">networkx.draw_networkx</span></code></a> can take as input different <a class="reference external" href="https://networkx.org/documentation/stable/reference/drawing.html#module-networkx.drawing.layout">graph layout</a> functions that return an <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>-coordinate for each vertex.</p>
<p>We will test this on a grid graph. We use <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.generators.lattice.grid_2d_graph.html"><code class="docutils literal notranslate"><span class="pre">networkx.grid_2d_graph</span></code></a> to construct such a graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">grid_2d_graph</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>One layout approach is to choose random locations for the nodes. Specifically, for every node, a position is generated by choosing each coordinate uniformly at random on the interval <span class="math notranslate nohighlight">\([0,1]\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">random_layout</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">535</span><span class="p">),</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">node_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e71c11388af95019112c93563223107aea5d4aa3b91dbab26b654c57b7ad303b.png" src="../Images/f62f7cee4755a1c33c92335ca3d78f91.png" data-original-src="https://mmids-textbook.github.io/_images/e71c11388af95019112c93563223107aea5d4aa3b91dbab26b654c57b7ad303b.png"/>
</div>
</div>
<p>Clearly, this is hard to read.</p>
<p>Another approach is to map the vertices to two eigenvectors, similarly to what we did for dimensionality reduction. The eigenvector associated to <span class="math notranslate nohighlight">\(\mu_1\)</span> is constant and therefore not useful for drawing. We try the next two. We use the Laplacian matrix. This is done using <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.drawing.layout.spectral_layout.html#networkx.drawing.layout.spectral_layout"><code class="docutils literal notranslate"><span class="pre">networkx.spectral_layout</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">spectral_layout</span><span class="p">(</span><span class="n">G</span><span class="p">),</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">node_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d9f18189eb6a51676466d9a2ffe9aa688e95d0f15f8697b68ee0c7c9ae2ada45.png" src="../Images/0ecdf40ac77cc8a246dd3cb1077c885d.png" data-original-src="https://mmids-textbook.github.io/_images/d9f18189eb6a51676466d9a2ffe9aa688e95d0f15f8697b68ee0c7c9ae2ada45.png"/>
</div>
</div>
<p>Interestingly, the outcome is provides a much more natural drawing of the graph, revealing its underlying structure as a grid. We will come back later to try to explain this, after we have developed further understanding of the spectral properties of the Laplacian matrix.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="laplacian-matrix-and-connectivity">
<h2><span class="section-number">5.4.2. </span>Laplacian matrix and connectivity<a class="headerlink" href="#laplacian-matrix-and-connectivity" title="Link to this heading">#</a></h2>
<p>As we indicated before, the Laplacian matrix contains information about the connectedness of <span class="math notranslate nohighlight">\(G\)</span>. We elaborate on a first concrete connection here. But first we will need a useful form of the Laplaican quadratic form <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> which enters in the variational charaterization of the eigenvalues.</p>
<p><strong>LEMMA</strong> <strong>(Laplacian Quadratic Form)</strong> <span class="math notranslate nohighlight">\(\idx{Laplacian quadratic form lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices and Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span>. We have the following formula for the Laplacian quadratic form</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T L \mathbf{x}
= \sum_{e = \{i,j\} \in E} (x_i - x_j)^2
\]</div>
<p>for any <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>Here is an intuitive way of interpreting this lemma. If one thinks of <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n\)</span> as a real-valued function over the vertices (i.e., it associates a real value <span class="math notranslate nohighlight">\(x_i\)</span> to vertex <span class="math notranslate nohighlight">\(i\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>), then the Laplacian quadratic form measures how “smooth” the function is over the graph in the following sense. A small value of <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> indicates that adjacent vertices tend to get assigned close values.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span>. We have that <span class="math notranslate nohighlight">\(L = B B^T\)</span>. Thus, for any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we have <span class="math notranslate nohighlight">\((B^T \mathbf{x})_k = x_v - x_u\)</span> if the edge <span class="math notranslate nohighlight">\(e_k = \{u, v\}\)</span> is oriented as <span class="math notranslate nohighlight">\((u,v)\)</span> under <span class="math notranslate nohighlight">\(B\)</span>. That implies</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T L \mathbf{x}
= \mathbf{x}^T B B^T \mathbf{x}
= \|B^T \mathbf{x}\|^2
= \sum_{e = \{i,j\} \in E} (x_i - x_j)^2.
\]</div>
<p>Since the latter is always nonnegative, it also implies that <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We are now ready to derive connectivity consequences. Recall that, for any graph <span class="math notranslate nohighlight">\(G\)</span>, the Laplacian eigenvalue <span class="math notranslate nohighlight">\(\mu_1 = 0\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Laplacian and Connectivity)</strong> <span class="math notranslate nohighlight">\(\idx{Laplacian and connectivity lemma}\xdi\)</span> If <span class="math notranslate nohighlight">\(G\)</span> is connected, then the Laplacian eigenvalue <span class="math notranslate nohighlight">\(\mu_2 &gt; 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> with <span class="math notranslate nohighlight">\(n = |V|\)</span> and let <span class="math notranslate nohighlight">\(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\)</span> be a spectral decomposition of its Laplacian <span class="math notranslate nohighlight">\(L\)</span> with <span class="math notranslate nohighlight">\(0 = \mu_1 \leq \cdots \leq \mu_n\)</span>. Suppose by way of contradiction that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span>. Any eigenvector <span class="math notranslate nohighlight">\(\mathbf{y} = (y_{1}, \ldots, y_{n})\)</span> with <span class="math notranslate nohighlight">\(0\)</span> eigenvalue satisfies <span class="math notranslate nohighlight">\(L \mathbf{y} = \mathbf{0}\)</span> by definition. By the <em>Laplacian Quadratic Form  Lemma</em> then</p>
<div class="math notranslate nohighlight">
\[
0 
= \mathbf{y}^T L \mathbf{y}
= \sum_{e = \{i, j\} \in E} (y_{i} - y_{j})^2.
\]</div>
<p>1- In order for this to hold, it must be that any two adjacent vertices <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> have <span class="math notranslate nohighlight">\(y_{i} = y_{j}\)</span>. That is, <span class="math notranslate nohighlight">\(\{i,j\} \in E\)</span> implies <span class="math notranslate nohighlight">\(y_i = y_j\)</span>.</p>
<p>2- Furthermore, because <span class="math notranslate nohighlight">\(G\)</span> is connected, between any two of its vertices <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> - adjacent or not - there is a path <span class="math notranslate nohighlight">\(u = w_0 \sim \cdots \sim w_k = v\)</span> along which the <span class="math notranslate nohighlight">\(y_{w}\)</span>’s must be the same. Thus <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a constant vector.</p>
<p>But that is a contradiction since the eigenvectors <span class="math notranslate nohighlight">\(\mathbf{y}_1, \ldots, \mathbf{y}_n\)</span> are in fact linearly independent, so that <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span> cannot both be a constant vector. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The quantity <span class="math notranslate nohighlight">\(\mu_2\)</span> is sometimes referred to as the <a class="reference external" href="https://mathworld.wolfram.com/AlgebraicConnectivity.html">algebraic connectivity</a><span class="math notranslate nohighlight">\(\idx{algebraic connectivity}\xdi\)</span> of the graph. The corresponding eigenvector, <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span>, is known as the <a class="reference external" href="https://mathworld.wolfram.com/FiedlerVector.html">Fiedler vector</a><span class="math notranslate nohighlight">\(\idx{Fiedler vector}\xdi\)</span>.</p>
<p>We state the following (more general) converse result without proof.</p>
<p><strong>LEMMA</strong> If <span class="math notranslate nohighlight">\(\mu_{k+1}\)</span> is the smallest nonzero Laplacian eigenvalue of <span class="math notranslate nohighlight">\(G\)</span>, then <span class="math notranslate nohighlight">\(G\)</span> has <span class="math notranslate nohighlight">\(k\)</span> connected components. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>We will be interested in more quantitative results of this type. Before proceeding, we start with a simple observation. By our proof of the <em>Spectral Theorem</em>, the largest eigenvalue <span class="math notranslate nohighlight">\(\mu_n\)</span> of the Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> is the solution to the optimization problem</p>
<div class="math notranslate nohighlight">
\[
\mu_n = \max\{\langle \mathbf{x}, L \mathbf{x}\rangle:\|\mathbf{x}\| = 1\}.
\]</div>
<p>Such extremal characterization is useful in order to bound the eigenvalue <span class="math notranslate nohighlight">\(\mu_n\)</span>, since any choice of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with <span class="math notranslate nohighlight">\(\|\mathbf{x}\| =1\)</span> gives a lower bound through the quantity <span class="math notranslate nohighlight">\(\langle \mathbf{x}, L \mathbf{x}\rangle\)</span>. That perspective will be key to our application to graph partitioning.</p>
<p>For now, we give a simple consequence.</p>
<p><strong>LEMMA</strong> <strong>(Laplacian and Maximum Degree)</strong> <span class="math notranslate nohighlight">\(\idx{Laplacian and maximum degree lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with maximum degree <span class="math notranslate nohighlight">\(\bar{\delta}\)</span>. Let <span class="math notranslate nohighlight">\(\mu_n\)</span> be the largest eigenvalue of its Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\bar{\delta}+1 \leq \mu_n \leq 2 \bar{\delta}.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> As explained before the statement of the lemma, for the lower bound it suffices to find a good test unit vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to plug into <span class="math notranslate nohighlight">\(\langle \mathbf{x}, L \mathbf{x}\rangle\)</span>. A clever choice does the trick.</p>
<p><em>Proof:</em> We start with the lower bound. Let <span class="math notranslate nohighlight">\(u \in V\)</span> be a vertex with degree <span class="math notranslate nohighlight">\(\bar{\delta}\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> be the vector with entries</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_i 
=
\begin{cases}
\bar{\delta} &amp; \text{if $i = u$}\\
-1 &amp; \text{if $\{i,u\} \in E$}\\
0 &amp; \text{o.w.}
\end{cases}
\end{split}\]</div>
<p>and let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> be the unit vector <span class="math notranslate nohighlight">\(\mathbf{z}/\|\mathbf{z}\|\)</span>. By definition of the degree of <span class="math notranslate nohighlight">\(u\)</span>, <span class="math notranslate nohighlight">\(\|\mathbf{z}\|^2 = \bar{\delta}^2 + \bar{\delta}(-1)^2 = \bar{\delta}(\bar{\delta}+1)\)</span>. Using the <em>Laplacian Quadratic Form Lemma</em>,</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{z}, L \mathbf{z}\rangle
=
\sum_{e = \{i, j\} \in E} (z_i - z_j)^2
\geq
\sum_{i: \{i, u\} \in E} (z_i - z_u)^2
=
\sum_{i: \{i, u\} \in E} (-1 - \bar{\delta})^2
= \bar{\delta} (\bar{\delta}+1)^2
\]</div>
<p>where we restricted the sum to those edges incident with <span class="math notranslate nohighlight">\(u\)</span> and used the fact that all terms in the sum are nonnegative. Finally</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{x}, L \mathbf{x}\rangle
= \left\langle \frac{\mathbf{z}}{\|\mathbf{z}\|}, 
L \frac{\mathbf{z}}{\|\mathbf{z}\|}\right\rangle
= \frac{1}{\|\mathbf{z}\|^2} \langle \mathbf{z}, L \mathbf{z}\rangle
= \frac{\bar{\delta} (\bar{\delta}+1)^2}{\bar{\delta}(\bar{\delta}+1)}
= \bar{\delta}+1
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[
\mu_n 
= \max\{\langle \mathbf{x}', L \mathbf{x}'\rangle:\|\mathbf{x}'\| = 1\}
\geq \langle \mathbf{x}, L \mathbf{x}\rangle
= \bar{\delta}+1
\]</div>
<p>as claimed.</p>
<p>We proceed with the lower bound. For any unit vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\langle \mathbf{x}, L \mathbf{x}\rangle
&amp;= \sum_{i,j} L_{ij} x_i x_j\\
&amp;\leq \sum_{i,j} |L_{ij}| |x_i| |x_j|\\
&amp;= \sum_{i,j} (D_{ij} + A_{ij})  |x_i| |x_j|\\
&amp;= \sum_{i} \delta(i) \,x_i^2 
+ \sum_{i,j} A_{ij}  |x_i| |x_j|.
\end{align*}\]</div>
<p>By the <em>Cauchy-Schwarz inequality</em>, this is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\leq \bar{\delta}
+ \left(\sum_{i,j} A_{ij}  x_i^2\right)^{1/2}
\left(\sum_{i,j} A_{ij}  x_j^2\right)^{1/2}\\
&amp;\leq \bar{\delta} +  \left( \bar{\delta} \sum_{i} x_i^2\right)^{1/2}
\left(\bar{\delta} \sum_{j} x_j^2\right)^{1/2}\\
&amp;\leq 2\bar{\delta}.
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We construct a graph with two connected components and check the results above. We work directly with the adjacency matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 1 1 0 0]
 [1 0 1 0 0]
 [1 1 0 0 0]
 [0 0 0 0 1]
 [0 0 0 1 0]]
</pre></div>
</div>
</div>
</div>
<p>Note the block structure.</p>
<p>The degrees can be obtained by summing the rows of the adjacency matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">degrees</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">degrees</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[2 2 2 1 1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">degrees</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2 0 0 0 0]
 [0 2 0 0 0]
 [0 0 2 0 0]
 [0 0 0 1 0]
 [0 0 0 0 1]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="n">D</span> <span class="o">-</span> <span class="n">A</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 2 -1 -1  0  0]
 [-1  2 -1  0  0]
 [-1 -1  2  0  0]
 [ 0  0  0  1 -1]
 [ 0  0  0 -1  1]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 3.00000000e+00 -3.77809194e-16  3.00000000e+00  2.00000000e+00
  0.00000000e+00]
</pre></div>
</div>
</div>
</div>
<p>Observe that (up to numerical error) there are two <span class="math notranslate nohighlight">\(0\)</span> eigenvalues and that the largest eigenvalue is greater or equal than the maximum degree plus one.</p>
<p>To compute the Laplacian matrix, one can also use the function <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.linalg.laplacianmatrix.laplacian_matrix.html"><code class="docutils literal notranslate"><span class="pre">networkx.laplacian_matrix</span></code></a>. For example, the Laplacian of the Petersen graph is the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">petersen_graph</span><span class="p">()</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">laplacian_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 3 -1  0  0 -1 -1  0  0  0  0]
 [-1  3 -1  0  0  0 -1  0  0  0]
 [ 0 -1  3 -1  0  0  0 -1  0  0]
 [ 0  0 -1  3 -1  0  0  0 -1  0]
 [-1  0  0 -1  3  0  0  0  0 -1]
 [-1  0  0  0  0  3  0 -1 -1  0]
 [ 0 -1  0  0  0  0  3  0 -1 -1]
 [ 0  0 -1  0  0 -1  0  3  0 -1]
 [ 0  0  0 -1  0 -1 -1  0  3  0]
 [ 0  0  0  0 -1  0 -1 -1  0  3]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 5.00000000e+00  2.00000000e+00 -2.80861083e-17  5.00000000e+00
  5.00000000e+00  2.00000000e+00  2.00000000e+00  5.00000000e+00
  2.00000000e+00  2.00000000e+00]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="variational-characterization-of-second-laplacian-eigenvalue">
<h2><span class="section-number">5.4.3. </span>Variational characterization of second Laplacian eigenvalue<a class="headerlink" href="#variational-characterization-of-second-laplacian-eigenvalue" title="Link to this heading">#</a></h2>
<p>The definition <span class="math notranslate nohighlight">\(A \mathbf{x} = \lambda \mathbf{x}\)</span> is perhaps not the best way to understand why the eigenvectors of the Laplacian matrix are useful. Instead the following application of the <em>Courant-Fischer theorem</em><span class="math notranslate nohighlight">\(\idx{Courant-Fischer Theorem}\xdi\)</span> provides much insight, as we will see in the rest of this chapter.</p>
<p><strong>THEOREM</strong> <strong>(Variational Characterization of <span class="math notranslate nohighlight">\(\mu_2\)</span>)</strong> <span class="math notranslate nohighlight">\(\idx{variational characterization of the algebraic connectivity}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices. Assume the Laplacian <span class="math notranslate nohighlight">\(L\)</span> of <span class="math notranslate nohighlight">\(G\)</span> has spectral decomposition <span class="math notranslate nohighlight">\(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\)</span> with <span class="math notranslate nohighlight">\(0 = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mu_2 
= \min\left\{
\sum_{\{i, j\} \in E}(x_i - x_j)^2 \,:
\,\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n, 
\sum_{i=1}^n x_i = 0, \sum_{j = 1}^n x_j^2=1
\right\}.
\end{align*}\]</div>
<p>Taking <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{y}_2\)</span> achieves this minimum. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> By the <em>Courant-Fischer Theorem</em>,</p>
<div class="math notranslate nohighlight">
\[
\mu_2 
= \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{V}_{n-1} 
= \mathrm{span}(\mathbf{y}_2, \ldots,  \mathbf{y}_n)
= \mathrm{span}(\mathbf{y}_1)^\perp\)</span>. Observe that, because we reverse the order of the eigenvalues compared to the convention used in the <em>Courant-Fischer theorem</em>, we must adapt the definition of <span class="math notranslate nohighlight">\(\mathcal{V}_{n-1}\)</span> slightly. Moreover we know that <span class="math notranslate nohighlight">\(\mathcal{R}_L(\mathbf{y}_2) = \mu_2\)</span>. We make a simple transformation of the problem.</p>
<p>We claim that</p>
<div class="math notranslate nohighlight">
\[
\mu_2
= \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1, \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}. \qquad (*)
\]</div>
<p>Indeed, if <span class="math notranslate nohighlight">\(\mathbf{u} \in \mathrm{span}(\mathbf{y}_1)^\perp\)</span> has unit norm, i.e., <span class="math notranslate nohighlight">\(\|\mathbf{u}\| = 1\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_L(\mathbf{u})
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle \mathbf{u},\mathbf{u}\rangle}
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
= \langle \mathbf{u}, L \mathbf{u}\rangle.
\]</div>
<p>In other words, we shown that</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u})
\leq \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1, \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}.
\]</div>
<p>To prove the other direction, for any <span class="math notranslate nohighlight">\(\mathbf{u} \neq \mathbf{0}\)</span>, we can normalize it by defining <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{u}/\|\mathbf{u}\|\)</span> and we note that</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_L(\mathbf{u})
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle \mathbf{u},\mathbf{u}\rangle}
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
= \left\langle \frac{\mathbf{u}}{\|\mathbf{u}\|}, L \frac{\mathbf{u}}{\|\mathbf{u}\|}\right\rangle
= \langle \mathbf{x}, L \mathbf{x}\rangle.
\]</div>
<p>Moreover <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{y}_1\rangle = 0\)</span> if only if <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y}_1\rangle = 0\)</span>. That establishes <span class="math notranslate nohighlight">\((*)\)</span>, since any objective value achieved in the original formulation can be achieved in the new one.</p>
<p>Using that <span class="math notranslate nohighlight">\(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\)</span>, the condition <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y}_1 \rangle = 0\)</span>, i.e., <span class="math notranslate nohighlight">\(\sum_{i=1}^n (x_i/\sqrt{n}) = 0\)</span>, is equivalent to <span class="math notranslate nohighlight">\(\sum_{i=1}^n x_i = 0\)</span>. Similary, the condition <span class="math notranslate nohighlight">\(\|\mathbf{x}\|=1\)</span> is equivalent, after squaring each side, to <span class="math notranslate nohighlight">\(\sum_{j=1}^n x_j^2 = 1\)</span>.</p>
<p>Finally, the claim follows from the <em>Laplacian Quadratic Form Lemma</em>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>One application of this extremal characterization is the graph drawing heuristic we described previously. Consider the entries of the second Laplacian eigenvector <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span>. Its entries are centered around <span class="math notranslate nohighlight">\(0\)</span> by the condition <span class="math notranslate nohighlight">\(\langle \mathbf{y}_1, \mathbf{y}_2\rangle  = 0\)</span>. Because it minimizes the following quantity over all centered unit vectors,</p>
<div class="math notranslate nohighlight">
\[
\sum_{\{i, j\} \in E} (x_i - x_j)^2
\]</div>
<p>the eigenvector <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span> tends to assign similar coordinates to adjacent vertices. A similar reasoning applies to the third Laplacian eigenvector, which in addition is orthogonal to the second one. So coordinates based on the second and third Laplacian eigenvectors should be expected to position adjacent vertices close-by and hence minimizing the need for long-range edges in the visualization. In particular, it reveals some of the underlying Euclidean geometry of the graph, as the next example shows.</p>
<p><strong>NUMERICAL CORNER:</strong> This is perhaps easiest to see on a path graph. Recall that NetworkX numbers vertices <span class="math notranslate nohighlight">\(0,\ldots,n-1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">path_graph</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the second Laplacian eigenvector (i.e., the eigenvector of the Laplacian matrix corresponding to the second smallest eigenvalue). We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.argsort.html"><code class="docutils literal notranslate"><span class="pre">numpy.argsort</span></code></a> to find the index of the second smallest eigenvalue. Because indices start at <code class="docutils literal notranslate"><span class="pre">0</span></code>, we want entry <code class="docutils literal notranslate"><span class="pre">1</span></code> of the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">laplacian_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">w</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">w</span><span class="p">)[</span><span class="mi">1</span><span class="p">]]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c346aff858ca3853432ca68560966ff40a69ed45112737ae2d576de9ef3cebb7.png" src="../Images/aa1979a98f2c16cedc4afdcc1b494809.png" data-original-src="https://mmids-textbook.github.io/_images/c346aff858ca3853432ca68560966ff40a69ed45112737ae2d576de9ef3cebb7.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Two-Component Graph)</strong> Let <span class="math notranslate nohighlight">\(G=(V,E)\)</span> be a graph with two connected components <span class="math notranslate nohighlight">\(\emptyset \neq V_1, V_2 \subseteq V\)</span>. By the properties of connected components, we have <span class="math notranslate nohighlight">\(V_1 \cap V_2 = \emptyset\)</span> and <span class="math notranslate nohighlight">\(V_1 \cup V_2 = V\)</span>. Assume the Laplacian <span class="math notranslate nohighlight">\(L\)</span> of <span class="math notranslate nohighlight">\(G\)</span> has spectral decomposition <span class="math notranslate nohighlight">\(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\)</span> with <span class="math notranslate nohighlight">\(0 = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\)</span>. We claimed earlier that for such a graph <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span>. We prove this here using the <em>Variational Characterization of <span class="math notranslate nohighlight">\(\mu_2\)</span></em></p>
<div class="math notranslate nohighlight">
\[
\mu_2 
= \min\left\{
\sum_{\{u, v\} \in E} (x_u - x_v)^2 \,:\,
\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{u=1}^n x_u = 0, \sum_{u = 1}^n x_u^2=1
\right\}.
\]</div>
<p>Based on this characterization, it suffices to find a vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> satisfying <span class="math notranslate nohighlight">\(\sum_{u=1}^n x_u = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_{u = 1}^n x_u^2=1\)</span> such that <span class="math notranslate nohighlight">\(\sum_{\{u, v\} \in E} (x_u - x_v)^2 = 0\)</span>. Indeed, since <span class="math notranslate nohighlight">\(\mu_2 \geq 0\)</span> and any such <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> gives an upper bound on <span class="math notranslate nohighlight">\(\mu_2\)</span>, we then necessarily have that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(\sum_{\{u, v\} \in E} (x_u - x_v)^2\)</span> to be <span class="math notranslate nohighlight">\(0\)</span>, one might be tempted to take a constant vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. But then we could not satisfy <span class="math notranslate nohighlight">\(\sum_{u=1}^n x_u = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_{u = 1}^n x_u^2=1\)</span> simultaneously. Instead, we modify this guess slightly. Because the graph has two connected components, there is no edge between <span class="math notranslate nohighlight">\(V_1\)</span> and <span class="math notranslate nohighlight">\(V_2\)</span>. Hence we can assign a different value to each component and still get <span class="math notranslate nohighlight">\(\sum_{\{u, v\} \in E} (x_u - x_v)^2 = 0\)</span>. So we look for a vector <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_n)\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x_u = \begin{cases}
\alpha, &amp; \text{if $u \in V_1$,}\\
\beta, &amp; \text{if $u \in V_2$.}
\end{cases}
\end{split}\]</div>
<p>To satisfy the constraints on <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we require</p>
<div class="math notranslate nohighlight">
\[
\sum_{u=1}^n x_u
= \sum_{u \in V_1} \alpha + \sum_{u \in V_2} \beta
= |V_1| \alpha + |V_2| \beta
= 0,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\sum_{u=1}^n x_u^2
= \sum_{u \in V_1} \alpha^2 + \sum_{u \in V_2} \beta^2
= |V_1| \alpha^2 + |V_2| \beta^2
= 1.
\]</div>
<p>Replacing the first equation in the second one, we get</p>
<div class="math notranslate nohighlight">
\[
|V_1| \left(\frac{-|V_2|\beta}{|V_1|}\right)^2 + |V_2| \beta^2
= \frac{|V_2|^2 \beta^2}{|V_1|} + |V_2| \beta^2
= 1,
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\beta^2 = \frac{|V_1|}{|V_2|(|V_2| + |V_1|)} = \frac{|V_1|}{n |V_2|}. 
\]</div>
<p>Take</p>
<div class="math notranslate nohighlight">
\[
\beta
= - \sqrt{\frac{|V_1|}{n |V_2|}},
\qquad
\alpha = \frac{-|V_2|\beta}{|V_1|} = \sqrt{\frac{|V_2|}{n |V_1|}}.
\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> we constructed is in fact an eigenvector of <span class="math notranslate nohighlight">\(L\)</span>. Indeed, let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span>. Then, for <span class="math notranslate nohighlight">\(e_k = \{u,v\}\)</span>, <span class="math notranslate nohighlight">\((B^T \mathbf{x})_k\)</span> is either <span class="math notranslate nohighlight">\(x_u - x_v\)</span> or <span class="math notranslate nohighlight">\(x_v - x_u\)</span>. In both cases, that is <span class="math notranslate nohighlight">\(0\)</span>. So <span class="math notranslate nohighlight">\(L \mathbf{x} = B B^T \mathbf{x} = \mathbf{0}\)</span>, that is, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(L\)</span> with eigenvalue <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>We have shown that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span> when <span class="math notranslate nohighlight">\(G\)</span> has two connected components. A slight modification of this argument shows that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span> whenever <span class="math notranslate nohighlight">\(G\)</span> is not connected. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following is NOT a property of the Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> of a graph <span class="math notranslate nohighlight">\(G\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(L\)</span> is symmetric.</p>
<p>b) <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite.</p>
<p>c) The constant unit vector <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}}(1,\ldots,1)\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(L\)</span> with eigenvalue 0.</p>
<p>d) <span class="math notranslate nohighlight">\(L\)</span> is positive definite.</p>
<p><strong>2</strong> Which vector is known as the Fiedler vector?</p>
<p>a) The eigenvector corresponding to the largest eigenvalue of the Laplacian matrix.</p>
<p>b) The eigenvector corresponding to the smallest eigenvalue of the Laplacian matrix.</p>
<p>c) The eigenvector corresponding to the second smallest eigenvalue of the Laplacian matrix.</p>
<p>d) The eigenvector corresponding to the average of all eigenvalues of the Laplacian matrix.</p>
<p><strong>3</strong> For a connected graph <span class="math notranslate nohighlight">\(G\)</span>, which of the following statements about the second smallest eigenvalue <span class="math notranslate nohighlight">\(\mu_2\)</span> of its Laplacian matrix is true?</p>
<p>a) <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mu_2 &lt; 0\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mu_2 &gt; 0\)</span></p>
<p>d) The value of <span class="math notranslate nohighlight">\(\mu_2\)</span> cannot be determined without additional information.</p>
<p><strong>4</strong> The Laplacian quadratic form <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> for a graph <span class="math notranslate nohighlight">\(G\)</span> with Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T L \mathbf{x} = \sum_{\{i,j\} \in E} (x_i - x_j)^2.
\]</div>
<p>What does this quadratic form measure?</p>
<p>a) The average distance between vertices in the graph.</p>
<p>b) The number of connected components in the graph.</p>
<p>c) The “smoothness” of the function <span class="math notranslate nohighlight">\(x\)</span> over the graph.</p>
<p>d) The degree of each vertex in the graph.</p>
<p><strong>5</strong> The Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> of a graph <span class="math notranslate nohighlight">\(G\)</span> can be decomposed as <span class="math notranslate nohighlight">\(L = B B^T\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is an oriented incidence matrix. What does this decomposition imply about <span class="math notranslate nohighlight">\(L\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(L\)</span> is positive definite</p>
<p>b) <span class="math notranslate nohighlight">\(L\)</span> is symmetric and positive semidefinite</p>
<p>c) <span class="math notranslate nohighlight">\(L\)</span> is antisymmetric</p>
<p>d) <span class="math notranslate nohighlight">\(L\)</span> is a diagonal matrix</p>
<p>Answer for 1: d. Justification: The text states that “because <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite, the eigenvalues are nonnegative,” but it does not claim that <span class="math notranslate nohighlight">\(L\)</span> is positive definite.</p>
<p>Answer for 2: c. Justification: The text refers to the eigenvector corresponding to <span class="math notranslate nohighlight">\(\mu_2\)</span> (the second smallest eigenvalue) as the Fiedler vector.</p>
<p>Answer for 3: c. Justification: The text proves that “If <span class="math notranslate nohighlight">\(G\)</span> is connected, then the Laplacian eigenvalue <span class="math notranslate nohighlight">\(\mu_2 &gt; 0\)</span>.”</p>
<p>Answer for 4: c. Justification: The text states that “the Laplacian quadratic form measures how ‘smooth’ the function <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is over the graph in the following sense. A small value of <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> indicates that adjacent vertices tend to get assigned close values.”</p>
<p>Answer for 5: b. Justification: The text states, “Let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span>. By construction, <span class="math notranslate nohighlight">\(L = B B^T\)</span>. This implies that <span class="math notranslate nohighlight">\(L\)</span> is symmetric and positive semidefinite.”</p>
</section>
&#13;

<h2><span class="section-number">5.4.1. </span>Eigenvalues of the Laplacian matrix: first observations<a class="headerlink" href="#eigenvalues-of-the-laplacian-matrix-first-observations" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices. Two observations:</p>
<p>1- Since the Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> of <span class="math notranslate nohighlight">\(G\)</span> is symmetric, by the <em>Spectral Theorem</em>, it has a spectral decomposition</p>
<div class="math notranslate nohighlight">
\[
L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T
\]</div>
<p>where the <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>’s form an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>.</p>
<p>2- Further, because <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite, the eigenvalues are nonnegative. By convention, we assume</p>
<div class="math notranslate nohighlight">
\[
0 \leq \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n.
\]</div>
<p>Note that this is the opposite order to we used in the previous section.</p>
<p>Another observation:</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices and Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span>. The constant unit vector</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y}_1 
= 
\frac{1}{\sqrt{n}} (1, \ldots, 1)
\]</div>
<p>is an eigenvector of <span class="math notranslate nohighlight">\(L\)</span> with eigenvalue <span class="math notranslate nohighlight">\(0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span> recall that <span class="math notranslate nohighlight">\(L = B B^T\)</span>. By construction <span class="math notranslate nohighlight">\(B^T \mathbf{y}_1 = \mathbf{0}\)</span> since each column of <span class="math notranslate nohighlight">\(B\)</span> has exactly one <span class="math notranslate nohighlight">\(1\)</span> and one <span class="math notranslate nohighlight">\(-1\)</span>. So <span class="math notranslate nohighlight">\(L \mathbf{y}_1 = B B^T \mathbf{y}_1 = \mathbf{0}\)</span> as claimed. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>In general, the constant vector may not be the only eigenvector with eigenvalue one.</p>
<p><strong>NUMERICAL CORNER:</strong> One use of the spectral decomposition of the Laplacian matrix is in graph drawing<span class="math notranslate nohighlight">\(\idx{graph drawing}\xdi\)</span>. We illustrate this next. Given a graph <span class="math notranslate nohighlight">\(G = (V, E)\)</span>, it is not clear a priori how to draw it in the plane since the only information available are adjacencies of vertices. One approach is just to position the vertices at random. The function <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw.html"><code class="docutils literal notranslate"><span class="pre">networkx.draw</span></code></a> or <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html#networkx.drawing.nx_pylab.draw_networkx"><code class="docutils literal notranslate"><span class="pre">networkx.draw_networkx</span></code></a> can take as input different <a class="reference external" href="https://networkx.org/documentation/stable/reference/drawing.html#module-networkx.drawing.layout">graph layout</a> functions that return an <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>-coordinate for each vertex.</p>
<p>We will test this on a grid graph. We use <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.generators.lattice.grid_2d_graph.html"><code class="docutils literal notranslate"><span class="pre">networkx.grid_2d_graph</span></code></a> to construct such a graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">grid_2d_graph</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>One layout approach is to choose random locations for the nodes. Specifically, for every node, a position is generated by choosing each coordinate uniformly at random on the interval <span class="math notranslate nohighlight">\([0,1]\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">random_layout</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">535</span><span class="p">),</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">node_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e71c11388af95019112c93563223107aea5d4aa3b91dbab26b654c57b7ad303b.png" src="../Images/f62f7cee4755a1c33c92335ca3d78f91.png" data-original-src="https://mmids-textbook.github.io/_images/e71c11388af95019112c93563223107aea5d4aa3b91dbab26b654c57b7ad303b.png"/>
</div>
</div>
<p>Clearly, this is hard to read.</p>
<p>Another approach is to map the vertices to two eigenvectors, similarly to what we did for dimensionality reduction. The eigenvector associated to <span class="math notranslate nohighlight">\(\mu_1\)</span> is constant and therefore not useful for drawing. We try the next two. We use the Laplacian matrix. This is done using <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.drawing.layout.spectral_layout.html#networkx.drawing.layout.spectral_layout"><code class="docutils literal notranslate"><span class="pre">networkx.spectral_layout</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">spectral_layout</span><span class="p">(</span><span class="n">G</span><span class="p">),</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">node_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d9f18189eb6a51676466d9a2ffe9aa688e95d0f15f8697b68ee0c7c9ae2ada45.png" src="../Images/0ecdf40ac77cc8a246dd3cb1077c885d.png" data-original-src="https://mmids-textbook.github.io/_images/d9f18189eb6a51676466d9a2ffe9aa688e95d0f15f8697b68ee0c7c9ae2ada45.png"/>
</div>
</div>
<p>Interestingly, the outcome is provides a much more natural drawing of the graph, revealing its underlying structure as a grid. We will come back later to try to explain this, after we have developed further understanding of the spectral properties of the Laplacian matrix.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
&#13;

<h2><span class="section-number">5.4.2. </span>Laplacian matrix and connectivity<a class="headerlink" href="#laplacian-matrix-and-connectivity" title="Link to this heading">#</a></h2>
<p>As we indicated before, the Laplacian matrix contains information about the connectedness of <span class="math notranslate nohighlight">\(G\)</span>. We elaborate on a first concrete connection here. But first we will need a useful form of the Laplaican quadratic form <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> which enters in the variational charaterization of the eigenvalues.</p>
<p><strong>LEMMA</strong> <strong>(Laplacian Quadratic Form)</strong> <span class="math notranslate nohighlight">\(\idx{Laplacian quadratic form lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices and Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span>. We have the following formula for the Laplacian quadratic form</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T L \mathbf{x}
= \sum_{e = \{i,j\} \in E} (x_i - x_j)^2
\]</div>
<p>for any <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>Here is an intuitive way of interpreting this lemma. If one thinks of <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n\)</span> as a real-valued function over the vertices (i.e., it associates a real value <span class="math notranslate nohighlight">\(x_i\)</span> to vertex <span class="math notranslate nohighlight">\(i\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>), then the Laplacian quadratic form measures how “smooth” the function is over the graph in the following sense. A small value of <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> indicates that adjacent vertices tend to get assigned close values.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span>. We have that <span class="math notranslate nohighlight">\(L = B B^T\)</span>. Thus, for any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we have <span class="math notranslate nohighlight">\((B^T \mathbf{x})_k = x_v - x_u\)</span> if the edge <span class="math notranslate nohighlight">\(e_k = \{u, v\}\)</span> is oriented as <span class="math notranslate nohighlight">\((u,v)\)</span> under <span class="math notranslate nohighlight">\(B\)</span>. That implies</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T L \mathbf{x}
= \mathbf{x}^T B B^T \mathbf{x}
= \|B^T \mathbf{x}\|^2
= \sum_{e = \{i,j\} \in E} (x_i - x_j)^2.
\]</div>
<p>Since the latter is always nonnegative, it also implies that <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We are now ready to derive connectivity consequences. Recall that, for any graph <span class="math notranslate nohighlight">\(G\)</span>, the Laplacian eigenvalue <span class="math notranslate nohighlight">\(\mu_1 = 0\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Laplacian and Connectivity)</strong> <span class="math notranslate nohighlight">\(\idx{Laplacian and connectivity lemma}\xdi\)</span> If <span class="math notranslate nohighlight">\(G\)</span> is connected, then the Laplacian eigenvalue <span class="math notranslate nohighlight">\(\mu_2 &gt; 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> with <span class="math notranslate nohighlight">\(n = |V|\)</span> and let <span class="math notranslate nohighlight">\(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\)</span> be a spectral decomposition of its Laplacian <span class="math notranslate nohighlight">\(L\)</span> with <span class="math notranslate nohighlight">\(0 = \mu_1 \leq \cdots \leq \mu_n\)</span>. Suppose by way of contradiction that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span>. Any eigenvector <span class="math notranslate nohighlight">\(\mathbf{y} = (y_{1}, \ldots, y_{n})\)</span> with <span class="math notranslate nohighlight">\(0\)</span> eigenvalue satisfies <span class="math notranslate nohighlight">\(L \mathbf{y} = \mathbf{0}\)</span> by definition. By the <em>Laplacian Quadratic Form  Lemma</em> then</p>
<div class="math notranslate nohighlight">
\[
0 
= \mathbf{y}^T L \mathbf{y}
= \sum_{e = \{i, j\} \in E} (y_{i} - y_{j})^2.
\]</div>
<p>1- In order for this to hold, it must be that any two adjacent vertices <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> have <span class="math notranslate nohighlight">\(y_{i} = y_{j}\)</span>. That is, <span class="math notranslate nohighlight">\(\{i,j\} \in E\)</span> implies <span class="math notranslate nohighlight">\(y_i = y_j\)</span>.</p>
<p>2- Furthermore, because <span class="math notranslate nohighlight">\(G\)</span> is connected, between any two of its vertices <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> - adjacent or not - there is a path <span class="math notranslate nohighlight">\(u = w_0 \sim \cdots \sim w_k = v\)</span> along which the <span class="math notranslate nohighlight">\(y_{w}\)</span>’s must be the same. Thus <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a constant vector.</p>
<p>But that is a contradiction since the eigenvectors <span class="math notranslate nohighlight">\(\mathbf{y}_1, \ldots, \mathbf{y}_n\)</span> are in fact linearly independent, so that <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span> cannot both be a constant vector. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The quantity <span class="math notranslate nohighlight">\(\mu_2\)</span> is sometimes referred to as the <a class="reference external" href="https://mathworld.wolfram.com/AlgebraicConnectivity.html">algebraic connectivity</a><span class="math notranslate nohighlight">\(\idx{algebraic connectivity}\xdi\)</span> of the graph. The corresponding eigenvector, <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span>, is known as the <a class="reference external" href="https://mathworld.wolfram.com/FiedlerVector.html">Fiedler vector</a><span class="math notranslate nohighlight">\(\idx{Fiedler vector}\xdi\)</span>.</p>
<p>We state the following (more general) converse result without proof.</p>
<p><strong>LEMMA</strong> If <span class="math notranslate nohighlight">\(\mu_{k+1}\)</span> is the smallest nonzero Laplacian eigenvalue of <span class="math notranslate nohighlight">\(G\)</span>, then <span class="math notranslate nohighlight">\(G\)</span> has <span class="math notranslate nohighlight">\(k\)</span> connected components. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>We will be interested in more quantitative results of this type. Before proceeding, we start with a simple observation. By our proof of the <em>Spectral Theorem</em>, the largest eigenvalue <span class="math notranslate nohighlight">\(\mu_n\)</span> of the Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> is the solution to the optimization problem</p>
<div class="math notranslate nohighlight">
\[
\mu_n = \max\{\langle \mathbf{x}, L \mathbf{x}\rangle:\|\mathbf{x}\| = 1\}.
\]</div>
<p>Such extremal characterization is useful in order to bound the eigenvalue <span class="math notranslate nohighlight">\(\mu_n\)</span>, since any choice of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with <span class="math notranslate nohighlight">\(\|\mathbf{x}\| =1\)</span> gives a lower bound through the quantity <span class="math notranslate nohighlight">\(\langle \mathbf{x}, L \mathbf{x}\rangle\)</span>. That perspective will be key to our application to graph partitioning.</p>
<p>For now, we give a simple consequence.</p>
<p><strong>LEMMA</strong> <strong>(Laplacian and Maximum Degree)</strong> <span class="math notranslate nohighlight">\(\idx{Laplacian and maximum degree lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with maximum degree <span class="math notranslate nohighlight">\(\bar{\delta}\)</span>. Let <span class="math notranslate nohighlight">\(\mu_n\)</span> be the largest eigenvalue of its Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\bar{\delta}+1 \leq \mu_n \leq 2 \bar{\delta}.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> As explained before the statement of the lemma, for the lower bound it suffices to find a good test unit vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to plug into <span class="math notranslate nohighlight">\(\langle \mathbf{x}, L \mathbf{x}\rangle\)</span>. A clever choice does the trick.</p>
<p><em>Proof:</em> We start with the lower bound. Let <span class="math notranslate nohighlight">\(u \in V\)</span> be a vertex with degree <span class="math notranslate nohighlight">\(\bar{\delta}\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> be the vector with entries</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_i 
=
\begin{cases}
\bar{\delta} &amp; \text{if $i = u$}\\
-1 &amp; \text{if $\{i,u\} \in E$}\\
0 &amp; \text{o.w.}
\end{cases}
\end{split}\]</div>
<p>and let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> be the unit vector <span class="math notranslate nohighlight">\(\mathbf{z}/\|\mathbf{z}\|\)</span>. By definition of the degree of <span class="math notranslate nohighlight">\(u\)</span>, <span class="math notranslate nohighlight">\(\|\mathbf{z}\|^2 = \bar{\delta}^2 + \bar{\delta}(-1)^2 = \bar{\delta}(\bar{\delta}+1)\)</span>. Using the <em>Laplacian Quadratic Form Lemma</em>,</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{z}, L \mathbf{z}\rangle
=
\sum_{e = \{i, j\} \in E} (z_i - z_j)^2
\geq
\sum_{i: \{i, u\} \in E} (z_i - z_u)^2
=
\sum_{i: \{i, u\} \in E} (-1 - \bar{\delta})^2
= \bar{\delta} (\bar{\delta}+1)^2
\]</div>
<p>where we restricted the sum to those edges incident with <span class="math notranslate nohighlight">\(u\)</span> and used the fact that all terms in the sum are nonnegative. Finally</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{x}, L \mathbf{x}\rangle
= \left\langle \frac{\mathbf{z}}{\|\mathbf{z}\|}, 
L \frac{\mathbf{z}}{\|\mathbf{z}\|}\right\rangle
= \frac{1}{\|\mathbf{z}\|^2} \langle \mathbf{z}, L \mathbf{z}\rangle
= \frac{\bar{\delta} (\bar{\delta}+1)^2}{\bar{\delta}(\bar{\delta}+1)}
= \bar{\delta}+1
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[
\mu_n 
= \max\{\langle \mathbf{x}', L \mathbf{x}'\rangle:\|\mathbf{x}'\| = 1\}
\geq \langle \mathbf{x}, L \mathbf{x}\rangle
= \bar{\delta}+1
\]</div>
<p>as claimed.</p>
<p>We proceed with the lower bound. For any unit vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\langle \mathbf{x}, L \mathbf{x}\rangle
&amp;= \sum_{i,j} L_{ij} x_i x_j\\
&amp;\leq \sum_{i,j} |L_{ij}| |x_i| |x_j|\\
&amp;= \sum_{i,j} (D_{ij} + A_{ij})  |x_i| |x_j|\\
&amp;= \sum_{i} \delta(i) \,x_i^2 
+ \sum_{i,j} A_{ij}  |x_i| |x_j|.
\end{align*}\]</div>
<p>By the <em>Cauchy-Schwarz inequality</em>, this is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\leq \bar{\delta}
+ \left(\sum_{i,j} A_{ij}  x_i^2\right)^{1/2}
\left(\sum_{i,j} A_{ij}  x_j^2\right)^{1/2}\\
&amp;\leq \bar{\delta} +  \left( \bar{\delta} \sum_{i} x_i^2\right)^{1/2}
\left(\bar{\delta} \sum_{j} x_j^2\right)^{1/2}\\
&amp;\leq 2\bar{\delta}.
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We construct a graph with two connected components and check the results above. We work directly with the adjacency matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 1 1 0 0]
 [1 0 1 0 0]
 [1 1 0 0 0]
 [0 0 0 0 1]
 [0 0 0 1 0]]
</pre></div>
</div>
</div>
</div>
<p>Note the block structure.</p>
<p>The degrees can be obtained by summing the rows of the adjacency matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">degrees</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">degrees</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[2 2 2 1 1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">degrees</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2 0 0 0 0]
 [0 2 0 0 0]
 [0 0 2 0 0]
 [0 0 0 1 0]
 [0 0 0 0 1]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="n">D</span> <span class="o">-</span> <span class="n">A</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 2 -1 -1  0  0]
 [-1  2 -1  0  0]
 [-1 -1  2  0  0]
 [ 0  0  0  1 -1]
 [ 0  0  0 -1  1]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 3.00000000e+00 -3.77809194e-16  3.00000000e+00  2.00000000e+00
  0.00000000e+00]
</pre></div>
</div>
</div>
</div>
<p>Observe that (up to numerical error) there are two <span class="math notranslate nohighlight">\(0\)</span> eigenvalues and that the largest eigenvalue is greater or equal than the maximum degree plus one.</p>
<p>To compute the Laplacian matrix, one can also use the function <a class="reference external" href="https://networkx.org/documentation/stable/reference/generated/networkx.linalg.laplacianmatrix.laplacian_matrix.html"><code class="docutils literal notranslate"><span class="pre">networkx.laplacian_matrix</span></code></a>. For example, the Laplacian of the Petersen graph is the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">petersen_graph</span><span class="p">()</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">laplacian_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 3 -1  0  0 -1 -1  0  0  0  0]
 [-1  3 -1  0  0  0 -1  0  0  0]
 [ 0 -1  3 -1  0  0  0 -1  0  0]
 [ 0  0 -1  3 -1  0  0  0 -1  0]
 [-1  0  0 -1  3  0  0  0  0 -1]
 [-1  0  0  0  0  3  0 -1 -1  0]
 [ 0 -1  0  0  0  0  3  0 -1 -1]
 [ 0  0 -1  0  0 -1  0  3  0 -1]
 [ 0  0  0 -1  0 -1 -1  0  3  0]
 [ 0  0  0  0 -1  0 -1 -1  0  3]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 5.00000000e+00  2.00000000e+00 -2.80861083e-17  5.00000000e+00
  5.00000000e+00  2.00000000e+00  2.00000000e+00  5.00000000e+00
  2.00000000e+00  2.00000000e+00]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
&#13;

<h2><span class="section-number">5.4.3. </span>Variational characterization of second Laplacian eigenvalue<a class="headerlink" href="#variational-characterization-of-second-laplacian-eigenvalue" title="Link to this heading">#</a></h2>
<p>The definition <span class="math notranslate nohighlight">\(A \mathbf{x} = \lambda \mathbf{x}\)</span> is perhaps not the best way to understand why the eigenvectors of the Laplacian matrix are useful. Instead the following application of the <em>Courant-Fischer theorem</em><span class="math notranslate nohighlight">\(\idx{Courant-Fischer Theorem}\xdi\)</span> provides much insight, as we will see in the rest of this chapter.</p>
<p><strong>THEOREM</strong> <strong>(Variational Characterization of <span class="math notranslate nohighlight">\(\mu_2\)</span>)</strong> <span class="math notranslate nohighlight">\(\idx{variational characterization of the algebraic connectivity}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V, E)\)</span> be a graph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices. Assume the Laplacian <span class="math notranslate nohighlight">\(L\)</span> of <span class="math notranslate nohighlight">\(G\)</span> has spectral decomposition <span class="math notranslate nohighlight">\(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\)</span> with <span class="math notranslate nohighlight">\(0 = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mu_2 
= \min\left\{
\sum_{\{i, j\} \in E}(x_i - x_j)^2 \,:
\,\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n, 
\sum_{i=1}^n x_i = 0, \sum_{j = 1}^n x_j^2=1
\right\}.
\end{align*}\]</div>
<p>Taking <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{y}_2\)</span> achieves this minimum. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> By the <em>Courant-Fischer Theorem</em>,</p>
<div class="math notranslate nohighlight">
\[
\mu_2 
= \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{V}_{n-1} 
= \mathrm{span}(\mathbf{y}_2, \ldots,  \mathbf{y}_n)
= \mathrm{span}(\mathbf{y}_1)^\perp\)</span>. Observe that, because we reverse the order of the eigenvalues compared to the convention used in the <em>Courant-Fischer theorem</em>, we must adapt the definition of <span class="math notranslate nohighlight">\(\mathcal{V}_{n-1}\)</span> slightly. Moreover we know that <span class="math notranslate nohighlight">\(\mathcal{R}_L(\mathbf{y}_2) = \mu_2\)</span>. We make a simple transformation of the problem.</p>
<p>We claim that</p>
<div class="math notranslate nohighlight">
\[
\mu_2
= \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1, \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}. \qquad (*)
\]</div>
<p>Indeed, if <span class="math notranslate nohighlight">\(\mathbf{u} \in \mathrm{span}(\mathbf{y}_1)^\perp\)</span> has unit norm, i.e., <span class="math notranslate nohighlight">\(\|\mathbf{u}\| = 1\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_L(\mathbf{u})
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle \mathbf{u},\mathbf{u}\rangle}
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
= \langle \mathbf{u}, L \mathbf{u}\rangle.
\]</div>
<p>In other words, we shown that</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{n-1}} \mathcal{R}_L(\mathbf{u})
\leq \min\left\{\langle \mathbf{x}, L \mathbf{x}\rangle\,:\ \|\mathbf{x}\|=1, \langle \mathbf{x}, \mathbf{y}_1\rangle = 0 \right\}.
\]</div>
<p>To prove the other direction, for any <span class="math notranslate nohighlight">\(\mathbf{u} \neq \mathbf{0}\)</span>, we can normalize it by defining <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{u}/\|\mathbf{u}\|\)</span> and we note that</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_L(\mathbf{u})
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\langle \mathbf{u},\mathbf{u}\rangle}
= \frac{\langle \mathbf{u}, L \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
= \left\langle \frac{\mathbf{u}}{\|\mathbf{u}\|}, L \frac{\mathbf{u}}{\|\mathbf{u}\|}\right\rangle
= \langle \mathbf{x}, L \mathbf{x}\rangle.
\]</div>
<p>Moreover <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{y}_1\rangle = 0\)</span> if only if <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y}_1\rangle = 0\)</span>. That establishes <span class="math notranslate nohighlight">\((*)\)</span>, since any objective value achieved in the original formulation can be achieved in the new one.</p>
<p>Using that <span class="math notranslate nohighlight">\(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\)</span>, the condition <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y}_1 \rangle = 0\)</span>, i.e., <span class="math notranslate nohighlight">\(\sum_{i=1}^n (x_i/\sqrt{n}) = 0\)</span>, is equivalent to <span class="math notranslate nohighlight">\(\sum_{i=1}^n x_i = 0\)</span>. Similary, the condition <span class="math notranslate nohighlight">\(\|\mathbf{x}\|=1\)</span> is equivalent, after squaring each side, to <span class="math notranslate nohighlight">\(\sum_{j=1}^n x_j^2 = 1\)</span>.</p>
<p>Finally, the claim follows from the <em>Laplacian Quadratic Form Lemma</em>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>One application of this extremal characterization is the graph drawing heuristic we described previously. Consider the entries of the second Laplacian eigenvector <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span>. Its entries are centered around <span class="math notranslate nohighlight">\(0\)</span> by the condition <span class="math notranslate nohighlight">\(\langle \mathbf{y}_1, \mathbf{y}_2\rangle  = 0\)</span>. Because it minimizes the following quantity over all centered unit vectors,</p>
<div class="math notranslate nohighlight">
\[
\sum_{\{i, j\} \in E} (x_i - x_j)^2
\]</div>
<p>the eigenvector <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span> tends to assign similar coordinates to adjacent vertices. A similar reasoning applies to the third Laplacian eigenvector, which in addition is orthogonal to the second one. So coordinates based on the second and third Laplacian eigenvectors should be expected to position adjacent vertices close-by and hence minimizing the need for long-range edges in the visualization. In particular, it reveals some of the underlying Euclidean geometry of the graph, as the next example shows.</p>
<p><strong>NUMERICAL CORNER:</strong> This is perhaps easiest to see on a path graph. Recall that NetworkX numbers vertices <span class="math notranslate nohighlight">\(0,\ldots,n-1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">path_graph</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot the second Laplacian eigenvector (i.e., the eigenvector of the Laplacian matrix corresponding to the second smallest eigenvalue). We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.argsort.html"><code class="docutils literal notranslate"><span class="pre">numpy.argsort</span></code></a> to find the index of the second smallest eigenvalue. Because indices start at <code class="docutils literal notranslate"><span class="pre">0</span></code>, we want entry <code class="docutils literal notranslate"><span class="pre">1</span></code> of the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">laplacian_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">w</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">w</span><span class="p">)[</span><span class="mi">1</span><span class="p">]]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c346aff858ca3853432ca68560966ff40a69ed45112737ae2d576de9ef3cebb7.png" src="../Images/aa1979a98f2c16cedc4afdcc1b494809.png" data-original-src="https://mmids-textbook.github.io/_images/c346aff858ca3853432ca68560966ff40a69ed45112737ae2d576de9ef3cebb7.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Two-Component Graph)</strong> Let <span class="math notranslate nohighlight">\(G=(V,E)\)</span> be a graph with two connected components <span class="math notranslate nohighlight">\(\emptyset \neq V_1, V_2 \subseteq V\)</span>. By the properties of connected components, we have <span class="math notranslate nohighlight">\(V_1 \cap V_2 = \emptyset\)</span> and <span class="math notranslate nohighlight">\(V_1 \cup V_2 = V\)</span>. Assume the Laplacian <span class="math notranslate nohighlight">\(L\)</span> of <span class="math notranslate nohighlight">\(G\)</span> has spectral decomposition <span class="math notranslate nohighlight">\(L = \sum_{i=1}^n \mu_i \mathbf{y}_i \mathbf{y}_i^T\)</span> with <span class="math notranslate nohighlight">\(0 = \mu_1 \leq \mu_2 \leq \cdots \leq \mu_n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}_1 = \frac{1}{\sqrt{n}}(1,\ldots,1)\)</span>. We claimed earlier that for such a graph <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span>. We prove this here using the <em>Variational Characterization of <span class="math notranslate nohighlight">\(\mu_2\)</span></em></p>
<div class="math notranslate nohighlight">
\[
\mu_2 
= \min\left\{
\sum_{\{u, v\} \in E} (x_u - x_v)^2 \,:\,
\mathbf{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n, \sum_{u=1}^n x_u = 0, \sum_{u = 1}^n x_u^2=1
\right\}.
\]</div>
<p>Based on this characterization, it suffices to find a vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> satisfying <span class="math notranslate nohighlight">\(\sum_{u=1}^n x_u = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_{u = 1}^n x_u^2=1\)</span> such that <span class="math notranslate nohighlight">\(\sum_{\{u, v\} \in E} (x_u - x_v)^2 = 0\)</span>. Indeed, since <span class="math notranslate nohighlight">\(\mu_2 \geq 0\)</span> and any such <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> gives an upper bound on <span class="math notranslate nohighlight">\(\mu_2\)</span>, we then necessarily have that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(\sum_{\{u, v\} \in E} (x_u - x_v)^2\)</span> to be <span class="math notranslate nohighlight">\(0\)</span>, one might be tempted to take a constant vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. But then we could not satisfy <span class="math notranslate nohighlight">\(\sum_{u=1}^n x_u = 0\)</span> and <span class="math notranslate nohighlight">\(\sum_{u = 1}^n x_u^2=1\)</span> simultaneously. Instead, we modify this guess slightly. Because the graph has two connected components, there is no edge between <span class="math notranslate nohighlight">\(V_1\)</span> and <span class="math notranslate nohighlight">\(V_2\)</span>. Hence we can assign a different value to each component and still get <span class="math notranslate nohighlight">\(\sum_{\{u, v\} \in E} (x_u - x_v)^2 = 0\)</span>. So we look for a vector <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_n)\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x_u = \begin{cases}
\alpha, &amp; \text{if $u \in V_1$,}\\
\beta, &amp; \text{if $u \in V_2$.}
\end{cases}
\end{split}\]</div>
<p>To satisfy the constraints on <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we require</p>
<div class="math notranslate nohighlight">
\[
\sum_{u=1}^n x_u
= \sum_{u \in V_1} \alpha + \sum_{u \in V_2} \beta
= |V_1| \alpha + |V_2| \beta
= 0,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\sum_{u=1}^n x_u^2
= \sum_{u \in V_1} \alpha^2 + \sum_{u \in V_2} \beta^2
= |V_1| \alpha^2 + |V_2| \beta^2
= 1.
\]</div>
<p>Replacing the first equation in the second one, we get</p>
<div class="math notranslate nohighlight">
\[
|V_1| \left(\frac{-|V_2|\beta}{|V_1|}\right)^2 + |V_2| \beta^2
= \frac{|V_2|^2 \beta^2}{|V_1|} + |V_2| \beta^2
= 1,
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\beta^2 = \frac{|V_1|}{|V_2|(|V_2| + |V_1|)} = \frac{|V_1|}{n |V_2|}. 
\]</div>
<p>Take</p>
<div class="math notranslate nohighlight">
\[
\beta
= - \sqrt{\frac{|V_1|}{n |V_2|}},
\qquad
\alpha = \frac{-|V_2|\beta}{|V_1|} = \sqrt{\frac{|V_2|}{n |V_1|}}.
\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> we constructed is in fact an eigenvector of <span class="math notranslate nohighlight">\(L\)</span>. Indeed, let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span>. Then, for <span class="math notranslate nohighlight">\(e_k = \{u,v\}\)</span>, <span class="math notranslate nohighlight">\((B^T \mathbf{x})_k\)</span> is either <span class="math notranslate nohighlight">\(x_u - x_v\)</span> or <span class="math notranslate nohighlight">\(x_v - x_u\)</span>. In both cases, that is <span class="math notranslate nohighlight">\(0\)</span>. So <span class="math notranslate nohighlight">\(L \mathbf{x} = B B^T \mathbf{x} = \mathbf{0}\)</span>, that is, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(L\)</span> with eigenvalue <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>We have shown that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span> when <span class="math notranslate nohighlight">\(G\)</span> has two connected components. A slight modification of this argument shows that <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span> whenever <span class="math notranslate nohighlight">\(G\)</span> is not connected. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following is NOT a property of the Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> of a graph <span class="math notranslate nohighlight">\(G\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(L\)</span> is symmetric.</p>
<p>b) <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite.</p>
<p>c) The constant unit vector <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}}(1,\ldots,1)\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(L\)</span> with eigenvalue 0.</p>
<p>d) <span class="math notranslate nohighlight">\(L\)</span> is positive definite.</p>
<p><strong>2</strong> Which vector is known as the Fiedler vector?</p>
<p>a) The eigenvector corresponding to the largest eigenvalue of the Laplacian matrix.</p>
<p>b) The eigenvector corresponding to the smallest eigenvalue of the Laplacian matrix.</p>
<p>c) The eigenvector corresponding to the second smallest eigenvalue of the Laplacian matrix.</p>
<p>d) The eigenvector corresponding to the average of all eigenvalues of the Laplacian matrix.</p>
<p><strong>3</strong> For a connected graph <span class="math notranslate nohighlight">\(G\)</span>, which of the following statements about the second smallest eigenvalue <span class="math notranslate nohighlight">\(\mu_2\)</span> of its Laplacian matrix is true?</p>
<p>a) <span class="math notranslate nohighlight">\(\mu_2 = 0\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mu_2 &lt; 0\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mu_2 &gt; 0\)</span></p>
<p>d) The value of <span class="math notranslate nohighlight">\(\mu_2\)</span> cannot be determined without additional information.</p>
<p><strong>4</strong> The Laplacian quadratic form <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> for a graph <span class="math notranslate nohighlight">\(G\)</span> with Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^T L \mathbf{x} = \sum_{\{i,j\} \in E} (x_i - x_j)^2.
\]</div>
<p>What does this quadratic form measure?</p>
<p>a) The average distance between vertices in the graph.</p>
<p>b) The number of connected components in the graph.</p>
<p>c) The “smoothness” of the function <span class="math notranslate nohighlight">\(x\)</span> over the graph.</p>
<p>d) The degree of each vertex in the graph.</p>
<p><strong>5</strong> The Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> of a graph <span class="math notranslate nohighlight">\(G\)</span> can be decomposed as <span class="math notranslate nohighlight">\(L = B B^T\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is an oriented incidence matrix. What does this decomposition imply about <span class="math notranslate nohighlight">\(L\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(L\)</span> is positive definite</p>
<p>b) <span class="math notranslate nohighlight">\(L\)</span> is symmetric and positive semidefinite</p>
<p>c) <span class="math notranslate nohighlight">\(L\)</span> is antisymmetric</p>
<p>d) <span class="math notranslate nohighlight">\(L\)</span> is a diagonal matrix</p>
<p>Answer for 1: d. Justification: The text states that “because <span class="math notranslate nohighlight">\(L\)</span> is positive semidefinite, the eigenvalues are nonnegative,” but it does not claim that <span class="math notranslate nohighlight">\(L\)</span> is positive definite.</p>
<p>Answer for 2: c. Justification: The text refers to the eigenvector corresponding to <span class="math notranslate nohighlight">\(\mu_2\)</span> (the second smallest eigenvalue) as the Fiedler vector.</p>
<p>Answer for 3: c. Justification: The text proves that “If <span class="math notranslate nohighlight">\(G\)</span> is connected, then the Laplacian eigenvalue <span class="math notranslate nohighlight">\(\mu_2 &gt; 0\)</span>.”</p>
<p>Answer for 4: c. Justification: The text states that “the Laplacian quadratic form measures how ‘smooth’ the function <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is over the graph in the following sense. A small value of <span class="math notranslate nohighlight">\(\mathbf{x}^T L \mathbf{x}\)</span> indicates that adjacent vertices tend to get assigned close values.”</p>
<p>Answer for 5: b. Justification: The text states, “Let <span class="math notranslate nohighlight">\(B\)</span> be an oriented incidence matrix of <span class="math notranslate nohighlight">\(G\)</span>. By construction, <span class="math notranslate nohighlight">\(L = B B^T\)</span>. This implies that <span class="math notranslate nohighlight">\(L\)</span> is symmetric and positive semidefinite.”</p>
    
</body>
</html>