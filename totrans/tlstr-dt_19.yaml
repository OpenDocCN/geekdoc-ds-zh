- en: 9  Clean, prepare, and test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://tellingstorieswithdata.com/09-clean_and_prepare.html](https://tellingstorieswithdata.com/09-clean_and_prepare.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Preparation](./09-clean_and_prepare.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[9  Clean, prepare, and test](./09-clean_and_prepare.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: Read *Data Feminism*, ([D’Ignazio and Klein 2020](99-references.html#ref-datafeminism2020))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on Chapter 5 “Unicorns, Janitors, Ninjas, Wizards, and Rock Stars”, which
    discusses the importance of considering different sources of data about the same
    process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *R for Data Science*, ([Wickham, Çetinkaya-Rundel, and Grolemund [2016]
    2023](99-references.html#ref-r4ds))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on Chapter 6 “Data tidying”, which provides an overview of tidy data and
    some strategies to obtain it.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *An introduction to data cleaning with R*, ([De Jonge and van der Loo 2013](99-references.html#ref-de2013introduction))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on Chapter 2 “From raw data to technically correct data”, which provides
    detailed information about reading data into R and various classes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *What The Washington Post Elections Engineering team had to learn about
    election data* ([Liu, Bronner, and Bowers 2022](99-references.html#ref-washingtonpostelections))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details several practical issues about real-world datasets.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *Column Names as Contracts*, ([Riederer 2020](99-references.html#ref-columnnamesascontracts))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduces the benefits of having a limited vocabulary for naming variables.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *Combining Statistical, Physical, and Historical Evidence to Improve Historical
    Sea-Surface Temperature Records*, ([Chan 2021](99-references.html#ref-Chan2021Combining))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details the difficulty of creating a dataset of temperatures from observations
    taken by different ships at different times.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key concepts and skills**'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and preparing a dataset is difficult work that involves making many
    decisions. Planning an endpoint and simulating the dataset that we would like
    to end up with are key elements of cleaning and preparing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can help to work in an iterative way, beginning with a small sample of the
    dataset. Write code to fix some aspect, and then iterate and generalize to additional
    tranches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During that process we should also develop a series of tests and checks that
    the dataset should pass. This should focus on key features that we would expect
    of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should be especially concerned about the class of variables, having clear
    names, and that the unique values of each variable are as expected given all this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software and packages**'
  prefs: []
  type: TYPE_NORMAL
- en: Base R ([R Core Team 2024](99-references.html#ref-citeR))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`janitor` ([Firke 2023](99-references.html#ref-janitor))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lubridate` ([Grolemund and Wickham 2011](99-references.html#ref-GrolemundWickham2011))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelsummary` ([Arel-Bundock 2022](99-references.html#ref-citemodelsummary))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opendatatoronto` ([Gelfand 2022](99-references.html#ref-citeSharla))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pdftools` ([Ooms 2022](99-references.html#ref-pdftools))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pointblank` ([Iannone and Vargas 2022](99-references.html#ref-pointblank))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readxl` ([Wickham and Bryan 2023](99-references.html#ref-readxl))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scales` ([Wickham and Seidel 2022](99-references.html#ref-scales))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stringi` ([Gagolewski 2022](99-references.html#ref-stringi))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`testthat` ([Wickham 2011](99-references.html#ref-testthat))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tidyverse` ([Wickham et al. 2019](99-references.html#ref-tidyverse))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tinytable` ([Arel-Bundock 2024](99-references.html#ref-tinytable))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validate` ([van der Loo and De Jonge 2021](99-references.html#ref-validate))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*## 9.1 Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: “Well, Lyndon, you may be right and they may be every bit as intelligent as
    you say,” said Rayburn, “but I’d feel a whole lot better about them if just one
    of them had run for sheriff once.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sam Rayburn reacting to Lyndon Johnson’s enthusiasm about John Kennedy’s incoming
    cabinet, as quoted in *The Best and the Brightest* ([Halberstam 1972, 41](99-references.html#ref-halberstam)).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this chapter we put in place more formal approaches for data cleaning and
    preparation. These are centered around:'
  prefs: []
  type: TYPE_NORMAL
- en: validity;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: internal consistency; and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: external consistency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your model does not care whether you validated your data, but you should. Validity
    means that the values in the dataset are not obviously wrong. For instance, with
    few exceptions, currencies should not have letters in them, names should not have
    numbers, and velocities should not be faster than the speed of light. Internal
    consistency means the dataset does not contradict itself. For instance, that might
    mean that constituent columns add to the total column. External consistency means
    that the dataset does not, in general, contradict outside sources, and is deliberate
    when it does. For instance, if our dataset purports to be about the population
    of cities, then we would expect that they are the same as, to a rough approximation,
    say, those available from relevant censuses on Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: SpaceX, the United States rocket company, uses cycles of ten or 50 Hertz (equivalent
    to 0.1 and 0.02 seconds, respectively) to control their rockets. Each cycle, the
    inputs from sensors, such as temperature and pressure, are read, processed, and
    used to make a decision, such as whether to adjust some setting ([Martin and Popper
    2021](99-references.html#ref-martinpopper)). We recommend a similar iterative
    approach of small adjustments during data cleaning and preparation. Rather than
    trying to make everything perfect from the start, just get started, and iterate
    through a process of small, continuous improvements.
  prefs: []
  type: TYPE_NORMAL
- en: To a large extent, the role of data cleaning and preparation is so great that
    the only people that understand a dataset are those that have cleaned it. Yet,
    the paradox of data cleaning is that often those that do the cleaning and preparation
    are those that have the least trust in the resulting dataset. At some point in
    every data science workflow, those doing the modeling should do some data cleaning.
    Even though few want to do it ([Sambasivan et al. 2021](99-references.html#ref-Sambasivan2021)),
    it can be as influential as modeling. To clean and prepare data is to make many
    decisions, some of which may have important effects on our results. For instance,
    Northcutt, Athalye, and Mueller ([2021](99-references.html#ref-labelsiswrongs))
    find the test sets of some popular datasets in computer science contain, on average,
    labels that are wrong in around three per cent of cases. Banes et al. ([2022](99-references.html#ref-Banes2022))
    re-visit the Sumatran orang-utan *(Pongo abelii)* reference genome and find that
    nine of the ten samples had some issue. And Du, Huddart, and Jiang ([2022](99-references.html#ref-eveninaccountingwhat))
    find a substantial difference between the as-filed and standardized versions of
    a company’s accounting data, especially for complex financial situations. Like
    Sam Rayburn wishing that Kennedy’s cabinet despite their intelligence, had experience
    in the nitty-gritty, a data scientist needs to immerse themselves in the messy
    reality of their dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The reproducibility crisis, which was identified early in psychology ([Open
    Science Collaboration 2015](99-references.html#ref-anniesfind)) but since extended
    to many other disciplines in the physical and social sciences, brought to light
    issues such as p-value “hacking”, researcher degrees of freedom, file-drawer issues,
    and even data and results fabrication ([Gelman and Loken 2013](99-references.html#ref-gelman2013garden)).
    Steps are now being put in place to address these. But there has been relatively
    little focus on the data gathering, cleaning, and preparation aspects of applied
    statistics, despite evidence that decisions made during these steps greatly affect
    statistical results ([Huntington-Klein et al. 2021](99-references.html#ref-huntington2021influence)).
    In this chapter we focus on these issues.
  prefs: []
  type: TYPE_NORMAL
- en: While the statistical practices that underpin data science are themselves correct
    and robust when applied to simulated datasets, data science is not typically conducted
    with data that follow the assumptions underlying the models that are commonly
    fit. For instance, data scientists are interested in “messy, unfiltered, and possibly
    unclean data—tainted by heteroskedasticity, complex dependence and missingness
    patterns—that until recently were avoided in polite conversations between more
    traditional statisticians” ([Craiu 2019](99-references.html#ref-craiu2019hiring)).
    Big data does not resolve this issue and may even exacerbate it. For instance,
    population inference based on larger amounts of poor-quality data, without adjusting
    for data issues, will just lead to more confidently wrong conclusions ([Meng 2018](99-references.html#ref-meng2018statistical)).
    The problems that are found in much of applied statistics research are not necessarily
    associated with researcher quality, or their biases ([Silberzahn et al. 2018](99-references.html#ref-silberzahn2018many)).
    Instead, they are a result of the context within which data science is conducted.
    This chapter provides an approach and tools to explicitly think about this work.
  prefs: []
  type: TYPE_NORMAL
- en: Gelman and Vehtari ([2021](99-references.html#ref-gelman2020most)), writing
    about the most important statistical ideas of the past 50 years, say that each
    of them enabled new ways of thinking about data analysis. These ideas brought
    into the tent of statistics, approaches that “had been considered more a matter
    of taste or philosophy”. The focus on data cleaning and preparation in this chapter
    is analogous, insofar as it represents a codification, or bringing inside the
    tent, of aspects that are typically, incorrectly, considered those of taste rather
    than core statistical concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for data cleaning and preparation that we advocate is:'
  prefs: []
  type: TYPE_NORMAL
- en: Save the original, unedited data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Begin with an end in mind by sketching and simulating.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write tests and documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the plan on a small sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate the plan.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generalize the execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update tests and documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will need a variety of skills to be effective, but this is the very stuff
    of data science. The approach needed is some combination of dogged and sensible.
    Perfect is very much the enemy of good enough when it comes to data cleaning.
    And to be specific, it is better to have 90 per cent of the data cleaned and prepared,
    and to start exploring that, before deciding whether it is worth the effort to
    clean and prepare the remaining 10 per cent. Because that remainder will likely
    take an awful lot of time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: All data regardless of whether they were obtained from farming, gathering, or
    hunting, will have issues. We need approaches that can deal with a variety of
    concerns, and more importantly, understand how they might affect our modeling
    ([Van den Broeck et al. 2005](99-references.html#ref-van2005data)). To clean data
    is to analyze data. This is because the process forces us to make choices about
    what we value in our results ([Au 2020](99-references.html#ref-thatrandyauperson)).
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 9.2.1 Save the original, unedited data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step is to save the original, unedited data into a separate, local
    folder. The original, unedited data establishes the foundation for reproducibility
    ([Wilson et al. 2017](99-references.html#ref-wilsongoodenough)). If we obtained
    our data from a third-party, such as a government website, then we have no control
    over whether they will continue to host that data, update it, or change the address
    at which it is available. Saving a local copy also reduces the burden that we
    impose on their servers.
  prefs: []
  type: TYPE_NORMAL
- en: Having locally saved the original, unedited data we must maintain a copy of
    it in that state, and not modify it. As we begin to clean and prepare it, we instead
    make these changes to a copy of the dataset. Maintaining the original, unedited
    dataset, and using scripts to create the dataset that we are interested in analyzing,
    ensures that our entire workflow is reproducible. It may be that the changes that
    we decide to make today, are not ones that we would make tomorrow, having learnt
    more about the dataset. We need to ensure that we have that data in the original,
    unedited state in case we need to return to it ([Borer et al. 2009](99-references.html#ref-Borer2009)).
  prefs: []
  type: TYPE_NORMAL
- en: We may not always be allowed to share that original, unedited data, but we can
    almost always create something similar. For instance, if we are using a restricted-use
    computer, then it may be that the best we can do is create a simulated version
    of the original, unedited data that conveys the main features, and include detailed
    access instructions in a README file.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Plan
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Planning the endpoint forces us to begin with an end in mind and is important
    for a variety of reasons. As with scraping data, introduced in [Chapter 7](07-gather.html),
    it helps us to be proactive about scope-creep. But with data cleaning it additionally
    forces us to really think about what we want the final dataset to look like.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to sketch the dataset that we are interested in. The key features
    of the sketch will be aspects such as the names of the columns, their class, and
    the possible range of values. For instance, we might be interested in the populations
    of US states. Our sketch might look like [Figure 9.1](#fig-sketchdataplan).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fbdef137b0956d4986807d8b9239c3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Planned dataset of US states and their populations'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the sketch forces us to decide that we want full names rather
    than abbreviations for the state names, and the population to be measured in millions.
    The process of sketching this endpoint has forced us to make decisions early on
    and be clear about our desired endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: We then implement that using code to simulate data. Again, this process forces
    us to think about what reasonable values look like in our dataset because we must
    decide which functions to use. We need to think carefully about the unique values
    of each variable. For instance, if the variable is meant to be “gender” then unique
    values such as “male”, “female”, “other”, and “unknown” may be expected, but a
    number such as “1,000” would likely be wrong. It also forces us to be explicit
    about names because we must assign the output of those functions to a variable.
    For instance, we could simulate some population data for the US states.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE2]*  *Our purpose, during data cleaning and preparation, is to then bring
    our original, unedited data close to that plan. Ideally, we would plan so that
    the desired endpoint of our dataset is “tidy data”. This is introduced in [Online
    Appendix A](20-r_essentials.html), but briefly, it means that ([Wickham, Çetinkaya-Rundel,
    and Grolemund [2016] 2023](99-references.html#ref-r4ds); [Wickham 2014, 4](99-references.html#ref-wickham2014tidy)):'
  prefs: []
  type: TYPE_NORMAL
- en: each variable is in its own column;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: each observation is in its own row; and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: each value is in its own cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Begin thinking about validity and internal consistency at this stage. What are
    some of the features that these data should have? Note these as you go through
    the process of simulating the dataset because we will draw on them to write tests.*  *###
    9.2.3 Start small
  prefs: []
  type: TYPE_NORMAL
- en: Having thoroughly planned we can turn to the original, unedited data that we
    are dealing with. Usually we want to manipulate the original, unedited data into
    a rectangular dataset as quickly as possible. This allows us to use familiar functions
    from the `tidyverse`. For instance, let us assume that we are starting with a
    `.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to look for regularities in the dataset. We want to end up
    with tabular data, which means that we need some type of delimiter to distinguish
    different columns. Ideally this might be features such as a comma, a semicolon,
    a tab, a double space, or a line break. In the following case we could take advantage
    of the comma.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In more challenging cases there may be some regular feature of the dataset that
    we can take advantage of. Sometimes various text is repeated, as in the following
    case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this case, although we do not have a traditional delimiter, we can use the
    regularity of “State is”, “and population is”, and “million” to get what we need.
    A more difficult case is when we do not have line breaks. This final case is illustrative
    of that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: One way to approach this is to take advantage of the different classes and values
    that we are looking for. For instance, we know that we are after US states, so
    there are only 50 possible options (setting D.C. to one side for the time being),
    and we could use the these as a delimiter. We could also use the fact that population
    is a number, and so separate based on a space followed by a number.
  prefs: []
  type: TYPE_NORMAL
- en: We will now convert this final case into tidy data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE7]*  *### 9.2.4 Write tests and documentation'
  prefs: []
  type: TYPE_NORMAL
- en: Having established a rectangular dataset, albeit a messy one, we should begin
    to look at the classes that we have. We do not necessarily want to fix the classes
    at this point, because that can result in lost data. But we look at the class
    to see what it is, compare it to our simulated dataset, and note the columns where
    it is different to see what changes need to be made. Background on `class()` is
    available in [Online Appendix A](20-r_essentials.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before changing the class and before going on to more bespoke issues, we should
    deal with some common issues including:'
  prefs: []
  type: TYPE_NORMAL
- en: Commas and other punctuation, such as denomination signs ($, €, £, etc.), in
    variables that should be numeric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inconsistent formatting of dates, such as “December” and “Dec” and “12” all
    in the one variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unexpected character encoding, especially in Unicode, which may not display
    consistently.[¹](#fn1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, we want to fix anything immediately obvious. For instance, we should
    remove commas that have been used to group digits in currencies. However, the
    situation will often feel overwhelming. What we need to do is to look at the unique
    values in each variable, and then triage what we will fix. We make the triage
    decision based on what is likely to have the largest impact. That usually means
    creating counts of the observations, sorting them in descending order, and then
    dealing with them in this order.
  prefs: []
  type: TYPE_NORMAL
- en: When the tests of membership are passed—which we initially establish based on
    simulation and experience—then we can change the class, and run all the tests
    again. We have adapted this idea from the software development approach of unit
    testing. Tests are crucial because they enable us to understand whether software
    (or in this case data) is fit for our purpose ([Irving et al. 2021](99-references.html#ref-researchsoftware)).
    Tests, especially in data science, are not static things that we just write once
    and then forget. Instead they should update and evolve as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Oh, you think we have good data on that!* *The simplification of reality can
    be especially seen in sports records, which necessarily must choose what to record.
    Sports records are fit for some purposes and not for others. For instance, chess
    is played on an 8 x 8 board of alternating black and white squares. The squares
    are denoted by a unique combination of both a letter (A-G) and a number (1-8).
    Most pieces have a unique abbreviation, for instance knights are N and bishops
    are B. Each game is independently recorded using this “algebraic notation” by
    each player. These records allow us to recreate the moves of the game. The 2021
    Chess World Championship was contested by Magnus Carlsen and Ian Nepomniachtchi.
    There were a variety of reasons this game was particularly noteworthy—including
    it being the longest world championship game—but one is the uncharacteristic mistakes
    that both Carlsen and Nepomniachtchi made. For instance, at Move 33 Carlsen did
    not exploit an opportunity; and at Move 36 a different move would have provided
    Nepomniachtchi with a promising endgame ([Doggers 2021](99-references.html#ref-PeterDoggers)).
    One reason for these mistakes may have been that both players at that point in
    the game had very little time remaining—they had to decide on their moves very
    quickly. But there is no sense of that in the representation provided by the game
    sheet because it does not record time remaining. The record is fit for purpose
    as a “correct” representation of what happened in the game; but not necessarily
    why it happened.*  *Let us run through an example with a collection of strings,
    some of which are slightly wrong. This type of output is typical of OCR, introduced
    in [Chapter 7](07-gather.html), which often gets most of the way there, but not
    quite.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*As before, we first get this into a rectangular dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE10]*  *We now need to decide which of these errors we are going to fix.
    To help us decide which are most important, we create a count.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE12]*  *The most common unique observation is the correct one. The next
    one—“PatricIa”—looks like the “i” has been incorrectly capitalized. This is true
    for “PatrIcia” as well. We can fix the capitalization issues with `str_to_title()`,
    which converts the first letter of each word in a string to uppercase and the
    rest to lowercase, and then redo the count.'
  prefs: []
  type: TYPE_NORMAL
- en: Background on strings is available in [Online Appendix A](20-r_essentials.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE14]*  *Already this is much better with 60 per cent of the values are
    correct, compared with the earlier 30 per cent. There are two more clear errors—“8tricia”
    and “Ptricia”—with the first distinguished by an “8” instead of a “P”, and the
    second missing an “a”. We can fix these issues with `str_replace_all()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE16]*  *We have achieved an 80 per cent outcome with not too much effort.
    The final two issues are more subtle. The first has occurred because the “i” has
    been incorrectly coded as a “1”. In some fonts this will show up, but in others
    it will be more difficult to see. This is a common issue, especially with OCR,
    and something to be aware of. The second occurs because of a trailing space. Trailing
    and leading spaces are another common issue and we can address them with `str_trim()`.
    After we fix these two remaining issues we have all entries corrected.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE18]*  *We have been doing the tests in our head in this example. We know
    that we are hoping for “Patricia”. But we can start to document this test as well.
    One way is to look to see if values other than “Patricia” exist in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*We can make things a little more imposing by stopping our code execution if
    the condition is not met with `stopifnot()`. To use this function we define a
    condition that we would like met. We could implement this type of check throughout
    our code. For instance if we expected there to be a certain number of observations
    in the dataset, or for a certain variable to have various properties, such as
    being an integer or a factor.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*We can use `stopifnot()` to ensure that our script is working as expected
    as it runs.'
  prefs: []
  type: TYPE_NORMAL
- en: Another way to write tests for our dataset is to use `testthat`. Although developed
    for testing packages, we can use the functionality to test our datasets. For instance,
    we can use `expect_length()` to check the length of a dataset and `expect_equal()`
    to check the content.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*If the tests pass then nothing happens, but if the tests fail then the script
    will stop.'
  prefs: []
  type: TYPE_NORMAL
- en: What do we test? It is a difficult problem, and we detail a range of more-specific
    tests in the next section. But broadly we test what we have, against what we expect.
    The engineers working on the software for the Apollo program in the 1960s initially
    considered writing tests to be “busy work” ([Mindell 2008, 170](99-references.html#ref-digitalapollo)).
    But they eventually came to realize that NASA would not have faith that software
    could be used to send men to the moon unless it was accompanied by a comprehensive
    suite of tests. And it is the same for data science.
  prefs: []
  type: TYPE_NORMAL
- en: Start with tests for validity. These will typically check the class of the variables,
    their unique values, and the number of observations. For instance, if we were
    using a recent dataset then columns that are years could be tested to ensure that
    all elements have four digits and start with a “2”. Baumgartner ([2021](99-references.html#ref-peterbaumgartnertesting))
    describes this as tests on the schema.
  prefs: []
  type: TYPE_NORMAL
- en: After that, turn to checks of internal consistency. For instance, if there are
    variables of different numeric responses, then check that the sum of those equals
    a total variable, or if it does not then this difference is explainable. Finally,
    turn to tests for external consistency. Here we want to use outside information
    to inform our tests. For instance, if we had a variable of the neonatal mortality
    rate (NMR) for Germany (this concept was introduced in [Chapter 2](02-drinking_from_a_fire_hose.html)),
    then we could look at the estimates from the World Health Organization (WHO),
    and ensure our NMR variable aligns. Experienced analysts do this all in their
    head. The issue is that it does not scale, can be inconsistent, and overloads
    on reputation. We return to this issue in [Chapter 12](12-ijalm.html) in the context
    of modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We write tests throughout our code, rather than right at the end. In particular,
    using `stopifnot()` statements on intermediate steps ensures that the dataset
    is being cleaned in a way that we expect. For instance, when merging two datasets
    we could check:'
  prefs: []
  type: TYPE_NORMAL
- en: The variable names in the datasets are unique, apart from the column/s to be
    used as the key/s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of observations of each type is being carried through appropriately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dimensions of the dataset are not being unexpectedly changed.**********  ****###
    9.2.5 Iterate, generalize, and update
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We could now iterate the plan. In this most recent case, we started with ten
    entries. There is no reason that we could not increase this to 100 or even 1,000\.
    We may need to generalize the cleaning procedures and tests. But eventually we
    would start to bring the dataset into some sort of order.******  ****## 9.3 Checking
    and testing
  prefs: []
  type: TYPE_NORMAL
- en: Robert Caro, the biographer of Lyndon Johnson introduced in [Chapter 4](04-writing_research.html),
    spent years tracking down everyone connected to the 36th President of the United
    States. Caro and his wife Ina went so far as to live in Texas Hill Country for
    three years so that they could better understand where Johnson was from. When
    Caro heard that Johnson, as a senator, would run to the Senate from where he stayed
    in D.C., he ran that route multiple times himself to try to understand why Johnson
    was running. Caro eventually understood it only when he ran the route as the sun
    was rising, just as Johnson had done; it turns out that the sun hits the Senate
    Rotunda in a particularly inspiring way ([Caro 2019, 156](99-references.html#ref-caroonworking)).
    This background work enabled him to uncover aspects that no one else knew. For
    instance, Johnson almost surely stole his first election win ([Caro 2019, 116](99-references.html#ref-caroonworking)).
    We need to understand our data to this same extent. We want to metaphorically
    turn every page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of negative space is well established in design. It refers to that
    which surrounds the subject. Sometimes negative space is used as an effect. For
    instance the logo of FedEx, an American logistics company, has negative space
    between the E and X that creates an arrow. In a similar way, we want to be cognizant
    of the data that we have, and the data that we do not have ([Hodgetts 2022](99-references.html#ref-citemyboy)).
    We are worried that the data that we do not have somehow has meaning, potentially
    even to the extent of changing our conclusions. When we are cleaning data, we
    are looking for anomalies. We are interested in values that are in the dataset
    that should not be, but also the opposite situation—values that should be in the
    dataset but are not. There are three tools that we use to identify these situations:
    graphs, counts, and tests.'
  prefs: []
  type: TYPE_NORMAL
- en: We also use these tools to ensure that we are not changing correct observations
    to incorrect. Especially when our cleaning and preparation requires many steps,
    it may be that fixes at one stage are undone later. We use graphs, counts, and
    especially tests, to prevent this. The importance of these grows exponentially
    with the size of the dataset. Small and medium datasets are more amenable to manual
    inspection and other aspects that rely on the analyst, while larger datasets especially
    require more efficient strategies ([Hand 2018](99-references.html#ref-hand2018statistical)).
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Graphs are an invaluable tool when cleaning data, because they show each observation
    in the dataset, potentially in relation to the other observations. They are useful
    for identifying when a value does not belong. For instance, if a value is expected
    to be numerical, but is still a character then it will not plot, and a warning
    will be displayed. Graphs will be especially useful for numerical data, but are
    still useful for text and categorical data. Let us pretend that we have a situation
    where we are interested in a person’s age, for some youth survey. We have the
    following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE23]'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](../Images/b05d9fcb26b3be254d854a37ed58303f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Before cleaning
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fc37b290e649f248b703dce819cb2e5.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) After cleaning
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.2: The ages in the simulated youth survey dataset identify a data
    issue'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9.2 (a)](#fig-youth-survey-1) shows an unexpected value of 150\. The
    most likely explanation is that the data were incorrectly entered, missing the
    decimal place, and should be 15.0\. We could fix that, document it, and then redo
    the graph, which would show that everything seemed more valid ([Figure 9.2 (b)](#fig-youth-survey-2)).**  **###
    9.3.2 Counts'
  prefs: []
  type: TYPE_NORMAL
- en: We want to focus on getting most of the data right, so we are interested in
    the counts of unique values. Hopefully most of the data are concentrated in the
    most common counts. But it can also be useful to invert it and see what is especially
    uncommon. The extent to which we want to deal with these depends on what we need.
    Ultimately, each time we fix one we are getting very few additional observations,
    potentially even just one. Counts are especially useful with text or categorical
    data but can be helpful with numerical data as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see an example of text data, each of which is meant to be “Australia”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE25]*  *The use of this count identifies where we should spend our time:
    changing “Australie” to “Australia” would almost double the amount of usable data.'
  prefs: []
  type: TYPE_NORMAL
- en: Turning, briefly to numeric data, Preece ([1981](99-references.html#ref-Preece1981))
    recommends plotting counts of the final digit of each observation in a variable.
    For instance, if the observations of the variable were “41.2”, “80.3”, “20.7”,
    “1.2”, “46.5”, “96.2”, “32.7”, “44.3”, “5.1”, and “49.0”. Then we note that 0,
    1 and 5 all occur once, 3 and 7 occur twice, and 2 occurs three times. We might
    expect that there should be a uniform distribution of these final digits. But
    that is surprisingly often not the case, and the ways in which it differs can
    be informative. For instance, it may be that data were rounded, or recorded by
    different collectors.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, later in this chapter we will gather, clean, and prepare some
    data from the 2019 Kenyan census. We pre-emptively use that dataset here and look
    at the count of the final digits of the ages. That is, say, from age 35 we take
    “5”, from age 74, we take “4”. [Table 9.1](#tbl-countofages) shows the expected
    age-heaping that occurs because some respondents reply to questions about age
    with a value to the closest 5 or 10\. If we had an age variable without that pattern
    then we might expect it had been constructed from a different type of question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9.1: Excess of 0 and 5 digits in counts of the final digits of single-year
    ages in Nairobi from the 2019 Kenyan census'
  prefs: []
  type: TYPE_NORMAL
- en: '| Final digit of age | Number of times |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 347,233 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 278,930 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 308,933 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 285,745 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 270,355 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 303,817 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 246,582 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 242,688 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 207,739 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 216,355 |*  *### 9.3.3 Tests'
  prefs: []
  type: TYPE_NORMAL
- en: As we said in [Chapter 3](03-workflow.html), if you write code, then you are
    a programmer, but there is a difference between someone coding for fun, and, say,
    writing the code that runs the James Webb Telescope. Following Weinberg ([1971,
    122](99-references.html#ref-weinbergpsychology)), we can distinguish between amateurs
    and professionals based the existence of subsequent users. When you first start
    out coding, you typically write code that only you will use. For instance, you
    may write some code for a class paper. After you get a grade, then in most cases,
    the code will not be run again. In contrast, a professional writes code for, and
    often with, other people.
  prefs: []
  type: TYPE_NORMAL
- en: Much academic research these days relies on code. If that research is to contribute
    to lasting knowledge, then the code that underpins it is being written for others
    and must work for others well after the researcher has moved to other projects.
    A professional places appropriate care on tasks that ensure code can be considered
    by others. A large part of that is tests.
  prefs: []
  type: TYPE_NORMAL
- en: Jet Propulsion Laboratory ([2009, 14](99-references.html#ref-jplcodingstandards))
    claim that analysis after the fact “often find at least one defect per one hundred
    lines of code written”. There is no reason to believe that code without tests
    is free of defects, just that they are not known. As such, we should strive to
    include tests in our code when possible. There is some infrastructure for testing
    data science code. For instance, in Python there is the Test-Driven Data Analysis
    library of Radcliffe ([2023](99-references.html#ref-tdda)), but more is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things are so important that we require that the cleaned dataset have
    them. These are conditions that we should check. They would typically come from
    experience, expert knowledge, or the planning and simulation stages. For instance,
    there should be no negative numbers in an age variable, and few ages above 110\.
    For these we could specifically require that the condition is met. Another example
    is when doing cross-country analysis, a list of country names that we know should
    be in our dataset would be useful. Our test would then be that there were:'
  prefs: []
  type: TYPE_NORMAL
- en: values not in that list that were in our dataset, or vice versa; and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: countries that we expected to be in our dataset that were not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To have a concrete example, let us consider if we were doing some analysis
    about the five largest counties in Kenya. From looking it up, we find these are:
    “Nairobi”, “Kiambu”, “Nakuru”, “Kakamega”, and “Bungoma”. We can create that variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Then pretend we have the following dataset, which contains errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE28]*  *Based on the count we know that we must fix some of them. There
    are two with numbers in the names.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE30]*  *At this point we can compare this with our known correct variable.
    We check both ways, i.e. is there anything in the correct variable not in our
    dataset, and is there anything in the dataset not in our correct variable. We
    use our check conditions to decide whether we are finished.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE32]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE34]**  **It is clear that we still have cleaning to do because not all
    the counties match what we were expecting.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3.1 Aspects to test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will talk about explicit tests for class and dates, given their outsized
    importance, and how common it is for them to go wrong. But other aspects to explicitly
    consider testing include:'
  prefs: []
  type: TYPE_NORMAL
- en: Variables of monetary values should be tested for reasonable bounds given the
    situation. In some cases negative values will not be possible. Sometimes an upper
    bound can be identified. Monetary variables should be numeric. They should not
    have commas or other separators. They should not contain symbols such as currency
    signs or semicolons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables of population values should likely not be negative. Populations of
    cities should likely be somewhere between 100,000 and 50,000,000\. They again
    should be numeric, and contain only numbers, no symbols.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Names should be character variables. They likely do not contain numbers. They
    may contain some limited set of symbols, and this would be context specific.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of observations is surprisingly easy to inadvertently change. While
    it is fine for this to happen deliberately, when it happens accidentally it can
    create substantial problems. The number of observations should be tested at the
    start of any data cleaning process against the data simulation and this expectation
    updated as necessary. It should be tested throughout the data cleaning process,
    but especially before and after any joins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More generally, work with experts and draw on prior knowledge to work out some
    reasonable features for the variables of interest and then implement these. For
    instance, consider how Baker ([2023](99-references.html#ref-scamswillnotsaveus))
    was able to quickly identify an error in a claim about user numbers by roughly
    comparing it with how many institutions in the US receive federal financial aid.
  prefs: []
  type: TYPE_NORMAL
- en: We can use `validate` to set up a series of tests. For instance, here we will
    simulate some data with clear issues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE36]*  *In this case, there is an impossible age, one observation in the
    gender variable that should not be there, and finally, income is a character variable
    instead of a numeric. We use `validator()` to establish rules we expect the data
    to satisfy and `confront()` to determine whether it does.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE38]*  *In this case, we can see that there are issues with the final three
    rules that we established. More generally, van der Loo ([2022](99-references.html#ref-datavalidationbook))
    provides many example tests that can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 6](06-farm.html), gender is something that we need
    to be especially careful about. We will typically have a small number of responses
    that are neither “male” or “female”. The correct way to deal with the situation
    depends on context. But if responses other than “male” or “female” are going to
    be removed from the dataset and ignored, because there are too few of them, showing
    respect for the respondent might mean including a brief discussion of how they
    were similar or different to the rest of the dataset. Plots and a more extensive
    discussion could then be included in an appendix.**  **#### 9.3.3.2 Class
  prefs: []
  type: TYPE_NORMAL
- en: 'It is sometimes said that Americans are obsessed with money, while the English
    are obsessed with class. In the case of data cleaning and preparation we need
    to be English. Class is critical and worthy of special attention. We introduce
    class in [Online Appendix A](20-r_essentials.html) and here we focus on “numeric”,
    “character”, and “factor”. Explicit checks of the class of variables are essential.
    Accidentally assigning the wrong class to a variable can have a large effect on
    subsequent analysis. It is important to:'
  prefs: []
  type: TYPE_NORMAL
- en: check whether some value should be a number or a factor; and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check that values are numbers not characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand why it is important to be clear about whether a value is a number
    or a factor, consider the following situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '*We use logistic regression, which we cover in more detail in [Chapter 12](12-ijalm.html),
    and first include “group” as an integer, then we include it as a factor. [Table 9.2](#tbl-effect-of-class)
    shows how different the results are and highlights the importance of getting the
    class of variables used in regression right. In the former, where group is an
    integer, we impose a consistent relationship between the different levels of the
    observations, whereas in the latter, where it is a factor, we enable more freedom.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 9.2: Examining the effect of class on regression results'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Group as integer | Group as factor |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (Intercept) | 1.417 | 1.099 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (1.755) | (1.155) |'
  prefs: []
  type: TYPE_TB
- en: '| group_as_integer | -0.666 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | (0.894) |  |'
  prefs: []
  type: TYPE_TB
- en: '| group_as_factor2 |  | -1.792 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | (1.683) |'
  prefs: []
  type: TYPE_TB
- en: '| group_as_factor3 |  | -1.099 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | (1.826) |'
  prefs: []
  type: TYPE_TB
- en: '| Num.Obs. | 9 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| AIC | 15.8 | 17.1 |'
  prefs: []
  type: TYPE_TB
- en: '| BIC | 16.2 | 17.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Log.Lik. | -5.891 | -5.545 |'
  prefs: []
  type: TYPE_TB
- en: '| F | 0.554 | 0.579 |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | 0.48 | 0.46 |*  *Class is so important, subtle, and can have such
    a pernicious effect on analysis, that analysis with a suite of tests that check
    class is easier to believe. Establishing this suite is especially valuable just
    before modeling, but it is worthwhile setting this up as part of data cleaning
    and preparation. One reason that Jane Street, the US proprietary trading firm,
    uses a particular programming language, OCaml, is that its type system makes it
    more reliable with regard to class ([Somers 2015](99-references.html#ref-somers2015)).
    When code matters, class is of vital concern.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many open questions around the effect and implications of type in
    computer science more generally but there has been some work. For instance, Gao,
    Bird, and Barr ([2017](99-references.html#ref-Gao2017)) find that the use of a
    static type system would have caught around 15 per cent of errors in production
    JavaScript systems. Languages have been developed, such as Typescript, where the
    primary difference, in this case from JavaScript, is that they are strongly typed.
    Turcotte et al. ([2020](99-references.html#ref-Turcotte2020)) examine some of
    the considerations for adding a type system in R. They develop a prototype that
    goes some way to addressing the technical issues, but acknowledge that large-scale
    implementation would be challenging for many reasons including the need for users
    to change.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this point in this book when we have used `read_csv()`, and other functions
    for importing data, we have allowed the function to guess the class of the variables.
    Moving forward we will be more deliberate and instead specify it ourselves using
    “col_types”. For instance, instead of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '*We recommend using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '*This is typically an iterative process of initially reading in the dataset,
    getting a quick sense of it, and then reading it in properly with only the necessary
    columns and classes specified. While this will require a little extra work of
    us, it is important that we are clear about class.****  ***#### 9.3.3.3 Dates'
  prefs: []
  type: TYPE_NORMAL
- en: A shibboleth for whether someone has worked with dates is their reaction when
    you tell them you are going to be working with dates. If they share a horror story,
    then they have likely worked with dates before!
  prefs: []
  type: TYPE_NORMAL
- en: 'Extensive checking of dates is important. Ideally, we would like dates to be
    in the following format: YYYY-MM-DD. There are differences of opinion as to what
    is an appropriate date format in the broader world. Reasonable people can differ
    on whether 1 July 2022 or July 1, 2022 is better, but YYYY-MM-DD is the international
    standard and we should use that in our date variables where possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A few tests that could be useful include:'
  prefs: []
  type: TYPE_NORMAL
- en: If a column is days of the week, then test that the only components are Monday,
    Tuesday, \(\dots\), Sunday. Further, test that all seven days are present. Similarly,
    for month.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test that the number of days is appropriate for each month, for instance, check
    that September has 30 days, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check whether the dates are in order in the dataset. This need not necessarily
    be the case, but often when it is not, there are issues worth exploring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check that the years are complete and appropriate to the analysis period.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 2](02-drinking_from_a_fire_hose.html) we introduced a dataset of
    shelter usage in Toronto in 2021 using `opendatatoronto`. Here we examine that
    same dataset, but for 2017, to illustrate some issues with dates. We first need
    to download the data.[²](#fn2)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '*We need to make the names easier to type and only keep relevant columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '*The main issue with this dataset will be the dates. We will find that the
    dates appear to be mostly year-month-day, but certain observations may be year-day-month.
    We use `ymd()` from `lubridate` to parse the date in that order.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE46]*  *The plot of the distribution of what purports to be the day component
    makes it clear that there are concerns ([Figure 9.3 (a)](#fig-homeless-daycount-1)).
    In particular we are concerned that the distribution of the days is not roughly
    uniform.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '*![](../Images/0da30e0c5ecff04de4a2d2e4bf3dc316.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Counts, by third component of occupancy date
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/721b0ada2b49576360d99a4f25d8ab3e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Comparison of row number with date
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.3: Examining the date in more detail'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, one graph that is especially useful when cleaning a dataset is
    the order the observations appear in the dataset. For instance, we would generally
    expect that there would be a rough ordering in terms of date. To examine whether
    this is the case, we can graph the date variable in the order it appears in the
    dataset ([Figure 9.3 (b)](#fig-homeless-daycount-2)).
  prefs: []
  type: TYPE_NORMAL
- en: While this is just a quick graph it illustrates the point—there are a lot in
    order, but not all. If they were in order, then we would expect them to be along
    the diagonal. It is odd that the data are not in order, especially as there appears
    to be something systematic initially. We can summarize the data to get a count
    of occupancy by day.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '*We are interested in the availability of shelter spots in Toronto for each
    day ([Figure 9.4](#fig-plotoccupancy)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '*![](../Images/63b0b58c04175b2869079ab7cd1f1b91.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.4: Occupancy per day in Toronto shelters*  *It is clear there seems
    to be an issue with the first 12 days of the month. We noted that when we look
    at the data it is a bit odd that it is not in order. From [Figure 9.3 (b)](#fig-homeless-daycount-2)
    it looks like there are some systematic issue that affects many observations.
    In general, it seems that it might be the case that in the date variable the first
    12 days are the wrong way around, i.e. we think it is year-month-day, but it is
    actually year-day-month. But there are exceptions. As a first pass, we can flip
    those first 12 days of each month and see if that helps. It will be fairly blunt,
    but hopefully gets us somewhere.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '*Now let us take a look ([Figure 9.5](#fig-sheltersdatebyrowadj)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '*![](../Images/00b8638269285159ed25f38ff6201f48.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Date of each row in order after adjustment
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86c40bcb780df61ad732c28ddaa432e2.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Toronto shelters daily occupancy after adjustment
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.5: Adjusted dates, occupancy in Toronto shelters'
  prefs: []
  type: TYPE_NORMAL
- en: 'It has not fixed all the issues. For instance, notice there are now no entries
    below the diagonal ([Figure 9.5 (a)](#fig-sheltersdatebyrowadj-1)). But we can
    see that has almost entirely taken care of the systematic differences ([Figure 9.5
    (b)](#fig-sheltersdatebyrowadj-2)). This is where we will leave this example.*********************  ***##
    9.4 Simulated example: running times'
  prefs: []
  type: TYPE_NORMAL
- en: To provide a specific example, which we will return to in [Chapter 12](12-ijalm.html),
    consider the time it takes someone to run five kilometers (which is a little over
    three miles), compared with the time it takes them to run a marathon ([Figure 12.2
    (a)](12-ijalm.html#fig-fivekmvsmarathon-1)).
  prefs: []
  type: TYPE_NORMAL
- en: Here we consider “simulate” and “acquire”, focused on testing. In the simulation
    we specify a relationship of 8.4, as that is roughly the ratio between a five-kilometer
    run and the 42.2 kilometer distance of a marathon (a little over 26 miles).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE53]*  *We can use our simulation to put in place various tests that we
    would want the actual data to satisfy. For instance, we want the class of the
    five kilometer and marathon run times to be numeric. And we want 200 observations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '*We know that any value that is less than 15 minutes or more than 30 minutes
    for the five-kilometer run time is likely something that needs to be followed
    up on.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '*Based on this maximum and the simulated relationship of 8.4, we would be surprised
    if we found any marathon times that were substantially over \(30\times8.4=252\)
    minutes, after we allow for a little bit of drift, say 300 minutes. (To be clear,
    there is nothing wrong with taking longer than this to run a marathon, but it
    is just unlikely based on our simulation parameters). And we would be surprised
    if the world record marathon time, 121 minutes as at the start of 2023, were improved
    by anything more than a minute or two, say, anything faster than 118 minutes.
    (It will turn out that our simulated data do not satisfy this and result in a
    implausibly fast 88 minute marathon time, which suggests a need to improve the
    simulation.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '*We can then take these tests to real data. Actual survey data on the relationship
    between five kilometer and marathon run times are available from Vickers and Vertosick
    ([2016](99-references.html#ref-Vickers2016)). After downloading the data, which
    Vickers and Vertosick ([2016](99-references.html#ref-Vickers2016)) make available
    as an “Additional file”, we can focus on the variables of interest and only individuals
    with both a five-kilometer time and a marathon time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE58]'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that we notice is that our data are in seconds, whereas we were
    expecting them to be in minutes. This is fine. Our simulation and tests can update,
    or we can adjust our data. Our simulation and tests retain their value even when
    the data turn out to be slightly different, which they inevitably will.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we will divide by sixty, and round, to shift our data into minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE60]*  *[PRE61]'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this case, our tests, which were written for the simulated data, identify
    that we have five kilometer run times that are faster that 15 minutes and longer
    than 30 minutes. They also identify marathon times that are longer than 300 minutes.
    If we were actually using this data for analysis, then our next step would be
    to plot the data, taking care to examine each of these points that our tests identified,
    and then either adjust the tests or the dataset.*******  ***## 9.5 Names'
  prefs: []
  type: TYPE_NORMAL
- en: An improved scanning software we developed identified gene name errors in 30.9%
    (3,436/11,117) of articles with supplementary Excel gene lists; a figure significantly
    higher than previously estimated. This is due to gene names being converted not
    just to dates and floating-point numbers, but also to internal date format (five-digit
    numbers).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abeysooriya et al. ([2021](99-references.html#ref-omggenes))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Names matter. The land on which much of this book was written is today named
    Canada, but for a long time was known as Turtle Island. Similarly, there is a
    big rock in the center of Australia. For a long time, it was called Uluru, then
    it was known as Ayers Rock. Today it has a dual name that combines both. And in
    parts of the US South, including signage surrounding the South Carolina State
    House, the US Civil War is referred to as the War of Northern Aggression. In these
    examples, the name that is used conveys information, not only about the user,
    but about the circumstances. Even the British Royal Family recognizes the power
    of names. In 1917 they changed from the House of Saxe-Coburg and Gotha to the
    House of Windsor. It was felt that the former was too Germanic given World War
    I. Names matter in everyday life. And they matter in our code, too.
  prefs: []
  type: TYPE_NORMAL
- en: 'When coding, names are critical and worthy of special attention because ([Hermans
    2021](99-references.html#ref-hermans2021programmer)):'
  prefs: []
  type: TYPE_NORMAL
- en: they help document our code as they are contained in the code;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: they make up a large proportion of any script;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: they are referred to a lot by others; and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: they help the reader understand what is happening in the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In addition to respecting the nature of the data, names need to satisfy two
    additional considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: they need to be machine readable, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: they need to be human readable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 9.5.1 Machine-readable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensuring machine-readable names can be an easier standard to meet. It usually
    means avoiding spaces and special characters. A space can be replaced with an
    underscore. For instance, we prefer “my_data” to “my data”. Avoiding spaces enables
    tab-completion which makes us more efficient. It also helps with reproducibility
    because spaces are considered differently by different operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, special characters should be removed because they can be inconsistent
    between different computers and languages. This is especially the case with slash,
    backslash, asterisk, and both single, and double quotation marks. Try to avoid
    using those in names.
  prefs: []
  type: TYPE_NORMAL
- en: Names should also be unique within a dataset, and unique within a collection
    of datasets unless that particular variable is being deliberately used as a key
    to join different datasets. This usually means that the domain is critical for
    effective names, and when working as part of a team this all gets much more difficult
    ([Hermans 2017](99-references.html#ref-hermans2017peter)). Names need to not only
    be unique, but notably different when there is a potential for confusion. For
    instance, for many years, the language PHP had both `mysql_escape_string` and
    `mysql_real_escape_string` ([Somers 2015](99-references.html#ref-somers2015)).
    It is easy to see how programmers may have accidentally written one when they
    meant the other.
  prefs: []
  type: TYPE_NORMAL
- en: An especially useful function to use to get closer to machine-readable names
    is `clean_names()` from `janitor`. This deals with those issues mentioned above
    as well as a few others.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE63]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE65]**  **### 9.5.2 Human-readable'
  prefs: []
  type: TYPE_NORMAL
- en: Programs must be written for people to read, and only incidentally for machines
    to execute
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abelson and Sussman ([1996](99-references.html#ref-abelson1996structure))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the same way that we emphasized in [Chapter 4](04-writing_research.html)
    that we write papers for the reader, here we emphasize that we write code for
    the reader. Human-readable names require an additional layer, and extensive consideration.
    Following Lockheed Martin ([2005, 25](99-references.html#ref-jsfcodingstandards)),
    we should avoid names that only differ by the use of the letter “O”, instead of
    the number “0” or the letter “D”. Similarly, “S” with “5”.
  prefs: []
  type: TYPE_NORMAL
- en: We should consider other cultures and how they may interpret some of the names
    that we use. We also need to consider different experience levels that subsequent
    users of the dataset may have. This is both in terms of experience with data science,
    but also experience with similar datasets. For instance, a variable called “flag”
    is often used to signal that a variable contains data that needs to be followed
    up with or treated carefully in some way. An experienced analyst will know this,
    but a beginner will not. Try to use meaningful names wherever possible ([Lin,
    Ali, and Wilson 2021](99-references.html#ref-lin2020ten)). It has been found that
    shorter names may take longer to comprehend ([Hofmeister, Siegmund, and Holt 2017](99-references.html#ref-shorternamestakelonger)),
    and so it is often useful to avoid uncommon abbreviations where possible.
  prefs: []
  type: TYPE_NORMAL
- en: Bryan ([2015](99-references.html#ref-jennybryanonnames)) recommends that file
    names, in particular, should consider the default ordering that a file manager
    will impose. This might mean adding prefixes such as “00-”, “01-”, etc to filenames
    (which might involve left-padding with zeros depending on the number of files).
    Critically it means using ISO 8601 for dates. That was introduced earlier and
    means that 2 December 2022 would be written “2022-12-02”. The reason for using
    such file names is to provide information to other people about the order of the
    files.
  prefs: []
  type: TYPE_NORMAL
- en: 'One interesting feature of R is that in certain cases partial matching on names
    is possible. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE67]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE69]**  **This behavior is not possible within the `tidyverse` (for instance,
    if `data.frame` were replaced with `tibble` in the above code). Partial matching
    should rarely be used. It makes it more difficult to understand code after a break,
    and for others to come to it fresh.'
  prefs: []
  type: TYPE_NORMAL
- en: Variable names should have a consistent structure. For instance, imposing the
    naming pattern `verb_noun`, as in `read_csv()`, then having one function that
    was `noun_verb`, perhaps `csv_read()`, would be inconsistent. That inconsistency
    imposes a significant cost because it makes it more difficult to remember the
    name of the function.
  prefs: []
  type: TYPE_NORMAL
- en: R, Python, and many of the other languages that are commonly used for data science
    are dynamically typed, as opposed to static typed. This means that class can be
    defined independently of declaring a variable. One interesting area of data science
    research is going partially toward static typed and understanding what that might
    mean. For instance, Python [enabled](https://peps.python.org/pep-0484/) type hints
    in 2014 ([Boykis 2019](99-references.html#ref-boykistypehints)). While not required,
    this goes someway to being more explicit about types.
  prefs: []
  type: TYPE_NORMAL
- en: Riederer ([2020](99-references.html#ref-columnnamesascontracts)) advises using
    variable names as contracts. We do this by establishing a controlled vocabulary
    for them. In this way, we would define a set of words that we can use in names.
    In the controlled vocabulary of Riederer ([2020](99-references.html#ref-columnnamesascontracts))
    a variable could start with an abbreviation for its class, then something specific
    to what it pertains to, and then various details.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we could consider column names of “age” and “sex”. Following Riederer
    ([2020](99-references.html#ref-columnnamesascontracts)) we may change these to
    be more informative of the class and other information. This issue is not settled,
    and there is not yet best practice. For instance, there are arguments against
    this in terms of readability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE71]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE73]**  **Even just trying to be a little more explicit and consistent
    about names throughout a project typically brings substantial benefits when we
    come to revisit the project later. Would a rose by any other name smell as sweet?
    Of course. But we call it a rose—or even better *Rosa rubiginosa*—because that
    helps others know what we are talking about, compared with, say, “red_thing”,
    “five_petaled_smell_nice”, “flower”, or “r_1”. It is clearer, and helps others
    efficiently understand.******  ***## 9.6 1996 Tanzanian DHS'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now go through the first of two examples. The Demographic and Health
    Surveys (DHS) play an important role in gathering data in areas where we may not
    have other datasets. Here we will clean and prepare a DHS table about household
    populations in Tanzania in 1996\. As a reminder, the workflow that we advocate
    in this book is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mbox{Plan}\rightarrow\mbox{Simulate}\rightarrow\mbox{Acquire}\rightarrow\mbox{Explore}\rightarrow\mbox{Share}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We are interested in the distribution of age-groups, gender, and urban/rural.
    A quick sketch might look like [Figure 9.6](#fig-tanzaniasketch).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1eb32209a83d034c9065a9f1174d9748.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Quick sketch of a dataset that we might be interested in'
  prefs: []
  type: TYPE_NORMAL
- en: We can then simulate a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE75]*  *Based on this simulation we are interested to test:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether there are only numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whether the sum of urban and rural match the total column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whether the sum of the age-groups match the total.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We begin by downloading the data.[³](#fn3)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '*When we have a PDF and want to read the content into R, then `pdf_text()`
    from `pdftools` is useful, as introduced in [Chapter 7](07-gather.html). It works
    well for many recently produced PDFs because the content is text which it can
    extract. But if the PDF is an image, then `pdf_text()` will not work. Instead,
    the PDF will first need to go through OCR, which was also introduced in [Chapter
    7](07-gather.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '*In this case we are interested in Table 2.1, which is on the 33rd page of
    the PDF ([Figure 9.7](#fig-tanzanian-dhs)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2affbc72f28af9c9b7ec9edd5176807b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: The page of interest in the 1996 Tanzanian DHS'
  prefs: []
  type: TYPE_NORMAL
- en: We use `stri_split_lines()` from `stringi` to focus on that particular page.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '*We first want to remove all the written content and focus on the table. We
    then want to convert that into a tibble so that we can use our familiar `tidyverse`
    approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE80]*  *All the columns have been collapsed into one, so we need to separate
    them. We will do this based on the existence of a space, which means we first
    need to change “Age group” to “Age-group” because we do not want that separated.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE82]*  *Now we need to clean the rows and columns. One helpful “negative
    space” approach to work out what we need to remove, is to look at what is left
    if we temporarily remove everything that we know we want. Whatever is left is
    then a candidate for being removed. In this case we know that we want the columns
    to contain numbers, so we remove numeric digits from all columns to see what might
    stand in our way of converting from string to numeric.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE84]*  *In this case we can see that some commas and semicolons have been
    incorrectly considered decimal places. Also, some tildes and blank lines need
    to be removed. After that we can impose the correct class.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE86]*  *Finally, we may wish to check that the sum of the constituent parts
    equals the total.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE88]*  *In this case we can see that it is a few tenths of a percentage
    point off.*********  ***## 9.7 2019 Kenyan census'
  prefs: []
  type: TYPE_NORMAL
- en: As a final example, let us consider a more extensive situation and gather, clean,
    and prepare some data from the 2019 Kenyan census. We will focus on creating a
    dataset of single-year counts, by gender, for Nairobi.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of population by age, sex, and administrative unit from the
    2019 Kenyan census can be downloaded [here](https://www.knbs.or.ke/?wpdmpro=2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units).
    While this format as a PDF makes it easy to look up a particular result, it is
    not overly useful if we want to model the data. In order to be able to do that,
    we need to convert this PDF into a tidy dataset that can be analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: 9.7.1 Gather and clean
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first need to download and read in the PDF of the 2019 Kenyan census.[⁴](#fn4)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '*We can use `pdf_text()` from `pdftools` again here.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '*In this example we will focus on the page of the PDF about Nairobi ([Figure 9.8](#fig-examplekenyancensuspage)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77252391ad58947295e8f5fd9704ab90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Page from the 2019 Kenyan census about Nairobi'
  prefs: []
  type: TYPE_NORMAL
- en: 9.7.1.1 Make rectangular
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first challenge is to get the dataset into a format that we can more easily
    manipulate. We will extract the relevant parts of the page. In this case, data
    about Nairobi is on page 410.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '*At this point the data are in a tibble. This allows us to use our familiar
    `dplyr` verbs. In particular we want to separate the columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '*They are side by side at the moment. We need to instead append to the bottom.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE94]'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE95]*  *Having got it into a rectangular format, we now need to clean the
    dataset to make it useful.****  ***#### 9.7.1.2 Validity'
  prefs: []
  type: TYPE_NORMAL
- en: To attain validity requires a number of steps. The first step is to make the
    numbers into actual numbers, rather than characters. Before we can convert the
    type, we need to remove anything that is not a number otherwise that cell will
    be converted into an NA. We first identify any values that are not numbers so
    that we can remove them, and `distinct()` is especially useful.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE97]*  *We need to remove commas. While we could use `janitor` here, it
    is worthwhile to at least first look at what is going on because sometimes there
    is odd stuff that `janitor` (and other packages) will not deal with in a way that
    we want. Nonetheless, having identified everything that needs to be removed, we
    can do the actual removal and convert our character column of numbers to integers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE99]**  **#### 9.7.1.3 Internal consistency'
  prefs: []
  type: TYPE_NORMAL
- en: 'The census has done some of the work of putting together age-groups for us,
    but we want to make it easy to just focus on the counts by single-year age. As
    such we will add a flag as to the type of age it is: an age-group, such as “ages
    0 to 5”, or a single age, such as “1”.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '*At the moment, age is a character variable. We have a decision to make here.
    We do not want it to be a character variable (because it will not graph properly),
    but we do not want it to be numeric, because there is `total` and `100+` in there.
    For now, we will just make it into a factor, and at least that will be able to
    be nicely graphed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]********  ***### 9.7.2 Check and test'
  prefs: []
  type: TYPE_NORMAL
- en: Having gathered and cleaned the data, we would like to run a few checks. Given
    the format of the data, we can check that “total” is the sum of “male” and “female”,
    which are the only two gender categories available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE103]*  *Finally, we want to check that the single-age counts sum to the
    age-groups.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE105]**  **### 9.7.3 Tidy-up'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are reasonably confident that everything is looking good, we can
    convert it to tidy format. This will make it easier to work with.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '*The original purpose of cleaning this dataset was to make a table that is
    used by Alexander and Alkema ([2022](99-references.html#ref-alexander2021bayesian)).
    We will return to this dataset, but just to bring this all together, we may like
    to make a graph of single-year counts, by gender, for Nairobi ([Figure 9.9](#fig-monicasnairobigraph)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '*![](../Images/e7b328c14ad08fbc5f8d85b215c51993.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.9: Distribution of age and gender in Nairobi in 2019, based on Kenyan
    census*  *A variety of features are clear from [Figure 9.9](#fig-monicasnairobigraph),
    including age-heaping, a slight difference in the ratio of male-female birth,
    and a substantial difference between ages 15 and 25.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we may wish to use more informative names. For instance, in the Kenyan
    data example earlier we have the following column names: “area”, “age”, “gender”,
    and “number”. If we were to use our column names as contracts, then these could
    be: “chr_area”, “fctr_group_age”, “chr_group_gender”, and “int_group_count”.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '*We can then use `pointblank` to set-up tests for us.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '*| Pointblank Validation |'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| [2024-11-21&#124;09:51:24]tibble column_names_as_contracts |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | STEP | COLUMNS | VALUES | TBL | EVAL | UNITS | PASS | FAIL | W | S
    | N | EXT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '|   | 1 | <svg width="30px" height="30px" viewBox="0 0 67 67" version="1.1"
    xlink="http://www.w3.org/1999/xlink"><title>col_is_character</title></svg>` col_is_character()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`▮``chr_group_gender`'
  prefs: []
  type: TYPE_NORMAL
- en: '| — |  | ✓ | `1` | `1` `1` | `0` `0` | — | — | — | — |'
  prefs: []
  type: TYPE_TB
- en: '|   | 2 | <svg width="30px" height="30px" viewBox="0 0 67 67" version="1.1"
    xlink="http://www.w3.org/1999/xlink"><title>col_is_factor</title></svg>` col_is_factor()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`▮``fctr_group_age`'
  prefs: []
  type: TYPE_NORMAL
- en: '| — |  | ✓ | `1` | `1` `1` | `0` `0` | — | — | — | — |'
  prefs: []
  type: TYPE_TB
- en: '|   | 3 | <svg width="30px" height="30px" viewBox="0 0 67 67" version="1.1"
    xlink="http://www.w3.org/1999/xlink"><title>col_is_integer</title></svg>` col_is_integer()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`▮``int_group_count`'
  prefs: []
  type: TYPE_NORMAL
- en: '| — |  | ✓ | `1` | `1` `1` | `0` `0` | — | — | — | — |'
  prefs: []
  type: TYPE_TB
- en: '|   | 4 | <svg width="30px" height="30px" viewBox="0 0 67 67" version="1.1"
    xlink="http://www.w3.org/1999/xlink"><title>col_vals_in_set</title></svg>` col_vals_in_set()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`▮``chr_group_gender`'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`male, female, total`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ✓ | `306` | `306` `1` | `0` `0` | — | — | — | — |'
  prefs: []
  type: TYPE_TB
- en: '| 2024-11-21 09:51:24 EST < 1 s 2024-11-21 09:51:24 EST |*********  ***## 9.8
    Exercises'
  prefs: []
  type: TYPE_NORMAL
- en: Practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(Plan)* Consider the following scenario: *You manage a shop with two employees
    and are interested in modeling their efficiency. The shop opens at 9am and closes
    at 5pm. The efficiency of the employees is mildly correlated and defined by the
    number of customers that they serve each hour. Be clear about whether you assume
    a negative or positive correlation.* Please sketch what that dataset could look
    like and then sketch a graph that you could build to show all observations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Simulate)* Please further consider the scenario described and simulate the
    situation. Please include five tests based on the simulated data. Submit a link
    to a GitHub Gist that contains your code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Acquire)* Please describe a possible source of such a dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Explore)* Please use `ggplot2` to build the graph that you sketched using
    the simulated data from step 1\. Submit a link to a GitHub Gist that contains
    your code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Communicate)* Please write two paragraphs about what you did.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we had a character variable “some_words” with one observation `"You know
    what"` within a dataset called `sayings`, then which of the following would split
    it into its constituent words (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`separate(data = sayings, col = some_words, into = c("one", "two", "three"),
    sep = " ")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`split(data = sayings, col = some_words, into = c("one", "two", "three"), sep
    = " ")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`divide(data = sayings, col = some_words, into = c("one", "two", "three"),
    sep = " ")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`part(data = sayings, col = some_words, into = c("one", "two", "three"), sep
    = " ")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`unattach(data = sayings, col = some_words, into = c("one", "two", "three"),
    sep = " ")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the following an example of tidy data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE111]'
  prefs: []
  type: TYPE_NORMAL
- en: Which function would change “lemons” into “lemonade”?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`str_replace(string = "lemons", pattern = "lemons", replacement = "lemonade")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`chr_replace(string = "lemons", pattern = "lemons", replacement = "lemonade")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`str_change(string = "lemons", pattern = "lemons", replacement = "lemonade")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`chr_change(string = "lemons", pattern = "lemons", replacement = "lemonade")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When dealing with ages, what are some desirable classes for the variable (select
    all that apply)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: integer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: matrix
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: numeric
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Please consider the following cities in Germany: “Berlin”, “Hamburg”, “Munich”,
    “Cologne”, “Frankfurt”, and “Rostock”. Use `testthat` to define three tests that
    could apply if we had a dataset with a variable “german_cities” that claimed to
    contain these, and only these, cities. Submit a link to a GitHub Gist.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which is the most acceptable format for dates in data science?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: YYYY-DD-MM
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: YYYY-MM-DD
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: DD-MM-YYYY
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: MM-MM-YYYY
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following likely does not belong? `c(15.9, 14.9, 16.6, 15.8, 16.7,
    17.9, I2.6, 11.5, 16.2, 19.5, 15.0)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With regard to “AV Rule 48” from Lockheed Martin ([2005, 25](99-references.html#ref-jsfcodingstandards))
    which of the following are not allowed to differ identifiers (select all that
    apply)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only a mixture of case
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The presence/absence of the underscore character
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The interchange of the letter “O” with the number “0” or the letter “D”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The interchange of the letter “I” with the number “1” or the letter “l”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: With regard to Preece ([1981](99-references.html#ref-Preece1981)) please discuss
    two ways in which final digits can be informative. Write at least a paragraph
    about each and include examples.*  *### Class activities
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pick a topic that you know well, then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibly simulate an idealized dataset in an R script.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Write five tests for it in a different R script.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Give the code to create the dataset (not the tests) to someone else, via a
    GitHub Gist. Have them use code to deliberately create three different data issues
    in the dataset and then send it back to you (some ideas include: inconsistent
    date formatting, adding missing values, adding negative values, changing the decimal
    place)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Do your tests identify the issues?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discuss, with the help of simulation, the following claim: “This house believes
    that strings are better than factors”.[⁵](#fn5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I obtain some data about income. Why would I be concerned if there were many
    respondents with income “999999”?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a new R project and use an R script to download and load the original
    “Adélie penguin data” ([Palmer Station Antarctica LTER and Gorman, Kristen 2020](99-references.html#ref-penguinsoriginaldata)),
    (some code[⁶](#fn6) is included below to help with this) then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `rename()` to improve the names.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the folder to a GitHub repo.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exchange the repo with another student.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a GitHub issue with two dot points about where the names could be improved.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss how your names compare with `palmerpenguins::penguins`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '**   Produce a tidy version of Anscombe’s Quartet, available using `anscombe`.'
  prefs: []
  type: TYPE_NORMAL
- en: How does the US General Social Survey code “don’t know”?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discuss the advantages and disadvantages of each option for coding missing
    data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '**   How do you think missing data should be coded in a dataset?'
  prefs: []
  type: TYPE_NORMAL
- en: Using a new R project, for a city that you are interested in:[⁷](#fn7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write reproducible code to download a ShotSpotter dataset from [here](http://justicetechlab.org/shotspotter-data/)
    and clean up the dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add your code to GitHub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exchange your repo with someone else in the class. They are to make a pull request
    that uses your dataset to make a graph.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the ImageNet dataset and open ten random images. Apply your own label. To
    what extent does your label agree with the actual label? What are the effects
    of this on the outcomes of models trained on this dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine you are interested in understanding intergenerational wealth, and one
    aspect of this is linking the educational outcomes of people today with those
    of people one hundred years ago, and in different countries. Please make the dataset
    consistent, and document why you made the choices that you made.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '**   Make tidy data from `datasets::UCBAdmissions`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `datasets::LifeCycleSavings`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First use `pivot_longer()` to make it long.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a graph using `ggplot`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then use `pivot_wider()` to change it back to wide.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a table using `tinytable::tt()` for Australia, Canada, New Zealand, and
    a country that you choose.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fix the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '**   Fix the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '**   Assume you live in Toronto, Canada. What is wrong with the following date?[⁸](#fn8)
    `2023-03-12 02:01:00`.'
  prefs: []
  type: TYPE_NORMAL
- en: Following the example in [Chapter 16](15-mrp.html) read in a “.dta” file from
    the [US General Social Survey](https://gss.norc.org/Get-The-Data), add the labels,
    and build a graph of some variable. What happens to the missing data?*****  ***###
    Task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With regard to Jordan ([2019](99-references.html#ref-Jordan2019Artificial)),
    D’Ignazio and Klein ([2020, chap. 6](99-references.html#ref-datafeminism2020)),
    Au ([2020](99-references.html#ref-thatrandyauperson)), and other relevant work,
    to what extent do you think we should let the data speak for themselves? Please
    write at least two pages.
  prefs: []
  type: TYPE_NORMAL
- en: Use Quarto, and include an appropriate title, author, date, link to a GitHub
    repo, and citations to produce a draft. After this, please pair with another student
    and exchange your written work. Update it based on their feedback, and be sure
    to acknowledge them by name in your paper. Submit a PDF.
  prefs: []
  type: TYPE_NORMAL
- en: 'Abelson, Harold, and Gerald Jay Sussman. 1996\. *Structure and Interpretation
    of Computer Programs*. Cambridge: The MIT Press.Abeysooriya, Mandhri, Megan Soria,
    Mary Sravya Kasu, and Mark Ziemann. 2021\. “Gene Name Errors: Lessons Not Learned.”
    *PLOS Computational Biology* 17 (7): 1–13\. [https://doi.org/10.1371/journal.pcbi.1008984](https://doi.org/10.1371/journal.pcbi.1008984).Alexander,
    Monica, and Leontine Alkema. 2022\. “A Bayesian Cohort Component Projection Model
    to Estimate Women of Reproductive Age at the Subnational Level in Data-Sparse
    Settings.” *Demography* 59 (5): 1713–37\. [https://doi.org/10.1215/00703370-10216406](https://doi.org/10.1215/00703370-10216406).Arel-Bundock,
    Vincent. 2022\. “modelsummary: Data and Model Summaries in R.” *Journal of Statistical
    Software* 103 (1): 1–23\. [https://doi.org/10.18637/jss.v103.i01](https://doi.org/10.18637/jss.v103.i01).———.
    2024\. *tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,”
    “Word,” “PNG,” “PDF,” and “Typst” Formats*. [https://vincentarelbundock.github.io/tinytable/](https://vincentarelbundock.github.io/tinytable/).Au,
    Randy. 2020\. “Data Cleaning IS Analysis, Not Grunt Work,” September. [https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt](https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt).Baker,
    Dominique. 2023\. “Scams Will Not Save Us (Tuition Dollars),” February. [http://www.dominiquebaker.com/blog/2023/2/16/scams-will-not-save-us-tuition-dollars](http://www.dominiquebaker.com/blog/2023/2/16/scams-will-not-save-us-tuition-dollars).Banes,
    Graham, Emily Fountain, Alyssa Karklus, Robert Fulton, Lucinda Antonacci-Fulton,
    and Joanne Nelson. 2022\. “Nine out of ten samples were mistakenly switched by
    The Orang-utan Genome Consortium.” *Scientific Data* 9 (1). [https://doi.org/10.1038/s41597-022-01602-0](https://doi.org/10.1038/s41597-022-01602-0).Baumgartner,
    Peter. 2021\. “Ways I Use Testing as a Data Scientist,” December. [https://www.peterbaumgartner.com/blog/testing-for-data-science/](https://www.peterbaumgartner.com/blog/testing-for-data-science/).Borer,
    Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009\.
    “Some Simple Guidelines for Effective Data Management.” *Bulletin of the Ecological
    Society of America* 90 (2): 205–14\. [https://doi.org/10.1890/0012-9623-90.2.205](https://doi.org/10.1890/0012-9623-90.2.205).Boykis,
    Vicki. 2019\. “A Deep Dive on Python Type Hints,” July. [https://vickiboykis.com/2019/07/08/a-deep-dive-on-python-type-hints/](https://vickiboykis.com/2019/07/08/a-deep-dive-on-python-type-hints/).Bryan,
    Jenny. 2015\. “Naming Things.” *Reproducible Science Workshop*, May. [https://speakerdeck.com/jennybc/how-to-name-files](https://speakerdeck.com/jennybc/how-to-name-files).Caro,
    Robert. 2019\. *Working*. 1st ed. New York: Knopf.Chan, Duo. 2021\. “Combining
    Statistical, Physical, and Historical Evidence to Improve Historical Sea-Surface
    Temperature Records.” *Harvard Data Science Review* 3 (1). [https://doi.org/10.1162/99608f92.edcee38f](https://doi.org/10.1162/99608f92.edcee38f).Craiu,
    Radu. 2019\. “The Hiring Gambit: In Search of the Twofer Data Scientist.” *Harvard
    Data Science Review* 1 (1). [https://doi.org/10.1162/99608f92.440445cb](https://doi.org/10.1162/99608f92.440445cb).D’Ignazio,
    Catherine, and Lauren Klein. 2020\. *Data Feminism*. Massachusetts: The MIT Press.
    [https://data-feminism.mitpress.mit.edu](https://data-feminism.mitpress.mit.edu).De
    Jonge, Edwin, and Mark van der Loo. 2013\. *An introduction to data cleaning with
    R*. Statistics Netherlands Heerlen. [https://cran.r-project.org/doc/contrib/de%5FJonge+van%5Fder%5FLoo-Introduction%5Fto%5Fdata%5Fcleaning%5Fwith%5FR.pdf](https://cran.r-project.org/doc/contrib/de%5FJonge+van%5Fder%5FLoo-Introduction%5Fto%5Fdata%5Fcleaning%5Fwith%5FR.pdf).Doggers,
    Peter. 2021\. “Carlsen Wins Game 6, Longest World Chess Championship Game of All
    Time,” December. [https://www.chess.com/news/view/fide-world-chess-championship-2021-game-6](https://www.chess.com/news/view/fide-world-chess-championship-2021-game-6).Du,
    Kai, Steven Huddart, and Xin Daniel Jiang. 2022\. “Lost in Standardization: Effects
    of Financial Statement Database Discrepancies on Inference.” *Journal of Accounting
    and Economics*, December, 101573\. [https://doi.org/10.1016/j.jacceco.2022.101573](https://doi.org/10.1016/j.jacceco.2022.101573).Firke,
    Sam. 2023\. *janitor: Simple Tools for Examining and Cleaning Dirty Data*. [https://CRAN.R-project.org/package=janitor](https://CRAN.R-project.org/package=janitor).Gagolewski,
    Marek. 2022\. “stringi: Fast and Portable Character String Processing in R.” *Journal
    of Statistical Software* 103 (2): 1–59\. [https://doi.org/10.18637/jss.v103.i02](https://doi.org/10.18637/jss.v103.i02).Gao,
    Zheng, Christian Bird, and Earl T. Barr. 2017\. “To Type or Not to Type: Quantifying
    Detectable Bugs in JavaScript.” In *2017 IEEE/ACM 39th International Conference
    on Software Engineering (ICSE)*. IEEE. [https://doi.org/10.1109/icse.2017.75](https://doi.org/10.1109/icse.2017.75).Gelfand,
    Sharla. 2022\. *opendatatoronto: Access the City of Toronto Open Data Portal*.
    [https://CRAN.R-project.org/package=opendatatoronto](https://CRAN.R-project.org/package=opendatatoronto).Gelman,
    Andrew, and Eric Loken. 2013\. “The Garden of Forking Paths: Why Multiple Comparisons
    Can Be a Problem, Even When There Is No ‘Fishing Expedition’ or ‘p-Hacking’ and
    the Research Hypothesis Was Posited Ahead of Time.” *Department of Statistics,
    Columbia University*. [http://www.stat.columbia.edu/~gelman/research/unpublished/p%5Fhacking.pdf](http://www.stat.columbia.edu/~gelman/research/unpublished/p%5Fhacking.pdf).Gelman,
    Andrew, and Aki Vehtari. 2021\. “What Are the Most Important Statistical Ideas
    of the Past 50 Years?” *Journal of the American Statistical Association* 116 (536):
    2087–97\. [https://doi.org/10.1080/01621459.2021.1938081](https://doi.org/10.1080/01621459.2021.1938081).Grolemund,
    Garrett, and Hadley Wickham. 2011\. “Dates and Times Made Easy with lubridate.”
    *Journal of Statistical Software* 40 (3): 1–25\. [https://doi.org/10.18637/jss.v040.i03](https://doi.org/10.18637/jss.v040.i03).Halberstam,
    David. 1972\. *The Best and the Brightest*. 1st ed. New York: Random House.Hand,
    David. 2018\. “Statistical Challenges of Administrative and Transaction Data.”
    *Journal of the Royal Statistical Society: Series A (Statistics in Society)* 181
    (3): 555–605\. [https://doi.org/10.1111/rssa.12315](https://doi.org/10.1111/rssa.12315).Hermans,
    Felienne. 2017\. “Peter Hilton on Naming.” *IEEE Software* 34 (3): 117–20\. [https://doi.org/10.1109/MS.2017.81](https://doi.org/10.1109/MS.2017.81).———.
    2021\. *The Programmer’s Brain: What Every Programmer Needs to Know about Cognition*.
    1st ed. New York: Simon; Schuster. [https://www.manning.com/books/the-programmers-brain](https://www.manning.com/books/the-programmers-brain).Hodgetts,
    Paul. 2022\. “The Negative Space of Data,” March. [https://hodgettsp.netlify.app/post/data-negativespace/](https://hodgettsp.netlify.app/post/data-negativespace/).Hofmeister,
    Johannes, Janet Siegmund, and Daniel Holt. 2017\. “Shorter Identifier Names Take
    Longer to Comprehend.” In *2017 IEEE 24th International Conference on Software
    Analysis, Evolution and Reengineering (SANER)*, 217–27\. [https://doi.org/10.1109/saner.2017.7884623](https://doi.org/10.1109/saner.2017.7884623).Huntington-Klein,
    Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli,
    Naibin Chen, et al. 2021\. “The Influence of Hidden Researcher Decisions in Applied
    Microeconomics.” *Economic Inquiry* 59: 944–60\. [https://doi.org/10.1111/ecin.12992](https://doi.org/10.1111/ecin.12992).Iannone,
    Richard, and Mauricio Vargas. 2022\. *pointblank: Data Validation and Organization
    of Metadata for Local and Remote Tables*. [https://CRAN.R-project.org/package=pointblank](https://CRAN.R-project.org/package=pointblank).Irving,
    Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte Wickham, and Greg
    Wilson. 2021\. *Research Software Engineering with Python*. Chapman; Hall/CRC.Jet
    Propulsion Laboratory. 2009\. “JPL Institutional Coding Standard for the C Programming
    Language.” *Document Number D-60411*, March. [https://web.archive.org/web/20111015064908/http://lars-lab.jpl.nasa.gov/JPL_Coding_Standard_C.pdf](https://web.archive.org/web/20111015064908/http://lars-lab.jpl.nasa.gov/JPL_Coding_Standard_C.pdf).Jordan,
    Michael. 2019\. “Artificial Intelligence–The Revolution Hasn’t Happened Yet.”
    *Harvard Data Science Review* 1 (1). [https://doi.org/10.1162/99608f92.f06c6e61](https://doi.org/10.1162/99608f92.f06c6e61).Lin,
    Sarah, Ibraheem Ali, and Greg Wilson. 2021\. “Ten Quick Tips for Making Things
    Findable.” *PLOS Computational Biology* 16 (12): 1–10\. [https://doi.org/10.1371/journal.pcbi.1008469](https://doi.org/10.1371/journal.pcbi.1008469).Liu,
    Emily, Lenny Bronner, and Jeremy Bowers. 2022\. “What the Washington Post Elections
    Engineering Team Had to Learn about Election Data.” *Washington Post Engineering*,
    April. [https://washpost.engineering/what-the-washington-post-elections-engineering-team-had-to-learn-about-election-data-a41603daf9ca](https://washpost.engineering/what-the-washington-post-elections-engineering-team-had-to-learn-about-election-data-a41603daf9ca).Lockheed
    Martin. 2005\. “Joint Strike Fighter Air Vehicle C++ Coding Standards For The
    System Development And Demonstration Program.” *Document Number 2RDU00001 Rev
    C*, December. [https://www.stroustrup.com/JSF-AV-rules.pdf](https://www.stroustrup.com/JSF-AV-rules.pdf).Martin,
    Charles, and Ben Popper. 2021\. “Don’t Push That Button: Exploring the Software
    That Flies SpaceX Rockets and Starships.” *The Overflow*, December. [https://stackoverflow.blog/2021/12/27/dont-push-that-button-exploring-the-software-that-flies-spacex-starships/](https://stackoverflow.blog/2021/12/27/dont-push-that-button-exploring-the-software-that-flies-spacex-starships/).Meng,
    Xiao-Li. 2018\. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large
    Populations, Big Data Paradox, and the 2016 US Presidential Election.” *The Annals
    of Applied Statistics* 12 (2): 685–726\. [https://doi.org/10.1214/18-AOAS1161SF](https://doi.org/10.1214/18-AOAS1161SF).Mindell,
    David. 2008\. *Digital Apollo: Human and Machine in Spaceflight*. 1st ed. New
    York: The MIT Press.Northcutt, Curtis, Anish Athalye, and Jonas Mueller. 2021\.
    “Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.”
    In *Proceedings of the 35th Conference on Neural Information Processing Systems
    Track on Datasets and Benchmarks*. [https://doi.org/10.48550/arXiv.2103.14749](https://doi.org/10.48550/arXiv.2103.14749).Ooms,
    Jeroen. 2022\. *pdftools: Text Extraction, Rendering and Converting of PDF Documents*.
    [https://CRAN.R-project.org/package=pdftools](https://CRAN.R-project.org/package=pdftools).Open
    Science Collaboration. 2015\. “Estimating the Reproducibility of Psychological
    Science.” *Science* 349 (6251): aac4716\. [https://doi.org/10.1126/science.aac4716](https://doi.org/10.1126/science.aac4716).Palmer
    Station Antarctica LTER, and Gorman, Kristen. 2020\. “Structural Size Measurements
    and Isotopic Signatures of Foraging Among Adult Male and Female Adélie Penguins
    (Pygoscelis Adeliae) Nesting Along the Palmer Archipelago Near Palmer Station,
    2007-2009.” [https://doi.org/10.6073/PASTA/98B16D7D563F265CB52372C8CA99E60F](https://doi.org/10.6073/PASTA/98B16D7D563F265CB52372C8CA99E60F).Preece,
    Donald Arthur. 1981\. “Distributions of Final Digits in Data.” *The Statistician*
    30 (1): 31\. [https://doi.org/10.2307/2987702](https://doi.org/10.2307/2987702).R
    Core Team. 2024\. *R: A Language and Environment for Statistical Computing*. Vienna,
    Austria: R Foundation for Statistical Computing. [https://www.R-project.org/](https://www.R-project.org/).Radcliffe,
    Nicholas. 2023\. *Test-Driven Data Analysis (Python TDDA library)*. [https://tdda.readthedocs.io/en/latest/index.html](https://tdda.readthedocs.io/en/latest/index.html).Riederer,
    Emily. 2020\. “Column Names as Contracts,” September. [https://emilyriederer.netlify.app/post/column-name-contracts/](https://emilyriederer.netlify.app/post/column-name-contracts/).Sambasivan,
    Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and
    Lora Aroyo. 2021\. “‘Everyone Wants to Do the Model Work, Not the Data Work’:
    Data Cascades in High-Stakes AI.” In *Proceedings of the 2021 CHI Conference on
    Human Factors in Computing Systems*. ACM. [https://doi.org/10.1145/3411764.3445518](https://doi.org/10.1145/3411764.3445518).Silberzahn,
    Raphael, Eric Uhlmann, Daniel Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey,
    Štěpán Bahnı́k, et al. 2018\. “Many Analysts, One Data Set: Making Transparent
    How Variations in Analytic Choices Affect Results.” *Advances in Methods and Practices
    in Psychological Science* 1 (3): 337–56\. [https://doi.org/10.1177/2515245917747646](https://doi.org/10.1177/2515245917747646).Somers,
    James. 2015\. “Toolkits for the Mind.” *MIT Technology Review*, April. [https://www.technologyreview.com/2015/04/02/168469/toolkits-for-the-mind/](https://www.technologyreview.com/2015/04/02/168469/toolkits-for-the-mind/).Turcotte,
    Alexi, Aviral Goel, Filip Křikava, and Jan Vitek. 2020\. “Designing Types for
    r, Empirically.” *Proceedings of the ACM on Programming Languages* 4 (OOPSLA):
    1–25\. [https://doi.org/10.1145/3428249](https://doi.org/10.1145/3428249).Van
    den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and Kobus Herbst.
    2005\. “Data Cleaning: Detecting, Diagnosing, and Editing Data Abnormalities.”
    *PLOS Medicine* 2 (10): e267\. [https://doi.org/10.1371/journal.pmed.0020267](https://doi.org/10.1371/journal.pmed.0020267).van
    der Loo, Mark. 2022\. *The Data Validation Cookbook*. [https://data-cleaning.github.io/validate/](https://data-cleaning.github.io/validate/).van
    der Loo, Mark, and Edwin De Jonge. 2021\. “Data Validation Infrastructure for
    R.” *Journal of Statistical Software* 97 (10): 1–33\. [https://doi.org/10.18637/jss.v097.i10](https://doi.org/10.18637/jss.v097.i10).Vickers,
    Andrew, and Emily Vertosick. 2016\. “An Empirical Study of Race Times in Recreational
    Endurance Runners.” *BMC Sports Science, Medicine and Rehabilitation* 8 (1). [https://doi.org/10.1186/s13102-016-0052-y](https://doi.org/10.1186/s13102-016-0052-y).Weinberg,
    Gerald. 1971\. *The Psychology of Computer Programming*. New York: Van Nostrand
    Reinhold Company.Wickham, Hadley. 2011\. “testthat: Get Started with Testing.”
    *The R Journal* 3: 5–10\. [https://journal.r-project.org/archive/2011-1/RJournal%5F2011-1%5FWickham.pdf](https://journal.r-project.org/archive/2011-1/RJournal%5F2011-1%5FWickham.pdf).———.
    2014\. “Tidy Data.” *Journal of Statistical Software* 59 (1): 1–23\. [https://doi.org/10.18637/jss.v059.i10](https://doi.org/10.18637/jss.v059.i10).Wickham,
    Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain
    François, Garrett Grolemund, et al. 2019\. “Welcome to the Tidyverse.” *Journal
    of Open Source Software* 4 (43): 1686\. [https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686).Wickham,
    Hadley, and Jennifer Bryan. 2023\. *readxl: Read Excel Files*. [https://CRAN.R-project.org/package=readxl](https://CRAN.R-project.org/package=readxl).Wickham,
    Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023\. *R for Data
    Science*. 2nd ed. O’Reilly Media. [https://r4ds.hadley.nz](https://r4ds.hadley.nz).Wickham,
    Hadley, and Dana Seidel. 2022\. *scales: Scale Functions for Visualization*. [https://CRAN.R-project.org/package=scales](https://CRAN.R-project.org/package=scales).Wilson,
    Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy Teal.
    2017\. “Good Enough Practices in Scientific Computing.” *PLOS Computational Biology*
    13 (6): 1–20\. [https://doi.org/10.1371/journal.pcbi.1005510](https://doi.org/10.1371/journal.pcbi.1005510).****
    **** * *'
  prefs: []
  type: TYPE_NORMAL
- en: By way of background, character encoding is needed for computers, which are
    based on strings of 0s and 1s, to be able to consider symbols such as alphabets.
    One source of particularly annoying data cleaning issues is different character
    encoding. This is especially common when dealing with foreign languages and odd
    characters. In general, we use an encoding called UTF-8\. The encoding of a character
    vector can be found using `Encoding()`.[↩︎](#fnref1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If this does not work, then the City of Toronto government may have moved the
    datasets. Instead use: `earlier_toronto_shelters <- read_csv("https://www.tellingstorieswithdata.com/inputs/data/earlier_toronto_shelters.csv")`.[↩︎](#fnref2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Or use: https://www.tellingstorieswithdata.com/inputs/pdfs/1996_Tanzania_DHS.pdf.[↩︎](#fnref3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the Kenyan government link breaks then replace their URL with: https://www.tellingstorieswithdata.com/inputs/pdfs/2019_Kenya_census.pdf.[↩︎](#fnref4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The underlying idea for this exercise is from Michael Donnelly.[↩︎](#fnref5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This code is from Christina Wei.[↩︎](#fnref6)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The idea for this exercise is from Taylor John Wright.[↩︎](#fnref7)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The idea for this exercise is from Derek Beaton.[↩︎](#fnref8)***********************
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
