<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>7  The statistics of least squares</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>7  The statistics of least squares</h1>
<blockquote>原文：<a href="https://mattblackwell.github.io/gov2002-book/ols_properties.html">https://mattblackwell.github.io/gov2002-book/ols_properties.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./linear_model.html">Regression</a></li><li class="breadcrumb-item"><a href="./ols_properties.html"><span class="chapter-number">7</span>  <span class="chapter-title">The statistics of least squares</span></a></li></ol></nav>
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The last chapter showcased the least squares estimator and investigated many of its more mechanical properties, which are essential for the practical application of OLS. But we still need to understand its statistical properties, as we discussed in Part I of this book: unbiasedness, sampling variance, consistency, and asymptotic normality. As we saw then, these properties fall into finite-sample (unbiasedness, sampling variance) and asymptotic (consistency, asymptotic normality).</p>
<p>In this chapter, we will focus on the asymptotic properties of OLS because those properties hold under the relatively mild conditions of the linear projection model introduced in <a href="linear_model.html#sec-linear-projection" class="quarto-xref"><span>Section 5.2</span></a>. We will see that OLS consistently estimates a coherent quantity of interest (the best linear predictor) regardless of whether the conditional expectation is linear. That is, for the asymptotic properties of the estimator, we will not need the commonly invoked linearity assumption. Later, when we investigate the finite-sample properties, we will show how linearity will help us establish unbiasedness and also how the normality of the errors can allow us to conduct exact, finite-sample inference. But these assumptions are very strong, so understanding what we can say about OLS without them is vital.</p>
<section id="large-sample-properties-of-ols" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="large-sample-properties-of-ols"><span class="header-section-number">7.1</span> Large-sample properties of OLS</h2>
<p>As we saw in <a href="asymptotics.html" class="quarto-xref"><span>Chapter 3</span></a>, we need two key ingredients to conduct statistical inference with the OLS estimator: (1) a consistent estimate of the variance of <span class="math inline">\(\bhat\)</span> and (2) the approximate distribution of <span class="math inline">\(\bhat\)</span> in large samples. Remember that, since <span class="math inline">\(\bhat\)</span> is a vector, the variance of that estimator will actually be a variance-covariance matrix. To obtain the two key ingredients, we first establish the consistency of OLS and then use the central limit theorem to derive its asymptotic distribution, which includes its variance.</p>
<p>We begin by setting out the assumptions needed for establishing the large-sample properties of OLS, which are the same as the assumptions needed to ensure that the best linear predictor, <span class="math inline">\(\bfbeta = \E[\X_{i}\X_{i}']^{-1}\E[\X_{i}Y_{i}]\)</span>, is well-defined and unique.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Linear projection assumptions
</div>
</div>
<div class="callout-body-container callout-body">
<p>The linear projection model makes the following assumptions:</p>
<ol type="1">
<li><p><span class="math inline">\(\{(Y_{i}, \X_{i})\}_{i=1}^n\)</span> are iid random vectors</p></li>
<li><p><span class="math inline">\(\E[Y^{2}_{i}] &lt; \infty\)</span> (finite outcome variance)</p></li>
<li><p><span class="math inline">\(\E[\Vert \X_{i}\Vert^{2}] &lt; \infty\)</span> (finite variances and covariances of covariates)</p></li>
<li><p><span class="math inline">\(\E[\X_{i}\X_{i}']\)</span> is positive definite (no linear dependence in the covariates)</p></li>
</ol>
</div>
</div>
<p>Recall that these are mild conditions on the joint distribution of <span class="math inline">\((Y_{i}, \X_{i})\)</span> and in particular, we are <strong>not</strong> assuming linearity of the CEF, <span class="math inline">\(\E[Y_{i} \mid \X_{i}]\)</span>, nor are we assuming any specific distribution for the data.</p>
<p>We can helpfully decompose the OLS estimator into the actual BLP coefficient plus estimation error as <span class="math display">\[
\bhat = \left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n \X_iY_i \right) = \bfbeta + \underbrace{\left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n \X_ie_i \right)}_{\text{estimation error}}.
\]</span></p>
<p>This decomposition will help us quickly establish the consistency of <span class="math inline">\(\bhat\)</span>. By the law of large numbers, we know that sample means will converge in probability to population expectations, so we have <span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \X_i\X_i' \inprob \E[\X_i\X_i'] \equiv \mb{Q}_{\X\X} \qquad \frac{1}{n} \sum_{i=1}^n \X_ie_i \inprob \E[\X_{i} e_{i}] = \mb{0},
\]</span> which implies by the continuous mapping theorem (the inverse is a continuous function) that <span class="math display">\[
\bhat \inprob \bfbeta + \mb{Q}_{\X\X}^{-1}\E[\X_ie_i] = \bfbeta,
\]</span> The linear projection assumptions ensure that the LLN applies to these sample means and that <span class="math inline">\(\E[\X_{i}\X_{i}']\)</span> is invertible.</p>
<div id="thm-ols-consistency" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.1</strong></span> Under the above linear projection assumptions, the OLS estimator is consistent for the best linear projection coefficients, <span class="math inline">\(\bhat \inprob \bfbeta\)</span>.</p>
</div>
<p>Thus, OLS should be close to the population linear regression in large samples under relatively mild conditions. Remember that this may not equal the conditional expectation if the CEF is nonlinear. What we can say is that OLS converges to the best <em>linear</em> approximation to the CEF. Of course, this also means that, if the CEF is linear, then OLS will consistently estimate the coefficients of the CEF.</p>
<p>To emphasize, the only assumptions made about the dependent variable are that it (1) has finite variance and (2) is iid. Under this assumption, the outcome could be continuous, categorical, binary, or event count.</p>
<p>Next, we would like to establish an asymptotic normality result for the OLS coefficients. We first review some key ideas about the Central Limit Theorem.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
CLT reminder
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose that we have a function of the data iid random vectors <span class="math inline">\(\X_1, \ldots, \X_n\)</span>, <span class="math inline">\(g(\X_{i})\)</span> where <span class="math inline">\(\E[g(\X_{i})] = 0\)</span> and so <span class="math inline">\(\V[g(\X_{i})] = \E[g(\X_{i})g(\X_{i})']\)</span>. Then if <span class="math inline">\(\E[\Vert g(\X_{i})\Vert^{2}] &lt; \infty\)</span>, the CLT implies that <span id="eq-clt-mean-zero"><span class="math display">\[
\sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} g(\X_{i}) - \E[g(\X_{i})]\right) = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} g(\X_{i}) \indist \N(0, \E[g(\X_{i})g(\X_{i}')])
\tag{7.1}\]</span></span></p>
</div>
</div>
<p>We now manipulate our decomposition to arrive at the <em>stabilized</em> version of the estimator, <span class="math display">\[
\sqrt{n}\left( \bhat - \bfbeta\right) = \left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{\sqrt{n}} \sum_{i=1}^n \X_ie_i \right).
\]</span> Recall that we stabilize an estimator to ensure it has a fixed variance as the sample size grows, allowing it to have a non-degenerate asymptotic distribution. The stabilization works by asymptotically centering it (that is, subtracting the value to which it converges) and multiplying by the square root of the sample size. We have already established that the first term on the right-hand side will converge in probability to <span class="math inline">\(\mb{Q}_{\X\X}^{-1}\)</span>. Notice that <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span>, so we can apply <a href="#eq-clt-mean-zero" class="quarto-xref">Equation <span>7.1</span></a> to the second term. The covariance matrix of <span class="math inline">\(\X_ie_{i}\)</span> is <span class="math display">\[
\mb{\Omega} = \V[\X_{i}e_{i}] = \E[\X_{i}e_{i}(\X_{i}e_{i})'] = \E[e_{i}^{2}\X_{i}\X_{i}'].
\]</span> The CLT will imply that <span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n \X_ie_i \indist \N(0, \mb{\Omega}).
\]</span> Combining these facts with Slutsky’s Theorem implies the following theorem.</p>
<div id="thm-ols-asymptotic-normality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.2</strong></span> Suppose that the linear projection assumptions hold and, in addition, we have <span class="math inline">\(\E[Y_{i}^{4}] &lt; \infty\)</span> and <span class="math inline">\(\E[\lVert\X_{i}\rVert^{4}] &lt; \infty\)</span>. Then the OLS estimator is asymptotically normal with <span class="math display">\[
\sqrt{n}\left( \bhat - \bfbeta\right) \indist \N(0, \mb{V}_{\bfbeta}),
\]</span> where <span class="math display">\[
\mb{V}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1} = \left( \E[\X_i\X_i'] \right)^{-1}\E[e_i^2\X_i\X_i']\left( \E[\X_i\X_i'] \right)^{-1}.
\]</span></p>
</div>
<p>Thus, with a large enough sample size we can approximate the distribution of <span class="math inline">\(\bhat\)</span> with a multivariate normal distribution with mean <span class="math inline">\(\bfbeta\)</span> and covariance matrix <span class="math inline">\(\mb{V}_{\bfbeta}/n\)</span>. In particular, the square root of the <span class="math inline">\(j\)</span>th diagonals of this matrix will be standard errors for <span class="math inline">\(\widehat{\beta}_j\)</span>. Knowing the shape of the OLS estimator’s multivariate distribution will allow us to conduct hypothesis tests and generate confidence intervals for both individual coefficients and groups of coefficients. But, first, we need an estimate of the covariance matrix.</p>
</section>
<section id="variance-estimation-for-ols" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="variance-estimation-for-ols"><span class="header-section-number">7.2</span> Variance estimation for OLS</h2>
<p>The asymptotic normality of OLS from the last section is of limited value without some way to estimate the covariance matrix, <span class="math display">\[
\mb{V}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1}.
\]</span> Since each term here is a population mean, this is an ideal place in which to drop a plug-in estimator. For now, we will use the following estimators: <span class="math display">\[
\begin{aligned}
  \mb{Q}_{\X\X} &amp;= \E[\X_{i}\X_{i}'] &amp; \widehat{\mb{Q}}_{\X\X} &amp;= \frac{1}{n} \sum_{i=1}^{n} \X_{i}\X_{i}' = \frac{1}{n}\Xmat'\Xmat \\
  \mb{\Omega} &amp;= \E[e_i^2\X_i\X_i'] &amp; \widehat{\mb{\Omega}} &amp; = \frac{1}{n}\sum_{i=1}^n\widehat{e}_i^2\X_i\X_i'.
\end{aligned}
\]</span> Under the assumptions of <a href="#thm-ols-asymptotic-normality" class="quarto-xref">Theorem <span>7.2</span></a>, the LLN will imply that these are consistent for the quantities we need, <span class="math inline">\(\widehat{\mb{Q}}_{\X\X} \inprob \mb{Q}_{\X\X}\)</span> and <span class="math inline">\(\widehat{\mb{\Omega}} \inprob \mb{\Omega}\)</span>. We can plug these into the variance formula to arrive at <span class="math display">\[
\begin{aligned}
  \widehat{\mb{V}}_{\bfbeta} &amp;= \widehat{\mb{Q}}_{\X\X}^{-1}\widehat{\mb{\Omega}}\widehat{\mb{Q}}_{\X\X}^{-1} \\
  &amp;= \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i' \right) \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1},
\end{aligned}
\]</span> which by the continuous mapping theorem is consistent, <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta} \inprob \mb{V}_{\bfbeta}\)</span>.</p>
<p>This estimator is sometimes called the <strong>robust variance estimator</strong> or, more accurately, the <strong>heteroskedasticity-consistent (HC) variance estimator</strong>. Why is it robust? Consider the standard <strong>homoskedasticity</strong> assumption that most statistical software packages make when estimating OLS variances: the variance of the errors does not depend on the covariates, or <span class="math inline">\(\V[e_{i}^{2} \mid \X_{i}] = \V[e_{i}^{2}]\)</span>. This assumption is stronger than needed, and we can rely on a weaker assumption that the squared errors are uncorrelated with a specific function of the covariates: <span class="math display">\[
\E[e_{i}^{2}\X_{i}\X_{i}'] = \E[e_{i}^{2}]\E[\X_{i}\X_{i}'] = \sigma^{2}\mb{Q}_{\X\X},
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the variance of the residuals (since <span class="math inline">\(\E[e_{i}] = 0\)</span>). Homoskedasticity simplifies the asymptotic variance of the stabilized estimator, <span class="math inline">\(\sqrt{n}(\bhat - \bfbeta)\)</span>, to <span class="math display">\[
\mb{V}^{\texttt{lm}}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\sigma^{2}\mb{Q}_{\X\X}\mb{Q}_{\X\X}^{-1} = \sigma^2\mb{Q}_{\X\X}^{-1}.
\]</span> We already have an estimator for <span class="math inline">\(\mb{Q}_{\X\X}\)</span>, but we need one for <span class="math inline">\(\sigma^2\)</span>. We can easily use the SSR, <span class="math display">\[
\widehat{\sigma}^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n} \widehat{e}_{i}^{2},
\]</span> where we use <span class="math inline">\(n-k-1\)</span> in the denominator instead of <span class="math inline">\(n\)</span> to correct for the residuals being slightly less variable than the actual errors (because OLS mechanically attempts to make the residuals small). For consistent variance estimation, <span class="math inline">\(n-k -1\)</span> or <span class="math inline">\(n\)</span> can be used, since either way <span class="math inline">\(\widehat{\sigma}^2 \inprob \sigma^2\)</span>. Thus, under homoskedasticity, we have <span class="math display">\[
\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}} = \widehat{\sigma}^{2}\left(\frac{1}{n}\Xmat'\Xmat\right)^{{-1}} = n\widehat{\sigma}^{2}\left(\Xmat'\Xmat\right)^{{-1}},
\]</span> This is the standard variance estimator used by <code>lm()</code> in R and <code>reg</code> in Stata.</p>
<p>How do these two estimators, <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}\)</span> and <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\)</span>, compare? Notice that the HC variance estimator and the homoskedasticity variance estimator will both be consistent when homoskedasticity holds. But as the “heteroskedasticity-consistent” label implies, only the HC variance estimator will be consistent when homoskedasticity fails to hold. So <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}\)</span> has the advantage of being consistent regardless of the homoskedasticity assumption. This advantage comes at a cost, however. When homoskedasticity is correct, <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\)</span> incorporates that assumption into the estimator whereas the HC variance estimator has to estimate it. The HC estimator will therefore have higher variance (the variance estimator will be more variable!) when homoskedasticity actually does hold.</p>
<!-- TODO: add discussion of King and Roberts -->
<p>Now that we have established the asymptotic normality of the OLS estimator and developed a consistent estimator of its variance, we can proceed with all of the statistical inference tools we discussed in Part I, including hypothesis tests and confidence intervals.</p>
<p>We begin by defining the estimated <strong>heteroskedasticity-consistent standard errors</strong> as <span class="math display">\[
\widehat{\se}(\widehat{\beta}_{j}) = \sqrt{\frac{[\widehat{\mb{V}}_{\bfbeta}]_{jj}}{n}},
\]</span> where <span class="math inline">\([\widehat{\mb{V}}_{\bfbeta}]_{jj}\)</span> is the <span class="math inline">\(j\)</span>th diagonal entry of the HC variance estimator. Note that we divide by <span class="math inline">\(\sqrt{n}\)</span> here because <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}\)</span> is a consistent estimator of the stabilized estimator <span class="math inline">\(\sqrt{n}(\bhat - \bfbeta)\)</span> not the estimator itself.</p>
<p>Hypothesis tests and confidence intervals for individual coefficients are almost precisely the same as with the most general case presented in Part I. For a two-sided test of <span class="math inline">\(H_0: \beta_j = b\)</span> versus <span class="math inline">\(H_1: \beta_j \neq b\)</span>, we can build the t-statistic and conclude that, under the null, <span class="math display">\[
\frac{\widehat{\beta}_j - b}{\widehat{\se}(\widehat{\beta}_{j})} \indist \N(0, 1).
\]</span> Statistical software will typically and helpfully provide the t-statistic for the null hypothesis of no (partial) linear relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>, <span class="math display">\[
t = \frac{\widehat{\beta}_{j}}{\widehat{\se}(\widehat{\beta}_{j})},
\]</span> which measures how large the estimated coefficient is in standard errors. With <span class="math inline">\(\alpha = 0.05\)</span>, asymptotic normality would imply that we reject this null when <span class="math inline">\(t &gt; 1.96\)</span>. We can form asymptotically-valid confidence intervals with <span class="math display">\[
\left[\widehat{\beta}_{j} - z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j}),\;\widehat{\beta}_{j} + z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j})\right].
\]</span> For reasons we will discuss below, standard software typically relies on the <span class="math inline">\(t\)</span> distribution instead of the normal for hypothesis testing and confidence intervals. Still, this difference is of little consequence in large samples.</p>
</section>
<section id="inference-for-multiple-parameters" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="inference-for-multiple-parameters"><span class="header-section-number">7.3</span> Inference for multiple parameters</h2>
<p>With multiple coefficients, we might have hypotheses that involve more than one coefficient. As an example, consider a regression with an interaction between two covariates, <span class="math display">\[
Y_i = \beta_0 + X_i\beta_1 + Z_i\beta_2 + X_iZ_i\beta_3 + e_i.
\]</span> Suppose we wanted to test the hypothesis that <span class="math inline">\(X_i\)</span> does not affect the best linear predictor for <span class="math inline">\(Y_i\)</span>. That would be <span class="math display">\[
H_{0}: \beta_{1} = 0 \text{ and } \beta_{3} = 0\quad\text{vs}\quad H_{1}: \beta_{1} \neq 0 \text{ or } \beta_{3} \neq 0,
\]</span> where we usually write the null more compactly as <span class="math inline">\(H_0: \beta_1 = \beta_3 = 0\)</span>.</p>
<p>To test this null hypothesis, we need a test statistic that discriminates between the two hypotheses: it should be large when the alternative is true and small enough when the null is true. With a single coefficient, we usually test the null hypothesis of <span class="math inline">\(H_0: \beta_j = b_0\)</span> with the <span class="math inline">\(t\)</span>-statistic, <span class="math display">\[
t = \frac{\widehat{\beta}_{j} - b_{0}}{\widehat{\se}(\widehat{\beta}_{j})},
\]</span> and we usually take the absolute value, <span class="math inline">\(|t|\)</span>, as our measure of how extreme our estimate is given the null distribution. But notice that we could also use the square of the <span class="math inline">\(t\)</span> statistic, which is <span id="eq-squared-t"><span class="math display">\[
t^{2} = \frac{\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{\V[\widehat{\beta}_{j}]} = \frac{n\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{[\mb{V}_{\bfbeta}]_{[jj]}}.
\tag{7.2}\]</span></span></p>
<p>While <span class="math inline">\(|t|\)</span> is the usual test statistic we use for two-sided tests, we could equivalently use <span class="math inline">\(t^2\)</span> and arrive at the exact same conclusions (as long as we knew the distribution of <span class="math inline">\(t^2\)</span> under the null hypothesis). It turns out that the <span class="math inline">\(t^2\)</span> version of the test statistic will generalize more easily to comparing multiple coefficients. This version of the test statistic suggests another general way to differentiate the null from the alternative: by taking the squared distance between them and dividing by the variance of the estimate.</p>
<p>Can we generalize this idea to hypotheses about multiple parameters? Adding the sum of squared distances for each component of the null hypothesis is straightforward. For our interaction example, that would be <span class="math display">\[
\widehat{\beta}_1^2 + \widehat{\beta}_3^2,
\]</span> Remember, however, that some of the estimated coefficients are noisier than others, so we should account for the uncertainty just like we did for the <span class="math inline">\(t\)</span>-statistic.</p>
<p>With multiple parameters and multiple coefficients, the variances will now require matrix algebra. We can write any hypothesis about linear functions of the coefficients as <span class="math inline">\(H_{0}: \mb{L}\bfbeta = \mb{c}\)</span>. For example, in the interaction case, we have <span class="math display">\[
\mb{L} =
\begin{pmatrix}
  0 &amp; 1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 1 \\
\end{pmatrix}
\qquad
\mb{c} =
\begin{pmatrix}
  0 \\
  0
\end{pmatrix}
\]</span> Thus, <span class="math inline">\(\mb{L}\bfbeta = \mb{0}\)</span> is equivalent to <span class="math inline">\(\beta_1 = 0\)</span> and <span class="math inline">\(\beta_3 = 0\)</span>. Notice that with other <span class="math inline">\(\mb{L}\)</span> matrices, we could represent more complicated hypotheses like <span class="math inline">\(2\beta_1 - \beta_2 = 34\)</span>, though we mostly stick to simpler functions. Let <span class="math inline">\(\widehat{\bs{\theta}} = \mb{L}\bhat\)</span> be the OLS estimate of the function of the coefficients. By the delta method (discussed in <a href="asymptotics.html#sec-delta-method" class="quarto-xref"><span>Section 3.9</span></a>), we have <span class="math display">\[
\sqrt{n}\left(\mb{L}\bhat - \mb{L}\bfbeta\right) \indist \N(0, \mb{L}\mb{V}_{\bfbeta}\mb{L}').
\]</span> We can now generalize the squared <span class="math inline">\(t\)</span> statistic in <a href="#eq-squared-t" class="quarto-xref">Equation <span>7.2</span></a> by taking the distances <span class="math inline">\(\mb{L}\bhat - \mb{c}\)</span> weighted by the variance-covariance matrix <span class="math inline">\(\mb{L}\mb{V}_{\bfbeta}\mb{L}'\)</span>, <span class="math display">\[
W = n(\mb{L}\bhat - \mb{c})'(\mb{L}\mb{V}_{\bfbeta}\mb{L}')^{-1}(\mb{L}\bhat - \mb{c}),
\]</span> which is called the <strong>Wald test statistic</strong>. This statistic generalizes the ideas of the t-statistic to multiple parameters. With the t-statistic, we recenter to have mean 0 and divide by the standard error to get a variance of 1. If we ignore the middle variance weighting, we have <span class="math inline">\((\mb{L}\bhat - \mb{c})'(\mb{L}\bhat - \mb{c})\)</span> which is just the sum of the squared deviations of the estimates from the null. Including the <span class="math inline">\((\mb{L}\mb{V}_{\bfbeta}\mb{L}')^{-1}\)</span> weight has the effect of rescaling the distribution of <span class="math inline">\(\mb{L}\bhat - \mb{c}\)</span> to make it rotationally symmetric around 0 (so the resulting dimensions are uncorrelated) with each dimension having an equal variance of 1. In this way, the Wald statistic transforms the random vectors to be mean-centered and have variance 1 (just the t-statistic), but also to have the resulting random variables in the vector be uncorrelated.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Why transform the data in this way? <a href="#fig-wald" class="quarto-xref">Figure <span>7.1</span></a> shows the contour plot of a hypothetical joint distribution of two coefficients from an OLS regression. We might want to know the distance between different points in the distribution and the mean, which in this case is <span class="math inline">\((1, 2)\)</span>. Without considering the joint distribution, the circle is obviously closer to the mean than the triangle. However, looking at the two points on the distribution, the circle is at a lower contour than the triangle, meaning it is more extreme than the triangle for this particular distribution. The Wald statistic, then, takes into consideration how much of a “climb” it is for <span class="math inline">\(\mb{L}\bhat\)</span> to get to <span class="math inline">\(\mb{c}\)</span> given the distribution of <span class="math inline">\(\mb{L}\bhat\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-wald" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wald-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/1786b9fe10a772e336cbed68c1df9692.png" class="img-fluid figure-img" width="672" data-original-src="https://mattblackwell.github.io/gov2002-book/ols_properties_files/figure-html/fig-wald-1.png"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wald-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 7.1: Hypothetical joint distribution of two slope coefficients. The circle is closer to the center of the distribution by the standard Euclidean distance, but the triangle is closer once you consider the joint distribution.
</figcaption>
</figure>
</div>
</div>
</div>
<p>If <span class="math inline">\(\mb{L}\)</span> only has one row, our Wald statistic is the same as the squared <span class="math inline">\(t\)</span> statistic, <span class="math inline">\(W = t^2\)</span>. This fact will help us think about the asymptotic distribution of <span class="math inline">\(W\)</span>. Note that as <span class="math inline">\(n\to\infty\)</span>, we know that by the asymptotic normality of <span class="math inline">\(\bhat\)</span>, <span class="math display">\[
t = \frac{\widehat{\beta}_{j} - \beta_{j}}{\widehat{\se}[\widehat{\beta}_{j}]} \indist \N(0,1)
\]</span> so <span class="math inline">\(t^2\)</span> will converge in distribution to a <span class="math inline">\(\chi^2_1\)</span> (since a <span class="math inline">\(\chi^2_1\)</span> distribution is just one standard normal distribution squared). After recentering and rescaling by the covariance matrix, <span class="math inline">\(W\)</span> converges to the sum of <span class="math inline">\(q\)</span> squared independent normals, where <span class="math inline">\(q\)</span> is the number of rows of <span class="math inline">\(\mb{L}\)</span>, or equivalently, the number of restrictions implied by the null hypothesis. Thus, under the null hypothesis of <span class="math inline">\(\mb{L}\bhat = \mb{c}\)</span>, we have <span class="math inline">\(W \indist \chi^2_{q}\)</span>.</p>
<p>We need to define the rejection region to use the Wald statistic in a hypothesis test. Because we are squaring each distance in <span class="math inline">\(W \geq 0\)</span>, larger values of <span class="math inline">\(W\)</span> indicate more disagreement with the null in either direction. Thus, for an <span class="math inline">\(\alpha\)</span>-level test of the joint null, we only need a one-sided rejection region of the form <span class="math inline">\(\P(W &gt; w_{\alpha}) = \alpha\)</span>. Obtaining these values is straightforward (see the above callout tip). For <span class="math inline">\(q = 2\)</span> and a <span class="math inline">\(\alpha = 0.05\)</span>, the critical value is roughly 6.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Chi-squared critical values
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can obtain critical values for the <span class="math inline">\(\chi^2_q\)</span> distribution using the <code>qchisq()</code> function in R. For example, if we wanted to obtain the critical value <span class="math inline">\(w\)</span> such that <span class="math inline">\(\P(W &gt; w_{\alpha}) = \alpha\)</span> for our two-parameter interaction example, we could use:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"/><span class="fu">qchisq</span>(<span class="at">p =</span> <span class="fl">0.95</span>, <span class="at">df =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.991465</code></pre>
</div>
</div>
</div>
</div>
<p>The Wald statistic is not a common test provided by standard statistical software functions like <code>lm()</code> in R, though it is fairly straightforward to implement “by hand.” Alternatively, packages like <a href="https://cran.r-project.org/web/packages/aod/index.html"><code>{aod}</code></a> or <a href="http://jepusto.github.io/clubSandwich/"><code>{clubSandwich}</code></a> have implementations of the test. What is reported by most software implementations of OLS (like <code>lm()</code> in R) is the F-statistic, which is <span class="math display">\[
F = \frac{W}{q}.
\]</span> This also typically uses the homoskedastic variance estimator <span class="math inline">\(\mb{V}^{\texttt{lm}}_{\bfbeta}\)</span> in <span class="math inline">\(W\)</span>. The p-values reported for such tests use the <span class="math inline">\(F_{q,n-k-1}\)</span> distribution because this is the exact distribution of the <span class="math inline">\(F\)</span> statistic when the errors are (a) homoskedastic and (b) normally distributed. When these assumptions do not hold, the <span class="math inline">\(F\)</span> distribution has no justification in statistical theory, but it is slightly more conservative than the <span class="math inline">\(\chi^2_q\)</span> distribution, and the inferences from the <span class="math inline">\(F\)</span> statistic will converge to those from the <span class="math inline">\(\chi^2_q\)</span> distribution as <span class="math inline">\(n\to\infty\)</span>. So it might be justified as an <em>ad hoc</em> small-sample adjustment to the Wald test. For example, if we used the <span class="math inline">\(F_{q,n-k-1}\)</span> with the interaction example where <span class="math inline">\(q=2\)</span> and we have, say, a sample size of <span class="math inline">\(n = 100\)</span>, then in that case, the critical value for the F test with <span class="math inline">\(\alpha = 0.05\)</span> is</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"/><span class="fu">qf</span>(<span class="fl">0.95</span>, <span class="at">df1 =</span> <span class="dv">2</span>, <span class="at">df2 =</span> <span class="dv">100</span> <span class="sc">-</span> <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"/></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.091191</code></pre>
</div>
</div>
<p>This result implies a critical value of 6.182 on the scale of the Wald statistic (multiplying it by <span class="math inline">\(q = 2\)</span>). Compared to the earlier critical value of 5.991 based on the <span class="math inline">\(\chi^2_2\)</span> distribution, we can see that the inferences will be very similar even in moderately-sized datasets.</p>
<p>Finally, note that the F-statistic reported by <code>lm()</code> in R is the test of all the coefficients being equal to 0 jointly except for the intercept. In modern quantitative social sciences, this test is seldom substantively interesting.</p>
</section>
<section id="finite-sample-properties-with-a-linear-cef" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="finite-sample-properties-with-a-linear-cef"><span class="header-section-number">7.4</span> Finite-sample properties with a linear CEF</h2>
<p>All the above results have been large-sample properties, and we have not addressed finite-sample properties like the sampling variance or unbiasedness. Under the linear projection assumption above, OLS is generally biased without stronger assumptions. This section introduces the stronger assumption that will allow us to establish stronger properties for OLS. As usual, however, remember that these stronger assumptions can be wrong.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Assumption: Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>The variables <span class="math inline">\((Y_{i}, \X_{i})\)</span> satisfy the linear CEF assumption. <span class="math display">\[
\begin{aligned}
  Y_{i} &amp;= \X_{i}'\bfbeta + e_{i} \\
  \E[e_{i}\mid \X_{i}] &amp; = 0.
\end{aligned}
\]</span></p></li>
<li><p>The design matrix is invertible <span class="math inline">\(\E[\X_{i}\X_{i}'] &gt; 0\)</span> (positive definite).</p></li>
</ol>
</div>
</div>
<p>We discussed the concept of a linear CEF extensively in <a href="linear_model.html" class="quarto-xref"><span>Chapter 5</span></a>. However, recall that the CEF might be linear mechanically if the model is <strong>saturated</strong> or when there are as many coefficients in the model as there are unique values of <span class="math inline">\(\X_i\)</span>. When a model is not saturated, the linear CEF assumption is just that: an assumption. What can this assumption do? It can aid in establishing some nice statistical properties in finite samples.</p>
<p>Before proceeding, note that, when focusing on the finite sample inference for OLS, we focused on its properties <strong>conditional on the observed covariates</strong>, such as <span class="math inline">\(\E[\bhat \mid \Xmat]\)</span> or <span class="math inline">\(\V[\bhat \mid \Xmat]\)</span>. The historical reason for this is that the researcher often chose these independent variables and so they were not random. Thus, sometimes <span class="math inline">\(\Xmat\)</span> is treated as “fixed” in some older texts, which might even omit explicit conditioning statements.</p>
<div id="thm-ols-unbiased" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.3</strong></span> Under the linear regression model assumption, OLS is unbiased for the population regression coefficients, <span class="math display">\[
\E[\bhat \mid \Xmat] = \bfbeta,
\]</span> and its conditional sampling variance is <span class="math display">\[
\mb{\V}_{\bhat} = \V[\bhat \mid \Xmat] = \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2_i \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1},
\]</span> where <span class="math inline">\(\sigma^2_{i} = \E[e_{i}^{2} \mid \Xmat]\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove the conditional unbiasedness, recall that we can write the OLS estimator as <span class="math display">\[
\bhat = \bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\mb{e},
\]</span> and so taking (conditional) expectations, we have <span class="math display">\[
\E[\bhat \mid \Xmat] = \bfbeta + \E[(\Xmat'\Xmat)^{-1}\Xmat'\mb{e} \mid \Xmat] = \bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\E[\mb{e} \mid \Xmat] = \bfbeta,
\]</span> because under the linear CEF assumption <span class="math inline">\(\E[\mb{e}\mid \Xmat] = 0\)</span>.</p>
<p>For the conditional sampling variance, we can use the same decomposition we have, <span class="math display">\[
\V[\bhat \mid \Xmat] = \V[\bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\mb{e} \mid \Xmat] = (\Xmat'\Xmat)^{-1}\Xmat'\V[\mb{e} \mid \Xmat]\Xmat(\Xmat'\Xmat)^{-1}.
\]</span> Since <span class="math inline">\(\E[\mb{e}\mid \Xmat] = 0\)</span>, we know that <span class="math inline">\(\V[\mb{e}\mid \Xmat] = \E[\mb{ee}' \mid \Xmat]\)</span>, which is a matrix with diagonal entries <span class="math inline">\(\E[e_{i}^{2} \mid \Xmat] = \sigma^2_i\)</span> and off-diagonal entries <span class="math inline">\(\E[e_{i}e_{j} \Xmat] = \E[e_{i}\mid \Xmat]\E[e_{j}\mid\Xmat] = 0\)</span>, where the first equality follows from the independence of the errors across units. Thus, <span class="math inline">\(\V[\mb{e} \mid \Xmat]\)</span> is a diagonal matrix with <span class="math inline">\(\sigma^2_i\)</span> along the diagonal, which means <span class="math display">\[
\Xmat'\V[\mb{e} \mid \Xmat]\Xmat = \sum_{i=1}^n \sigma^2_i \X_i\X_i',
\]</span> establishing the conditional sampling variance.</p>
</div>
<p>This means that, for any realization of the covariates, <span class="math inline">\(\Xmat\)</span>, OLS is unbiased for the true regression coefficients <span class="math inline">\(\bfbeta\)</span>. By the law of iterated expectation, we also know that it is unconditionally unbiased<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> as well since <span class="math display">\[
\E[\bhat] = \E[\E[\bhat \mid \Xmat]] = \bfbeta.
\]</span> The difference between these two statements usually isn’t incredibly meaningful.</p>
<p>There are a lot of variances flying around, so reviewing them is helpful. Above, we derived the asymptotic variance of <span class="math inline">\(\mb{Z}_{n} = \sqrt{n}(\bhat - \bfbeta)\)</span>, <span class="math display">\[
\mb{V}_{\bfbeta} = \left( \E[\X_i\X_i'] \right)^{-1}\E[e_i^2\X_i\X_i']\left( \E[\X_i\X_i'] \right)^{-1},
\]</span> which implies that the approximate variance of <span class="math inline">\(\bhat\)</span> will be <span class="math inline">\(\mb{V}_{\bfbeta} / n\)</span> because <span class="math display">\[
\bhat = \frac{Z_n}{\sqrt{n}} + \bfbeta \quad\implies\quad \bhat \overset{a}{\sim} \N(\bfbeta, n^{-1}\mb{V}_{\bfbeta}),
\]</span> where <span class="math inline">\(\overset{a}{\sim}\)</span> means asymptotically distributed as. Under the linear CEF, the conditional sampling variance of <span class="math inline">\(\bhat\)</span> has a similar form and will be similar to the<br/>
<span class="math display">\[
\mb{V}_{\bhat} = \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2_i \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \approx \mb{V}_{\bfbeta} / n.
\]</span> In practice, these two derivations lead to basically the same variance estimator. Recall that the heteroskedastic-consistent variance estimator <span class="math display">\[
\widehat{\mb{V}}_{\bfbeta} = \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i' \right) \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1},
\]</span> is a valid plug-in estimator for the asymptotic variance and <span class="math display">\[
\widehat{\mb{V}}_{\bhat} = n^{-1}\widehat{\mb{V}}_{\bfbeta}.
\]</span> Thus, in practice, the asymptotic and finite-sample results under a linear CEF justify the same variance estimator.</p>
<section id="linear-cef-model-under-homoskedasticity" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="linear-cef-model-under-homoskedasticity"><span class="header-section-number">7.4.1</span> Linear CEF model under homoskedasticity</h3>
<p>If we are willing to assume that the standard errors are homoskedastic, we can derive even stronger results for OLS. Stronger assumptions typically lead to stronger conclusions, but, obviously, those conclusions may not be robust to assumption violations. But homoskedasticity of errors is such a historically important assumption that statistical software implementations of OLS like <code>lm()</code> in R assume it by default.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Assumption: Homoskedasticity with a linear CEF
</div>
</div>
<div class="callout-body-container callout-body">
<p>In addition to the linear CEF assumption, we further assume that <span class="math display">\[
\E[e_i^2 \mid \X_i] = \E[e_i^2] = \sigma^2,
\]</span> or that variance of the errors does not depend on the covariates.</p>
</div>
</div>
<div id="thm-homoskedasticity" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.4</strong></span> Under a linear CEF model with homoskedastic errors, the conditional sampling variance is <span class="math display">\[
\mb{V}^{\texttt{lm}}_{\bhat} = \V[\bhat \mid \Xmat] = \sigma^2 \left( \Xmat'\Xmat \right)^{-1},
\]</span> and the variance estimator <span class="math display">\[
\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} = \widehat{\sigma}^2 \left( \Xmat'\Xmat \right)^{-1} \quad\text{where,}\quad \widehat{\sigma}^2 = \frac{1}{n - k - 1} \sum_{i=1}^n \widehat{e}_i^2
\]</span> is unbiased, <span class="math inline">\(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} \mid \Xmat] = \mb{V}^{\texttt{lm}}_{\bhat}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Under homoskedasticity <span class="math inline">\(\sigma^2_i = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>. Recall that <span class="math inline">\(\sum_{i=1}^n \X_i\X_i' = \Xmat'\Xmat\)</span>. Thus, the conditional sampling variance from <a href="#thm-ols-unbiased" class="quarto-xref">Theorem <span>7.3</span></a>, <span class="math display">\[
\begin{aligned}
\V[\bhat \mid \Xmat] &amp;= \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2 \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \\ &amp;= \sigma^2\left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \\&amp;= \sigma^2\left( \Xmat'\Xmat \right)^{-1}\left( \Xmat'\Xmat \right) \left( \Xmat'\Xmat \right)^{-1} \\&amp;= \sigma^2\left( \Xmat'\Xmat \right)^{-1} = \mb{V}^{\texttt{lm}}_{\bhat}.
\end{aligned}
\]</span></p>
<p>For unbiasedness, we just need to show that <span class="math inline">\(\E[\widehat{\sigma}^{2} \mid \Xmat] = \sigma^2\)</span>. Recall that we defined <span class="math inline">\(\mb{M}_{\Xmat}\)</span> as the residual-maker because <span class="math inline">\(\mb{M}_{\Xmat}\mb{Y} = \widehat{\mb{e}}\)</span>. We can use this to connect the residuals to the standard errors, <span class="math display">\[
\mb{M}_{\Xmat}\mb{e} = \mb{M}_{\Xmat}\mb{Y} - \mb{M}_{\Xmat}\Xmat\bfbeta = \mb{M}_{\Xmat}\mb{Y} = \widehat{\mb{e}},
\]</span> so <span class="math display">\[
\V[\widehat{\mb{e}} \mid \Xmat] = \mb{M}_{\Xmat}\V[\mb{e} \mid \Xmat] = \mb{M}_{\Xmat}\sigma^2,
\]</span> where the first equality holds because <span class="math inline">\(\mb{M}_{\Xmat} = \mb{I}_{n} - \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\)</span> is constant conditional on <span class="math inline">\(\Xmat\)</span>. Notice that the diagonal entries of this matrix are the variances of particular residuals <span class="math inline">\(\widehat{e}_i\)</span> and that the diagonal entries of the annihilator matrix are <span class="math inline">\(1 - h_{ii}\)</span> (since the <span class="math inline">\(h_{ii}\)</span> are the diagonal entries of <span class="math inline">\(\mb{P}_{\Xmat}\)</span>). Thus, we have <span class="math display">\[
\V[\widehat{e}_i \mid \Xmat] = \E[\widehat{e}_{i}^{2} \mid \Xmat] = (1 - h_{ii})\sigma^{2}.
\]</span> In the last chapter in <a href="least_squares.html#sec-leverage" class="quarto-xref"><span>Section 6.9.1</span></a>, we established that one property of these leverage values is <span class="math inline">\(\sum_{i=1}^n h_{ii} = k+ 1\)</span>, so <span class="math inline">\(\sum_{i=1}^n 1- h_{ii} = n - k - 1\)</span> and we have <span class="math display">\[
\begin{aligned}
  \E[\widehat{\sigma}^{2} \mid \Xmat] &amp;= \frac{1}{n-k-1} \sum_{i=1}^{n} \E[\widehat{e}_{i}^{2} \mid \Xmat] \\
                                      &amp;= \frac{\sigma^{2}}{n-k-1} \sum_{i=1}^{n} 1 - h_{ii} \\
                                      &amp;= \sigma^{2}.
\end{aligned}
\]</span> This establishes <span class="math inline">\(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} \mid \Xmat] = \mb{V}^{\texttt{lm}}_{\bhat}\)</span>.</p>
</div>
<p>Thus, under the linear CEF model and homoskedasticity of the errors, we have an unbiased variance estimator that is a simple function of the sum of squared residuals and the design matrix. Most statistical software packages estimate standard errors using <span class="math inline">\(\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}\)</span>.</p>
<p>The final result we can derive for the linear CEF under the homoskedasticity assumption is an optimality result. That is, we might ask if there is another estimator for <span class="math inline">\(\bfbeta\)</span> that would outperform OLS in the sense of having a lower sampling variance. Perhaps surprisingly, no linear estimator for <span class="math inline">\(\bfbeta\)</span> has a lower conditional variance, meaning that OLS is the <strong>best linear unbiased estimator</strong>, often jovially shortened to BLUE. This result is famously known as the Gauss-Markov Theorem.</p>
<div id="thm-gauss-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.5</strong></span> Let <span class="math inline">\(\widetilde{\bfbeta} = \mb{AY}\)</span> be a linear and unbiased estimator for <span class="math inline">\(\bfbeta\)</span>. Under the linear CEF model with homoskedastic errors, <span class="math display">\[
\V[\widetilde{\bfbeta}\mid \Xmat] \geq \V[\bhat \mid \Xmat].
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Note that if <span class="math inline">\(\widetilde{\bfbeta}\)</span> is unbiased then <span class="math inline">\(\E[\widetilde{\bfbeta} \mid \Xmat] = \bfbeta\)</span> and so <span class="math display">\[
\bfbeta = \E[\mb{AY} \mid \Xmat] = \mb{A}\E[\mb{Y} \mid \Xmat] = \mb{A}\Xmat\bfbeta,
\]</span> which implies that <span class="math inline">\(\mb{A}\Xmat = \mb{I}_n\)</span>. Rewrite the competitor as <span class="math inline">\(\widetilde{\bfbeta} = \bhat + \mb{BY}\)</span> where, <span class="math display">\[
\mb{B} = \mb{A} - \left(\Xmat'\Xmat\right)^{-1}\Xmat'.
\]</span> and note that <span class="math inline">\(\mb{A}\Xmat = \mb{I}_n\)</span> implies that <span class="math inline">\(\mb{B}\Xmat = 0\)</span>. We now have <span class="math display">\[
\begin{aligned}
  \widetilde{\bfbeta} &amp;= \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{Y} \\
                      &amp;= \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\Xmat\bfbeta + \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{e} \\
                      &amp;= \bfbeta + \mb{B}\Xmat\bfbeta + \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{e} \\
  &amp;= \bfbeta + \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{e}
\end{aligned}
\]</span> The variance of the competitor is, thus, <span class="math display">\[
\begin{aligned}
  \V[\widetilde{\bfbeta} \mid \Xmat]
  &amp;= \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\V[\mb{e}\mid \Xmat]\left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)' \\
  &amp;= \sigma^{2}\left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\left( \Xmat\left(\Xmat'\Xmat\right)^{-1} + \mb{B}'\right) \\
  &amp;= \sigma^{2}\left(\left(\Xmat'\Xmat\right)^{-1}\Xmat'\Xmat\left(\Xmat'\Xmat\right)^{-1} + \left(\Xmat'\Xmat\right)^{-1}\Xmat'\mb{B}' + \mb{B}\Xmat\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\\
  &amp;= \sigma^{2}\left(\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\\
  &amp;\geq \sigma^{2}\left(\Xmat'\Xmat\right)^{-1} \\
  &amp;= \V[\bhat \mid \Xmat]
\end{aligned}
\]</span> The first equality comes from the properties of covariance matrices, the second is due to the homoskedasticity assumption, and the fourth is due to <span class="math inline">\(\mb{B}\Xmat = 0\)</span>, which implies that <span class="math inline">\(\Xmat'\mb{B}' = 0\)</span> as well. The fifth inequality holds because matrix products of the form <span class="math inline">\(\mb{BB}'\)</span> are positive definite if <span class="math inline">\(\mb{B}\)</span> is of full rank (which we have assumed it is).</p>
</div>
<p>In this proof, we saw that the variance of the competing estimator had variance <span class="math inline">\(\sigma^2\left(\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\)</span> which we argued was “greater than 0” in the matrix sense, which is also called positive definite. What does this mean practically? Remember that any positive definite matrix must have strictly positive diagonal entries and that the diagonal entries of <span class="math inline">\(\V[\bhat \mid \Xmat]\)</span> and <span class="math inline">\(V[\widetilde{\bfbeta}\mid \Xmat]\)</span> are the variances of the individual parameters, <span class="math inline">\(\V[\widehat{\beta}_{j} \mid \Xmat]\)</span> and <span class="math inline">\(\V[\widetilde{\beta}_{j} \mid \Xmat]\)</span>. Thus, the variances of the individual parameters will be larger for <span class="math inline">\(\widetilde{\bfbeta}\)</span> than for <span class="math inline">\(\bhat\)</span>.</p>
<p>Many textbooks cite the Gauss-Markov theorem as a critical advantage of OLS over other methods, but recognizing its limitations is essential. It requires linearity and homoskedastic error assumptions, and these can be false in many applications.</p>
<p>Finally, note that while we have shown this result for linear estimators, <span class="citation" data-cites="Hansen22">Hansen (<a href="references.html#ref-Hansen22" role="doc-biblioref">2022</a>)</span> proves a more general version of this result that applies to any unbiased estimator.</p>
</section>
</section>
<section id="the-normal-linear-model" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="the-normal-linear-model"><span class="header-section-number">7.5</span> The normal linear model</h2>
<p>Finally, we add the strongest and thus least loved of the classical linear regression assumption: (conditional) normality of the errors. Historically the reason to use this assumption was that finite-sample inference hits a roadblock without some knowledge of the sampling distribution of <span class="math inline">\(\bhat\)</span>. Under the linear CEF model, we saw that <span class="math inline">\(\bhat\)</span> is unbiased, and under homoskedasticity, we could produce an unbiased estimator of the conditional variance. But for hypothesis testing or for generating confidence intervals, we need to make probability statements about the estimator, and, for that, we need to know its exact distribution. When the sample size is large, we can rely on the CLT and know <span class="math inline">\(\bhat\)</span> is approximately normal. But how do we proceed in small samples? Historically we would have assumed (conditional) normality of the errors, basically proceeding with some knowledge that we were wrong but hopefully not too wrong.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
The normal linear regression model
</div>
</div>
<div class="callout-body-container callout-body">
<p>In addition to the linear CEF assumption, we assume that <span class="math display">\[
e_i \mid \Xmat \sim \N(0, \sigma^2).
\]</span></p>
</div>
</div>
<p>There are a couple of important points:</p>
<ul>
<li>The assumption here is not that <span class="math inline">\((Y_{i}, \X_{i})\)</span> are jointly normal (though this would be sufficient for the assumption to hold), but rather that <span class="math inline">\(Y_i\)</span> is normally distributed conditional on <span class="math inline">\(\X_i\)</span>.</li>
<li>Notice that the normal regression model has the homoskedasticity assumption baked in.</li>
</ul>
<div id="thm-normal-ols" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.6</strong></span> Under the normal linear regression model, we have <span class="math display">\[
\begin{aligned}
  \bhat \mid \Xmat &amp;\sim \N\left(\bfbeta, \sigma^{2}\left(\Xmat'\Xmat\right)^{-1}\right) \\
  \frac{\widehat{\beta}_{j} - \beta_{j}}{[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}]_{jj}/\sqrt{n}} &amp;\sim t_{n-k-1} \\
  W/q &amp;\sim F_{q, n-k-1}.
\end{aligned}
\]</span></p>
</div>
<p>This theorem says that in the normal linear regression model, the coefficients follow a normal distribution, the t-statistics follow a <span class="math inline">\(t\)</span>-distribution, and a transformation of the Wald statistic follows an <span class="math inline">\(F\)</span> distribution. These are <strong>exact</strong> results and do not rely on large-sample approximations. Under the assumption of conditional normality of the errors, the results are as valid for <span class="math inline">\(n = 5\)</span> as for <span class="math inline">\(n = 500,000\)</span>.</p>
<p>Few people believe errors follow a normal distribution, so why even present these results? Unfortunately, most statistical software implementations of OLS implicitly assume this when calculating p-values for tests or constructing confidence intervals. In R, for example, the p-value associated with the <span class="math inline">\(t\)</span>-statistic reported by <code>lm()</code> relies on the <span class="math inline">\(t_{n-k-1}\)</span> distribution, and the critical values used to construct confidence intervals with <code>confint()</code> use that distribution as well. When normality does not hold, there is no principled reason to use the <span class="math inline">\(t\)</span> or the <span class="math inline">\(F\)</span> distributions in this way. But we might hold our nose and use this <em>ad hoc</em> procedure under two rationalizations:</p>
<ul>
<li><span class="math inline">\(\bhat\)</span> is asymptotically normal. This approximation might, however, be poor in smaller finite samples. The <span class="math inline">\(t\)</span> distribution will make inference more conservative in these cases (wider confidence intervals, smaller test rejection regions), which might help offset its poor approximation of the normal distribution in small samples.</li>
<li>As <span class="math inline">\(n\to\infty\)</span>, the <span class="math inline">\(t_{n-k-1}\)</span> will converge to a standard normal distribution, so the <em>ad hoc</em> adjustment will not matter much for medium to large samples.</li>
</ul>
<p>These arguments are not very convincing since whether the <span class="math inline">\(t\)</span> approximation will be any better than the normal in finite samples is unclear. But it may be the best we can do while we go and find more data.</p>
</section>
<section id="summary" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">7.6</span> Summary</h2>
<p>In this chapter, we discussed the large-sample properties of OLS, which are quite strong. Under mild conditions, OLS is consistent for the population linear regression coefficients and is asymptotically normal. The variance of the OLS estimator, and thus the variance estimator, depends on whether the projection errors are assumed to be unrelated to the covariates (<strong>homoskedastic</strong>) or possibly related (<strong>heteroskedastic</strong>). Confidence intervals and hypothesis tests for individual OLS coefficients are largely the same as discussed in Part I of this book, and we can obtain finite-sample properties of OLS such as conditional unbiasedness if we assume the conditional expectation function is linear. If we further assume the errors are normally distributed, we can derive confidence intervals and hypothesis tests that are valid for all sample sizes.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Hansen22" class="csl-entry" role="listitem">
Hansen, Bruce E. 2022. <span>“A <span>Modern Gauss</span>–<span>Markov Theorem</span>.”</span> <em>Econometrica</em> 90 (3): 1283–94. <a href="https://doi.org/10.3982/ECTA19255">https://doi.org/10.3982/ECTA19255</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>The form of the Wald statistic is that of a weighted inner product, <span class="math inline">\(\mb{x}'\mb{Ay}\)</span>, where <span class="math inline">\(\mb{A}\)</span> is a symmetric positive-definite weighting matrix.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>We are basically ignoring some edge cases when it comes to discrete covariates here. In particular, we assume that <span class="math inline">\(\Xmat'\Xmat\)</span> is nonsingular with probability one. However, this assumption can fail if we have a binary covariate since there is some chance (however slight) that the entire column will be all ones or all zeros, which would lead to a singular matrix <span class="math inline">\(\Xmat'\Xmat\)</span>. Practically this is not a big deal, but it does mean that we have to ignore this issue theoretically or focus on conditional unbiasedness.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    
</body>
</html>