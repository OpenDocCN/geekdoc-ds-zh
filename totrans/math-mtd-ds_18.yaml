- en: '3.1\. Motivating example: analyzing customer satisfaction#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap03_opt/01_motiv/roch-mmids-opt-motiv.html](https://mmids-textbook.github.io/chap03_opt/01_motiv/roch-mmids-opt-motiv.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Figure:** Helpful map of ML by scitkit-learn ([Source](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '![ml-cheat-sheet](../Images/19ac9e49b2f297976e40fee63e1c4ba0.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: We now turn to classification\(\idx{classification}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/Statistical_classification):'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning and statistics, classification is the problem of identifying
    to which of a set of categories (sub-populations) a new observation belongs, on
    the basis of a training set of data containing observations (or instances) whose
    category membership is known. Examples are assigning a given email to the “spam”
    or “non-spam” class, and assigning a diagnosis to a given patient based on observed
    characteristics of the patient (sex, blood pressure, presence or absence of certain
    symptoms, etc.). Classification is an example of pattern recognition. In the terminology
    of machine learning, classification is considered an instance of supervised learning,
    i.e., learning where a training set of correctly identified observations is available.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We will illustrate this problem on an [airline customer satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)
    dataset available on [Kaggle](https://www.kaggle.com), an excellent source of
    data and community contributed analyses. The background is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of the details of customers who have already flown with
    them. The feedback of the customers on various context and their flight data has
    been consolidated. The main purpose of this dataset is to predict whether a future
    customer would be satisfied with their service given the details of the other
    parameters values.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We first load the data and convert it to an appropriate matrix representation.
    We (or, more precisely, ChatGPT) pre-processed the original file to remove rows
    with missing data or 0 ratings, convert categorical variables into one-hot encodings,
    and keep only a subset of the rows and columns. You can see the details of the
    pre-processing in this [chat history](https://chatgpt.com/share/c5070b9c-f33f-4a37-a793-fde0d7cb7b06).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is a large dataset. Here are the first five rows and first 6 colums.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It has 100,000 rows and 24 columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The column names are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first column indicates whether a customer was satisfied (with `1` meaning
    satisfied). The next 6 columns give some information about the customers, e.g.,
    their age or whether they are members of a loyalty program with the airline. The
    following three columns give information about the flight, with names that should
    be self-explanatory: `Flight Distance`, `Departure Delay in Minutes`, and `Arrival
    Delay in Minutes`. The remaining columns give the customers’ ratings, between
    `1` and `5`, of various feature, e.g., `Baggage handling`, `Checkin service`.'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal will be to predict the first column, `Satisfied`, from the rest of
    the columns. For this, we transform our data into NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Some features may affect satisfication more than others. Let us look at age
    for instance. The following code extracts the `Age` column from `X` (i.e., column
    \(0\)) and computes the fraction of satisfied customers in several age bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Explanation by ChatGPT (who wrote the code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[`numpy.digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html)
    bins the age data into the specified age bins. The `-1` adjustment is to match
    zero-based indexing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[`numpy.bincount`](https://numpy.org/doc/stable/reference/generated/numpy.bincount.html)
    counts the occurrences of each bin index. The `minlength` parameter ensures that
    the resulting array length matches the number of age bins (`age_labels`). This
    is important if some bins have zero counts, ensuring the counts array covers all
    bins.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`freq_satisfied = counts_satisfied / counts_all` calculates the satisfaction
    frequency for each age group by dividing the counts of satisfied customers by
    the total counts in each age group.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The results are plotted using matplotlib’s [`matplotlib.pyplot.bar`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html)
    function. We see in particular that younger people tend to be more dissatisfied.
    Of course, this might be because they cannot afford the most expensive services.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/67ecfc3b63a5bad3ac48daf7707938436d621202289556c52f6b1fd57d71f323.png](../Images/d5cb3f8cc065e1efab42d21f968d71b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The input data is now of the form \(\{(\mathbf{x}_i, y_i) : i=1,\ldots, n\}\)
    where \(\mathbf{x}_i \in \mathbb{R}^d\) are the features and \(y_i \in \{0,1\}\)
    is the label. Above we use the matrix representation \(X \in \mathbb{R}^{n \times
    d}\) with rows \(\mathbf{x}_i^T\), \(i = 1,\ldots, n\) and \(\mathbf{y} = (y_1,
    \ldots, y_n) \in \{0,1\}^n\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal:'
  prefs: []
  type: TYPE_NORMAL
- en: 'learn a classifier from the examples \(\{(\mathbf{x}_i, y_i) : i=1,\ldots,
    n\}\), that is, a function \(\hat{f} : \mathbb{R}^d \to \mathbb{R}\) such that
    \(\hat{f}(\mathbf{x}_i) \approx y_i\).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We may want to enforce that the output is in \(\{0,1\}\) as well. This problem
    is referred to as [binary classification](https://en.wikipedia.org/wiki/Binary_classification).
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural approach to this type of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\(\idx{supervised
    learning}\xdi\) problem is to define two objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Family of classifiers:** A class \(\widehat{\mathcal{F}}\) of classifiers
    from which to pick \(\hat{f}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loss function:** A loss function \(\ell(\hat{f}, (\mathbf{x},y))\) which
    quantifies how good of a fit \(\hat{f}(\mathbf{x})\) is to \(y\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our goal is then to solve
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\hat{f} \in \widehat{\mathcal{F}}} \frac{1}{n} \sum_{i=1}^n \ell(\hat{f},
    (\mathbf{x}_i, y_i)), \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, we seek to find a classifier among \(\widehat{\mathcal{F}}\) that minimizes
    the average loss over the examples.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression),
    we consider linear classifiers of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{f}(\mathbf{x}) = \sigma(\mathbf{x}^T \boldsymbol{\theta}) \qquad \text{with}
    \qquad \sigma(t) = \frac{1}{1 + e^{-t}} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\boldsymbol{\theta} \in \mathbb{R}^d\) is a parameter vector. And we
    use the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\hat{f}, (\mathbf{x}, y)) = - y \log(\sigma(\mathbf{x}^T \boldsymbol{\theta}))
    - (1-y) \log(1- \sigma(\mathbf{x}^T \boldsymbol{\theta})). \]
  prefs: []
  type: TYPE_NORMAL
- en: In parametric form, the problem boils down to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\boldsymbol{\theta} \in \mathbb{R}^d} - \frac{1}{n} \sum_{i=1}^n y_i
    \log(\sigma(\mathbf{x}_i^T \boldsymbol{\theta})) - \frac{1}{n} \sum_{i=1}^n (1-y_i)
    \log(1- \sigma(\mathbf{x}_i^T \boldsymbol{\theta})). \]
  prefs: []
  type: TYPE_NORMAL
- en: To obtain a prediction in \(\{0,1\}\) here, we could cutoff \(\hat{f}(\mathbf{x})\)
    at a threshold \(\tau \in [0,1]\), that is, return \(\mathbf{1}\{\hat{f}(\mathbf{x})
    > \tau\}\).
  prefs: []
  type: TYPE_NORMAL
- en: We will explain in a later chapter where this choice comes from.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this chapter is to develop some of the mathematical theory and
    algorithms needed to solve this type of optimization formulation.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to help you explore the following
    hypothesis about this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Younger people tend to be more dissatisfied because they cannot afford the best
    services.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, consider the relationship between age, satisfaction, and class
    (e.g., economy, business, etc.). Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the distribution of class types among different age groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the satisfaction levels within each class type for different age groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)
  prefs: []
  type: TYPE_NORMAL
