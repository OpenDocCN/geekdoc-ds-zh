["```cpp\ncudaError_t cudaSetDevice(int device_num);\n```", "```cpp\ncudaError_t cudaGetDeviceCount(int ∗ device_count);\n```", "```cpp\nCUDA_CALL(cudaGetDeviceCount(&num_devices));\n```", "```cpp\ncudaError_t cudaGetDeviceProperties(struct cudaDeviceProp ∗ properties, int device);\n```", "```cpp\nstruct cudaDeviceProp device_0_prop;\nCUDA_CALL(cudaGetDeviceProperties(&device_0_prop, 0));\n```", "```cpp\nstruct cudaDeviceProp device_prop;\nint chosen_device;\n```", "```cpp\nmemset(device_prop, 0, sizeof(cudaDeviceProp));\ndevice_prop.major = 2;\ndevice_prop.minor = 0;\n```", "```cpp\nif (cudaChooseDevice(&chosen_device, device_prop) != cudaErrorInvalidValue)\n{\nCUDA_CALL(cudaSetDevice(chosen_device));\n}\n```", "```cpp\nvoid fill_array(u32 ∗ data, const u32 num_elements)\n{\n for (u32 i=0; i< num_elements; i++)\n {\n  data[i] = i;\n }\n}\n```", "```cpp\nvoid check_array(char ∗ device_prefix,\n                 u32 ∗ data,\n                 const u32 num_elements)\n{\n bool error_found = false;\n```", "```cpp\n for (u32 i=0; i< num_elements; i++)\n {\n  if (data[i] != (i∗2))\n  {\n   printf(\"%sError: %u %u\",\n          device_prefix,\n          i,\n          data[i]);\n```", "```cpp\n   error_found = true;\n  }\n }\n```", "```cpp\n if (error_found == false)\n  printf(\"%sArray check passed\", device_prefix);\n}\n```", "```cpp\n__global__ void gpu_test_kernel(u32 ∗ data)\n{\n const int tid = (blockIdx.x ∗ blockDim.x)\n     + threadIdx.x;\n data[tid] ∗= 2;\n```", "```cpp\n// Define maximum number of supported devices\n#define MAX_NUM_DEVICES (4)\n```", "```cpp\n// Define the number of elements to use in the array\n#define NUM_ELEM (1024∗1024∗8)\n```", "```cpp\n// Define one stream per GPU\ncudaStream_t stream[MAX_NUM_DEVICES];\n```", "```cpp\n// Define a string to prefix output messages with so\n// we know which GPU generated it\nchar device_prefix[MAX_NUM_DEVICES][300];\n```", "```cpp\n// Define one working array per device, on the device\nu32 ∗ gpu_data[MAX_NUM_DEVICES];\n```", "```cpp\n// Define CPU source and destination arrays, one per GPU\nu32 ∗ cpu_src_data[MAX_NUM_DEVICES];\nu32 ∗ cpu_dest_data[MAX_NUM_DEVICES];\n```", "```cpp\n// Host program to be called from main\n__host__ void gpu_kernel(void)\n{\n // No dynamic allocation of shared memory required\n const int shared_memory_usage = 0;\n```", "```cpp\n // Define the size in bytes of a single GPU’s worth\n // of data\n const size_t single_gpu_chunk_size = (sizeof(u32) ∗\n           NUM_ELEM);\n```", "```cpp\n // Define the number of threads and blocks to launch\n const int num_threads = 256;\n const int num_blocks = ((NUM_ELEM + (num_threads-1))\n       / num_threads);\n```", "```cpp\n // Identify how many devices and clip to the maximum\n // defined\n int num_devices;\n```", "```cpp\n if (num_devices > MAX_NUM_DEVICES)\n  num_devices = MAX_NUM_DEVICES;\n```", "```cpp\n // Run one memcpy and kernel on each device\n for (int device_num=0;\n      device_num < num_devices;\n      device_num++)\n {\n  // Select the correct device\n  CUDA_CALL(cudaSetDevice(device_num));\n```", "```cpp\n  // Generate a prefix for all screen messages\n  struct cudaDeviceProp device_prop;\n  CUDA_CALL(cudaGetDeviceProperties(&device_prop,\n                                    device_num));\n  sprintf(&device_prefix[device_num][0], \"\\nID:%d %s:\", device_num, device_prop.name);\n```", "```cpp\n  // Create a new stream on that device\n  CUDA_CALL(cudaStreamCreate(&stream[device_num]));\n```", "```cpp\n  // Allocate memory on the GPU\n  CUDA_CALL(cudaMalloc((void∗∗)&gpu_data[device_num],\n                        single_gpu_chunk_size));\n```", "```cpp\n  // Allocate page locked memory on the CPU\n  CUDA_CALL(cudaMallocHost((void ∗∗)\n                            &cpu_src_data[device_num],\n                            single_gpu_chunk_size));\n```", "```cpp\n  CUDA_CALL(cudaMallocHost((void ∗∗)\n                            &cpu_dest_data[device_num],\n                            single_gpu_chunk_size));\n```", "```cpp\n  // Fill it with a known pattern\n  fill_array(cpu_src_data[device_num], NUM_ELEM);\n```", "```cpp\n  // Copy a chunk of data from the CPU to the GPU\n```", "```cpp\n  CUDA_CALL(cudaMemcpyAsync(gpu_data[device_num],\n            cpu_src_data[device_num],\n            single_gpu_chunk_size,\n            cudaMemcpyHostToDevice,\n            stream[device_num]));\n```", "```cpp\n  // Invoke the GPU kernel using the newly created\n  // stream - asynchronous invokation\n  gpu_test_kernel<<<num_blocks,\n                    num_threads,\n                    shared_memory_usage,\n    stream[device_num]>>>(gpu_data[device_num]);\n```", "```cpp\n  cuda_error_check(device_prefix[device_num],\n                   \"Failed to invoke gpu_test_kernel\");\n```", "```cpp\n  // Now push memory copies to the host into\n  // the streams\n  // Copy a chunk of data from the GPU to the CPU\n  // asynchronous\n  CUDA_CALL(cudaMemcpyAsync(cpu_dest_data[device_num],\n                            gpu_data[device_num],\n                            single_gpu_chunk_size,\n                            cudaMemcpyDeviceToHost,\n                            stream[device_num]));\n }\n```", "```cpp\n // Process the data as it comes back from the GPUs\n // Overlaps CPU execution with GPU execution\n for (int device_num=0;\n   device_num < num_devices;\n   device_num++)\n {\n  // Select the correct device\n```", "```cpp\n  // Wait for all commands in the stream to complete\n  CUDA_CALL(cudaStreamSynchronize(stream[device_num]));\n```", "```cpp\n  // GPU data and stream are now used, so\n  // clear them up\n  CUDA_CALL(cudaStreamDestroy(stream[device_num]));\n  CUDA_CALL(cudaFree(gpu_data[device_num]));\n```", "```cpp\n  // Data has now arrived in\n  // cpu_dest_data[device_num]\n  check_array( device_prefix[device_num],\n               cpu_dest_data[device_num],\n               NUM_ELEM);\n```", "```cpp\n  // Clean up CPU allocations\n  CUDA_CALL(cudaFreeHost(cpu_src_data[device_num]));\n  CUDA_CALL(cudaFreeHost(cpu_dest_data[device_num]));\n```", "```cpp\n  // Release the device context\n  CUDA_CALL(cudaDeviceReset());\n }\n}\n```", "```cpp\n// Define a start and stop event per stream\ncudaEvent_t kernel_start_event[MAX_NUM_DEVICES];\ncudaEvent_t memcpy_to_start_event[MAX_NUM_DEVICES];\ncudaEvent_t memcpy_from_start_event[MAX_NUM_DEVICES];\ncudaEvent_t memcpy_from_stop_event[MAX_NUM_DEVICES];\n```", "```cpp\n// Push the start event into the stream\nCUDA_CALL(cudaEventRecord(memcpy_to_start_event[device_num], stream[device_num]));\n```", "```cpp\n// Wait for all commands in the stream to complete\nCUDA_CALL(cudaStreamSynchronize(stream[device_num]));\n```", "```cpp\n// Get the elapsed time between the copy\n// and kernel start\nCUDA_CALL(cudaEventElapsedTime(&time_copy_to_ms,\n memcpy_to_start_event[device_num],\n kernel_start_event[device_num]));\n```", "```cpp\n// Get the elapsed time between the kernel start\n// and copy back start\nCUDA_CALL(cudaEventElapsedTime(&time_kernel_ms,\n kernel_start_event[device_num],\n memcpy_from_start_event[device_num]));\n```", "```cpp\n// Get the elapsed time between the copy back start\n// and copy back start\nCUDA_CALL(cudaEventElapsedTime(&time_copy_from_ms,\n memcpy_from_start_event[device_num],\n memcpy_from_stop_event[device_num]));\n```", "```cpp\n// Get the elapsed time between the overall start\n// and stop events\nCUDA_CALL(cudaEventElapsedTime(&time_exec_ms,\n memcpy_to_start_event[device_num],\n memcpy_from_stop_event[device_num]));\n```", "```cpp\n// Print the elapsed time\nconst float gpu_time = (time_copy_to_ms + time_kernel_ms + time_copy_from_ms);\n```", "```cpp\nprintf(\"%sCopy To  : %.2f ms\",\n       device_prefix[device_num], time_copy_to_ms);\n```", "```cpp\nprintf(\"%sKernel   : %.2f ms\",\n       device_prefix[device_num], time_kernel_ms);\n```", "```cpp\nprintf(\"%sCopy Back  : %.2f ms\",\n       device_prefix[device_num], time_copy_from_ms);\n```", "```cpp\nprintf(\"%sComponent Time : %.2f ms\",\n       device_prefix[device_num], gpu_time);\n```", "```cpp\nprintf(\"%sExecution Time : %.2f ms\",\n       device_prefix[device_num], time_exec_ms);\n```", "```cpp\n__global__ void gpu_test_kernel(u32 ∗ data, const u32 iter)\n{\n const int tid = (blockIdx.x ∗ blockDim.x)\n     + threadIdx.x;\n```", "```cpp\n for (u32 i=0; i<iter; i++)\n {\n  data[tid] ∗= 2;\n  data[tid] /= 2;\n }\n}\n```", "```cpp\nID:0 GeForce GTX 470:Copy To        :   20.22 ms\nID:0 GeForce GTX 470:Kernel         : 4883.55 ms\nID:0 GeForce GTX 470:Copy Back      :   10.01 ms\nID:0 GeForce GTX 470:Component Time : 4913.78 ms\nID:0 GeForce GTX 470:Execution Time : 4913.78 ms\nID:0 GeForce GTX 470:Array check passed\n```", "```cpp\nID:1 GeForce 9800 GT:Copy To        :    20.77 ms\nID:1 GeForce 9800 GT:Kernel         : 25279.57 ms\nID:1 GeForce 9800 GT:Copy Back      :    10.02 ms\nID:1 GeForce 9800 GT:Component Time : 25310.37 ms\nID:1 GeForce 9800 GT:Execution Time : 25310.37 ms\nID:1 GeForce 9800 GT:Array check passed\n```", "```cpp\nID:2 GeForce GTX 260:Copy To        :    20.88 ms\nID:2 GeForce GTX 260:Kernel         : 14268.92 ms\nID:2 GeForce GTX 260:Copy Back      :    10.00 ms\nID:2 GeForce GTX 260:Component Time : 14299.80 ms\nID:2 GeForce GTX 260:Execution Time : 14299.80 ms\nID:2 GeForce GTX 260:Array check passed\n```", "```cpp\nID:3 GeForce GTX 460:Copy To        :   20.11 ms\nID:3 GeForce GTX 460:Kernel         : 6652.78 ms\nID:3 GeForce GTX 460:Copy Back      :    9.94 ms\nID:3 GeForce GTX 460:Component Time : 6682.83 ms\nID:3 GeForce GTX 460:Execution Time : 6682.83 ms\nID:3 GeForce GTX 460:Array check passed\n```", "```cpp\ncudaError_t cudaEventQuery (cudaEvent_t event);\n```", "```cpp\n// Give back control to CPU thread\nCUDA_CALL(cudaSetDeviceFlags(cudaDeviceScheduleYield));\n```", "```cpp\n// Give up control of CPU threads for some milliseconds\nvoid snooze(const unsigned int ms)\n{\n#ifdef _WIN32\n Sleep(ms);\n#else\n if ((ms/1000) <= 0)\n  sleep(1);\n else\n  sleep(ms/1000);\n#endif\n}\n```", "```cpp\nprintf(\"\\nWaiting\");\n```", "```cpp\nu32 results_to_process = num_devices;\nu32 sleep_count = 0;\n```", "```cpp\n// While there are results still to process\nwhile(results_to_process != 0)\n{\n // Process the data as it comes back from the GPUs\n // Overlaps CPU execution with GPU execution\n for (int device_num=0;\n      device_num < num_devices;\n      device_num++)\n {\n  // If results are pending from this device\n  if (processed_result[device_num] == false)\n  {\n   // Try to process the data from the device\n   processed_result[device_num] =\n        process_result(device_num);\n```", "```cpp\n   // If we were able to process the data\n   if (processed_result[device_num] == true)\n   {\n    // Then decrease the number of pending\n    // results\n    results_to_process--;\n```", "```cpp\n    // print the time host waited\n    printf(\"%sHost wait time : %u ms\\n\",\n           device_prefix[device_num],\n           sleep_count ∗ 100);\n```", "```cpp\n    // If there are still more to process\n    // print start of progress indicator\n    if (results_to_process != 0)\n     printf(\"\\nWaiting\");\n```", "```cpp\n    fflush(stdout);\n   }\n   else\n   {\n    printf(\".\");\n    fflush(stdout);\n```", "```cpp\n  }\n```", "```cpp\n  // Try again in 100ms\n  sleep_count++;\n  snooze(100);\n }\n}\n```", "```cpp\nfor (int device_num=0;\n     device_num < num_devices;\n     device_num++)\n{\n cleanup(device_num);\n}\n```", "```cpp\nWaiting……………………………………………\nID:0 GeForce GTX 470:Copy To  : 20.84 ms\nID:0 GeForce GTX 470:Kernel         : 4883.16 ms\nID:0 GeForce GTX 470:Copy Back      :   10.24 ms\nID:0 GeForce GTX 470:Component Time : 4914.24 ms\nID:0 GeForce GTX 470:Execution Time : 4914.24 ms\nID:0 GeForce GTX 470:Array check passed\nID:0 GeForce GTX 470:Host wait time : 5200 ms\n```", "```cpp\nWaiting…………\nID:3 GeForce GTX 460:Copy To        :   20.58 ms\nID:3 GeForce GTX 460:Kernel         : 6937.48 ms\nID:3 GeForce GTX 460:Copy Back      :   10.21 ms\nID:3 GeForce GTX 460:Component Time : 6968.27 ms\nID:3 GeForce GTX 460:Execution Time : 6968.27 ms\nID:3 GeForce GTX 460:Array check passed\nID:3 GeForce GTX 460:Host wait time : 7100 ms\n```", "```cpp\nWaiting………………………………\nID:2 GeForce GTX 260:Copy To        :    21.43 ms\nID:2 GeForce GTX 260:Kernel         : 14269.09 ms\nID:2 GeForce GTX 260:Copy Back      :    10.03 ms\nID:2 GeForce GTX 260:Component Time : 14300.55 ms\nID:2 GeForce GTX 260:Execution Time : 14300.55 ms\nID:2 GeForce GTX 260:Array check passed\nID:2 GeForce GTX 260:Host wait time : 14600 ms\n```", "```cpp\nWaiting……………………\n```", "```cpp\nID:1 GeForce 9800 GT:Kernel         : 25275.88 ms\nID:1 GeForce 9800 GT:Copy Back      :    11.01 ms\nID:1 GeForce 9800 GT:Component Time : 25308.08 ms\nID:1 GeForce 9800 GT:Execution Time : 25308.08 ms\nID:1 GeForce 9800 GT:Array check passed\nID:1 GeForce 9800 GT:Host wait time : 25300 ms\n```", "```cpp\n__host__ void get_and_push_work(const int num_devices,\n                                const size_t single_gpu_chunk_size,\n                                const u32 new_work_blocks)\n{\n // Work out the total number to process\n // Number already scheduled plus new work\n u32 results_to_process = num_devices +\n                          new_work_blocks;\n```", "```cpp\n // Keep track of the number of calculations in flow\n u32 results_being_calculated = num_devices;\n```", "```cpp\n // Keep track of how long the CPU needs to sleep\n u32 sleep_count = 0;\n```", "```cpp\n // While there are results still to process\n while(results_to_process != 0)\n {\n  // Process the data as it comes back from the GPUs\n  // Overlaps CPU execution with GPU execution\n  for (int device_num=0;\n       device_num < num_devices;\n       device_num++)\n  {\n   // Assume will process nothing\n   bool processed_a_result = false;\n```", "```cpp\n   // If results are pending from this device\n   if (processed_result[device_num] == false)\n   {\n    // Try to process the data from the device\n    processed_result[device_num] =\n        process_result(device_num);\n```", "```cpp\n    // If we were able to process the data\n    if (processed_result[device_num] == true)\n    {\n     // Then decrease the number of pending\n     // results\n     results_to_process--;\n```", "```cpp\n     // Increment the number this device\n     // processed\n     num_processed[device_num]++;\n```", "```cpp\n     // Decreate the number in flow\n     results_being_calculated--;\n```", "```cpp\n     // Note we processed at least\n     // one result\n     processed_a_result = true;\n```", "```cpp\n     // print the time host waited\n     printf(\"%sHost wait time : %u ms\\n\",\n            device_prefix[device_num],\n            sleep_count ∗ 100);\n```", "```cpp\n     // If there are still more blocks\n     // to process\n     if (results_to_process >\n         results_being_calculated)\n     {\n      // Give more work to the\n      // finished GPU\n      push_work_into_queue(device_num,\n                           single_gpu_chunk_size);\n```", "```cpp\n      // Set flag to say GPU has work\n      processed_result[device_num] =\n              false;\n```", "```cpp\n      // Increment the number of\n      // active tasks\n      results_being_calculated++;\n```", "```cpp\n      // Format output\n      printf(\"\\n\");\n     }\n     fflush(stdout);\n    }\n   }\n```", "```cpp\n   // If we processed no results then sleep\n   if (processed_a_result == false)\n   {\n    sleep_count++;\n    printf(\".\");\n    fflush(stdout);\n```", "```cpp\n    // Try again in 100ms\n    snooze(100);\n   }\n  }\n }\n}\n```", "```cpp\n// Define N streams per GPU\ncudaStream_t stream[MAX_NUM_DEVICES][MAX_NUM_STREAMS];\n```", "```cpp\n// Define the number of active streams per device\nconst u32 streams_per_device[MAX_NUM_DEVICES] =\n{\n 10, /∗ GTX470 ∗/\n 2, /∗ 9800 GT ∗/\n 4, /∗ GTX260 ∗/\n 8, /∗ 460 GTX ∗/\n};\n```", "```cpp\nClient:\n```", "```cpp\nzmq::context_t context(1);\nzmq::socket_t socket(context, ZMQ_REQ);\nsocket.connect(\"tcp://localhost:5555\");\n```", "```cpp\nServer:\n```", "```cpp\nzmq::context_t context(1);\nzmq::socket_t socket(context, ZMQ_REP);\nsocket.bind(\"tcp://∗:5555\");\n```", "```cpp\n// Host program to be called from main\n__host__ void gpu_kernel_client(const u32 pid)\n{\n printf(\"\\nRunning as Client\");\n```", "```cpp\n // Init Network\n zmq::context_t context(1);\n zmq::socket_t socket(context, ZMQ_REQ);\n socket.connect(\"tcp://localhost:5555\");\n```", "```cpp\n // GPU params\n size_t chunk_size;\n u32 active_streams;\n```", "```cpp\n // Setup all available devices\n setup_devices(&num_devices,\n               &active_streams,\n               &chunk_size);\n```", "```cpp\n u32 results_to_process;\n get_work_range_from_server(pid,\n                            &results_to_process,\n                            &socket);\n```", "```cpp\n // Generate CPU data for input data\n generate_cpu_data_range(0, results_to_process);\n```", "```cpp\n // Keep track of pending results\n u32 pending_results = results_to_process;\n```", "```cpp\n // While there is still work to be completed\n while (pending_results != 0)\n {\n  // Try to distribute work to each GPU\n  u32 work_distributed = distribute_work(num_devices,\n                                         chunk_size,\n                                         pending_results);\n```", "```cpp\n  // Collect work from GPU\n  u32 work_collected = collect_work(num_devices,\n                                    chunk_size);\n```", "```cpp\n  // Decrement remaining count\n  pending_results -= work_collected;\n```", "```cpp\n  // Post completed work units to server\n  if (work_collected > 0)\n  {\n   send_completed_units_to_server(pid,\n                                  chunk_size,\n                                  &socket);\n  }\n```", "```cpp\n  // If no work was distributed, or collected\n  // and we’ve not finished yet then sleep\n  if ( (work_distributed == 0) &&\n       (work_collected == 0) &&\n       (pending_results != 0) )\n  {\n   printf(\".\");\n```", "```cpp\n   snooze(100);\n  }\n }\n```", "```cpp\n // Print summary of how many each device processed\n for (u32 device_num=0u;\n      device_num < num_devices;\n      device_num++)\n {\n  printf(\"%s processed: %u\",\n         device_prefix[device_num],\n         num_processed[device_num]);\n }\n```", "```cpp\n printf(\"\\nTotal: src:%u dest:%u\",\n        unprocessed_idx, completed_idx);\n```", "```cpp\n cleanup_devices(num_devices);\n}\n```", "```cpp\n__host__ u32 distribute_work(const int num_devices,\n                             const size_t chunk_size,\n                             u32 pending_results)\n{\n u32 work_units_scheduled = 0;\n```", "```cpp\n // Cycle through each device\n for (u32 device_num = 0;\n      device_num < num_devices;\n      device_num++)\n {\n  u32 stream_num = 0;  \n  bool allocated_work = false;\n```", "```cpp\n  while ( (allocated_work == false) &&\n```", "```cpp\n  {\n   // If there is more work to schedule\n   if (pending_results > 0)\n   {\n    // If the device is available\n    if (processed_result[device_num][stream_num] == true)\n    {\n     // Allocate a job to the GPU\n     push_work_into_queue(device_num,\n                          chunk_size,\n                          stream_num);\n```", "```cpp\n     // Set flag to say GPU has work pending\nprocessed_result[device_num][stream_num] = false;\n```", "```cpp\n     // Keep track of how many new\n     // units were issued\n     work_units_scheduled++;\n```", "```cpp\n     // Move onto next device\n     allocated_work = true;\n```", "```cpp\n     pending_results--;\n    }\n   }\n   stream_num++;\n  }\n }\n```", "```cpp\n return work_units_scheduled;\n}\n```", "```cpp\n__host__ void push_work_into_queue(const u32 device_num,\n                                   const size_t chunk_size,\n                                   const u32 stream_num)\n{\n // No dynamic allocation of shared memory required\n const int shared_memory_usage = 0;\n```", "```cpp\n // Define the number of threads and blocks to launch\n const int num_threads = 256;\n const int num_blocks = ((NUM_ELEM + (num_threads-1))\n                         / num_threads);\n```", "```cpp\n memcpy(cpu_src_data[device_num][stream_num],\n        cpu_unprocessed_data[unprocessed_idx % MAX_IN_QUEUED_PACKETS],\n        chunk_size);\n```", "```cpp\n // Processed this packet\n unprocessed_idx++;\n```", "```cpp\n // Select the correct device\n CUDA_CALL(cudaSetDevice(device_num));\n```", "```cpp\n // Push the start event into the stream\n CUDA_CALL(cudaEventRecord(memcpy_to_start_event[device_num][stream_num], stream[device_num][stream_num]));\n```", "```cpp\n // Copy a chunk of data from the CPU to the GPU\n // asynchronous\n CUDA_CALL(cudaMemcpyAsync(gpu_data[device_num][stream_num],\ncpu_src_data[device_num][stream_num], chunk_size, cudaMemcpyHostToDevice, stream[device_num][stream_num]));\n```", "```cpp\n // Push the start event into the stream\nCUDA_CALL(cudaEventRecord(kernel_start_event[device_num][stream_num], stream[device_num][stream_num]));\n```", "```cpp\n // Invoke the GPU kernel using the newly created\n // stream - asynchronous invokation\n gpu_test_kernel<<<num_blocks,\n                   num_threads,\n                   shared_memory_usage,\n                   stream[device_num][stream_num]>>>\n                   (gpu_data[device_num][stream_num],\n                   kernel_iter);\n```", "```cpp\n cuda_error_check(device_prefix[device_num],\n                  \"Failed to invoke gpu_test_kernel\");\n```", "```cpp\n // Push the start event into the stream\n CUDA_CALL(cudaEventRecord(memcpy_from_start_event[device_num][stream_num], stream[device_num][stream_num]));\n```", "```cpp\n // Copy a chunk of data from the GPU to the CPU\n // asynchronous\n CUDA_CALL(cudaMemcpyAsync(cpu_dest_data[device_num][stream_num], gpu_data[device_num][stream_num], single_gpu_chunk_size, cudaMemcpyDeviceToHost, stream[device_num][stream_num]));\n```", "```cpp\nCUDA_CALL(cudaEventRecord(memcpy_from_stop_event[device_num][stream_num], stream[device_num][stream_num]));\n```", "```cpp\n}\n```", "```cpp\n__host__ u32 collect_work(const int num_devices,\n       const size_t chunk_size)\n{\n // Keep track of the number of results processed\n u32 results_processed = 0;\n```", "```cpp\n // Cycle through each device\n for (u32 device_num=0;\n   device_num < num_devices;\n   device_num++)\n {\n  // Then cycle through streams\n  for(u32 stream_num=0;\n      stream_num < streams_per_device[device_num];\n      stream_num++)\n  {\n   // If results are pending from this device\n   if (processed_result[device_num][stream_num] == false)\n   {\n    // Try to process the data from the device\n    processed_result[device_num][stream_num] = process_result(device_num, stream_num, chunk_size);\n```", "```cpp\n    // If we were able to process the data\n    if (processed_result[device_num][stream_num] == true)\n    {\n     // Increment the number this device\n     // processed\n     num_processed[device_num]++;\n```", "```cpp\n     // Increment this run’s count\n     results_processed++;\n    }\n   }\n  }\n```", "```cpp\n return results_processed;\n}\n```", "```cpp\n__host__ bool process_result(const u32 device_num,\n                             const u32 stream_num,\n                             const size_t chunk_size)\n{\n bool result;\n```", "```cpp\n bool stop_event_hit = (cudaEventQuery(memcpy_from_stop_event[device_num][stream_num]) == cudaSuccess);\n```", "```cpp\n // Space is avaiable if network_out_idx is not\n // more than the total queue length behind\n bool output_space_avail = ((completed_idx - network_out_idx) < MAX_OUT_QUEUED_PACKETS);\n```", "```cpp\n // If the stop event has been hit AND\n // we have room in the output queue\n if (stop_event_hit && output_space_avail)\n {\n  float time_copy_to_ms = 0.0F;\n  float time_copy_from_ms = 0.0F;\n  float time_kernel_ms = 0.0F;\n  float time_exec_ms = 0.0F;\n```", "```cpp\n  // Select the correct device\n  CUDA_CALL(cudaSetDevice(device_num));\n```", "```cpp\n  // Get the elapsed time between the copy\n  // and kernel start\n  CUDA_CALL(cudaEventElapsedTime(&time_copy_to_ms, memcpy_to_start_event[device_num][stream_num], kernel_start_event[device_num][stream_num]));\n```", "```cpp\n  // Get the elapsed time between the kernel start\n  // and copy back start\n  CUDA_CALL(cudaEventElapsedTime(&time_kernel_ms,\nkernel_start_event[device_num][stream_num],\nmemcpy_from_start_event[device_num][stream_num]));\n```", "```cpp\n  // Get the elapsed time between the copy back start\n  // and copy back start\n  CUDA_CALL(cudaEventElapsedTime(&time_copy_from_ms,\n```", "```cpp\nmemcpy_from_stop_event[device_num][stream_num]));\n```", "```cpp\n  // Get the elapsed time between the overall start\n  // and stop events\n  CUDA_CALL(cudaEventElapsedTime(&time_exec_ms,\nmemcpy_to_start_event[device_num][stream_num],\nmemcpy_from_stop_event[device_num][stream_num]));\n```", "```cpp\n  // Print the elapsed time\n  const float gpu_time = (time_copy_to_ms +\n                          time_kernel_ms +\n                          time_copy_from_ms);\n```", "```cpp\n  printf(\"%sCopy To  : %.2f ms\",\n         device_prefix[device_num], time_copy_to_ms);\n```", "```cpp\n  printf(\"%sKernel   : %.2f ms\",\n         device_prefix[device_num], time_kernel_ms);\n```", "```cpp\n  printf(\"%sCopy Back  : %.2f ms\",\n         device_prefix[device_num],\n         time_copy_from_ms);\n```", "```cpp\n  printf(\"%sComponent Time : %.2f ms\",\n         device_prefix[device_num], gpu_time);\n```", "```cpp\n  printf(\"%sExecution Time : %.2f ms\",\n         device_prefix[device_num], time_exec_ms);\n  fflush(stdout);\n```", "```cpp\n  // Data has now arrived in\n  // cpu_dest_data[device_num]\n  check_array( device_prefix[device_num],\n               cpu_dest_data[device_num][stream_num],\n               NUM_ELEM);\n```", "```cpp\n  // Copy results into completed work queue\n  memcpy(cpu_completed_data[completed_idx % MAX_OUT_QUEUED_PACKETS],\n    cpu_dest_data[device_num][stream_num],\n    chunk_size);\n```", "```cpp\n  printf(\"\\nProcessed work unit: %u\", completed_idx);\n  fflush(stdout);\n\n  // Incremenet the destination idx\n  // Single array per CPU\n```", "```cpp\n  result = true;\n }\n else\n {\n  result = false;\n }\n```", "```cpp\n return result;\n}\n```", "```cpp\n__host__ void send_completed_units_to_server(\n const u32 pid,\n const size_t chunk_size,\n zmq::socket_t ∗ socket)\n{\n for (u32 packet=network_out_idx;\n      packet < completed_idx;\n      packet++)\n {\n  // Define a client message\n  CLIENT_MSG_T client_msg;\n  client_msg.id.pid = pid;\n  client_msg.id.ip = 0;\n  client_msg.id.msg_type = 0;\n  client_msg.id.msg_num = packet;\n  memset(client_msg.data, 0, CLIENT_MSG_DATA_SIZE);\n```", "```cpp\n  SERVER_MSG_T server_msg;\n  memset(&server_msg, 0, sizeof(SERVER_MSG_T) );\n```", "```cpp\n  // Create object to send to server\n  zmq::message_t request(sizeof(CLIENT_MSG_T));\n  zmq::message_t reply;\n```", "```cpp\n  // Copy in the output data\n  memcpy(client_msg.data,\n         cpu_completed_data[packet % MAX_OUT_QUEUED_PACKETS],\n         chunk_size);\n```", "```cpp\n  memcpy( (void∗) request.data(), &client_msg, sizeof(CLIENT_MSG_T) );\n```", "```cpp\n  // Send to server\n  printf(\"\\nSending data %u to server\", packet);\n  socket->send(request);\n```", "```cpp\n  // Free output buffer\n  network_out_idx++;\n```", "```cpp\n  // Wait for a reply\n  socket->recv(&reply);\n```", "```cpp\n  // Decode the reply\n  memcpy( &server_msg, (void∗) reply.data(), sizeof(SERVER_MSG_T) );\n  printf(\"\\nReceived acknowledge from server\");\n }\n}\n```"]