- en: 3.3\. Optimality conditions#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap03_opt/03_optimality/roch-mmids-opt-optimality.html](https://mmids-textbook.github.io/chap03_opt/03_optimality/roch-mmids-opt-optimality.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this section, we derive optimality conditions for unconstrained continuous
    optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be interested in unconstrained optimization of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(f : \mathbb{R}^d \to \mathbb{R}\). In this subsection, we define several
    notions of solution and derive characterizations.'
  prefs: []
  type: TYPE_NORMAL
- en: We have observed before that, in general, finding a global minimizer and certifying
    that one has been found can be difficult unless some special structure is present.
    Therefore weaker notions of solution are needed. We previously introduced the
    concept of a local minimizer. In words, \(\mathbf{x}^*\) is a local minimizer
    if there is open ball around \(\mathbf{x}^*\) where it attains the minimum value.
    The difference between global and local minimizers is illustrated in the next
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. First-order conditions[#](#first-order-conditions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local minimizers can be characterized in terms of the gradient. We first define
    the concept of directional derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Directional derivative** Partial derivatives measure the rate of change of
    a function along the axes. More generally:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Directional Derivative)** \(\idx{directional derivative}\xdi\)
    Let \(f : D \to \mathbb{R}\) where \(D \subseteq \mathbb{R}^d\), let \(\mathbf{x}_0
    = (x_{0,1},\ldots,x_{0,d}) \in D\) be an interior point of \(D\) and let \(\mathbf{v}
    = (v_1,\ldots,v_d) \in \mathbb{R}^d\) be a nonzero vector. The directional derivative
    of \(f\) at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} &= \lim_{h
    \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{v}) - f(\mathbf{x}_0)}{h}\\ &= \lim_{h
    \to 0} \frac{f(x_{0,1} + h v_1,\ldots,x_{0,d} + h v_d) - f(x_{0,1},\ldots,x_{0,d})}{h}
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: provided the limit exists. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Typically, \(\mathbf{v}\) is a unit vector.
  prefs: []
  type: TYPE_NORMAL
- en: Note that taking \(\mathbf{v} = \mathbf{e}_i\) recovers the \(i\)-th partial
    derivative
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{e}_i} = \lim_{h \to 0}
    \frac{f(\mathbf{x}_0 + h \mathbf{e}_i) - f(\mathbf{x}_0)}{h} = \frac{\partial
    f (\mathbf{x}_0)}{\partial x_i}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, a general directional derivative can be expressed in terms of the
    partial derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Directional Derivative and Gradient)** \(\idx{directional derivative
    and gradient theorem}\xdi\) Let \(f : D \to \mathbb{R}\) where \(D \subseteq \mathbb{R}^d\),
    let \(\mathbf{x}_0 \in D\) be an interior point of \(D\) and let \(\mathbf{v}
    \in \mathbb{R}^d\) be a vector. Assume that \(f\) is continuously differentiable
    at \(\mathbf{x}_0\). Then the directional derivative of \(f\) at \(\mathbf{x}_0\)
    in the direction \(\mathbf{v}\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla f(\mathbf{x}_0)^T
    \mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, when \(\mathbf{v}\) is a unit vector, the directional derivative
    is the length of the orthogonal projection of the gradient onto \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* To bring out the partial derivatives, we re-write the directional
    derivative as the derivative of a composition of \(f\) with an affine function.
    We then use the *Chain Rule*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Consider the composition \(\beta(h) = f(\boldsymbol{\alpha}(h))\)
    where \(\boldsymbol{\alpha}(h) = \mathbf{x}_0 + h \mathbf{v}\). Observe that \(\boldsymbol{\alpha}(0)=
    \mathbf{x}_0\) and \(\beta(0)= f(\mathbf{x}_0)\). Then, by definition of the derivative,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\mathrm{d} \beta(0)}{\mathrm{d} h} = \lim_{h \to 0} \frac{\beta(h)
    - \beta(0)}{h} = \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{v}) - f(\mathbf{x}_0)}{h}
    = \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Applying the *Chain Rule* and the parametric line example from the previous
    section, we arrive at
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\mathrm{d} \beta(0)}{\mathrm{d} h} = \nabla f(\boldsymbol{\alpha}(0))^T
    \boldsymbol{\alpha}'(0) = \nabla f(\mathbf{x}_0)^T \mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '![Contour plot of the function . At the point , the gradient and a directional
    derivative are shown (with the help from ChatGPT; converted and adapted from (Source))](../Images/8e2172c7f2d739f2e83162ecdcafe29e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**KNOWLEDGE CHECK:** Let \(f : \mathbb{R}^2 \to \mathbb{R}\) be continuously
    differentiable. Suppose that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} = \frac{3}{\sqrt{5}}
    \qquad \text{and} \qquad \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{w}}
    = \frac{5}{\sqrt{5}} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{v} = (1,2)/\sqrt{5}\) and \(\mathbf{w} = (2,1)/\sqrt{5}\). Compute
    the gradient of \(f\) at \(\mathbf{x}_0\). \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Descent direction** Earlier in the book, we proved a key insight about the
    derivative of a single-variable function \(f\) at a point \(x_0\): it tells us
    where to find smaller values. We generalize the *Descent Direction Lemma* to the
    multivariable case.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we observe that in the continuously differentiable case the directional
    derivative gives a criterion for descent directions.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Descent Direction and Directional Derivative)** \(\idx{descent
    direction and directional derivative lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be continuously differentiable at \(\mathbf{x}_0\). A vector \(\mathbf{v}\) is
    a descent direction for \(f\) at \(\mathbf{x}_0\) if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla f(\mathbf{x}_0)^T
    \mathbf{v} < 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, if the directional derivative of \(f\) at \(\mathbf{x}_0\) in the direction
    \(\mathbf{v}\) is negative. \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* In anticipation of the proof of the second-order condition, we
    use the *Mean Value Theorem* to show that \(f\) takes smaller values in direction
    \(\mathbf{v}\). A simpler proof based on the definition of the directional derivative
    is also possible (Try it!).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Suppose there is \(\mathbf{v} \in \mathbb{R}^d\) such that \(\nabla
    f(\mathbf{x}_0)^T \mathbf{v} = -\eta < 0\). For \(\alpha > 0\), the *Mean Value
    Theorem* implies that there is \(\xi_\alpha \in (0,1)\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v})^T(\alpha \mathbf{v}) = f(\mathbf{x}_0) + \alpha
    \nabla f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})^T \mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We want to show that the second term on the right-hand side is negative. We
    cannot immediately apply our condition on \(\mathbf{v}\) as the gradient in the
    previous equation is taken at \(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v}\),
    not \(\mathbf{x}_0\).
  prefs: []
  type: TYPE_NORMAL
- en: The gradient is continuous (in the sense that all its components are continuous).
    In particular, the function \(\nabla f(\mathbf{x})^T \mathbf{v}\) is continuous
    as a linear combination of continuous functions. By the definition of continuity,
    for any \(\epsilon > 0\) – say \(\epsilon = \eta/2\) – there is \(\delta > 0\)
    small enough such that all \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\) satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\nabla f(\mathbf{x})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T
    \mathbf{v}\,\right| &< \epsilon = \eta/2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Take \(\alpha^* > 0\) small enough that \(\mathbf{x}_0 + \alpha^* \mathbf{v}
    \in B_\delta(\mathbf{x}_0)\). Then, for all \(\alpha \in (0,\alpha^*)\), whatever
    \(\xi_\alpha \in (0,1)\) is, it holds that \(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v} \in B_\delta(\mathbf{x}_0)\). Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})^T \mathbf{v}
    &= \nabla f(\mathbf{x}_0)^T \mathbf{v} + (\nabla f(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T \mathbf{v})\\ &\leq \nabla
    f(\mathbf{x}_0)^T \mathbf{v} + \left|\nabla f(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T \mathbf{v}\,\right|\\ &< -\eta
    + \eta/2\\ &= - \eta/2 < 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by definition of \(\eta\). That implies
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) < f(\mathbf{x}_0) - \alpha \eta/2 < f(\mathbf{x}_0),
    \quad \forall \alpha \in (0,\alpha^*) \]
  prefs: []
  type: TYPE_NORMAL
- en: and proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Descent Direction)** \(\idx{descent direction lemma}\xdi\) Let
    \(f : \mathbb{R}^d \to \mathbb{R}\) be continuously differentiable at \(\mathbf{x}_0\)
    and assume that \(\nabla f(\mathbf{x}_0) \neq 0\). Then \(f\) has a descent direction
    at \(\mathbf{x}_0\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Take \(\mathbf{v} = - \nabla f(\mathbf{x}_0)\). Then \(\nabla f(\mathbf{x}_0)^T
    \mathbf{v} = - \|\nabla f(\mathbf{x}_0)\|^2 < 0\) since \(\nabla f(\mathbf{x}_0)
    \neq \mathbf{0}\). \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: This leads to the following fundamental result.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(First-Order Necessary Optimality Condition)** \(\idx{first-order
    necessary optimality condition}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be continuously differentiable on \(\mathbb{R}^d\). If \(\mathbf{x}_0\) is a local
    minimizer, then \(\nabla f(\mathbf{x}_0) = \mathbf{0}\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* In a descent direction, \(f\) decreases hence there cannot be
    one at a local minimizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We argue by contradiction. Suppose that \(\nabla f(\mathbf{x}_0) \neq
    \mathbf{0}\). By the *Descent Direction Lemma*, there is a descent direction \(\mathbf{v}
    \in \mathbb{R}^d\) at \(\mathbf{x}_0\). That implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) < f(\mathbf{x}_0), \quad \forall \alpha
    \in (0, \alpha^*) \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\alpha^* > 0\). So every open ball around \(\mathbf{x}_0\) has a
    point achieving a smaller value than \(f(\mathbf{x}_0)\). Thus \(\mathbf{x}_0\)
    is not a local minimizer, a contradiction. So it must be that \(\nabla f(\mathbf{x}_0)
    = \mathbf{0}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: A point satisfying the first-order necessary conditions is called a stationary
    point.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Stationary Point)** \(\idx{stationary point}\xdi\) Let \(f
    : D \to \mathbb{R}\) be continuously differentiable on an open set \(D \subseteq
    \mathbb{R}^d\). If \(\nabla f(\mathbf{x}_0) = \mathbf{0}\), we say that \(\mathbf{x}_0
    \in D\) is a stationary point of \(f\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Rayleight Quotient)** Let \(A \in \mathbb{R}^{d \times d}\)
    be a symmetric matrix. The associated Rayleigh quotient\(\idx{Rayleigh quotient}\xdi\)
    is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{R}_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} \]
  prefs: []
  type: TYPE_NORMAL
- en: which is defined for any \(\mathbf{u} = (u_1,\ldots,u_d) \neq \mathbf{0}\) in
    \(\mathbb{R}^{d}\). As a function from \(\mathbb{R}^{d} \setminus \{\mathbf{0}\}\)
    to \(\mathbb{R}\), \(\mathcal{R}_A(\mathbf{u})\) is continuously differentiable.
    We find its stationary points.
  prefs: []
  type: TYPE_NORMAL
- en: We use the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule) and
    our previous results on the gradient of quadratic functions. Specifically, note
    that (using that \(A\) is symmetric)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial}{\partial u_i} \mathcal{R}_A(\mathbf{u}) &=
    \frac{\left(\frac{\partial}{\partial u_i} \langle \mathbf{u}, A \mathbf{u} \rangle\right)
    \langle \mathbf{u}, \mathbf{u} \rangle - \langle \mathbf{u}, A \mathbf{u} \rangle
    \left( \frac{\partial}{\partial u_i} \langle \mathbf{u}, \mathbf{u} \rangle\right)}{\langle
    \mathbf{u}, \mathbf{u} \rangle^2}\\ &= \frac{2\left(\frac{\partial}{\partial u_i}
    \frac{1}{2}\mathbf{u}^T A \mathbf{u}\right) \|\mathbf{u}\|^2 - \mathbf{u}^T A
    \mathbf{u} \left( \frac{\partial}{\partial u_i} \sum_{j=1}^d u_j^2\right)}{\|\mathbf{u}\|^4}\\
    &= \frac{2\left(A \mathbf{u}\right)_i \|\mathbf{u}\|^2 - \mathbf{u}^T A \mathbf{u}
    \left( 2 u_i \right)}{\|\mathbf{u}\|^4}\\ &= \frac{2}{\|\mathbf{u}\|^2}\left\{\left(A
    \mathbf{u}\right)_i - \mathcal{R}_A(\mathbf{u}) u_i \right\}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In vector form this is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla \mathcal{R}_A(\mathbf{u}) = \frac{2}{\|\mathbf{u}\|^2} \left\{A \mathbf{u}
    - \mathcal{R}_A(\mathbf{u}) \,\mathbf{u} \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The stationary points satisfy \(\nabla \mathcal{R}_A(\mathbf{u}) = \mathbf{0}\),
    or after getting rid of the denominator and rearranging,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{u} = \mathcal{R}_A(\mathbf{u}) \,\mathbf{u}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The solutions to this system are eigenvectors of \(A\), that is, they satisfy
    \(A\mathbf{u} = \lambda \mathbf{u}\) for some eigenvalue \(\lambda\). If \(\mathbf{q}_i\)
    is a unit eigenvector of \(A\) with eigenvalue \(\lambda_i\), then we have that
    \(\mathcal{R}_A(\mathbf{q}_i) = \lambda_i\) (Check it!) and
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{q}_i = \mathcal{R}_A(\mathbf{q}_i) \,\mathbf{q}_i = \lambda_i \mathbf{q}_i.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvectors of \(A\) are not in general local minimizers of its Rayleigh
    quotient. In fact one of them – the largest one – is a global maximizer! \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Second-order conditions[#](#second-order-conditions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local minimizers can also be characterized in terms of the Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: We will make use of *Taylor’s Theorem*, a generalization of the *Mean Value
    Theorem* that provides polynomial approximations to a function around a point.
    We restrict ourselves to the case of a linear approximation with second-order
    error term, which will suffice for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Taylor’s theorem** We begin by reviewing the single-variable case, which
    we will use to prove the general verison.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Taylor)** \(\idx{Taylor''s theorem}\xdi\) Let \(f: D \to \mathbb{R}\)
    where \(D \subseteq \mathbb{R}\). Suppose \(f\) has a continuous derivative on
    \([a,b]\) and that its second derivative exists on \((a,b)\). Then for any \(x
    \in [a, b]\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x) = f(a) + (x-a) f'(a) + \frac{1}{2} (x-a)^2 f''(\xi) \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(a < \xi < x\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: The third term on the right-hand side of *Taylor’s Theorem* is called the Lagrange
    remainder. It can be seen as an error term between \(f(x)\) and the linear approximation
    \(f(a) + (x-a) f'(a)\). There are [other forms](https://en.wikipedia.org/wiki/Taylor%27s_theorem#Explicit_formulas_for_the_remainder)
    for the remainder. The form we stated here is useful when one has a bound on the
    second derivative. Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Consider \(f(x) = e^x\). Then \(f''(x) = f''''(x) = e^x\).
    Suppose we are interested in approximating \(f\) in the interval \([0,1]\). We
    take \(a=0\) and \(b=1\) in *Taylor’s Theorem*. The linear term is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(a) + (x-a) f'(a) = 1 + x e^0 = 1 + x. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then for any \(x \in [0,1]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x) = 1 + x + \frac{1}{2}x^2 e^{\xi_x} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\xi_x \in (0,1)\) depends on \(x\). We get a uniform bound on the error
    over \([0,1]\) by replacing \(\xi_x\) with its worst possible value over \([0,1]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ |f(x) - (1+x)| \leq \frac{1}{2}x^2 e^{\xi_x} \leq \frac{e}{2} x^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If we plot the upper and lower bounds, we see that \(f\) indeed falls within
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/3679f4fd83bc012d44e92e99af6e84eb0f84c2d88f5be251891c9521ea8a7fc9.png](../Images/c339c626ad8f04a3c45b8a598e231b2a.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In the case of several variables, we again restrict ourselves to the second
    order. For the more general version, see e.g. [Wikipedia](https://en.wikipedia.org/wiki/Taylor's_theorem#Taylor's_theorem_for_multivariate_functions).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Taylor)** \(\idx{Taylor''s theorem}\xdi\) Let \(f : D \to \mathbb{R}\)
    where \(D \subseteq \mathbb{R}^d\). Let \(\mathbf{x}_0 \in D\) and \(\delta >
    0\) be such that \(B_\delta(\mathbf{x}_0) \subseteq D\). If \(f\) is twice continuously
    differentiable on \(B_\delta(\mathbf{x}_0)\), then for any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} -
    \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \,\mathbf{H}_f(\mathbf{x}_0
    + \xi (\mathbf{x} - \mathbf{x}_0)) \,(\mathbf{x} - \mathbf{x}_0), \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\xi \in (0,1)\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: As in the single-variable case, we think of \(f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T
    (\mathbf{x} - \mathbf{x}_0)\) for fixed \(\mathbf{x}_0\) as a linear – or more
    accurately affine – approximation to \(f\) at \(\mathbf{x}_0\). The third term
    on the right-hand side above quantifies the error of this approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We apply the single-variable result to \(\phi(t) = f(\boldsymbol{\alpha}(t))\).
    We use the *Chain Rule* to compute the needed derivatives.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(\mathbf{p} = \mathbf{x} - \mathbf{x}_0\) and \(\phi(t) = f(\boldsymbol{\alpha}(t))\)
    where \(\boldsymbol{\alpha}(t) = \mathbf{x}_0 + t \mathbf{p}\). Observe that \(\phi(0)
    = f(\mathbf{x}_0)\) and \(\phi(1) = f(\mathbf{x})\). As observed in the proof
    of the *Mean Value Theorem*, \(\phi''(t) = \nabla f(\boldsymbol{\alpha}(t))^T
    \mathbf{p}\). By the *Chain Rule* and our previous *Parametric Line Example*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \phi''(t) &= \frac{\mathrm{d}}{\mathrm{d} t} \left[\sum_{i=1}^d
    \frac{\partial f(\boldsymbol{\alpha}(t))}{\partial x_i} p_i \right]\\ &= \sum_{i=1}^d
    \left(\nabla \frac{\partial f(\boldsymbol{\alpha}(t))}{\partial x_i}\right)^T
    \boldsymbol{\alpha}'(t) \,p_i \\ &= \sum_{i=1}^d \sum_{j=1}^d \frac{\partial^2
    f(\boldsymbol{\alpha}(t))}{\partial x_j \partial x_i} p_j p_i\\ &= \mathbf{p}^T
    \,\mathbf{H}_f(\mathbf{x}_0 + t \mathbf{p}) \,\mathbf{p}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, \(\phi\) has continuous first and second derivatives on \([0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: By *Taylor’s Theorem* in the single-variable case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi(t) = \phi(0) + t \phi'(0) + \frac{1}{2} t^2 \phi''(\xi) \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\xi \in (0,t)\). Plugging in the expressions for \(\phi(0)\), \(\phi'(0)\)
    and \(\phi''(\xi)\) and taking \(t=1\) gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider the function \(f(x_1, x_2) = x_1 x_2 + x_1^2 + e^{x_1}
    \cos x_2\). We apply *Taylor’s Theorem* with \(\mathbf{x}_0 = (0, 0)\) and \(\mathbf{x}
    = (x_1, x_2)\). The gradient is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(x_1, x_2) = (x_2 + 2 x_1 + e^{x_1} \cos x_2, x_1 - e^{x_1} \sin
    x_2 ) \]
  prefs: []
  type: TYPE_NORMAL
- en: and the Hessian is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 2 + e^{x_1} \cos x_2
    & 1 - e^{x_1} \sin x_2\\ 1 - e^{x_1} \sin x_2 & - e^{x_1} \cos x_2 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: So \(f(0,0) = 1\) and \(\nabla f(0,0) = (1, 0)\). Thus, by *Taylor’s Theorem*,
    there is \(\xi \in (0,1)\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x_1, x_2) = 1 + x_1 + \frac{1}{2}[2 x_1^2 + 2 x_1 x_2 + (x_1^2 - x_2^2)
    \,e^{\xi x_1} \cos(\xi x_2) - 2 x_1 x_2 e^{\xi x_1} \sin(\xi x_2)]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Second directional derivative** To control the error term in *Taylor’s Theorem*,
    it will be convenient to introduce a notion of second directional derivative.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Second Directional Derivative)** \(\idx{second directional
    derivative}\xdi\) Let \(f : D \to \mathbb{R}\) where \(D \subseteq \mathbb{R}^d\),
    let \(\mathbf{x}_0 \in D\) be an interior point of \(D\) and let \(\mathbf{v}
    \in \mathbb{R}^d\) be a nonzero vector. The second directional derivative of \(f\)
    at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f (\mathbf{x}_0)}{\partial \mathbf{v}^2} = \lim_{h \to 0}
    \frac{1}{h} \left[\frac{\partial f(\mathbf{x}_0 + h \mathbf{v})}{\partial \mathbf{v}}
    - \frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}}\right] \]
  prefs: []
  type: TYPE_NORMAL
- en: provided the limit exists. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Typically, \(\mathbf{v}\) is a unit vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Second Directional Derivative and Hessian)** \(\idx{second directional
    derivative and Hessian theorem}\xdi\) Let \(f : D \to \mathbb{R}\) where \(D \subseteq
    \mathbb{R}^d\), let \(\mathbf{x}_0 \in D\) be an interior point of \(D\) and let
    \(\mathbf{v} \in \mathbb{R}^d\) be a vector. Assume that \(f\) is twice continuously
    differentiable at \(\mathbf{x}_0\). Then the second directional derivative of
    \(f\) at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f (\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T
    H_f(\mathbf{x}_0) \,\mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: Note the similarity to the quadratic term in *Taylor’s Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We have already done this calculation in the proof of *Taylor’s
    Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Then, by definition of the derivative,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \lim_{h \to 0} \frac{1}{h} \left[\frac{\partial f(\mathbf{x}_0
    + h \mathbf{v})}{\partial \mathbf{v}} - \frac{\partial f(\mathbf{x}_0)}{\partial
    \mathbf{v}}\right] &= \lim_{h \to 0} \frac{1}{h} \left[\nabla f(\mathbf{x}_0 +
    h \mathbf{v})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T \mathbf{v}\right]\\ &= \lim_{h
    \to 0} \frac{1}{h} \sum_{i=1}^n v_i \left[\frac{\partial f(\mathbf{x}_0 + h \mathbf{v})}{\partial
    x_i} - \frac{\partial f(\mathbf{x}_0)}{\partial x_i} \right]\\ &= \sum_{i=1}^n
    v_i \lim_{h \to 0} \frac{1}{h} \left[\frac{\partial f(\mathbf{x}_0 + h \mathbf{v})}{\partial
    x_i} - \frac{\partial f(\mathbf{x}_0)}{\partial x_i} \right]\\ &= \sum_{i=1}^n
    v_i \frac{\partial g_i (\mathbf{x}_0)}{\partial \mathbf{v}}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(g_i(\mathbf{x}_0) = \frac{\partial f(\mathbf{x}_0)}{\partial x_i}\).
    So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial g_i (\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla g_i(\mathbf{x}_0)^T
    \mathbf{v} = \sum_{j=1}^n v_j \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_i\partial
    x_j} \]
  prefs: []
  type: TYPE_NORMAL
- en: by the *Directional Derivative and Gradient Theorem*. Plugging back above we
    get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f (\mathbf{x}_0)}{\partial \mathbf{v}^2} = \sum_{i=1}^n
    v_i \sum_{j=1}^n v_j \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_i\partial x_j}
    = \mathbf{v}^T H_f(\mathbf{x}_0) \,\mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: So going back to *Taylor’s Theorem*
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} -
    \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \,\mathbf{H}_f(\mathbf{x}_0
    + \xi (\mathbf{x} - \mathbf{x}_0)) \,(\mathbf{x} - \mathbf{x}_0), \]
  prefs: []
  type: TYPE_NORMAL
- en: we see that the second term on the right-hand side is the directional derivative
    at \(\mathbf{x}_0\) in the direction \(\mathbf{x} - \mathbf{x}_0\) and that the
    third term is half of the second directional derivative at \(\mathbf{x}_0 + \xi
    (\mathbf{x} - \mathbf{x}_0)\) in the same direction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Necessary condition** When \(f\) is twice continuously differentiable, we
    get a necessary condition based on the Hessian.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Second-Order Necessary Optimality Condition)** \(\idx{second-order
    necessary optimality condition}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be twice continuously differentiable on \(\mathbb{R}^d\). If \(\mathbf{x}_0\)
    is a local minimizer, then \(\nabla f(\mathbf{x}_0) = \mathbf{0}\) and \(\mathbf{H}_f(\mathbf{x}_0)\)
    is positive semidefinite. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* By *Taylor’s Theorem* and the *First-Order Necessary Optimality
    Condition*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \nabla
    f(\mathbf{x}_0)^T(\alpha \mathbf{v}) + \frac{1}{2}(\alpha \mathbf{v})^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) (\alpha \mathbf{v})\\ &= f(\mathbf{x}_0) + \frac{1}{2}\alpha^2
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(\mathbf{H}_f\) is positive semidefinite in a neighborhood around \(\mathbf{x}_0\),
    then the second term on the right-hand side is nonnegative, which is necessary
    for \(\mathbf{x}_0\) to be a local minimizer. Formally we argue by contradiction:
    indeed, if \(\mathbf{H}_f\) is not positive semidefinite, then there must exists
    a direction in which the second directional derivative is negative; since the
    gradient is \(\mathbf{0}\) at \(\mathbf{x}_0\), intuitively the directional derivative
    must become negative in that direction as well and the function must decrease.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We argue by contradiction. Suppose that \(\mathbf{H}_f(\mathbf{x}_0)\)
    is not positive semidefinite. By definition, there must be a unit vector \(\mathbf{v}\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}, \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} \rangle = - \eta
    < 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, \(\mathbf{v}\) is a direction in which the second directional derivative
    is negative.
  prefs: []
  type: TYPE_NORMAL
- en: For \(\alpha > 0\), *Taylor’s Theorem* implies that there is \(\xi_\alpha \in
    (0,1)\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \nabla
    f(\mathbf{x}_0)^T(\alpha \mathbf{v}) + \frac{1}{2} (\alpha \mathbf{v})^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) (\alpha \mathbf{v})\\ &= f(\mathbf{x}_0) + \frac{1}{2}
    \alpha^2 \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v}
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used \(\nabla f(\mathbf{x}_0) = \mathbf{0}\) by the *First-Order Necessary
    Optimality Condition*. We want to show that the second term on the right-hand
    side is negative.
  prefs: []
  type: TYPE_NORMAL
- en: The Hessian is continuous (in the sense that all its entries are continuous
    functions of \(\mathbf{x}\)). In particular, the second directional derivative
    \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}) \,\mathbf{v}\) is continuous as a linear
    combination of continuous functions. So, by definition of continuity, for any
    \(\epsilon > 0\) – say \(\epsilon = \eta/2\) – there is \(\delta > 0\) small enough
    that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left| \mathbf{v}^T \mathbf{H}_f(\mathbf{x}) \,\mathbf{v} -
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} \right| &< \eta/2 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: Take \(\alpha^* > 0\) small enough that \(\mathbf{x}_0 + \alpha^* \mathbf{v}
    \in B_\delta(\mathbf{x}_0)\). Then, for all \(\alpha \in (0,\alpha^*)\), whatever
    \(\xi_\alpha \in (0,1)\) is, it holds that \(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v} \in B_\delta(\mathbf{x}_0)\). Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v}) \,\mathbf{v} &= \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v}
    + (\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v}) \,\mathbf{v}
    - \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} )\\ &\leq \mathbf{v}^T
    \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} + |\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) \,\mathbf{v} - \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0)
    \,\mathbf{v}|\\ &< -\eta + \eta/2\\ &< - \eta/2 < 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by definition of \(\eta\). That implies
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) < f(\mathbf{x}_0) - \alpha^2 \eta/4 <
    f(\mathbf{x}_0). \]
  prefs: []
  type: TYPE_NORMAL
- en: Since this holds for all sufficiently small \(\alpha\), every open ball around
    \(\mathbf{x}_0\) has a point achieving a lower value than \(f(\mathbf{x}_0)\).
    Thus \(\mathbf{x}_0\) is not a local minimizer, a contradiction. So it must be
    that \(\mathbf{H}_f(\mathbf{x}_0) \succeq \mathbf{0}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Sufficient condition** The necessary condition above is not in general sufficient,
    as the following example shows.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Let \(f(x) = x^3\). Then \(f''(x) = 3 x^2\) and \(f''''(x)
    = 6 x\) so that \(f''(0) = 0\) and \(f''''(0) \geq 0\). Hence \(x=0\) is a stationary
    point. But \(x=0\) is not a local minimizer. Indeed \(f(0) = 0\) but, for any
    \(\delta > 0\), \(f(-\delta) < 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/2124857c662385a175abf776e89126ff842e450579382110f506759f09254a95.png](../Images/4a1cd8247ab18de84607516e9c6f7863.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We give sufficient conditions for a point to be a local minimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Second-Order Sufficient Optimality Condition)** \(\idx{second-order
    sufficient optimality condition}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be twice continuously differentiable on \(\mathbb{R}^d\). If \(\nabla f(\mathbf{x}_0)
    = \mathbf{0}\) and \(\mathbf{H}_f(\mathbf{x}_0)\) is positive definite, then \(\mathbf{x}_0\)
    is a strict local minimizer. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use *Taylor’s Theorem* again. This time we use the positive
    definiteness of the Hessian to bound the value of the function from below.'
  prefs: []
  type: TYPE_NORMAL
- en: We will need a lemma.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Quadratic Form and Frobenius Norm)** \(\idx{quadratic form and
    Frobenius norm lemma}\xdi\) Let \(A = (a_{i,j})_{i,j}\) and \(B = (b_{i,j})_{i,j}\)
    be matrices in \(\mathbb{R}^{n \times m}\). For any unit vectors \(\mathbf{u}
    \in \mathbb{R}^n\) and \(\mathbf{v} \in \mathbb{R}^m\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left| \mathbf{u}^T A \,\mathbf{v} - \mathbf{u}^T B \,\mathbf{v} \right|
    \leq \|A - B\|_F. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Cauchy-Schwarz inequality*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left| \mathbf{u}^T A \,\mathbf{v} - \mathbf{u}^T B \,\mathbf{v}
    \right| &= \left| \sum_{i=1}^n \sum_{j=1}^m u_i v_j (a_{i,j} - b_{i,j}) \right|\\
    &\leq \sqrt{\sum_{i=1}^n \sum_{j=1}^m u_i^2 v_j^2} \sqrt{\sum_{i=1}^n \sum_{j=1}^m
    (a_{i,j} - b_{i,j})^2}\\ &= \|A - B\|_F, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\mathbf{u}\) and \(\mathbf{v}\) have unit norm on the last
    line. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Second-Order Sufficient Optimality Condition)* By *Taylor’s Theorem*,
    for all unit vectors \(\mathbf{v} \in \mathbb{R}^d\) and \(\alpha \in \mathbb{R}\),
    there is \(\xi_{\alpha} \in (0,1)\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \nabla
    f(\mathbf{x}_0)^T(\alpha \mathbf{v}) + \frac{1}{2}(\alpha \mathbf{v})^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) (\alpha \mathbf{v})\\ &= f(\mathbf{x}_0) + \frac{1}{2}\alpha^2
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v},
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\nabla f(\mathbf{x}_0) = \mathbf{0}\). The second term
    on the last line is \(0\) at \(\mathbf{v} = \mathbf{0}\). Our goal is to show
    that it is strictly positive (except at \(\mathbf{0}\)) in a neighborhood of \(\mathbf{0}\).
  prefs: []
  type: TYPE_NORMAL
- en: The set \(\mathbb{S}^{d-1}\) of unit vectors in \(\mathbb{R}^d\) is closed and
    bounded. The expression \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v}\),
    viewed as a function of \(\mathbf{v}\), is continuous since it is a polynomial.
    Hence, by the *Extreme Value Theorem*, it attains its minimum on \(\mathbb{S}^{d-1}\).
    By our assumption that \(\mathbf{H}_f(\mathbf{x}_0)\) is positive definite, that
    minimum must be strictly positive, say \(\mu > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: By the *Quadratic Form and Frobenius Norm Lemma* (ignoring the absolute value),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} - \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\,\mathbf{v} \leq \|\mathbf{H}_f(\mathbf{x}_0) - \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\|_F. \]
  prefs: []
  type: TYPE_NORMAL
- en: The Frobenius norm above is continuous in \(\mathbf{w}\) as a composition of
    continuous functions. Moreover, we have at \(\mathbf{w} = \mathbf{0}\) that this
    Frobenius norm is \(0\). Hence, by definition of continuity, for any \(\epsilon
    > 0\) – say \(\epsilon := \mu/2\) – there is \(\delta > 0\) such that \(\mathbf{w}
    \in B_{\delta}(\mathbf{0})\) implies \(\|\mathbf{H}_f(\mathbf{x}_0) - \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\|_F < \epsilon = \mu/2\).
  prefs: []
  type: TYPE_NORMAL
- en: Since \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} > \mu\), the inequality
    in the previous display implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \mathbf{w})\,\mathbf{v} \geq \mathbf{v}^T
    \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} - \|\mathbf{H}_f(\mathbf{x}_0) - \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\|_F > \frac{\mu}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This holds for any unit vector \(\mathbf{v}\) and any \(\mathbf{w} \in B_{\delta}(\mathbf{0})\).
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our Taylor expansion, for \(\alpha > 0\) small enough (not depending
    on \(\mathbf{v}\); why?), it holds that \(\mathbf{w} = \xi_\alpha \alpha \mathbf{v}
    \in B_{\delta}(\mathbf{0})\) so that we get from the previous inequality
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \frac{1}{2}\alpha^2
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v}\\
    &> f(\mathbf{x}_0) + \frac{1}{4} \alpha^2 \mu \\ &> f(\mathbf{x}_0). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore \(\mathbf{x}_0\) is a strict local minimizer. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Adding equality constraints[#](#adding-equality-constraints "Link to
    this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we have considered *unconstrained* optimization problems, that is,
    the variable \(\mathbf{x}\) can take any value in \(\mathbb{R}^d\). However, it
    is common to impose conditions on \(\mathbf{x}\). Hence, we consider the *constrained*\(\idx{constrained
    optimization}\xdi\) minimization problem
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathscr{X}} f(\mathbf{x}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathscr{X} \subset \mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** For instance, the entries of \(\mathbf{x}\) may have to satisfy
    certain bounds. In that case, we would have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathscr{X} = \{\mathbf{x} = (x_1,\ldots,x_d) \in \mathbb{R}^d:x_i \in [a_i,
    b_i], \forall i\} \]
  prefs: []
  type: TYPE_NORMAL
- en: for some constants \(a_i < b_i\), \(i=1,\ldots,d\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In this more general problem, the notion of global and local minimizer can be
    adapted straightforwardly. Note that, for simplicity, we will assume that \(f\)
    is defined over all of \(\mathbb{R}^d\). When \(\mathbf{x} \in \mathscr{X}\),
    it is said to be feasible.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Global minimizer)** \(\idx{global minimizer or maximizer}\xdi\)
    Let \(f : \mathbb{R}^d \to \mathbb{R}\). The point \(\mathbf{x}^* \in \mathscr{X}\)
    is a global minimizer of \(f\) over \(\mathscr{X}\) if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) \geq f(\mathbf{x}^*), \quad \forall \mathbf{x} \in \mathscr{X}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Local minimizer)** \(\idx{local minimizer or maximizer}\xdi\)
    Let \(f : \mathbb{R}^d \to \mathbb{R}\). The point \(\mathbf{x}^* \in \mathscr{X}\)
    is a local minimizer of \(f\) over \(\mathscr{X}\) if there is \(\delta > 0\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) \geq f(\mathbf{x}^*), \quad \forall \mathbf{x} \in (B_{\delta}(\mathbf{x}^*)
    \setminus \{\mathbf{x}^*\}) \cap \mathscr{X}. \]
  prefs: []
  type: TYPE_NORMAL
- en: If the inequality is strict, we say that \(\mathbf{x}^*\) is a strict local
    minimizer. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this subsection, we restrict ourselves to one important class of constraints:
    equality constraints. That is, we consider the minimization problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\text{min} f(\mathbf{x})\\ &\text{s.t.}\ h_i(\mathbf{x}) =
    0,\ \forall i \in [\ell] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'where s.t. stands for “subject to”. In other words, we only allow those \(\mathbf{x}''s\)
    such that \(h_i(\mathbf{x}) = 0\) for all \(i\). Here \(f : \mathbb{R}^d \to \mathbb{R}\)
    and \(h_i : \mathbb{R}^d \to \mathbb{R}\), \(i\in [\ell]\). We will sometimes
    use the notation \(\mathbf{h} : \mathbb{R}^d \to \mathbb{R}^\ell\), where \(\mathbf{h}(\mathbf{x})
    = (h_1(\mathbf{x}), \ldots, h_\ell(\mathbf{x}))\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** If we want to minimize \(2 x_1^2 + 3 x_2^2\) over all two-dimensional
    unit vectors \(\mathbf{x} = (x_1, x_2)\), then we can let'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = 2 x_1^2 + 3 x_2^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_1(\mathbf{x}) = 1 - x_1^2 - x_2^2 = 1 - \|\mathbf{x}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Observe that we could have chosen a different equality constraint to express
    the same minimization problem. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The following theorem generalizes the *First-Order Necessary Optimality Condition*.
    The proof is omitted.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Lagrange Multipliers)** \(\idx{Lagrange multipliers theorem}\xdi\)
    Assume \(f : \mathbb{R}^d \to \mathbb{R}\) and \(h_i : \mathbb{R}^d \to \mathbb{R}\),
    \(i\in [\ell]\), are continuously differentiable. Let \(\mathbf{x}^*\) be a local
    minimizer of \(f\) s.t. \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\). Assume further
    that the vectors \(\nabla h_i (\mathbf{x}^*)\), \(i \in [\ell]\), are linearly
    independent. Then there exists a unique vector'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \blambda^* = (\lambda_1^*, \ldots, \lambda_\ell^*) \]
  prefs: []
  type: TYPE_NORMAL
- en: satisfying
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \nabla h_i(\mathbf{x}^*)
    = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: The quantities \(\lambda_1^*, \ldots, \lambda_\ell^*\) are called Lagrange multipliers\(\idx{Lagrange
    multipliers}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** Returning to the previous example,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}) = \left( \frac{\partial f(\mathbf{x})}{\partial x_1},
    \frac{\partial f(\mathbf{x})}{\partial x_2} \right) = (4 x_1, 6 x_2) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla h_1(\mathbf{x}) = \left( \frac{\partial h_1(\mathbf{x})}{\partial
    x_1}, \frac{\partial h_1(\mathbf{x})}{\partial x_2} \right) = (- 2 x_1, - 2 x_2).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The conditions in the theorem read
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &4 x_1 - 2 \lambda_1 x_1 = 0\\ &6 x_2 - 2 \lambda_1 x_2 = 0.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The constraint \(x_1^2 + x_2^2 = 1\) must also be satisfied. Observe that the
    linear independence condition is automatically satisfied since there is only one
    constraint.
  prefs: []
  type: TYPE_NORMAL
- en: There are several cases to consider.
  prefs: []
  type: TYPE_NORMAL
- en: 1- If neither \(x_1\) nor \(x_2\) is \(0\), then the first equation gives \(\lambda_1
    = 2\) while the second one gives \(\lambda_1 = 3\). So that case cannot happen.
  prefs: []
  type: TYPE_NORMAL
- en: 2- If \(x_1 = 0\), then \(x_2 = 1\) or \(x_2 = -1\) by the constraint and the
    second equation gives \(\lambda_1 = 3\) in either case.
  prefs: []
  type: TYPE_NORMAL
- en: 3- If \(x_2 = 0\), then \(x_1 = 1\) or \(x_1 = -1\) by the constraint and the
    first equation gives \(\lambda_1 = 2\) in either case.
  prefs: []
  type: TYPE_NORMAL
- en: Does any of these last four solutions, i.e., \((x_1,x_2,\lambda_1) = (0,1,3)\),
    \((x_1,x_2,\lambda_1) = (0,-1,3)\), \((x_1,x_2,\lambda_1) = (1,0,2)\) and \((x_1,x_2,\lambda_1)
    = (-1,0,2)\), actually correspond to a local minimizer?
  prefs: []
  type: TYPE_NORMAL
- en: This problem can be solved manually. Indeed, replace \(x_2^2 = 1 - x_1^2\) into
    the objective function to obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[ 2 x_1^2 + 3(1 - x_1^2) = -x_1^2 + 3. \]
  prefs: []
  type: TYPE_NORMAL
- en: This is minimized for the largest value that \(x_1^2\) can take, namely when
    \(x_1 = 1\) or \(x_1 = -1\). Indeed, we must have \(0 \leq x_1^2 \leq x_1^2 +
    x_2^2 = 1\). So both \((x_1, x_2) = (1,0)\) and \((x_1, x_2) = (-1,0)\) are global
    minimizers. A fortiori, they must be local minimizers.
  prefs: []
  type: TYPE_NORMAL
- en: What about \((x_1,x_2) = (0,1)\) and \((x_1,x_2) = (0,-1)\)? Arguing as above,
    they in fact correspond to global *maximizers* of the objective function. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Assume \(\mathbf{x}\) is feasible, that is, \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\).
    We let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathscr{F}_{\mathbf{h}}(\mathbf{x}) = \left\{ \mathbf{v} \in \mathbb{R}^d
    \,:\, \nabla h_i(\mathbf{x})^T \mathbf{v} = \mathbf{0},\ \forall i \in [\ell]
    \right\} \]
  prefs: []
  type: TYPE_NORMAL
- en: be the linear subspace of first-order feasible directions\(\idx{first-order
    feasible directions}\xdi\) at \(\mathbf{x}\). To explain the name, note that by
    a first-order Taylor expansion, if \(\mathbf{v} \in \mathscr{F}_{\mathbf{h}}(\mathbf{x})\)
    then it holds that
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_i(\mathbf{x} + \delta \mathbf{v}) \approx h_i(\mathbf{x}) + \delta \nabla
    h_i(\mathbf{x})^T \mathbf{v} = \mathbf{0} \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: The theorem says that, if \(\mathbf{x}^*\) is a local minimizer, then the gradient
    of \(f\) is orthogonal to the set of first-order feasible directions at \(\mathbf{x}^*\).
    Indeed, any \(\mathbf{v} \in \mathscr{F}_{\mathbf{h}}(\mathbf{x}^*)\) satisfies
    by the theorem that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}^*)^T \mathbf{v} = \left(- \sum_{i=1}^\ell \lambda^*_i
    \nabla h_i(\mathbf{x}^*)\right)^T \mathbf{v} = - \sum_{i=1}^\ell \lambda^*_i \nabla
    h_i(\mathbf{x}^*)^T \mathbf{v} = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, following a first-order feasible direction does not alter the objective
    function value up to second-order error
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}^* + \alpha \mathbf{v}) \approx f(\mathbf{x}^*) + \alpha \nabla
    f(\mathbf{x}^*)^T \mathbf{v} = f(\mathbf{x}^*). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Returning to the previous example, the points satisfying
    \(h_1(\mathbf{x}) = 0\) sit on the circle of radius \(1\) around the origin. We
    have already seen that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla h_1(\mathbf{x}) = \left( \frac{\partial h_1(\mathbf{x})}{\partial
    x_1}, \frac{\partial h_1(\mathbf{x})}{\partial x_2} \right) = (- 2 x_1, - 2 x_2).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Here is code illustrating the theorem (with help from ChatGPT). We first compute
    the function \(h_1\) at a grid of points using [`numpy.meshgrid`](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We use [`matplotlib.pyplot.contour`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html)
    to plot the constraint set as a [contour line](https://en.wikipedia.org/wiki/Contour_line)
    (for the constant value \(0\)) of \(h_1\). Gradients of \(h_1\) are plotted at
    a collection of `points` with the [`matplotlib.pyplot.quiver`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html)
    function, which is used for plotting vectors as arrows. We see that the directions
    of first-order feasible directions are orthogonal to the arrows, and therefore
    are tangent to the constraint set.
  prefs: []
  type: TYPE_NORMAL
- en: At those same `points`, we also plot the gradient of \(f\), which recall is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}) = \left( \frac{\partial f(\mathbf{x})}{\partial x_1},
    \frac{\partial f(\mathbf{x})}{\partial x_2} \right) = (4 x_1, 6 x_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: We make all gradients into unit vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/a5aed8b6b668174566d2d532e4b400fcf5707acf3befc2f4f7b86d23fc3b68a5.png](../Images/b947d14ae52253453dee08ef202017c5.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that, at \((-1,0)\) and \((1,0)\), the gradient is indeed orthogonal
    to the first-order feasible directions.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: A feasible vector \(\mathbf{x}\) is said to be regular if the vectors \(\nabla
    h_i (\mathbf{x}^*)\), \(i \in [\ell]\), are linearly independent. We re-formulate
    the previous theorem in terms of the Lagrangian function, which is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ L(\mathbf{x}, \blambda) = f(\mathbf{x}) + \sum_{i=1}^\ell \lambda_i h_i(\mathbf{x}),
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\blambda = (\lambda_1,\ldots,\lambda_\ell)\). Then, by the theorem,
    a regular local minimizer satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\nabla_{\mathbf{x}} L(\mathbf{x}, \blambda) = \mathbf{0}\\
    &\nabla_{\blambda} L(\mathbf{x}, \blambda) = \mathbf{0}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Here the notation \(\nabla_{\mathbf{x}}\) (respectively \(\nabla_{\blambda}\))
    indicates that we are taking the vector of partial derivatives with respect to
    only the variables in \(\mathbf{x}\) (respectively \(\blambda\)).
  prefs: []
  type: TYPE_NORMAL
- en: To see that these equations hold, note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla_{\mathbf{x}} L(\mathbf{x}, \blambda) = \nabla f(\mathbf{x}) + \sum_{i=1}^\ell
    \lambda_i \nabla h_i(\mathbf{x}) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla_{\blambda} L(\mathbf{x}, \blambda) = \mathbf{h}(\mathbf{x}). \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(\nabla_{\mathbf{x}} L(\mathbf{x}, \blambda) = \mathbf{0}\) is a restatement
    of the Lagrange multipliers condition and \(\nabla_{\blambda} L(\mathbf{x}, \blambda)
    = \mathbf{0}\) is a restatement of feasibility. Together, they form a system of
    \(d + \ell\) equations in \(d + \ell\) variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider the constrained minimization problem on \(\mathbb{R}^3\)
    where the objective function is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = \frac{1}{2}(x_1^2 + x_2^2 + x_3^2) \]
  prefs: []
  type: TYPE_NORMAL
- en: and the only constraint function is
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_1(\mathbf{x}) = 3 - x_1 - x_2 - x_3. \]
  prefs: []
  type: TYPE_NORMAL
- en: The gradients are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}) = (x_1, x_2, x_3) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla h_1(\mathbf{x}) = (-1, -1, -1). \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, regularity is always satisfied since there is only one non-zero
    vector to consider.
  prefs: []
  type: TYPE_NORMAL
- en: So we are looking for solutions to the system of equations
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &x_1 - \lambda_1 = 0\\ &x_2 - \lambda_1 = 0\\ &x_3 - \lambda_1
    = 0\\ &3 - x_1 - x_2 - x_3 = 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The first three equations imply that \(x_1 = x_2 = x_3 = \lambda\). Replacing
    in the fourth equation gives \(3 - 3 \lambda_1 = 0\) so \(\lambda_1 = 1\). Hence,
    \(x_1 = x_2 = x_3 = 1\) and this is the only solution.
  prefs: []
  type: TYPE_NORMAL
- en: So any local minimizer, if it exists, must be the vector \((1,1,1)\) with Lagrange
    multiplier \(1\). How can we know for sure whether this is the case? \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: As in the unconstrained case, there are *sufficient* conditions. As in that
    case as well, they involve second-order derivatives. We give one such theorem
    next without proof.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** Assume \(f : \mathbb{R}^d \to \mathbb{R}\) and \(h_i : \mathbb{R}^d
    \to \mathbb{R}\), \(i\in [\ell]\), are twice continuously differentiable. Let
    \(\mathbf{x}^* \in \mathbb{R}^d\) and \(\blambda^* \in \mathbb{R}^\ell\) satisfy'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \nabla
    h_i(\mathbf{x}^*) &= \mathbf{0}\\ \mathbf{h}(\mathbf{x}^*) &= \mathbf{0} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T\left( \mathbf{H}_f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i
    \mathbf{H}_{h_i}(\mathbf{x}^*) \right) \mathbf{v} > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(\mathbf{v} \in \mathscr{F}_{\mathbf{h}}(\mathbf{x})\). Then \(\mathbf{x}^*\)
    a strict local minimizer of \(f\) s.t. \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\).
    \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** We return to the previous example. We found a
    unique solution'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (x_1^*, x_2^*, x_3^*, \lambda_1^*) = (1,1,1,1) \]
  prefs: []
  type: TYPE_NORMAL
- en: to the system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \nabla
    h_i(\mathbf{x}^*) &= \mathbf{0}\\ \mathbf{h}(\mathbf{x}^*) &= \mathbf{0} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To check the second-order condition, we need the Hessians. It is straighforward
    to compute the second-order partial derivatives, which do not depend on \(\mathbf{x}\).
    We obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_{f}(\mathbf{x}) = I_{3 \times 3} \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_{h_1}(\mathbf{x}) = \mathbf{0}_{3 \times 3}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \mathbf{H}_{h_i}(\mathbf{x}^*)
    = I_{3 \times 3} \]
  prefs: []
  type: TYPE_NORMAL
- en: and it follows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T\left( \mathbf{H}_f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i
    \mathbf{H}_{h_i}(\mathbf{x}^*) \right) \mathbf{v} = \mathbf{v}^T I_{3 \times 3}
    \mathbf{v} = \|\mathbf{v}\|^2 > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: for any non-zero vector, including those in \(\mathscr{F}_{\mathbf{h}}(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: It follows from the previous theorem that \(\mathbf{x}^*\) is a strict local
    minimizer of the constrained problem. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is the correct definition of a global minimizer
    \(\mathbf{x}^*\) of a function \(f: \mathbb{R}^d \to \mathbb{R}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(f(\mathbf{x}) \geq f(\mathbf{x}^*)\) for all \(x\) in some open ball around
    \(\mathbf{x}^*\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(f(\mathbf{x}) \geq f(\mathbf{x}^*)\) for all \(\mathbf{x} \in \mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\nabla f(\mathbf{x}^*) = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}^*) \mathbf{v} > 0\) for all \(\mathbf{v}
    \in \mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Let \(f: \mathbb{R}^d \to \mathbb{R}\) be continuously differentiable
    at \(\mathbf{x}_0\). The directional derivative of \(f\) at \(\mathbf{x}_0\) in
    the direction \(\mathbf{v} \in \mathbb{R}^d\) is NOT given by:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla f(\mathbf{x}_0)^T
    \mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \mathbf{v}^T \nabla
    f(\mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0)
    \mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \lim_{h \to 0} \frac{f(\mathbf{x}_0
    + h\mathbf{v}) - f(\mathbf{x}_0)}{h}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Let \(f: \mathbb{R}^d \to \mathbb{R}\) be twice continuously differentiable.
    If \(\nabla f(\mathbf{x}_0) = 0\) and \(\mathbf{H}_f(\mathbf{x}_0)\) is positive
    definite, then \(\mathbf{x}_0\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: a) A global minimizer of \(f\).
  prefs: []
  type: TYPE_NORMAL
- en: b) A local minimizer of \(f\), but not necessarily a strict local minimizer.
  prefs: []
  type: TYPE_NORMAL
- en: c) A strict local minimizer of \(f\).
  prefs: []
  type: TYPE_NORMAL
- en: d) A saddle point of \(f\).
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Consider the optimization problem \(\min_\mathbf{x} f(\mathbf{x})\) subject
    to \(\mathbf{h}(\mathbf{x}) = 0\), where \(f: \mathbb{R}^d \to \mathbb{R}\) and
    \(h: \mathbb{R}^d \to \mathbb{R}^\ell\) are continuously differentiable. Let \(\mathbf{x}^*\)
    be a local minimizer and assume that the vectors \(\nabla h_i(\mathbf{x}^*), i
    \in [\ell]\), are linearly independent. According to the Lagrange Multipliers
    theorem, which of the following must be true?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\nabla f(\mathbf{x}^*) = \mathbf{0}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda_i^* \nabla h_i(\mathbf{x}^*)
    = \mathbf{0}\) for some \(\lambda^* \in \mathbb{R}^\ell\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{h}(\mathbf{x}^*) = \mathbf{0}\).
  prefs: []
  type: TYPE_NORMAL
- en: d) Both b and c.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following is a correct statement of Taylor’s Theorem (to
    second order) for a twice continuously differentiable function \(f: D \to \mathbb{R}\),
    where \(D \subseteq \mathbb{R}^d\), at an interior point \(\mathbf{x}_0 \in D\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2}(\mathbf{x}
    - \mathbf{x}_0)^T \mathbf{H}_f(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))(\mathbf{x}
    - \mathbf{x}_0)\) for some \(\xi \in (0,1)\).
  prefs: []
  type: TYPE_NORMAL
- en: b) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \nabla f(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))^T (\mathbf{x} - \mathbf{x}_0)
    + \frac{1}{2}(\mathbf{x} - \mathbf{x}_0)^T \mathbf{H}_f(\mathbf{x}_0 + \xi(\mathbf{x}
    - \mathbf{x}_0))(\mathbf{x} - \mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: c) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \nabla f(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))^T (\mathbf{x} - \mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: d) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \frac{1}{2}(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))^T \mathbf{H}_f(\mathbf{x}_0)(\mathbf{x}_0
    + \xi(\mathbf{x} - \mathbf{x}_0))\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.1: b. Justification: The text states that “The point \(\mathbf{x}^*
    \in \mathbb{R}^d\) is a global minimizer of \(f\) over \(\mathbb{R}^d\) if \(f(\mathbf{x})
    \geq f(\mathbf{x}^*), \forall \mathbf{x} \in \mathbb{R}^d\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.7: c. Justification: The text states the Directional Derivative
    from Gradient theorem: “Assume that \(f\) is continuously differentiable at \(\mathbf{x}_0\).
    Then the directional derivative of \(f\) at \(\mathbf{x}_0\) in the direction
    \(\mathbf{v}\) is given by \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}}
    = \nabla f(\mathbf{x}_0)^T \mathbf{v}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.9: c. Justification: The text states the Second-Order Sufficient
    Condition theorem: “If \(\nabla f(\mathbf{x}_0) = \mathbf{0}\) and \(\mathbf{H}_f(\mathbf{x}_0)\)
    is positive definite, then \(\mathbf{x}_0\) is a strict local minimizer.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.12: d. Justification: The Lagrange Multipliers theorem states
    that under the given conditions, there exists a unique vector \(\boldsymbol{\lambda}^*
    = (\lambda_1^*, \ldots, \lambda_\ell^*)\) satisfying \(\nabla f(\mathbf{x}^*)
    + \sum_{i=1}^\ell \lambda_i^* \nabla h_i(\mathbf{x}^*) = \mathbf{0}\) and \(\mathbf{h}(\mathbf{x}^*)
    = \mathbf{0}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.14: a. Justification: This is the statement of Taylor’s Theorem
    as presented in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. First-order conditions[#](#first-order-conditions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local minimizers can be characterized in terms of the gradient. We first define
    the concept of directional derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Directional derivative** Partial derivatives measure the rate of change of
    a function along the axes. More generally:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Directional Derivative)** \(\idx{directional derivative}\xdi\)
    Let \(f : D \to \mathbb{R}\) where \(D \subseteq \mathbb{R}^d\), let \(\mathbf{x}_0
    = (x_{0,1},\ldots,x_{0,d}) \in D\) be an interior point of \(D\) and let \(\mathbf{v}
    = (v_1,\ldots,v_d) \in \mathbb{R}^d\) be a nonzero vector. The directional derivative
    of \(f\) at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} &= \lim_{h
    \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{v}) - f(\mathbf{x}_0)}{h}\\ &= \lim_{h
    \to 0} \frac{f(x_{0,1} + h v_1,\ldots,x_{0,d} + h v_d) - f(x_{0,1},\ldots,x_{0,d})}{h}
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: provided the limit exists. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Typically, \(\mathbf{v}\) is a unit vector.
  prefs: []
  type: TYPE_NORMAL
- en: Note that taking \(\mathbf{v} = \mathbf{e}_i\) recovers the \(i\)-th partial
    derivative
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{e}_i} = \lim_{h \to 0}
    \frac{f(\mathbf{x}_0 + h \mathbf{e}_i) - f(\mathbf{x}_0)}{h} = \frac{\partial
    f (\mathbf{x}_0)}{\partial x_i}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, a general directional derivative can be expressed in terms of the
    partial derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Directional Derivative and Gradient)** \(\idx{directional derivative
    and gradient theorem}\xdi\) Let \(f : D \to \mathbb{R}\) where \(D \subseteq \mathbb{R}^d\),
    let \(\mathbf{x}_0 \in D\) be an interior point of \(D\) and let \(\mathbf{v}
    \in \mathbb{R}^d\) be a vector. Assume that \(f\) is continuously differentiable
    at \(\mathbf{x}_0\). Then the directional derivative of \(f\) at \(\mathbf{x}_0\)
    in the direction \(\mathbf{v}\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla f(\mathbf{x}_0)^T
    \mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, when \(\mathbf{v}\) is a unit vector, the directional derivative
    is the length of the orthogonal projection of the gradient onto \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* To bring out the partial derivatives, we re-write the directional
    derivative as the derivative of a composition of \(f\) with an affine function.
    We then use the *Chain Rule*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Consider the composition \(\beta(h) = f(\boldsymbol{\alpha}(h))\)
    where \(\boldsymbol{\alpha}(h) = \mathbf{x}_0 + h \mathbf{v}\). Observe that \(\boldsymbol{\alpha}(0)=
    \mathbf{x}_0\) and \(\beta(0)= f(\mathbf{x}_0)\). Then, by definition of the derivative,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\mathrm{d} \beta(0)}{\mathrm{d} h} = \lim_{h \to 0} \frac{\beta(h)
    - \beta(0)}{h} = \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{v}) - f(\mathbf{x}_0)}{h}
    = \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Applying the *Chain Rule* and the parametric line example from the previous
    section, we arrive at
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\mathrm{d} \beta(0)}{\mathrm{d} h} = \nabla f(\boldsymbol{\alpha}(0))^T
    \boldsymbol{\alpha}'(0) = \nabla f(\mathbf{x}_0)^T \mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '![Contour plot of the function . At the point , the gradient and a directional
    derivative are shown (with the help from ChatGPT; converted and adapted from (Source))](../Images/8e2172c7f2d739f2e83162ecdcafe29e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**KNOWLEDGE CHECK:** Let \(f : \mathbb{R}^2 \to \mathbb{R}\) be continuously
    differentiable. Suppose that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} = \frac{3}{\sqrt{5}}
    \qquad \text{and} \qquad \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{w}}
    = \frac{5}{\sqrt{5}} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{v} = (1,2)/\sqrt{5}\) and \(\mathbf{w} = (2,1)/\sqrt{5}\). Compute
    the gradient of \(f\) at \(\mathbf{x}_0\). \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Descent direction** Earlier in the book, we proved a key insight about the
    derivative of a single-variable function \(f\) at a point \(x_0\): it tells us
    where to find smaller values. We generalize the *Descent Direction Lemma* to the
    multivariable case.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we observe that in the continuously differentiable case the directional
    derivative gives a criterion for descent directions.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Descent Direction and Directional Derivative)** \(\idx{descent
    direction and directional derivative lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be continuously differentiable at \(\mathbf{x}_0\). A vector \(\mathbf{v}\) is
    a descent direction for \(f\) at \(\mathbf{x}_0\) if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla f(\mathbf{x}_0)^T
    \mathbf{v} < 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, if the directional derivative of \(f\) at \(\mathbf{x}_0\) in the direction
    \(\mathbf{v}\) is negative. \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* In anticipation of the proof of the second-order condition, we
    use the *Mean Value Theorem* to show that \(f\) takes smaller values in direction
    \(\mathbf{v}\). A simpler proof based on the definition of the directional derivative
    is also possible (Try it!).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Suppose there is \(\mathbf{v} \in \mathbb{R}^d\) such that \(\nabla
    f(\mathbf{x}_0)^T \mathbf{v} = -\eta < 0\). For \(\alpha > 0\), the *Mean Value
    Theorem* implies that there is \(\xi_\alpha \in (0,1)\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v})^T(\alpha \mathbf{v}) = f(\mathbf{x}_0) + \alpha
    \nabla f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})^T \mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We want to show that the second term on the right-hand side is negative. We
    cannot immediately apply our condition on \(\mathbf{v}\) as the gradient in the
    previous equation is taken at \(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v}\),
    not \(\mathbf{x}_0\).
  prefs: []
  type: TYPE_NORMAL
- en: The gradient is continuous (in the sense that all its components are continuous).
    In particular, the function \(\nabla f(\mathbf{x})^T \mathbf{v}\) is continuous
    as a linear combination of continuous functions. By the definition of continuity,
    for any \(\epsilon > 0\) – say \(\epsilon = \eta/2\) – there is \(\delta > 0\)
    small enough such that all \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\) satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\nabla f(\mathbf{x})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T
    \mathbf{v}\,\right| &< \epsilon = \eta/2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Take \(\alpha^* > 0\) small enough that \(\mathbf{x}_0 + \alpha^* \mathbf{v}
    \in B_\delta(\mathbf{x}_0)\). Then, for all \(\alpha \in (0,\alpha^*)\), whatever
    \(\xi_\alpha \in (0,1)\) is, it holds that \(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v} \in B_\delta(\mathbf{x}_0)\). Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})^T \mathbf{v}
    &= \nabla f(\mathbf{x}_0)^T \mathbf{v} + (\nabla f(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T \mathbf{v})\\ &\leq \nabla
    f(\mathbf{x}_0)^T \mathbf{v} + \left|\nabla f(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T \mathbf{v}\,\right|\\ &< -\eta
    + \eta/2\\ &= - \eta/2 < 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by definition of \(\eta\). That implies
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) < f(\mathbf{x}_0) - \alpha \eta/2 < f(\mathbf{x}_0),
    \quad \forall \alpha \in (0,\alpha^*) \]
  prefs: []
  type: TYPE_NORMAL
- en: and proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Descent Direction)** \(\idx{descent direction lemma}\xdi\) Let
    \(f : \mathbb{R}^d \to \mathbb{R}\) be continuously differentiable at \(\mathbf{x}_0\)
    and assume that \(\nabla f(\mathbf{x}_0) \neq 0\). Then \(f\) has a descent direction
    at \(\mathbf{x}_0\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Take \(\mathbf{v} = - \nabla f(\mathbf{x}_0)\). Then \(\nabla f(\mathbf{x}_0)^T
    \mathbf{v} = - \|\nabla f(\mathbf{x}_0)\|^2 < 0\) since \(\nabla f(\mathbf{x}_0)
    \neq \mathbf{0}\). \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: This leads to the following fundamental result.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(First-Order Necessary Optimality Condition)** \(\idx{first-order
    necessary optimality condition}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be continuously differentiable on \(\mathbb{R}^d\). If \(\mathbf{x}_0\) is a local
    minimizer, then \(\nabla f(\mathbf{x}_0) = \mathbf{0}\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* In a descent direction, \(f\) decreases hence there cannot be
    one at a local minimizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We argue by contradiction. Suppose that \(\nabla f(\mathbf{x}_0) \neq
    \mathbf{0}\). By the *Descent Direction Lemma*, there is a descent direction \(\mathbf{v}
    \in \mathbb{R}^d\) at \(\mathbf{x}_0\). That implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) < f(\mathbf{x}_0), \quad \forall \alpha
    \in (0, \alpha^*) \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\alpha^* > 0\). So every open ball around \(\mathbf{x}_0\) has a
    point achieving a smaller value than \(f(\mathbf{x}_0)\). Thus \(\mathbf{x}_0\)
    is not a local minimizer, a contradiction. So it must be that \(\nabla f(\mathbf{x}_0)
    = \mathbf{0}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: A point satisfying the first-order necessary conditions is called a stationary
    point.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Stationary Point)** \(\idx{stationary point}\xdi\) Let \(f
    : D \to \mathbb{R}\) be continuously differentiable on an open set \(D \subseteq
    \mathbb{R}^d\). If \(\nabla f(\mathbf{x}_0) = \mathbf{0}\), we say that \(\mathbf{x}_0
    \in D\) is a stationary point of \(f\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Rayleight Quotient)** Let \(A \in \mathbb{R}^{d \times d}\)
    be a symmetric matrix. The associated Rayleigh quotient\(\idx{Rayleigh quotient}\xdi\)
    is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{R}_A(\mathbf{u}) = \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle
    \mathbf{u}, \mathbf{u} \rangle} \]
  prefs: []
  type: TYPE_NORMAL
- en: which is defined for any \(\mathbf{u} = (u_1,\ldots,u_d) \neq \mathbf{0}\) in
    \(\mathbb{R}^{d}\). As a function from \(\mathbb{R}^{d} \setminus \{\mathbf{0}\}\)
    to \(\mathbb{R}\), \(\mathcal{R}_A(\mathbf{u})\) is continuously differentiable.
    We find its stationary points.
  prefs: []
  type: TYPE_NORMAL
- en: We use the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule) and
    our previous results on the gradient of quadratic functions. Specifically, note
    that (using that \(A\) is symmetric)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial}{\partial u_i} \mathcal{R}_A(\mathbf{u}) &=
    \frac{\left(\frac{\partial}{\partial u_i} \langle \mathbf{u}, A \mathbf{u} \rangle\right)
    \langle \mathbf{u}, \mathbf{u} \rangle - \langle \mathbf{u}, A \mathbf{u} \rangle
    \left( \frac{\partial}{\partial u_i} \langle \mathbf{u}, \mathbf{u} \rangle\right)}{\langle
    \mathbf{u}, \mathbf{u} \rangle^2}\\ &= \frac{2\left(\frac{\partial}{\partial u_i}
    \frac{1}{2}\mathbf{u}^T A \mathbf{u}\right) \|\mathbf{u}\|^2 - \mathbf{u}^T A
    \mathbf{u} \left( \frac{\partial}{\partial u_i} \sum_{j=1}^d u_j^2\right)}{\|\mathbf{u}\|^4}\\
    &= \frac{2\left(A \mathbf{u}\right)_i \|\mathbf{u}\|^2 - \mathbf{u}^T A \mathbf{u}
    \left( 2 u_i \right)}{\|\mathbf{u}\|^4}\\ &= \frac{2}{\|\mathbf{u}\|^2}\left\{\left(A
    \mathbf{u}\right)_i - \mathcal{R}_A(\mathbf{u}) u_i \right\}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In vector form this is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla \mathcal{R}_A(\mathbf{u}) = \frac{2}{\|\mathbf{u}\|^2} \left\{A \mathbf{u}
    - \mathcal{R}_A(\mathbf{u}) \,\mathbf{u} \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The stationary points satisfy \(\nabla \mathcal{R}_A(\mathbf{u}) = \mathbf{0}\),
    or after getting rid of the denominator and rearranging,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{u} = \mathcal{R}_A(\mathbf{u}) \,\mathbf{u}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The solutions to this system are eigenvectors of \(A\), that is, they satisfy
    \(A\mathbf{u} = \lambda \mathbf{u}\) for some eigenvalue \(\lambda\). If \(\mathbf{q}_i\)
    is a unit eigenvector of \(A\) with eigenvalue \(\lambda_i\), then we have that
    \(\mathcal{R}_A(\mathbf{q}_i) = \lambda_i\) (Check it!) and
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{q}_i = \mathcal{R}_A(\mathbf{q}_i) \,\mathbf{q}_i = \lambda_i \mathbf{q}_i.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvectors of \(A\) are not in general local minimizers of its Rayleigh
    quotient. In fact one of them – the largest one – is a global maximizer! \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Second-order conditions[#](#second-order-conditions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local minimizers can also be characterized in terms of the Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: We will make use of *Taylor’s Theorem*, a generalization of the *Mean Value
    Theorem* that provides polynomial approximations to a function around a point.
    We restrict ourselves to the case of a linear approximation with second-order
    error term, which will suffice for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Taylor’s theorem** We begin by reviewing the single-variable case, which
    we will use to prove the general verison.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Taylor)** \(\idx{Taylor''s theorem}\xdi\) Let \(f: D \to \mathbb{R}\)
    where \(D \subseteq \mathbb{R}\). Suppose \(f\) has a continuous derivative on
    \([a,b]\) and that its second derivative exists on \((a,b)\). Then for any \(x
    \in [a, b]\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x) = f(a) + (x-a) f'(a) + \frac{1}{2} (x-a)^2 f''(\xi) \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(a < \xi < x\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: The third term on the right-hand side of *Taylor’s Theorem* is called the Lagrange
    remainder. It can be seen as an error term between \(f(x)\) and the linear approximation
    \(f(a) + (x-a) f'(a)\). There are [other forms](https://en.wikipedia.org/wiki/Taylor%27s_theorem#Explicit_formulas_for_the_remainder)
    for the remainder. The form we stated here is useful when one has a bound on the
    second derivative. Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Consider \(f(x) = e^x\). Then \(f''(x) = f''''(x) = e^x\).
    Suppose we are interested in approximating \(f\) in the interval \([0,1]\). We
    take \(a=0\) and \(b=1\) in *Taylor’s Theorem*. The linear term is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(a) + (x-a) f'(a) = 1 + x e^0 = 1 + x. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then for any \(x \in [0,1]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x) = 1 + x + \frac{1}{2}x^2 e^{\xi_x} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\xi_x \in (0,1)\) depends on \(x\). We get a uniform bound on the error
    over \([0,1]\) by replacing \(\xi_x\) with its worst possible value over \([0,1]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ |f(x) - (1+x)| \leq \frac{1}{2}x^2 e^{\xi_x} \leq \frac{e}{2} x^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If we plot the upper and lower bounds, we see that \(f\) indeed falls within
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/3679f4fd83bc012d44e92e99af6e84eb0f84c2d88f5be251891c9521ea8a7fc9.png](../Images/c339c626ad8f04a3c45b8a598e231b2a.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In the case of several variables, we again restrict ourselves to the second
    order. For the more general version, see e.g. [Wikipedia](https://en.wikipedia.org/wiki/Taylor's_theorem#Taylor's_theorem_for_multivariate_functions).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Taylor)** \(\idx{Taylor''s theorem}\xdi\) Let \(f : D \to \mathbb{R}\)
    where \(D \subseteq \mathbb{R}^d\). Let \(\mathbf{x}_0 \in D\) and \(\delta >
    0\) be such that \(B_\delta(\mathbf{x}_0) \subseteq D\). If \(f\) is twice continuously
    differentiable on \(B_\delta(\mathbf{x}_0)\), then for any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} -
    \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \,\mathbf{H}_f(\mathbf{x}_0
    + \xi (\mathbf{x} - \mathbf{x}_0)) \,(\mathbf{x} - \mathbf{x}_0), \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\xi \in (0,1)\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: As in the single-variable case, we think of \(f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T
    (\mathbf{x} - \mathbf{x}_0)\) for fixed \(\mathbf{x}_0\) as a linear – or more
    accurately affine – approximation to \(f\) at \(\mathbf{x}_0\). The third term
    on the right-hand side above quantifies the error of this approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We apply the single-variable result to \(\phi(t) = f(\boldsymbol{\alpha}(t))\).
    We use the *Chain Rule* to compute the needed derivatives.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(\mathbf{p} = \mathbf{x} - \mathbf{x}_0\) and \(\phi(t) = f(\boldsymbol{\alpha}(t))\)
    where \(\boldsymbol{\alpha}(t) = \mathbf{x}_0 + t \mathbf{p}\). Observe that \(\phi(0)
    = f(\mathbf{x}_0)\) and \(\phi(1) = f(\mathbf{x})\). As observed in the proof
    of the *Mean Value Theorem*, \(\phi''(t) = \nabla f(\boldsymbol{\alpha}(t))^T
    \mathbf{p}\). By the *Chain Rule* and our previous *Parametric Line Example*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \phi''(t) &= \frac{\mathrm{d}}{\mathrm{d} t} \left[\sum_{i=1}^d
    \frac{\partial f(\boldsymbol{\alpha}(t))}{\partial x_i} p_i \right]\\ &= \sum_{i=1}^d
    \left(\nabla \frac{\partial f(\boldsymbol{\alpha}(t))}{\partial x_i}\right)^T
    \boldsymbol{\alpha}'(t) \,p_i \\ &= \sum_{i=1}^d \sum_{j=1}^d \frac{\partial^2
    f(\boldsymbol{\alpha}(t))}{\partial x_j \partial x_i} p_j p_i\\ &= \mathbf{p}^T
    \,\mathbf{H}_f(\mathbf{x}_0 + t \mathbf{p}) \,\mathbf{p}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, \(\phi\) has continuous first and second derivatives on \([0,1]\).
  prefs: []
  type: TYPE_NORMAL
- en: By *Taylor’s Theorem* in the single-variable case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi(t) = \phi(0) + t \phi'(0) + \frac{1}{2} t^2 \phi''(\xi) \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\xi \in (0,t)\). Plugging in the expressions for \(\phi(0)\), \(\phi'(0)\)
    and \(\phi''(\xi)\) and taking \(t=1\) gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider the function \(f(x_1, x_2) = x_1 x_2 + x_1^2 + e^{x_1}
    \cos x_2\). We apply *Taylor’s Theorem* with \(\mathbf{x}_0 = (0, 0)\) and \(\mathbf{x}
    = (x_1, x_2)\). The gradient is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(x_1, x_2) = (x_2 + 2 x_1 + e^{x_1} \cos x_2, x_1 - e^{x_1} \sin
    x_2 ) \]
  prefs: []
  type: TYPE_NORMAL
- en: and the Hessian is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{H}_f(x_1, x_2) = \begin{pmatrix} 2 + e^{x_1} \cos x_2
    & 1 - e^{x_1} \sin x_2\\ 1 - e^{x_1} \sin x_2 & - e^{x_1} \cos x_2 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: So \(f(0,0) = 1\) and \(\nabla f(0,0) = (1, 0)\). Thus, by *Taylor’s Theorem*,
    there is \(\xi \in (0,1)\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x_1, x_2) = 1 + x_1 + \frac{1}{2}[2 x_1^2 + 2 x_1 x_2 + (x_1^2 - x_2^2)
    \,e^{\xi x_1} \cos(\xi x_2) - 2 x_1 x_2 e^{\xi x_1} \sin(\xi x_2)]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Second directional derivative** To control the error term in *Taylor’s Theorem*,
    it will be convenient to introduce a notion of second directional derivative.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Second Directional Derivative)** \(\idx{second directional
    derivative}\xdi\) Let \(f : D \to \mathbb{R}\) where \(D \subseteq \mathbb{R}^d\),
    let \(\mathbf{x}_0 \in D\) be an interior point of \(D\) and let \(\mathbf{v}
    \in \mathbb{R}^d\) be a nonzero vector. The second directional derivative of \(f\)
    at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f (\mathbf{x}_0)}{\partial \mathbf{v}^2} = \lim_{h \to 0}
    \frac{1}{h} \left[\frac{\partial f(\mathbf{x}_0 + h \mathbf{v})}{\partial \mathbf{v}}
    - \frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}}\right] \]
  prefs: []
  type: TYPE_NORMAL
- en: provided the limit exists. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Typically, \(\mathbf{v}\) is a unit vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Second Directional Derivative and Hessian)** \(\idx{second directional
    derivative and Hessian theorem}\xdi\) Let \(f : D \to \mathbb{R}\) where \(D \subseteq
    \mathbb{R}^d\), let \(\mathbf{x}_0 \in D\) be an interior point of \(D\) and let
    \(\mathbf{v} \in \mathbb{R}^d\) be a vector. Assume that \(f\) is twice continuously
    differentiable at \(\mathbf{x}_0\). Then the second directional derivative of
    \(f\) at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f (\mathbf{x}_0)}{\partial \mathbf{v}^2} = \mathbf{v}^T
    H_f(\mathbf{x}_0) \,\mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: Note the similarity to the quadratic term in *Taylor’s Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We have already done this calculation in the proof of *Taylor’s
    Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Then, by definition of the derivative,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \lim_{h \to 0} \frac{1}{h} \left[\frac{\partial f(\mathbf{x}_0
    + h \mathbf{v})}{\partial \mathbf{v}} - \frac{\partial f(\mathbf{x}_0)}{\partial
    \mathbf{v}}\right] &= \lim_{h \to 0} \frac{1}{h} \left[\nabla f(\mathbf{x}_0 +
    h \mathbf{v})^T \mathbf{v} - \nabla f(\mathbf{x}_0)^T \mathbf{v}\right]\\ &= \lim_{h
    \to 0} \frac{1}{h} \sum_{i=1}^n v_i \left[\frac{\partial f(\mathbf{x}_0 + h \mathbf{v})}{\partial
    x_i} - \frac{\partial f(\mathbf{x}_0)}{\partial x_i} \right]\\ &= \sum_{i=1}^n
    v_i \lim_{h \to 0} \frac{1}{h} \left[\frac{\partial f(\mathbf{x}_0 + h \mathbf{v})}{\partial
    x_i} - \frac{\partial f(\mathbf{x}_0)}{\partial x_i} \right]\\ &= \sum_{i=1}^n
    v_i \frac{\partial g_i (\mathbf{x}_0)}{\partial \mathbf{v}}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(g_i(\mathbf{x}_0) = \frac{\partial f(\mathbf{x}_0)}{\partial x_i}\).
    So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial g_i (\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla g_i(\mathbf{x}_0)^T
    \mathbf{v} = \sum_{j=1}^n v_j \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_i\partial
    x_j} \]
  prefs: []
  type: TYPE_NORMAL
- en: by the *Directional Derivative and Gradient Theorem*. Plugging back above we
    get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 f (\mathbf{x}_0)}{\partial \mathbf{v}^2} = \sum_{i=1}^n
    v_i \sum_{j=1}^n v_j \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_i\partial x_j}
    = \mathbf{v}^T H_f(\mathbf{x}_0) \,\mathbf{v}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: So going back to *Taylor’s Theorem*
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} -
    \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T \,\mathbf{H}_f(\mathbf{x}_0
    + \xi (\mathbf{x} - \mathbf{x}_0)) \,(\mathbf{x} - \mathbf{x}_0), \]
  prefs: []
  type: TYPE_NORMAL
- en: we see that the second term on the right-hand side is the directional derivative
    at \(\mathbf{x}_0\) in the direction \(\mathbf{x} - \mathbf{x}_0\) and that the
    third term is half of the second directional derivative at \(\mathbf{x}_0 + \xi
    (\mathbf{x} - \mathbf{x}_0)\) in the same direction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Necessary condition** When \(f\) is twice continuously differentiable, we
    get a necessary condition based on the Hessian.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Second-Order Necessary Optimality Condition)** \(\idx{second-order
    necessary optimality condition}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be twice continuously differentiable on \(\mathbb{R}^d\). If \(\mathbf{x}_0\)
    is a local minimizer, then \(\nabla f(\mathbf{x}_0) = \mathbf{0}\) and \(\mathbf{H}_f(\mathbf{x}_0)\)
    is positive semidefinite. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* By *Taylor’s Theorem* and the *First-Order Necessary Optimality
    Condition*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \nabla
    f(\mathbf{x}_0)^T(\alpha \mathbf{v}) + \frac{1}{2}(\alpha \mathbf{v})^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) (\alpha \mathbf{v})\\ &= f(\mathbf{x}_0) + \frac{1}{2}\alpha^2
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(\mathbf{H}_f\) is positive semidefinite in a neighborhood around \(\mathbf{x}_0\),
    then the second term on the right-hand side is nonnegative, which is necessary
    for \(\mathbf{x}_0\) to be a local minimizer. Formally we argue by contradiction:
    indeed, if \(\mathbf{H}_f\) is not positive semidefinite, then there must exists
    a direction in which the second directional derivative is negative; since the
    gradient is \(\mathbf{0}\) at \(\mathbf{x}_0\), intuitively the directional derivative
    must become negative in that direction as well and the function must decrease.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We argue by contradiction. Suppose that \(\mathbf{H}_f(\mathbf{x}_0)\)
    is not positive semidefinite. By definition, there must be a unit vector \(\mathbf{v}\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}, \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} \rangle = - \eta
    < 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, \(\mathbf{v}\) is a direction in which the second directional derivative
    is negative.
  prefs: []
  type: TYPE_NORMAL
- en: For \(\alpha > 0\), *Taylor’s Theorem* implies that there is \(\xi_\alpha \in
    (0,1)\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \nabla
    f(\mathbf{x}_0)^T(\alpha \mathbf{v}) + \frac{1}{2} (\alpha \mathbf{v})^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) (\alpha \mathbf{v})\\ &= f(\mathbf{x}_0) + \frac{1}{2}
    \alpha^2 \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v}
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used \(\nabla f(\mathbf{x}_0) = \mathbf{0}\) by the *First-Order Necessary
    Optimality Condition*. We want to show that the second term on the right-hand
    side is negative.
  prefs: []
  type: TYPE_NORMAL
- en: The Hessian is continuous (in the sense that all its entries are continuous
    functions of \(\mathbf{x}\)). In particular, the second directional derivative
    \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}) \,\mathbf{v}\) is continuous as a linear
    combination of continuous functions. So, by definition of continuity, for any
    \(\epsilon > 0\) – say \(\epsilon = \eta/2\) – there is \(\delta > 0\) small enough
    that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left| \mathbf{v}^T \mathbf{H}_f(\mathbf{x}) \,\mathbf{v} -
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} \right| &< \eta/2 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: Take \(\alpha^* > 0\) small enough that \(\mathbf{x}_0 + \alpha^* \mathbf{v}
    \in B_\delta(\mathbf{x}_0)\). Then, for all \(\alpha \in (0,\alpha^*)\), whatever
    \(\xi_\alpha \in (0,1)\) is, it holds that \(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v} \in B_\delta(\mathbf{x}_0)\). Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha
    \mathbf{v}) \,\mathbf{v} &= \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v}
    + (\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v}) \,\mathbf{v}
    - \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} )\\ &\leq \mathbf{v}^T
    \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} + |\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) \,\mathbf{v} - \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0)
    \,\mathbf{v}|\\ &< -\eta + \eta/2\\ &< - \eta/2 < 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by definition of \(\eta\). That implies
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}_0 + \alpha \mathbf{v}) < f(\mathbf{x}_0) - \alpha^2 \eta/4 <
    f(\mathbf{x}_0). \]
  prefs: []
  type: TYPE_NORMAL
- en: Since this holds for all sufficiently small \(\alpha\), every open ball around
    \(\mathbf{x}_0\) has a point achieving a lower value than \(f(\mathbf{x}_0)\).
    Thus \(\mathbf{x}_0\) is not a local minimizer, a contradiction. So it must be
    that \(\mathbf{H}_f(\mathbf{x}_0) \succeq \mathbf{0}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Sufficient condition** The necessary condition above is not in general sufficient,
    as the following example shows.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Let \(f(x) = x^3\). Then \(f''(x) = 3 x^2\) and \(f''''(x)
    = 6 x\) so that \(f''(0) = 0\) and \(f''''(0) \geq 0\). Hence \(x=0\) is a stationary
    point. But \(x=0\) is not a local minimizer. Indeed \(f(0) = 0\) but, for any
    \(\delta > 0\), \(f(-\delta) < 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/2124857c662385a175abf776e89126ff842e450579382110f506759f09254a95.png](../Images/4a1cd8247ab18de84607516e9c6f7863.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We give sufficient conditions for a point to be a local minimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Second-Order Sufficient Optimality Condition)** \(\idx{second-order
    sufficient optimality condition}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\)
    be twice continuously differentiable on \(\mathbb{R}^d\). If \(\nabla f(\mathbf{x}_0)
    = \mathbf{0}\) and \(\mathbf{H}_f(\mathbf{x}_0)\) is positive definite, then \(\mathbf{x}_0\)
    is a strict local minimizer. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use *Taylor’s Theorem* again. This time we use the positive
    definiteness of the Hessian to bound the value of the function from below.'
  prefs: []
  type: TYPE_NORMAL
- en: We will need a lemma.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Quadratic Form and Frobenius Norm)** \(\idx{quadratic form and
    Frobenius norm lemma}\xdi\) Let \(A = (a_{i,j})_{i,j}\) and \(B = (b_{i,j})_{i,j}\)
    be matrices in \(\mathbb{R}^{n \times m}\). For any unit vectors \(\mathbf{u}
    \in \mathbb{R}^n\) and \(\mathbf{v} \in \mathbb{R}^m\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left| \mathbf{u}^T A \,\mathbf{v} - \mathbf{u}^T B \,\mathbf{v} \right|
    \leq \|A - B\|_F. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Cauchy-Schwarz inequality*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left| \mathbf{u}^T A \,\mathbf{v} - \mathbf{u}^T B \,\mathbf{v}
    \right| &= \left| \sum_{i=1}^n \sum_{j=1}^m u_i v_j (a_{i,j} - b_{i,j}) \right|\\
    &\leq \sqrt{\sum_{i=1}^n \sum_{j=1}^m u_i^2 v_j^2} \sqrt{\sum_{i=1}^n \sum_{j=1}^m
    (a_{i,j} - b_{i,j})^2}\\ &= \|A - B\|_F, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\mathbf{u}\) and \(\mathbf{v}\) have unit norm on the last
    line. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Second-Order Sufficient Optimality Condition)* By *Taylor’s Theorem*,
    for all unit vectors \(\mathbf{v} \in \mathbb{R}^d\) and \(\alpha \in \mathbb{R}\),
    there is \(\xi_{\alpha} \in (0,1)\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \nabla
    f(\mathbf{x}_0)^T(\alpha \mathbf{v}) + \frac{1}{2}(\alpha \mathbf{v})^T \mathbf{H}_f(\mathbf{x}_0
    + \xi_\alpha \alpha \mathbf{v}) (\alpha \mathbf{v})\\ &= f(\mathbf{x}_0) + \frac{1}{2}\alpha^2
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v},
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\nabla f(\mathbf{x}_0) = \mathbf{0}\). The second term
    on the last line is \(0\) at \(\mathbf{v} = \mathbf{0}\). Our goal is to show
    that it is strictly positive (except at \(\mathbf{0}\)) in a neighborhood of \(\mathbf{0}\).
  prefs: []
  type: TYPE_NORMAL
- en: The set \(\mathbb{S}^{d-1}\) of unit vectors in \(\mathbb{R}^d\) is closed and
    bounded. The expression \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v}\),
    viewed as a function of \(\mathbf{v}\), is continuous since it is a polynomial.
    Hence, by the *Extreme Value Theorem*, it attains its minimum on \(\mathbb{S}^{d-1}\).
    By our assumption that \(\mathbf{H}_f(\mathbf{x}_0)\) is positive definite, that
    minimum must be strictly positive, say \(\mu > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: By the *Quadratic Form and Frobenius Norm Lemma* (ignoring the absolute value),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} - \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\,\mathbf{v} \leq \|\mathbf{H}_f(\mathbf{x}_0) - \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\|_F. \]
  prefs: []
  type: TYPE_NORMAL
- en: The Frobenius norm above is continuous in \(\mathbf{w}\) as a composition of
    continuous functions. Moreover, we have at \(\mathbf{w} = \mathbf{0}\) that this
    Frobenius norm is \(0\). Hence, by definition of continuity, for any \(\epsilon
    > 0\) – say \(\epsilon := \mu/2\) – there is \(\delta > 0\) such that \(\mathbf{w}
    \in B_{\delta}(\mathbf{0})\) implies \(\|\mathbf{H}_f(\mathbf{x}_0) - \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\|_F < \epsilon = \mu/2\).
  prefs: []
  type: TYPE_NORMAL
- en: Since \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} > \mu\), the inequality
    in the previous display implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \mathbf{w})\,\mathbf{v} \geq \mathbf{v}^T
    \mathbf{H}_f(\mathbf{x}_0) \,\mathbf{v} - \|\mathbf{H}_f(\mathbf{x}_0) - \mathbf{H}_f(\mathbf{x}_0
    + \mathbf{w})\|_F > \frac{\mu}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This holds for any unit vector \(\mathbf{v}\) and any \(\mathbf{w} \in B_{\delta}(\mathbf{0})\).
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our Taylor expansion, for \(\alpha > 0\) small enough (not depending
    on \(\mathbf{v}\); why?), it holds that \(\mathbf{w} = \xi_\alpha \alpha \mathbf{v}
    \in B_{\delta}(\mathbf{0})\) so that we get from the previous inequality
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{x}_0 + \alpha \mathbf{v}) &= f(\mathbf{x}_0) + \frac{1}{2}\alpha^2
    \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0 + \xi_\alpha \alpha \mathbf{v})\,\mathbf{v}\\
    &> f(\mathbf{x}_0) + \frac{1}{4} \alpha^2 \mu \\ &> f(\mathbf{x}_0). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore \(\mathbf{x}_0\) is a strict local minimizer. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Adding equality constraints[#](#adding-equality-constraints "Link to
    this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we have considered *unconstrained* optimization problems, that is,
    the variable \(\mathbf{x}\) can take any value in \(\mathbb{R}^d\). However, it
    is common to impose conditions on \(\mathbf{x}\). Hence, we consider the *constrained*\(\idx{constrained
    optimization}\xdi\) minimization problem
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathscr{X}} f(\mathbf{x}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathscr{X} \subset \mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** For instance, the entries of \(\mathbf{x}\) may have to satisfy
    certain bounds. In that case, we would have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathscr{X} = \{\mathbf{x} = (x_1,\ldots,x_d) \in \mathbb{R}^d:x_i \in [a_i,
    b_i], \forall i\} \]
  prefs: []
  type: TYPE_NORMAL
- en: for some constants \(a_i < b_i\), \(i=1,\ldots,d\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In this more general problem, the notion of global and local minimizer can be
    adapted straightforwardly. Note that, for simplicity, we will assume that \(f\)
    is defined over all of \(\mathbb{R}^d\). When \(\mathbf{x} \in \mathscr{X}\),
    it is said to be feasible.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Global minimizer)** \(\idx{global minimizer or maximizer}\xdi\)
    Let \(f : \mathbb{R}^d \to \mathbb{R}\). The point \(\mathbf{x}^* \in \mathscr{X}\)
    is a global minimizer of \(f\) over \(\mathscr{X}\) if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) \geq f(\mathbf{x}^*), \quad \forall \mathbf{x} \in \mathscr{X}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Local minimizer)** \(\idx{local minimizer or maximizer}\xdi\)
    Let \(f : \mathbb{R}^d \to \mathbb{R}\). The point \(\mathbf{x}^* \in \mathscr{X}\)
    is a local minimizer of \(f\) over \(\mathscr{X}\) if there is \(\delta > 0\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) \geq f(\mathbf{x}^*), \quad \forall \mathbf{x} \in (B_{\delta}(\mathbf{x}^*)
    \setminus \{\mathbf{x}^*\}) \cap \mathscr{X}. \]
  prefs: []
  type: TYPE_NORMAL
- en: If the inequality is strict, we say that \(\mathbf{x}^*\) is a strict local
    minimizer. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this subsection, we restrict ourselves to one important class of constraints:
    equality constraints. That is, we consider the minimization problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\text{min} f(\mathbf{x})\\ &\text{s.t.}\ h_i(\mathbf{x}) =
    0,\ \forall i \in [\ell] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'where s.t. stands for “subject to”. In other words, we only allow those \(\mathbf{x}''s\)
    such that \(h_i(\mathbf{x}) = 0\) for all \(i\). Here \(f : \mathbb{R}^d \to \mathbb{R}\)
    and \(h_i : \mathbb{R}^d \to \mathbb{R}\), \(i\in [\ell]\). We will sometimes
    use the notation \(\mathbf{h} : \mathbb{R}^d \to \mathbb{R}^\ell\), where \(\mathbf{h}(\mathbf{x})
    = (h_1(\mathbf{x}), \ldots, h_\ell(\mathbf{x}))\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** If we want to minimize \(2 x_1^2 + 3 x_2^2\) over all two-dimensional
    unit vectors \(\mathbf{x} = (x_1, x_2)\), then we can let'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = 2 x_1^2 + 3 x_2^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_1(\mathbf{x}) = 1 - x_1^2 - x_2^2 = 1 - \|\mathbf{x}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Observe that we could have chosen a different equality constraint to express
    the same minimization problem. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The following theorem generalizes the *First-Order Necessary Optimality Condition*.
    The proof is omitted.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Lagrange Multipliers)** \(\idx{Lagrange multipliers theorem}\xdi\)
    Assume \(f : \mathbb{R}^d \to \mathbb{R}\) and \(h_i : \mathbb{R}^d \to \mathbb{R}\),
    \(i\in [\ell]\), are continuously differentiable. Let \(\mathbf{x}^*\) be a local
    minimizer of \(f\) s.t. \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\). Assume further
    that the vectors \(\nabla h_i (\mathbf{x}^*)\), \(i \in [\ell]\), are linearly
    independent. Then there exists a unique vector'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \blambda^* = (\lambda_1^*, \ldots, \lambda_\ell^*) \]
  prefs: []
  type: TYPE_NORMAL
- en: satisfying
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \nabla h_i(\mathbf{x}^*)
    = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: The quantities \(\lambda_1^*, \ldots, \lambda_\ell^*\) are called Lagrange multipliers\(\idx{Lagrange
    multipliers}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** Returning to the previous example,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}) = \left( \frac{\partial f(\mathbf{x})}{\partial x_1},
    \frac{\partial f(\mathbf{x})}{\partial x_2} \right) = (4 x_1, 6 x_2) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla h_1(\mathbf{x}) = \left( \frac{\partial h_1(\mathbf{x})}{\partial
    x_1}, \frac{\partial h_1(\mathbf{x})}{\partial x_2} \right) = (- 2 x_1, - 2 x_2).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The conditions in the theorem read
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &4 x_1 - 2 \lambda_1 x_1 = 0\\ &6 x_2 - 2 \lambda_1 x_2 = 0.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The constraint \(x_1^2 + x_2^2 = 1\) must also be satisfied. Observe that the
    linear independence condition is automatically satisfied since there is only one
    constraint.
  prefs: []
  type: TYPE_NORMAL
- en: There are several cases to consider.
  prefs: []
  type: TYPE_NORMAL
- en: 1- If neither \(x_1\) nor \(x_2\) is \(0\), then the first equation gives \(\lambda_1
    = 2\) while the second one gives \(\lambda_1 = 3\). So that case cannot happen.
  prefs: []
  type: TYPE_NORMAL
- en: 2- If \(x_1 = 0\), then \(x_2 = 1\) or \(x_2 = -1\) by the constraint and the
    second equation gives \(\lambda_1 = 3\) in either case.
  prefs: []
  type: TYPE_NORMAL
- en: 3- If \(x_2 = 0\), then \(x_1 = 1\) or \(x_1 = -1\) by the constraint and the
    first equation gives \(\lambda_1 = 2\) in either case.
  prefs: []
  type: TYPE_NORMAL
- en: Does any of these last four solutions, i.e., \((x_1,x_2,\lambda_1) = (0,1,3)\),
    \((x_1,x_2,\lambda_1) = (0,-1,3)\), \((x_1,x_2,\lambda_1) = (1,0,2)\) and \((x_1,x_2,\lambda_1)
    = (-1,0,2)\), actually correspond to a local minimizer?
  prefs: []
  type: TYPE_NORMAL
- en: This problem can be solved manually. Indeed, replace \(x_2^2 = 1 - x_1^2\) into
    the objective function to obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[ 2 x_1^2 + 3(1 - x_1^2) = -x_1^2 + 3. \]
  prefs: []
  type: TYPE_NORMAL
- en: This is minimized for the largest value that \(x_1^2\) can take, namely when
    \(x_1 = 1\) or \(x_1 = -1\). Indeed, we must have \(0 \leq x_1^2 \leq x_1^2 +
    x_2^2 = 1\). So both \((x_1, x_2) = (1,0)\) and \((x_1, x_2) = (-1,0)\) are global
    minimizers. A fortiori, they must be local minimizers.
  prefs: []
  type: TYPE_NORMAL
- en: What about \((x_1,x_2) = (0,1)\) and \((x_1,x_2) = (0,-1)\)? Arguing as above,
    they in fact correspond to global *maximizers* of the objective function. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Assume \(\mathbf{x}\) is feasible, that is, \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\).
    We let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathscr{F}_{\mathbf{h}}(\mathbf{x}) = \left\{ \mathbf{v} \in \mathbb{R}^d
    \,:\, \nabla h_i(\mathbf{x})^T \mathbf{v} = \mathbf{0},\ \forall i \in [\ell]
    \right\} \]
  prefs: []
  type: TYPE_NORMAL
- en: be the linear subspace of first-order feasible directions\(\idx{first-order
    feasible directions}\xdi\) at \(\mathbf{x}\). To explain the name, note that by
    a first-order Taylor expansion, if \(\mathbf{v} \in \mathscr{F}_{\mathbf{h}}(\mathbf{x})\)
    then it holds that
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_i(\mathbf{x} + \delta \mathbf{v}) \approx h_i(\mathbf{x}) + \delta \nabla
    h_i(\mathbf{x})^T \mathbf{v} = \mathbf{0} \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: The theorem says that, if \(\mathbf{x}^*\) is a local minimizer, then the gradient
    of \(f\) is orthogonal to the set of first-order feasible directions at \(\mathbf{x}^*\).
    Indeed, any \(\mathbf{v} \in \mathscr{F}_{\mathbf{h}}(\mathbf{x}^*)\) satisfies
    by the theorem that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}^*)^T \mathbf{v} = \left(- \sum_{i=1}^\ell \lambda^*_i
    \nabla h_i(\mathbf{x}^*)\right)^T \mathbf{v} = - \sum_{i=1}^\ell \lambda^*_i \nabla
    h_i(\mathbf{x}^*)^T \mathbf{v} = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, following a first-order feasible direction does not alter the objective
    function value up to second-order error
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}^* + \alpha \mathbf{v}) \approx f(\mathbf{x}^*) + \alpha \nabla
    f(\mathbf{x}^*)^T \mathbf{v} = f(\mathbf{x}^*). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Returning to the previous example, the points satisfying
    \(h_1(\mathbf{x}) = 0\) sit on the circle of radius \(1\) around the origin. We
    have already seen that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla h_1(\mathbf{x}) = \left( \frac{\partial h_1(\mathbf{x})}{\partial
    x_1}, \frac{\partial h_1(\mathbf{x})}{\partial x_2} \right) = (- 2 x_1, - 2 x_2).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Here is code illustrating the theorem (with help from ChatGPT). We first compute
    the function \(h_1\) at a grid of points using [`numpy.meshgrid`](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We use [`matplotlib.pyplot.contour`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html)
    to plot the constraint set as a [contour line](https://en.wikipedia.org/wiki/Contour_line)
    (for the constant value \(0\)) of \(h_1\). Gradients of \(h_1\) are plotted at
    a collection of `points` with the [`matplotlib.pyplot.quiver`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html)
    function, which is used for plotting vectors as arrows. We see that the directions
    of first-order feasible directions are orthogonal to the arrows, and therefore
    are tangent to the constraint set.
  prefs: []
  type: TYPE_NORMAL
- en: At those same `points`, we also plot the gradient of \(f\), which recall is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}) = \left( \frac{\partial f(\mathbf{x})}{\partial x_1},
    \frac{\partial f(\mathbf{x})}{\partial x_2} \right) = (4 x_1, 6 x_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: We make all gradients into unit vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/a5aed8b6b668174566d2d532e4b400fcf5707acf3befc2f4f7b86d23fc3b68a5.png](../Images/b947d14ae52253453dee08ef202017c5.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that, at \((-1,0)\) and \((1,0)\), the gradient is indeed orthogonal
    to the first-order feasible directions.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: A feasible vector \(\mathbf{x}\) is said to be regular if the vectors \(\nabla
    h_i (\mathbf{x}^*)\), \(i \in [\ell]\), are linearly independent. We re-formulate
    the previous theorem in terms of the Lagrangian function, which is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ L(\mathbf{x}, \blambda) = f(\mathbf{x}) + \sum_{i=1}^\ell \lambda_i h_i(\mathbf{x}),
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\blambda = (\lambda_1,\ldots,\lambda_\ell)\). Then, by the theorem,
    a regular local minimizer satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\nabla_{\mathbf{x}} L(\mathbf{x}, \blambda) = \mathbf{0}\\
    &\nabla_{\blambda} L(\mathbf{x}, \blambda) = \mathbf{0}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Here the notation \(\nabla_{\mathbf{x}}\) (respectively \(\nabla_{\blambda}\))
    indicates that we are taking the vector of partial derivatives with respect to
    only the variables in \(\mathbf{x}\) (respectively \(\blambda\)).
  prefs: []
  type: TYPE_NORMAL
- en: To see that these equations hold, note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla_{\mathbf{x}} L(\mathbf{x}, \blambda) = \nabla f(\mathbf{x}) + \sum_{i=1}^\ell
    \lambda_i \nabla h_i(\mathbf{x}) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla_{\blambda} L(\mathbf{x}, \blambda) = \mathbf{h}(\mathbf{x}). \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(\nabla_{\mathbf{x}} L(\mathbf{x}, \blambda) = \mathbf{0}\) is a restatement
    of the Lagrange multipliers condition and \(\nabla_{\blambda} L(\mathbf{x}, \blambda)
    = \mathbf{0}\) is a restatement of feasibility. Together, they form a system of
    \(d + \ell\) equations in \(d + \ell\) variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider the constrained minimization problem on \(\mathbb{R}^3\)
    where the objective function is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = \frac{1}{2}(x_1^2 + x_2^2 + x_3^2) \]
  prefs: []
  type: TYPE_NORMAL
- en: and the only constraint function is
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_1(\mathbf{x}) = 3 - x_1 - x_2 - x_3. \]
  prefs: []
  type: TYPE_NORMAL
- en: The gradients are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{x}) = (x_1, x_2, x_3) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla h_1(\mathbf{x}) = (-1, -1, -1). \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular, regularity is always satisfied since there is only one non-zero
    vector to consider.
  prefs: []
  type: TYPE_NORMAL
- en: So we are looking for solutions to the system of equations
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &x_1 - \lambda_1 = 0\\ &x_2 - \lambda_1 = 0\\ &x_3 - \lambda_1
    = 0\\ &3 - x_1 - x_2 - x_3 = 0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The first three equations imply that \(x_1 = x_2 = x_3 = \lambda\). Replacing
    in the fourth equation gives \(3 - 3 \lambda_1 = 0\) so \(\lambda_1 = 1\). Hence,
    \(x_1 = x_2 = x_3 = 1\) and this is the only solution.
  prefs: []
  type: TYPE_NORMAL
- en: So any local minimizer, if it exists, must be the vector \((1,1,1)\) with Lagrange
    multiplier \(1\). How can we know for sure whether this is the case? \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: As in the unconstrained case, there are *sufficient* conditions. As in that
    case as well, they involve second-order derivatives. We give one such theorem
    next without proof.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** Assume \(f : \mathbb{R}^d \to \mathbb{R}\) and \(h_i : \mathbb{R}^d
    \to \mathbb{R}\), \(i\in [\ell]\), are twice continuously differentiable. Let
    \(\mathbf{x}^* \in \mathbb{R}^d\) and \(\blambda^* \in \mathbb{R}^\ell\) satisfy'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \nabla
    h_i(\mathbf{x}^*) &= \mathbf{0}\\ \mathbf{h}(\mathbf{x}^*) &= \mathbf{0} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T\left( \mathbf{H}_f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i
    \mathbf{H}_{h_i}(\mathbf{x}^*) \right) \mathbf{v} > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(\mathbf{v} \in \mathscr{F}_{\mathbf{h}}(\mathbf{x})\). Then \(\mathbf{x}^*\)
    a strict local minimizer of \(f\) s.t. \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\).
    \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** We return to the previous example. We found a
    unique solution'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (x_1^*, x_2^*, x_3^*, \lambda_1^*) = (1,1,1,1) \]
  prefs: []
  type: TYPE_NORMAL
- en: to the system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \nabla
    h_i(\mathbf{x}^*) &= \mathbf{0}\\ \mathbf{h}(\mathbf{x}^*) &= \mathbf{0} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To check the second-order condition, we need the Hessians. It is straighforward
    to compute the second-order partial derivatives, which do not depend on \(\mathbf{x}\).
    We obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_{f}(\mathbf{x}) = I_{3 \times 3} \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_{h_1}(\mathbf{x}) = \mathbf{0}_{3 \times 3}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i \mathbf{H}_{h_i}(\mathbf{x}^*)
    = I_{3 \times 3} \]
  prefs: []
  type: TYPE_NORMAL
- en: and it follows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^T\left( \mathbf{H}_f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda^*_i
    \mathbf{H}_{h_i}(\mathbf{x}^*) \right) \mathbf{v} = \mathbf{v}^T I_{3 \times 3}
    \mathbf{v} = \|\mathbf{v}\|^2 > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: for any non-zero vector, including those in \(\mathscr{F}_{\mathbf{h}}(\mathbf{x})\).
  prefs: []
  type: TYPE_NORMAL
- en: It follows from the previous theorem that \(\mathbf{x}^*\) is a strict local
    minimizer of the constrained problem. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is the correct definition of a global minimizer
    \(\mathbf{x}^*\) of a function \(f: \mathbb{R}^d \to \mathbb{R}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(f(\mathbf{x}) \geq f(\mathbf{x}^*)\) for all \(x\) in some open ball around
    \(\mathbf{x}^*\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(f(\mathbf{x}) \geq f(\mathbf{x}^*)\) for all \(\mathbf{x} \in \mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\nabla f(\mathbf{x}^*) = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{v}^T \mathbf{H}_f(\mathbf{x}^*) \mathbf{v} > 0\) for all \(\mathbf{v}
    \in \mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Let \(f: \mathbb{R}^d \to \mathbb{R}\) be continuously differentiable
    at \(\mathbf{x}_0\). The directional derivative of \(f\) at \(\mathbf{x}_0\) in
    the direction \(\mathbf{v} \in \mathbb{R}^d\) is NOT given by:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \nabla f(\mathbf{x}_0)^T
    \mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \mathbf{v}^T \nabla
    f(\mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \mathbf{v}^T \mathbf{H}_f(\mathbf{x}_0)
    \mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}} = \lim_{h \to 0} \frac{f(\mathbf{x}_0
    + h\mathbf{v}) - f(\mathbf{x}_0)}{h}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Let \(f: \mathbb{R}^d \to \mathbb{R}\) be twice continuously differentiable.
    If \(\nabla f(\mathbf{x}_0) = 0\) and \(\mathbf{H}_f(\mathbf{x}_0)\) is positive
    definite, then \(\mathbf{x}_0\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: a) A global minimizer of \(f\).
  prefs: []
  type: TYPE_NORMAL
- en: b) A local minimizer of \(f\), but not necessarily a strict local minimizer.
  prefs: []
  type: TYPE_NORMAL
- en: c) A strict local minimizer of \(f\).
  prefs: []
  type: TYPE_NORMAL
- en: d) A saddle point of \(f\).
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Consider the optimization problem \(\min_\mathbf{x} f(\mathbf{x})\) subject
    to \(\mathbf{h}(\mathbf{x}) = 0\), where \(f: \mathbb{R}^d \to \mathbb{R}\) and
    \(h: \mathbb{R}^d \to \mathbb{R}^\ell\) are continuously differentiable. Let \(\mathbf{x}^*\)
    be a local minimizer and assume that the vectors \(\nabla h_i(\mathbf{x}^*), i
    \in [\ell]\), are linearly independent. According to the Lagrange Multipliers
    theorem, which of the following must be true?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\nabla f(\mathbf{x}^*) = \mathbf{0}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\nabla f(\mathbf{x}^*) + \sum_{i=1}^\ell \lambda_i^* \nabla h_i(\mathbf{x}^*)
    = \mathbf{0}\) for some \(\lambda^* \in \mathbb{R}^\ell\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{h}(\mathbf{x}^*) = \mathbf{0}\).
  prefs: []
  type: TYPE_NORMAL
- en: d) Both b and c.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following is a correct statement of Taylor’s Theorem (to
    second order) for a twice continuously differentiable function \(f: D \to \mathbb{R}\),
    where \(D \subseteq \mathbb{R}^d\), at an interior point \(\mathbf{x}_0 \in D\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2}(\mathbf{x}
    - \mathbf{x}_0)^T \mathbf{H}_f(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))(\mathbf{x}
    - \mathbf{x}_0)\) for some \(\xi \in (0,1)\).
  prefs: []
  type: TYPE_NORMAL
- en: b) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \nabla f(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))^T (\mathbf{x} - \mathbf{x}_0)
    + \frac{1}{2}(\mathbf{x} - \mathbf{x}_0)^T \mathbf{H}_f(\mathbf{x}_0 + \xi(\mathbf{x}
    - \mathbf{x}_0))(\mathbf{x} - \mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: c) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \nabla f(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))^T (\mathbf{x} - \mathbf{x}_0)\).
  prefs: []
  type: TYPE_NORMAL
- en: d) For any \(\mathbf{x} \in B_\delta(\mathbf{x}_0)\), \(f(\mathbf{x}) = f(\mathbf{x}_0)
    + \frac{1}{2}(\mathbf{x}_0 + \xi(\mathbf{x} - \mathbf{x}_0))^T \mathbf{H}_f(\mathbf{x}_0)(\mathbf{x}_0
    + \xi(\mathbf{x} - \mathbf{x}_0))\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.1: b. Justification: The text states that “The point \(\mathbf{x}^*
    \in \mathbb{R}^d\) is a global minimizer of \(f\) over \(\mathbb{R}^d\) if \(f(\mathbf{x})
    \geq f(\mathbf{x}^*), \forall \mathbf{x} \in \mathbb{R}^d\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.7: c. Justification: The text states the Directional Derivative
    from Gradient theorem: “Assume that \(f\) is continuously differentiable at \(\mathbf{x}_0\).
    Then the directional derivative of \(f\) at \(\mathbf{x}_0\) in the direction
    \(\mathbf{v}\) is given by \(\frac{\partial f(\mathbf{x}_0)}{\partial \mathbf{v}}
    = \nabla f(\mathbf{x}_0)^T \mathbf{v}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.9: c. Justification: The text states the Second-Order Sufficient
    Condition theorem: “If \(\nabla f(\mathbf{x}_0) = \mathbf{0}\) and \(\mathbf{H}_f(\mathbf{x}_0)\)
    is positive definite, then \(\mathbf{x}_0\) is a strict local minimizer.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.12: d. Justification: The Lagrange Multipliers theorem states
    that under the given conditions, there exists a unique vector \(\boldsymbol{\lambda}^*
    = (\lambda_1^*, \ldots, \lambda_\ell^*)\) satisfying \(\nabla f(\mathbf{x}^*)
    + \sum_{i=1}^\ell \lambda_i^* \nabla h_i(\mathbf{x}^*) = \mathbf{0}\) and \(\mathbf{h}(\mathbf{x}^*)
    = \mathbf{0}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for Q3.3.14: a. Justification: This is the statement of Taylor’s Theorem
    as presented in the text.'
  prefs: []
  type: TYPE_NORMAL
