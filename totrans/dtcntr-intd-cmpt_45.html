<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>15Â Halloween AnalysisğŸ”—</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>15Â Halloween AnalysisğŸ”—</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://dcic-world.org/2025-08-27/amortized-analysis.html">https://dcic-world.org/2025-08-27/amortized-analysis.html</a></blockquote><table cellspacing="0" cellpadding="0"><tr><td><p><span class="hspace">Â Â Â Â </span><a href="#%28part._.A_.First_.Example%29" class="toclink" data-pltdoc="x">15.1<span class="hspace">Â </span>A First Example</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â </span><a href="#%28part._.The_.New_.Form_of_.Analysis%29" class="toclink" data-pltdoc="x">15.2<span class="hspace">Â </span>The New Form of Analysis</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â </span><a href="#%28part._queue-data-structure%29" class="toclink" data-pltdoc="x">15.3<span class="hspace">Â </span>An Example: Queues from Lists</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â Â Â </span><a href="#%28part._.List_.Representations%29" class="toclink" data-pltdoc="x">15.3.1<span class="hspace">Â </span>List Representations</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â Â Â </span><a href="#%28part._.A_.First_.Analysis%29" class="toclink" data-pltdoc="x">15.3.2<span class="hspace">Â </span>A First Analysis</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â Â Â </span><a href="#%28part._.More_.Liberal_.Sequences_of_.Operations%29" class="toclink" data-pltdoc="x">15.3.3<span class="hspace">Â </span>More Liberal Sequences of Operations</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â Â Â </span><a href="#%28part._.A_.Second_.Analysis%29" class="toclink" data-pltdoc="x">15.3.4<span class="hspace">Â </span>A Second Analysis</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â Â Â </span><a href="#%28part._worst-case-ops-amort%29" class="toclink" data-pltdoc="x">15.3.5<span class="hspace">Â </span>Amortization Versus Individual Operations</a></p></td></tr><tr><td><p><span class="hspace">Â Â Â Â </span><a href="#%28part._.Reading_.More%29" class="toclink" data-pltdoc="x">15.4<span class="hspace">Â </span>Reading More</a></p></td></tr></table><p>In <a href="predicting-growth.html" data-pltdoc="x">Predicting Growth</a>, we introduced the idea of big-Oh
complexity to measure the worst-case time of a computation. As we see
in <a href="sets-from-lists.html#%28part._choosing-set-reps%29" data-pltdoc="x">Choosing Between Representations</a>, however, this is sometimes too coarse
a bound when the complexity is heavily dependent on the exact sequence
of operations run. Now, we will consider a different style of
complexity analysis that better accommodates operation sequences.</p><section class="SsectionLevel3" id="section 15.1"><h3 class="heading">15.1<span class="stt">Â </span><a name="(part._.A_.First_.Example)"/>A First Example<span class="button-group"><a href="#(part._.A_.First_.Example)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p/><div class="SIntrapara">Consider, for instance, a set that starts out empty, followed by a
sequence of \(k\) insertions and then \(k\) membership tests, and
suppose we are using the representation with<span class="emph">out</span>
duplicates. Insertion time is proportional to the size of the set (and
list); this is initially \(0\), then \(1\), and so on, until it reaches
size \(k\). Therefore, the total cost of the sequence of insertions is
\(k \cdot (k+1) / 2\). The membership tests cost \(k\) each in
the worst case, because weâ€™ve inserted up to \(k\)
distinct elements into the set. The total time is then
</div><div class="SIntrapara"><p type="math/tex" style="white-space: pre;" class="stt">\begin{equation*}k^2 / 2 + k / 2 + k^2\end{equation*}</p></div><div class="SIntrapara">for a total of \(2k\) operations, yielding an average of
</div><div class="SIntrapara"><p type="math/tex" style="white-space: pre;" class="stt">\begin{equation*}\frac{3}{4} k + \frac{1}{4}\end{equation*}</p></div><div class="SIntrapara">steps <span class="emph">per operation in the worst case</span>.</div></section><section class="SsectionLevel3" id="section 15.2"><h3 class="heading">15.2<span class="stt">Â </span><a name="(part._.The_.New_.Form_of_.Analysis)"/>The New Form of Analysis<span class="button-group"><a href="#(part._.The_.New_.Form_of_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p>What have we computed? We are still computing a <span class="emph">worst case</span>
cost, because we have taken the cost of each operation in the sequence
in the worst case. We are then computing the <span class="emph">average cost</span> per
operation. Therefore, this is a <span class="emph">average of worst
cases</span>.<span class="refelem"><span class="refcolumn"><span class="refcontent">Importantly, this is different from what is known
as <span style="font-style: italic">average-case analysis</span>, which uses probability theory to
compute the estimated cost of the computation. We have not used any
probability here.</span></span></span> Note that because this is an average per operation,
it does not say anything about how bad any one operation can be
(which, as we will see [<a href="#%28part._worst-case-ops-amort%29" data-pltdoc="x">Amortization Versus Individual Operations</a>], can be quite
a bit worse); it only says what their average is.</p><p>In the above case, this new analysis did not yield any big
surprises. We have found that on average we spend about \(k\) steps
per operation; a big-Oh analysis would have told us that weâ€™re
performing \(2k\) operations with a cost of \(O([k \rightarrow k])\)
each in the number of distinct elements; per operation, then, we are
performing roughly linear work in the worst-case number of set
elements.</p><p>As we will soon see, however, this wonâ€™t always be the case: this new
analysis can cough up pleasant surprises.</p><p>Before we proceed, we should give this analysis its name. Formally,
it is called <span style="font-style: italic">amortized analysis</span>. Amortization is the process of
spreading a payment out over an extended but fixed term. In the same
way, we spread out the cost of a computation over a fixed sequence,
then determine how much each payment will be.<span class="refelem"><span class="refcolumn"><span class="refcontent">We have given
it a whimsical name because
<a href="http://en.wikipedia.org/wiki/Halloween">Halloween</a>
is a(n American) holiday devoted to ghosts, ghouls, and other symbols
of death. <span class="emph">Amortization</span> comes from the Latin root <span class="emph">mort-</span>,
which means death, because an amortized analysis is one conducted â€œat
the deathâ€, i.e., at the end of a fixed sequence of operations.</span></span></span></p></section><section class="SsectionLevel3" id="section 15.3"><h3 class="heading">15.3<span class="stt">Â </span><a name="(part._queue-data-structure)"/>An Example: Queues from Lists<span class="button-group"><a href="#(part._queue-data-structure)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p>We have seen lists [<a href="tables-to-lists.html" data-pltdoc="x">From Tables to Lists</a>] and sets [<a href="part_sets.html" data-pltdoc="x">Several Variations on Sets</a>].
Here we focus on queues, which too can be represented as lists:
<a href="queues-from-lists.html" data-pltdoc="x">Queues from Lists</a>. If you have not read that material, itâ€™s worth
reading at least the early portions now. In this section, we will ignore the
various programming niceties discussed there, and focus on raw list
representations to make an algorithmic point.</p><section class="SsectionLevel4" id="section 15.3.1"><h4 class="heading">15.3.1<span class="stt">Â </span><a name="(part._.List_.Representations)"/>List Representations<span class="button-group"><a href="#(part._.List_.Representations)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Consider two natural ways of defining queues using lists. One is that every
<span class="emph">enqueue</span> is implemented with <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">link</code></span>, while every
<span class="emph">dequeue</span> requires traversing the whole list until its
end. Conversely, we could make enqueuing traverse to the end, and
dequeuing correspond to <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">.rest</code></span>. Either way, one of these
operations will take constant time while the other will be linear in
the length of the list representing the queue. (This should be loosely
reminiscent of trade-offs we ran into when representing sets as lists:
<a href="sets-from-lists.html" data-pltdoc="x">Representing Sets as Lists</a>.)</p><p>In fact, however, the above paragraph contains a key insight that will
let us do better.</p><p>Observe that if we store the queue in a list with
most-recently-enqueued element first, enqueuing is cheap (constant
time). In contrast, if we store the queue in the reverse order, then
dequeuing is constant time. It would be wonderful if we could have
both, but once we pick an order we must give up one or the
other. Unless, that is, we pick...both.</p><p>One half of this is easy. We simply enqueue elements into a list with
the most recent addition first. Now for the (first) crucial insight:
when we need to dequeue, we <span class="emph">reverse the list</span>. Now, dequeuing
also takes constant time.</p></section><section class="SsectionLevel4" id="section 15.3.2"><h4 class="heading">15.3.2<span class="stt">Â </span><a name="(part._.A_.First_.Analysis)"/>A First Analysis<span class="button-group"><a href="#(part._.A_.First_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Of course, to fully analyze the complexity of this data structure, we
must also account for the reversal. In the worst case, we might argue
that <span class="emph">any</span> operation might reverse (because it might be the first
dequeue); therefore, the worst-case time of any operation is the time
it takes to reverse, which is linear in the length of the list (which
corresponds to the elements of the queue).</p><p>However, this answer should be unsatisfying. If we perform \(k\)
enqueues followed by \(k\) dequeues, then each of the enqueues takes
one step; each of the last \(k-1\) dequeues takes one step; and only
the first dequeue requires a reversal, which takes steps proportional
to the number of elements in the list, which at that point is
\(k\). Thus, the total cost of operations for this sequence is
\(k \cdot 1 + k + (k-1) \cdot 1 = 3k-1\) for a total of
\(2k\) operations, giving an <span class="emph">amortized</span> complexity of
effectively <span class="emph">constant</span> time per operation!</p></section><section class="SsectionLevel4" id="section 15.3.3"><h4 class="heading">15.3.3<span class="stt">Â </span><a name="(part._.More_.Liberal_.Sequences_of_.Operations)"/>More Liberal Sequences of Operations<span class="button-group"><a href="#(part._.More_.Liberal_.Sequences_of_.Operations)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>In the process of this, however, weâ€™ve quietly glossed over something that you
may not have picked up on: in our candidate sequence all dequeues
followed all enqueues. What happens on the next enqueue? Because the
list is now reversed, it will have to take a linear amount of time! So
we have only partially solved the problem.</p><p/><div class="SIntrapara">Now we can introduce the second insight: have <span class="emph">two lists instead
of one</span>. One of them will be the tail of the queue, where new elements
get enqueued; the other will be the head of the queue, where they get
dequeued:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Queue&lt;T&gt;:
  | queue(tail :: List&lt;T&gt;, head :: List&lt;T&gt;)
end

mt-q :: Queue = queue(empty, empty)</code></pre></div></div></div><div class="SIntrapara">Provided the tail is stored so that the most recent element is the
first, then enqueuing takes constant time:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun enqueue&lt;T&gt;(q :: Queue&lt;T&gt;, e :: T) -&gt; Queue&lt;T&gt;:
  queue(link(e, q.tail), q.head)
end</code></pre></div></div></div><p/><div class="SIntrapara">For dequeuing to take constant time, the head of the queue must be
stored in the reverse direction. However, how does any element ever
get from the tail to the head? Easy: when we try to dequeue and find
no elements in the head, we reverse the (entire) tail into the head
(resulting in an empty tail). We will first define a datatype to
represent the response from dequeuing:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Response&lt;T&gt;:
  | elt-and-q(e :: T, r :: Queue&lt;T&gt;)
end</code></pre></div></div></div><div class="SIntrapara">Now for the implementation of <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">dequeue</code></span>:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun dequeue&lt;T&gt;(q :: Queue&lt;T&gt;) -&gt; Response&lt;T&gt;:
  cases (List) q.head:
    | empty =&gt;
      new-head = q.tail.reverse()
      elt-and-q(new-head.first,
        queue(empty, new-head.rest))
    | link(f, r) =&gt;
      elt-and-q(f,
        queue(q.tail, r))
  end
end</code></pre></div></div></div></section><section class="SsectionLevel4" id="section 15.3.4"><h4 class="heading">15.3.4<span class="stt">Â </span><a name="(part._.A_.Second_.Analysis)"/>A Second Analysis<span class="button-group"><a href="#(part._.A_.Second_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>We can now reason about sequences of operations as we did before, by
adding up costs and averaging. However, another way to think of it is
this. Letâ€™s give each element in the queue three â€œcreditsâ€. Each
credit can be used for one constant-time operation.</p><p>One credit gets used up in enqueuing. So long as the element stays in
the tail list, it still has two credits to spare. When it needs to be
moved to the head list, it spends one more credit in the link step of
reversal.  Finally, the dequeuing operation performs one operation
too.</p><p>Because the element does not run out of credits, we know it must have
had enough. These credits reflect the cost of operations on that
element. From this (very informal) analysis, we can conclude that in
the worst case, any permutation of enqueues and dequeues will still
cost only a constant amount of amortized time.</p></section><section class="SsectionLevel4" id="section 15.3.5"><h4 class="heading">15.3.5<span class="stt">Â </span><a name="(part._worst-case-ops-amort)"/>Amortization Versus Individual Operations<span class="button-group"><a href="#(part._worst-case-ops-amort)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Note, however, that the constant represents an average across the
sequence of operations. It does not put a bound on the cost of any one
operation. Indeed, as we have seen above, when dequeue finds the head
list empty it reverses the tail, which takes time linear in the size
of the tailâ€”<wbr/>not constant at all! Therefore, we should be careful to
not assume that every step in the sequence will is
bounded. Nevertheless, an amortized analysis sometimes gives us a much
more nuanced understanding of the real behavior of a data structure
than a worst-case analysis does on its own.</p></section></section><section class="SsectionLevel3" id="section 15.4"><h3 class="heading">15.4<span class="stt">Â </span><a name="(part._.Reading_.More)"/>Reading More<span class="button-group"><a href="#(part._.Reading_.More)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p>At this point we have only briefly touched on the subject of amortized
analysis. A very nice
<a href="https://web.archive.org/web/20131020020356/http://www.cs.princeton.edu/~fiebrink/423/AmortizedAnalysisExplained_Fiebrink.pdf">tutorial by Rebecca Fiebrink</a>
provides much more information. The authoritative book on algorithms,
<span style="font-style: italic">Introduction to Algorithms</span> by
Cormen, Leiserson, Rivest, and Stein,
covers amortized analysis in extensive detail.</p></section>&#13;
<h3 class="heading">15.1<span class="stt">Â </span><a name="(part._.A_.First_.Example)"/>A First Example<span class="button-group"><a href="#(part._.A_.First_.Example)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p/><div class="SIntrapara">Consider, for instance, a set that starts out empty, followed by a
sequence of \(k\) insertions and then \(k\) membership tests, and
suppose we are using the representation with<span class="emph">out</span>
duplicates. Insertion time is proportional to the size of the set (and
list); this is initially \(0\), then \(1\), and so on, until it reaches
size \(k\). Therefore, the total cost of the sequence of insertions is
\(k \cdot (k+1) / 2\). The membership tests cost \(k\) each in
the worst case, because weâ€™ve inserted up to \(k\)
distinct elements into the set. The total time is then
</div><div class="SIntrapara"><p type="math/tex" style="white-space: pre;" class="stt">\begin{equation*}k^2 / 2 + k / 2 + k^2\end{equation*}</p></div><div class="SIntrapara">for a total of \(2k\) operations, yielding an average of
</div><div class="SIntrapara"><p type="math/tex" style="white-space: pre;" class="stt">\begin{equation*}\frac{3}{4} k + \frac{1}{4}\end{equation*}</p></div><div class="SIntrapara">steps <span class="emph">per operation in the worst case</span>.</div>&#13;
<h3 class="heading">15.2<span class="stt">Â </span><a name="(part._.The_.New_.Form_of_.Analysis)"/>The New Form of Analysis<span class="button-group"><a href="#(part._.The_.New_.Form_of_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p>What have we computed? We are still computing a <span class="emph">worst case</span>
cost, because we have taken the cost of each operation in the sequence
in the worst case. We are then computing the <span class="emph">average cost</span> per
operation. Therefore, this is a <span class="emph">average of worst
cases</span>.<span class="refelem"><span class="refcolumn"><span class="refcontent">Importantly, this is different from what is known
as <span style="font-style: italic">average-case analysis</span>, which uses probability theory to
compute the estimated cost of the computation. We have not used any
probability here.</span></span></span> Note that because this is an average per operation,
it does not say anything about how bad any one operation can be
(which, as we will see [<a href="#%28part._worst-case-ops-amort%29" data-pltdoc="x">Amortization Versus Individual Operations</a>], can be quite
a bit worse); it only says what their average is.</p><p>In the above case, this new analysis did not yield any big
surprises. We have found that on average we spend about \(k\) steps
per operation; a big-Oh analysis would have told us that weâ€™re
performing \(2k\) operations with a cost of \(O([k \rightarrow k])\)
each in the number of distinct elements; per operation, then, we are
performing roughly linear work in the worst-case number of set
elements.</p><p>As we will soon see, however, this wonâ€™t always be the case: this new
analysis can cough up pleasant surprises.</p><p>Before we proceed, we should give this analysis its name. Formally,
it is called <span style="font-style: italic">amortized analysis</span>. Amortization is the process of
spreading a payment out over an extended but fixed term. In the same
way, we spread out the cost of a computation over a fixed sequence,
then determine how much each payment will be.<span class="refelem"><span class="refcolumn"><span class="refcontent">We have given
it a whimsical name because
<a href="http://en.wikipedia.org/wiki/Halloween">Halloween</a>
is a(n American) holiday devoted to ghosts, ghouls, and other symbols
of death. <span class="emph">Amortization</span> comes from the Latin root <span class="emph">mort-</span>,
which means death, because an amortized analysis is one conducted â€œat
the deathâ€, i.e., at the end of a fixed sequence of operations.</span></span></span></p>&#13;
<h3 class="heading">15.3<span class="stt">Â </span><a name="(part._queue-data-structure)"/>An Example: Queues from Lists<span class="button-group"><a href="#(part._queue-data-structure)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p>We have seen lists [<a href="tables-to-lists.html" data-pltdoc="x">From Tables to Lists</a>] and sets [<a href="part_sets.html" data-pltdoc="x">Several Variations on Sets</a>].
Here we focus on queues, which too can be represented as lists:
<a href="queues-from-lists.html" data-pltdoc="x">Queues from Lists</a>. If you have not read that material, itâ€™s worth
reading at least the early portions now. In this section, we will ignore the
various programming niceties discussed there, and focus on raw list
representations to make an algorithmic point.</p><section class="SsectionLevel4" id="section 15.3.1"><h4 class="heading">15.3.1<span class="stt">Â </span><a name="(part._.List_.Representations)"/>List Representations<span class="button-group"><a href="#(part._.List_.Representations)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Consider two natural ways of defining queues using lists. One is that every
<span class="emph">enqueue</span> is implemented with <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">link</code></span>, while every
<span class="emph">dequeue</span> requires traversing the whole list until its
end. Conversely, we could make enqueuing traverse to the end, and
dequeuing correspond to <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">.rest</code></span>. Either way, one of these
operations will take constant time while the other will be linear in
the length of the list representing the queue. (This should be loosely
reminiscent of trade-offs we ran into when representing sets as lists:
<a href="sets-from-lists.html" data-pltdoc="x">Representing Sets as Lists</a>.)</p><p>In fact, however, the above paragraph contains a key insight that will
let us do better.</p><p>Observe that if we store the queue in a list with
most-recently-enqueued element first, enqueuing is cheap (constant
time). In contrast, if we store the queue in the reverse order, then
dequeuing is constant time. It would be wonderful if we could have
both, but once we pick an order we must give up one or the
other. Unless, that is, we pick...both.</p><p>One half of this is easy. We simply enqueue elements into a list with
the most recent addition first. Now for the (first) crucial insight:
when we need to dequeue, we <span class="emph">reverse the list</span>. Now, dequeuing
also takes constant time.</p></section><section class="SsectionLevel4" id="section 15.3.2"><h4 class="heading">15.3.2<span class="stt">Â </span><a name="(part._.A_.First_.Analysis)"/>A First Analysis<span class="button-group"><a href="#(part._.A_.First_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Of course, to fully analyze the complexity of this data structure, we
must also account for the reversal. In the worst case, we might argue
that <span class="emph">any</span> operation might reverse (because it might be the first
dequeue); therefore, the worst-case time of any operation is the time
it takes to reverse, which is linear in the length of the list (which
corresponds to the elements of the queue).</p><p>However, this answer should be unsatisfying. If we perform \(k\)
enqueues followed by \(k\) dequeues, then each of the enqueues takes
one step; each of the last \(k-1\) dequeues takes one step; and only
the first dequeue requires a reversal, which takes steps proportional
to the number of elements in the list, which at that point is
\(k\). Thus, the total cost of operations for this sequence is
\(k \cdot 1 + k + (k-1) \cdot 1 = 3k-1\) for a total of
\(2k\) operations, giving an <span class="emph">amortized</span> complexity of
effectively <span class="emph">constant</span> time per operation!</p></section><section class="SsectionLevel4" id="section 15.3.3"><h4 class="heading">15.3.3<span class="stt">Â </span><a name="(part._.More_.Liberal_.Sequences_of_.Operations)"/>More Liberal Sequences of Operations<span class="button-group"><a href="#(part._.More_.Liberal_.Sequences_of_.Operations)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>In the process of this, however, weâ€™ve quietly glossed over something that you
may not have picked up on: in our candidate sequence all dequeues
followed all enqueues. What happens on the next enqueue? Because the
list is now reversed, it will have to take a linear amount of time! So
we have only partially solved the problem.</p><p/><div class="SIntrapara">Now we can introduce the second insight: have <span class="emph">two lists instead
of one</span>. One of them will be the tail of the queue, where new elements
get enqueued; the other will be the head of the queue, where they get
dequeued:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Queue&lt;T&gt;:
  | queue(tail :: List&lt;T&gt;, head :: List&lt;T&gt;)
end

mt-q :: Queue = queue(empty, empty)</code></pre></div></div></div><div class="SIntrapara">Provided the tail is stored so that the most recent element is the
first, then enqueuing takes constant time:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun enqueue&lt;T&gt;(q :: Queue&lt;T&gt;, e :: T) -&gt; Queue&lt;T&gt;:
  queue(link(e, q.tail), q.head)
end</code></pre></div></div></div><p/><div class="SIntrapara">For dequeuing to take constant time, the head of the queue must be
stored in the reverse direction. However, how does any element ever
get from the tail to the head? Easy: when we try to dequeue and find
no elements in the head, we reverse the (entire) tail into the head
(resulting in an empty tail). We will first define a datatype to
represent the response from dequeuing:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Response&lt;T&gt;:
  | elt-and-q(e :: T, r :: Queue&lt;T&gt;)
end</code></pre></div></div></div><div class="SIntrapara">Now for the implementation of <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">dequeue</code></span>:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun dequeue&lt;T&gt;(q :: Queue&lt;T&gt;) -&gt; Response&lt;T&gt;:
  cases (List) q.head:
    | empty =&gt;
      new-head = q.tail.reverse()
      elt-and-q(new-head.first,
        queue(empty, new-head.rest))
    | link(f, r) =&gt;
      elt-and-q(f,
        queue(q.tail, r))
  end
end</code></pre></div></div></div></section><section class="SsectionLevel4" id="section 15.3.4"><h4 class="heading">15.3.4<span class="stt">Â </span><a name="(part._.A_.Second_.Analysis)"/>A Second Analysis<span class="button-group"><a href="#(part._.A_.Second_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>We can now reason about sequences of operations as we did before, by
adding up costs and averaging. However, another way to think of it is
this. Letâ€™s give each element in the queue three â€œcreditsâ€. Each
credit can be used for one constant-time operation.</p><p>One credit gets used up in enqueuing. So long as the element stays in
the tail list, it still has two credits to spare. When it needs to be
moved to the head list, it spends one more credit in the link step of
reversal.  Finally, the dequeuing operation performs one operation
too.</p><p>Because the element does not run out of credits, we know it must have
had enough. These credits reflect the cost of operations on that
element. From this (very informal) analysis, we can conclude that in
the worst case, any permutation of enqueues and dequeues will still
cost only a constant amount of amortized time.</p></section><section class="SsectionLevel4" id="section 15.3.5"><h4 class="heading">15.3.5<span class="stt">Â </span><a name="(part._worst-case-ops-amort)"/>Amortization Versus Individual Operations<span class="button-group"><a href="#(part._worst-case-ops-amort)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Note, however, that the constant represents an average across the
sequence of operations. It does not put a bound on the cost of any one
operation. Indeed, as we have seen above, when dequeue finds the head
list empty it reverses the tail, which takes time linear in the size
of the tailâ€”<wbr/>not constant at all! Therefore, we should be careful to
not assume that every step in the sequence will is
bounded. Nevertheless, an amortized analysis sometimes gives us a much
more nuanced understanding of the real behavior of a data structure
than a worst-case analysis does on its own.</p></section>&#13;
<h4 class="heading">15.3.1<span class="stt">Â </span><a name="(part._.List_.Representations)"/>List Representations<span class="button-group"><a href="#(part._.List_.Representations)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Consider two natural ways of defining queues using lists. One is that every
<span class="emph">enqueue</span> is implemented with <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">link</code></span>, while every
<span class="emph">dequeue</span> requires traversing the whole list until its
end. Conversely, we could make enqueuing traverse to the end, and
dequeuing correspond to <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">.rest</code></span>. Either way, one of these
operations will take constant time while the other will be linear in
the length of the list representing the queue. (This should be loosely
reminiscent of trade-offs we ran into when representing sets as lists:
<a href="sets-from-lists.html" data-pltdoc="x">Representing Sets as Lists</a>.)</p><p>In fact, however, the above paragraph contains a key insight that will
let us do better.</p><p>Observe that if we store the queue in a list with
most-recently-enqueued element first, enqueuing is cheap (constant
time). In contrast, if we store the queue in the reverse order, then
dequeuing is constant time. It would be wonderful if we could have
both, but once we pick an order we must give up one or the
other. Unless, that is, we pick...both.</p><p>One half of this is easy. We simply enqueue elements into a list with
the most recent addition first. Now for the (first) crucial insight:
when we need to dequeue, we <span class="emph">reverse the list</span>. Now, dequeuing
also takes constant time.</p>&#13;
<h4 class="heading">15.3.2<span class="stt">Â </span><a name="(part._.A_.First_.Analysis)"/>A First Analysis<span class="button-group"><a href="#(part._.A_.First_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Of course, to fully analyze the complexity of this data structure, we
must also account for the reversal. In the worst case, we might argue
that <span class="emph">any</span> operation might reverse (because it might be the first
dequeue); therefore, the worst-case time of any operation is the time
it takes to reverse, which is linear in the length of the list (which
corresponds to the elements of the queue).</p><p>However, this answer should be unsatisfying. If we perform \(k\)
enqueues followed by \(k\) dequeues, then each of the enqueues takes
one step; each of the last \(k-1\) dequeues takes one step; and only
the first dequeue requires a reversal, which takes steps proportional
to the number of elements in the list, which at that point is
\(k\). Thus, the total cost of operations for this sequence is
\(k \cdot 1 + k + (k-1) \cdot 1 = 3k-1\) for a total of
\(2k\) operations, giving an <span class="emph">amortized</span> complexity of
effectively <span class="emph">constant</span> time per operation!</p>&#13;
<h4 class="heading">15.3.3<span class="stt">Â </span><a name="(part._.More_.Liberal_.Sequences_of_.Operations)"/>More Liberal Sequences of Operations<span class="button-group"><a href="#(part._.More_.Liberal_.Sequences_of_.Operations)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>In the process of this, however, weâ€™ve quietly glossed over something that you
may not have picked up on: in our candidate sequence all dequeues
followed all enqueues. What happens on the next enqueue? Because the
list is now reversed, it will have to take a linear amount of time! So
we have only partially solved the problem.</p><p/><div class="SIntrapara">Now we can introduce the second insight: have <span class="emph">two lists instead
of one</span>. One of them will be the tail of the queue, where new elements
get enqueued; the other will be the head of the queue, where they get
dequeued:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Queue&lt;T&gt;:
  | queue(tail :: List&lt;T&gt;, head :: List&lt;T&gt;)
end

mt-q :: Queue = queue(empty, empty)</code></pre></div></div></div><div class="SIntrapara">Provided the tail is stored so that the most recent element is the
first, then enqueuing takes constant time:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun enqueue&lt;T&gt;(q :: Queue&lt;T&gt;, e :: T) -&gt; Queue&lt;T&gt;:
  queue(link(e, q.tail), q.head)
end</code></pre></div></div></div><p/><div class="SIntrapara">For dequeuing to take constant time, the head of the queue must be
stored in the reverse direction. However, how does any element ever
get from the tail to the head? Easy: when we try to dequeue and find
no elements in the head, we reverse the (entire) tail into the head
(resulting in an empty tail). We will first define a datatype to
represent the response from dequeuing:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">data Response&lt;T&gt;:
  | elt-and-q(e :: T, r :: Queue&lt;T&gt;)
end</code></pre></div></div></div><div class="SIntrapara">Now for the implementation of <span title="Pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">dequeue</code></span>:
</div><div class="SIntrapara"><p/><div class="sourceCodeWrapper"><span data-label="Pyret" class="sourceLangLabel"/><div class="sourceCode"><pre data-lang="pyret" class="sourceCode"><code data-lang="pyret" class="sourceCode">fun dequeue&lt;T&gt;(q :: Queue&lt;T&gt;) -&gt; Response&lt;T&gt;:
  cases (List) q.head:
    | empty =&gt;
      new-head = q.tail.reverse()
      elt-and-q(new-head.first,
        queue(empty, new-head.rest))
    | link(f, r) =&gt;
      elt-and-q(f,
        queue(q.tail, r))
  end
end</code></pre></div></div></div>&#13;
<h4 class="heading">15.3.4<span class="stt">Â </span><a name="(part._.A_.Second_.Analysis)"/>A Second Analysis<span class="button-group"><a href="#(part._.A_.Second_.Analysis)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>We can now reason about sequences of operations as we did before, by
adding up costs and averaging. However, another way to think of it is
this. Letâ€™s give each element in the queue three â€œcreditsâ€. Each
credit can be used for one constant-time operation.</p><p>One credit gets used up in enqueuing. So long as the element stays in
the tail list, it still has two credits to spare. When it needs to be
moved to the head list, it spends one more credit in the link step of
reversal.  Finally, the dequeuing operation performs one operation
too.</p><p>Because the element does not run out of credits, we know it must have
had enough. These credits reflect the cost of operations on that
element. From this (very informal) analysis, we can conclude that in
the worst case, any permutation of enqueues and dequeues will still
cost only a constant amount of amortized time.</p>&#13;
<h4 class="heading">15.3.5<span class="stt">Â </span><a name="(part._worst-case-ops-amort)"/>Amortization Versus Individual Operations<span class="button-group"><a href="#(part._worst-case-ops-amort)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h4><p>Note, however, that the constant represents an average across the
sequence of operations. It does not put a bound on the cost of any one
operation. Indeed, as we have seen above, when dequeue finds the head
list empty it reverses the tail, which takes time linear in the size
of the tailâ€”<wbr/>not constant at all! Therefore, we should be careful to
not assume that every step in the sequence will is
bounded. Nevertheless, an amortized analysis sometimes gives us a much
more nuanced understanding of the real behavior of a data structure
than a worst-case analysis does on its own.</p>&#13;
<h3 class="heading">15.4<span class="stt">Â </span><a name="(part._.Reading_.More)"/>Reading More<span class="button-group"><a href="#(part._.Reading_.More)" class="heading-anchor" title="Link to here">ğŸ”—</a><span style="visibility: hidden"> </span></span></h3><p>At this point we have only briefly touched on the subject of amortized
analysis. A very nice
<a href="https://web.archive.org/web/20131020020356/http://www.cs.princeton.edu/~fiebrink/423/AmortizedAnalysisExplained_Fiebrink.pdf">tutorial by Rebecca Fiebrink</a>
provides much more information. The authoritative book on algorithms,
<span style="font-style: italic">Introduction to Algorithms</span> by
Cormen, Leiserson, Rivest, and Stein,
covers amortized analysis in extensive detail.</p>    
</body>
</html>