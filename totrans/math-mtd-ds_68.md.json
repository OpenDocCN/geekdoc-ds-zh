["```py\ndata = pd.read_csv('advertising.csv')\ndata.head() \n```", "```py\nn = len(data.index)\nprint(n) \n```", "```py\n200 \n```", "```py\nTV = data['TV'].to_numpy()\nradio = data['radio'].to_numpy()\nnewspaper = data['newspaper'].to_numpy()\nsales = data['sales'].to_numpy()\nfeatures = np.stack((TV, radio, newspaper), axis=-1)\nA = np.column_stack((np.ones(n), features))\ncoeff = mmids.ls_by_qr(A, sales)\nprint(coeff) \n```", "```py\n[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03] \n```", "```py\nnp.mean((A @ coeff - sales)**2) \n```", "```py\n2.7841263145109365 \n```", "```py\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\n\nfeatures_tensor = torch.tensor(features, dtype=torch.float32)\nsales_tensor = torch.tensor(sales, dtype=torch.float32).view(-1, 1)\n\nBATCH_SIZE = 64\ntrain_dataset = TensorDataset(features_tensor, sales_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n```", "```py\nmodel = nn.Sequential(\n    nn.Linear(3, 1)  # 3 input features, 1 output value\n) \n```", "```py\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=1e-5) \n```", "```py\nepochs = 100\nfor epoch in range(epochs):\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\") \n```", "```py\nweights = model[0].weight.detach().numpy()\nbias = model[0].bias.detach().numpy()\nprint(\"Weights:\", weights)\nprint(\"Bias:\", bias) \n```", "```py\nWeights: [[0.05736413 0.11314777 0.08020781]]\nBias: [-0.02631279] \n```", "```py\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    total_loss = 0\n    for inputs, targets in train_loader:\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        total_loss += loss.item()\n\n    print(f\"Mean Squared Error on Training Set: {total_loss  /  len(train_loader)}\") \n```", "```py\nMean Squared Error on Training Set: 7.885213494300842 \n```", "```py\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                               download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                              download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() \n                      else (\"mps\" if torch.backends.mps.is_available() \n                            else \"cpu\"))\nprint(\"Using device:\", device) \n```", "```py\nUsing device: mps \n```", "```py\nmodel = nn.Sequential(\n    # First convolution, operating upon a 28x28 image\n    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Second convolution, operating upon a 14x14 image\n    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Third convolution, operating upon a 7x7 image\n    nn.Conv2d(32, 32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Flatten the tensor\n    nn.Flatten(),\n\n    # Fully connected layer\n    nn.Linear(32 * 3 * 3, 10),\n).to(device) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters()) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device) \n```", "```py\nEpoch 1/3 \n```", "```py\nEpoch 2/3 \n```", "```py\nEpoch 3/3 \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 88.6% accuracy \n```", "```py\ntrain_dataset = datasets.MNIST(root='./data', train=True, \n                                      download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.MNIST(root='./data', train=False, \n                                     download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters()) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device) \n```", "```py\nEpoch 1/3 \n```", "```py\nEpoch 2/3 \n```", "```py\nEpoch 3/3 \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 98.6% accuracy \n```", "```py\ndata = pd.read_csv('advertising.csv')\ndata.head() \n```", "```py\nn = len(data.index)\nprint(n) \n```", "```py\n200 \n```", "```py\nTV = data['TV'].to_numpy()\nradio = data['radio'].to_numpy()\nnewspaper = data['newspaper'].to_numpy()\nsales = data['sales'].to_numpy()\nfeatures = np.stack((TV, radio, newspaper), axis=-1)\nA = np.column_stack((np.ones(n), features))\ncoeff = mmids.ls_by_qr(A, sales)\nprint(coeff) \n```", "```py\n[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03] \n```", "```py\nnp.mean((A @ coeff - sales)**2) \n```", "```py\n2.7841263145109365 \n```", "```py\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\n\nfeatures_tensor = torch.tensor(features, dtype=torch.float32)\nsales_tensor = torch.tensor(sales, dtype=torch.float32).view(-1, 1)\n\nBATCH_SIZE = 64\ntrain_dataset = TensorDataset(features_tensor, sales_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n```", "```py\nmodel = nn.Sequential(\n    nn.Linear(3, 1)  # 3 input features, 1 output value\n) \n```", "```py\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=1e-5) \n```", "```py\nepochs = 100\nfor epoch in range(epochs):\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\") \n```", "```py\nweights = model[0].weight.detach().numpy()\nbias = model[0].bias.detach().numpy()\nprint(\"Weights:\", weights)\nprint(\"Bias:\", bias) \n```", "```py\nWeights: [[0.05736413 0.11314777 0.08020781]]\nBias: [-0.02631279] \n```", "```py\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    total_loss = 0\n    for inputs, targets in train_loader:\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        total_loss += loss.item()\n\n    print(f\"Mean Squared Error on Training Set: {total_loss  /  len(train_loader)}\") \n```", "```py\nMean Squared Error on Training Set: 7.885213494300842 \n```", "```py\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                               download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                              download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() \n                      else (\"mps\" if torch.backends.mps.is_available() \n                            else \"cpu\"))\nprint(\"Using device:\", device) \n```", "```py\nUsing device: mps \n```", "```py\nmodel = nn.Sequential(\n    # First convolution, operating upon a 28x28 image\n    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Second convolution, operating upon a 14x14 image\n    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Third convolution, operating upon a 7x7 image\n    nn.Conv2d(32, 32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Flatten the tensor\n    nn.Flatten(),\n\n    # Fully connected layer\n    nn.Linear(32 * 3 * 3, 10),\n).to(device) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters()) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device) \n```", "```py\nEpoch 1/3 \n```", "```py\nEpoch 2/3 \n```", "```py\nEpoch 3/3 \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 88.6% accuracy \n```", "```py\ntrain_dataset = datasets.MNIST(root='./data', train=True, \n                                      download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.MNIST(root='./data', train=False, \n                                     download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters()) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device) \n```", "```py\nEpoch 1/3 \n```", "```py\nEpoch 2/3 \n```", "```py\nEpoch 3/3 \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 98.6% accuracy \n```", "```py\ndata = pd.read_csv('advertising.csv')\ndata.head() \n```", "```py\nn = len(data.index)\nprint(n) \n```", "```py\n200 \n```", "```py\nTV = data['TV'].to_numpy()\nradio = data['radio'].to_numpy()\nnewspaper = data['newspaper'].to_numpy()\nsales = data['sales'].to_numpy()\nfeatures = np.stack((TV, radio, newspaper), axis=-1)\nA = np.column_stack((np.ones(n), features))\ncoeff = mmids.ls_by_qr(A, sales)\nprint(coeff) \n```", "```py\n[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03] \n```", "```py\nnp.mean((A @ coeff - sales)**2) \n```", "```py\n2.7841263145109365 \n```", "```py\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\n\nfeatures_tensor = torch.tensor(features, dtype=torch.float32)\nsales_tensor = torch.tensor(sales, dtype=torch.float32).view(-1, 1)\n\nBATCH_SIZE = 64\ntrain_dataset = TensorDataset(features_tensor, sales_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n```", "```py\nmodel = nn.Sequential(\n    nn.Linear(3, 1)  # 3 input features, 1 output value\n) \n```", "```py\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=1e-5) \n```", "```py\nepochs = 100\nfor epoch in range(epochs):\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\") \n```", "```py\nweights = model[0].weight.detach().numpy()\nbias = model[0].bias.detach().numpy()\nprint(\"Weights:\", weights)\nprint(\"Bias:\", bias) \n```", "```py\nWeights: [[0.05736413 0.11314777 0.08020781]]\nBias: [-0.02631279] \n```", "```py\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    total_loss = 0\n    for inputs, targets in train_loader:\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        total_loss += loss.item()\n\n    print(f\"Mean Squared Error on Training Set: {total_loss  /  len(train_loader)}\") \n```", "```py\nMean Squared Error on Training Set: 7.885213494300842 \n```", "```py\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                               download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                              download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() \n                      else (\"mps\" if torch.backends.mps.is_available() \n                            else \"cpu\"))\nprint(\"Using device:\", device) \n```", "```py\nUsing device: mps \n```", "```py\nmodel = nn.Sequential(\n    # First convolution, operating upon a 28x28 image\n    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Second convolution, operating upon a 14x14 image\n    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Third convolution, operating upon a 7x7 image\n    nn.Conv2d(32, 32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n\n    # Flatten the tensor\n    nn.Flatten(),\n\n    # Fully connected layer\n    nn.Linear(32 * 3 * 3, 10),\n).to(device) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters()) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device) \n```", "```py\nEpoch 1/3 \n```", "```py\nEpoch 2/3 \n```", "```py\nEpoch 3/3 \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 88.6% accuracy \n```", "```py\ntrain_dataset = datasets.MNIST(root='./data', train=True, \n                                      download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.MNIST(root='./data', train=False, \n                                     download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters()) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device) \n```", "```py\nEpoch 1/3 \n```", "```py\nEpoch 2/3 \n```", "```py\nEpoch 3/3 \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 98.6% accuracy \n```"]