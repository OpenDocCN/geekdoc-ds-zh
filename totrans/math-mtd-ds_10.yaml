- en: '2.1\. Motivating example: predicting sales#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html](https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Figure:** Helpful map of ML by scitkit-learn ([Source](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '![ml-cheat-sheet](../Images/19ac9e49b2f297976e40fee63e1c4ba0.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following dataset is from the excellent textbook [[ISLP]](https://www.statlearning.com/).
    Quoting [ISLP, Section 2.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we are statistical consultants hired by a client to provide advice
    on how to improve sales of a particular product. The `advertising` data set consists
    of the `sales` of that product in 200 different markets, along with advertising
    budgets for the product in each of those markets for three different media: `TV`,
    `radio`, and `newspaper`. […] It is not possible for our client to directly increase
    sales of the product. On the other hand, they can control the advertising expenditure
    in each of the three media. Therefore, if we determine that there is an association
    between advertising and sales, then we can instruct our client to adjust advertising
    budgets, thereby indirectly increasing sales. In other words, our goal is to develop
    an accurate model that can be used to predict sales on the basis of the three
    media budgets.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This a [regression](https://en.wikipedia.org/wiki/Regression_analysis) problem.
    That is, we want to estimate the relationship between an outcome variable and
    one or more predictors (or features). We load the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|  | TV | radio | newspaper | sales |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 230.1 | 37.8 | 69.2 | 22.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 44.5 | 39.3 | 45.1 | 10.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 17.2 | 45.9 | 69.3 | 9.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 151.5 | 41.3 | 58.5 | 18.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 180.8 | 10.8 | 58.4 | 12.9 |'
  prefs: []
  type: TYPE_TB
- en: We will focus for now on the TV budget.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We make a scatterplot showing the relation between those two quantities.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/513c36bbfdc95bb1193feb1a81faf6b12f49523a3e5972c61f8ebcbd7f20c997.png](../Images/b4db7dd46ce45bb3079256c749e70bf5.png)'
  prefs: []
  type: TYPE_IMG
- en: There does seem to be a relationship between the two. Roughly a higher TV budget
    is linked to higher sales, although the correspondence is not perfect. To express
    the relationship more quantitatively, we seek a function \(f\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ y \approx f(x) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(x\) denotes a sample TV budget and \(y\) is the corresponding observed
    sales. We might posit for instance that there exists a true \(f\) and that each
    observation is disrupted by some noise \(\varepsilon\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = f(x) + \varepsilon. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural way to estimate such an \(f\) from data is [\(k\)-nearest-neighbors
    (\(k\)-NN) regression](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression)\(\idx{k-NN
    regression}\xdi\). Let the data be of the form \(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)
    where \(\mathbf{x}_i \in \mathbb{R}^d\) and \(y_i \in \mathbb{R}\). In our case
    \(\mathbf{x}_i\) is the (real-valued) TV budget of the \(i\)-th sample (so \(d=1\))
    and \(y_i\) is the corresponding sales. For each \(\mathbf{x}\) (not necessarily
    in the data), we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: find the \(k\) nearest \(\mathbf{x}_i\)’s to \(\mathbf{x}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: take an average of the corresponding \(y_i\)’s.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implement this method in Python. We use the function [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)
    to sort an array and the function [`numpy.absolute`](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html)
    to compute the absolute deviation. Our quick implementation here assumes that
    the \(\mathbf{x}_i\)’s are scalars.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For \(k=3\) and a grid of \(1000\) points, we get the following approximation
    \(\hat{f}\). Here the function [`numpy.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)
    creates an array of equally spaced points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/30b183f214399989ff56602bf85c4b553a4721e53dab5b33add8516d6bcb98ea.png](../Images/9d112f3fd54d45fb5d476e64c7a0950c.png)'
  prefs: []
  type: TYPE_IMG
- en: A higher \(k\) produces something less wiggly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/7182fdeb5a509496e08f6c00d370ca685553b74659c2124a83e0903a410a97f2.png](../Images/26d6f8c976ff1dbb81a5d207dab76ad6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One downside of \(k\)-NN regression is that it does not give an easily interpretable
    relationship: if I increase my TV budget by \(\Delta\) dollars, how is it expected
    to affect the sales? Another issue arises in high dimension where the counter-intuitive
    phenomena we discussed in a previous section can have a significant impact. Recall
    in particular the *High-dimensional Cube Theorem*. If we have \(d\) predictors
    – where \(d\) is large – and our data is distributed uniformly in a bounded region,
    then any given \(\mathbf{x}\) will be far from any of our data points. In that
    case, the \(y\)-values of the closest \(\mathbf{x}_i\)’s may not be predictive.
    This is referred to as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{curse
    of dimensionality}\xdi\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot for more details on the Curse
    of Dimensionality and how it arises in data science. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: One way out is to make stronger assumptions on the function \(f\). For instance,
    we can assume that the true relationship is (approximately) affine, that is, \(y
    \approx \beta_0 + \beta_1 x\), or if we have \(d\) predictors
  prefs: []
  type: TYPE_NORMAL
- en: \[ y \approx \beta_0 + \sum_{j=1}^d \beta_j x_j. \]
  prefs: []
  type: TYPE_NORMAL
- en: How do we estimate appropriate intercept and coefficients? The standard approach
    is to minimize the sum of the squared errors
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \((\mathbf{x}_{i})_j\) is the \(j\)-th entry of input vector \(\mathbf{x}_i
    \in \mathbb{R}^d\) and \(y_i \in \mathbb{R}\) is the corresponding \(y\)-value.
    This is called [multiple linear regression](https://en.wikipedia.org/wiki/Linear_regression).
  prefs: []
  type: TYPE_NORMAL
- en: It is a [least-squares problem](https://en.wikipedia.org/wiki/Least_squares).
    We re-write it in a more convenient matrix form and combine \(\beta_0\) with the
    other \(\beta_i\)’s by adding a dummy predictor to each sample. Let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix},
    \quad\quad A = \begin{pmatrix} 1 & \mathbf{x}_1^T \\ 1 & \mathbf{x}_2^T \\ \vdots
    & \vdots \\ 1 & \mathbf{x}_n^T \end{pmatrix} \quad\text{and}\quad \boldsymbol{\beta}
    = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_d \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then observe that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{y} - A \boldsymbol{\beta}\|^2 &= \sum_{i=1}^n \left(y_i
    - (A \boldsymbol{\beta})_i\right)^2\\ &= \sum_{i=1}^n \left(y_i - \left\{\beta_0
    + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The linear least-squares problem is then formulated as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\boldsymbol{\beta} \in \mathbb{R}^{d+1}} \|\mathbf{y} - A \boldsymbol{\beta}\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, we are looking for a linear combination of the columns of \(A\) that
    is closest to \(\mathbf{y}\) in Euclidean distance. Indeed, minimizing the squared
    Euclidean distance is equivalent to minimizing its square root, as the latter
    in an increasing function.
  prefs: []
  type: TYPE_NORMAL
- en: One could solve this optimization problem through calculus (and we will come
    back to this approach later in the book), but understanding the geometric and
    algebraic structure of the problem turns out to provide powerful insights into
    its solution – and that of many of problems in data science. It will also be an
    opportunity to review some basic linear-algebraic concepts along the way.
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to the `advertising` dataset later in the chapter.
  prefs: []
  type: TYPE_NORMAL
