<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Chapter 3 Computational Bayesian data analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Chapter 3 Computational Bayesian data analysis</h1>
<blockquote>原文：<a href="https://bruno.nicenboim.me/bayescogsci/ch-compbda.html">https://bruno.nicenboim.me/bayescogsci/ch-compbda.html</a></blockquote>
<div id="ch-compbda" class="section level1 hasAnchor" number="3">

<p>In the previous chapter, we learned how to analytically derive the posterior distribution of the parameters in our model.
In practice, however, this is possible for only a very limited number of cases. Although the numerator in Bayes’ rule, the unnormalized posterior, is easy to calculate (by multiplying the likelihood and the probability density/mass function), the denominator, the marginal likelihood, requires us to carry out an integration; see Equation <a href="ch-compbda.html#eq:bayesbrms">(3.1)</a>.</p>
<p><span class="math display" id="eq:bayesbrms">\[\begin{equation}
\begin{aligned}
p(\boldsymbol{\Theta}|\boldsymbol{y}) &amp;= \cfrac{ p(\boldsymbol{y}|\boldsymbol{\Theta}) \cdot p(\boldsymbol{\Theta}) }{p(\boldsymbol{y})}\\
p(\boldsymbol{\Theta}|\boldsymbol{y}) &amp;= \cfrac{ p(\boldsymbol{y}|\boldsymbol{\Theta}) \cdot p(\boldsymbol{\Theta}) }{\int_{\boldsymbol{\Theta}} p(\boldsymbol{y}|\boldsymbol{\Theta}) \cdot p(\boldsymbol{\Theta}) d\boldsymbol{\Theta} }
\end{aligned}
\tag{3.1}
\end{equation}\]</span></p>
<p>Unless we are dealing with conjugate distributions, the solution will be extremely hard to derive or there will be no analytical solution. This was the major bottleneck of Bayesian analysis in the past, and required Bayesian practitioners to program an approximation method by themselves before they could even begin the Bayesian analysis. Fortunately, many of the probabilistic programming languages freely available today (see the next section for a listing) allow us to define our models without having to acquire expert knowledge about the relevant numerical techniques.</p>
<div id="sec-sampling" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Deriving the  posterior through  sampling<a href="ch-compbda.html#sec-sampling" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Let’s say that we want to derive the posterior of the model from section <a href="ch-introBDA.html#sec-analytical">2.2</a>, that is, the posterior distribution of the cloze probability of “<em>umbrella</em>,” <span class="math inline">\(\theta\)</span>, given the following data: a word (e.g., “<em>umbrella</em>”) was answered 80 out of 100 times, and assuming a binomial distribution as the likelihood function, and <span class="math inline">\(\mathit{Beta}(a=4,b=4)\)</span> as a prior distribution for the cloze probability. If we can generate samples<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> from the posterior distribution of <span class="math inline">\(\theta\)</span>, instead of an analytically derived posterior distribution, given enough samples we will have a good approximation of the posterior distribution. Obtaining samples from the posterior will be the only viable option in the models that we will discuss in this book. By “obtaining samples,” we are talking about a situation analogous to when we use <code>rbinom()</code> or <code>rnorm()</code> to obtain samples from a particular distribution. For more details about sampling algorithms, see the section Further Reading (section <a href="ch-compbda.html#sec-ch3furtherreading">3.10</a>).</p>
<p>Thanks to  probabilistic programming languages, it will be relatively straightforward to get these samples, and we will discuss how we will do it in more detail in the next section. For now let’s assume that we used some probabilistic programming language to obtain 20000 samples from the posterior distribution of the cloze probability, <span class="math inline">\(\theta\)</span>: 0.782, 0.722, 0.782, 0.727, 0.839, 0.828, 0.769, 0.806, 0.839, 0.832, 0.728, 0.805, 0.764, 0.756, 0.791, 0.739, 0.774, 0.758, 0.815, 0.808, . Figure <a href="ch-compbda.html#fig:betapost">3.1</a> shows that the approximation of the posterior looks quite similar to the analytically derived posterior. The difference between the analytically computed and approximated mean and variance are <span class="math inline">\(-0.0001\)</span> and <span class="math inline">\(0.000003\)</span> respectively.</p>

<div class="figure"><span style="display:block;" id="fig:betapost"/>
<img src="../Images/ed657f238dd81d502c9e96e015f32093.png" alt="Histogram of the samples of \(\theta\) from the posterior distribution generated via sampling. The black line shows the density plot of the analytically derived posterior." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/betapost-1.svg"/>
<p class="caption">
FIGURE 3.1: Histogram of the samples of <span class="math inline">\(\theta\)</span> from the posterior distribution generated via sampling. The black line shows the density plot of the analytically derived posterior.
</p>
</div>
</div>
<div id="bayesian-regression-models-using-stan-brms" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span>  Bayesian Regression Models using Stan:  brms<a href="ch-compbda.html#bayesian-regression-models-using-stan-brms" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>The surge in popularity of Bayesian statistics is closely tied to the increase in computing power and the appearance of  probabilistic programming languages, such as WinBUGS <span class="citation">(Lunn et al. <a href="#ref-lunn2000winbugs" role="doc-biblioref">2000</a>)</span>, JAGS <span class="citation">(Plummer <a href="#ref-plummer2016jags" role="doc-biblioref">2016</a>)</span>, PyMC3 <span class="citation">(Salvatier, Wiecki, and Fonnesbeck <a href="#ref-Salvatier2016" role="doc-biblioref">2016</a>)</span>, Turing <span class="citation">(Ge, Xu, and Ghahramani <a href="#ref-turing" role="doc-biblioref">2018</a>)</span>, and Stan <span class="citation">(Carpenter et al. <a href="#ref-carpenter2017stan" role="doc-biblioref">2017</a>)</span>; for a historical review, see <span class="citation">Plummer (<a href="#ref-plummer2022simulation" role="doc-biblioref">2022</a>)</span>.</p>
<p>These probabilistic programming languages allow the user to define models without having to deal (for the most part) with the complexities of the sampling process. However, they require learning a new language since the user has to fully specify the statistical model using a particular syntax.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Furthermore, some knowledge of the sampling process is needed to correctly parameterize the models and to avoid convergence issues (these topics will be covered in detail in chapter <a href="ch-complexstan.html#ch-complexstan">9</a>).</p>
<p>There are some alternatives that allow Bayesian inference in R without having to fully specify the model “by hand.” The R packages <code>rstanarm</code> <span class="citation">(Goodrich et al. <a href="#ref-rstanarm" role="doc-biblioref">2018</a>)</span> and <code>brms</code> <span class="citation">(Bürkner <a href="#ref-R-brms" role="doc-biblioref">2024</a>)</span> provide Bayesian equivalents of many popular R model-fitting functions, such as (g)lmer <span class="citation">(Bates, Mächler, et al. <a href="#ref-R-lme4" role="doc-biblioref">2015</a>)</span> and many others; both <code>rstanarm</code> and <code>brms</code> use Stan as the back-end for estimation and sampling. The package R-INLA <span class="citation">(Lindgren and Rue <a href="#ref-lindgren2015bayesian" role="doc-biblioref">2015</a>)</span> allows for fitting a limited selection of likelihood functions and priors in comparison to <code>rstanarm</code> and <code>brms</code> (R-INLA can fit models that can be expressed as latent Gaussian models). This package uses the integrated nested Laplace approximation (INLA) method for carrying out Bayesian inference, rather than a sampling algorithm as in the other probabilistic languages listed above. Another alternative is JASP <span class="citation">(JASP Team <a href="#ref-JASP2019" role="doc-biblioref">2019</a>)</span>, which provides a graphical user interface for both frequentist and Bayesian modeling, and is intended to be an open-source alternative to SPSS.</p>
<p>We will focus on <code>brms</code> in this part of the book. This is because it can be useful for a smooth transition from frequentist models to their Bayesian equivalents. The package <code>brms</code> is not only powerful enough to satisfy the statistical needs of many cognitive scientists, it has the added benefit that the Stan code can be inspected (with the <code>brms</code> functions <code>make_stancode()</code> and <code>make_standata()</code>), allowing the user to customize their models or learn from the code produced internally by <code>brms</code> to eventually transition to writing the models entirely in Stan. In the introduction to Stan in chapter <a href="ch-introstan.html#ch-introstan">8</a>, we implement in Stan the models presented in the current and the following chapters.</p>
<div id="sec-simplenormal" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> A simple linear model: A single subject pressing a button repeatedly (a finger tapping task)<a href="ch-compbda.html#sec-simplenormal" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>To illustrate the basic steps for fitting a model using <code>brms</code>, consider the following example of a finger tapping task <span class="citation">(for a review, see Hubel et al. <a href="#ref-hubel2013computerized" role="doc-biblioref">2013</a>)</span>. Suppose that a subject first sees a blank screen. Then, after a certain amount of time (say <span class="math inline">\(200\)</span> ms), the subject sees a cross in the middle of a screen, and as soon as they see the cross, they tap on the space bar as fast as they can until the experiment is over (<span class="math inline">\(361\)</span> trials). The dependent measure here is the time it takes in milliseconds from one press of the space bar to the next one. The data in each trial are therefore finger tapping times in milliseconds. Suppose that the research question is: how long does it take for this particular subject to press a key?</p>
<p>Let’s model the data with the following assumptions:</p>
<ol style="list-style-type: decimal">
<li>There is a true (unknown) underlying time, <span class="math inline">\(\mu\)</span> ms, that the subject needs to press the space bar.</li>
<li>There is some noise in this process.</li>
<li>The noise is normally distributed (this assumption is questionable given that finger tapping as also response times are generally skewed; we will fix this assumption later).<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></li>
</ol>
<p>This means that the likelihood for each observation <span class="math inline">\(n\)</span> will be:</p>
<p><span class="math display" id="eq:rtlik">\[\begin{equation}
t_n \sim \mathit{Normal}(\mu, \sigma)
\tag{3.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n =1, \ldots, N\)</span>, and <span class="math inline">\(t\)</span> is the dependent variable (finger tapping times in milliseconds). The variable <span class="math inline">\(N\)</span> indexes the total number of data points. The symbol <span class="math inline">\(\mu\)</span> indicates the <em>location</em> of the normal distribution function; the location parameter shifts the distribution left or right on the horizontal axis. For the normal distribution, the location is also the mean of the distribution. The symbol <span class="math inline">\(\sigma\)</span> indicates the <em>scale</em> of the distribution; as the scale decreases, the distribution gets narrower. This compressing approaches a spike (all the probability mass get concentrated near one point) as the scale parameter approaches zero. For the normal distribution, the scale is also its standard deviation.</p>
<p>The reader may have encountered the model shown in Equation <a href="ch-compbda.html#eq:rtlik">(3.2)</a> in the form shown in Equation <a href="ch-compbda.html#eq:rtlikLM">(3.3)</a>:</p>
<p><span class="math display" id="eq:rtlikLM">\[\begin{equation}
t_n = \mu + \varepsilon_n \hbox{, where } \varepsilon_n \stackrel{iid}{\sim} \mathit{Normal}(0,\sigma) \tag{3.3}
\end{equation}\]</span></p>
<p>When the model is written in this way, it should be understood as meaning that each data point <span class="math inline">\(t_n\)</span> has some variability around a mean value <span class="math inline">\(\mu\)</span>, and that variability has standard deviation <span class="math inline">\(\sigma\)</span>. The term “iid” (independent and identically distributed) implies that the residuals are independently generated (they are not correlated with any of the other residual values). The statement of the model in Equation <a href="ch-compbda.html#eq:rtlikLM">(3.3)</a> is exactly the same as saying that the observed data points <span class="math inline">\(t_n\)</span> are iid and are coming from the <span class="math inline">\(Normal(\mu,\sigma)\)</span> distribution.</p>
<p>For a frequentist model that will give us the  maximum likelihood estimate (the sample mean) of the time it takes to press the space bar, this would be enough information to write the formula in R, <code>t ~ 1</code>, and plug it into the function <code>lm()</code> together with the data: <code>lm(t ~ 1, data)</code>. The meaning of the <code>1</code> here is that <code>lm()</code> will estimate the intercept in the model, which is the estimate of <span class="math inline">\(\mu\)</span> in our example. If the reader is completely unfamiliar with linear models, the references in section <a href="ch-reg.html#sec-ch4furtherreading">4.5</a> will be helpful.</p>
<p>For a Bayesian linear model, we will also need to define priors for the two parameters in our model. Let’s say that we know for sure that the time it takes to press a key will be positive and lower than a minute (or <span class="math inline">\(60000\)</span> ms), but we don’t want to make a commitment regarding which values are more likely. We encode what we know about the noise in the task in <span class="math inline">\(\sigma\)</span>: we know that this parameter must be positive and we’ll assume that any value below <span class="math inline">\(2000\)</span> ms is equally likely. These priors are in general strongly discouraged: A flat (or very wide) prior will almost never be the best approximation of what we know. Prior specification is discussed in detail in the online chapter <a href="ch-priors.html#ch-priors">E</a>.</p>
<p>In this case, even if we know very little about the task, we know that pressing the spacebar will take at most a couple of seconds. We’ll start with flat priors in this section for pedagogical purposes; the next sections will already show more realistic uses of priors.</p>
<p><span class="math display" id="eq:rtpriors">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Uniform}(0, 60000) \\
\sigma &amp;\sim \mathit{Uniform}(0, 2000)
\end{aligned}
\tag{3.4}
\end{equation}\]</span></p>
<p>First, load the data frame <code>df_spacebar</code> from the <code>bcogsci</code> package:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb94-1"><a href="ch-compbda.html#cb94-1" aria-hidden="true"/><span class="kw">data</span>(<span class="st">"df_spacebar"</span>)</span>
<span id="cb94-2"><a href="ch-compbda.html#cb94-2" aria-hidden="true"/>df_spacebar</span></code></pre></div>
<pre><code>## # A tibble: 361 × 2
##       t trial
##   &lt;int&gt; &lt;int&gt;
## 1   141     1
## 2   138     2
## 3   128     3
## # ℹ 358 more rows</code></pre>
<p>It is always a good idea to plot the data before doing anything else; see Figure <a href="ch-compbda.html#fig:m1visualize">3.2</a>. As we suspected, the data look a bit skewed, but we ignore this for the moment.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb96-1"><a href="ch-compbda.html#cb96-1" aria-hidden="true"/></span>
<span id="cb96-2"><a href="ch-compbda.html#cb96-2" aria-hidden="true"/><span class="kw">ggplot</span>(df_spacebar, <span class="kw">aes</span>(t)) <span class="op">+</span></span>
<span id="cb96-3"><a href="ch-compbda.html#cb96-3" aria-hidden="true"/><span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span></span>
<span id="cb96-4"><a href="ch-compbda.html#cb96-4" aria-hidden="true"/><span class="st">  </span><span class="kw">xlab</span>(<span class="st">"Finger tapping times"</span>) <span class="op">+</span></span>
<span id="cb96-5"><a href="ch-compbda.html#cb96-5" aria-hidden="true"/><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Button-press data"</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m1visualize"/>
<img src="../Images/b14a590009b985b0df7a58a9b31ed27c.png" alt="Visualizing the button-press data." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/m1visualize-1.svg"/>
<p class="caption">
FIGURE 3.2: Visualizing the button-press data.
</p>
</div>
<div id="specifying-the-model-in-brms" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Specifying the model in <code>brms</code><a href="ch-compbda.html#specifying-the-model-in-brms" class="anchor-section" aria-label="Anchor link to header"/></h4>
<p>Fit the model defined by equations <a href="ch-compbda.html#eq:rtlik">(3.2)</a> and <a href="ch-compbda.html#eq:rtpriors">(3.4)</a> with <code>brms</code> in the following way.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb97-1"><a href="ch-compbda.html#cb97-1" aria-hidden="true"/>fit_press &lt;-</span>
<span id="cb97-2"><a href="ch-compbda.html#cb97-2" aria-hidden="true"/><span class="st">  </span><span class="kw">brm</span>(t <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb97-3"><a href="ch-compbda.html#cb97-3" aria-hidden="true"/>      <span class="dt">data =</span> df_spacebar,</span>
<span id="cb97-4"><a href="ch-compbda.html#cb97-4" aria-hidden="true"/>      <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb97-5"><a href="ch-compbda.html#cb97-5" aria-hidden="true"/>      <span class="dt">prior =</span></span>
<span id="cb97-6"><a href="ch-compbda.html#cb97-6" aria-hidden="true"/> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>, <span class="dv">60000</span>), <span class="dt">class =</span> Intercept, <span class="dt">lb =</span> <span class="dv">0</span>, <span class="dt">ub =</span> <span class="dv">60000</span>),</span>
<span id="cb97-7"><a href="ch-compbda.html#cb97-7" aria-hidden="true"/>   <span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>, <span class="dv">2000</span>), <span class="dt">class =</span> sigma, <span class="dt">lb =</span> <span class="dv">0</span>, <span class="dt">ub =</span> <span class="dv">2000</span>)),</span>
<span id="cb97-8"><a href="ch-compbda.html#cb97-8" aria-hidden="true"/>      <span class="dt">chains =</span> <span class="dv">4</span>,</span>
<span id="cb97-9"><a href="ch-compbda.html#cb97-9" aria-hidden="true"/>      <span class="dt">iter =</span> <span class="dv">2000</span>,</span>
<span id="cb97-10"><a href="ch-compbda.html#cb97-10" aria-hidden="true"/>      <span class="dt">warmup =</span> <span class="dv">1000</span>)</span></code></pre></div>
<p>The <code>brms</code> code has some differences from a model fit with <code>lm()</code>. At this beginning stage, we’ll focus on the following options:</p>
<ol style="list-style-type: decimal">
<li>The term <code>family = gaussian()</code> makes it explicit that the underlying likelihood function is a normal distribution (Gaussian  and normal are synonyms). This detail is implicit in the R function <code>lm()</code>. Other linking functions are possible, exactly as in the <code>glm()</code> function in R. The default for <code>brms</code> that corresponds to the <code>lm()</code> function is <code>family = gaussian()</code>.</li>
<li>The term <code>prior</code> takes as argument a list of prior specifications. Although this  specification of priors is optional, the researcher should always explicitly specify each prior. Otherwise, <code>brms</code> will define priors by default, which may or may not be appropriate for the research area. In cases where the distribution has a restricted coverage, that is, not every value is valid (e.g., smaller than <span class="math inline">\(0\)</span> or larger than <span class="math inline">\(60000\)</span> are not valid for the intercept), we need to set lower and upper boundaries with <code>lb</code> and <code>ub</code>.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></li>
<li>The term  <code>chains</code> refers to the number of independent runs for sampling (by default four).</li>
<li>The term  <code>iter</code> refers to the  number of iterations that the sampler makes to sample from the posterior distribution of each parameter (by default <code>2000</code>).</li>
<li>The term  <code>warmup</code> refers to the number of iterations from the start of sampling that are eventually discarded (by default half of <code>iter</code>).</li>
</ol>
<p>The last three options, <code>chains</code>, <code>iter</code>, <code>warmup</code> determine the behavior of the sampling algorithm: the No-U-Turn Sampler <span class="citation">(NUTS; Hoffman and Gelman <a href="#ref-hoffmanNoUTurnSamplerAdaptively2014" role="doc-biblioref">2014</a>)</span> extension of Hamiltonian Monte Carlo <span class="citation">(Duane et al. <a href="#ref-duaneHybridMonteCarlo1987" role="doc-biblioref">1987</a>; Neal <a href="#ref-nealMCMCUsingHamiltonian2011" role="doc-biblioref">2011</a>)</span>. We will discuss sampling in a bit more depth in chapter <a href="ch-introstan.html#ch-introstan">8</a>, but the basic process is explained next.</p>
</div>
<div id="sec-convergencenut" class="section level4 hasAnchor" number="3.2.1.2">
<h4><span class="header-section-number">3.2.1.2</span> Sampling and convergence in a nutshell<a href="ch-compbda.html#sec-convergencenut" class="anchor-section" aria-label="Anchor link to header"/></h4>
<p>The following is a gross oversimplification of the sampling process: The code specification starts (by default) four chains independently from each other. Each chain “searches” for samples of the posterior distribution in a multidimensional space, where each parameter corresponds to a dimension. The shape of this space is determined by the priors and the likelihood. The chains start at random locations, and in each iteration they take one sample each for all the parameters. When  sampling begins, the samples may or may not belong to the posterior distributions of the parameters. Eventually, the chains end up in the vicinity of the posterior distribution, and from that point onward the samples will belong to the posterior.</p>
<p>Thus, when sampling begins, the samples from the different chains can be far from each other, but at some point they will “converge” and start delivering samples from the posterior distributions. Although there are no guarantees that the number of iterations we run the chains for will be sufficient for obtaining samples from the posteriors, the default values of <code>brms</code> (and Stan) are in many cases sufficient to achieve  convergence. When the default number of iterations does not suffice, <code>brms</code> (actually, Stan) will print out warnings, with suggestions for fixing the convergence problems. If all the chains converge to the same distribution, by removing the “warmup” samples, we make sure that we do not get samples from the initial path to the posterior distributions. The default in <code>brms</code> is that half of the total number of iterations in each chain (which default to 2000) will count as “warmup”. So, if one runs a model with four chains and the default number of iterations, we will obtain a total of 4000 samples from the four chains, after discarding the warmup iterations.</p>
<p>Figure <a href="ch-compbda.html#fig:fwarmup">3.3</a>(a) shows the path of the chains from the warmup phase onwards. Such plots are called  traceplots. The warmup is shown only for illustration purposes; generally, one should only inspect the chains after the point where convergence has (presumably) been achieved (i.e., after the dashed line). After convergence has occurred, a visual diagnostic check is that chains should look like a “fat hairy caterpillar.” Compare the traceplots of our model in Figure <a href="ch-compbda.html#fig:fwarmup">3.3</a>(a) with the traceplots of a model that did not converge, shown in Figure <a href="ch-compbda.html#fig:fwarmup">3.3</a>(b).</p>
<p>Traceplots are not always diagnostic as regards convergence. The traceplots might look fine, but the model may not have converged. Fortunately, Stan automatically runs several diagnostics with the information from the chains, and if there are no warnings after fitting the model and the traceplots look fine, we can be reasonably sure that the model converged, and assume that our samples are from the true posterior distribution. However, it is necessary to run more than one chain (preferably four), with a couple of thousands of iterations (at least) in order for the diagnostics to work.</p>

<div class="figure"><span style="display:block;" id="fig:fwarmup"/>
<img src="../Images/88e39ee2135d22bfbf43d3e82f42d9f6.png" alt="(a) Traceplots of our brms model for the button-pressing data. All the chains start from initial values above 200 and are outside of the plot. (b) Traceplots of a model that did not converge. We can diagnose the non-convergence by the observing that the chains do not overlap—each chain seems to be sampling from a different distribution." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/fwarmup-1.svg"/>
<p class="caption">
FIGURE 3.3: (a) Traceplots of our <code>brms</code> model for the button-pressing data. All the chains start from initial values above <code>200</code> and are outside of the plot. (b) Traceplots of a model that <strong>did not</strong> converge. We can diagnose the non-convergence by the observing that the chains do not overlap—each chain seems to be sampling from a different distribution.
</p>
</div>
</div>
<div id="output-of-brms" class="section level4 hasAnchor" number="3.2.1.3">
<h4><span class="header-section-number">3.2.1.3</span> Output of <code>brms</code><a href="ch-compbda.html#output-of-brms" class="anchor-section" aria-label="Anchor link to header"/></h4>
<p>Once the model has been fit (and assuming that we got no warning messages about convergence problems), we can print out the  samples of the  posterior distributions of each of the parameters using <code>as_draws_df()</code> (which stores metadata about the chains) or with <code>as.data.frame()</code>:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb98-1"><a href="ch-compbda.html#cb98-1" aria-hidden="true"/><span class="kw">as_draws_df</span>(fit_press) <span class="op">%&gt;%</span></span>
<span id="cb98-2"><a href="ch-compbda.html#cb98-2" aria-hidden="true"/><span class="st">  </span><span class="kw">head</span>(<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## # A draws_df: 3 iterations, 1 chains, and 5 variables
##   b_Intercept sigma Intercept lprior  lp__
## 1         168    25       168    -19 -1683
## 2         169    24       169    -19 -1683
## 3         169    25       169    -19 -1683
## # ... hidden reserved variables {'.chain', '.iteration', '.draw'}</code></pre>
<p>The term <code>b_Intercept</code> in the <code>brms</code> output corresponds to our <span class="math inline">\(\mu\)</span>. We can ignore the last three columns: <code>Intercept</code> is an auxiliary intercept assuming centered predictors, which coincides here with <code>b_Intercept</code> and is discussed in online section <a href="regression-models-with-brms---extended.html#app-intercept">A.3</a>, <code>lprior</code> is the log-density of the (joint) prior distribution and it is there for compatibility with the package <code>priorsense</code> (<a href="https://github.com/n-kall/priorsense" class="uri">https://github.com/n-kall/priorsense</a>), and <code>lp</code> is not really part of the posterior, it’s the log-density of the unnormalized posterior for each iteration (<code>lp</code> is discussed in online section <a href="advanced-models-with-stan---extended.html#app-target">B.1</a>, which sould be read in the context of chapter <a href="ch-introstan.html#ch-introstan">8</a>).</p>
<p>Plot the density and traceplots of each parameter after the warmup with <code>plot(fit_press)</code> (Figure <a href="ch-compbda.html#fig:densitytrace">3.4</a>).</p>

<div class="figure"><span style="display:block;" id="fig:densitytrace"/>
<img src="../Images/2921e1da57361c328e8e5cbf2e062094.png" alt="Density and traceplots of our brms model for the button-pressing data." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/densitytrace-1.svg"/>
<p class="caption">
FIGURE 3.4: Density and  traceplots of our <code>brms</code> model for the button-pressing data.
</p>
</div>
<p>Printing the object with the <code>brms</code> fit provides a nice, if somewhat verbose, summary:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb100-1"><a href="ch-compbda.html#cb100-1" aria-hidden="true"/>fit_press</span>
<span id="cb100-2"><a href="ch-compbda.html#cb100-2" aria-hidden="true"/><span class="co"># posterior_summary(fit_press) is also useful</span></span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: t ~ 1 
##    Data: df_spacebar (Number of observations: 361) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Regression Coefficients:
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   168.63      1.29   166.17   171.23 1.00     3482     2624
## 
## Further Distributional Parameters:
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    25.02      0.96    23.26    26.98 1.00     2990     2588
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>The  <code>Estimate</code> is just the mean of the posterior samples,  <code>Est.Error</code> is the standard deviation of the posterior and the CIs mark the lower and upper bounds of the 95%  credible intervals (to distinguish credible intervals from frequentist confidence intervals, the former will be abbreviated as CrIs):</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb102-1"><a href="ch-compbda.html#cb102-1" aria-hidden="true"/><span class="kw">as_draws_df</span>(fit_press)<span class="op">$</span>b_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mean</span>()</span></code></pre></div>
<pre><code>## [1] 169</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb104-1"><a href="ch-compbda.html#cb104-1" aria-hidden="true"/><span class="kw">as_draws_df</span>(fit_press)<span class="op">$</span>b_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sd</span>()</span></code></pre></div>
<pre><code>## [1] 1.29</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb106-1"><a href="ch-compbda.html#cb106-1" aria-hidden="true"/><span class="kw">as_draws_df</span>(fit_press)<span class="op">$</span>b_Intercept <span class="op">%&gt;%</span></span>
<span id="cb106-2"><a href="ch-compbda.html#cb106-2" aria-hidden="true"/><span class="st">  </span><span class="kw">quantile</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">.975</span>))</span></code></pre></div>
<pre><code>##  2.5% 97.5% 
##   166   171</code></pre>
<p>Furthermore, the summary provides the <code>Rhat</code>, <code>Bulk_ESS</code>, and <code>Tail_ESS</code> of each parameter.  R-hat compares the between- and within-chain estimates of each parameter. R-hat is larger than 1 when chains have not mixed well, one can only rely on the model if the R-hats for all the parameters are less than 1.05. (R warnings will appear otherwise).  Bulk ESS (bulk  effective sample size) is a measure of sampling efficiency in the bulk of the posterior distribution, that is the effective sample size for the mean and median estimates, whereas  tail ESS (tail effective sample size) indicates the sampling efficiency at the tails of the distribution, that is the minimum of effective sample sizes for 5% and 95% quantiles. The effective sample size is generally smaller than the number of post-warmup samples, because the samples from the chains are not independent (they are correlated to some extent), and carry less information about the posterior distribution in comparison to independent samples.
In some cases, however, the effective sample size is actually larger than the number of post-warmup samples. This might happen for parameters with a normally distributed posterior (in the unconstrained space, see online section <a href="advanced-models-with-stan---extended.html#app-target">B.1</a>) and low dependence on the other parameters <span class="citation">(Vehtari et al. <a href="#ref-vehtari2019ranknormalization" role="doc-biblioref">2021</a>)</span>. A very low effective sample size indicates sampling problems (and is accompanied by R warnings) and in general appears together with chains that are not properly mixed. As a rule of thumb, <span class="citation">Vehtari et al. (<a href="#ref-vehtari2019ranknormalization" role="doc-biblioref">2021</a>)</span> suggest that a minimum effective sample size of <span class="math inline">\(400\)</span> is required for statistical summaries.</p>
<p>We see that we can fit our model without problems, and we get some posterior distributions for our parameters. However, we should ask ourselves the following questions before we can interpret the posterior distributions of the model:</p>
<ol style="list-style-type: decimal">
<li>What information are the priors encoding? Do the priors make sense?</li>
<li>Does the likelihood assumed in the model make sense for the data?</li>
</ol>
<p>We’ll try to answer these questions by looking at the <em>prior and posterior predictive distributions</em>, and by doing sensitivity analyses. This is explained in the sections that follow.</p>
</div>
</div>
</div>
<div id="sec-priorpred" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Prior predictive distribution<a href="ch-compbda.html#sec-priorpred" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>We had defined the following priors for our linear model:</p>
<p><span class="math display" id="eq:rtpriorsrepeated">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Uniform}(0, 60000) \\
\sigma &amp;\sim \mathit{Uniform}(0, 2000)
\end{aligned}
\tag{3.5}
\end{equation}\]</span></p>
<p>These  priors encode assumptions about the kind of data we would expect to see in a future study.
To understand these assumptions, we are going to generate data from the model; such distribution of data, which is generated entirely by the prior distributions, is called the prior predictive distribution. Generating prior predictive distributions repeatedly helps us to check whether the priors make sense. What we want to know here is, do the priors generate realistic-looking data?</p>
<p>Formally, we want to know the density <span class="math inline">\(p(\cdot)\)</span> of data points <span class="math inline">\(y_{pred_1},\dots,y_{pred_N}\)</span> from a data set <span class="math inline">\(\boldsymbol{y_{pred}}\)</span> of length <span class="math inline">\(N\)</span>, given a vector of priors <span class="math inline">\(\boldsymbol{\Theta}\)</span> and our likelihood <span class="math inline">\(p(\cdot|\boldsymbol{\Theta})\)</span>; (in our example, <span class="math inline">\(\boldsymbol{\Theta}=\langle\mu,\sigma \rangle\)</span>). The prior predictive density is written as follows:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(\boldsymbol{y_{pred}}) &amp;= p(y_{pred_1},\dots,y_{pred_n})\\
&amp;= \int_{\boldsymbol{\Theta}} p(y_{pred_1}|\boldsymbol{\Theta})\cdot p(y_{pred_2}|\boldsymbol{\Theta})\cdots p(y_{pred_N}|\boldsymbol{\Theta}) p(\boldsymbol{\Theta}) \, \mathrm{d}\boldsymbol{\Theta}
\end{aligned}
\end{equation}\]</span></p>
<p>In essence, the vector of parameters is integrated out (see section <a href="ch-intro.html#sec-marginal">1.7</a>). This yields the probability distribution of possible data sets given the priors and the likelihood, <em>before any observations are taken into account</em>.</p>
<p>The integration can be carried out computationally by generating samples from the prior distribution.</p>
<p>Here is one way to generate prior predictive distributions:</p>
<p>Repeat the following many times:</p>
<ol style="list-style-type: decimal">
<li>Take one sample from each of the priors.</li>
<li>Plug those samples into the probability density/mass function used as the likelihood in the model to generate a data set <span class="math inline">\(y_{pred_1},\ldots,y_{pred_n}\)</span>.</li>
</ol>
<p>Each sample is an imaginary or potential data set.</p>
<p>Create a function that does this:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb108-1"><a href="ch-compbda.html#cb108-1" aria-hidden="true"/>normal_predictive_distribution &lt;-</span>
<span id="cb108-2"><a href="ch-compbda.html#cb108-2" aria-hidden="true"/><span class="st">  </span><span class="cf">function</span>(mu_samples, sigma_samples, N_obs) {</span>
<span id="cb108-3"><a href="ch-compbda.html#cb108-3" aria-hidden="true"/>    <span class="co"># empty data frame with headers:</span></span>
<span id="cb108-4"><a href="ch-compbda.html#cb108-4" aria-hidden="true"/>    df_pred &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">trialn =</span> <span class="kw">numeric</span>(<span class="dv">0</span>),</span>
<span id="cb108-5"><a href="ch-compbda.html#cb108-5" aria-hidden="true"/>                      <span class="dt">t_pred =</span> <span class="kw">numeric</span>(<span class="dv">0</span>),</span>
<span id="cb108-6"><a href="ch-compbda.html#cb108-6" aria-hidden="true"/>                      <span class="dt">iter =</span> <span class="kw">numeric</span>(<span class="dv">0</span>))</span>
<span id="cb108-7"><a href="ch-compbda.html#cb108-7" aria-hidden="true"/>    <span class="co"># i iterates from 1 to the length of mu_samples,</span></span>
<span id="cb108-8"><a href="ch-compbda.html#cb108-8" aria-hidden="true"/>    <span class="co"># which we assume is identical to</span></span>
<span id="cb108-9"><a href="ch-compbda.html#cb108-9" aria-hidden="true"/>    <span class="co"># the length of the sigma_samples:</span></span>
<span id="cb108-10"><a href="ch-compbda.html#cb108-10" aria-hidden="true"/>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(mu_samples)) {</span>
<span id="cb108-11"><a href="ch-compbda.html#cb108-11" aria-hidden="true"/>      mu &lt;-<span class="st"> </span>mu_samples[i]</span>
<span id="cb108-12"><a href="ch-compbda.html#cb108-12" aria-hidden="true"/>      sigma &lt;-<span class="st"> </span>sigma_samples[i]</span>
<span id="cb108-13"><a href="ch-compbda.html#cb108-13" aria-hidden="true"/>      df_pred &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(df_pred,</span>
<span id="cb108-14"><a href="ch-compbda.html#cb108-14" aria-hidden="true"/>                           <span class="kw">tibble</span>(<span class="dt">trialn =</span> <span class="kw">seq_len</span>(N_obs), </span>
<span id="cb108-15"><a href="ch-compbda.html#cb108-15" aria-hidden="true"/>                                  <span class="co"># seq_len generates 1, 2,..., N_obs</span></span>
<span id="cb108-16"><a href="ch-compbda.html#cb108-16" aria-hidden="true"/>                                  <span class="dt">t_pred =</span> <span class="kw">rnorm</span>(N_obs, mu, sigma),</span>
<span id="cb108-17"><a href="ch-compbda.html#cb108-17" aria-hidden="true"/>                                  <span class="dt">iter =</span> i))</span>
<span id="cb108-18"><a href="ch-compbda.html#cb108-18" aria-hidden="true"/>    }</span>
<span id="cb108-19"><a href="ch-compbda.html#cb108-19" aria-hidden="true"/>    df_pred</span>
<span id="cb108-20"><a href="ch-compbda.html#cb108-20" aria-hidden="true"/>  }</span></code></pre></div>
<p>The following code produces <span class="math inline">\(1000\)</span> samples of the prior predictive distribution of the model that we defined in section <a href="ch-compbda.html#sec-simplenormal">3.2.1</a>. This means that it will produce <span class="math inline">\(361000\)</span> predicted values (<span class="math inline">\(361\)</span> predicted observations for each of the <span class="math inline">\(1000\)</span> simulations). Although this approach works, it’s quite slow (a couple of seconds). See the online section <a href="regression-models-with-brms---extended.html#app-efficientpriorpd">A.1</a> for a more efficient version of this function. Section <a href="ch-compbda.html#sec-lognormal">3.7.2</a> will show that it’s possible to use <code>brms</code> to sample from the priors, ignoring the <code>t</code> in the data by setting <code>sample_prior = "only"</code>. However, since <code>brms</code> still depends on Stan’s sampler, which uses Hamiltonian Monte Carlo, the prior sampling process can also fail to converge, especially when one uses very uninformative priors, like the ones used in this example. In contrast, our function above, which uses <code>rnorm()</code>, cannot have convergence issues and will always produce multiple sets of prior predictive data <span class="math inline">\(y_{pred_1},\ldots,y_{pred_n}\)</span>.</p>
<p>The code below uses the <code>tic()</code> and <code>toc()</code> functions from the <code>tictoc</code> package to print out the time it takes to run the code.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb109-1"><a href="ch-compbda.html#cb109-1" aria-hidden="true"/>N_samples &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb109-2"><a href="ch-compbda.html#cb109-2" aria-hidden="true"/>N_obs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_spacebar)</span>
<span id="cb109-3"><a href="ch-compbda.html#cb109-3" aria-hidden="true"/>mu_samples &lt;-<span class="st"> </span><span class="kw">runif</span>(N_samples, <span class="dv">0</span>, <span class="dv">60000</span>)</span>
<span id="cb109-4"><a href="ch-compbda.html#cb109-4" aria-hidden="true"/>sigma_samples &lt;-<span class="st"> </span><span class="kw">runif</span>(N_samples, <span class="dv">0</span>, <span class="dv">2000</span>)</span>
<span id="cb109-5"><a href="ch-compbda.html#cb109-5" aria-hidden="true"/><span class="kw">tic</span>()</span>
<span id="cb109-6"><a href="ch-compbda.html#cb109-6" aria-hidden="true"/>prior_pred &lt;-</span>
<span id="cb109-7"><a href="ch-compbda.html#cb109-7" aria-hidden="true"/><span class="st">  </span><span class="kw">normal_predictive_distribution</span>(<span class="dt">mu_samples =</span> mu_samples,</span>
<span id="cb109-8"><a href="ch-compbda.html#cb109-8" aria-hidden="true"/>                                 <span class="dt">sigma_samples =</span> sigma_samples,</span>
<span id="cb109-9"><a href="ch-compbda.html#cb109-9" aria-hidden="true"/>                                 <span class="dt">N_obs =</span> N_obs)</span>
<span id="cb109-10"><a href="ch-compbda.html#cb109-10" aria-hidden="true"/><span class="kw">toc</span>()</span></code></pre></div>
<pre><code>## 1.39 sec elapsed</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb111-1"><a href="ch-compbda.html#cb111-1" aria-hidden="true"/>prior_pred</span></code></pre></div>
<pre><code>## # A tibble: 361,000 × 3
##   trialn t_pred  iter
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1      1 16710.     1
## 2      2 16686.     1
## 3      3 17245.     1
## # ℹ 360,997 more rows</code></pre>
<pre><code>## 0.321 sec elapsed</code></pre>
<p>Figure <a href="ch-compbda.html#fig:priorpred-simple">3.5</a> shows the first 18 samples of the prior predictive distribution (i.e., 18 independently generated prior predicted data sets) with the code below.</p>

<div class="sourceCode" id="cb114"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb114-1"><a href="ch-compbda.html#cb114-1" aria-hidden="true"/>prior_pred <span class="op">%&gt;%</span></span>
<span id="cb114-2"><a href="ch-compbda.html#cb114-2" aria-hidden="true"/><span class="st">  </span><span class="kw">filter</span>(iter <span class="op">&lt;=</span><span class="st"> </span><span class="dv">18</span>) <span class="op">%&gt;%</span></span>
<span id="cb114-3"><a href="ch-compbda.html#cb114-3" aria-hidden="true"/><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(t_pred)) <span class="op">+</span></span>
<span id="cb114-4"><a href="ch-compbda.html#cb114-4" aria-hidden="true"/><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">after_stat</span>(density))) <span class="op">+</span></span>
<span id="cb114-5"><a href="ch-compbda.html#cb114-5" aria-hidden="true"/><span class="st">  </span><span class="kw">xlab</span>(<span class="st">"predicted t (ms)"</span>) <span class="op">+</span></span>
<span id="cb114-6"><a href="ch-compbda.html#cb114-6" aria-hidden="true"/><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">40</span>,</span>
<span id="cb114-7"><a href="ch-compbda.html#cb114-7" aria-hidden="true"/>                                   <span class="dt">vjust =</span> <span class="dv">1</span>,</span>
<span id="cb114-8"><a href="ch-compbda.html#cb114-8" aria-hidden="true"/>                                   <span class="dt">hjust =</span> <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb114-9"><a href="ch-compbda.html#cb114-9" aria-hidden="true"/><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.0005</span>),</span>
<span id="cb114-10"><a href="ch-compbda.html#cb114-10" aria-hidden="true"/>                     <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.00025</span>, <span class="fl">0.0005</span>),</span>
<span id="cb114-11"><a href="ch-compbda.html#cb114-11" aria-hidden="true"/>                     <span class="dt">name =</span> <span class="st">"density"</span>) <span class="op">+</span></span>
<span id="cb114-12"><a href="ch-compbda.html#cb114-12" aria-hidden="true"/><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>iter, <span class="dt">ncol =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:priorpred-simple"/>
<img src="../Images/29726df01a7c3d4fd9275f104415874b.png" alt="Eighteen samples from the prior predictive distribution of the model defined in section 3.2.1." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/priorpred-simple-1.svg"/>
<p class="caption">
FIGURE 3.5: Eighteen samples from the prior predictive distribution of the model defined in section <a href="ch-compbda.html#sec-simplenormal">3.2.1</a>.
</p>
</div>
<p>The prior predictive distribution in Figure <a href="ch-compbda.html#fig:priorpred-simple">3.5</a> shows data sets generated from the model that are not realistic: Apart from the fact that the data sets show that finger tapping times distributions are symmetrical–and we know from prior experience with such data that they are generally right-skewed–some data sets present finger tapping times that are unrealistically long. Worse yet, if we inspect enough samples from the prior predicted data, it will become clear that a few data sets have negative finger tapping time values.</p>
<p>We can also look at the distribution of summary statistics in the prior predictive data. Even if we don’t know beforehand what the data should look like, it’s very likely that we have some expectations for possible mean, minimum, or maximum values. For example, in the button-pressing example, it seems reasonable to assume that average finger tapping times are between <span class="math inline">\(200\)</span>-<span class="math inline">\(600\)</span> ms; finger tapping times are very unlikely to be below <span class="math inline">\(50\)</span> ms, and even long lapses of attention won’t be greater than a couple of seconds.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> Three distributions of summary statistics are shown in Figure <a href="ch-compbda.html#fig:priorpred-stats">3.6</a>.</p>

<div class="figure"><span style="display:block;" id="fig:priorpred-stats"/>
<img src="../Images/6582c96d95048d11a69bd66a87b88252.png" alt="The prior predictive distributions of the mean, minimum, and maximum values of the button-pressing model defined in section 3.2.1." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/priorpred-stats-1.svg"/>
<p class="caption">
FIGURE 3.6: The prior predictive distributions of the mean, minimum, and maximum values of the button-pressing model defined in section <a href="ch-compbda.html#sec-simplenormal">3.2.1</a>.
</p>
</div>
<p>Figure <a href="ch-compbda.html#fig:priorpred-stats">3.6</a> shows that we used much less prior information than what we could have: Our priors were encoding the information that any mean between <span class="math inline">\(0\)</span> and <span class="math inline">\(60000\)</span> ms is equally likely. It seems clear that a value close to <span class="math inline">\(0\)</span> or to <span class="math inline">\(60000\)</span> ms would be extremely surprising. This wide range of mean values occurs because of the uniform prior on <span class="math inline">\(\mu\)</span>. Similarly, maximum values are quite “uniform”, spanning a much wider range than what one would expect. Finally, in the distribution of minimum values, negative finger tapping times occur. This might seem surprising (our prior for <span class="math inline">\(\mu\)</span> excluded negative values), but the reason that negative values appear is that the prior is interpreted together with the likelihood <span class="citation">(Gelman, Simpson, and Betancourt <a href="#ref-gelmanPriorCanOften2017" role="doc-biblioref">2017</a>)</span>, and the likelihood is a normal distribution, which will allow for negative samples even if the location parameter <span class="math inline">\(\mu\)</span> is constrained to have only positive values.</p>
<p>To summarize the above discussion, the priors used in the example are clearly not very realistic given what we might know about finger tapping times for such a button-pressing task. This raises the question: what priors should we have chosen? In the next section, we consider this question.</p>
</div>
<div id="sec-sensitivity" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> The influence of priors:  sensitivity analysis<a href="ch-compbda.html#sec-sensitivity" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>For most cases that we will encounter in this book, there are four main classes of priors that we can choose from. Among Bayesians, there is no fixed nomenclature for classifying different kinds of priors. For this book, we have chosen specific names for each type of prior, but this is just a convention that we follow for consistency. There are also other classes of prior that we do not discuss in this book. An example is  improper priors such as <span class="math inline">\(\mathit{Uniform}(-\infty,+\infty)\)</span>, which are not proper probability distributions because the  area under the curve does not sum to one.</p>
<p>When thinking about priors, the reader should not get hung up on what precisely the name is for a particular type of prior; they should rather focus on what that prior means in the context of the research problem.</p>
<div id="flat-uninformative-priors" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span>  Flat, uninformative priors<a href="ch-compbda.html#flat-uninformative-priors" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>One option is to choose priors that are as uninformative as possible. The idea behind this approach is to let the data “speak for itself” and to not bias the statistical inference with “subjective” priors. There are several issues with this approach: First, the prior is as subjective as the likelihood, and in fact, different choices of likelihood might have a much stronger impact on the posterior than different choices of priors. Second, uninformative priors are in general unrealistic because they give equal weight to all values within the support of the prior distribution, ignoring the fact that usually there is some minimal information about the parameters of interest. Usually, at the very least, the order of magnitude is known (response times or finger tapping times will be in milliseconds and not days, EEG signals some microvolts and not volts, etc.). Third, uninformative priors make the sampling slower and might lead to convergence problems. Unless there is a large amount of data, it would be wise to avoid such priors. Fourth, it is not always clear which parameterization of a given distribution the flat priors should be assigned to. For example, the normal distribution is sometimes defined based on its standard deviation (<span class="math inline">\(\sigma\)</span>), variance (<span class="math inline">\(\sigma^2\)</span>), or precision (<span class="math inline">\(1/\sigma^2\)</span>): a flat prior for the standard deviation is not flat for the precision of the distribution. Although it is sometimes possible to find an uninformative prior that is invariant under a change of parameters <span class="citation">(also called Jeffreys priors; Jaynes <a href="#ref-jaynes2003probability" role="doc-biblioref">2003</a>, sec. 6.15; Jeffreys <a href="#ref-jeffreys1939theory" role="doc-biblioref">1939</a>, Chapter 3)</span>, this is not always the case. Finally, if Bayes factors need to be computed, uninformative priors can lead to very misleading conclusions (chapter <a href="ch-bf.html#ch-bf">13</a>).</p>
<p>In the button-pressing example discussed in this chapter, an example of a flat, uninformative prior would be <span class="math inline">\(\mu \sim \mathit{Uniform}(-10^{20},10^{20})\)</span>. On the millisecond scale, this is a very strange prior to use for a parameter representing mean button-pressing time: it allows for impossibly large positive values, and it also allows negative button-pressing times, which is of course impossible. It is technically possible to use such a prior, but it wouldn’t make much sense.</p>
</div>
<div id="regularizing-priors" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span>  Regularizing priors<a href="ch-compbda.html#regularizing-priors" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>If there does not exist much prior information (and if this information cannot be worked out through reasoning about the problem), and there is enough data (what “enough” means here will presently become clear when we look at specific examples), it is fine to use <em>regularizing priors</em>. These are priors that down-weight extreme values (that is, they provide regularization), they are usually not very informative, and mostly let the likelihood dominate in determining the posteriors. These priors are theory-neutral; that is, they usually do not bias the parameters to values supported by any prior belief or theory. The idea behind this type of prior is to help to stabilize computation. These priors are sometimes called  <em>weakly informative</em> or  <em>mildly informative</em> priors in the Bayesian literature. For many applications, they perform well, but as discussed in chapter <a href="ch-bf.html#ch-bf">13</a>, they tend to be problematic if Bayes factors need to be computed.</p>
<p>In the button-pressing example, an example of a regularizing prior would be <span class="math inline">\(\mu \sim \mathit{Normal}_{+}(0,1000)\)</span>. This is a normal distribution prior truncated at <span class="math inline">\(0\)</span> ms, and allows a relatively constrained range of positive values for button-pressing times (roughly, up to <span class="math inline">\(2000\)</span> ms or so). This is a regularizing prior because it rules out negative button-pressing times and down-weights extreme values over <span class="math inline">\(2000\)</span> ms.
Here, one could assume a non-truncated prior like <span class="math inline">\(\mathit{Normal}(0,1000)\)</span>. This could still be seen as a regularizing prior even though we don’t expect <span class="math inline">\(\mu\)</span> to be negative; the data would ensure that the posterior distribution has positive values (because we would have no negative button-pressing times in the data).</p>
</div>
<div id="principled-priors" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span>  Principled priors<a href="ch-compbda.html#principled-priors" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>The idea here is to have priors that encode all (or most of) the theory-neutral information that the researcher has. Since one generally knows what one’s data do and do not look like, it is possible to build priors that truly reflect the properties of potential data sets, using prior predictive checks. In this book, many examples of this class of priors will come up.</p>
<p>In the button-pressing data, an example of a principled prior would be <span class="math inline">\(\mu \sim \mathit{Normal}_{+}(250,100)\)</span>. This prior is not overly restrictive, but represents a guess about plausible button-pressing times. Prior predictive checks using principled priors should produce realistic distributions of the dependent variable.</p>
</div>
<div id="informative-priors" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span>  Informative priors<a href="ch-compbda.html#informative-priors" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>There are cases where a lot of prior knowledge exists. In general, unless there are very good reasons for having relatively informative priors (see chapter <a href="ch-bf.html#ch-bf">13</a>), it is not a good idea to let the priors have too much influence on the posterior. An example where informative priors would be important is when investigating a language-impaired population from which we can’t get many subjects, but a lot of previously published papers exist on the research topic.</p>
<p>In the button-pressing data, an informative prior could be based on a meta-analysis of previously published or existing data, or the result of prior elicitation from an expert (or multiple experts) on the topic under investigation. An example of an informative prior would be <span class="math inline">\(\mu \sim \mathit{Normal}_{+}(200,20)\)</span>. This prior will have some influence on the posterior for <span class="math inline">\(\mu\)</span>, especially when one has relatively sparse data.</p>
<p>These four options constitute a continuum. The uniform prior from the last model (section <a href="ch-compbda.html#sec-simplenormal">3.2.1</a>) falls between flat, uninformative and regularizing priors. In practical data analysis situations, we are mostly going to choose priors that fall between regularizing and principled. Informative priors, in the sense defined above, will be used only relatively rarely; but they become more important to consider when doing Bayes factor analyses (chapter <a href="ch-bf.html#ch-bf">13</a>).</p>
</div>
</div>
<div id="sec-revisit" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Revisiting the button-pressing example with different priors<a href="ch-compbda.html#sec-revisit" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>What would happen if even wider priors were used for the model defined previously (in section <a href="ch-compbda.html#sec-simplenormal">3.2.1</a>)? Suppose that every value between <span class="math inline">\(-10^{6}\)</span> and <span class="math inline">\(10^{6}\)</span> ms is assumed to be equally likely for the location parameter <span class="math inline">\(\mu\)</span>. This prior is clearly unrealistic and actually makes no sense at all: we are not expecting negative finger tapping times. Regarding the standard deviation, one could assume that any value between <span class="math inline">\(0\)</span> and <span class="math inline">\(10^{6}\)</span> is equally likely.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> The likelihood remains unchanged.</p>
<p><span class="math display" id="eq:rtpriorsflat">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Uniform}(-10^{6}, 10^{6}) \\
\sigma &amp;\sim \mathit{Uniform}(0,  10^{6})
\end{aligned}
\tag{3.6}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb115-1"><a href="ch-compbda.html#cb115-1" aria-hidden="true"/><span class="co"># The default settings are used when they are not set explicitly:</span></span>
<span id="cb115-2"><a href="ch-compbda.html#cb115-2" aria-hidden="true"/><span class="co"># 4 chains, with half of the iterations (set as 3000) as warmup.</span></span>
<span id="cb115-3"><a href="ch-compbda.html#cb115-3" aria-hidden="true"/>fit_press_unif &lt;-<span class="st"> </span><span class="kw">brm</span>(t <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb115-4"><a href="ch-compbda.html#cb115-4" aria-hidden="true"/>                      <span class="dt">data =</span> df_spacebar,</span>
<span id="cb115-5"><a href="ch-compbda.html#cb115-5" aria-hidden="true"/>                      <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb115-6"><a href="ch-compbda.html#cb115-6" aria-hidden="true"/>                      <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="op">-</span><span class="dv">10</span><span class="op">^</span><span class="dv">6</span>, <span class="dv">10</span><span class="op">^</span><span class="dv">6</span>),</span>
<span id="cb115-7"><a href="ch-compbda.html#cb115-7" aria-hidden="true"/>                                      <span class="dt">class =</span> Intercept,</span>
<span id="cb115-8"><a href="ch-compbda.html#cb115-8" aria-hidden="true"/>                                      <span class="dt">lb =</span> <span class="dv">-10</span><span class="op">^</span><span class="dv">6</span>,</span>
<span id="cb115-9"><a href="ch-compbda.html#cb115-9" aria-hidden="true"/>                                      <span class="dt">ub =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">6</span>),</span>
<span id="cb115-10"><a href="ch-compbda.html#cb115-10" aria-hidden="true"/>                                <span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>, <span class="dv">10</span><span class="op">^</span><span class="dv">6</span>),</span>
<span id="cb115-11"><a href="ch-compbda.html#cb115-11" aria-hidden="true"/>                                      <span class="dt">class =</span> sigma,</span>
<span id="cb115-12"><a href="ch-compbda.html#cb115-12" aria-hidden="true"/>                                      <span class="dt">lb =</span> <span class="dv">0</span>,</span>
<span id="cb115-13"><a href="ch-compbda.html#cb115-13" aria-hidden="true"/>                                      <span class="dt">ub =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">6</span>)),</span>
<span id="cb115-14"><a href="ch-compbda.html#cb115-14" aria-hidden="true"/>                      <span class="dt">iter =</span> <span class="dv">3000</span>,</span>
<span id="cb115-15"><a href="ch-compbda.html#cb115-15" aria-hidden="true"/>                      <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.99</span>,</span>
<span id="cb115-16"><a href="ch-compbda.html#cb115-16" aria-hidden="true"/>                                     <span class="dt">max_treedepth =</span> <span class="dv">15</span>))</span></code></pre></div>
<p>Even with these extremely unrealistic priors, which require us to change the number of iterations,<code>iter</code>, and <code>adapt_delta</code> and <code>max_treedepth</code> default values to achieve convergence, the output of the model is virtually identical to the previous one (see Figure <a href="ch-compbda.html#fig:postcomp">3.7</a>).</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb116-1"><a href="ch-compbda.html#cb116-1" aria-hidden="true"/>fit_press_unif</span></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   168.68      1.28   166.18   171.22 1.00     4192     2840
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    25.02      0.94    23.24    27.02 1.01      602      469
## 
## ...</code></pre>
<p>Next, consider what happens if very informative priors are used. Assume that mean values very close to <span class="math inline">\(400\)</span> ms are the most likely, and that the standard deviation of the finger tapping times is very close to <span class="math inline">\(100\)</span> ms. Given that this is a model of button-pressing times, such an informative prior seems wrong—<span class="math inline">\(200\)</span> ms seems like a more realistic mean button-pressing time, not <span class="math inline">\(400\)</span> ms. You can check this by doing an experiment yourself and looking at the recorded times; software like Linger (<a href="http://tedlab.mit.edu/~dr/Linger/" class="uri">http://tedlab.mit.edu/~dr/Linger/</a>) makes it easy to set up such an experiment.</p>
<p>The <span class="math inline">\(\mathit{Normal}_+\)</span> notation indicates a normal distribution truncated at zero such that only positive values are allowed (see online section <a href="regression-models-with-brms---extended.html#app-truncation">A.2</a> discusses this type of distribution in detail). Even though the prior for the standard deviation is restricted to be positive, we are not required to add <code>lb = 0</code> to the prior, and it is automatically taken into account by <code>brms</code>.</p>
<p><span class="math display" id="eq:infrtpriors">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Normal}(400, 10) \\
\sigma &amp;\sim \mathit{Normal}_+(100, 10)
\end{aligned}
\tag{3.7}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb118-1"><a href="ch-compbda.html#cb118-1" aria-hidden="true"/>fit_press_inf &lt;-<span class="st"> </span><span class="kw">brm</span>(t <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb118-2"><a href="ch-compbda.html#cb118-2" aria-hidden="true"/>  <span class="dt">data =</span> df_spacebar,</span>
<span id="cb118-3"><a href="ch-compbda.html#cb118-3" aria-hidden="true"/>  <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb118-4"><a href="ch-compbda.html#cb118-4" aria-hidden="true"/>  <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">400</span>, <span class="dv">10</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb118-5"><a href="ch-compbda.html#cb118-5" aria-hidden="true"/>            <span class="co"># `brms` knows that SDs need to be bounded</span></span>
<span id="cb118-6"><a href="ch-compbda.html#cb118-6" aria-hidden="true"/>            <span class="co"># to exclude values below zero:</span></span>
<span id="cb118-7"><a href="ch-compbda.html#cb118-7" aria-hidden="true"/>            <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">100</span>, <span class="dv">10</span>), <span class="dt">class =</span> sigma)))</span></code></pre></div>
<p>Despite these unrealistic but informative priors, the likelihood mostly dominates and the new posterior means and credible intervals are just a couple of milliseconds away from the previous estimates (see Figure <a href="ch-compbda.html#fig:postcomp">3.7</a>):</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb119-1"><a href="ch-compbda.html#cb119-1" aria-hidden="true"/>fit_press_inf</span></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   172.95      1.42   170.20   175.77 1.00     2639     2367
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    26.11      1.06    24.19    28.26 1.00     2599     2561
## 
## ...</code></pre>
<p>As a final example of a sensitivity analysis, choose some principled priors. Assuming that we have some prior experience with previous similar experiments, suppose the mean response time is expected to be around <span class="math inline">\(200\)</span> ms, with a 95% probability of the mean ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(400\)</span> ms. This uncertainty is perhaps unreasonably large, but one might want to allow a bit more uncertainty than one really thinks is reasonable <span class="citation">(this kind of conservativity in allowing somewhat more uncertainty is sometimes called Cromwell’s rule in Bayesian statistics; see O’Hagan and Forster <a href="#ref-kendall2004" role="doc-biblioref">2004</a>, sec. 3.19)</span>. In such a case, one can decide on the prior <span class="math inline">\(\mathit{Normal}(200, 100)\)</span>. Given that the experiment involves only one subject and the task is very simple, one might not expect the residual standard deviation <span class="math inline">\(\sigma\)</span> to be very large: As an example, one can settle on a location of <span class="math inline">\(50\)</span> ms for a truncated normal distribution, but still allow for relatively large uncertainty: <span class="math inline">\(Normal_{+}(50,50)\)</span>.
The prior specifications are summarized below.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Normal}(200, 100) \\
\sigma &amp;\sim \mathit{Normal}_+(50, 50)
\end{aligned}
\end{equation}\]</span></p>
<p>Why are these priors principled? The designation “principled” here largely depends on our  domain knowledge. The online chapter <a href="ch-priors.html#ch-priors">E</a> discusses how one can use domain knowledge when specifying priors.</p>
<p>One can achieve a better understanding of what a particular set of priors implies by visualizing the priors graphically, and carrying out prior predictive checks. These steps are skipped here, but these issues are discussed in detail in the online chapters <a href="ch-priors.html#ch-priors">E</a> and <a href="ch-workflow.html#ch-workflow">F</a>. These chapters will give more detailed information about choosing priors and on developing a principled workflow for Bayesian data analysis.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb121-1"><a href="ch-compbda.html#cb121-1" aria-hidden="true"/>fit_press_prin &lt;-</span>
<span id="cb121-2"><a href="ch-compbda.html#cb121-2" aria-hidden="true"/><span class="st">  </span><span class="kw">brm</span>(t <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb121-3"><a href="ch-compbda.html#cb121-3" aria-hidden="true"/>      <span class="dt">data =</span> df_spacebar,</span>
<span id="cb121-4"><a href="ch-compbda.html#cb121-4" aria-hidden="true"/>      <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb121-5"><a href="ch-compbda.html#cb121-5" aria-hidden="true"/>      <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">200</span>, <span class="dv">100</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb121-6"><a href="ch-compbda.html#cb121-6" aria-hidden="true"/>                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">50</span>, <span class="dv">50</span>), <span class="dt">class =</span> sigma)))</span></code></pre></div>
<p>The new estimates are virtually the same as before (see Figure <a href="ch-compbda.html#fig:postcomp">3.7</a>):</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb122-1"><a href="ch-compbda.html#cb122-1" aria-hidden="true"/>fit_press_prin</span></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   168.68      1.30   166.17   171.26 1.00     3775     2676
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    25.01      0.95    23.21    27.07 1.00     4210     2622
## 
## ...</code></pre>
<p>The above examples of using different priors should not be misunderstood to mean that priors never matter. When there is enough data, the likelihood will dominate in determining the posterior distributions. What constitutes “enough” data is also a function of the complexity of the model; as a general rule, more complex models require more data.</p>

<div class="figure"><span style="display:block;" id="fig:postcomp"/>
<img src="../Images/6b8b133fb6c8ee6ae69b7a0aaea21ef2.png" alt="Comparison of the posterior distributions from the model with “realistically” bounded uniform prior distribution (though still not recommended), fit_press, against the model with extremely unrealistically wide priors, fit_press_unif, the model with misspecified informative priors, fit_press_inf, and the model with principled priors, fit_press_prin. All the posteriors virtually overlap except for the posterior of fit_press_inf, which is shifted by a few milliseconds." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/postcomp-1.svg"/>
<p class="caption">
FIGURE 3.7: Comparison of the posterior distributions from the model with “realistically” bounded uniform prior distribution (though still not recommended), <code>fit_press</code>, against the model with extremely unrealistically wide priors, <code>fit_press_unif</code>, the model with misspecified informative priors, <code>fit_press_inf</code>, and the model with principled priors, <code>fit_press_prin</code>. All the posteriors virtually overlap except for the posterior of <code>fit_press_inf</code>, which is shifted by a few milliseconds.
</p>
</div>
<p>Even in cases where there is enough data and the likelihood dominates in determining the posteriors, regularizing, principled priors (i.e., priors that are more consistent with our a priori beliefs about the data) will in general speed-up model convergence.</p>
<p>In order to determine the extent to which the posterior is influenced by the priors, it is a good practice to carry out a sensitivity analysis: try different priors and either verify that the posterior doesn’t change drastically, or report how the posterior is affected by some specific priors <span class="citation">(for examples from psycholinguistics, see Vasishth et al. <a href="#ref-VasishthetalPLoSOne2013" role="doc-biblioref">2013</a>; Vasishth and Engelmann <a href="#ref-VasishthEngelmann2022" role="doc-biblioref">2022</a>)</span>. Chapter <a href="ch-bf.html#ch-bf">13</a> will demonstrate that sensitivity analysis becomes crucial for reporting Bayes factors; even in cases where the choice of priors does not affect the posterior distribution, it generally affects the Bayes factor.</p>
</div>
<div id="sec-ppd" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span>  Posterior predictive distribution<a href="ch-compbda.html#sec-ppd" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>The posterior predictive distribution is a collection of data sets generated from the model (the likelihood and the priors). Having obtained the posterior distributions of the parameters after taking into account the data, the posterior distributions can be used to generate future data from the model. In other words, given the posterior distributions of the parameters of the model, the posterior predictive distribution gives us some indication of what future data might look like.</p>
<p>Once the posterior distributions <span class="math inline">\(p(\boldsymbol{\Theta}\mid \boldsymbol{y})\)</span> are available, the predictions based on these distributions can be generated by integrating out the parameters:</p>
<p><span class="math display">\[\begin{equation}
p(\boldsymbol{y_{pred}}\mid \boldsymbol{y} ) = \int_{\boldsymbol{\Theta}} p(\boldsymbol{y_{pred}}, \boldsymbol{\Theta}\mid \boldsymbol{y})\, \mathrm{d}\boldsymbol{\Theta}= \int_{\boldsymbol{\Theta}}
p(\boldsymbol{y_{pred}}\mid \boldsymbol{\Theta},\boldsymbol{y})p(\boldsymbol{\Theta}\mid \boldsymbol{y})\, \mathrm{d}\boldsymbol{\Theta}
\end{equation}\]</span></p>
<p>Assuming that past and future observations are conditionally independent<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> given <span class="math inline">\(\boldsymbol{\Theta}\)</span>, i.e., <span class="math inline">\(p(\boldsymbol{y_{pred}}\mid \boldsymbol{\Theta},\boldsymbol{y})= p(\boldsymbol{y_{pred}}\mid \boldsymbol{\Theta})\)</span>, the above equation can be written as:</p>
<p><span class="math display" id="eq:postpp">\[\begin{equation}
p(\boldsymbol{y_{pred}}\mid \boldsymbol{y} )=\int_{\boldsymbol{\Theta}} p(\boldsymbol{y_{pred}}\mid \boldsymbol{\Theta}) p(\boldsymbol{\Theta}\mid \boldsymbol{y})\, \mathrm{d}\boldsymbol{\Theta}
\tag{3.8}
\end{equation}\]</span></p>
<p>In Equation <a href="ch-compbda.html#eq:postpp">(3.8)</a>, we are conditioning <span class="math inline">\(\boldsymbol{y_{pred}}\)</span> only on <span class="math inline">\(\boldsymbol{y}\)</span>, we do not condition on what we don’t know (<span class="math inline">\(\boldsymbol{\Theta}\)</span>); the unknown parameters have been integrated out. This posterior predictive distribution has important differences from predictions obtained with the frequentist approach. The frequentist approach gives a point estimate of each predicted observation given the maximum likelihood estimate of <span class="math inline">\(\boldsymbol{\Theta}\)</span> (a point value), whereas the Bayesian approach gives a distribution of values for each predicted observation. As with the prior predictive distribution, the integration can be carried out computationally by generating samples from the posterior predictive distribution. The same function that we created before, <code>normal_predictive_distribution()</code>, can be used here. The only difference is that instead of sampling <code>mu</code> and <code>sigma</code> from the priors, the samples come from the posterior.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb124-1"><a href="ch-compbda.html#cb124-1" aria-hidden="true"/>N_obs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_spacebar)</span>
<span id="cb124-2"><a href="ch-compbda.html#cb124-2" aria-hidden="true"/>mu_samples &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_press)<span class="op">$</span>b_Intercept</span>
<span id="cb124-3"><a href="ch-compbda.html#cb124-3" aria-hidden="true"/>sigma_samples &lt;-<span class="st"> </span><span class="kw">as_draws_df</span>(fit_press)<span class="op">$</span>sigma</span>
<span id="cb124-4"><a href="ch-compbda.html#cb124-4" aria-hidden="true"/><span class="kw">normal_predictive_distribution</span>(<span class="dt">mu_samples =</span> mu_samples,</span>
<span id="cb124-5"><a href="ch-compbda.html#cb124-5" aria-hidden="true"/>                               <span class="dt">sigma_samples =</span> sigma_samples,</span>
<span id="cb124-6"><a href="ch-compbda.html#cb124-6" aria-hidden="true"/>                               <span class="dt">N_obs =</span> N_obs)</span></code></pre></div>
<pre><code>## # A tibble: 1,444,000 × 3
##    iter trialn t_pred
##   &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;
## 1     1      1   128.
## 2     1      2   134.
## 3     1      3   167.
## # ℹ 1,443,997 more rows</code></pre>
<p>The <code>brms</code> function  <code>posterior_predict()</code> is a convenient function that delivers samples from the posterior predictive distribution. Using the command <code>posterior_predict(fit_press)</code> yields the predicted finger tapping times in a matrix, with the samples as rows and the observations (data-points) as columns. (Bear in mind that if a model is fit with <code>sample_prior = "only"</code>, the dependent variable is ignored and <code>posterior_predict()</code> will yield samples from the prior predictive distribution).</p>
<p>The posterior predictive distribution can be used to examine the “descriptive adequacy” of the model under consideration <span class="citation">(Gelman et al. <a href="#ref-Gelman14" role="doc-biblioref">2014</a>, Chapter 6; Shiffrin et al. <a href="#ref-shiffrinSurveyModelEvaluation2008" role="doc-biblioref">2008</a>)</span>. Examining the posterior predictive distribution to establish  descriptive adequacy is called  posterior predictive checking. The goal here is to establish that the posterior predictive data look more or less similar to the observed data. Achieving descriptive adequacy means that the current data could have been generated by the model. Although passing a test of descriptive adequacy is not strong evidence in favor of a model, a major failure in descriptive adequacy can be interpreted as strong evidence against a model <span class="citation">(Shiffrin et al. <a href="#ref-shiffrinSurveyModelEvaluation2008" role="doc-biblioref">2008</a>)</span>.
For this reason, comparing the descriptive adequacy of different models is not enough to differentiate between their relative performance.</p>
<p>When doing model comparison, it is important to consider the criteria that <span class="citation">Roberts and Pashler (<a href="#ref-rp" role="doc-biblioref">2000</a>)</span> define. Although <span class="citation">Roberts and Pashler (<a href="#ref-rp" role="doc-biblioref">2000</a>)</span> are more interested in process models and not necessarily Bayesian models, their criteria are important for any kind of model comparison. Their main point is that it is not enough to have a good fit to the data for a model to be convincing. One should check that the range of predictions that the model makes is reasonably constrained; if a model can capture any possible outcome, then the model fit to a particular data set is not so informative. In the Bayesian modeling context, although posterior predictive checks are important, they should been as only sanity checks to assess whether the model behavior is reasonable (for more on this point, see the online chapter <a href="ch-workflow.html#ch-workflow">F</a>).</p>
<p>In many cases, one can simply use the plotting functions provided with <code>brms</code> (these act as wrappers for  <code>bayesplot</code> functions). For example, the plotting function  <code>pp_check()</code> takes as arguments the model, the number of predicted data sets, and the type of visualization, and it can display different visualizations of posterior predictive checks. In these type of plots, the observed data are plotted as <span class="math inline">\(y\)</span> and predicted data as <span class="math inline">\(y_{rep}\)</span>. Below, we use <code>pp_check()</code> to investigate how well the observed distribution of finger tapping times fit our model based on some number (<span class="math inline">\(11\)</span> and <span class="math inline">\(100\)</span>) of samples of the posterior predictive distributions (that is, simulated data sets); see Figures <a href="ch-compbda.html#fig:normalppc">3.8</a> and <a href="ch-compbda.html#fig:normalppc2">3.9</a>.</p>

<div class="sourceCode" id="cb126"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb126-1"><a href="ch-compbda.html#cb126-1" aria-hidden="true"/><span class="kw">pp_check</span>(fit_press, <span class="dt">ndraws =</span> <span class="dv">11</span>, <span class="dt">type =</span> <span class="st">"hist"</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:normalppc"/>
<img src="../Images/6950f5f2641ead399d16632ff5700d14.png" alt="Histograms of eleven samples from the posterior predictive distribution of the model fit_press (\(y_{rep}\))." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/normalppc-1.svg"/>
<p class="caption">
FIGURE 3.8: Histograms of eleven samples from the posterior predictive distribution of the model <code>fit_press</code> (<span class="math inline">\(y_{rep}\)</span>).
</p>
</div>

<div class="sourceCode" id="cb127"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb127-1"><a href="ch-compbda.html#cb127-1" aria-hidden="true"/><span class="kw">pp_check</span>(fit_press, <span class="dt">ndraws =</span> <span class="dv">100</span>, <span class="dt">type =</span> <span class="st">"dens_overlay"</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:normalppc2"/>
<img src="../Images/d76b604646bdfb12fbc2a645d03546f6.png" alt="A posterior predictive check that shows the fit of the model fit_press in comparison to data sets from the posterior predictive distribution using an overlay of density plots." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/normalppc2-1.svg"/>
<p class="caption">
FIGURE 3.9: A posterior predictive check that shows the fit of the model <code>fit_press</code> in comparison to data sets from the posterior predictive distribution using an overlay of density plots.
</p>
</div>
<p>The data is slightly skewed and has no values smaller than <span class="math inline">\(100\)</span> ms, but the predictive distributions are centered and symmetrical; see Figures <a href="ch-compbda.html#fig:normalppc">3.8</a> and <a href="ch-compbda.html#fig:normalppc2">3.9</a>. This posterior predictive check shows a slight mismatch between the observed and predicted data. Can we build a better model? We’ll come back to this issue in the next section.</p>
</div>
<div id="the-influence-of-the-likelihood" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> The influence of the likelihood<a href="ch-compbda.html#the-influence-of-the-likelihood" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Finger tapping times (and response times in general) are not usually normally distributed. A more realistic distribution is the log-normal. A random variable (such as time) that is log-normally distributed takes only positive real values and is right-skewed. Although other distributions can also produce data with such properties, the log-normal will turn out to be a pretty reasonable distribution for finger tapping times and response times.</p>
<div id="sec-lnfirst" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> The  log-normal likelihood<a href="ch-compbda.html#sec-lnfirst" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>If <span class="math inline">\(\boldsymbol{y}\)</span> is log-normally distributed, this means that <span class="math inline">\(\log(\boldsymbol{y})\)</span> is normally distributed.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> The  log-normal distribution is also defined using the parameters location, <span class="math inline">\(\mu\)</span>, and scale, <span class="math inline">\(\sigma\)</span>, but these are on the log ms scale; they correspond to the mean and standard deviation of the logarithm of the data <span class="math inline">\(\boldsymbol{y}\)</span>, <span class="math inline">\(\log(\boldsymbol{y})\)</span>, which will be normally distributed. Thus, when we model some data <span class="math inline">\(\boldsymbol{y}\)</span> using the log-normal likelihood, the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are on a different scale than the data <span class="math inline">\(\boldsymbol{y}\)</span>. Equation <a href="ch-compbda.html#eq:lognormalequations">(3.9)</a> shows the relationship between the log-normal and the normal.</p>
<p><span class="math display" id="eq:lognormalequations">\[\begin{equation}
\begin{aligned}
\log(\boldsymbol{y}) &amp;\sim \mathit{Normal}( \mu, \sigma)\\
\boldsymbol{y} &amp;\sim \mathit{LogNormal}( \mu, \sigma)
\end{aligned}
\tag{3.9}
\end{equation}\]</span></p>
<p>We can obtain samples from the log-normal distribution, using the normal distribution by first setting an auxiliary variable <span class="math inline">\(z\)</span>, so that <span class="math inline">\(z = \log(y)\)</span>. This means that <span class="math inline">\(z \sim \mathit{Normal}(\mu, \sigma)\)</span>.
Then we can just use <span class="math inline">\(exp(z)\)</span> as samples from the <span class="math inline">\(\mathit{LogNormal}(\mu, \sigma)\)</span>, since <span class="math inline">\(\exp(z) =\exp(\log(y)) = y\)</span>. The code below produces Figure <a href="ch-compbda.html#fig:logndemo">3.10</a>.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb128-1"><a href="ch-compbda.html#cb128-1" aria-hidden="true"/>mu &lt;-<span class="st"> </span><span class="dv">6</span></span>
<span id="cb128-2"><a href="ch-compbda.html#cb128-2" aria-hidden="true"/>sigma &lt;-<span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb128-3"><a href="ch-compbda.html#cb128-3" aria-hidden="true"/>N &lt;-<span class="st"> </span><span class="dv">500000</span></span>
<span id="cb128-4"><a href="ch-compbda.html#cb128-4" aria-hidden="true"/><span class="co"># Generate N random samples from a log-normal distribution</span></span>
<span id="cb128-5"><a href="ch-compbda.html#cb128-5" aria-hidden="true"/>sl &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(N, mu, sigma)</span>
<span id="cb128-6"><a href="ch-compbda.html#cb128-6" aria-hidden="true"/><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">samples =</span> sl), <span class="kw">aes</span>(samples)) <span class="op">+</span></span>
<span id="cb128-7"><a href="ch-compbda.html#cb128-7" aria-hidden="true"/><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">after_stat</span>(density)), <span class="dt">binwidth =</span> <span class="dv">50</span>) <span class="op">+</span></span>
<span id="cb128-8"><a href="ch-compbda.html#cb128-8" aria-hidden="true"/><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Log-normal distribution</span><span class="ch">\n</span><span class="st">"</span>) <span class="op">+</span></span>
<span id="cb128-9"><a href="ch-compbda.html#cb128-9" aria-hidden="true"/><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>))</span>
<span id="cb128-10"><a href="ch-compbda.html#cb128-10" aria-hidden="true"/><span class="co"># Generate N random samples from a normal distribution,</span></span>
<span id="cb128-11"><a href="ch-compbda.html#cb128-11" aria-hidden="true"/><span class="co"># and then exponentiate them</span></span>
<span id="cb128-12"><a href="ch-compbda.html#cb128-12" aria-hidden="true"/>sn &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">rnorm</span>(N, mu, sigma))</span>
<span id="cb128-13"><a href="ch-compbda.html#cb128-13" aria-hidden="true"/><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">samples =</span> sn), <span class="kw">aes</span>(samples)) <span class="op">+</span></span>
<span id="cb128-14"><a href="ch-compbda.html#cb128-14" aria-hidden="true"/><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">after_stat</span>(density)), <span class="dt">binwidth =</span> <span class="dv">50</span>) <span class="op">+</span></span>
<span id="cb128-15"><a href="ch-compbda.html#cb128-15" aria-hidden="true"/><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Exponentiated samples from</span><span class="ch">\n</span><span class="st">a normal distribution"</span>) <span class="op">+</span></span>
<span id="cb128-16"><a href="ch-compbda.html#cb128-16" aria-hidden="true"/><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:logndemo"/>
<img src="../Images/318135851e5c540b9fc30bb9b6f93ee6.png" alt="Two log-normal distributions with the same parameters generated by either generating samples from a log-normal distribution or exponentiating samples from a normal distribution." width="49%" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/logndemo-1.svg"/><img src="../Images/081dffb48f09a996b139aad827733364.png" alt="Two log-normal distributions with the same parameters generated by either generating samples from a log-normal distribution or exponentiating samples from a normal distribution." width="49%" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/logndemo-2.svg"/>
<p class="caption">
FIGURE 3.10: Two log-normal distributions with the same parameters generated by either generating samples from a log-normal distribution or exponentiating samples from a normal distribution.
</p>
</div>
</div>
<div id="sec-lognormal" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Using a log-normal likelihood to fit data from a single subject pressing a button repeatedly<a href="ch-compbda.html#sec-lognormal" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>If we assume that finger tapping times are log-normally distributed, the likelihood function changes as follows:</p>
<p><span class="math display">\[\begin{equation}
t_n \sim \mathit{LogNormal}(\mu,\sigma)
\end{equation}\]</span></p>
<p>But now the scale of the priors needs to change! Let’s start with uniform priors for ease of exposition, even though, as we mentioned earlier, these are not really appropriate here. (More realistic priors are discussed below.)</p>
<p><span class="math display" id="eq:logpriorsunif">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Uniform}(0, 11) \\
\sigma &amp;\sim \mathit{Uniform}(0, 1) \\
\end{aligned}
\tag{3.10}
\end{equation}\]</span></p>
<p>Because the parameters are on a different scale than the dependent variable, their interpretation changes and it is more complex than if we were dealing with a linear model that assumes a normal likelihood (location and scale do not coincide with the mean and standard deviation of the log-normal):</p>
<ul>
<li> <em>The location <span class="math inline">\(\mu\)</span></em>: In our previous linear model, <span class="math inline">\(\mu\)</span> represented the mean (in a normal distribution, the mean happens to be identical to the median and the mode). But now, the mean needs to be calculated by computing <span class="math inline">\(\exp(\mu +\sigma ^{2}/2)\)</span>. In other words, in the log-normal, the mean is dependent on both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. The median is just <span class="math inline">\(\exp(\mu)\)</span>. Notice that the prior of <span class="math inline">\(\mu\)</span> is not on the milliseconds scale, but rather on the log milliseconds scale.</li>
<li> <em>The scale <span class="math inline">\(\sigma\)</span></em>: This is the standard deviation of the normal distribution of <span class="math inline">\(\log(\boldsymbol{y})\)</span>. The standard deviation of a log-normal distribution with <em>location</em> <span class="math inline">\(\mu\)</span> and <em>scale</em> <span class="math inline">\(\sigma\)</span> will be <span class="math inline">\(\exp(\mu +\sigma ^{2}/2)\times \sqrt{\exp(\sigma^2)- 1}\)</span>. Unlike the normal distribution, the spread of the log-normal distribution depends on both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</li>
</ul>
<p>To understand the meaning of the priors on the millisecond scale, both the priors and the likelihood need to be taken into account. Generating a prior predictive distribution will help in interpreting the priors. This distribution can be generated by just exponentiating the samples produced by <code>normal_predictive_distribution()</code> (or, alternatively, edit the function by replacing <code>rnorm()</code> with <code>rlnorm()</code>).</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb129-1"><a href="ch-compbda.html#cb129-1" aria-hidden="true"/>N_samples &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb129-2"><a href="ch-compbda.html#cb129-2" aria-hidden="true"/>N_obs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_spacebar)</span>
<span id="cb129-3"><a href="ch-compbda.html#cb129-3" aria-hidden="true"/>mu_samples &lt;-<span class="st"> </span><span class="kw">runif</span>(N_samples, <span class="dv">0</span>, <span class="dv">11</span>)</span>
<span id="cb129-4"><a href="ch-compbda.html#cb129-4" aria-hidden="true"/>sigma_samples &lt;-<span class="st"> </span><span class="kw">runif</span>(N_samples, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb129-5"><a href="ch-compbda.html#cb129-5" aria-hidden="true"/>prior_pred_ln &lt;-</span>
<span id="cb129-6"><a href="ch-compbda.html#cb129-6" aria-hidden="true"/><span class="st">  </span><span class="kw">normal_predictive_distribution</span>(<span class="dt">mu_samples =</span> mu_samples,</span>
<span id="cb129-7"><a href="ch-compbda.html#cb129-7" aria-hidden="true"/>                                 <span class="dt">sigma_samples =</span> sigma_samples,</span>
<span id="cb129-8"><a href="ch-compbda.html#cb129-8" aria-hidden="true"/>                                 <span class="dt">N_obs =</span> N_obs) <span class="op">%&gt;%</span></span>
<span id="cb129-9"><a href="ch-compbda.html#cb129-9" aria-hidden="true"/><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">t_pred =</span> <span class="kw">exp</span>(t_pred))</span></code></pre></div>
<p>Next, plot the distribution of some representative statistics; see Figure <a href="ch-compbda.html#fig:priorpredlogunif">3.11</a>.</p>

<div class="figure"><span style="display:block;" id="fig:priorpredlogunif"/>
<img src="../Images/50718c6c0e8d62a9daf6747ae835b69d.png" alt="The prior predictive distribution of the mean, median, minimum, and maximum value of the log-normal model with priors defined in Equation (3.10), that is \(\mu \sim \mathit{Uniform}(0, 11)\) and \(\sigma \sim \mathit{Uniform}(0, 1)\). The x-axis shows values back-transformed from the log-scale." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/priorpredlogunif-1.svg"/>
<p class="caption">
FIGURE 3.11: The prior predictive distribution of the mean, median, minimum, and maximum value of the log-normal model with priors defined in Equation <a href="ch-compbda.html#eq:logpriorsunif">(3.10)</a>, that is <span class="math inline">\(\mu \sim \mathit{Uniform}(0, 11)\)</span> and <span class="math inline">\(\sigma \sim \mathit{Uniform}(0, 1)\)</span>. The x-axis shows values back-transformed from the log-scale.
</p>
</div>
<p>We cannot generate negative values any more, since <span class="math inline">\(\exp(\)</span>any finite real number<span class="math inline">\() &gt; 0\)</span>. These priors might work in the sense that the model might converge; but it would be better to have regularizing priors for the model. An example of regularizing priors:</p>
<p><span class="math display" id="eq:logpriorsnorm">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim \mathit{Normal}(6, 1.5) \\
\sigma &amp;\sim \mathit{Normal}_+(0, 1) \\
\end{aligned}
\tag{3.11}
\end{equation}\]</span></p>
<p>The prior for <span class="math inline">\(\sigma\)</span> here is a  truncated distribution, and although its location is zero, this is not its mean. We can calculate its approximate mean from a large number of random samples of the prior distribution using the function  <code>rtnorm()</code> from the package <code>extraDistr</code>. In this function, we have to set the parameter <code>a = 0</code> to express the fact that the normal distribution is truncated from the left at 0. (Online section <a href="regression-models-with-brms---extended.html#app-truncation">A.2</a> discusses this type of
distribution in detail):</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb130-1"><a href="ch-compbda.html#cb130-1" aria-hidden="true"/><span class="kw">mean</span>(<span class="kw">rtnorm</span>(<span class="dv">100000</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">a =</span> <span class="dv">0</span>))</span></code></pre></div>
<pre><code>## [1] 0.798</code></pre>
<p>Even before generating the prior predictive distributions, we can calculate the range within which we are 95% sure that the expected median of the observations will lie. We do this by looking at what happens at two standard deviations away from the mean of the prior, <span class="math inline">\(\mu\)</span>, that is <span class="math inline">\(6 - 2\times 1.5\)</span> and <span class="math inline">\(6 + 2\times 1.5\)</span>, and exponentiating these values:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb132-1"><a href="ch-compbda.html#cb132-1" aria-hidden="true"/><span class="kw">c</span>(<span class="dt">lower =</span> <span class="kw">exp</span>(<span class="dv">6</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="fl">1.5</span>),</span>
<span id="cb132-2"><a href="ch-compbda.html#cb132-2" aria-hidden="true"/>  <span class="dt">higher =</span> <span class="kw">exp</span>(<span class="dv">6</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="fl">1.5</span>))</span></code></pre></div>
<pre><code>##  lower higher 
##   20.1 8103.1</code></pre>
<p>This means that the prior for <span class="math inline">\(\mu\)</span> is still not too informative (these are medians; the actual values generated by the log-normal distribution can be much more spread out). Next, plot the distribution of some representative statistics of the prior predictive distributions.
<code>brms</code> allows one to sample from the priors, ignoring the observed data <code>t</code> , by setting <code>sample_prior = "only"</code> in the <code>brm</code> function.</p>
<p>If we want to use <code>brms</code> to generate prior predictive data in this manner even before we have any data, we do need to include a data frame with the relevant dependent variable (<code>y</code> in this case). Setting <code>sample_prior = "only"</code> will ignore the values of the dependent variable.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb134-1"><a href="ch-compbda.html#cb134-1" aria-hidden="true"/>fit_prior_press_ln &lt;-</span>
<span id="cb134-2"><a href="ch-compbda.html#cb134-2" aria-hidden="true"/><span class="st">  </span><span class="kw">brm</span>(t <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb134-3"><a href="ch-compbda.html#cb134-3" aria-hidden="true"/>      <span class="dt">data =</span> df_spacebar,</span>
<span id="cb134-4"><a href="ch-compbda.html#cb134-4" aria-hidden="true"/>      <span class="dt">family =</span> <span class="kw">lognormal</span>(),</span>
<span id="cb134-5"><a href="ch-compbda.html#cb134-5" aria-hidden="true"/>      <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb134-6"><a href="ch-compbda.html#cb134-6" aria-hidden="true"/>                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma)),</span>
<span id="cb134-7"><a href="ch-compbda.html#cb134-7" aria-hidden="true"/>      <span class="dt">sample_prior =</span> <span class="st">"only"</span>,</span>
<span id="cb134-8"><a href="ch-compbda.html#cb134-8" aria-hidden="true"/>      <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>))</span></code></pre></div>
<p>To avoid the warnings, increase the <code>adapt_delta</code> parameter’s default value from <span class="math inline">\(0.8\)</span> to <span class="math inline">\(0.9\)</span> to simulate the data. Since Stan samples from the prior distributions in the same way that it samples from the posterior distribution, one should not ignore warnings; always ensure that the model converges. In that respect, the custom function <code>normal_predictive_distribution()</code> defined in section <a href="ch-compbda.html#sec-priorpred">3.3</a> has the advantage that it will always yield independent samples from the prior distribution and will not experience any convergence problems. This is because it just relies on the <code>rnorm()</code> function in R.</p>
<p>Plot the prior predictive distribution of means with the following code. In a prior predictive distribution, we generally want to ignore the data; this requires setting <code>prefix = "ppd"</code> in <code>pp_check()</code>.</p>
<p>To plot the distribution of minimum, and maximum values, just replace <code>mean</code> with <code>min</code>, and <code>max</code> respectively. The distributions of the three statistics are displayed in Figure <a href="ch-compbda.html#fig:priorpredlognorm">3.12</a>.</p>

<div class="sourceCode" id="cb135"><pre class="sourceCode r fold-hide"><code class="sourceCode r"><span id="cb135-1"><a href="ch-compbda.html#cb135-1" aria-hidden="true"/></span>
<span id="cb135-2"><a href="ch-compbda.html#cb135-2" aria-hidden="true"/>p1 &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_press_ln,</span>
<span id="cb135-3"><a href="ch-compbda.html#cb135-3" aria-hidden="true"/>               <span class="dt">type =</span> <span class="st">"stat"</span>,</span>
<span id="cb135-4"><a href="ch-compbda.html#cb135-4" aria-hidden="true"/>               <span class="dt">stat =</span> <span class="st">"mean"</span>,</span>
<span id="cb135-5"><a href="ch-compbda.html#cb135-5" aria-hidden="true"/>               <span class="dt">prefix =</span> <span class="st">"ppd"</span>) <span class="op">+</span></span>
<span id="cb135-6"><a href="ch-compbda.html#cb135-6" aria-hidden="true"/><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">300000</span>)) <span class="op">+</span></span>
<span id="cb135-7"><a href="ch-compbda.html#cb135-7" aria-hidden="true"/><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">"Finger tapping times (ms)"</span>,</span>
<span id="cb135-8"><a href="ch-compbda.html#cb135-8" aria-hidden="true"/>                     <span class="dt">trans =</span> <span class="st">"log"</span>,</span>
<span id="cb135-9"><a href="ch-compbda.html#cb135-9" aria-hidden="true"/>                     <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">100000</span>),</span>
<span id="cb135-10"><a href="ch-compbda.html#cb135-10" aria-hidden="true"/>                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">"0.001"</span>, <span class="st">"1"</span>, <span class="st">"100"</span>, <span class="st">"1000"</span>, <span class="st">"10000"</span>,</span>
<span id="cb135-11"><a href="ch-compbda.html#cb135-11" aria-hidden="true"/>                                <span class="st">"100000"</span>)) <span class="op">+</span></span>
<span id="cb135-12"><a href="ch-compbda.html#cb135-12" aria-hidden="true"/><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Prior predictive distribution of means"</span>)</span>
<span id="cb135-13"><a href="ch-compbda.html#cb135-13" aria-hidden="true"/>p2 &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_press_ln,</span>
<span id="cb135-14"><a href="ch-compbda.html#cb135-14" aria-hidden="true"/>               <span class="dt">type =</span> <span class="st">"stat"</span>,</span>
<span id="cb135-15"><a href="ch-compbda.html#cb135-15" aria-hidden="true"/>               <span class="dt">stat =</span> <span class="st">"min"</span>,</span>
<span id="cb135-16"><a href="ch-compbda.html#cb135-16" aria-hidden="true"/>               <span class="dt">prefix =</span> <span class="st">"ppd"</span>) <span class="op">+</span></span>
<span id="cb135-17"><a href="ch-compbda.html#cb135-17" aria-hidden="true"/><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">300000</span>)) <span class="op">+</span></span>
<span id="cb135-18"><a href="ch-compbda.html#cb135-18" aria-hidden="true"/><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">"Finger tapping times (ms)"</span>,</span>
<span id="cb135-19"><a href="ch-compbda.html#cb135-19" aria-hidden="true"/>                     <span class="dt">trans =</span> <span class="st">"log"</span>,</span>
<span id="cb135-20"><a href="ch-compbda.html#cb135-20" aria-hidden="true"/>                     <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">100000</span>),</span>
<span id="cb135-21"><a href="ch-compbda.html#cb135-21" aria-hidden="true"/>                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">"0.001"</span>, <span class="st">"1"</span>, <span class="st">"100"</span>, <span class="st">"1000"</span>, <span class="st">"10000"</span>,</span>
<span id="cb135-22"><a href="ch-compbda.html#cb135-22" aria-hidden="true"/>                                <span class="st">"100000"</span>)) <span class="op">+</span></span>
<span id="cb135-23"><a href="ch-compbda.html#cb135-23" aria-hidden="true"/><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Prior predictive distribution of minimum values"</span>)</span>
<span id="cb135-24"><a href="ch-compbda.html#cb135-24" aria-hidden="true"/>p3 &lt;-<span class="st"> </span><span class="kw">pp_check</span>(fit_prior_press_ln,</span>
<span id="cb135-25"><a href="ch-compbda.html#cb135-25" aria-hidden="true"/>               <span class="dt">type =</span> <span class="st">"stat"</span>,</span>
<span id="cb135-26"><a href="ch-compbda.html#cb135-26" aria-hidden="true"/>               <span class="dt">stat =</span> <span class="st">"max"</span>,</span>
<span id="cb135-27"><a href="ch-compbda.html#cb135-27" aria-hidden="true"/>               <span class="dt">prefix =</span> <span class="st">"ppd"</span>) <span class="op">+</span></span>
<span id="cb135-28"><a href="ch-compbda.html#cb135-28" aria-hidden="true"/><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">300000</span>)) <span class="op">+</span></span>
<span id="cb135-29"><a href="ch-compbda.html#cb135-29" aria-hidden="true"/><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">"Finger tapping times (ms)"</span>,</span>
<span id="cb135-30"><a href="ch-compbda.html#cb135-30" aria-hidden="true"/>                     <span class="dt">trans =</span> <span class="st">"log"</span>,</span>
<span id="cb135-31"><a href="ch-compbda.html#cb135-31" aria-hidden="true"/>                     <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">100000</span>),</span>
<span id="cb135-32"><a href="ch-compbda.html#cb135-32" aria-hidden="true"/>                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">"0.001"</span>, <span class="st">"1"</span>, <span class="st">"10"</span>, <span class="st">"1000"</span>, <span class="st">"10000"</span>,</span>
<span id="cb135-33"><a href="ch-compbda.html#cb135-33" aria-hidden="true"/>                                <span class="st">"100000"</span>)) <span class="op">+</span></span>
<span id="cb135-34"><a href="ch-compbda.html#cb135-34" aria-hidden="true"/><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Prior predictive distribution of maximum values"</span>)</span>
<span id="cb135-35"><a href="ch-compbda.html#cb135-35" aria-hidden="true"/><span class="kw">plot_grid</span>(p1, p2, p3, <span class="dt">nrow =</span> <span class="dv">3</span>, <span class="dt">ncol =</span><span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:priorpredlognorm"/>
<img src="../Images/a571b72cfab0ec71b0f7f70982434c37.png" alt="The prior predictive distributions of the mean, maximum, and minimum values of the log-normal model with priors defined in Equation (3.11). The prior predictive distributions are labeled \(y_{pred}\). The x-axis shows values back-transformed from the log-scale." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/priorpredlognorm-1.svg"/>
<p class="caption">
FIGURE 3.12: The prior predictive distributions of the mean, maximum, and minimum values of the log-normal model with priors defined in Equation <a href="ch-compbda.html#eq:logpriorsnorm">(3.11)</a>. The prior predictive distributions are labeled <span class="math inline">\(y_{pred}\)</span>. The x-axis shows values back-transformed from the log-scale.
</p>
</div>
<p>Figure <a href="ch-compbda.html#fig:priorpredlognorm">3.12</a> shows that the priors used here are still quite uninformative. The tails of the prior predictive distributions that correspond to our normal priors shown in Figure <a href="ch-compbda.html#fig:priorpredlognorm">3.12</a> are even further to the right, reaching more extreme values than for the prior predictive distributions generated by uniform priors (shown in Figure <a href="ch-compbda.html#fig:priorpredlogunif">3.11</a>). The new priors are still far from representing our prior knowledge. We could run more iterations of choosing priors and generating prior predictive distributions until we have priors that generate realistic data. However, given that the bulk of the distributions of the mean, maximum, minimum values lies roughly in the correct order of magnitude, these priors are going to be acceptable. In general, summary statistics (e.g., mean, median, min, max) can be used to test whether the priors are in a plausible range. This can be done by defining, for the particular research problem under study, the extreme data that would be very implausible to ever observe (e.g., reading times at a word larger than one minute) and choosing priors such that such extreme finger tapping times occur only very rarely in the prior predictive distribution.</p>
<p>Next, fit the model; recall that both the distribution family and prior change in comparison to the previous example.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb136-1"><a href="ch-compbda.html#cb136-1" aria-hidden="true"/>fit_press_ln &lt;-</span>
<span id="cb136-2"><a href="ch-compbda.html#cb136-2" aria-hidden="true"/><span class="st">  </span><span class="kw">brm</span>(t <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb136-3"><a href="ch-compbda.html#cb136-3" aria-hidden="true"/>      <span class="dt">data =</span> df_spacebar,</span>
<span id="cb136-4"><a href="ch-compbda.html#cb136-4" aria-hidden="true"/>      <span class="dt">family =</span> <span class="kw">lognormal</span>(),</span>
<span id="cb136-5"><a href="ch-compbda.html#cb136-5" aria-hidden="true"/>      <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb136-6"><a href="ch-compbda.html#cb136-6" aria-hidden="true"/>                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma)))</span></code></pre></div>
<p>When we look at the summary of the posterior, the parameters are on the log-scale:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb137-1"><a href="ch-compbda.html#cb137-1" aria-hidden="true"/>fit_press_ln</span></code></pre></div>
<pre><code>## ...
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     5.12      0.01     5.10     5.13 1.00     4098     2796
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.13      0.01     0.13     0.15 1.00     2463     1999
## 
## ...</code></pre>
<p>If the research goal is to find out how long it takes to press the space bar in milliseconds, we need to transform the <span class="math inline">\(\mu\)</span> (or <code>Intercept</code> in the model) to milliseconds. Because the median of the log-normal distribution is <span class="math inline">\(\exp(\mu)\)</span>, the following returns the median estimate in milliseconds (recall that for the mean we would need to calculate <span class="math inline">\(\exp(\mu +\sigma ^{2}/2)\)</span>):</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb139-1"><a href="ch-compbda.html#cb139-1" aria-hidden="true"/>estimate_ms &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">as_draws_df</span>(fit_press_ln)<span class="op">$</span>b_Intercept)</span></code></pre></div>
<p>To display the mean and 95% credible interval of these samples, type:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb140-1"><a href="ch-compbda.html#cb140-1" aria-hidden="true"/><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(estimate_ms),</span>
<span id="cb140-2"><a href="ch-compbda.html#cb140-2" aria-hidden="true"/>  <span class="kw">quantile</span>(estimate_ms, <span class="dt">probs =</span> <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>)))</span></code></pre></div>
<pre><code>##  mean  2.5% 97.5% 
##   167   165   169</code></pre>
<p>Next, check whether the predicted data sets look similar to
the observed data set. See Figure <a href="ch-compbda.html#fig:logppc">3.13</a>; compare this with the earlier Figure <a href="ch-compbda.html#fig:normalppc2">3.9</a>.</p>

<div class="sourceCode" id="cb142"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb142-1"><a href="ch-compbda.html#cb142-1" aria-hidden="true"/><span class="kw">pp_check</span>(fit_press_ln, <span class="dt">ndraws =</span> <span class="dv">100</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:logppc"/>
<img src="../Images/1ca36e870d3ba5a0f7a25fa6fcd8f79d.png" alt="The posterior predictive distribution of fit_press_ln." width="672" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/logppc-1.svg"/>
<p class="caption">
FIGURE 3.13: The posterior predictive distribution of <code>fit_press_ln</code>.
</p>
</div>
<p>The key question is: Are the posterior predicted data now more similar to the observed data, compared to the case where we had a normal likelihood? According to Figure <a href="ch-compbda.html#fig:logppc">3.13</a>, it seems so, but it’s not easy to tell.</p>
<p>Another way to examine the extent to which the predicted data looks similar to the observed data would be to look at the distribution of some summary statistic. As with prior predictive distributions, examine the distribution of representative summary statistics for the data sets generated by different models. However, in contrast with what occurs with prior predictive distributions, at this point we have a clear reference, our observations, and this means that we can compare the summary statistics with the observed statistics from our data. We suspect that the normal distribution would generate finger tapping times that are too fast (since the normal distribution is symmetrical) and that the log-normal distribution may capture the long tail better than the normal model. Based on this supposition, compute the distribution of minimum and maximum values for the posterior predictive distributions, and compare them with the minimum and maximum value respectively in the data. The function <code>pp_check()</code> implements this by specifying <code>stat</code> either <code>"min"</code> or <code>"max"</code> for both <code>fit_press</code>, and <code>fit_press_ln</code>; an example is shown below. The plots are shown in Figures <a href="ch-compbda.html#fig:ppcheckmin">3.14</a> and <a href="ch-compbda.html#fig:ppcheckmax">3.15</a>.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb143-1"><a href="ch-compbda.html#cb143-1" aria-hidden="true"/><span class="kw">pp_check</span>(fit_press, <span class="dt">type =</span> <span class="st">"stat"</span>, <span class="dt">stat =</span> <span class="st">"min"</span>)</span></code></pre></div>

<div class="figure"><span style="display:block;" id="fig:ppcheckmin"/>
<img src="../Images/f10b6153131b979898dabaad91a0a780.png" alt="The distributions of minimum values in a posterior predictive check, using the normal and log-normal probability density functions. The minimum in the data is \(110\) ms." width="45%" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/ppcheckmin-1.svg"/><img src="../Images/f0447b734f05ef191a54add024b56cc2.png" alt="The distributions of minimum values in a posterior predictive check, using the normal and log-normal probability density functions. The minimum in the data is \(110\) ms." width="45%" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/ppcheckmin-2.svg"/>
<p class="caption">
FIGURE 3.14: The distributions of minimum values in a posterior predictive check, using the normal and log-normal probability density functions. The minimum in the data is <span class="math inline">\(110\)</span> ms.
</p>
</div>

<div class="figure"><span style="display:block;" id="fig:ppcheckmax"/>
<img src="../Images/c14cf134aa05b971b7fae86ae1bfb405.png" alt="The distributions of maximum values in a posterior predictive check using the normal and log-normal. The maximum in the data is \(409\) ms." width="45%" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/ppcheckmax-1.svg"/><img src="../Images/04012bec32a6cee12702b3a0c85a1e01.png" alt="The distributions of maximum values in a posterior predictive check using the normal and log-normal. The maximum in the data is \(409\) ms." width="45%" data-original-src="https://bruno.nicenboim.me/bayescogsci/bayescogsci_files/figure-html/ppcheckmax-2.svg"/>
<p class="caption">
FIGURE 3.15: The distributions of maximum values in a posterior predictive check using the normal and log-normal. The maximum in the data is <span class="math inline">\(409\)</span> ms.
</p>
</div>
<p>Figure <a href="ch-compbda.html#fig:ppcheckmin">3.14</a> shows that the log-normal likelihood does a slightly better job since the minimum value is contained in the bulk of the log-normal distribution and in the tail of the normal one. Figure <a href="ch-compbda.html#fig:ppcheckmax">3.15</a> shows that both models are unable to capture the maximum value of the observed data. One explanation for this is that the log-normal-ish observations in our data are being generated by the task of pressing as fast as possible, whereas the observations with long finger tapping times are possibly being generated by lapses of attention.
If this assumption is correct, this would mean that the distribution of button pressing times is a mixture of two distributions; modeling such a mixture process involves more complex tools that we will take up in chapter <a href="ch-mixture.html#ch-mixture">17</a>.</p>
<p>This completes our introduction to <code>brms</code>. We are now ready to learn more about regression models.</p>
</div>
</div>
<div id="list-of-the-most-important-commands" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> List of the most important commands<a href="ch-compbda.html#list-of-the-most-important-commands" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Here is a list of the most important commands we learned in this chapter.</p>
<ul>
<li>The core <code>brms</code> function for fitting models, for generating prior predictive and posterior predictive data:</li>
</ul>
<div class="sourceCode" id="cb144"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb144-1"><a href="ch-compbda.html#cb144-1" aria-hidden="true"/>fit_press &lt;-</span>
<span id="cb144-2"><a href="ch-compbda.html#cb144-2" aria-hidden="true"/><span class="st">  </span><span class="kw">brm</span>(t <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb144-3"><a href="ch-compbda.html#cb144-3" aria-hidden="true"/>      <span class="dt">data =</span> df_spacebar,</span>
<span id="cb144-4"><a href="ch-compbda.html#cb144-4" aria-hidden="true"/>      <span class="dt">family =</span> <span class="kw">gaussian</span>(),</span>
<span id="cb144-5"><a href="ch-compbda.html#cb144-5" aria-hidden="true"/>      <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>, <span class="dv">60000</span>),</span>
<span id="cb144-6"><a href="ch-compbda.html#cb144-6" aria-hidden="true"/>                      <span class="dt">class =</span> Intercept,</span>
<span id="cb144-7"><a href="ch-compbda.html#cb144-7" aria-hidden="true"/>                      <span class="dt">lb =</span> <span class="dv">0</span>,</span>
<span id="cb144-8"><a href="ch-compbda.html#cb144-8" aria-hidden="true"/>                      <span class="dt">ub =</span> <span class="dv">60000</span>),</span>
<span id="cb144-9"><a href="ch-compbda.html#cb144-9" aria-hidden="true"/>                <span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>, <span class="dv">2000</span>),</span>
<span id="cb144-10"><a href="ch-compbda.html#cb144-10" aria-hidden="true"/>                      <span class="dt">class =</span> sigma,</span>
<span id="cb144-11"><a href="ch-compbda.html#cb144-11" aria-hidden="true"/>                      <span class="dt">lb =</span> <span class="dv">0</span>,</span>
<span id="cb144-12"><a href="ch-compbda.html#cb144-12" aria-hidden="true"/>                      <span class="dt">ub =</span> <span class="dv">2000</span>)),</span>
<span id="cb144-13"><a href="ch-compbda.html#cb144-13" aria-hidden="true"/>      <span class="co">## uncomment for prior predictive:</span></span>
<span id="cb144-14"><a href="ch-compbda.html#cb144-14" aria-hidden="true"/>      <span class="co">## sample_prior = "only",</span></span>
<span id="cb144-15"><a href="ch-compbda.html#cb144-15" aria-hidden="true"/>      <span class="co">## uncomment when dealing with divergent transitions</span></span>
<span id="cb144-16"><a href="ch-compbda.html#cb144-16" aria-hidden="true"/>      <span class="co">## control = list(adapt_delta = .9)</span></span>
<span id="cb144-17"><a href="ch-compbda.html#cb144-17" aria-hidden="true"/>      <span class="co">## default values for chains, iterations and warmup:</span></span>
<span id="cb144-18"><a href="ch-compbda.html#cb144-18" aria-hidden="true"/>      <span class="dt">chains =</span> <span class="dv">4</span>,</span>
<span id="cb144-19"><a href="ch-compbda.html#cb144-19" aria-hidden="true"/>      <span class="dt">iter =</span> <span class="dv">2000</span>,</span>
<span id="cb144-20"><a href="ch-compbda.html#cb144-20" aria-hidden="true"/>      <span class="dt">warmup =</span> <span class="dv">1000</span>)</span></code></pre></div>
<ul>
<li>Extract samples from fitted model:</li>
</ul>
<div class="sourceCode" id="cb145"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb145-1"><a href="ch-compbda.html#cb145-1" aria-hidden="true"/><span class="kw">as_draws_df</span>(fit_press)</span></code></pre></div>
<ul>
<li>Basic plot of posteriors</li>
</ul>
<div class="sourceCode" id="cb146"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb146-1"><a href="ch-compbda.html#cb146-1" aria-hidden="true"/><span class="kw">plot</span>(fit_press)</span></code></pre></div>
<ul>
<li>Plot prior predictive/posterior predictive data</li>
</ul>
<div class="sourceCode" id="cb147"><pre class="sourceCode r fold-show"><code class="sourceCode r"><span id="cb147-1"><a href="ch-compbda.html#cb147-1" aria-hidden="true"/><span class="co">## Posterior predictive check:</span></span>
<span id="cb147-2"><a href="ch-compbda.html#cb147-2" aria-hidden="true"/><span class="kw">pp_check</span>(fit_press, <span class="dt">ndraws =</span> <span class="dv">100</span>, <span class="dt">type =</span> <span class="st">"dens_overlay"</span>)</span>
<span id="cb147-3"><a href="ch-compbda.html#cb147-3" aria-hidden="true"/><span class="co">## Plot posterior predictive distribution of statistical summaries:</span></span>
<span id="cb147-4"><a href="ch-compbda.html#cb147-4" aria-hidden="true"/><span class="kw">pp_check</span>(fit_press, <span class="dt">ndraws =</span> <span class="dv">100</span>, <span class="dt">type =</span> <span class="st">"stat"</span>, <span class="dt">stat =</span> <span class="st">"mean"</span>)</span>
<span id="cb147-5"><a href="ch-compbda.html#cb147-5" aria-hidden="true"/><span class="co">## Plot prior predictive distribution of statistical summaries:</span></span>
<span id="cb147-6"><a href="ch-compbda.html#cb147-6" aria-hidden="true"/><span class="kw">pp_check</span>(fit_press, <span class="dt">ndraws =</span> <span class="dv">100</span>, <span class="dt">type =</span> <span class="st">"stat"</span>, <span class="dt">stat =</span> <span class="st">"mean"</span>,</span>
<span id="cb147-7"><a href="ch-compbda.html#cb147-7" aria-hidden="true"/>         <span class="dt">prefix =</span> <span class="st">"ppd"</span>)</span></code></pre></div>
</div>
<div id="summary-2" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Summary<a href="ch-compbda.html#summary-2" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>This chapter showed how to fit and interpret a Bayesian model with a normal likelihood. We looked at the effect of priors by investigating prior predictive distributions and by carrying out a sensitivity analysis. We also looked at the fit of the posterior, by inspecting the posterior predictive distribution (which gives us some idea about the descriptive adequacy of the model). We also showed how to fit a Bayesian model with a log-normal likelihood, and how to compare the predictive accuracy of different models.</p>
</div>
<div id="sec-ch3furtherreading" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Further reading<a href="ch-compbda.html#sec-ch3furtherreading" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Sampling algorithms are discussed in detail in <span class="citation">Gamerman and Lopes (<a href="#ref-gamerman" role="doc-biblioref">2006</a>)</span>. Also helpful are the sections on sampling from the short open-source book by Bob Carpenter, <em>Probability and Statistics: a simulation-based introduction</em> (<a href="https://github.com/bob-carpenter/prob-stats" class="uri">https://github.com/bob-carpenter/prob-stats</a>), and the sections on sampling algorithms in <span class="citation">Lambert (<a href="#ref-lambert2018student" role="doc-biblioref">2018</a>)</span> and <span class="citation">Lynch (<a href="#ref-lynch2007introduction" role="doc-biblioref">2007</a>)</span>.
The evolution of probabilistic programming languages for Bayesian inference is discussed in <span class="citation">Štrumbelj et al. (<a href="#ref-vstrumbelj2024past" role="doc-biblioref">2024</a>)</span>.
Introductory linear modeling theory is covered in <span class="citation">Dobson and Barnett (<a href="#ref-dobson2011introduction" role="doc-biblioref">2011</a>)</span>; more advanced treatments are in <span class="citation">Montgomery, Peck, and Vining (<a href="#ref-monty" role="doc-biblioref">2012</a>)</span> and <span class="citation">Seber and Lee (<a href="#ref-seber" role="doc-biblioref">2003</a>)</span>. Generalized linear models are covered in detail in <span class="citation">McCullagh and Nelder (<a href="#ref-mccullagh2019generalized" role="doc-biblioref">2019</a>)</span>. The reader may also benefit from our own freely available online lecture notes on linear modeling: <a href="https://github.com/vasishth/LM" class="uri">https://github.com/vasishth/LM</a>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references hanging-indent">
<div id="ref-R-lme4">
<p>Bates, Douglas M., Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01">https://doi.org/10.18637/jss.v067.i01</a>.</p>
</div>
<div id="ref-blitzstein2014introduction">
<p>Blitzstein, Joseph K., and Jessica Hwang. 2014. <em>Introduction to Probability</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-R-brms_a">
<p>Bürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” <em>Journal of Statistical Software</em> 80 (1): 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>.</p>
</div>
<div id="ref-R-brms">
<p>Bürkner, Paul-Christian. 2024. <em>brms: Bayesian Regression Models Using “Stan”</em>. <a href="https://github.com/paul-buerkner/brms">https://github.com/paul-buerkner/brms</a>.</p>
</div>
<div id="ref-carpenter2017stan">
<p>Carpenter, Bob, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael J. Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. “Stan: A Probabilistic Programming Language.” <em>Journal of Statistical Software</em> 76 (1).</p>
</div>
<div id="ref-dobson2011introduction">
<p>Dobson, Annette J., and Adrian Barnett. 2011. <em>An Introduction to Generalized Linear Models</em>. CRC Press.</p>
</div>
<div id="ref-duaneHybridMonteCarlo1987">
<p>Duane, Simon, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. 1987. “Hybrid Monte Carlo.” <em>Physics Letters B</em> 195 (2): 216–22. <a href="https://doi.org/10.1016/0370-2693(87)91197-X">https://doi.org/10.1016/0370-2693(87)91197-X</a>.</p>
</div>
<div id="ref-gamerman">
<p>Gamerman, Dani, and Hedibert F. Lopes. 2006. <em>Markov chain Monte Carlo: Stochastic simulation for Bayesian inference</em>. CRC Press.</p>
</div>
<div id="ref-turing">
<p>Ge, Hong, Kai Xu, and Zoubin Ghahramani. 2018. “Turing: A Language for Flexible Probabilistic Inference.” In <em>Proceedings of Machine Learning Research</em>, edited by Amos Storkey and Fernando Perez-Cruz, 84:1682–90. Playa Blanca, Lanzarote, Canary Islands: PMLR. <a href="http://proceedings.mlr.press/v84/ge18b.html">http://proceedings.mlr.press/v84/ge18b.html</a>.</p>
</div>
<div id="ref-Gelman14">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. <em>Bayesian Data Analysis</em>. Third Edition. Boca Raton, FL: Chapman; Hall/CRC Press.</p>
</div>
<div id="ref-gelmanPriorCanOften2017">
<p>Gelman, Andrew, Daniel P. Simpson, and Michael J. Betancourt. 2017. “The Prior Can Often Only Be Understood in the Context of the Likelihood.” <em>Entropy</em> 19 (10): 555. <a href="https://doi.org/10.3390/e19100555">https://doi.org/10.3390/e19100555</a>.</p>
</div>
<div id="ref-rstanarm">
<p>Goodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2018. “Rstanarm: Bayesian Applied Regression Modeling via Stan.” <a href="http://mc-stan.org/">http://mc-stan.org/</a>.</p>
</div>
<div id="ref-hoffmanNoUTurnSamplerAdaptively2014">
<p>Hoffman, Matthew D., and Andrew Gelman. 2014. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>Journal of Machine Learning Research</em> 15 (1): 1593–1623. <a href="http://dl.acm.org/citation.cfm?id=2627435.2638586">http://dl.acm.org/citation.cfm?id=2627435.2638586</a>.</p>
</div>
<div id="ref-hubel2013computerized">
<p>Hubel, Kerry A., Bruce Reed, E. William Yund, Timothy J. Herron, and David L. Woods. 2013. “Computerized Measures of Finger Tapping: Effects of Hand Dominance, Age, and Sex.” <em>Perceptual and Motor Skills</em> 116 (3): 929–52. <a href="https://doi.org/https://doi.org/10.2466/25.29.PMS.116.3.929-952">https://doi.org/https://doi.org/10.2466/25.29.PMS.116.3.929-952</a>.</p>
</div>
<div id="ref-JASP2019">
<p>JASP Team. 2019. “JASP (Version 0.11.1)[Computer software].” <a href="https://jasp-stats.org/">https://jasp-stats.org/</a>.</p>
</div>
<div id="ref-jaynes2003probability">
<p>Jaynes, Edwin T. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge University Press.</p>
</div>
<div id="ref-jeffreys1939theory">
<p>Jeffreys, Harold. 1939. <em>Theory of Probability</em>. Oxford: Clarendon Press.</p>
</div>
<div id="ref-lambert2018student">
<p>Lambert, Ben. 2018. <em>A Student’s Guide to Bayesian Statistics</em>. London, UK: Sage.</p>
</div>
<div id="ref-lindgren2015bayesian">
<p>Lindgren, Finn, and Håvard Rue. 2015. “Bayesian Spatial Modelling with R-INLA.” <em>Journal of Statistical Software</em> 63 (1): 1–25. <a href="https://doi.org/%2010.18637/jss.v063.i19">https://doi.org/ 10.18637/jss.v063.i19</a>.</p>
</div>
<div id="ref-luce1986response">
<p>Luce, R. Duncan. 1991. <em>Response Times: Their Role in Inferring Elementary Mental Organization</em>. Oxford University Press.</p>
</div>
<div id="ref-lunn2000winbugs">
<p>Lunn, David J., Andrew Thomas, Nichola G. Best, and David J. Spiegelhalter. 2000. “WinBUGS-A Bayesian Modelling Framework: Concepts, Structure, and Extensibility.” <em>Statistics and Computing</em> 10 (4): 325–37.</p>
</div>
<div id="ref-lynch2007introduction">
<p>Lynch, Scott Michael. 2007. <em>Introduction to Applied Bayesian Statistics and Estimation for Social Scientists</em>. New York, NY: Springer.</p>
</div>
<div id="ref-mccullagh2019generalized">
<p>McCullagh, Peter, and J. A. Nelder. 2019. <em>Generalized Linear Models</em>. Second Edition. Boca Raton, Florida: Chapman; Hall/CRC.</p>
</div>
<div id="ref-monty">
<p>Montgomery, D. C., E. A. Peck, and G. G. Vining. 2012. <em>An Introduction to Linear Regression Analysis</em>. 5th ed. Hoboken, NJ: Wiley.</p>
</div>
<div id="ref-nealMCMCUsingHamiltonian2011">
<p>Neal, Radford M. 2011. “MCMC Using Hamiltonian Dynamics.” In <em>Handbook of Markov Chain Monte Carlo</em>, edited by Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Taylor &amp; Francis. <a href="https://doi.org/10.1201/b10905-10">https://doi.org/10.1201/b10905-10</a>.</p>
</div>
<div id="ref-kendall2004">
<p>O’Hagan, Anthony, and Jonathan J. Forster. 2004. “Kendall’s Advanced Theory of Statistics, Vol. 2B: Bayesian Inference.” Wiley.</p>
</div>
<div id="ref-plummer2016jags">
<p>Plummer, Martin. 2016. “JAGS Version 4.2.0 User Manual.”</p>
</div>
<div id="ref-plummer2022simulation">
<p>Plummer, Martin. 2022. “Simulation-Based Bayesian Analysis.” <em>Annual Review of Statistics and Its Application</em>.</p>
</div>
<div id="ref-rp">
<p>Roberts, Seth, and Harold Pashler. 2000. “How Persuasive Is a Good Fit? A Comment on Theory Testing.” <em>Psychological Review</em> 107 (2): 358–67. <a href="https://doi.org/https://doi.org/10.1037/0033-295X.107.2.358">https://doi.org/https://doi.org/10.1037/0033-295X.107.2.358</a>.</p>
</div>
<div id="ref-Salvatier2016">
<p>Salvatier, John, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. “Probabilistic Programming in Python Using PyMC3.” <em>PeerJ Computer Science</em> 2 (April): e55. <a href="https://doi.org/10.7717/peerj-cs.55">https://doi.org/10.7717/peerj-cs.55</a>.</p>
</div>
<div id="ref-seber">
<p>Seber, George A. F., and Allen J. Lee. 2003. <em>Linear Regression Analysis</em>. 2nd Edition. Hoboken, NJ: John Wiley; Sons.</p>
</div>
<div id="ref-shiffrinSurveyModelEvaluation2008">
<p>Shiffrin, Richard M, Michael D Lee, Woojae Kim, and Eric-Jan Wagenmakers. 2008. “A Survey of Model Evaluation Approaches with a Tutorial on Hierarchical Bayesian Methods.” <em>Cognitive Science: A Multidisciplinary Journal</em> 32 (8): 1248–84. <a href="https://doi.org/10.1080/03640210802414826">https://doi.org/10.1080/03640210802414826</a>.</p>
</div>
<div id="ref-vstrumbelj2024past">
<p>Štrumbelj, Erik, Alexandre Bouchard-Côté, Jukka Corander, Andrew Gelman, Håvard Rue, Lawrence Murray, Henri Pesonen, Martin Plummer, and Aki Vehtari. 2024. “Past, Present and Future of Software for Bayesian Inference.” <em>Statistical Science</em> 39 (1): 46–61.</p>
</div>
<div id="ref-VasishthetalPLoSOne2013">
<p>Vasishth, Shravan, Zhong Chen, Qiang Li, and Gueilan Guo. 2013. “Processing Chinese Relative Clauses: Evidence for the Subject-Relative Advantage.” <em>PLoS ONE</em> 8 (10): 1–14. <a href="https://doi.org/https://doi.org/10.1371/journal.pone.0077006">https://doi.org/https://doi.org/10.1371/journal.pone.0077006</a>.</p>
</div>
<div id="ref-VasishthEngelmann2022">
<p>Vasishth, Shravan, and Felix Engelmann. 2022. <em>Sentence Comprehension as a Cognitive Process: A Computational Approach</em>. Cambridge, UK: Cambridge University Press. <a href="https://books.google.de/books?id=6KZKzgEACAAJ">https://books.google.de/books?id=6KZKzgEACAAJ</a>.</p>
</div>
<div id="ref-vehtari2019ranknormalization">
<p>Vehtari, Aki, Andrew Gelman, Daniel P. Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved <span class="math inline">\(\widehat{R}\)</span> for Assessing Convergence of MCMC.” <em>Bayesian Analysis</em> 16 (2): 667–718. <a href="https://doi.org/10.1214/20-BA1221">https://doi.org/10.1214/20-BA1221</a>.</p>
</div>
</div>
<div class="footnotes">
<hr/>
<ol start="9">
<li id="fn9"><p>In the <code>brms</code> package <span class="citation">(Bürkner <a href="#ref-R-brms_a" role="doc-biblioref">2017</a>)</span>, samples from the posterior are referred to as draws.<a href="ch-compbda.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>The Python package PyMC3 and the Julia library Turing are recent exceptions since they are fully integrated into their respective languages.<a href="ch-compbda.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>We refer to the time it takes for a subject to respond or react to a stimulus as response time <span class="citation">(response and reaction times are often used interchangeably, cf. Luce <a href="#ref-luce1986response" role="doc-biblioref">1991</a>)</span>. In this case, however, there are no stimuli other than the cross on the screen, and the subject only taps the space bar as soon as they see the cross.<a href="ch-compbda.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>There is an additional problem here. Although the parameter for the intercept is assigned a uniform distribution bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(60000\)</span> ms, the sampler might start sampling from an initial value outside this range producing warnings. The sampler can start from an initial value that is outside the <span class="math inline">\(0\)</span>-<span class="math inline">\(60000\)</span> range because the initial value is chosen randomly (unless the user specifies an initial value explicitly). <a href="ch-compbda.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>We’ll see later how to generate prior predictive distributions of statistics such as mean, minimum, or maximum value in section <a href="ch-compbda.html#sec-lognormal">3.7.2</a> using <code>brms</code> and <code>pp_check()</code>.<a href="ch-compbda.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>Even though, in theory, one could use even wider priors, in practice, these are the widest priors that achieve convergence using <code>brms</code>.<a href="ch-compbda.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>Two events A and B are defined to be conditionally independent given some event E if <span class="math inline">\(P(A\cap B | E) = P(A|E) P(B|E)\)</span>. See chapter 2 of <span class="citation">Blitzstein and Hwang (<a href="#ref-blitzstein2014introduction" role="doc-biblioref">2014</a>)</span> for examples and more discussion of conditional independence.<a href="ch-compbda.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>More precisely, <span class="math inline">\(\log_e(\boldsymbol{y})\)</span> or <span class="math inline">\(\ln(\boldsymbol{y})\)</span>, but we’ll write it as just <span class="math inline">\(\log(\boldsymbol{y})\)</span>.<a href="ch-compbda.html#fnref16" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
                
</body>
</html>