- en: '6.3\. Modeling more complex dependencies 1: using conditional independence#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap06_prob/03_joint/roch-mmids-prob-joint.html](https://mmids-textbook.github.io/chap06_prob/03_joint/roch-mmids-prob-joint.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this section, we discuss the first of two standard techniques for constructing
    joint distributions from simpler building blocks: (1) imposing conditional independence
    relations and (2) marginalizing out an unobserved random variable. Combining them
    produces a large class of models known as probabilistic graphical models, which
    we do not discuss in generality. As before, we make our rigorous derivations in
    the finite support case, but these can be adapted to the continuous or hybrid
    cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1\. Review of conditioning[#](#review-of-conditioning "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first review the concept of conditioning, which generally plays a key role
    in probabilistic modeling and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional probability** We start with events. Throughout, we work on a
    fixed probability space \((\Omega, \mathcal{F}, \P)\), which we assume is discrete,
    i.e., the number of elements in \(\Omega\) is countable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Probability)** \(\idx{conditional probability}\xdi\)
    Let \(A\) and \(B\) be two events with \(\mathbb{P}[B] > 0\). The conditional
    probability of \(A\) given \(B\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A|B] = \frac{\P[A \cap B]}{\P[B]}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuitive interpretation goes something like this: knowing that event \(B\)
    has occurred, the updated probability of observing \(A\) is the probability of
    its restriction to \(B\) properly normalized to reflect that outcomes outside
    \(B\) have updated probability \(0\).'
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probabilities generally behave like “unconditional” probabilities.
    (See for instance Problems 6.8, 7.1, and 7.9.)
  prefs: []
  type: TYPE_NORMAL
- en: Independence can be characterized in terms of conditional probability. In words,
    \(A\) and \(B\) are independent if conditioning on one of them having taken place
    does not change the probability of the other occurring.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Let \(A\) and \(B\) be two events of positive probability. Then \(A\)
    and \(B\) are independent, which we will denote as \(A \indep B\), if and only
    if \(\P[A|B] = \P[A]\) and \(\P[B|A] = \P[B]\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* If \(A\) and \(B\) are independent, then \(\P[A \cap B] = \P[A] \P[B]\)
    which implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A|B] = \frac{\P[A \cap B]}{\P[B]} = \frac{\P[A] \P[B]}{\P[B]} = \P[A].
    \]
  prefs: []
  type: TYPE_NORMAL
- en: In the other direction,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A] = \P[A|B] = \frac{\P[A \cap B]}{\P[B]} \]
  prefs: []
  type: TYPE_NORMAL
- en: implies \(\P[A \cap B] = \P[A]\P[B]\) after rearranging. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability is often used in three fundamental ways, which we
    recall next. Proofs can be found in most probability textbooks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiplication Rule:** \(\idx{multiplication rule}\xdi\) For any collection
    of events \(A_1,\ldots,A_r\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \P\left[\cap_{i=1}^r A_i\right] = \prod_{i=1}^r \P\left[A_i \,\middle|\,
    \cap_{j=1}^{i-1} A_j \right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Law of Total Probability:** \(\idx{law of total probability}\xdi\) For any
    event \(B\) and any [partition](https://en.wikipedia.org/wiki/Partition_of_a_set#Definition_and_Notation)\(\idx{partition}\xdi\)
    \(A_1,\ldots,A_r\) of \(\Omega\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \P[B] = \sum_{i=1}^r \P[B|A_i] \P[A_i]. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes’ Rule:** \(\idx{Bayes'' yule}\xdi\) For any events \(A\) and \(B\)
    with positive probability,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \P[A|B] = \frac{\P[B|A]\P[A]}{\P[B]}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It is implicit that all formulas above hold provided all conditional probabilities
    are well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditioning on a random variable** Conditional probabilities extend naturally
    to random variables. If \(X\) is a discrete random variable, we let \(p_X\) be
    its probability mass function and \(\S_X\) be its support, that is, the set of
    values where it has positive probability. Then we can for instance condition on
    the event \(\{X = x\}\) for any \(x \in \S_X\).'
  prefs: []
  type: TYPE_NORMAL
- en: We define next the conditional probability mass function.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Probability Mass Function)** Let \(X\) and \(Y\)
    be discrete random variables with joint probability mass function \(p_{X, Y}\)
    and marginals \(p_X\) and \(p_Y\). The conditional probability mass function\(\idx{conditional
    probability mass function}\xdi\) of \(X\) given \(Y\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{X|Y}(x|y) := P[X=x|Y=y] = \frac{p_{X,Y}(x,y)}{p_Y(y)} \]
  prefs: []
  type: TYPE_NORMAL
- en: which is defined for all \(x \in \S_X\) and \(y \in \S_Y\). \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: The conditional expectation can then be defined in a natural way as the expectation
    over the conditional probability mass function.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Expectation)** \(\idx{conditional expectation}\xdi\)
    Let \(X\) and \(Y\) be discrete random variables where \(X\) takes real values
    and has a finite mean. The conditional expectation of \(X\) given \(Y = y\) is
    given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[X|Y=y] = \sum_{x \in \S_X} x\, p_{X|Y}(x|y). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: More generally, for a function \(f\) over the range of \(X\), we can define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[f(X)|Y=y] = \sum_{x \in \S_X} f(x)\, p_{X|Y}(x|y). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We mention one useful formula: the *Law of Total Expectation*\(\idx{law of
    total expectation}\xdi\), the expectation version of the *Law of Total Probability*.
    It reads'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[f(X)] = \sum_{y \in \S_Y} \E[f(X)|Y=y] \,p_Y(y). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional expectation as least-squares estimator** Thinking of \(\E[X|Y=y]\)
    as a function of \(y\) leads to a fundamental characterization of the conditional
    expectation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** Let \(X\) and \(Y\) be discrete random variables where \(X\) takes
    real values and has a finite variance. Then the conditional expectation \(h(y)
    = \E[X|Y=y]\) minimizes the least squares criterion'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{h} \E\left[(X - h(Y))^2\right] \]
  prefs: []
  type: TYPE_NORMAL
- en: where the minimum is over all real-valued functions of \(y\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Think of \(h(y)\) as a vector \(\mathbf{h} = (h_y)_{y \in \S_Y}\),
    indexed by \(\S_Y\) (which is countable by assumption), with \(h_y = h(y) \in
    \mathbb{R}\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathcal{L}(\mathbf{h}) &=\E\left[(X - h(Y))^2\right]\\ &=
    \sum_{x\in \S_X} \sum_{y \in \S_Y} (x - h_y)^2 p_{X,Y}(x,y)\\ &= \sum_{y \in \S_Y}
    \left[\sum_{x\in \S_X} (x - h_y)^2 p_{X,Y}(x,y)\right]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the sum in the square brackets (which we denote by \(q_y\) and think
    of as a function of \(h_y\)) gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} q_y(h_y) &:= \sum_{x\in \S_X} (x - h_y)^2 p_{X,Y}(x,y)\\ &=
    \sum_{x\in \S_X} [x^2 - 2 x h_y + h_y^2] \,p_{X,Y}(x,y)\\ &= \left\{\sum_{x\in
    \S_X} x^2 p_{X,Y}(x,y)\right\} + \left\{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)\right\}
    h_y + \left\{p_Y(y)\right\} h_y^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Miminizing a Quadratic Function Lemma*, the unique global minimum of
    \(q_y(h_y)\) - provided \(p_Y(y) > 0\) - is attained at
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_y = - \frac{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)}{2 p_Y(y)}. \]
  prefs: []
  type: TYPE_NORMAL
- en: After rearranging, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_y = \sum_{x\in \S_X} x \frac{p_{X,Y}(x,y)}{p_Y(y)} = \sum_{x\in \S_X} x
    p_{X|Y}(x|y) = \E[X|Y=y] \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional independence** Next, we discuss conditional independence. We
    begin with the formal definition.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Independence)** \(\idx{conditional independence}\xdi\)
    Let \(A, B, C\) be events such that \(\P[C] > 0\). Then \(A\) and \(B\) are conditionally
    independent given \(C\), denoted \(A \indep B | C\), if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A \cap B| C] = \P[A|C] \,\P[B|C]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'In words, quoting [Wikipedia](https://en.wikipedia.org/wiki/Conditional_independence):'
  prefs: []
  type: TYPE_NORMAL
- en: \(A\) and \(B\) are conditionally independent given \(C\) if and only if, given
    knowledge that \(C\) occurs, knowledge of whether \(A\) occurs provides no information
    on the likelihood of \(B\) occurring, and knowledge of whether \(B\) occurs provides
    no information on the likelihood of \(A\) occurring.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In general, conditionally independent events are not (unconditionally) independent.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Imagine I have two six-sided dice. Die 1 has faces \(\{1,3,5,7,9,11\}\)
    and die 2 has faces \(\{2, 4, 6, 8, 10, 12\}\). Suppose I perform the following
    experiment: I pick one of the two dice uniformly at random, and then I roll that
    die twice. Let \(X_1\) and \(X_2\) be the outcomes of the rolls. Consider the
    events \(A = \{X_1 = 1\}\), \(B = \{X_2 = 2\}\), and \(C = \{\text{die 1 is picked}\}\).
    The events \(A\) and \(B\) are clearly dependent: if \(A\) occurs, then I know
    that die 1 was picked, and hence \(B\) cannot occur. Knowledge of one event provides
    information about the likelihood of the other event occurring. Formally, by the
    law of total probability,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A] = \P[A|C]\P[C] + \P[A|C^c]\P[C^c] = \frac{1}{6}\frac{1}{2} + 0 \frac{1}{2}
    = \frac{1}{12}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly \(\P[B] = \frac{1}{12}\). Yet \(\P[A \cap B] = 0 \neq \frac{1}{12}
    \frac{1}{12}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we claim that \(A\) and \(B\) are conditionally independent
    given \(C\). Again this is intuitively clear: once I pick a die, the two rolls
    are independent. For a given die choice, knowledge of one roll provides no information
    about the likelihood of the other roll. Note that the phrase “for a given die
    choice” is critical in the last statement. Formally, by our experiment, we have
    \(\P[A|C] = 1/6\), \(\P[B|C] = 0\) and \(\P[A \cap B|C] = 0\). So indeed'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A \cap B| C] = \P[A|C] \,\P[B|C] \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Conditional independence is naturally extended to random vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Independence of Random Vectors)** Let \(\bX,
    \bY, \bW\) be discrete random vectors. Then \(\bX\) and \(\bY\) are said to be
    conditionally independent given \(\bW\), denoted \(\bX \indep \bY | \bW\), if
    for all \(\bx \in \S_\bX\), \(\by \in \S_\bY\) and \(\bw \in \S_\bW\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[\bX = \bx, \bY = \by|\bW = \bw] = \P[\bX = \bx |\bW = \bw] \,\P[\bY =
    \by|\bW = \bw]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: An important consequence is that we can drop the conditioning by the independent
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Role of Independence)** \(\idx{role of independence lemma}\xdi\)
    Let \(\bX, \bY, \bW\) be discrete random vectors such that \(\bX \indep \bY |
    \bW\). For all \(\bx \in \S_\bX\), \(\by \in \S_\bY\) and \(\bw \in \S_\bW\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[\bX = \bx | \bY=\by, \bW=\bw] = \P[\bX = \bx | \bW = \bw]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* In a previous exercise, we showed that \(A \indep B | C\) implies
    \(\P[A | B\cap C] = \P[A | C]\). That implies the claim. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The concept of conditional independence is closely related
    to the concept of d-separation in probabilistic graphical models. Ask your favorite
    AI chatbot to explain d-separation. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2\. The basic configurations[#](#the-basic-configurations "Link to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A powerful approach for constructing complex probability distributions is the
    use of conditional independence. The case of three random variables exemplifies
    key probabilistic relationships. By the product rule, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x, Y=y]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This is conveniently represented through a digraph where the vertices are the
    variables. Recall that an arrow \((i,j)\), from \(i\) to \(j\), indicates that
    \(i\) is a parent of \(j\) and that \(j\) is a child of \(i\). Let \(\pa(i)\)
    be the set of parents of \(i\). The digraph \(G = (V, E)\) below encodes the following
    sampling scheme, referred as ancestral sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: First we pick \(X\) according to its marginal \(\P[X=x]\). Note that \(X\) has
    no parent in \(G\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second we pick \(Y\) according to the conditional probability distribution (CPD)
    \(\P[Y=y|X=x]\). Note that \(X\) is the only parent of \(Y\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally we pick \(Z\) according to the CPD \(\P[Z=z|X=x, Y=y]\). Note that the
    parents of \(Z\) are \(X\) and \(Y\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The full case](../Images/b73cb01c1b1f5d32087884467af21283.png)'
  prefs: []
  type: TYPE_IMG
- en: The graph above is acyclic, that is, it has no directed cycle. The variables
    \(X, Y, Z\) are in [topological order](https://en.wikipedia.org/wiki/Topological_sorting)\(\idx{topological
    order}\xdi\), that is, all edges \((i,j)\) are such that \(i\) comes before \(j\)
    in that order.
  prefs: []
  type: TYPE_NORMAL
- en: The same joint distribution can be represented by a different digraph if the
    product rule is used in a different order. For instance,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[Z=z] \,\P[Y=y|Z=z] \,\P[X=x | Z=z, Y=y] \]
  prefs: []
  type: TYPE_NORMAL
- en: is represented by the following digraph. A topological order this time is \(Z,
    Y, X\).
  prefs: []
  type: TYPE_NORMAL
- en: '![Another full case](../Images/ed39c216e1efdc59d602be7c9e2c744a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The fork** \(\idx{fork}\xdi\) Removing edges in the first graph above encodes
    conditional independence relations. For instance, removing the edge from \(Y\)
    to \(Z\) gives the following graph, known as a fork. We denote this configuration
    as \(Y \leftarrow X \rightarrow Z\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The fork](../Images/77cb78740ee4952c17900d2626c7ad2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The joint distribution simplifies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x]. \]
  prefs: []
  type: TYPE_NORMAL
- en: So, in this case, what has changed is that the CPD of \(Z\) does not depend
    on the value of \(Y\). From the *Role of Independence* lemma, this corresponds
    to assuming the conditional independence \(Z \indep Y|X\). Indeed, we can check
    that claim directly from the joint distribution
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[Y= y, Z=z|X=x] &= \frac{\P[X=x, Y= y, Z=z]}{\P[X=x]}\\ &=
    \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x]}{\P[X=x]}\\ &= \P[Y=y|X=x] \,\P[Z=z
    | X=x] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: '**The chain** \(\idx{chain}\xdi\) Removing the edge from \(X\) to \(Z\) gives
    the following graph, known as a chain (or pipe). We denote this configuration
    as \(X \rightarrow Y \rightarrow Z\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The chain](../Images/5bfd7c9b4911ca0e5931761d25c8457e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The joint distribution simplifies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]. \]
  prefs: []
  type: TYPE_NORMAL
- en: In this case, what has changed is that the CPD of \(Z\) does not depend on the
    value of \(X\). Compare that to the fork. The corresponding conditional independence
    relation is \(Z \indep X|Y\). Indeed, we can check that claim directly
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X= x, Z=z|Y=y] &= \frac{\P[X=x, Y= y, Z=z]}{\P[Y=y]}\\ &=
    \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to use *Bayes’ Rule* to get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]}\\
    &= \frac{\P[Y=y|X=x]\,\P[X=x]}{\P[Y=y]} \P[Z=z | Y=y]\\ &= \P[X=x|Y=y] \,\P[Z=z
    | Y=y] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: For any \(x, y, z\) where the joint probability is positive, we can re-write
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X=x, Y=y, Z=z]\\ &= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]\\
    &= \P[Y=y] \,\P[X=x|Y=y] \,\P[Z=z | Y=y], \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y] = \P[X=x] \,\P[Y=y|X=x] = \P[Y=y] \,\P[X=x|Y=y] \]
  prefs: []
  type: TYPE_NORMAL
- en: by definition of the conditional probability. In other words, we have shown
    that the chain \(X \rightarrow Y \rightarrow Z\) is in fact equivalent to the
    fork \(X \leftarrow Y \rightarrow Z\). In particular, they both correspond to
    assuming the conditional independence relation \(Z \indep X|Y\), although they
    capture a different way to sample the joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**The collider** \(\idx{collider}\xdi\) Removing the edge from \(X\) to \(Y\)
    gives the following graph, known as a collider. We denote this configuration as
    \(X \rightarrow Z \leftarrow Y\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The collider](../Images/a4600775ea580d4809bd8b11895a447d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The joint distribution simplifies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y]. \]
  prefs: []
  type: TYPE_NORMAL
- en: In this case, what has changed is that the CPD of \(Y\) does not depend on the
    value of \(X\). Compare that to the fork and the chain. This time we have \(X
    \indep Y\). Indeed, we can check that claim directly
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X= x, Y=y] &= \sum_{z \in \S_z} \P[X=x, Y=y, Z=z]\\ &= \sum_{z
    \in \S_z} \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y]\\ &= \P[X=x] \,\P[Y=y] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. In particular, the collider cannot be reframed as a chain or fork
    as its underlying assumption is stronger.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps counter-intuitively, conditioning on \(Z\) makes \(X\) and \(Y\) dependent
    in general. This is known as explaining away or Berkson’s Paradox.
  prefs: []
  type: TYPE_NORMAL
- en: '6.3.3\. Example: Naive Bayes[#](#example-naive-bayes "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model-based justification we gave for logistic regression in the subsection
    on generalized linear models used a so-called [discriminative approach](https://en.wikipedia.org/wiki/Discriminative_model)\(\idx{discriminative
    model}\xdi\), where the conditional distribution of the target \(y\) given the
    features \(\mathbf{x}\) is specified – but not the full distribution of the data
    \((\mathbf{x}, y)\). Here we give an example of the [generative approach](https://en.wikipedia.org/wiki/Generative_model)\(\idx{generative
    model}\xdi\), which models the full distribution. For a discussion of the benefits
    and drawbacks of each approach, see for example [here](https://en.wikipedia.org/wiki/Discriminative_model#Contrast_with_generative_model).
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes\(\idx{Naive Bayes}\xdi\) model is a simple discrete model for
    supervised learning. It is useful for document classification for instance, and
    we will use that terminology here to be concrete. We assume that a document has
    a single topic \(C\) from a list \(\mathcal{C} = \{1, \ldots, K\}\) with probability
    distribution \(\pi_k = \P[C = k]\). There is a vocabulary of size \(M\) and we
    record the presence or absence of a word \(m\) in the document with a Bernoulli
    variable \(X_m \in \{0,1\}\), where \(p_{k,m} = \P[X_m = 1|C = k]\). We denote
    by \(\bX = (X_1, \ldots, X_M)\) the corresponding vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conditional independence assumption comes next: we assume that, given a
    topic \(C\), the word occurrences are independent. That is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[\bX = \bx|C=k] &= \prod_{m=1}^M \P[X_m = x_m|C = k]\\ &=
    \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the joint distribution is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[C = k, \bX = \bx] &= \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \pi_k
    \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, this is similar to a fork with \(C\) at its center and \(M\) prongs
    for the \(X_m\)s. This is represented using the so-called plate notation. The
    box with the \(M\) in the corner below indicates that \(X_m\) is repeated \(M\)
    times, all copies being conditionally independent given \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: '![Naives Bayes](../Images/1e486313a3d40a4fea7001344a53121b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Model fitting** Before using the model for prediction, one must first fit
    the model from training data \(\{\bx_i, c_i\}_{i=1}^n\). In this case, it means
    estimating the unknown parameters \(\bpi\) and \(\{\bp_k\}_{k=1}^K\), where \(\bp_k
    = (p_{k,1},\ldots, p_{k,M})\). For each \(k, m\) let'
  prefs: []
  type: TYPE_NORMAL
- en: \[ N_{k,m} = \sum_{i=1}^n \mathbf{1}_{\{c_i = k\}} x_{i,m}, \quad N_{k} = \sum_{i=1}^n
    \mathbf{1}_{\{c_i = k\}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We use maximum likelihood estimation which, recall, entails finding the parameters
    that maximize the probability of observing the data
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i, c_i\}) = \prod_{i=1}^n \pi_{c_i} \prod_{m=1}^M
    p_{c_i, m}^{x_{i,m}} (1-p_{c_i, m})^{1-x_{i,m}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, as usual, we assume that the samples are independent and identically distributed.
    We take a logarithm to turn the products into sums and consider the negative log-likelihood
    (NLL)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} & L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\})\\ &\quad = - \sum_{i=1}^n
    \log \pi_{c_i} - \sum_{i=1}^n \sum_{m=1}^M [x_{i,m} \log p_{c_{i}, m} + (1-x_{i,m})
    \log (1-p_{c_i, m})]\\ &\quad = - \sum_{k=1}^K N_k \log \pi_k - \sum_{k=1}^K \sum_{m=1}^M
    [N_{k,m} \log p_{k,m} + (N_k-N_{k,m}) \log (1-p_{k,m})]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The NLL can be broken up naturally into several terms that depend on different
    sets of parameters – and therefore can be optimized separately. First, there is
    a term that depends only on the \(\pi_k\)’s
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_0(\bpi; \{\bx_i, c_i\}) = - \sum_{k=1}^K N_k \log \pi_k. \]
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the sum can be further split into \(KM\) terms, each depending only
    on \(p_{km}\) for a fixed \(k\) and m
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{k,m}(p_{k,m}; \{\bx_i, c_i\}) = - N_{k,m} \log p_{k,m} - (N_k-N_{k,m})
    \log (1-p_{k,m}). \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\}) = J_0(\bpi; \{\bx_i, c_i\}) + \sum_{k=1}^K
    \sum_{m=1}^M J_{k,m}(p_{k,m}; \{\bx_i, c_i\}). \]
  prefs: []
  type: TYPE_NORMAL
- en: We minimize these terms separately. We assume that \(N_k > 0\) for all \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a special case of maximum likelihood estimation, which we previously
    worked out in an example, where we consider the space of all probability distributions
    over a finite set. The maximum likelihood estimator in that case is given by the
    empirical frequencies. Notice that minimizing \(J_0(\bpi; \{\bx_i, c_i\})\) is
    precisely of this form: we observe \(N_k\) samples from class \(k\) and we seek
    the maximum likelihood estimator of, \(\pi_k\), the probability of observing \(k\).
    Hence the solution is simply'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_k = \frac{N_k}{N}, \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(k\). Similarly, for each \(k\), \(m\), \(J_{k,m}\) is of that form
    as well. Here the states correspond to word \(m\) being present or absent in a
    document of class \(k\), and we observe \(N_{k,m}\) documents of type \(k\) where
    the word \(m\) is present. So the solution is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{p}_{k,m} = \frac{N_{k,m}}{N_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(k, m\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction** To predict the class of a new document, it is natural to maximize
    over \(k\) the probability that \(\{C=k\}\) given \(\{\bX = \bx\}\). By Bayes’
    rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[C=k | \bX = \bx] &= \frac{\P[C = k, \bX = \bx]}{\P[\bX =
    \bx]}\\ &= \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}} {\sum_{k'=1}^K
    \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_m} (1-p_{k',m})^{1-x_m}}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: As the denominator does not in fact depend on \(k\), maximizing \(\P[C=k | \bX
    = \bx]\) boils down to maximizing the numerator \(\pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}\), which is straighforward to compute. As we did previously,
    we take a negative logarithm – which has some numerical advantages – and we refer
    to it as the *score*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &- \log\left(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\right)\\
    &\qquad = -\log\pi_k - \sum_{m=1}^M [x_m \log p_{k,m} + (1-x_m) \log (1-p_{k,m})].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, taking a negative logarithm turns out to be a good idea here
    because computing a product of probabilities can produce very small numbers that,
    when they fall beneath machine precision, are approximated by zero. This is called
    [underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow)\(\idx{underflow}\xdi\).
    By taking a negative logarithm, these probabilities are transformed into positive
    numbers of reasonable magnitude and the product becomes of sum of these. Moreover,
    because this transformation is monotone, we can use the transformed values directly
    to compute the optimal score, which is our ultimate goal in the prediction step.
    Since the parameters are unknown, we use \(\hat{\pi}_k\) and \(\hat{p}_{k,m}\)
    in place of \(\pi_k\) and \(p_{k,m}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot for more information on the issue
    of underflow, and its cousin overflow\(\idx{overflow}\xdi\), in particular in
    the context of multiypling probabilities. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: While maximum likehood estimation has [desirable theoretical properties](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties),
    it does suffer from [overfitting](https://towardsdatascience.com/parameter-inference-maximum-aposteriori-estimate-49f3cd98267a).
    If for instance a particular word \(m\) does not occur in any training document,
    then the probability of observing a new document that happens to contain that
    word is estimated to be \(0\) for any class (i.e., \(\hat{p}_{k,m} = 0\) for all
    \(k\) so that \(\hat \pi_k \prod_{m=1}^M \hat{p}_{k,m}^{x_m} (1-\hat{p}_{k,m})^{1-x_m}
    = 0\) for all \(k\) ) and the maximization problem above is not well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to deal with this is [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)\(\idx{Laplace
    smoothing}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bar{\pi}_k = \frac{N_k + \alpha}{N + K \alpha}, \quad \bar{p}_{k,m} = \frac{N_{k,m}
    + \beta}{N_k + 2 \beta} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\alpha, \beta > 0\), which can be justified using a Bayesian or regularization
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: We implement the Naive Bayes model with Laplace smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: We encode the data into a table, where the rows are the classes and the columns
    are the features. The entries are the corresponding \(N_{k,m}\)s. In addition
    we provide the vector \((N_k)_k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using `N_k[:, np.newaxis]` reshapes the one-dimensional array `N_k` into a two-dimensional
    column vector. For example, if `N_k` has a shape of \((K,)\), then `N_k[:, np.newaxis]`
    changes its shape to \((K, 1)\). This allows the division in the expression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: to work correctly with [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html),
    ensuring that each element in a row of `N_km` is divided by the corresponding
    value in `N_k`.
  prefs: []
  type: TYPE_NORMAL
- en: The next function computes the negative logarithm of \(\pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}\), that is, the score of \(k\), and outputs a \(k\) achieving
    the minimum score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We use a simple example from [Stack Overflow](https://stackoverflow.com/questions/10059594/):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** Let’s say we have data on 1000 pieces of fruit. They happen to
    be Banana, Orange or some Other Fruit. We know 3 characteristics about each fruit:
    whether it is long, whether it is sweet, and if its color is yellow[, as displayed
    in the table below].'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Fruit | Long | Sweet | Yellow | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Banana | 400 | 350 | 450 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| Orange | 0 | 150 | 300 | 300 |'
  prefs: []
  type: TYPE_TB
- en: '| Other | 100 | 150 | 50 | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 500 | 650 | 800 | 1000 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We run `nb_fit_table` on our simple dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Continuing on with our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we are given the properties of an unknown fruit, and asked to
    classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana?
    Is it an Orange? Or Is it some Other Fruit?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We run `nb_predict` on our dataset with the additional fruit from the quote
    above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Laplace smoothing is a special case of a more general technique
    known as Bayesian parameter estimation. Ask your favorite AI chatbot to explain
    Bayesian parameter estimation and how it relates to maximum likelihood estimation
    and Laplace smoothing. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following statements is **not** true about conditional probabilities?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[A|B] = \frac{\mathbb{P}[A \cap B]}{\mathbb{P}[B]}\) for events
    \(A\) and \(B\) with \(\mathbb{P}[B] > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: b) If \(A\) and \(B\) are independent, then \(\mathbb{P}[A|B] = \mathbb{P}[A]\).
  prefs: []
  type: TYPE_NORMAL
- en: c) Conditional probabilities can be used to express the multiplication rule
    and the law of total probability.
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[A|B] = \mathbb{P}[B|A]\) for any events \(A\) and \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is the correct mathematical expression for the
    conditional independence of events \(A\) and \(B\) given event \(C\), denoted
    as \(A \perp\!\!\!\perp B \mid C\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] + \mathbb{P}[B \mid
    C]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[A \cup B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[A \mid B \cap C] = \mathbb{P}[A \mid C]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** In the fork configuration \(Y \leftarrow X \rightarrow Z\), which of
    the following conditional independence relations always holds?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(X \perp\!\!\!\perp Y \mid Z\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(Y \perp\!\!\!\perp Z \mid X\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(X \perp\!\!\!\perp Z \mid Y\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(Y \perp\!\!\!\perp Z\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In the collider configuration \(X \rightarrow Z \leftarrow Y\), which
    of the following conditional independence relations always holds?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(X \perp\!\!\!\perp Y \mid Z\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(Y \perp\!\!\!\perp Z \mid X\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(X \perp\!\!\!\perp Z \mid Y\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(X \perp\!\!\!\perp Y\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following best describes the graphical representation of
    the Naive Bayes model for document classification?'
  prefs: []
  type: TYPE_NORMAL
- en: a) A chain with the topic variable at the center and word variables as the links.
  prefs: []
  type: TYPE_NORMAL
- en: b) A collider with the topic variable at the center and word variables as the
    parents.
  prefs: []
  type: TYPE_NORMAL
- en: c) A fork with the topic variable at the center and word variables as the prongs.
  prefs: []
  type: TYPE_NORMAL
- en: d) A complete graph with edges between all pairs of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: d. Justification: In general, \(\mathbb{P}[A|B] \neq \mathbb{P}[B|A]\).
    Bayes’ rule provides the correct relationship between these two conditional probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: The text states, “Then \(A\) and \(B\) are
    conditionally independent given \(C\), denoted \(A \perp\!\!\!\perp B \mid C\),
    if \(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: The text states, “Removing the edge from \(Y\)
    to \(Z\) gives the following graph, known as a fork. We denote this configuration
    as \(Y \leftarrow X \rightarrow Z\). […] The corresponding conditional independence
    relation is \(Z \perp\!\!\!\perp Y \mid X\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text states, “Removing the edge from \(X\)
    to \(Y\) gives the following graph, known as a collider. We denote this configuration
    as \(X \rightarrow Z \leftarrow Y\). […] This time we have \(X \perp\!\!\!\perp
    Y\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: The text states, “Graphically, this is similar
    to a fork with \(C\) at its center and \(M\) prongs for the \(X_m\)s.”'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1\. Review of conditioning[#](#review-of-conditioning "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first review the concept of conditioning, which generally plays a key role
    in probabilistic modeling and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional probability** We start with events. Throughout, we work on a
    fixed probability space \((\Omega, \mathcal{F}, \P)\), which we assume is discrete,
    i.e., the number of elements in \(\Omega\) is countable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Probability)** \(\idx{conditional probability}\xdi\)
    Let \(A\) and \(B\) be two events with \(\mathbb{P}[B] > 0\). The conditional
    probability of \(A\) given \(B\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A|B] = \frac{\P[A \cap B]}{\P[B]}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuitive interpretation goes something like this: knowing that event \(B\)
    has occurred, the updated probability of observing \(A\) is the probability of
    its restriction to \(B\) properly normalized to reflect that outcomes outside
    \(B\) have updated probability \(0\).'
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probabilities generally behave like “unconditional” probabilities.
    (See for instance Problems 6.8, 7.1, and 7.9.)
  prefs: []
  type: TYPE_NORMAL
- en: Independence can be characterized in terms of conditional probability. In words,
    \(A\) and \(B\) are independent if conditioning on one of them having taken place
    does not change the probability of the other occurring.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Let \(A\) and \(B\) be two events of positive probability. Then \(A\)
    and \(B\) are independent, which we will denote as \(A \indep B\), if and only
    if \(\P[A|B] = \P[A]\) and \(\P[B|A] = \P[B]\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* If \(A\) and \(B\) are independent, then \(\P[A \cap B] = \P[A] \P[B]\)
    which implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A|B] = \frac{\P[A \cap B]}{\P[B]} = \frac{\P[A] \P[B]}{\P[B]} = \P[A].
    \]
  prefs: []
  type: TYPE_NORMAL
- en: In the other direction,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A] = \P[A|B] = \frac{\P[A \cap B]}{\P[B]} \]
  prefs: []
  type: TYPE_NORMAL
- en: implies \(\P[A \cap B] = \P[A]\P[B]\) after rearranging. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability is often used in three fundamental ways, which we
    recall next. Proofs can be found in most probability textbooks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiplication Rule:** \(\idx{multiplication rule}\xdi\) For any collection
    of events \(A_1,\ldots,A_r\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \P\left[\cap_{i=1}^r A_i\right] = \prod_{i=1}^r \P\left[A_i \,\middle|\,
    \cap_{j=1}^{i-1} A_j \right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Law of Total Probability:** \(\idx{law of total probability}\xdi\) For any
    event \(B\) and any [partition](https://en.wikipedia.org/wiki/Partition_of_a_set#Definition_and_Notation)\(\idx{partition}\xdi\)
    \(A_1,\ldots,A_r\) of \(\Omega\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \P[B] = \sum_{i=1}^r \P[B|A_i] \P[A_i]. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes’ Rule:** \(\idx{Bayes'' yule}\xdi\) For any events \(A\) and \(B\)
    with positive probability,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \P[A|B] = \frac{\P[B|A]\P[A]}{\P[B]}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It is implicit that all formulas above hold provided all conditional probabilities
    are well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditioning on a random variable** Conditional probabilities extend naturally
    to random variables. If \(X\) is a discrete random variable, we let \(p_X\) be
    its probability mass function and \(\S_X\) be its support, that is, the set of
    values where it has positive probability. Then we can for instance condition on
    the event \(\{X = x\}\) for any \(x \in \S_X\).'
  prefs: []
  type: TYPE_NORMAL
- en: We define next the conditional probability mass function.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Probability Mass Function)** Let \(X\) and \(Y\)
    be discrete random variables with joint probability mass function \(p_{X, Y}\)
    and marginals \(p_X\) and \(p_Y\). The conditional probability mass function\(\idx{conditional
    probability mass function}\xdi\) of \(X\) given \(Y\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{X|Y}(x|y) := P[X=x|Y=y] = \frac{p_{X,Y}(x,y)}{p_Y(y)} \]
  prefs: []
  type: TYPE_NORMAL
- en: which is defined for all \(x \in \S_X\) and \(y \in \S_Y\). \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: The conditional expectation can then be defined in a natural way as the expectation
    over the conditional probability mass function.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Expectation)** \(\idx{conditional expectation}\xdi\)
    Let \(X\) and \(Y\) be discrete random variables where \(X\) takes real values
    and has a finite mean. The conditional expectation of \(X\) given \(Y = y\) is
    given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[X|Y=y] = \sum_{x \in \S_X} x\, p_{X|Y}(x|y). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: More generally, for a function \(f\) over the range of \(X\), we can define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[f(X)|Y=y] = \sum_{x \in \S_X} f(x)\, p_{X|Y}(x|y). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We mention one useful formula: the *Law of Total Expectation*\(\idx{law of
    total expectation}\xdi\), the expectation version of the *Law of Total Probability*.
    It reads'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[f(X)] = \sum_{y \in \S_Y} \E[f(X)|Y=y] \,p_Y(y). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional expectation as least-squares estimator** Thinking of \(\E[X|Y=y]\)
    as a function of \(y\) leads to a fundamental characterization of the conditional
    expectation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** Let \(X\) and \(Y\) be discrete random variables where \(X\) takes
    real values and has a finite variance. Then the conditional expectation \(h(y)
    = \E[X|Y=y]\) minimizes the least squares criterion'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{h} \E\left[(X - h(Y))^2\right] \]
  prefs: []
  type: TYPE_NORMAL
- en: where the minimum is over all real-valued functions of \(y\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Think of \(h(y)\) as a vector \(\mathbf{h} = (h_y)_{y \in \S_Y}\),
    indexed by \(\S_Y\) (which is countable by assumption), with \(h_y = h(y) \in
    \mathbb{R}\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathcal{L}(\mathbf{h}) &=\E\left[(X - h(Y))^2\right]\\ &=
    \sum_{x\in \S_X} \sum_{y \in \S_Y} (x - h_y)^2 p_{X,Y}(x,y)\\ &= \sum_{y \in \S_Y}
    \left[\sum_{x\in \S_X} (x - h_y)^2 p_{X,Y}(x,y)\right]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the sum in the square brackets (which we denote by \(q_y\) and think
    of as a function of \(h_y\)) gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} q_y(h_y) &:= \sum_{x\in \S_X} (x - h_y)^2 p_{X,Y}(x,y)\\ &=
    \sum_{x\in \S_X} [x^2 - 2 x h_y + h_y^2] \,p_{X,Y}(x,y)\\ &= \left\{\sum_{x\in
    \S_X} x^2 p_{X,Y}(x,y)\right\} + \left\{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)\right\}
    h_y + \left\{p_Y(y)\right\} h_y^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Miminizing a Quadratic Function Lemma*, the unique global minimum of
    \(q_y(h_y)\) - provided \(p_Y(y) > 0\) - is attained at
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_y = - \frac{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)}{2 p_Y(y)}. \]
  prefs: []
  type: TYPE_NORMAL
- en: After rearranging, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_y = \sum_{x\in \S_X} x \frac{p_{X,Y}(x,y)}{p_Y(y)} = \sum_{x\in \S_X} x
    p_{X|Y}(x|y) = \E[X|Y=y] \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional independence** Next, we discuss conditional independence. We
    begin with the formal definition.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Independence)** \(\idx{conditional independence}\xdi\)
    Let \(A, B, C\) be events such that \(\P[C] > 0\). Then \(A\) and \(B\) are conditionally
    independent given \(C\), denoted \(A \indep B | C\), if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A \cap B| C] = \P[A|C] \,\P[B|C]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'In words, quoting [Wikipedia](https://en.wikipedia.org/wiki/Conditional_independence):'
  prefs: []
  type: TYPE_NORMAL
- en: \(A\) and \(B\) are conditionally independent given \(C\) if and only if, given
    knowledge that \(C\) occurs, knowledge of whether \(A\) occurs provides no information
    on the likelihood of \(B\) occurring, and knowledge of whether \(B\) occurs provides
    no information on the likelihood of \(A\) occurring.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In general, conditionally independent events are not (unconditionally) independent.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Imagine I have two six-sided dice. Die 1 has faces \(\{1,3,5,7,9,11\}\)
    and die 2 has faces \(\{2, 4, 6, 8, 10, 12\}\). Suppose I perform the following
    experiment: I pick one of the two dice uniformly at random, and then I roll that
    die twice. Let \(X_1\) and \(X_2\) be the outcomes of the rolls. Consider the
    events \(A = \{X_1 = 1\}\), \(B = \{X_2 = 2\}\), and \(C = \{\text{die 1 is picked}\}\).
    The events \(A\) and \(B\) are clearly dependent: if \(A\) occurs, then I know
    that die 1 was picked, and hence \(B\) cannot occur. Knowledge of one event provides
    information about the likelihood of the other event occurring. Formally, by the
    law of total probability,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A] = \P[A|C]\P[C] + \P[A|C^c]\P[C^c] = \frac{1}{6}\frac{1}{2} + 0 \frac{1}{2}
    = \frac{1}{12}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly \(\P[B] = \frac{1}{12}\). Yet \(\P[A \cap B] = 0 \neq \frac{1}{12}
    \frac{1}{12}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we claim that \(A\) and \(B\) are conditionally independent
    given \(C\). Again this is intuitively clear: once I pick a die, the two rolls
    are independent. For a given die choice, knowledge of one roll provides no information
    about the likelihood of the other roll. Note that the phrase “for a given die
    choice” is critical in the last statement. Formally, by our experiment, we have
    \(\P[A|C] = 1/6\), \(\P[B|C] = 0\) and \(\P[A \cap B|C] = 0\). So indeed'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[A \cap B| C] = \P[A|C] \,\P[B|C] \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Conditional independence is naturally extended to random vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Conditional Independence of Random Vectors)** Let \(\bX,
    \bY, \bW\) be discrete random vectors. Then \(\bX\) and \(\bY\) are said to be
    conditionally independent given \(\bW\), denoted \(\bX \indep \bY | \bW\), if
    for all \(\bx \in \S_\bX\), \(\by \in \S_\bY\) and \(\bw \in \S_\bW\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[\bX = \bx, \bY = \by|\bW = \bw] = \P[\bX = \bx |\bW = \bw] \,\P[\bY =
    \by|\bW = \bw]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: An important consequence is that we can drop the conditioning by the independent
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Role of Independence)** \(\idx{role of independence lemma}\xdi\)
    Let \(\bX, \bY, \bW\) be discrete random vectors such that \(\bX \indep \bY |
    \bW\). For all \(\bx \in \S_\bX\), \(\by \in \S_\bY\) and \(\bw \in \S_\bW\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[\bX = \bx | \bY=\by, \bW=\bw] = \P[\bX = \bx | \bW = \bw]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* In a previous exercise, we showed that \(A \indep B | C\) implies
    \(\P[A | B\cap C] = \P[A | C]\). That implies the claim. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The concept of conditional independence is closely related
    to the concept of d-separation in probabilistic graphical models. Ask your favorite
    AI chatbot to explain d-separation. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2\. The basic configurations[#](#the-basic-configurations "Link to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A powerful approach for constructing complex probability distributions is the
    use of conditional independence. The case of three random variables exemplifies
    key probabilistic relationships. By the product rule, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x, Y=y]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This is conveniently represented through a digraph where the vertices are the
    variables. Recall that an arrow \((i,j)\), from \(i\) to \(j\), indicates that
    \(i\) is a parent of \(j\) and that \(j\) is a child of \(i\). Let \(\pa(i)\)
    be the set of parents of \(i\). The digraph \(G = (V, E)\) below encodes the following
    sampling scheme, referred as ancestral sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: First we pick \(X\) according to its marginal \(\P[X=x]\). Note that \(X\) has
    no parent in \(G\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second we pick \(Y\) according to the conditional probability distribution (CPD)
    \(\P[Y=y|X=x]\). Note that \(X\) is the only parent of \(Y\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally we pick \(Z\) according to the CPD \(\P[Z=z|X=x, Y=y]\). Note that the
    parents of \(Z\) are \(X\) and \(Y\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The full case](../Images/b73cb01c1b1f5d32087884467af21283.png)'
  prefs: []
  type: TYPE_IMG
- en: The graph above is acyclic, that is, it has no directed cycle. The variables
    \(X, Y, Z\) are in [topological order](https://en.wikipedia.org/wiki/Topological_sorting)\(\idx{topological
    order}\xdi\), that is, all edges \((i,j)\) are such that \(i\) comes before \(j\)
    in that order.
  prefs: []
  type: TYPE_NORMAL
- en: The same joint distribution can be represented by a different digraph if the
    product rule is used in a different order. For instance,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[Z=z] \,\P[Y=y|Z=z] \,\P[X=x | Z=z, Y=y] \]
  prefs: []
  type: TYPE_NORMAL
- en: is represented by the following digraph. A topological order this time is \(Z,
    Y, X\).
  prefs: []
  type: TYPE_NORMAL
- en: '![Another full case](../Images/ed39c216e1efdc59d602be7c9e2c744a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The fork** \(\idx{fork}\xdi\) Removing edges in the first graph above encodes
    conditional independence relations. For instance, removing the edge from \(Y\)
    to \(Z\) gives the following graph, known as a fork. We denote this configuration
    as \(Y \leftarrow X \rightarrow Z\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The fork](../Images/77cb78740ee4952c17900d2626c7ad2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The joint distribution simplifies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x]. \]
  prefs: []
  type: TYPE_NORMAL
- en: So, in this case, what has changed is that the CPD of \(Z\) does not depend
    on the value of \(Y\). From the *Role of Independence* lemma, this corresponds
    to assuming the conditional independence \(Z \indep Y|X\). Indeed, we can check
    that claim directly from the joint distribution
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[Y= y, Z=z|X=x] &= \frac{\P[X=x, Y= y, Z=z]}{\P[X=x]}\\ &=
    \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x]}{\P[X=x]}\\ &= \P[Y=y|X=x] \,\P[Z=z
    | X=x] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: '**The chain** \(\idx{chain}\xdi\) Removing the edge from \(X\) to \(Z\) gives
    the following graph, known as a chain (or pipe). We denote this configuration
    as \(X \rightarrow Y \rightarrow Z\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The chain](../Images/5bfd7c9b4911ca0e5931761d25c8457e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The joint distribution simplifies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]. \]
  prefs: []
  type: TYPE_NORMAL
- en: In this case, what has changed is that the CPD of \(Z\) does not depend on the
    value of \(X\). Compare that to the fork. The corresponding conditional independence
    relation is \(Z \indep X|Y\). Indeed, we can check that claim directly
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X= x, Z=z|Y=y] &= \frac{\P[X=x, Y= y, Z=z]}{\P[Y=y]}\\ &=
    \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to use *Bayes’ Rule* to get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]}\\
    &= \frac{\P[Y=y|X=x]\,\P[X=x]}{\P[Y=y]} \P[Z=z | Y=y]\\ &= \P[X=x|Y=y] \,\P[Z=z
    | Y=y] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed.
  prefs: []
  type: TYPE_NORMAL
- en: For any \(x, y, z\) where the joint probability is positive, we can re-write
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X=x, Y=y, Z=z]\\ &= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]\\
    &= \P[Y=y] \,\P[X=x|Y=y] \,\P[Z=z | Y=y], \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y] = \P[X=x] \,\P[Y=y|X=x] = \P[Y=y] \,\P[X=x|Y=y] \]
  prefs: []
  type: TYPE_NORMAL
- en: by definition of the conditional probability. In other words, we have shown
    that the chain \(X \rightarrow Y \rightarrow Z\) is in fact equivalent to the
    fork \(X \leftarrow Y \rightarrow Z\). In particular, they both correspond to
    assuming the conditional independence relation \(Z \indep X|Y\), although they
    capture a different way to sample the joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**The collider** \(\idx{collider}\xdi\) Removing the edge from \(X\) to \(Y\)
    gives the following graph, known as a collider. We denote this configuration as
    \(X \rightarrow Z \leftarrow Y\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The collider](../Images/a4600775ea580d4809bd8b11895a447d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The joint distribution simplifies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X=x, Y=y, Z=z] = \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y]. \]
  prefs: []
  type: TYPE_NORMAL
- en: In this case, what has changed is that the CPD of \(Y\) does not depend on the
    value of \(X\). Compare that to the fork and the chain. This time we have \(X
    \indep Y\). Indeed, we can check that claim directly
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X= x, Y=y] &= \sum_{z \in \S_z} \P[X=x, Y=y, Z=z]\\ &= \sum_{z
    \in \S_z} \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y]\\ &= \P[X=x] \,\P[Y=y] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. In particular, the collider cannot be reframed as a chain or fork
    as its underlying assumption is stronger.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps counter-intuitively, conditioning on \(Z\) makes \(X\) and \(Y\) dependent
    in general. This is known as explaining away or Berkson’s Paradox.
  prefs: []
  type: TYPE_NORMAL
- en: '6.3.3\. Example: Naive Bayes[#](#example-naive-bayes "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model-based justification we gave for logistic regression in the subsection
    on generalized linear models used a so-called [discriminative approach](https://en.wikipedia.org/wiki/Discriminative_model)\(\idx{discriminative
    model}\xdi\), where the conditional distribution of the target \(y\) given the
    features \(\mathbf{x}\) is specified – but not the full distribution of the data
    \((\mathbf{x}, y)\). Here we give an example of the [generative approach](https://en.wikipedia.org/wiki/Generative_model)\(\idx{generative
    model}\xdi\), which models the full distribution. For a discussion of the benefits
    and drawbacks of each approach, see for example [here](https://en.wikipedia.org/wiki/Discriminative_model#Contrast_with_generative_model).
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes\(\idx{Naive Bayes}\xdi\) model is a simple discrete model for
    supervised learning. It is useful for document classification for instance, and
    we will use that terminology here to be concrete. We assume that a document has
    a single topic \(C\) from a list \(\mathcal{C} = \{1, \ldots, K\}\) with probability
    distribution \(\pi_k = \P[C = k]\). There is a vocabulary of size \(M\) and we
    record the presence or absence of a word \(m\) in the document with a Bernoulli
    variable \(X_m \in \{0,1\}\), where \(p_{k,m} = \P[X_m = 1|C = k]\). We denote
    by \(\bX = (X_1, \ldots, X_M)\) the corresponding vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conditional independence assumption comes next: we assume that, given a
    topic \(C\), the word occurrences are independent. That is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[\bX = \bx|C=k] &= \prod_{m=1}^M \P[X_m = x_m|C = k]\\ &=
    \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the joint distribution is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[C = k, \bX = \bx] &= \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \pi_k
    \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, this is similar to a fork with \(C\) at its center and \(M\) prongs
    for the \(X_m\)s. This is represented using the so-called plate notation. The
    box with the \(M\) in the corner below indicates that \(X_m\) is repeated \(M\)
    times, all copies being conditionally independent given \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: '![Naives Bayes](../Images/1e486313a3d40a4fea7001344a53121b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Model fitting** Before using the model for prediction, one must first fit
    the model from training data \(\{\bx_i, c_i\}_{i=1}^n\). In this case, it means
    estimating the unknown parameters \(\bpi\) and \(\{\bp_k\}_{k=1}^K\), where \(\bp_k
    = (p_{k,1},\ldots, p_{k,M})\). For each \(k, m\) let'
  prefs: []
  type: TYPE_NORMAL
- en: \[ N_{k,m} = \sum_{i=1}^n \mathbf{1}_{\{c_i = k\}} x_{i,m}, \quad N_{k} = \sum_{i=1}^n
    \mathbf{1}_{\{c_i = k\}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We use maximum likelihood estimation which, recall, entails finding the parameters
    that maximize the probability of observing the data
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i, c_i\}) = \prod_{i=1}^n \pi_{c_i} \prod_{m=1}^M
    p_{c_i, m}^{x_{i,m}} (1-p_{c_i, m})^{1-x_{i,m}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, as usual, we assume that the samples are independent and identically distributed.
    We take a logarithm to turn the products into sums and consider the negative log-likelihood
    (NLL)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} & L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\})\\ &\quad = - \sum_{i=1}^n
    \log \pi_{c_i} - \sum_{i=1}^n \sum_{m=1}^M [x_{i,m} \log p_{c_{i}, m} + (1-x_{i,m})
    \log (1-p_{c_i, m})]\\ &\quad = - \sum_{k=1}^K N_k \log \pi_k - \sum_{k=1}^K \sum_{m=1}^M
    [N_{k,m} \log p_{k,m} + (N_k-N_{k,m}) \log (1-p_{k,m})]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The NLL can be broken up naturally into several terms that depend on different
    sets of parameters – and therefore can be optimized separately. First, there is
    a term that depends only on the \(\pi_k\)’s
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_0(\bpi; \{\bx_i, c_i\}) = - \sum_{k=1}^K N_k \log \pi_k. \]
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the sum can be further split into \(KM\) terms, each depending only
    on \(p_{km}\) for a fixed \(k\) and m
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{k,m}(p_{k,m}; \{\bx_i, c_i\}) = - N_{k,m} \log p_{k,m} - (N_k-N_{k,m})
    \log (1-p_{k,m}). \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\}) = J_0(\bpi; \{\bx_i, c_i\}) + \sum_{k=1}^K
    \sum_{m=1}^M J_{k,m}(p_{k,m}; \{\bx_i, c_i\}). \]
  prefs: []
  type: TYPE_NORMAL
- en: We minimize these terms separately. We assume that \(N_k > 0\) for all \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a special case of maximum likelihood estimation, which we previously
    worked out in an example, where we consider the space of all probability distributions
    over a finite set. The maximum likelihood estimator in that case is given by the
    empirical frequencies. Notice that minimizing \(J_0(\bpi; \{\bx_i, c_i\})\) is
    precisely of this form: we observe \(N_k\) samples from class \(k\) and we seek
    the maximum likelihood estimator of, \(\pi_k\), the probability of observing \(k\).
    Hence the solution is simply'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_k = \frac{N_k}{N}, \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(k\). Similarly, for each \(k\), \(m\), \(J_{k,m}\) is of that form
    as well. Here the states correspond to word \(m\) being present or absent in a
    document of class \(k\), and we observe \(N_{k,m}\) documents of type \(k\) where
    the word \(m\) is present. So the solution is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{p}_{k,m} = \frac{N_{k,m}}{N_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(k, m\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction** To predict the class of a new document, it is natural to maximize
    over \(k\) the probability that \(\{C=k\}\) given \(\{\bX = \bx\}\). By Bayes’
    rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[C=k | \bX = \bx] &= \frac{\P[C = k, \bX = \bx]}{\P[\bX =
    \bx]}\\ &= \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}} {\sum_{k'=1}^K
    \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_m} (1-p_{k',m})^{1-x_m}}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: As the denominator does not in fact depend on \(k\), maximizing \(\P[C=k | \bX
    = \bx]\) boils down to maximizing the numerator \(\pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}\), which is straighforward to compute. As we did previously,
    we take a negative logarithm – which has some numerical advantages – and we refer
    to it as the *score*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &- \log\left(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\right)\\
    &\qquad = -\log\pi_k - \sum_{m=1}^M [x_m \log p_{k,m} + (1-x_m) \log (1-p_{k,m})].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, taking a negative logarithm turns out to be a good idea here
    because computing a product of probabilities can produce very small numbers that,
    when they fall beneath machine precision, are approximated by zero. This is called
    [underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow)\(\idx{underflow}\xdi\).
    By taking a negative logarithm, these probabilities are transformed into positive
    numbers of reasonable magnitude and the product becomes of sum of these. Moreover,
    because this transformation is monotone, we can use the transformed values directly
    to compute the optimal score, which is our ultimate goal in the prediction step.
    Since the parameters are unknown, we use \(\hat{\pi}_k\) and \(\hat{p}_{k,m}\)
    in place of \(\pi_k\) and \(p_{k,m}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot for more information on the issue
    of underflow, and its cousin overflow\(\idx{overflow}\xdi\), in particular in
    the context of multiypling probabilities. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: While maximum likehood estimation has [desirable theoretical properties](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties),
    it does suffer from [overfitting](https://towardsdatascience.com/parameter-inference-maximum-aposteriori-estimate-49f3cd98267a).
    If for instance a particular word \(m\) does not occur in any training document,
    then the probability of observing a new document that happens to contain that
    word is estimated to be \(0\) for any class (i.e., \(\hat{p}_{k,m} = 0\) for all
    \(k\) so that \(\hat \pi_k \prod_{m=1}^M \hat{p}_{k,m}^{x_m} (1-\hat{p}_{k,m})^{1-x_m}
    = 0\) for all \(k\) ) and the maximization problem above is not well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to deal with this is [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)\(\idx{Laplace
    smoothing}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bar{\pi}_k = \frac{N_k + \alpha}{N + K \alpha}, \quad \bar{p}_{k,m} = \frac{N_{k,m}
    + \beta}{N_k + 2 \beta} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\alpha, \beta > 0\), which can be justified using a Bayesian or regularization
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: We implement the Naive Bayes model with Laplace smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: We encode the data into a table, where the rows are the classes and the columns
    are the features. The entries are the corresponding \(N_{k,m}\)s. In addition
    we provide the vector \((N_k)_k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Using `N_k[:, np.newaxis]` reshapes the one-dimensional array `N_k` into a two-dimensional
    column vector. For example, if `N_k` has a shape of \((K,)\), then `N_k[:, np.newaxis]`
    changes its shape to \((K, 1)\). This allows the division in the expression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: to work correctly with [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html),
    ensuring that each element in a row of `N_km` is divided by the corresponding
    value in `N_k`.
  prefs: []
  type: TYPE_NORMAL
- en: The next function computes the negative logarithm of \(\pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}\), that is, the score of \(k\), and outputs a \(k\) achieving
    the minimum score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We use a simple example from [Stack Overflow](https://stackoverflow.com/questions/10059594/):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** Let’s say we have data on 1000 pieces of fruit. They happen to
    be Banana, Orange or some Other Fruit. We know 3 characteristics about each fruit:
    whether it is long, whether it is sweet, and if its color is yellow[, as displayed
    in the table below].'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Fruit | Long | Sweet | Yellow | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Banana | 400 | 350 | 450 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| Orange | 0 | 150 | 300 | 300 |'
  prefs: []
  type: TYPE_TB
- en: '| Other | 100 | 150 | 50 | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 500 | 650 | 800 | 1000 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We run `nb_fit_table` on our simple dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Continuing on with our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we are given the properties of an unknown fruit, and asked to
    classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana?
    Is it an Orange? Or Is it some Other Fruit?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We run `nb_predict` on our dataset with the additional fruit from the quote
    above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Laplace smoothing is a special case of a more general technique
    known as Bayesian parameter estimation. Ask your favorite AI chatbot to explain
    Bayesian parameter estimation and how it relates to maximum likelihood estimation
    and Laplace smoothing. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following statements is **not** true about conditional probabilities?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[A|B] = \frac{\mathbb{P}[A \cap B]}{\mathbb{P}[B]}\) for events
    \(A\) and \(B\) with \(\mathbb{P}[B] > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: b) If \(A\) and \(B\) are independent, then \(\mathbb{P}[A|B] = \mathbb{P}[A]\).
  prefs: []
  type: TYPE_NORMAL
- en: c) Conditional probabilities can be used to express the multiplication rule
    and the law of total probability.
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[A|B] = \mathbb{P}[B|A]\) for any events \(A\) and \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is the correct mathematical expression for the
    conditional independence of events \(A\) and \(B\) given event \(C\), denoted
    as \(A \perp\!\!\!\perp B \mid C\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] + \mathbb{P}[B \mid
    C]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[A \cup B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[A \mid B \cap C] = \mathbb{P}[A \mid C]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** In the fork configuration \(Y \leftarrow X \rightarrow Z\), which of
    the following conditional independence relations always holds?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(X \perp\!\!\!\perp Y \mid Z\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(Y \perp\!\!\!\perp Z \mid X\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(X \perp\!\!\!\perp Z \mid Y\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(Y \perp\!\!\!\perp Z\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In the collider configuration \(X \rightarrow Z \leftarrow Y\), which
    of the following conditional independence relations always holds?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(X \perp\!\!\!\perp Y \mid Z\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(Y \perp\!\!\!\perp Z \mid X\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(X \perp\!\!\!\perp Z \mid Y\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(X \perp\!\!\!\perp Y\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following best describes the graphical representation of
    the Naive Bayes model for document classification?'
  prefs: []
  type: TYPE_NORMAL
- en: a) A chain with the topic variable at the center and word variables as the links.
  prefs: []
  type: TYPE_NORMAL
- en: b) A collider with the topic variable at the center and word variables as the
    parents.
  prefs: []
  type: TYPE_NORMAL
- en: c) A fork with the topic variable at the center and word variables as the prongs.
  prefs: []
  type: TYPE_NORMAL
- en: d) A complete graph with edges between all pairs of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: d. Justification: In general, \(\mathbb{P}[A|B] \neq \mathbb{P}[B|A]\).
    Bayes’ rule provides the correct relationship between these two conditional probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: The text states, “Then \(A\) and \(B\) are
    conditionally independent given \(C\), denoted \(A \perp\!\!\!\perp B \mid C\),
    if \(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: The text states, “Removing the edge from \(Y\)
    to \(Z\) gives the following graph, known as a fork. We denote this configuration
    as \(Y \leftarrow X \rightarrow Z\). […] The corresponding conditional independence
    relation is \(Z \perp\!\!\!\perp Y \mid X\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text states, “Removing the edge from \(X\)
    to \(Y\) gives the following graph, known as a collider. We denote this configuration
    as \(X \rightarrow Z \leftarrow Y\). […] This time we have \(X \perp\!\!\!\perp
    Y\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: The text states, “Graphically, this is similar
    to a fork with \(C\) at its center and \(M\) prongs for the \(X_m\)s.”'
  prefs: []
  type: TYPE_NORMAL
