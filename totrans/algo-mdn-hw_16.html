<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Throughput Computing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Throughput Computing</h1>
<blockquote>原文：<a href="https://en.algorithmica.org/hpc/pipelining/throughput/">https://en.algorithmica.org/hpc/pipelining/throughput/</a></blockquote><div id="search"><input id="search-bar" type="search" placeholder="Search this book…" oninput="search()"/><div id="search-count"/><div id="search-results"/></div><header><div class="info"/></header><article><p>Optimizing for <em>latency</em> is usually quite different from optimizing for <em>throughput</em>:</p><ul><li>When optimizing data structure queries or small one-time or branchy algorithms, you need to <a href="../tables">look up the latencies</a> of its instructions, mentally construct the execution graph of the computation, and then try to reorganize it so that the critical path is shorter.</li><li>When optimizing hot loops and large-dataset algorithms, you need to look up the throughputs of their instructions, count how many times each one is used per iteration, determine which of them is the bottleneck, and then try to restructure the loop so that it is used less often.</li></ul><p>The last advice only works for <em>data-parallel</em> loops, where each iteration is fully independent of the previous one. When there is some interdependency between consecutive iterations, there may potentially be a pipeline stall caused by a <a href="../hazards">data hazard</a> as the next iteration is waiting for the previous one to complete.</p><span class="anchor" id="example"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/pipelining/throughput/#example">#</a>Example</h3><p>As a simple example, consider how the sum of an array is computed:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span></code></pre></div><p>Let’s assume for a moment that the compiler doesn’t <a href="/hpc/simd">vectorize</a> this loop, <a href="/hpc/cpu-cache/bandwidth">the memory bandwidth</a> isn’t a concern, and that the loop is <a href="/hpc/architecture/loops">unrolled</a> so that we don’t pay any additional cost associated with maintaining the loop variables. In this case, the computation becomes very simple:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span></code></pre></div><p>How fast can we compute this? At exactly one cycle per element — because we need one cycle each iteration to <code>add</code> another value to <code>s</code>. The latency of the memory read doesn’t matter because the CPU can start it ahead of time.</p><p>But we can go higher than that. The <em>throughput</em> of <code>add</code><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> is 2 on my CPU (Zen 2), meaning we could theoretically execute two of them every cycle. But right now this isn’t possible: while <code>s</code> is being used to accumulate $i$-th element, it can’t be used for $(i+1)$-th for at least one cycle.</p><p>The solution is to use <em>two</em> accumulators and just sum up odd and and even elements separately:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">s0</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">s0</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">s1</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">s0</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">s1</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"/><span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="n">s0</span> <span class="o">+</span> <span class="n">s1</span><span class="p">;</span>
</span></span></code></pre></div><p>Now our superscalar CPU can execute these two “threads” simultaneously, and our computation no longer has any critical paths that limit the throughput.</p><span class="anchor" id="the-general-case"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/pipelining/throughput/#the-general-case">#</a>The General Case</h3><p>If an instruction has a latency of $x$ and a throughput of $y$, then you would need to use $x \cdot y$ accumulators to saturate it. This also implies that you need $x \cdot y$ logical registers to hold their values, which is an important consideration for CPU designs, limiting the maximum number of usable execution units for high-latency instructions.</p><p>This technique is mostly used with <a href="/hpc/simd">SIMD</a> and not in scalar code. You can <a href="/hpc/simd/reduction">generalize</a> the code above and compute sums and other reductions faster than the compiler.</p><p>In general, when optimizing loops, you usually have just one or a few <em>execution ports</em> that you want to utilize to their fullest, and you engineer the rest of the loop around them. As different instructions may use different sets of ports, it is not always clear which one is going to be overused. In situations like this, <a href="/hpc/profiling/mca">machine code analyzers</a> can be very helpful for finding the bottlenecks of small assembly loops.</p><section class="footnotes" role="doc-endnotes"><hr/><ol><li id="fn:1" role="doc-endnote"><p>The throughput of register-register <code>add</code> is 4, but since we are reading its second operand from memory, it is bottlenecked by the throughput of memory <code>mov</code>, which is 2 on Zen 2. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎</a></p></li></ol></section></article><div class="nextprev"><div class="left"><a href="https://en.algorithmica.org/hpc/pipelining/tables/" id="prev-article">← Instruction Tables</a></div><div class="right"><a href="https://en.algorithmica.org/hpc/compilation/" id="next-article">../Compilation →</a></div></div>    
</body>
</html>