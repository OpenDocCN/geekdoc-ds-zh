["```\n#include  <algorithm>\n#include  <vector>\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(a.begin(),  a.end());\n} \n```", "```\n#include  <algorithm>\n#include  <vector>\n#include  <execution> // To get std::execution\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(\n  std::execution::par_unseq,  // This algorithm can be run in parallel\n  a.begin(),  a.end()\n  );\n} \n```", "```\ndefault:  build\n\n# Set compiler\nKOKKOS_PATH  =  $(shell  pwd)/kokkos\nCXX  =  hipcc\n# CXX = ${KOKKOS_PATH}/bin/nvcc_wrapper\n\n# Variables for the Makefile.kokkos\nKOKKOS_DEVICES  =  \"HIP\"\n# KOKKOS_DEVICES = \"Cuda\"\nKOKKOS_ARCH  =  \"VEGA90A\"\n# KOKKOS_ARCH = \"Volta70\"\nKOKKOS_CUDA_OPTIONS  =  \"enable_lambda,force_uvm\"\n\n# Include Makefile.kokkos\ninclude $(KOKKOS_PATH)/Makefile.kokkos\n\nbuild:  $(KOKKOS_LINK_DEPENDS) $(KOKKOS_CPP_DEPENDS) hello.cpp\n  $(CXX)  $(KOKKOS_CPPFLAGS)  $(KOKKOS_CXXFLAGS)  $(KOKKOS_LDFLAGS)  hello.cpp  $(KOKKOS_LIBS)  -o  hello \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main(int  argc,  char*  argv[])  {\n  Kokkos::initialize(argc,  argv);\n  std::cout  <<  \"Execution Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace).name()  <<  std::endl;\n  std::cout  <<  \"Memory Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace::memory_space).name()  <<  std::endl;\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  PrgEnv-cray-amd\n$ CC  program.cpp  -lOpenCL  -o  program  # C++ program\n$ cc  program.c  -lOpenCL  -o  program  # C program \n```", "```\n// Initialize OpenCL\ncl::Device  device  =  cl::Device::getDefault();\ncl::Context  context(device);\ncl::CommandQueue  queue(context,  device); \n```", "```\n// Initialize OpenCL\ncl_int  err;  // Error code returned by API calls\ncl_platform_id  platform;\nerr  =  clGetPlatformIDs(1,  &platform,  NULL);\nassert(err  ==  CL_SUCCESS);  // Checking error codes is skipped later for brevity\ncl_device_id  device;\nerr  =  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\ncl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  &err);\ncl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  &err); \n```", "```\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global int *a) {\n int i = get_global_id(0);\n a[i] = i;\n }\n)\"; \n```", "```\ncl::Program  program(context,  kernel_source);\nprogram.build({device});\ncl::Kernel  kernel_dot(program,  \"dot\"); \n```", "```\ncl_int  err;\ncl_program  program  =  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  &err);\nerr  =  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\ncl_kernel  kernel_dot  =  clCreateKernel(program,  \"vector_add\",  &err); \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  acpp/24.06.0 \n```", "```\nint  main()  {\n  // Create an out-of-order queue on the default device:\n  sycl::queue  q;\n  // Now we can submit tasks to q!\n} \n```", "```\n// Iterate over all available devices\nfor  (const  auto  &device  :  sycl::device::get_devices())  {\n  // Print the device name\n  std::cout  <<  \"Creating a queue on \"  <<  device.get_info<sycl::info::device::name>()  <<  \"\\n\";\n  // Create an in-order queue for the current device\n  sycl::queue  q(device,  {sycl::property::queue::in_order()});\n  // Now we can submit tasks to q!\n} \n```", "```\n// Create a buffer of n integers\nauto  buf  =  sycl::buffer<int>(sycl::range<1>(n));\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Create write-only accessor for buf\n  auto  acc  =  buf.get_access<sycl::access_mode::write>(cgh);\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is written to the buffer via acc\n  acc[i]  =  /*...*/\n  });\n});\n/* If we now submit another kernel with accessor to buf, it will not\n * start running until the kernel above is done */ \n```", "```\n// Create a shared (migratable) allocation of n integers\n// Unlike with buffers, we need to specify a queue (or, explicitly, a device and a context)\nint*  v  =  sycl::malloc_shared<int>(n,  q);\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is directly written to v\n  v[i]  =  /*...*/\n  });\n});\n// If we want to access v, we have to ensure that the kernel has finished\nq.wait();\n// After we're done, the memory must be deallocated\nsycl::free(v,  q); \n```", "```\n$ salloc  -A  project_465002387  -N  1  -t  1:00:00  -p  standard-g  --gpus-per-node=1\n....\nsalloc: Granted job allocation 123456\n\n$ module  load  LUMI/24.03  partition/G\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  rocm/6.0.3  acpp/24.06.0 \n```", "```\n> $ srun  acpp-info  -l\n> =================Backend information===================\n> Loaded backend 0: HIP\n>  Found device: AMD Instinct MI250X\n> Loaded backend 1: OpenMP\n>  Found device: hipSYCL OpenMP host device \n> ```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nint  main()  {\n  // Create an in-order queue\n  sycl::queue  q{sycl::property::queue::in_order()};\n  // Print the device name, just for fun\n  std::cout  <<  \"Running on \"\n  <<  q.get_device().get_info<sycl::info::device::name>()  <<  std::endl;\n  const  int  n  =  1024;  // Vector size\n\n  // Allocate device and host memory for the first input vector\n  float  *d_x  =  sycl::malloc_device<float>(n,  q);\n  float  *h_x  =  sycl::malloc_host<float>(n,  q);\n // Bonus question: Can we use `std::vector` here instead of `malloc_host`? // TODO: Allocate second input vector on device and host, d_y and h_y  // Allocate device and host memory for the output vector\n  float  *d_z  =  sycl::malloc_device<float>(n,  q);\n  float  *h_z  =  sycl::malloc_host<float>(n,  q);\n\n  // Initialize values on host\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  h_x[i]  =  i;\n // TODO: Initialize h_y somehow  }\n  const  float  alpha  =  0.42f;\n\n  q.copy<float>(h_x,  d_x,  n);\n // TODO: Copy h_y to d_y // Bonus question: Why don't we need to wait before using the data? \n  // Run the kernel\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n // TODO: Modify the code to compute z[i] = alpha * x[i] + y[i]  d_z[i]  =  alpha  *  d_x[i];\n  });\n\n // TODO: Copy d_z to h_z // TODO: Wait for the copy to complete \n  // Check the results\n  bool  ok  =  true;\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  float  ref  =  alpha  *  h_x[i]  +  h_y[i];  // Reference value\n  float  tol  =  1e-5;  // Relative tolerance\n  if  (std::abs((h_z[i]  -  ref))  >  tol  *  std::abs(ref))  {\n  std::cout  <<  i  <<  \" \"  <<  h_z[i]  <<  \" \"  <<  h_x[i]  <<  \" \"  <<  h_y[i]\n  <<  std::endl;\n  ok  =  false;\n  break;\n  }\n  }\n  if  (ok)\n  std::cout  <<  \"Results are correct!\"  <<  std::endl;\n  else\n  std::cout  <<  \"Results are NOT correct!\"  <<  std::endl;\n\n  // Free allocated memory\n  sycl::free(d_x,  q);\n  sycl::free(h_x,  q);\n // TODO: Free d_y, h_y.  sycl::free(d_y,  q);\n  sycl::free(h_y,  q);\n\n  return  0;\n} \n```", "```\n$ acpp  -O3  exercise-sycl-saxpy.cpp  -o  exercise-sycl-saxpy\n$ srun  ./exercise-sycl-saxpy\nRunning on AMD Instinct MI250X\nResults are correct! \n```", "```\n    git  clone  https://github.com/alpaka-group/alpaka3.git\n    cd  alpaka \n    ```", "```\n    export  ALPAKA_DIR=/path/to/your/alpaka/install/dir \n    ```", "```\n    mkdir  build\n    cmake  -B  build  -S  .  -DCMAKE_INSTALL_PREFIX=$ALPAKA_DIR\n    cmake  --build  build  --parallel \n    ```", "```\n    export  CMAKE_PREFIX_PATH=$ALPAKA_DIR:$CMAKE_PREFIX_PATH \n    ```", "```\n> cmake_minimum_required(VERSION  3.25)\n> project(myAlpakaApp  VERSION  1.0)\n> \n> # Find installed alpaka\n> find_package(alpaka  REQUIRED)\n> \n> # Build the executable\n> add_executable(myAlpakaApp  main.cpp)\n> target_link_libraries(myAlpakaApp  PRIVATE  alpaka::alpaka)\n> alpaka_finalize(myAlpakaApp) \n> ```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n> #include  <alpaka/alpaka.hpp>\n> #include  <cstdlib>\n> #include  <iostream>\n> \n> namespace  ap  =  alpaka;\n> \n> auto  getDeviceSpec()\n> {\n>   /* Select a device, possible combinations of api+deviceKind:\n>  * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n>  * oneApi+amdGpu, oneApi+nvidiaGpu\n>  */\n>   return  ap::onHost::DeviceSpec{ap::api::hip,  ap::deviceKind::amdGpu};\n> }\n> \n> int  main(int  argc,  char**  argv)\n> {\n>   // Initialize device specification and selector\n>   ap::onHost::DeviceSpec  devSpec  =  getDeviceSpec();\n>   auto  deviceSelector  =  ap::onHost::makeDeviceSelector(devSpec);\n> \n>   // Query available devices\n>   auto  num_devices  =  deviceSelector.getDeviceCount();\n>   std::cout  <<  \"Number of available devices: \"  <<  num_devices  <<  \"\\n\";\n> \n>   if  (num_devices  ==  0)  {\n>   std::cerr  <<  \"No devices found for the selected backend\\n\";\n>   return  EXIT_FAILURE;\n>   }\n> \n>   // Select and initialize the first device\n>   auto  device  =  deviceSelector.makeDevice(0);\n>   std::cout  <<  \"Using device: \"  <<  device.getName()  <<  \"\\n\";\n> \n>   return  EXIT_SUCCESS;\n> } \n> ```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  myProject\n{\n  namespace  ap  =  alpaka;\n  // Your code here\n} \n```", "```\nauto  devSelector  =  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\nif  (devSelector.getDeviceCount()  ==  0)\n{\n  throw  std::runtime_error(\"No device found!\");\n}\nauto  device  =  devSelector.makeDevice(0); \n```", "```\nauto  queue  =  device.makeQueue();\nauto  nonBlockingQueue  =  device.makeQueue(ap::queueKind::nonBlocking);\nauto  blockingQueue  =  device.makeQueue(ap::queueKind::blocking);\n\nauto  event  =  device.makeEvent();\nqueue.enqueue(event);\nap::onHost::wait(event);\nap::onHost::wait(queue); \n```", "```\nauto  hostBuffer  =  ap::onHost::allocHost<DataType>(extent3D);\nauto  devBuffer  =  ap::onHost::alloc<DataType>(device,  extentMd);\nauto  devMappedBuffer  =  ap::onHost::allocMapped<DataType>(device,  extentMd);\n\nauto  hostView  =  ap::makeView(api::host,  externPtr,  ap::Vec{numElements});\nauto  devNonOwningView  =  devBuffer.getView();\n\nap::onHost::memset(queue,  devBuffer,  uint8_t{0});\nap::onHost::memcpy(queue,  devBuffer,  hostBuffer);\nap::onHost::fill(queue,  devBuffer,  DataType{42}); \n```", "```\nconstexpr  uint32_t  dim  =  2u;\nusing  IdxType  =  size_t;\nusing  DataType  =  int;\n\nIdxType  valueX,  valueY;\nauto  extentMD  =  ap::Vec{valueY,  valueX};\n\nauto  frameSpec  =  ap::onHost::FrameSpec{numFramesMd,  frameExtentMd};\nauto  tunedSpec  =  ap::onHost::getFrameSpec<DataType>(device,  extentMd);\n\nqueue.enqueue(tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...});\n\nauto  executor  =  ap::exec::cpuSerial;\nqueue.enqueue(executor,  tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...}); \n```", "```\nstruct  MyKernel\n{\n  ALPAKA_FN_ACC  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,  auto...  args)  const\n  {\n  auto  idxMd  =  acc.getIdxWithin(ap::onAcc::origin::grid,  ap::onAcc::unit::blocks);\n\n  auto  sharedMdArray  =\n  ap::onAcc::declareSharedMdArray<float,  ap::uniqueId()>(acc,  ap::CVec<uint32_t,  3,  4>{});\n\n  ap::onAcc::syncBlockThreads(acc);\n  auto  old  =  onAcc::atomicAdd(acc,  args...);\n  ap::onAcc::memFence(acc,  ap::onAcc::scope::block);\n  auto  sinValue  =  ap::math::sin(args[0]);\n  }\n}; \n```", "```\n    mkdir  my_alpaka_project  &&  cd  my_alpaka_project \n    ```", "```\n    cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\n    cmake  --build  build  --parallel \n    ```", "```\n    ./build/myAlpakaApp \n    ```", "```\nNumber of available devices: 1\nUsing device: [Device Name] \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::hip, ap::deviceKind::amdGpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  -x  hip  --offload-arch=gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host, ap::deviceKind::cpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::cuda, ap::deviceKind::nvidiaGpu);\nnvcc  -I  $ALPAKA_DIR/include/  -std=c++20  --expt-relaxed-constexpr  -x  cuda  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::cpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64_x86_64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::intelGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::amdGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=amd_gpu_gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::nvidiaGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=nvptx64-nvidia-cuda  -Xsycl-target-backend=nvptx64-nvidia-cuda  --offload-arch=sm_80  main.cpp\n./a.out \n```", "```\n$ srun  -p  dev-g  --gpus  1  -N  1  -n  1  --time=00:20:00  --account=project_465002387  --pty  bash\n....\nsrun: job 1234 queued and waiting for resources\nsrun: job 1234 has been allocated resources\n\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n$ rocm-smi\n\n======================================= ROCm System Management Interface =======================================\n================================================= Concise Info =================================================\nDevice  [Model : Revision]    Temp    Power  Partitions      SCLK    MCLK     Fan  Perf    PwrCap  VRAM%  GPU%\n Name (20 chars)       (Edge)  (Avg)  (Mem, Compute)\n================================================================================================================\n0       [0x0b0c : 0x00]       45.0Â°C  N/A    N/A, N/A        800Mhz  1600Mhz  0%   manual  0.0W      0%   0%\n AMD INSTINCT MI200 (\n================================================================================================================\n============================================= End of ROCm SMI Log ============================================== \n```", "```\ncmake_minimum_required(VERSION  3.25)\nproject(vectorAdd  LANGUAGES  CXX  VERSION  1.0)\n#Use CMake's FetchContent to download and integrate alpaka3 directly from GitHub\ninclude(FetchContent)\n#Declare where to fetch alpaka3 from\n#This will download the library at configure time\nFetchContent_Declare(alpaka3  GIT_REPOSITORY  https://github.com/alpaka-group/alpaka3.git  GIT_TAG  dev)\n#Make alpaka3 available for use in this project\n#This downloads, configures, and makes the library targets available\nFetchContent_MakeAvailable(alpaka3)\n#Finalize the alpaka FetchContent setup\nalpaka_FetchContent_Finalize() #Create the executable target from the source file\nadd_executable(vectorAdd  main.cpp)\n#Link the alpaka library to the executable\ntarget_link_libraries(vectorAdd  PRIVATE  alpaka::alpaka)\n#Finalize the alpaka configuration for this target\n#This sets up backend - specific compiler flags and dependencies\nalpaka_finalize(vectorAdd) \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise vector addition on device\n ap::onHost::transform(queue,  c,  std::plus{},  a,  b); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n$ mkdir  alpakaExercise  &&  cd  alpakaExercise\n$ vim  CMakeLists.txt\nand now paste the CMakeLsits here (Press i, followed by Ctrl+Shift+V)\nPress esc and then :wq to exit vim\n$ vim  main.cpp\nSimilarly, paste the C++ code here \n```", "```\nconfigure step, we additionaly specify that HIP is available\n$ cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\nbuild\n$ cmake  --build  build  --parallel\nrun\n$ ./build/vectorAdd\nUsing alpaka device: AMD Instinct MI250X id=0\nc[0] = 1\nc[1] = 2\nc[2] = 3\nc[3] = 4\nc[4] = 5 \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  AddKernel  {\n constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const  &acc, ap::concepts::IMdSpan  auto  c, ap::concepts::IMdSpan  auto  const  a, ap::concepts::IMdSpan  auto  const  b)  const  { for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid, ap::IdxRange{c.getExtents()}))  { c[idx]  =  a[idx]  +  b[idx]; } } }; \nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n auto  frameSpec  =  ap::onHost::getFrameSpec<int>(devAcc,  c.getExtents()); \n // Call the element-wise addition kernel on device queue.enqueue(frameSpec,  ap::KernelBundle{AddKernel{},  c,  a,  b}); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <algorithm>\n#include  <cstdio>\n#include  <execution>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  5;\n\n  // Allocate arrays\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  std::transform(std::execution::par_unseq,  a.begin(),  a.end(),  b.begin(),\n  c.begin(),  [](int  i,  int  j)  {  return  i  *  j;  });\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *b  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *c  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  Kokkos::kokkos_free(b);\n  Kokkos::kokkos_free(c);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 220\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =\n  \"                                                 \\\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) { \\\n int i = get_global_id(0);                                                        \\\n c[i] = a[i] * b[i];                                                              \\\n }                                                                                  \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"dot\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *b  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *c  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n  clSetKernelArgSVMPointer(kernel,  1,  b);\n  clSetKernelArgSVMPointer(kernel,  2,  c);\n\n  // Create mappings for host and initialize values\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  a,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  b,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n  clEnqueueSVMUnmap(queue,  b,  0,  NULL,  NULL);\n\n  size_t  globalSize  =  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  c,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  clEnqueueSVMUnmap(queue,  c,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n  clSVMFree(context,  b);\n  clSVMFree(context,  c);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(n,  q);\n  int  *b  =  sycl::malloc_shared<int>(n,  q);\n  int  *c  =  sycl::malloc_shared<int>(n,  q);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c[i]  =  a[i]  *  b[i];\n  }).wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n  sycl::free(b,  q);\n  sycl::free(c,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints on Kokkos host memory space\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_a(\"h_a\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_b(\"h_b\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_c(\"h_c\",  n);\n\n  // Allocate space for 5 ints on Kokkos default memory space (eg, GPU memory)\n  Kokkos::View<int  *>  a(\"a\",  n);\n  Kokkos::View<int  *>  b(\"b\",  n);\n  Kokkos::View<int  *>  c(\"c\",  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy from host to device\n  Kokkos::deep_copy(a,  h_a);\n  Kokkos::deep_copy(b,  h_b);\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Copy from device to host\n  Kokkos::deep_copy(h_c,  c);\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) {\n int i = get_global_id(0);\n c[i] = a[i] * b[i];\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device.\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_dot(program,  \"dot\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Create buffers and copy input data to device.\n  cl::Buffer  dev_a(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  a.data());\n  cl::Buffer  dev_b(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  b.data());\n  cl::Buffer  dev_c(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int));\n\n  // Pass arguments to device kernel\n  kernel_dot.setArg(0,  dev_a);\n  kernel_dot.setArg(1,  dev_b);\n  kernel_dot.setArg(2,  dev_c);\n\n  // We don't need to apply any offset to thread IDs\n  queue.enqueueNDRangeKernel(kernel_dot,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(dev_c,  CL_TRUE,  0,  n  *  sizeof(int),  c.data());\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints\n  auto  a_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  b_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  c_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n\n  // Initialize values\n  // We should use curly braces to limit host accessors' lifetime\n  //    and indicate when we're done working with them:\n  {\n  auto  a_host_acc  =  a_buf.get_host_access();\n  auto  b_host_acc  =  b_buf.get_host_access();\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a_host_acc[i]  =  i;\n  b_host_acc[i]  =  1;\n  }\n  }\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create read accessors over a_buf and b_buf\n  auto  a_acc  =  a_buf.get_access<sycl::access_mode::read>(cgh);\n  auto  b_acc  =  b_buf.get_access<sycl::access_mode::read>(cgh);\n  // Create write accesor over c_buf\n  auto  c_acc  =  c_buf.get_access<sycl::access_mode::write>(cgh);\n  // Run element-wise multiplication on device\n  cgh.parallel_for<class  vec_add>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c_acc[i]  =  a_acc[i]  *  b_acc[i];\n  });\n  });\n\n  // No need to synchronize, creating the accessor for c_buf will do it\n  // automatically\n  {\n  const  auto  c_host_acc  =  c_buf.get_host_access();\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c_host_acc[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // Allocate memory on the device and inherit the extents from h_a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // allocate memory on the device and inherit the extents from a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(nx  *  sizeof(int));\n\n  // Create 'n' execution space instances (maps to streams in CUDA/HIP)\n  auto  ex  =  Kokkos::Experimental::partition_space(\n  Kokkos::DefaultExecutionSpace(),  1,  1,  1,  1,  1);\n\n  // Launch 'n' potentially asynchronous kernels\n  // Each kernel has their own execution space instances\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  Kokkos::parallel_for(\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(\n  ex[region],  nx  /  n  *  region,  nx  /  n  *  (region  +  1)),\n  KOKKOS_LAMBDA(const  int  i)  {  a[i]  =  region  +  i;  });\n  }\n\n  // Sync execution space instances (maps to streams in CUDA/HIP)\n  for  (unsigned  region  =  0;  region  <  n;  region++)\n  ex[region].fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 200\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =  \"              \\\n __kernel void async(__global int *a) { \\\n int i = get_global_id(0);            \\\n int region = i / get_global_size(0); \\\n a[i] = region + i;                   \\\n }                                      \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"async\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  nx  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  size_t  offset  =  (nx  /  n)  *  region;\n  size_t  size  =  nx  /  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  &offset,  &size,  NULL,  0,  NULL,\n  NULL);\n  }\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  a,  nx  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(nx,  q);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  const  int  iShifted  =  i  +  nx  /  n  *  region;\n  a[iShifted]  =  region  +  iShifted;\n  });\n  }\n\n  // Synchronize\n  q.wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  unsigned  nPerRegion  =  nx  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  ap::onHost::iota<int>(queues[region],  regionOffset,\n  a.getSubView(regionOffset,  nx  -  regionOffset));\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  IdxAssignKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  a,\n  unsigned  region,\n  unsigned  n)  const  {\n  unsigned  nPerRegion  =  a.getExtents().x()  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  for  (auto  [idx]  :\n  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{regionOffset,  regionOffset  +  nPerRegion}))  {\n  a[idx]  =  idx;\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(nx,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues[region].enqueue(\n  frameSpec,  ap::KernelBundle{IdxAssignKernel{},  a,  region,  n});\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <cstdio>\n#include  <execution>\n#include  <numeric>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  10;\n\n  std::vector<int>  a(n);\n\n  std::iota(a.begin(),  a.end(),  0);  // Fill the array\n\n  // Run reduction on the device\n  int  sum  =  std::reduce(std::execution::par_unseq,  a.cbegin(),  a.cend(),  0,\n  std::plus<int>{});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Run sum reduction kernel\n  Kokkos::parallel_reduce(\n  n,  KOKKOS_LAMBDA(const  int  i,  int  &lsum)  {  lsum  +=  i;  },  sum);\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void reduce(__global int* sum, __local int* local_mem) {\n\n // Get work group and work item information\n int gsize = get_global_size(0); // global work size\n int gid = get_global_id(0); // global work item index\n int lsize = get_local_size(0); // local work size\n int lid = get_local_id(0); // local work item index\n\n // Store reduced item into local memory\n local_mem[lid] = gid; // initialize local memory\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n\n // Perform reduction across the local work group\n for (int s = 1; s < lsize; s *= 2) { // loop over local memory with stride doubling each iteration\n if (lid % (2 * s) == 0 && (lid + s) < lsize) {\n local_mem[lid] += local_mem[lid + s];\n }\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n }\n\n if (lid == 0) { // only one work item per work group\n atomic_add(sum, local_mem[0]); // add partial sum to global sum atomically\n }\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_reduce(program,  \"reduce\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Create buffer for sum\n  cl::Buffer  buffer(context,  CL_MEM_READ_WRITE  |  CL_MEM_COPY_HOST_PTR,\n  sizeof(int),  &sum);\n\n  // Pass arguments to device kernel\n  kernel_reduce.setArg(0,  buffer);  // pass buffer to device\n  kernel_reduce.setArg(1,  sizeof(int),  NULL);  // allocate local memory\n\n  // Enqueue kernel\n  queue.enqueueNDRangeKernel(kernel_reduce,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(buffer,  CL_TRUE,  0,  sizeof(int),  &sum);\n\n  // Print result\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  return  0;\n} \n```", "```\n// We use built-in sycl::reduction mechanism in this example.\n// The manual implementation of the reduction kernel can be found in\n// the \"Non-portable kernel models\" chapter.\n\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  sycl::queue  q;\n  unsigned  n  =  10;\n\n  // Initialize sum\n  int  sum  =  0;\n  {\n  // Create a buffer for sum to get the reduction results\n  sycl::buffer<int>  sum_buf{&sum,  1};\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create temporary object describing variables with reduction semantics\n  auto  sum_acc  =  sum_buf.get_access<sycl::access_mode::read_write>(cgh);\n  // We can use built-in reduction primitive\n  auto  sum_reduction  =  sycl::reduction(sum_acc,  sycl::plus<int>());\n\n  // A reference to the reducer is passed to the lambda\n  cgh.parallel_for(\n  sycl::range<1>{n},  sum_reduction,\n  [=](sycl::id<1>  idx,  auto  &reducer)  {  reducer.combine(idx[0]);  });\n  }).wait();\n  // The contents of sum_buf are copied back to sum by the destructor of\n  // sum_buf\n  }\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  10;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  sum  =  ap::onHost::allocUnified<int>(devAcc,  1);\n\n  // Run element-wise multiplication on device\n  ap::onHost::reduce(queue,  0,  sum,  std::plus{},  ap::LinearizedIdxGenerator{n});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum[0]);\n\n  return  0;\n} \n```", "```\n#include  <algorithm>\n#include  <vector>\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(a.begin(),  a.end());\n} \n```", "```\n#include  <algorithm>\n#include  <vector>\n#include  <execution> // To get std::execution\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(\n  std::execution::par_unseq,  // This algorithm can be run in parallel\n  a.begin(),  a.end()\n  );\n} \n```", "```\ndefault:  build\n\n# Set compiler\nKOKKOS_PATH  =  $(shell  pwd)/kokkos\nCXX  =  hipcc\n# CXX = ${KOKKOS_PATH}/bin/nvcc_wrapper\n\n# Variables for the Makefile.kokkos\nKOKKOS_DEVICES  =  \"HIP\"\n# KOKKOS_DEVICES = \"Cuda\"\nKOKKOS_ARCH  =  \"VEGA90A\"\n# KOKKOS_ARCH = \"Volta70\"\nKOKKOS_CUDA_OPTIONS  =  \"enable_lambda,force_uvm\"\n\n# Include Makefile.kokkos\ninclude $(KOKKOS_PATH)/Makefile.kokkos\n\nbuild:  $(KOKKOS_LINK_DEPENDS) $(KOKKOS_CPP_DEPENDS) hello.cpp\n  $(CXX)  $(KOKKOS_CPPFLAGS)  $(KOKKOS_CXXFLAGS)  $(KOKKOS_LDFLAGS)  hello.cpp  $(KOKKOS_LIBS)  -o  hello \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main(int  argc,  char*  argv[])  {\n  Kokkos::initialize(argc,  argv);\n  std::cout  <<  \"Execution Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace).name()  <<  std::endl;\n  std::cout  <<  \"Memory Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace::memory_space).name()  <<  std::endl;\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  PrgEnv-cray-amd\n$ CC  program.cpp  -lOpenCL  -o  program  # C++ program\n$ cc  program.c  -lOpenCL  -o  program  # C program \n```", "```\n// Initialize OpenCL\ncl::Device  device  =  cl::Device::getDefault();\ncl::Context  context(device);\ncl::CommandQueue  queue(context,  device); \n```", "```\n// Initialize OpenCL\ncl_int  err;  // Error code returned by API calls\ncl_platform_id  platform;\nerr  =  clGetPlatformIDs(1,  &platform,  NULL);\nassert(err  ==  CL_SUCCESS);  // Checking error codes is skipped later for brevity\ncl_device_id  device;\nerr  =  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\ncl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  &err);\ncl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  &err); \n```", "```\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global int *a) {\n int i = get_global_id(0);\n a[i] = i;\n }\n)\"; \n```", "```\ncl::Program  program(context,  kernel_source);\nprogram.build({device});\ncl::Kernel  kernel_dot(program,  \"dot\"); \n```", "```\ncl_int  err;\ncl_program  program  =  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  &err);\nerr  =  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\ncl_kernel  kernel_dot  =  clCreateKernel(program,  \"vector_add\",  &err); \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  acpp/24.06.0 \n```", "```\nint  main()  {\n  // Create an out-of-order queue on the default device:\n  sycl::queue  q;\n  // Now we can submit tasks to q!\n} \n```", "```\n// Iterate over all available devices\nfor  (const  auto  &device  :  sycl::device::get_devices())  {\n  // Print the device name\n  std::cout  <<  \"Creating a queue on \"  <<  device.get_info<sycl::info::device::name>()  <<  \"\\n\";\n  // Create an in-order queue for the current device\n  sycl::queue  q(device,  {sycl::property::queue::in_order()});\n  // Now we can submit tasks to q!\n} \n```", "```\n// Create a buffer of n integers\nauto  buf  =  sycl::buffer<int>(sycl::range<1>(n));\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Create write-only accessor for buf\n  auto  acc  =  buf.get_access<sycl::access_mode::write>(cgh);\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is written to the buffer via acc\n  acc[i]  =  /*...*/\n  });\n});\n/* If we now submit another kernel with accessor to buf, it will not\n * start running until the kernel above is done */ \n```", "```\n// Create a shared (migratable) allocation of n integers\n// Unlike with buffers, we need to specify a queue (or, explicitly, a device and a context)\nint*  v  =  sycl::malloc_shared<int>(n,  q);\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is directly written to v\n  v[i]  =  /*...*/\n  });\n});\n// If we want to access v, we have to ensure that the kernel has finished\nq.wait();\n// After we're done, the memory must be deallocated\nsycl::free(v,  q); \n```", "```\n$ salloc  -A  project_465002387  -N  1  -t  1:00:00  -p  standard-g  --gpus-per-node=1\n....\nsalloc: Granted job allocation 123456\n\n$ module  load  LUMI/24.03  partition/G\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  rocm/6.0.3  acpp/24.06.0 \n```", "```\n> $ srun  acpp-info  -l\n> =================Backend information===================\n> Loaded backend 0: HIP\n>  Found device: AMD Instinct MI250X\n> Loaded backend 1: OpenMP\n>  Found device: hipSYCL OpenMP host device \n> ```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nint  main()  {\n  // Create an in-order queue\n  sycl::queue  q{sycl::property::queue::in_order()};\n  // Print the device name, just for fun\n  std::cout  <<  \"Running on \"\n  <<  q.get_device().get_info<sycl::info::device::name>()  <<  std::endl;\n  const  int  n  =  1024;  // Vector size\n\n  // Allocate device and host memory for the first input vector\n  float  *d_x  =  sycl::malloc_device<float>(n,  q);\n  float  *h_x  =  sycl::malloc_host<float>(n,  q);\n // Bonus question: Can we use `std::vector` here instead of `malloc_host`? // TODO: Allocate second input vector on device and host, d_y and h_y  // Allocate device and host memory for the output vector\n  float  *d_z  =  sycl::malloc_device<float>(n,  q);\n  float  *h_z  =  sycl::malloc_host<float>(n,  q);\n\n  // Initialize values on host\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  h_x[i]  =  i;\n // TODO: Initialize h_y somehow  }\n  const  float  alpha  =  0.42f;\n\n  q.copy<float>(h_x,  d_x,  n);\n // TODO: Copy h_y to d_y // Bonus question: Why don't we need to wait before using the data? \n  // Run the kernel\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n // TODO: Modify the code to compute z[i] = alpha * x[i] + y[i]  d_z[i]  =  alpha  *  d_x[i];\n  });\n\n // TODO: Copy d_z to h_z // TODO: Wait for the copy to complete \n  // Check the results\n  bool  ok  =  true;\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  float  ref  =  alpha  *  h_x[i]  +  h_y[i];  // Reference value\n  float  tol  =  1e-5;  // Relative tolerance\n  if  (std::abs((h_z[i]  -  ref))  >  tol  *  std::abs(ref))  {\n  std::cout  <<  i  <<  \" \"  <<  h_z[i]  <<  \" \"  <<  h_x[i]  <<  \" \"  <<  h_y[i]\n  <<  std::endl;\n  ok  =  false;\n  break;\n  }\n  }\n  if  (ok)\n  std::cout  <<  \"Results are correct!\"  <<  std::endl;\n  else\n  std::cout  <<  \"Results are NOT correct!\"  <<  std::endl;\n\n  // Free allocated memory\n  sycl::free(d_x,  q);\n  sycl::free(h_x,  q);\n // TODO: Free d_y, h_y.  sycl::free(d_y,  q);\n  sycl::free(h_y,  q);\n\n  return  0;\n} \n```", "```\n$ acpp  -O3  exercise-sycl-saxpy.cpp  -o  exercise-sycl-saxpy\n$ srun  ./exercise-sycl-saxpy\nRunning on AMD Instinct MI250X\nResults are correct! \n```", "```\n    git  clone  https://github.com/alpaka-group/alpaka3.git\n    cd  alpaka \n    ```", "```\n    export  ALPAKA_DIR=/path/to/your/alpaka/install/dir \n    ```", "```\n    mkdir  build\n    cmake  -B  build  -S  .  -DCMAKE_INSTALL_PREFIX=$ALPAKA_DIR\n    cmake  --build  build  --parallel \n    ```", "```\n    export  CMAKE_PREFIX_PATH=$ALPAKA_DIR:$CMAKE_PREFIX_PATH \n    ```", "```\n> cmake_minimum_required(VERSION  3.25)\n> project(myAlpakaApp  VERSION  1.0)\n> \n> # Find installed alpaka\n> find_package(alpaka  REQUIRED)\n> \n> # Build the executable\n> add_executable(myAlpakaApp  main.cpp)\n> target_link_libraries(myAlpakaApp  PRIVATE  alpaka::alpaka)\n> alpaka_finalize(myAlpakaApp) \n> ```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n> #include  <alpaka/alpaka.hpp>\n> #include  <cstdlib>\n> #include  <iostream>\n> \n> namespace  ap  =  alpaka;\n> \n> auto  getDeviceSpec()\n> {\n>   /* Select a device, possible combinations of api+deviceKind:\n>  * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n>  * oneApi+amdGpu, oneApi+nvidiaGpu\n>  */\n>   return  ap::onHost::DeviceSpec{ap::api::hip,  ap::deviceKind::amdGpu};\n> }\n> \n> int  main(int  argc,  char**  argv)\n> {\n>   // Initialize device specification and selector\n>   ap::onHost::DeviceSpec  devSpec  =  getDeviceSpec();\n>   auto  deviceSelector  =  ap::onHost::makeDeviceSelector(devSpec);\n> \n>   // Query available devices\n>   auto  num_devices  =  deviceSelector.getDeviceCount();\n>   std::cout  <<  \"Number of available devices: \"  <<  num_devices  <<  \"\\n\";\n> \n>   if  (num_devices  ==  0)  {\n>   std::cerr  <<  \"No devices found for the selected backend\\n\";\n>   return  EXIT_FAILURE;\n>   }\n> \n>   // Select and initialize the first device\n>   auto  device  =  deviceSelector.makeDevice(0);\n>   std::cout  <<  \"Using device: \"  <<  device.getName()  <<  \"\\n\";\n> \n>   return  EXIT_SUCCESS;\n> } \n> ```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  myProject\n{\n  namespace  ap  =  alpaka;\n  // Your code here\n} \n```", "```\nauto  devSelector  =  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\nif  (devSelector.getDeviceCount()  ==  0)\n{\n  throw  std::runtime_error(\"No device found!\");\n}\nauto  device  =  devSelector.makeDevice(0); \n```", "```\nauto  queue  =  device.makeQueue();\nauto  nonBlockingQueue  =  device.makeQueue(ap::queueKind::nonBlocking);\nauto  blockingQueue  =  device.makeQueue(ap::queueKind::blocking);\n\nauto  event  =  device.makeEvent();\nqueue.enqueue(event);\nap::onHost::wait(event);\nap::onHost::wait(queue); \n```", "```\nauto  hostBuffer  =  ap::onHost::allocHost<DataType>(extent3D);\nauto  devBuffer  =  ap::onHost::alloc<DataType>(device,  extentMd);\nauto  devMappedBuffer  =  ap::onHost::allocMapped<DataType>(device,  extentMd);\n\nauto  hostView  =  ap::makeView(api::host,  externPtr,  ap::Vec{numElements});\nauto  devNonOwningView  =  devBuffer.getView();\n\nap::onHost::memset(queue,  devBuffer,  uint8_t{0});\nap::onHost::memcpy(queue,  devBuffer,  hostBuffer);\nap::onHost::fill(queue,  devBuffer,  DataType{42}); \n```", "```\nconstexpr  uint32_t  dim  =  2u;\nusing  IdxType  =  size_t;\nusing  DataType  =  int;\n\nIdxType  valueX,  valueY;\nauto  extentMD  =  ap::Vec{valueY,  valueX};\n\nauto  frameSpec  =  ap::onHost::FrameSpec{numFramesMd,  frameExtentMd};\nauto  tunedSpec  =  ap::onHost::getFrameSpec<DataType>(device,  extentMd);\n\nqueue.enqueue(tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...});\n\nauto  executor  =  ap::exec::cpuSerial;\nqueue.enqueue(executor,  tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...}); \n```", "```\nstruct  MyKernel\n{\n  ALPAKA_FN_ACC  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,  auto...  args)  const\n  {\n  auto  idxMd  =  acc.getIdxWithin(ap::onAcc::origin::grid,  ap::onAcc::unit::blocks);\n\n  auto  sharedMdArray  =\n  ap::onAcc::declareSharedMdArray<float,  ap::uniqueId()>(acc,  ap::CVec<uint32_t,  3,  4>{});\n\n  ap::onAcc::syncBlockThreads(acc);\n  auto  old  =  onAcc::atomicAdd(acc,  args...);\n  ap::onAcc::memFence(acc,  ap::onAcc::scope::block);\n  auto  sinValue  =  ap::math::sin(args[0]);\n  }\n}; \n```", "```\n    mkdir  my_alpaka_project  &&  cd  my_alpaka_project \n    ```", "```\n    cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\n    cmake  --build  build  --parallel \n    ```", "```\n    ./build/myAlpakaApp \n    ```", "```\nNumber of available devices: 1\nUsing device: [Device Name] \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::hip, ap::deviceKind::amdGpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  -x  hip  --offload-arch=gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host, ap::deviceKind::cpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::cuda, ap::deviceKind::nvidiaGpu);\nnvcc  -I  $ALPAKA_DIR/include/  -std=c++20  --expt-relaxed-constexpr  -x  cuda  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::cpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64_x86_64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::intelGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::amdGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=amd_gpu_gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::nvidiaGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=nvptx64-nvidia-cuda  -Xsycl-target-backend=nvptx64-nvidia-cuda  --offload-arch=sm_80  main.cpp\n./a.out \n```", "```\n$ srun  -p  dev-g  --gpus  1  -N  1  -n  1  --time=00:20:00  --account=project_465002387  --pty  bash\n....\nsrun: job 1234 queued and waiting for resources\nsrun: job 1234 has been allocated resources\n\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n$ rocm-smi\n\n======================================= ROCm System Management Interface =======================================\n================================================= Concise Info =================================================\nDevice  [Model : Revision]    Temp    Power  Partitions      SCLK    MCLK     Fan  Perf    PwrCap  VRAM%  GPU%\n Name (20 chars)       (Edge)  (Avg)  (Mem, Compute)\n================================================================================================================\n0       [0x0b0c : 0x00]       45.0Â°C  N/A    N/A, N/A        800Mhz  1600Mhz  0%   manual  0.0W      0%   0%\n AMD INSTINCT MI200 (\n================================================================================================================\n============================================= End of ROCm SMI Log ============================================== \n```", "```\ncmake_minimum_required(VERSION  3.25)\nproject(vectorAdd  LANGUAGES  CXX  VERSION  1.0)\n#Use CMake's FetchContent to download and integrate alpaka3 directly from GitHub\ninclude(FetchContent)\n#Declare where to fetch alpaka3 from\n#This will download the library at configure time\nFetchContent_Declare(alpaka3  GIT_REPOSITORY  https://github.com/alpaka-group/alpaka3.git  GIT_TAG  dev)\n#Make alpaka3 available for use in this project\n#This downloads, configures, and makes the library targets available\nFetchContent_MakeAvailable(alpaka3)\n#Finalize the alpaka FetchContent setup\nalpaka_FetchContent_Finalize() #Create the executable target from the source file\nadd_executable(vectorAdd  main.cpp)\n#Link the alpaka library to the executable\ntarget_link_libraries(vectorAdd  PRIVATE  alpaka::alpaka)\n#Finalize the alpaka configuration for this target\n#This sets up backend - specific compiler flags and dependencies\nalpaka_finalize(vectorAdd) \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise vector addition on device\n ap::onHost::transform(queue,  c,  std::plus{},  a,  b); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n$ mkdir  alpakaExercise  &&  cd  alpakaExercise\n$ vim  CMakeLists.txt\nand now paste the CMakeLsits here (Press i, followed by Ctrl+Shift+V)\nPress esc and then :wq to exit vim\n$ vim  main.cpp\nSimilarly, paste the C++ code here \n```", "```\nconfigure step, we additionaly specify that HIP is available\n$ cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\nbuild\n$ cmake  --build  build  --parallel\nrun\n$ ./build/vectorAdd\nUsing alpaka device: AMD Instinct MI250X id=0\nc[0] = 1\nc[1] = 2\nc[2] = 3\nc[3] = 4\nc[4] = 5 \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  AddKernel  {\n constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const  &acc, ap::concepts::IMdSpan  auto  c, ap::concepts::IMdSpan  auto  const  a, ap::concepts::IMdSpan  auto  const  b)  const  { for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid, ap::IdxRange{c.getExtents()}))  { c[idx]  =  a[idx]  +  b[idx]; } } }; \nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n auto  frameSpec  =  ap::onHost::getFrameSpec<int>(devAcc,  c.getExtents()); \n // Call the element-wise addition kernel on device queue.enqueue(frameSpec,  ap::KernelBundle{AddKernel{},  c,  a,  b}); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <algorithm>\n#include  <cstdio>\n#include  <execution>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  5;\n\n  // Allocate arrays\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  std::transform(std::execution::par_unseq,  a.begin(),  a.end(),  b.begin(),\n  c.begin(),  [](int  i,  int  j)  {  return  i  *  j;  });\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *b  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *c  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  Kokkos::kokkos_free(b);\n  Kokkos::kokkos_free(c);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 220\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =\n  \"                                                 \\\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) { \\\n int i = get_global_id(0);                                                        \\\n c[i] = a[i] * b[i];                                                              \\\n }                                                                                  \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"dot\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *b  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *c  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n  clSetKernelArgSVMPointer(kernel,  1,  b);\n  clSetKernelArgSVMPointer(kernel,  2,  c);\n\n  // Create mappings for host and initialize values\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  a,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  b,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n  clEnqueueSVMUnmap(queue,  b,  0,  NULL,  NULL);\n\n  size_t  globalSize  =  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  c,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  clEnqueueSVMUnmap(queue,  c,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n  clSVMFree(context,  b);\n  clSVMFree(context,  c);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(n,  q);\n  int  *b  =  sycl::malloc_shared<int>(n,  q);\n  int  *c  =  sycl::malloc_shared<int>(n,  q);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c[i]  =  a[i]  *  b[i];\n  }).wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n  sycl::free(b,  q);\n  sycl::free(c,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints on Kokkos host memory space\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_a(\"h_a\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_b(\"h_b\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_c(\"h_c\",  n);\n\n  // Allocate space for 5 ints on Kokkos default memory space (eg, GPU memory)\n  Kokkos::View<int  *>  a(\"a\",  n);\n  Kokkos::View<int  *>  b(\"b\",  n);\n  Kokkos::View<int  *>  c(\"c\",  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy from host to device\n  Kokkos::deep_copy(a,  h_a);\n  Kokkos::deep_copy(b,  h_b);\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Copy from device to host\n  Kokkos::deep_copy(h_c,  c);\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) {\n int i = get_global_id(0);\n c[i] = a[i] * b[i];\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device.\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_dot(program,  \"dot\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Create buffers and copy input data to device.\n  cl::Buffer  dev_a(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  a.data());\n  cl::Buffer  dev_b(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  b.data());\n  cl::Buffer  dev_c(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int));\n\n  // Pass arguments to device kernel\n  kernel_dot.setArg(0,  dev_a);\n  kernel_dot.setArg(1,  dev_b);\n  kernel_dot.setArg(2,  dev_c);\n\n  // We don't need to apply any offset to thread IDs\n  queue.enqueueNDRangeKernel(kernel_dot,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(dev_c,  CL_TRUE,  0,  n  *  sizeof(int),  c.data());\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints\n  auto  a_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  b_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  c_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n\n  // Initialize values\n  // We should use curly braces to limit host accessors' lifetime\n  //    and indicate when we're done working with them:\n  {\n  auto  a_host_acc  =  a_buf.get_host_access();\n  auto  b_host_acc  =  b_buf.get_host_access();\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a_host_acc[i]  =  i;\n  b_host_acc[i]  =  1;\n  }\n  }\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create read accessors over a_buf and b_buf\n  auto  a_acc  =  a_buf.get_access<sycl::access_mode::read>(cgh);\n  auto  b_acc  =  b_buf.get_access<sycl::access_mode::read>(cgh);\n  // Create write accesor over c_buf\n  auto  c_acc  =  c_buf.get_access<sycl::access_mode::write>(cgh);\n  // Run element-wise multiplication on device\n  cgh.parallel_for<class  vec_add>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c_acc[i]  =  a_acc[i]  *  b_acc[i];\n  });\n  });\n\n  // No need to synchronize, creating the accessor for c_buf will do it\n  // automatically\n  {\n  const  auto  c_host_acc  =  c_buf.get_host_access();\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c_host_acc[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // Allocate memory on the device and inherit the extents from h_a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // allocate memory on the device and inherit the extents from a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(nx  *  sizeof(int));\n\n  // Create 'n' execution space instances (maps to streams in CUDA/HIP)\n  auto  ex  =  Kokkos::Experimental::partition_space(\n  Kokkos::DefaultExecutionSpace(),  1,  1,  1,  1,  1);\n\n  // Launch 'n' potentially asynchronous kernels\n  // Each kernel has their own execution space instances\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  Kokkos::parallel_for(\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(\n  ex[region],  nx  /  n  *  region,  nx  /  n  *  (region  +  1)),\n  KOKKOS_LAMBDA(const  int  i)  {  a[i]  =  region  +  i;  });\n  }\n\n  // Sync execution space instances (maps to streams in CUDA/HIP)\n  for  (unsigned  region  =  0;  region  <  n;  region++)\n  ex[region].fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 200\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =  \"              \\\n __kernel void async(__global int *a) { \\\n int i = get_global_id(0);            \\\n int region = i / get_global_size(0); \\\n a[i] = region + i;                   \\\n }                                      \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"async\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  nx  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  size_t  offset  =  (nx  /  n)  *  region;\n  size_t  size  =  nx  /  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  &offset,  &size,  NULL,  0,  NULL,\n  NULL);\n  }\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  a,  nx  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(nx,  q);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  const  int  iShifted  =  i  +  nx  /  n  *  region;\n  a[iShifted]  =  region  +  iShifted;\n  });\n  }\n\n  // Synchronize\n  q.wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  unsigned  nPerRegion  =  nx  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  ap::onHost::iota<int>(queues[region],  regionOffset,\n  a.getSubView(regionOffset,  nx  -  regionOffset));\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  IdxAssignKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  a,\n  unsigned  region,\n  unsigned  n)  const  {\n  unsigned  nPerRegion  =  a.getExtents().x()  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  for  (auto  [idx]  :\n  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{regionOffset,  regionOffset  +  nPerRegion}))  {\n  a[idx]  =  idx;\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(nx,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues[region].enqueue(\n  frameSpec,  ap::KernelBundle{IdxAssignKernel{},  a,  region,  n});\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <cstdio>\n#include  <execution>\n#include  <numeric>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  10;\n\n  std::vector<int>  a(n);\n\n  std::iota(a.begin(),  a.end(),  0);  // Fill the array\n\n  // Run reduction on the device\n  int  sum  =  std::reduce(std::execution::par_unseq,  a.cbegin(),  a.cend(),  0,\n  std::plus<int>{});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Run sum reduction kernel\n  Kokkos::parallel_reduce(\n  n,  KOKKOS_LAMBDA(const  int  i,  int  &lsum)  {  lsum  +=  i;  },  sum);\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void reduce(__global int* sum, __local int* local_mem) {\n\n // Get work group and work item information\n int gsize = get_global_size(0); // global work size\n int gid = get_global_id(0); // global work item index\n int lsize = get_local_size(0); // local work size\n int lid = get_local_id(0); // local work item index\n\n // Store reduced item into local memory\n local_mem[lid] = gid; // initialize local memory\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n\n // Perform reduction across the local work group\n for (int s = 1; s < lsize; s *= 2) { // loop over local memory with stride doubling each iteration\n if (lid % (2 * s) == 0 && (lid + s) < lsize) {\n local_mem[lid] += local_mem[lid + s];\n }\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n }\n\n if (lid == 0) { // only one work item per work group\n atomic_add(sum, local_mem[0]); // add partial sum to global sum atomically\n }\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_reduce(program,  \"reduce\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Create buffer for sum\n  cl::Buffer  buffer(context,  CL_MEM_READ_WRITE  |  CL_MEM_COPY_HOST_PTR,\n  sizeof(int),  &sum);\n\n  // Pass arguments to device kernel\n  kernel_reduce.setArg(0,  buffer);  // pass buffer to device\n  kernel_reduce.setArg(1,  sizeof(int),  NULL);  // allocate local memory\n\n  // Enqueue kernel\n  queue.enqueueNDRangeKernel(kernel_reduce,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(buffer,  CL_TRUE,  0,  sizeof(int),  &sum);\n\n  // Print result\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  return  0;\n} \n```", "```\n// We use built-in sycl::reduction mechanism in this example.\n// The manual implementation of the reduction kernel can be found in\n// the \"Non-portable kernel models\" chapter.\n\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  sycl::queue  q;\n  unsigned  n  =  10;\n\n  // Initialize sum\n  int  sum  =  0;\n  {\n  // Create a buffer for sum to get the reduction results\n  sycl::buffer<int>  sum_buf{&sum,  1};\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create temporary object describing variables with reduction semantics\n  auto  sum_acc  =  sum_buf.get_access<sycl::access_mode::read_write>(cgh);\n  // We can use built-in reduction primitive\n  auto  sum_reduction  =  sycl::reduction(sum_acc,  sycl::plus<int>());\n\n  // A reference to the reducer is passed to the lambda\n  cgh.parallel_for(\n  sycl::range<1>{n},  sum_reduction,\n  [=](sycl::id<1>  idx,  auto  &reducer)  {  reducer.combine(idx[0]);  });\n  }).wait();\n  // The contents of sum_buf are copied back to sum by the destructor of\n  // sum_buf\n  }\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  10;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  sum  =  ap::onHost::allocUnified<int>(devAcc,  1);\n\n  // Run element-wise multiplication on device\n  ap::onHost::reduce(queue,  0,  sum,  std::plus{},  ap::LinearizedIdxGenerator{n});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum[0]);\n\n  return  0;\n} \n```", "```\n#include  <algorithm>\n#include  <vector>\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(a.begin(),  a.end());\n} \n```", "```\n#include  <algorithm>\n#include  <vector>\n#include  <execution> // To get std::execution\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(\n  std::execution::par_unseq,  // This algorithm can be run in parallel\n  a.begin(),  a.end()\n  );\n} \n```", "```\n#include  <algorithm>\n#include  <vector>\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(a.begin(),  a.end());\n} \n```", "```\n#include  <algorithm>\n#include  <vector>\n#include  <execution> // To get std::execution\n\nvoid  f(std::vector<int>&  a)  {\n  std::sort(\n  std::execution::par_unseq,  // This algorithm can be run in parallel\n  a.begin(),  a.end()\n  );\n} \n```", "```\ndefault:  build\n\n# Set compiler\nKOKKOS_PATH  =  $(shell  pwd)/kokkos\nCXX  =  hipcc\n# CXX = ${KOKKOS_PATH}/bin/nvcc_wrapper\n\n# Variables for the Makefile.kokkos\nKOKKOS_DEVICES  =  \"HIP\"\n# KOKKOS_DEVICES = \"Cuda\"\nKOKKOS_ARCH  =  \"VEGA90A\"\n# KOKKOS_ARCH = \"Volta70\"\nKOKKOS_CUDA_OPTIONS  =  \"enable_lambda,force_uvm\"\n\n# Include Makefile.kokkos\ninclude $(KOKKOS_PATH)/Makefile.kokkos\n\nbuild:  $(KOKKOS_LINK_DEPENDS) $(KOKKOS_CPP_DEPENDS) hello.cpp\n  $(CXX)  $(KOKKOS_CPPFLAGS)  $(KOKKOS_CXXFLAGS)  $(KOKKOS_LDFLAGS)  hello.cpp  $(KOKKOS_LIBS)  -o  hello \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main(int  argc,  char*  argv[])  {\n  Kokkos::initialize(argc,  argv);\n  std::cout  <<  \"Execution Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace).name()  <<  std::endl;\n  std::cout  <<  \"Memory Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace::memory_space).name()  <<  std::endl;\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\ndefault:  build\n\n# Set compiler\nKOKKOS_PATH  =  $(shell  pwd)/kokkos\nCXX  =  hipcc\n# CXX = ${KOKKOS_PATH}/bin/nvcc_wrapper\n\n# Variables for the Makefile.kokkos\nKOKKOS_DEVICES  =  \"HIP\"\n# KOKKOS_DEVICES = \"Cuda\"\nKOKKOS_ARCH  =  \"VEGA90A\"\n# KOKKOS_ARCH = \"Volta70\"\nKOKKOS_CUDA_OPTIONS  =  \"enable_lambda,force_uvm\"\n\n# Include Makefile.kokkos\ninclude $(KOKKOS_PATH)/Makefile.kokkos\n\nbuild:  $(KOKKOS_LINK_DEPENDS) $(KOKKOS_CPP_DEPENDS) hello.cpp\n  $(CXX)  $(KOKKOS_CPPFLAGS)  $(KOKKOS_CXXFLAGS)  $(KOKKOS_LDFLAGS)  hello.cpp  $(KOKKOS_LIBS)  -o  hello \n```", "```\n#include  <Kokkos_Core.hpp>\n#include  <iostream>\n\nint  main(int  argc,  char*  argv[])  {\n  Kokkos::initialize(argc,  argv);\n  std::cout  <<  \"Execution Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace).name()  <<  std::endl;\n  std::cout  <<  \"Memory Space: \"  <<\n  typeid(Kokkos::DefaultExecutionSpace::memory_space).name()  <<  std::endl;\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  PrgEnv-cray-amd\n$ CC  program.cpp  -lOpenCL  -o  program  # C++ program\n$ cc  program.c  -lOpenCL  -o  program  # C program \n```", "```\n// Initialize OpenCL\ncl::Device  device  =  cl::Device::getDefault();\ncl::Context  context(device);\ncl::CommandQueue  queue(context,  device); \n```", "```\n// Initialize OpenCL\ncl_int  err;  // Error code returned by API calls\ncl_platform_id  platform;\nerr  =  clGetPlatformIDs(1,  &platform,  NULL);\nassert(err  ==  CL_SUCCESS);  // Checking error codes is skipped later for brevity\ncl_device_id  device;\nerr  =  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\ncl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  &err);\ncl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  &err); \n```", "```\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global int *a) {\n int i = get_global_id(0);\n a[i] = i;\n }\n)\"; \n```", "```\ncl::Program  program(context,  kernel_source);\nprogram.build({device});\ncl::Kernel  kernel_dot(program,  \"dot\"); \n```", "```\ncl_int  err;\ncl_program  program  =  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  &err);\nerr  =  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\ncl_kernel  kernel_dot  =  clCreateKernel(program,  \"vector_add\",  &err); \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  PrgEnv-cray-amd\n$ CC  program.cpp  -lOpenCL  -o  program  # C++ program\n$ cc  program.c  -lOpenCL  -o  program  # C program \n```", "```\n// Initialize OpenCL\ncl::Device  device  =  cl::Device::getDefault();\ncl::Context  context(device);\ncl::CommandQueue  queue(context,  device); \n```", "```\n// Initialize OpenCL\ncl_int  err;  // Error code returned by API calls\ncl_platform_id  platform;\nerr  =  clGetPlatformIDs(1,  &platform,  NULL);\nassert(err  ==  CL_SUCCESS);  // Checking error codes is skipped later for brevity\ncl_device_id  device;\nerr  =  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\ncl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  &err);\ncl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  &err); \n```", "```\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global int *a) {\n int i = get_global_id(0);\n a[i] = i;\n }\n)\"; \n```", "```\ncl::Program  program(context,  kernel_source);\nprogram.build({device});\ncl::Kernel  kernel_dot(program,  \"dot\"); \n```", "```\ncl_int  err;\ncl_program  program  =  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  &err);\nerr  =  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\ncl_kernel  kernel_dot  =  clCreateKernel(program,  \"vector_add\",  &err); \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  acpp/24.06.0 \n```", "```\nint  main()  {\n  // Create an out-of-order queue on the default device:\n  sycl::queue  q;\n  // Now we can submit tasks to q!\n} \n```", "```\n// Iterate over all available devices\nfor  (const  auto  &device  :  sycl::device::get_devices())  {\n  // Print the device name\n  std::cout  <<  \"Creating a queue on \"  <<  device.get_info<sycl::info::device::name>()  <<  \"\\n\";\n  // Create an in-order queue for the current device\n  sycl::queue  q(device,  {sycl::property::queue::in_order()});\n  // Now we can submit tasks to q!\n} \n```", "```\n// Create a buffer of n integers\nauto  buf  =  sycl::buffer<int>(sycl::range<1>(n));\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Create write-only accessor for buf\n  auto  acc  =  buf.get_access<sycl::access_mode::write>(cgh);\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is written to the buffer via acc\n  acc[i]  =  /*...*/\n  });\n});\n/* If we now submit another kernel with accessor to buf, it will not\n * start running until the kernel above is done */ \n```", "```\n// Create a shared (migratable) allocation of n integers\n// Unlike with buffers, we need to specify a queue (or, explicitly, a device and a context)\nint*  v  =  sycl::malloc_shared<int>(n,  q);\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is directly written to v\n  v[i]  =  /*...*/\n  });\n});\n// If we want to access v, we have to ensure that the kernel has finished\nq.wait();\n// After we're done, the memory must be deallocated\nsycl::free(v,  q); \n```", "```\n$ salloc  -A  project_465002387  -N  1  -t  1:00:00  -p  standard-g  --gpus-per-node=1\n....\nsalloc: Granted job allocation 123456\n\n$ module  load  LUMI/24.03  partition/G\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  rocm/6.0.3  acpp/24.06.0 \n```", "```\n> $ srun  acpp-info  -l\n> =================Backend information===================\n> Loaded backend 0: HIP\n>  Found device: AMD Instinct MI250X\n> Loaded backend 1: OpenMP\n>  Found device: hipSYCL OpenMP host device \n> ```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nint  main()  {\n  // Create an in-order queue\n  sycl::queue  q{sycl::property::queue::in_order()};\n  // Print the device name, just for fun\n  std::cout  <<  \"Running on \"\n  <<  q.get_device().get_info<sycl::info::device::name>()  <<  std::endl;\n  const  int  n  =  1024;  // Vector size\n\n  // Allocate device and host memory for the first input vector\n  float  *d_x  =  sycl::malloc_device<float>(n,  q);\n  float  *h_x  =  sycl::malloc_host<float>(n,  q);\n // Bonus question: Can we use `std::vector` here instead of `malloc_host`? // TODO: Allocate second input vector on device and host, d_y and h_y  // Allocate device and host memory for the output vector\n  float  *d_z  =  sycl::malloc_device<float>(n,  q);\n  float  *h_z  =  sycl::malloc_host<float>(n,  q);\n\n  // Initialize values on host\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  h_x[i]  =  i;\n // TODO: Initialize h_y somehow  }\n  const  float  alpha  =  0.42f;\n\n  q.copy<float>(h_x,  d_x,  n);\n // TODO: Copy h_y to d_y // Bonus question: Why don't we need to wait before using the data? \n  // Run the kernel\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n // TODO: Modify the code to compute z[i] = alpha * x[i] + y[i]  d_z[i]  =  alpha  *  d_x[i];\n  });\n\n // TODO: Copy d_z to h_z // TODO: Wait for the copy to complete \n  // Check the results\n  bool  ok  =  true;\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  float  ref  =  alpha  *  h_x[i]  +  h_y[i];  // Reference value\n  float  tol  =  1e-5;  // Relative tolerance\n  if  (std::abs((h_z[i]  -  ref))  >  tol  *  std::abs(ref))  {\n  std::cout  <<  i  <<  \" \"  <<  h_z[i]  <<  \" \"  <<  h_x[i]  <<  \" \"  <<  h_y[i]\n  <<  std::endl;\n  ok  =  false;\n  break;\n  }\n  }\n  if  (ok)\n  std::cout  <<  \"Results are correct!\"  <<  std::endl;\n  else\n  std::cout  <<  \"Results are NOT correct!\"  <<  std::endl;\n\n  // Free allocated memory\n  sycl::free(d_x,  q);\n  sycl::free(h_x,  q);\n // TODO: Free d_y, h_y.  sycl::free(d_y,  q);\n  sycl::free(h_y,  q);\n\n  return  0;\n} \n```", "```\n$ acpp  -O3  exercise-sycl-saxpy.cpp  -o  exercise-sycl-saxpy\n$ srun  ./exercise-sycl-saxpy\nRunning on AMD Instinct MI250X\nResults are correct! \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  acpp/24.06.0 \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  acpp/24.06.0 \n```", "```\nint  main()  {\n  // Create an out-of-order queue on the default device:\n  sycl::queue  q;\n  // Now we can submit tasks to q!\n} \n```", "```\n// Iterate over all available devices\nfor  (const  auto  &device  :  sycl::device::get_devices())  {\n  // Print the device name\n  std::cout  <<  \"Creating a queue on \"  <<  device.get_info<sycl::info::device::name>()  <<  \"\\n\";\n  // Create an in-order queue for the current device\n  sycl::queue  q(device,  {sycl::property::queue::in_order()});\n  // Now we can submit tasks to q!\n} \n```", "```\n// Create a buffer of n integers\nauto  buf  =  sycl::buffer<int>(sycl::range<1>(n));\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Create write-only accessor for buf\n  auto  acc  =  buf.get_access<sycl::access_mode::write>(cgh);\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is written to the buffer via acc\n  acc[i]  =  /*...*/\n  });\n});\n/* If we now submit another kernel with accessor to buf, it will not\n * start running until the kernel above is done */ \n```", "```\n// Create a shared (migratable) allocation of n integers\n// Unlike with buffers, we need to specify a queue (or, explicitly, a device and a context)\nint*  v  =  sycl::malloc_shared<int>(n,  q);\n// Submit a kernel into a queue; cgh is a helper object\nq.submit([&](sycl::handler  &cgh)  {\n  // Define a kernel: n threads execute the following lambda\n  cgh.parallel_for<class  KernelName>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  // The data is directly written to v\n  v[i]  =  /*...*/\n  });\n});\n// If we want to access v, we have to ensure that the kernel has finished\nq.wait();\n// After we're done, the memory must be deallocated\nsycl::free(v,  q); \n```", "```\n$ salloc  -A  project_465002387  -N  1  -t  1:00:00  -p  standard-g  --gpus-per-node=1\n....\nsalloc: Granted job allocation 123456\n\n$ module  load  LUMI/24.03  partition/G\n$ module  use  /appl/local/csc/modulefiles\n$ module  load  rocm/6.0.3  acpp/24.06.0 \n```", "```\n> $ srun  acpp-info  -l\n> =================Backend information===================\n> Loaded backend 0: HIP\n>  Found device: AMD Instinct MI250X\n> Loaded backend 1: OpenMP\n>  Found device: hipSYCL OpenMP host device \n> ```", "```\n#include  <iostream>\n#include  <sycl/sycl.hpp>\n#include  <vector>\n\nint  main()  {\n  // Create an in-order queue\n  sycl::queue  q{sycl::property::queue::in_order()};\n  // Print the device name, just for fun\n  std::cout  <<  \"Running on \"\n  <<  q.get_device().get_info<sycl::info::device::name>()  <<  std::endl;\n  const  int  n  =  1024;  // Vector size\n\n  // Allocate device and host memory for the first input vector\n  float  *d_x  =  sycl::malloc_device<float>(n,  q);\n  float  *h_x  =  sycl::malloc_host<float>(n,  q);\n // Bonus question: Can we use `std::vector` here instead of `malloc_host`? // TODO: Allocate second input vector on device and host, d_y and h_y  // Allocate device and host memory for the output vector\n  float  *d_z  =  sycl::malloc_device<float>(n,  q);\n  float  *h_z  =  sycl::malloc_host<float>(n,  q);\n\n  // Initialize values on host\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  h_x[i]  =  i;\n // TODO: Initialize h_y somehow  }\n  const  float  alpha  =  0.42f;\n\n  q.copy<float>(h_x,  d_x,  n);\n // TODO: Copy h_y to d_y // Bonus question: Why don't we need to wait before using the data? \n  // Run the kernel\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n // TODO: Modify the code to compute z[i] = alpha * x[i] + y[i]  d_z[i]  =  alpha  *  d_x[i];\n  });\n\n // TODO: Copy d_z to h_z // TODO: Wait for the copy to complete \n  // Check the results\n  bool  ok  =  true;\n  for  (int  i  =  0;  i  <  n;  i++)  {\n  float  ref  =  alpha  *  h_x[i]  +  h_y[i];  // Reference value\n  float  tol  =  1e-5;  // Relative tolerance\n  if  (std::abs((h_z[i]  -  ref))  >  tol  *  std::abs(ref))  {\n  std::cout  <<  i  <<  \" \"  <<  h_z[i]  <<  \" \"  <<  h_x[i]  <<  \" \"  <<  h_y[i]\n  <<  std::endl;\n  ok  =  false;\n  break;\n  }\n  }\n  if  (ok)\n  std::cout  <<  \"Results are correct!\"  <<  std::endl;\n  else\n  std::cout  <<  \"Results are NOT correct!\"  <<  std::endl;\n\n  // Free allocated memory\n  sycl::free(d_x,  q);\n  sycl::free(h_x,  q);\n // TODO: Free d_y, h_y.  sycl::free(d_y,  q);\n  sycl::free(h_y,  q);\n\n  return  0;\n} \n```", "```\n$ acpp  -O3  exercise-sycl-saxpy.cpp  -o  exercise-sycl-saxpy\n$ srun  ./exercise-sycl-saxpy\nRunning on AMD Instinct MI250X\nResults are correct! \n```", "```\n    git  clone  https://github.com/alpaka-group/alpaka3.git\n    cd  alpaka \n    ```", "```\n    export  ALPAKA_DIR=/path/to/your/alpaka/install/dir \n    ```", "```\n    mkdir  build\n    cmake  -B  build  -S  .  -DCMAKE_INSTALL_PREFIX=$ALPAKA_DIR\n    cmake  --build  build  --parallel \n    ```", "```\n    export  CMAKE_PREFIX_PATH=$ALPAKA_DIR:$CMAKE_PREFIX_PATH \n    ```", "```\n> cmake_minimum_required(VERSION  3.25)\n> project(myAlpakaApp  VERSION  1.0)\n> \n> # Find installed alpaka\n> find_package(alpaka  REQUIRED)\n> \n> # Build the executable\n> add_executable(myAlpakaApp  main.cpp)\n> target_link_libraries(myAlpakaApp  PRIVATE  alpaka::alpaka)\n> alpaka_finalize(myAlpakaApp) \n> ```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n> #include  <alpaka/alpaka.hpp>\n> #include  <cstdlib>\n> #include  <iostream>\n> \n> namespace  ap  =  alpaka;\n> \n> auto  getDeviceSpec()\n> {\n>   /* Select a device, possible combinations of api+deviceKind:\n>  * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n>  * oneApi+amdGpu, oneApi+nvidiaGpu\n>  */\n>   return  ap::onHost::DeviceSpec{ap::api::hip,  ap::deviceKind::amdGpu};\n> }\n> \n> int  main(int  argc,  char**  argv)\n> {\n>   // Initialize device specification and selector\n>   ap::onHost::DeviceSpec  devSpec  =  getDeviceSpec();\n>   auto  deviceSelector  =  ap::onHost::makeDeviceSelector(devSpec);\n> \n>   // Query available devices\n>   auto  num_devices  =  deviceSelector.getDeviceCount();\n>   std::cout  <<  \"Number of available devices: \"  <<  num_devices  <<  \"\\n\";\n> \n>   if  (num_devices  ==  0)  {\n>   std::cerr  <<  \"No devices found for the selected backend\\n\";\n>   return  EXIT_FAILURE;\n>   }\n> \n>   // Select and initialize the first device\n>   auto  device  =  deviceSelector.makeDevice(0);\n>   std::cout  <<  \"Using device: \"  <<  device.getName()  <<  \"\\n\";\n> \n>   return  EXIT_SUCCESS;\n> } \n> ```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  myProject\n{\n  namespace  ap  =  alpaka;\n  // Your code here\n} \n```", "```\nauto  devSelector  =  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\nif  (devSelector.getDeviceCount()  ==  0)\n{\n  throw  std::runtime_error(\"No device found!\");\n}\nauto  device  =  devSelector.makeDevice(0); \n```", "```\nauto  queue  =  device.makeQueue();\nauto  nonBlockingQueue  =  device.makeQueue(ap::queueKind::nonBlocking);\nauto  blockingQueue  =  device.makeQueue(ap::queueKind::blocking);\n\nauto  event  =  device.makeEvent();\nqueue.enqueue(event);\nap::onHost::wait(event);\nap::onHost::wait(queue); \n```", "```\nauto  hostBuffer  =  ap::onHost::allocHost<DataType>(extent3D);\nauto  devBuffer  =  ap::onHost::alloc<DataType>(device,  extentMd);\nauto  devMappedBuffer  =  ap::onHost::allocMapped<DataType>(device,  extentMd);\n\nauto  hostView  =  ap::makeView(api::host,  externPtr,  ap::Vec{numElements});\nauto  devNonOwningView  =  devBuffer.getView();\n\nap::onHost::memset(queue,  devBuffer,  uint8_t{0});\nap::onHost::memcpy(queue,  devBuffer,  hostBuffer);\nap::onHost::fill(queue,  devBuffer,  DataType{42}); \n```", "```\nconstexpr  uint32_t  dim  =  2u;\nusing  IdxType  =  size_t;\nusing  DataType  =  int;\n\nIdxType  valueX,  valueY;\nauto  extentMD  =  ap::Vec{valueY,  valueX};\n\nauto  frameSpec  =  ap::onHost::FrameSpec{numFramesMd,  frameExtentMd};\nauto  tunedSpec  =  ap::onHost::getFrameSpec<DataType>(device,  extentMd);\n\nqueue.enqueue(tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...});\n\nauto  executor  =  ap::exec::cpuSerial;\nqueue.enqueue(executor,  tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...}); \n```", "```\nstruct  MyKernel\n{\n  ALPAKA_FN_ACC  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,  auto...  args)  const\n  {\n  auto  idxMd  =  acc.getIdxWithin(ap::onAcc::origin::grid,  ap::onAcc::unit::blocks);\n\n  auto  sharedMdArray  =\n  ap::onAcc::declareSharedMdArray<float,  ap::uniqueId()>(acc,  ap::CVec<uint32_t,  3,  4>{});\n\n  ap::onAcc::syncBlockThreads(acc);\n  auto  old  =  onAcc::atomicAdd(acc,  args...);\n  ap::onAcc::memFence(acc,  ap::onAcc::scope::block);\n  auto  sinValue  =  ap::math::sin(args[0]);\n  }\n}; \n```", "```\n    mkdir  my_alpaka_project  &&  cd  my_alpaka_project \n    ```", "```\n    cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\n    cmake  --build  build  --parallel \n    ```", "```\n    ./build/myAlpakaApp \n    ```", "```\nNumber of available devices: 1\nUsing device: [Device Name] \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::hip, ap::deviceKind::amdGpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  -x  hip  --offload-arch=gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host, ap::deviceKind::cpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::cuda, ap::deviceKind::nvidiaGpu);\nnvcc  -I  $ALPAKA_DIR/include/  -std=c++20  --expt-relaxed-constexpr  -x  cuda  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::cpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64_x86_64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::intelGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::amdGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=amd_gpu_gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::nvidiaGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=nvptx64-nvidia-cuda  -Xsycl-target-backend=nvptx64-nvidia-cuda  --offload-arch=sm_80  main.cpp\n./a.out \n```", "```\n$ srun  -p  dev-g  --gpus  1  -N  1  -n  1  --time=00:20:00  --account=project_465002387  --pty  bash\n....\nsrun: job 1234 queued and waiting for resources\nsrun: job 1234 has been allocated resources\n\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n$ rocm-smi\n\n======================================= ROCm System Management Interface =======================================\n================================================= Concise Info =================================================\nDevice  [Model : Revision]    Temp    Power  Partitions      SCLK    MCLK     Fan  Perf    PwrCap  VRAM%  GPU%\n Name (20 chars)       (Edge)  (Avg)  (Mem, Compute)\n================================================================================================================\n0       [0x0b0c : 0x00]       45.0Â°C  N/A    N/A, N/A        800Mhz  1600Mhz  0%   manual  0.0W      0%   0%\n AMD INSTINCT MI200 (\n================================================================================================================\n============================================= End of ROCm SMI Log ============================================== \n```", "```\ncmake_minimum_required(VERSION  3.25)\nproject(vectorAdd  LANGUAGES  CXX  VERSION  1.0)\n#Use CMake's FetchContent to download and integrate alpaka3 directly from GitHub\ninclude(FetchContent)\n#Declare where to fetch alpaka3 from\n#This will download the library at configure time\nFetchContent_Declare(alpaka3  GIT_REPOSITORY  https://github.com/alpaka-group/alpaka3.git  GIT_TAG  dev)\n#Make alpaka3 available for use in this project\n#This downloads, configures, and makes the library targets available\nFetchContent_MakeAvailable(alpaka3)\n#Finalize the alpaka FetchContent setup\nalpaka_FetchContent_Finalize() #Create the executable target from the source file\nadd_executable(vectorAdd  main.cpp)\n#Link the alpaka library to the executable\ntarget_link_libraries(vectorAdd  PRIVATE  alpaka::alpaka)\n#Finalize the alpaka configuration for this target\n#This sets up backend - specific compiler flags and dependencies\nalpaka_finalize(vectorAdd) \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise vector addition on device\n ap::onHost::transform(queue,  c,  std::plus{},  a,  b); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n$ mkdir  alpakaExercise  &&  cd  alpakaExercise\n$ vim  CMakeLists.txt\nand now paste the CMakeLsits here (Press i, followed by Ctrl+Shift+V)\nPress esc and then :wq to exit vim\n$ vim  main.cpp\nSimilarly, paste the C++ code here \n```", "```\nconfigure step, we additionaly specify that HIP is available\n$ cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\nbuild\n$ cmake  --build  build  --parallel\nrun\n$ ./build/vectorAdd\nUsing alpaka device: AMD Instinct MI250X id=0\nc[0] = 1\nc[1] = 2\nc[2] = 3\nc[3] = 4\nc[4] = 5 \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  AddKernel  {\n constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const  &acc, ap::concepts::IMdSpan  auto  c, ap::concepts::IMdSpan  auto  const  a, ap::concepts::IMdSpan  auto  const  b)  const  { for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid, ap::IdxRange{c.getExtents()}))  { c[idx]  =  a[idx]  +  b[idx]; } } }; \nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n auto  frameSpec  =  ap::onHost::getFrameSpec<int>(devAcc,  c.getExtents()); \n // Call the element-wise addition kernel on device queue.enqueue(frameSpec,  ap::KernelBundle{AddKernel{},  c,  a,  b}); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n    git  clone  https://github.com/alpaka-group/alpaka3.git\n    cd  alpaka \n    ```", "```\n    export  ALPAKA_DIR=/path/to/your/alpaka/install/dir \n    ```", "```\n    mkdir  build\n    cmake  -B  build  -S  .  -DCMAKE_INSTALL_PREFIX=$ALPAKA_DIR\n    cmake  --build  build  --parallel \n    ```", "```\n    export  CMAKE_PREFIX_PATH=$ALPAKA_DIR:$CMAKE_PREFIX_PATH \n    ```", "```\n> cmake_minimum_required(VERSION  3.25)\n> project(myAlpakaApp  VERSION  1.0)\n> \n> # Find installed alpaka\n> find_package(alpaka  REQUIRED)\n> \n> # Build the executable\n> add_executable(myAlpakaApp  main.cpp)\n> target_link_libraries(myAlpakaApp  PRIVATE  alpaka::alpaka)\n> alpaka_finalize(myAlpakaApp) \n> ```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n> #include  <alpaka/alpaka.hpp>\n> #include  <cstdlib>\n> #include  <iostream>\n> \n> namespace  ap  =  alpaka;\n> \n> auto  getDeviceSpec()\n> {\n>   /* Select a device, possible combinations of api+deviceKind:\n>  * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n>  * oneApi+amdGpu, oneApi+nvidiaGpu\n>  */\n>   return  ap::onHost::DeviceSpec{ap::api::hip,  ap::deviceKind::amdGpu};\n> }\n> \n> int  main(int  argc,  char**  argv)\n> {\n>   // Initialize device specification and selector\n>   ap::onHost::DeviceSpec  devSpec  =  getDeviceSpec();\n>   auto  deviceSelector  =  ap::onHost::makeDeviceSelector(devSpec);\n> \n>   // Query available devices\n>   auto  num_devices  =  deviceSelector.getDeviceCount();\n>   std::cout  <<  \"Number of available devices: \"  <<  num_devices  <<  \"\\n\";\n> \n>   if  (num_devices  ==  0)  {\n>   std::cerr  <<  \"No devices found for the selected backend\\n\";\n>   return  EXIT_FAILURE;\n>   }\n> \n>   // Select and initialize the first device\n>   auto  device  =  deviceSelector.makeDevice(0);\n>   std::cout  <<  \"Using device: \"  <<  device.getName()  <<  \"\\n\";\n> \n>   return  EXIT_SUCCESS;\n> } \n> ```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  myProject\n{\n  namespace  ap  =  alpaka;\n  // Your code here\n} \n```", "```\nauto  devSelector  =  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\nif  (devSelector.getDeviceCount()  ==  0)\n{\n  throw  std::runtime_error(\"No device found!\");\n}\nauto  device  =  devSelector.makeDevice(0); \n```", "```\nauto  queue  =  device.makeQueue();\nauto  nonBlockingQueue  =  device.makeQueue(ap::queueKind::nonBlocking);\nauto  blockingQueue  =  device.makeQueue(ap::queueKind::blocking);\n\nauto  event  =  device.makeEvent();\nqueue.enqueue(event);\nap::onHost::wait(event);\nap::onHost::wait(queue); \n```", "```\nauto  hostBuffer  =  ap::onHost::allocHost<DataType>(extent3D);\nauto  devBuffer  =  ap::onHost::alloc<DataType>(device,  extentMd);\nauto  devMappedBuffer  =  ap::onHost::allocMapped<DataType>(device,  extentMd);\n\nauto  hostView  =  ap::makeView(api::host,  externPtr,  ap::Vec{numElements});\nauto  devNonOwningView  =  devBuffer.getView();\n\nap::onHost::memset(queue,  devBuffer,  uint8_t{0});\nap::onHost::memcpy(queue,  devBuffer,  hostBuffer);\nap::onHost::fill(queue,  devBuffer,  DataType{42}); \n```", "```\nconstexpr  uint32_t  dim  =  2u;\nusing  IdxType  =  size_t;\nusing  DataType  =  int;\n\nIdxType  valueX,  valueY;\nauto  extentMD  =  ap::Vec{valueY,  valueX};\n\nauto  frameSpec  =  ap::onHost::FrameSpec{numFramesMd,  frameExtentMd};\nauto  tunedSpec  =  ap::onHost::getFrameSpec<DataType>(device,  extentMd);\n\nqueue.enqueue(tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...});\n\nauto  executor  =  ap::exec::cpuSerial;\nqueue.enqueue(executor,  tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...}); \n```", "```\nstruct  MyKernel\n{\n  ALPAKA_FN_ACC  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,  auto...  args)  const\n  {\n  auto  idxMd  =  acc.getIdxWithin(ap::onAcc::origin::grid,  ap::onAcc::unit::blocks);\n\n  auto  sharedMdArray  =\n  ap::onAcc::declareSharedMdArray<float,  ap::uniqueId()>(acc,  ap::CVec<uint32_t,  3,  4>{});\n\n  ap::onAcc::syncBlockThreads(acc);\n  auto  old  =  onAcc::atomicAdd(acc,  args...);\n  ap::onAcc::memFence(acc,  ap::onAcc::scope::block);\n  auto  sinValue  =  ap::math::sin(args[0]);\n  }\n}; \n```", "```\n    mkdir  my_alpaka_project  &&  cd  my_alpaka_project \n    ```", "```\n    cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\n    cmake  --build  build  --parallel \n    ```", "```\n    ./build/myAlpakaApp \n    ```", "```\nNumber of available devices: 1\nUsing device: [Device Name] \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  myProject\n{\n  namespace  ap  =  alpaka;\n  // Your code here\n} \n```", "```\nauto  devSelector  =  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\nif  (devSelector.getDeviceCount()  ==  0)\n{\n  throw  std::runtime_error(\"No device found!\");\n}\nauto  device  =  devSelector.makeDevice(0); \n```", "```\nauto  queue  =  device.makeQueue();\nauto  nonBlockingQueue  =  device.makeQueue(ap::queueKind::nonBlocking);\nauto  blockingQueue  =  device.makeQueue(ap::queueKind::blocking);\n\nauto  event  =  device.makeEvent();\nqueue.enqueue(event);\nap::onHost::wait(event);\nap::onHost::wait(queue); \n```", "```\nauto  hostBuffer  =  ap::onHost::allocHost<DataType>(extent3D);\nauto  devBuffer  =  ap::onHost::alloc<DataType>(device,  extentMd);\nauto  devMappedBuffer  =  ap::onHost::allocMapped<DataType>(device,  extentMd);\n\nauto  hostView  =  ap::makeView(api::host,  externPtr,  ap::Vec{numElements});\nauto  devNonOwningView  =  devBuffer.getView();\n\nap::onHost::memset(queue,  devBuffer,  uint8_t{0});\nap::onHost::memcpy(queue,  devBuffer,  hostBuffer);\nap::onHost::fill(queue,  devBuffer,  DataType{42}); \n```", "```\nconstexpr  uint32_t  dim  =  2u;\nusing  IdxType  =  size_t;\nusing  DataType  =  int;\n\nIdxType  valueX,  valueY;\nauto  extentMD  =  ap::Vec{valueY,  valueX};\n\nauto  frameSpec  =  ap::onHost::FrameSpec{numFramesMd,  frameExtentMd};\nauto  tunedSpec  =  ap::onHost::getFrameSpec<DataType>(device,  extentMd);\n\nqueue.enqueue(tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...});\n\nauto  executor  =  ap::exec::cpuSerial;\nqueue.enqueue(executor,  tunedSpec,  ap::KernelBundle{kernel,  kernelArgs...}); \n```", "```\nstruct  MyKernel\n{\n  ALPAKA_FN_ACC  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,  auto...  args)  const\n  {\n  auto  idxMd  =  acc.getIdxWithin(ap::onAcc::origin::grid,  ap::onAcc::unit::blocks);\n\n  auto  sharedMdArray  =\n  ap::onAcc::declareSharedMdArray<float,  ap::uniqueId()>(acc,  ap::CVec<uint32_t,  3,  4>{});\n\n  ap::onAcc::syncBlockThreads(acc);\n  auto  old  =  onAcc::atomicAdd(acc,  args...);\n  ap::onAcc::memFence(acc,  ap::onAcc::scope::block);\n  auto  sinValue  =  ap::math::sin(args[0]);\n  }\n}; \n```", "```\n    mkdir  my_alpaka_project  &&  cd  my_alpaka_project \n    ```", "```\n    cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\n    cmake  --build  build  --parallel \n    ```", "```\n    ./build/myAlpakaApp \n    ```", "```\nNumber of available devices: 1\nUsing device: [Device Name] \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::hip, ap::deviceKind::amdGpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  -x  hip  --offload-arch=gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host, ap::deviceKind::cpu);\n# We use CC to refer to the compiler to work smoothly with the LUMI environment\nCC  -I  $ALPAKA_DIR/include/  -std=c++20  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::cuda, ap::deviceKind::nvidiaGpu);\nnvcc  -I  $ALPAKA_DIR/include/  -std=c++20  --expt-relaxed-constexpr  -x  cuda  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::cpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64_x86_64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::intelGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=spir64  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::amdGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=amd_gpu_gfx90a  main.cpp\n./a.out \n```", "```\n# use the following in C++ code\n# auto devSelector = ap::onHost::makeDeviceSelector(ap::api::oneApi, ap::deviceKind::nvidiaGpu);\nicpx  -I  $ALPAKA_DIR/include/  -std=c++20  -fsycl  -fsycl-targets=nvptx64-nvidia-cuda  -Xsycl-target-backend=nvptx64-nvidia-cuda  --offload-arch=sm_80  main.cpp\n./a.out \n```", "```\n$ srun  -p  dev-g  --gpus  1  -N  1  -n  1  --time=00:20:00  --account=project_465002387  --pty  bash\n....\nsrun: job 1234 queued and waiting for resources\nsrun: job 1234 has been allocated resources\n\n$ module  load  LUMI/24.03  partition/G\n$ module  load  rocm/6.0.3\n$ module  load  buildtools/24.03\n$ module  load  PrgEnv-amd\n$ module  load  craype-accel-amd-gfx90a\n$ export  CXX=hipcc \n```", "```\n$ rocm-smi\n\n======================================= ROCm System Management Interface =======================================\n================================================= Concise Info =================================================\nDevice  [Model : Revision]    Temp    Power  Partitions      SCLK    MCLK     Fan  Perf    PwrCap  VRAM%  GPU%\n Name (20 chars)       (Edge)  (Avg)  (Mem, Compute)\n================================================================================================================\n0       [0x0b0c : 0x00]       45.0Â°C  N/A    N/A, N/A        800Mhz  1600Mhz  0%   manual  0.0W      0%   0%\n AMD INSTINCT MI200 (\n================================================================================================================\n============================================= End of ROCm SMI Log ============================================== \n```", "```\ncmake_minimum_required(VERSION  3.25)\nproject(vectorAdd  LANGUAGES  CXX  VERSION  1.0)\n#Use CMake's FetchContent to download and integrate alpaka3 directly from GitHub\ninclude(FetchContent)\n#Declare where to fetch alpaka3 from\n#This will download the library at configure time\nFetchContent_Declare(alpaka3  GIT_REPOSITORY  https://github.com/alpaka-group/alpaka3.git  GIT_TAG  dev)\n#Make alpaka3 available for use in this project\n#This downloads, configures, and makes the library targets available\nFetchContent_MakeAvailable(alpaka3)\n#Finalize the alpaka FetchContent setup\nalpaka_FetchContent_Finalize() #Create the executable target from the source file\nadd_executable(vectorAdd  main.cpp)\n#Link the alpaka library to the executable\ntarget_link_libraries(vectorAdd  PRIVATE  alpaka::alpaka)\n#Finalize the alpaka configuration for this target\n#This sets up backend - specific compiler flags and dependencies\nalpaka_finalize(vectorAdd) \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise vector addition on device\n ap::onHost::transform(queue,  c,  std::plus{},  a,  b); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n$ mkdir  alpakaExercise  &&  cd  alpakaExercise\n$ vim  CMakeLists.txt\nand now paste the CMakeLsits here (Press i, followed by Ctrl+Shift+V)\nPress esc and then :wq to exit vim\n$ vim  main.cpp\nSimilarly, paste the C++ code here \n```", "```\nconfigure step, we additionaly specify that HIP is available\n$ cmake  -B  build  -S  .  -Dalpaka_DEP_HIP=ON\nbuild\n$ cmake  --build  build  --parallel\nrun\n$ ./build/vectorAdd\nUsing alpaka device: AMD Instinct MI250X id=0\nc[0] = 1\nc[1] = 2\nc[2] = 3\nc[3] = 4\nc[4] = 5 \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  AddKernel  {\n constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const  &acc, ap::concepts::IMdSpan  auto  c, ap::concepts::IMdSpan  auto  const  a, ap::concepts::IMdSpan  auto  const  b)  const  { for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid, ap::IdxRange{c.getExtents()}))  { c[idx]  =  a[idx]  +  b[idx]; } } }; \nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n\n  // auto devSelector = ap::onHost::makeDeviceSelector(ap::api::host,\n  // ap::deviceKind::cpu);\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n auto  frameSpec  =  ap::onHost::getFrameSpec<int>(devAcc,  c.getExtents()); \n // Call the element-wise addition kernel on device queue.enqueue(frameSpec,  ap::KernelBundle{AddKernel{},  c,  a,  b}); \n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <algorithm>\n#include  <cstdio>\n#include  <execution>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  5;\n\n  // Allocate arrays\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  std::transform(std::execution::par_unseq,  a.begin(),  a.end(),  b.begin(),\n  c.begin(),  [](int  i,  int  j)  {  return  i  *  j;  });\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *b  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *c  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  Kokkos::kokkos_free(b);\n  Kokkos::kokkos_free(c);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 220\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =\n  \"                                                 \\\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) { \\\n int i = get_global_id(0);                                                        \\\n c[i] = a[i] * b[i];                                                              \\\n }                                                                                  \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"dot\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *b  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *c  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n  clSetKernelArgSVMPointer(kernel,  1,  b);\n  clSetKernelArgSVMPointer(kernel,  2,  c);\n\n  // Create mappings for host and initialize values\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  a,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  b,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n  clEnqueueSVMUnmap(queue,  b,  0,  NULL,  NULL);\n\n  size_t  globalSize  =  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  c,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  clEnqueueSVMUnmap(queue,  c,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n  clSVMFree(context,  b);\n  clSVMFree(context,  c);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(n,  q);\n  int  *b  =  sycl::malloc_shared<int>(n,  q);\n  int  *c  =  sycl::malloc_shared<int>(n,  q);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c[i]  =  a[i]  *  b[i];\n  }).wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n  sycl::free(b,  q);\n  sycl::free(c,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints on Kokkos host memory space\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_a(\"h_a\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_b(\"h_b\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_c(\"h_c\",  n);\n\n  // Allocate space for 5 ints on Kokkos default memory space (eg, GPU memory)\n  Kokkos::View<int  *>  a(\"a\",  n);\n  Kokkos::View<int  *>  b(\"b\",  n);\n  Kokkos::View<int  *>  c(\"c\",  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy from host to device\n  Kokkos::deep_copy(a,  h_a);\n  Kokkos::deep_copy(b,  h_b);\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Copy from device to host\n  Kokkos::deep_copy(h_c,  c);\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) {\n int i = get_global_id(0);\n c[i] = a[i] * b[i];\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device.\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_dot(program,  \"dot\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Create buffers and copy input data to device.\n  cl::Buffer  dev_a(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  a.data());\n  cl::Buffer  dev_b(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  b.data());\n  cl::Buffer  dev_c(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int));\n\n  // Pass arguments to device kernel\n  kernel_dot.setArg(0,  dev_a);\n  kernel_dot.setArg(1,  dev_b);\n  kernel_dot.setArg(2,  dev_c);\n\n  // We don't need to apply any offset to thread IDs\n  queue.enqueueNDRangeKernel(kernel_dot,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(dev_c,  CL_TRUE,  0,  n  *  sizeof(int),  c.data());\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints\n  auto  a_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  b_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  c_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n\n  // Initialize values\n  // We should use curly braces to limit host accessors' lifetime\n  //    and indicate when we're done working with them:\n  {\n  auto  a_host_acc  =  a_buf.get_host_access();\n  auto  b_host_acc  =  b_buf.get_host_access();\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a_host_acc[i]  =  i;\n  b_host_acc[i]  =  1;\n  }\n  }\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create read accessors over a_buf and b_buf\n  auto  a_acc  =  a_buf.get_access<sycl::access_mode::read>(cgh);\n  auto  b_acc  =  b_buf.get_access<sycl::access_mode::read>(cgh);\n  // Create write accesor over c_buf\n  auto  c_acc  =  c_buf.get_access<sycl::access_mode::write>(cgh);\n  // Run element-wise multiplication on device\n  cgh.parallel_for<class  vec_add>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c_acc[i]  =  a_acc[i]  *  b_acc[i];\n  });\n  });\n\n  // No need to synchronize, creating the accessor for c_buf will do it\n  // automatically\n  {\n  const  auto  c_host_acc  =  c_buf.get_host_access();\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c_host_acc[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // Allocate memory on the device and inherit the extents from h_a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // allocate memory on the device and inherit the extents from a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(nx  *  sizeof(int));\n\n  // Create 'n' execution space instances (maps to streams in CUDA/HIP)\n  auto  ex  =  Kokkos::Experimental::partition_space(\n  Kokkos::DefaultExecutionSpace(),  1,  1,  1,  1,  1);\n\n  // Launch 'n' potentially asynchronous kernels\n  // Each kernel has their own execution space instances\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  Kokkos::parallel_for(\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(\n  ex[region],  nx  /  n  *  region,  nx  /  n  *  (region  +  1)),\n  KOKKOS_LAMBDA(const  int  i)  {  a[i]  =  region  +  i;  });\n  }\n\n  // Sync execution space instances (maps to streams in CUDA/HIP)\n  for  (unsigned  region  =  0;  region  <  n;  region++)\n  ex[region].fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 200\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =  \"              \\\n __kernel void async(__global int *a) { \\\n int i = get_global_id(0);            \\\n int region = i / get_global_size(0); \\\n a[i] = region + i;                   \\\n }                                      \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"async\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  nx  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  size_t  offset  =  (nx  /  n)  *  region;\n  size_t  size  =  nx  /  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  &offset,  &size,  NULL,  0,  NULL,\n  NULL);\n  }\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  a,  nx  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(nx,  q);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  const  int  iShifted  =  i  +  nx  /  n  *  region;\n  a[iShifted]  =  region  +  iShifted;\n  });\n  }\n\n  // Synchronize\n  q.wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  unsigned  nPerRegion  =  nx  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  ap::onHost::iota<int>(queues[region],  regionOffset,\n  a.getSubView(regionOffset,  nx  -  regionOffset));\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  IdxAssignKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  a,\n  unsigned  region,\n  unsigned  n)  const  {\n  unsigned  nPerRegion  =  a.getExtents().x()  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  for  (auto  [idx]  :\n  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{regionOffset,  regionOffset  +  nPerRegion}))  {\n  a[idx]  =  idx;\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(nx,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues[region].enqueue(\n  frameSpec,  ap::KernelBundle{IdxAssignKernel{},  a,  region,  n});\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <cstdio>\n#include  <execution>\n#include  <numeric>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  10;\n\n  std::vector<int>  a(n);\n\n  std::iota(a.begin(),  a.end(),  0);  // Fill the array\n\n  // Run reduction on the device\n  int  sum  =  std::reduce(std::execution::par_unseq,  a.cbegin(),  a.cend(),  0,\n  std::plus<int>{});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Run sum reduction kernel\n  Kokkos::parallel_reduce(\n  n,  KOKKOS_LAMBDA(const  int  i,  int  &lsum)  {  lsum  +=  i;  },  sum);\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void reduce(__global int* sum, __local int* local_mem) {\n\n // Get work group and work item information\n int gsize = get_global_size(0); // global work size\n int gid = get_global_id(0); // global work item index\n int lsize = get_local_size(0); // local work size\n int lid = get_local_id(0); // local work item index\n\n // Store reduced item into local memory\n local_mem[lid] = gid; // initialize local memory\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n\n // Perform reduction across the local work group\n for (int s = 1; s < lsize; s *= 2) { // loop over local memory with stride doubling each iteration\n if (lid % (2 * s) == 0 && (lid + s) < lsize) {\n local_mem[lid] += local_mem[lid + s];\n }\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n }\n\n if (lid == 0) { // only one work item per work group\n atomic_add(sum, local_mem[0]); // add partial sum to global sum atomically\n }\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_reduce(program,  \"reduce\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Create buffer for sum\n  cl::Buffer  buffer(context,  CL_MEM_READ_WRITE  |  CL_MEM_COPY_HOST_PTR,\n  sizeof(int),  &sum);\n\n  // Pass arguments to device kernel\n  kernel_reduce.setArg(0,  buffer);  // pass buffer to device\n  kernel_reduce.setArg(1,  sizeof(int),  NULL);  // allocate local memory\n\n  // Enqueue kernel\n  queue.enqueueNDRangeKernel(kernel_reduce,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(buffer,  CL_TRUE,  0,  sizeof(int),  &sum);\n\n  // Print result\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  return  0;\n} \n```", "```\n// We use built-in sycl::reduction mechanism in this example.\n// The manual implementation of the reduction kernel can be found in\n// the \"Non-portable kernel models\" chapter.\n\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  sycl::queue  q;\n  unsigned  n  =  10;\n\n  // Initialize sum\n  int  sum  =  0;\n  {\n  // Create a buffer for sum to get the reduction results\n  sycl::buffer<int>  sum_buf{&sum,  1};\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create temporary object describing variables with reduction semantics\n  auto  sum_acc  =  sum_buf.get_access<sycl::access_mode::read_write>(cgh);\n  // We can use built-in reduction primitive\n  auto  sum_reduction  =  sycl::reduction(sum_acc,  sycl::plus<int>());\n\n  // A reference to the reducer is passed to the lambda\n  cgh.parallel_for(\n  sycl::range<1>{n},  sum_reduction,\n  [=](sycl::id<1>  idx,  auto  &reducer)  {  reducer.combine(idx[0]);  });\n  }).wait();\n  // The contents of sum_buf are copied back to sum by the destructor of\n  // sum_buf\n  }\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  10;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  sum  =  ap::onHost::allocUnified<int>(devAcc,  1);\n\n  // Run element-wise multiplication on device\n  ap::onHost::reduce(queue,  0,  sum,  std::plus{},  ap::LinearizedIdxGenerator{n});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum[0]);\n\n  return  0;\n} \n```", "```\n#include  <algorithm>\n#include  <cstdio>\n#include  <execution>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  5;\n\n  // Allocate arrays\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  std::transform(std::execution::par_unseq,  a.begin(),  a.end(),  b.begin(),\n  c.begin(),  [](int  i,  int  j)  {  return  i  *  j;  });\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *b  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n  int  *c  =  (int  *)Kokkos::kokkos_malloc(n  *  sizeof(int));\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  Kokkos::kokkos_free(b);\n  Kokkos::kokkos_free(c);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 220\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =\n  \"                                                 \\\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) { \\\n int i = get_global_id(0);                                                        \\\n c[i] = a[i] * b[i];                                                              \\\n }                                                                                  \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"dot\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *b  =  clSVMAlloc(context,  CL_MEM_READ_ONLY,  n  *  sizeof(int),  0);\n  int  *c  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n  clSetKernelArgSVMPointer(kernel,  1,  b);\n  clSetKernelArgSVMPointer(kernel,  2,  c);\n\n  // Create mappings for host and initialize values\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  a,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_WRITE,  b,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n  clEnqueueSVMUnmap(queue,  b,  0,  NULL,  NULL);\n\n  size_t  globalSize  =  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  NULL,  &globalSize,  NULL,  0,  NULL,\n  NULL);\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  c,  n  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  clEnqueueSVMUnmap(queue,  c,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n  clSVMFree(context,  b);\n  clSVMFree(context,  c);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(n,  q);\n  int  *b  =  sycl::malloc_shared<int>(n,  q);\n  int  *c  =  sycl::malloc_shared<int>(n,  q);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c[i]  =  a[i]  *  b[i];\n  }).wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n  sycl::free(b,  q);\n  sycl::free(c,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  b  =  ap::onHost::allocUnified<int>(devAcc,  n);\n  auto  c  =  ap::onHost::allocUnified<int>(devAcc,  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints on Kokkos host memory space\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_a(\"h_a\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_b(\"h_b\",  n);\n  Kokkos::View<int  *,  Kokkos::HostSpace>  h_c(\"h_c\",  n);\n\n  // Allocate space for 5 ints on Kokkos default memory space (eg, GPU memory)\n  Kokkos::View<int  *>  a(\"a\",  n);\n  Kokkos::View<int  *>  b(\"b\",  n);\n  Kokkos::View<int  *>  c(\"c\",  n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy from host to device\n  Kokkos::deep_copy(a,  h_a);\n  Kokkos::deep_copy(b,  h_b);\n\n  // Run element-wise multiplication on device\n  Kokkos::parallel_for(n,  KOKKOS_LAMBDA(const  int  i)  {  c[i]  =  a[i]  *  b[i];  });\n\n  // Copy from device to host\n  Kokkos::deep_copy(h_c,  c);\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void dot(__global const int *a, __global const int *b, __global int *c) {\n int i = get_global_id(0);\n c[i] = a[i] * b[i];\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device.\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_dot(program,  \"dot\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  5;\n\n  std::vector<int>  a(n),  b(n),  c(n);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a[i]  =  i;\n  b[i]  =  1;\n  }\n\n  // Create buffers and copy input data to device.\n  cl::Buffer  dev_a(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  a.data());\n  cl::Buffer  dev_b(context,  CL_MEM_READ_ONLY  |  CL_MEM_COPY_HOST_PTR,\n  n  *  sizeof(int),  b.data());\n  cl::Buffer  dev_c(context,  CL_MEM_WRITE_ONLY,  n  *  sizeof(int));\n\n  // Pass arguments to device kernel\n  kernel_dot.setArg(0,  dev_a);\n  kernel_dot.setArg(1,  dev_b);\n  kernel_dot.setArg(2,  dev_c);\n\n  // We don't need to apply any offset to thread IDs\n  queue.enqueueNDRangeKernel(kernel_dot,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(dev_c,  CL_TRUE,  0,  n  *  sizeof(int),  c.data());\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n\n  // Allocate space for 5 ints\n  auto  a_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  b_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n  auto  c_buf  =  sycl::buffer<int>(sycl::range<1>(n));\n\n  // Initialize values\n  // We should use curly braces to limit host accessors' lifetime\n  //    and indicate when we're done working with them:\n  {\n  auto  a_host_acc  =  a_buf.get_host_access();\n  auto  b_host_acc  =  b_buf.get_host_access();\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  a_host_acc[i]  =  i;\n  b_host_acc[i]  =  1;\n  }\n  }\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create read accessors over a_buf and b_buf\n  auto  a_acc  =  a_buf.get_access<sycl::access_mode::read>(cgh);\n  auto  b_acc  =  b_buf.get_access<sycl::access_mode::read>(cgh);\n  // Create write accesor over c_buf\n  auto  c_acc  =  c_buf.get_access<sycl::access_mode::write>(cgh);\n  // Run element-wise multiplication on device\n  cgh.parallel_for<class  vec_add>(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  c_acc[i]  =  a_acc[i]  *  b_acc[i];\n  });\n  });\n\n  // No need to synchronize, creating the accessor for c_buf will do it\n  // automatically\n  {\n  const  auto  c_host_acc  =  c_buf.get_host_access();\n  // Print results\n  for  (unsigned  i  =  0;  i  <  n;  i++)\n  printf(\"c[%d] = %d\\n\",  i,  c_host_acc[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // Allocate memory on the device and inherit the extents from h_a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  // Run element-wise multiplication on device\n  ap::onHost::transform(queue,  c,  std::multiplies{},  a,  b);\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  MulKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  c,\n  ap::concepts::IMdSpan  auto  const  a,\n  ap::concepts::IMdSpan  auto  const  b)  const  {\n  for  (auto  idx  :  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{c.getExtents()}))  {\n  c[idx]  =  a[idx]  *  b[idx];\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate memory that is accessible on host\n  auto  h_a  =  ap::onHost::allocHost<int>(n);\n  auto  h_b  =  ap::onHost::allocHostLike(h_a);\n  auto  h_c  =  ap::onHost::allocHostLike(h_a);\n\n  // allocate memory on the device and inherit the extents from a\n  auto  a  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  b  =  ap::onHost::allocLike(devAcc,  h_a);\n  auto  c  =  ap::onHost::allocLike(devAcc,  h_a);\n\n  // Initialize values on host\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  h_a[i]  =  i;\n  h_b[i]  =  1;\n  }\n\n  // Copy host memory element wise to the device memory\n  ap::onHost::memcpy(queue,  a,  h_a);\n  ap::onHost::memcpy(queue,  b,  h_b);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(n,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  queue.enqueue(frameSpec,  ap::KernelBundle{MulKernel{},  c,  a,  b});\n\n  // Copy the device result back to host memory\n  ap::onHost::memcpy(queue,  h_c,  c);\n\n  for  (unsigned  i  =  0;  i  <  n;  i++)  {\n  printf(\"c[%d] = %d\\n\",  i,  h_c[i]);\n  }\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate on Kokkos default memory space (Unified Memory)\n  int  *a  =  (int  *)Kokkos::kokkos_malloc(nx  *  sizeof(int));\n\n  // Create 'n' execution space instances (maps to streams in CUDA/HIP)\n  auto  ex  =  Kokkos::Experimental::partition_space(\n  Kokkos::DefaultExecutionSpace(),  1,  1,  1,  1,  1);\n\n  // Launch 'n' potentially asynchronous kernels\n  // Each kernel has their own execution space instances\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  Kokkos::parallel_for(\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(\n  ex[region],  nx  /  n  *  region,  nx  /  n  *  (region  +  1)),\n  KOKKOS_LAMBDA(const  int  i)  {  a[i]  =  region  +  i;  });\n  }\n\n  // Sync execution space instances (maps to streams in CUDA/HIP)\n  for  (unsigned  region  =  0;  region  <  n;  region++)\n  ex[region].fence();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free Kokkos allocation (Unified Memory)\n  Kokkos::kokkos_free(a);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C API here, since SVM support in C++ API is unstable on\n// ROCm\n#define CL_TARGET_OPENCL_VERSION 200\n#include  <CL/cl.h>\n#include  <stdio.h>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  char  *kernel_source  =  \"              \\\n __kernel void async(__global int *a) { \\\n int i = get_global_id(0);            \\\n int region = i / get_global_size(0); \\\n a[i] = region + i;                   \\\n }                                      \\\n \";\n\nint  main(int  argc,  char  *argv[])  {\n  // Initialize OpenCL\n  cl_platform_id  platform;\n  clGetPlatformIDs(1,  &platform,  NULL);\n  cl_device_id  device;\n  clGetDeviceIDs(platform,  CL_DEVICE_TYPE_GPU,  1,  &device,  NULL);\n  cl_context  context  =  clCreateContext(NULL,  1,  &device,  NULL,  NULL,  NULL);\n  cl_command_queue  queue  =  clCreateCommandQueue(context,  device,  0,  NULL);\n\n  // Compile OpenCL program for found device.\n  cl_program  program  =\n  clCreateProgramWithSource(context,  1,  &kernel_source,  NULL,  NULL);\n  clBuildProgram(program,  1,  &device,  NULL,  NULL,  NULL);\n  cl_kernel  kernel  =  clCreateKernel(program,  \"async\",  NULL);\n\n  // Set problem dimensions\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Create SVM buffer objects on host side\n  int  *a  =  clSVMAlloc(context,  CL_MEM_WRITE_ONLY,  nx  *  sizeof(int),  0);\n\n  // Pass arguments to device kernel\n  clSetKernelArgSVMPointer(kernel,  0,  a);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  size_t  offset  =  (nx  /  n)  *  region;\n  size_t  size  =  nx  /  n;\n  clEnqueueNDRangeKernel(queue,  kernel,  1,  &offset,  &size,  NULL,  0,  NULL,\n  NULL);\n  }\n\n  // Create mapping for host and print results\n  clEnqueueSVMMap(queue,  CL_TRUE,  CL_MAP_READ,  a,  nx  *  sizeof(int),  0,  NULL,\n  NULL);\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n  clEnqueueSVMUnmap(queue,  a,  0,  NULL,  NULL);\n\n  // Free SVM buffers\n  clSVMFree(context,  a);\n\n  return  0;\n} \n```", "```\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n\n  sycl::queue  q;\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  // Allocate shared memory (Unified Shared Memory)\n  int  *a  =  sycl::malloc_shared<int>(nx,  q);\n\n  // Launch multiple potentially asynchronous kernels on different parts of the\n  // array\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  q.parallel_for(sycl::range<1>{n},  [=](sycl::id<1>  i)  {\n  const  int  iShifted  =  i  +  nx  /  n  *  region;\n  a[iShifted]  =  region  +  iShifted;\n  });\n  }\n\n  // Synchronize\n  q.wait();\n\n  // Print results\n  for  (unsigned  i  =  0;  i  <  nx;  i++)\n  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  // Free shared memory allocation (Unified Memory)\n  sycl::free(a,  q);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  unsigned  nPerRegion  =  nx  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  ap::onHost::iota<int>(queues[region],  regionOffset,\n  a.getSubView(regionOffset,  nx  -  regionOffset));\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nstruct  IdxAssignKernel  {\n  constexpr  void  operator()(ap::onAcc::concepts::Acc  auto  const&  acc,\n  ap::concepts::IMdSpan  auto  a,\n  unsigned  region,\n  unsigned  n)  const  {\n  unsigned  nPerRegion  =  a.getExtents().x()  /  n;\n  unsigned  regionOffset  =  nPerRegion  *  region;\n  for  (auto  [idx]  :\n  ap::onAcc::makeIdxMap(acc,  ap::onAcc::worker::threadsInGrid,\n  ap::IdxRange{regionOffset,  regionOffset  +  nPerRegion}))  {\n  a[idx]  =  idx;\n  }\n  }\n};\n\nauto  main()  ->  int  {\n  unsigned  n  =  5;\n  unsigned  nx  =  20;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Non-blocking device queue (requires synchronization)\n  using  QueueType  =\n  ap::onHost::Queue<ALPAKA_TYPEOF(devAcc),  ap::queueKind::NonBlocking>;\n  std::vector<QueueType>  queues;\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues.emplace_back(devAcc.makeQueue(ap::queueKind::nonBlocking));\n  }\n\n  // Allocate unified memory that is accessible on host and device\n  auto  a  =  ap::onHost::allocUnified<int>(devAcc,  nx);\n\n  unsigned  frameExtent  =  32u;\n  auto  frameSpec  =\n  ap::onHost::FrameSpec{ap::divExZero(nx,  frameExtent),  frameExtent};\n\n  // Run element-wise multiplication on device\n  for  (unsigned  region  =  0;  region  <  n;  region++)  {\n  queues[region].enqueue(\n  frameSpec,  ap::KernelBundle{IdxAssignKernel{},  a,  region,  n});\n  }\n  // Wait for the device, includes all queues\n  ap::onHost::wait(devAcc);\n\n  for  (unsigned  i  =  0;  i  <  nx;  i++)  printf(\"a[%d] = %d\\n\",  i,  a[i]);\n\n  return  0;\n} \n```", "```\n#include  <cstdio>\n#include  <execution>\n#include  <numeric>\n#include  <vector>\n\nint  main()  {\n  unsigned  n  =  10;\n\n  std::vector<int>  a(n);\n\n  std::iota(a.begin(),  a.end(),  0);  // Fill the array\n\n  // Run reduction on the device\n  int  sum  =  std::reduce(std::execution::par_unseq,  a.cbegin(),  a.cend(),  0,\n  std::plus<int>{});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n\n  return  0;\n} \n```", "```\n#include  <Kokkos_Core.hpp>\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize Kokkos\n  Kokkos::initialize(argc,  argv);\n\n  {\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Run sum reduction kernel\n  Kokkos::parallel_reduce(\n  n,  KOKKOS_LAMBDA(const  int  i,  int  &lsum)  {  lsum  +=  i;  },  sum);\n\n  // Kokkos synchronization\n  Kokkos::fence();\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  // Finalize Kokkos\n  Kokkos::finalize();\n  return  0;\n} \n```", "```\n// We're using OpenCL C++ API here; there is also C API in <CL/cl.h>\n#define CL_TARGET_OPENCL_VERSION 110\n#define CL_HPP_TARGET_OPENCL_VERSION 110\n#include  <CL/cl.hpp>\n\n// For larger kernels, we can store source in a separate file\nstatic  const  std::string  kernel_source  =  R\"(\n __kernel void reduce(__global int* sum, __local int* local_mem) {\n\n // Get work group and work item information\n int gsize = get_global_size(0); // global work size\n int gid = get_global_id(0); // global work item index\n int lsize = get_local_size(0); // local work size\n int lid = get_local_id(0); // local work item index\n\n // Store reduced item into local memory\n local_mem[lid] = gid; // initialize local memory\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n\n // Perform reduction across the local work group\n for (int s = 1; s < lsize; s *= 2) { // loop over local memory with stride doubling each iteration\n if (lid % (2 * s) == 0 && (lid + s) < lsize) {\n local_mem[lid] += local_mem[lid + s];\n }\n barrier(CLK_LOCAL_MEM_FENCE); // synchronize local memory\n }\n\n if (lid == 0) { // only one work item per work group\n atomic_add(sum, local_mem[0]); // add partial sum to global sum atomically\n }\n }\n  )\";\n\nint  main(int  argc,  char  *argv[])  {\n\n  // Initialize OpenCL\n  cl::Device  device  =  cl::Device::getDefault();\n  cl::Context  context(device);\n  cl::CommandQueue  queue(context,  device);\n\n  // Compile OpenCL program for found device\n  cl::Program  program(context,  kernel_source);\n  program.build({device});\n  cl::Kernel  kernel_reduce(program,  \"reduce\");\n\n  {\n  // Set problem dimensions\n  unsigned  n  =  10;\n\n  // Initialize sum variable\n  int  sum  =  0;\n\n  // Create buffer for sum\n  cl::Buffer  buffer(context,  CL_MEM_READ_WRITE  |  CL_MEM_COPY_HOST_PTR,\n  sizeof(int),  &sum);\n\n  // Pass arguments to device kernel\n  kernel_reduce.setArg(0,  buffer);  // pass buffer to device\n  kernel_reduce.setArg(1,  sizeof(int),  NULL);  // allocate local memory\n\n  // Enqueue kernel\n  queue.enqueueNDRangeKernel(kernel_reduce,  cl::NullRange,  cl::NDRange(n),\n  cl::NullRange);\n\n  // Read result\n  queue.enqueueReadBuffer(buffer,  CL_TRUE,  0,  sizeof(int),  &sum);\n\n  // Print result\n  printf(\"sum = %d\\n\",  sum);\n  }\n\n  return  0;\n} \n```", "```\n// We use built-in sycl::reduction mechanism in this example.\n// The manual implementation of the reduction kernel can be found in\n// the \"Non-portable kernel models\" chapter.\n\n#include  <sycl/sycl.hpp>\n\nint  main()  {\n  sycl::queue  q;\n  unsigned  n  =  10;\n\n  // Initialize sum\n  int  sum  =  0;\n  {\n  // Create a buffer for sum to get the reduction results\n  sycl::buffer<int>  sum_buf{&sum,  1};\n\n  // Submit a SYCL kernel into a queue\n  q.submit([&](sycl::handler  &cgh)  {\n  // Create temporary object describing variables with reduction semantics\n  auto  sum_acc  =  sum_buf.get_access<sycl::access_mode::read_write>(cgh);\n  // We can use built-in reduction primitive\n  auto  sum_reduction  =  sycl::reduction(sum_acc,  sycl::plus<int>());\n\n  // A reference to the reducer is passed to the lambda\n  cgh.parallel_for(\n  sycl::range<1>{n},  sum_reduction,\n  [=](sycl::id<1>  idx,  auto  &reducer)  {  reducer.combine(idx[0]);  });\n  }).wait();\n  // The contents of sum_buf are copied back to sum by the destructor of\n  // sum_buf\n  }\n  // Print results\n  printf(\"sum = %d\\n\",  sum);\n} \n```", "```\n#include  <alpaka/alpaka.hpp>\n\nnamespace  ap  =  alpaka;\n\nauto  main()  ->  int  {\n  unsigned  n  =  10;\n\n  /* Select a device, possible combinations:\n * host+cpu, cuda+nvidiaGpu, hip+amdGpu, oneApi+intelGpu, oneApi+cpu,\n * oneApi+amdGpu, oneApi+nvidiaGpu\n */\n  auto  devSelector  =\n  ap::onHost::makeDeviceSelector(ap::api::hip,  ap::deviceKind::amdGpu);\n  ap::onHost::Device  devAcc  =  devSelector.makeDevice(0);\n  printf(\"Using alpaka device: %s\\n\",  devAcc.getName().c_str());\n\n  // Blocking device queue (requires synchronization)\n  ap::onHost::Queue  queue  =  devAcc.makeQueue(ap::queueKind::blocking);\n\n  // Allocate unified memory that is accessible on host and device\n  auto  sum  =  ap::onHost::allocUnified<int>(devAcc,  1);\n\n  // Run element-wise multiplication on device\n  ap::onHost::reduce(queue,  0,  sum,  std::plus{},  ap::LinearizedIdxGenerator{n});\n\n  // Print results\n  printf(\"sum = %d\\n\",  sum[0]);\n\n  return  0;\n} \n```"]