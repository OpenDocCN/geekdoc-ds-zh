<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>7.5. Application: random walks on graphs and PageRank#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>7.5. Application: random walks on graphs and PageRank#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap07_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html">https://mmids-textbook.github.io/chap07_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html</a></blockquote>

<p>As we mentioned earlier in this chapter, a powerful way to extract information about the structure of a network is to analyze the behavior of a walker randomly “diffusing” on it.</p>
<section id="random-walk-on-a-graph">
<h2><span class="section-number">7.5.1. </span>Random walk on a graph<a class="headerlink" href="#random-walk-on-a-graph" title="Link to this heading">#</a></h2>
<p>We first specialize the theory of the previous section to random walks on graphs. We start with the case of digraphs. The undirected case leads to useful simplifications.</p>
<p><strong>Directed case</strong> We first define a random walk on a digraph.</p>
<p><strong>DEFINITION</strong> <strong>(Random Walk on a Digraph)</strong> <span class="math notranslate nohighlight">\(\idx{random walk on a graph}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a directed graph. If a vertex does not have an outgoing edge (i.e., an edge with it as its source), add a self-loop to it. A random walk on <span class="math notranslate nohighlight">\(G\)</span> is a time-homogeneous Markov chain <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> with state space <span class="math notranslate nohighlight">\(\mathcal{S} = V\)</span> and transition probabilities</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta^+(i)},
\qquad
\forall i \in V, j \in N^+(i),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta^+(i)\)</span> is the out-degree of <span class="math notranslate nohighlight">\(i\)</span>, i.e., the number of outgoing edges and <span class="math notranslate nohighlight">\(N^+(i) = \{j \in V:(i,j) \in E\}\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In words, at each step, we choose an outgoing edge from the current state uniformly at random. Choosing a self-loop (i.e., an edge of the form <span class="math notranslate nohighlight">\((i,i)\)</span>) means staying where we are.</p>
<p>Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a digraph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices. Without loss of generality, we let the vertex set be <span class="math notranslate nohighlight">\([n] = \{1,\ldots,n\}\)</span>. The adjacency matrix of <span class="math notranslate nohighlight">\(G\)</span> is denoted as <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j}\)</span>. We define the out-degree matrix <span class="math notranslate nohighlight">\(D\)</span> as the diagonal matrix with diagonal entries <span class="math notranslate nohighlight">\(\delta^+(i)\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>. That is,</p>
<div class="math notranslate nohighlight">
\[
D = \mathrm{diag}(A \mathbf{1}).
\]</div>
<p><strong>LEMMA</strong> <strong>(Transition Matrix in Terms of Adjacency)</strong> <span class="math notranslate nohighlight">\(\idx{transition matrix in terms of adjacency}\xdi\)</span> The transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> satisfying the conditions of the definition above is</p>
<div class="math notranslate nohighlight">
\[
P = D^{-1} A.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The formula follows immediately from the definition. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We specialize irreducibility to the case of random walk on a digraph.</p>
<p><strong>LEMMA</strong> <strong>(Irreducibility)</strong> <span class="math notranslate nohighlight">\(\idx{irreducibility lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a digraph. Random walk on <span class="math notranslate nohighlight">\(G\)</span> is irreducible if and only if <span class="math notranslate nohighlight">\(G\)</span> is strongly connected. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Simply note that the transition graph of the walk is <span class="math notranslate nohighlight">\(G\)</span> itself. We have seen previously that irreducibility is equivalent to the transition graph being strongly connected. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>In the undirected case, more structure emerges as we detail next.</p>
<p><strong>Undirected case</strong> Specializing the previous definitions and observations to undirected graphs, we get the following.</p>
<p>It will be convenient to allow self-loops, i.e., entry <span class="math notranslate nohighlight">\(a_{i,i}\)</span> of the adjacency matrix can be <span class="math notranslate nohighlight">\(1\)</span> for some <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>DEFINITION</strong> <strong>(Random Walk on a Graph)</strong> <span class="math notranslate nohighlight">\(\idx{random walk on a graph}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a graph. If a vertex is isolated, add a self-loop to it. A random walk on <span class="math notranslate nohighlight">\(G\)</span> is a time-homogeneous Markov chain <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> with state space <span class="math notranslate nohighlight">\(\mathcal{S} = V\)</span> and transition probabilities</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta(i)},
\qquad
\forall i \in V, j \in N(i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta(i)\)</span> is the degree of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(N(i) = \{j \in V: \{i,j\} \in E\}\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>As we have seen previously, the transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> satisfying the conditions of the definition above is <span class="math notranslate nohighlight">\(P = D^{-1} A\)</span>, where <span class="math notranslate nohighlight">\(D = \mathrm{diag}(A \mathbf{1})\)</span> is the degree matrix.</p>
<p>For instance, we have previously derived the transition matrix for random walk on the Petersen graph.</p>
<p>We specialize irreducibility to the case of random walk on a graph.</p>
<p><strong>LEMMA</strong> <strong>(Irreducibility)</strong> <span class="math notranslate nohighlight">\(\idx{irreducibility lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a graph. Random walk on <span class="math notranslate nohighlight">\(G\)</span> is irreducible if and only if <span class="math notranslate nohighlight">\(G\)</span> is connected. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> We only prove one direction. Suppose <span class="math notranslate nohighlight">\(G\)</span> is connected. Then between any two vertices <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> there is a sequence of vertices <span class="math notranslate nohighlight">\(z_0 = i, z_1, \ldots, z_r = j\)</span> such that <span class="math notranslate nohighlight">\(\{z_{\ell-1},z_\ell\} \in E\)</span> for all <span class="math notranslate nohighlight">\(\ell = 1,\ldots,r\)</span>. In particular, <span class="math notranslate nohighlight">\(a_{z_{\ell-1},z_\ell} &gt; 0\)</span> which implies <span class="math notranslate nohighlight">\(p_{z_{\ell-1},z_\ell} &gt; 0\)</span>. That proves irreducibility. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>By the previous lemma and the <em>Existence of a Stationary Distribution</em>, provided <span class="math notranslate nohighlight">\(G\)</span> is connected, it has a unique stationary distribution. It turns out to be straightforward to compute it as we see in the next subsection.</p>
<p><strong>Reversible chains</strong> A Markov chain is said to be reversible if it satisfies the detailed balance conditions.</p>
<p><strong>DEFINITION</strong> <strong>(Reversibility)</strong> <span class="math notranslate nohighlight">\(\idx{reversible}\xdi\)</span> A transition matrix <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span> is reversible with respect to a probability distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i=1}^n\)</span> if it satisfies the so-called detailed balance conditions</p>
<div class="math notranslate nohighlight">
\[
\pi_i p_{i,j} = \pi_j p_{j,i}, \qquad \forall i,j. 
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The next theorem explains why this definition is useful to us.</p>
<p><strong>THEOREM</strong> <strong>(Reversibility and Stationarity)</strong> <span class="math notranslate nohighlight">\(\idx{reversibility and stationarity theorem}\xdi\)</span>  Let <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span> be a transition matrix reversible with respect to a probability distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i=1}^n\)</span>. Then <span class="math notranslate nohighlight">\(\bpi\)</span> is a stationary distribution of <span class="math notranslate nohighlight">\(P\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> Just check the definition.</p>
<p><em>Proof:</em> For any <span class="math notranslate nohighlight">\(j\)</span>, by the definition of reversibility,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i} \pi_i p_{i,j}
= \sum_{i} \pi_j p_{j,i}
= \pi_j \sum_{i}  p_{j,i}
= \pi_j,
\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(P\)</span> is stochastic in the last equality. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We return to random walk on a graph. We show that it is reversible and derive the stationary dsitribution.</p>
<p><strong>THEOREM</strong> <strong>(Stationary Distribution on a Graph)</strong> <span class="math notranslate nohighlight">\(\idx{stationary distribution on a graph}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a graph. Assume further that <span class="math notranslate nohighlight">\(G\)</span> is connected. Then the unique stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in V.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We prove this in two parts. We first argue that <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i \in V}\)</span> is indeed a probability distribution. Then we show that the transition matrix <span class="math notranslate nohighlight">\(P\)</span> is reversible with respect to <span class="math notranslate nohighlight">\(\bpi\)</span>.</p>
<p><em>Proof:</em> We first show that <span class="math notranslate nohighlight">\(\bpi = (\pi_v)_{v \in V}\)</span> is a probability distribution. Its entries are non-negative by definition. Further</p>
<div class="math notranslate nohighlight">
\[
\sum_{i \in V} \pi_i
= \sum_{i \in V}\frac{\delta(i)}{\sum_{i \in V} \delta(i)}
= \frac{\sum_{i \in V} \delta(i)}{\sum_{i \in V} \delta(i)}
= 1.
\]</div>
<p>It remains to establish reversibility. For any <span class="math notranslate nohighlight">\(i, j\)</span>, by definition,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi_i p_{i,j}
&amp;= \frac{\delta(i)}{\sum_{i \in V} \delta(i)} \frac{a_{i,j}}{\sum_{k} a_{i,k}}\\
&amp;= \frac{\delta(i)}{\sum_{i \in V} \delta(i)} \frac{a_{i,j}}{\delta(i)}\\
&amp;= \frac{1}{\sum_{i \in V} \delta(i)} a_{i,j}.
\end{align*}\]</div>
<p>Changing the roles of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> gives the same expression since <span class="math notranslate nohighlight">\(a_{j,i} = a_{i,j}\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
<section id="pagerank">
<h2><span class="section-number">7.5.2. </span>PageRank<a class="headerlink" href="#pagerank" title="Link to this heading">#</a></h2>
<p>One is often interested in identifying central nodes in a network. Intuitively, they should correspond to entities (e.g., individuals or webpages depending on the network) that are particularly influential or authoritative. There are many ways of uncovering such nodes. Formally one defines a notion of <a class="reference external" href="https://en.wikipedia.org/wiki/Centrality">node centrality</a><span class="math notranslate nohighlight">\(\idx{node centrality}\xdi\)</span>, which ranks nodes by importance. Here we focus on one important such notion, PageRank. We will see that it is closely related to random walks on graphs.</p>
<p><strong>A notion of centrality for directed graphs</strong> We start with the directed case.</p>
<p>Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a digraph on <span class="math notranslate nohighlight">\(n\)</span> vertices. We seek to associate a measure of importance to each vertex. We will denote this (row) vector by</p>
<div class="math notranslate nohighlight">
\[
\mathbf{PR}
= (\mathrm{PR}_1, \ldots, \mathrm{PR}_n)^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{PR}\)</span> stands for PageRank<span class="math notranslate nohighlight">\(\idx{PageRank}\xdi\)</span>.</p>
<p>We posit that each vertex has a certain amount of influence associated to it and that it distributes that influence equally among the neighbors it points to. We seek a (row) vector <span class="math notranslate nohighlight">\(\mathbf{z} = (z_1,\ldots,z_n)^T\)</span> that satisfies an equation of the form</p>
<div class="math notranslate nohighlight">
\[
z_i 
= \sum_{j \in N^-(i)} z_j \frac{1}{\delta^+(j)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta^+(j) = |N^+(j)|\)</span> is the out-degree of <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(N^-(i)\)</span> is the set of vertices <span class="math notranslate nohighlight">\(j\)</span> with an edge <span class="math notranslate nohighlight">\((j,i)\)</span>. Observe that we explicitly take into account the direction of the edges. We think of an edge <span class="math notranslate nohighlight">\((j,i)\)</span> as an indication that <span class="math notranslate nohighlight">\(j\)</span> values <span class="math notranslate nohighlight">\(i\)</span>.</p>
<ul class="simple">
<li><p>On the web for instance, a link towards a page indicates that the destination page has information of value. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/PageRank">Wikipedia</a>:</p></li>
</ul>
<blockquote>
<div><p>PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.</p>
</div></blockquote>
<ul class="simple">
<li><p>On X (formerly known as Twitter), following an account is an indication that the latter is of interest.</p></li>
</ul>
<p>We have already encountered this set of equations. Consider random walk on the directed graph <span class="math notranslate nohighlight">\(G\)</span> (where a self-loop is added to each vertex without an outgoing edge). That is, at every step, we pick an outgoing edge of the current state uniformly at random. Then the transition matrix is</p>
<div class="math notranslate nohighlight">
\[
P = D^{-1} A,
\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the diagonal matrix with the out-degrees on its diagonal.</p>
<p>A stationary distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_1,\ldots,\pi_n)^T\)</span> is a row vector satisfying in this case</p>
<div class="math notranslate nohighlight">
\[
\pi_i
= \sum_{j=1}^n \pi_j p_{j,i}
= \sum_{j \in N^-(i)} \pi_j \frac{1}{\delta^+(j)}.
\]</div>
<p>So <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span>.</p>
<p>If the graph <span class="math notranslate nohighlight">\(G\)</span> is strongly connected, we know that there is a unique stationary distribution by the <em>Existence of a Stationary Distribution</em>. In many real-world digraphs, however, that assumption is not satisfied. To ensure that a meaningful solution can still be found, we modify the walk slightly.</p>
<p>To make the walk irreducible, we add a small probability at each step of landing at a uniformly chosen node. This is sometimes referred to as <em>teleporting</em>. That is, we define the transition matrix</p>
<div class="math notranslate nohighlight">
\[
Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, 
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span> known as the damping factor (or teleporting parameter). A typical choice is <span class="math notranslate nohighlight">\(\alpha = 0.85\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\frac{1}{n} \mathbf{1} \mathbf{1}^T\)</span> is a stochastic matrix. Indeed,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \mathbf{1} \mathbf{1}^T \mathbf{1}
= \frac{1}{n} \mathbf{1} n
= \mathbf{1}.
\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(Q\)</span> is a stochastic matrix (as a convex combination of stochastic matrices).</p>
<p>Moreover, <span class="math notranslate nohighlight">\(Q\)</span> is clearly irreducible since it is strictly positive. That is, for any <span class="math notranslate nohighlight">\(x, y \in [n]\)</span>, one can reach <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span> in a single step</p>
<div class="math notranslate nohighlight">
\[
Q_{x,y} = \alpha P_{x,y} + (1-\alpha) \frac{1}{n} &gt; 0.
\]</div>
<p>This also holds for <span class="math notranslate nohighlight">\(x = y\)</span> so the chain is lazy.</p>
<p>Finally, we define <span class="math notranslate nohighlight">\(\mathbf{PR}\)</span> as the unique stationary distribution of <span class="math notranslate nohighlight">\(Q = (q_{i,j})_{i,j=1}^n\)</span>, that is, the solution to</p>
<div class="math notranslate nohighlight">
\[
\mathrm{PR}_i
= \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i},
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{PR} \geq 0\)</span></p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \mathrm{PR}_i = 1.
\]</div>
<p>Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/PageRank">Wikipedia</a>:</p>
<blockquote>
<div><p>The formula uses a model of a random surfer who reaches their target site after several clicks, then switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. It can be understood as a Markov chain in which the states are pages, and the transitions are the links between pages – all of which are all equally probable.</p>
</div></blockquote>
<p>Here is an implementation of the PageRank algorithm. We will need a function that takes as input an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> and returns the corresponding transition matrix <span class="math notranslate nohighlight">\(P\)</span>. Some vertices have no outgoing links. To avoid dividing by <span class="math notranslate nohighlight">\(0\)</span>, we add a self-loop to <em>all vertices with out-degree <span class="math notranslate nohighlight">\(0\)</span></em>. We <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.fill_diagonal.html"><code class="docutils literal notranslate"><span class="pre">numpy.fill_diagonal</span></code></a> for this purpose. (The boolean values in <code class="docutils literal notranslate"><span class="pre">sinks</span></code> below automatically convert to <code class="docutils literal notranslate"><span class="pre">0.0</span></code> and <code class="docutils literal notranslate"><span class="pre">1.0</span></code> when used in the numerical matrix context. So this effectively adds self-loops only to sink nodes – nodes that had no outgoing edges now have a single outgoing edge pointing back to themselves.)</p>
<p>Also, because the adjacency matrix and the vector of out-degrees have different shapes, we turn <code class="docutils literal notranslate"><span class="pre">out_deg</span></code> into a column vector using <a class="reference external" href="https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis"><code class="docutils literal notranslate"><span class="pre">numpy.newaxis</span></code></a> to ensure that the division is <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html#broadcastable-arrays">done one column at a time</a>. (There are many ways of doing this, <a class="reference external" href="https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array">but some are slower than others</a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">transition_from_adjacency</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">sinks</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">==</span> <span class="mf">0.</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">sinks</span><span class="p">)</span>
    <span class="n">out_deg</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">/</span> <span class="n">out_deg</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">P</span>
</pre></div>
</div>
</div>
</div>
<p>The following function adds the damping factor. Here <code class="docutils literal notranslate"><span class="pre">mu</span></code> will be the uniform distribution. It gets added (after scaling by <code class="docutils literal notranslate"><span class="pre">1-alpha</span></code>) one row at a time to <code class="docutils literal notranslate"><span class="pre">P</span></code> (again after scaling by <code class="docutils literal notranslate"><span class="pre">alpha</span></code>). This time we do not need to reshape <code class="docutils literal notranslate"><span class="pre">mu</span></code> as the sum is done one row at a time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">add_damping</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">P</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu</span>
    <span class="k">return</span> <span class="n">Q</span>
</pre></div>
</div>
</div>
</div>
<p>When computing PageRank, we multiply from the left.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">pagerank</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">mu</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">@</span> <span class="n">Q</span>
    
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> Let’s try a star with edges pointing out. Along the way, we check that our functions work how we expect them to.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">G_outstar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">G_outstar</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_outstar</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> 
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/660932284bae2affdddf5662aeec676a1cf3903af81ef0cecb91e479b6585e26.png" src="../Images/ff153b67326802af34ceb69f23f4985b.png" data-original-src="https://mmids-textbook.github.io/_images/660932284bae2affdddf5662aeec676a1cf3903af81ef0cecb91e479b6585e26.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_outstar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_outstar</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_outstar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 1 1 1 1 1 1 1]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<p>We compute the matrices <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>. We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html"><code class="docutils literal notranslate"><span class="pre">numpy.set_printoptions</span></code></a> to condense the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">P_outstar</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A_outstar</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">formatter</span><span class="o">=</span><span class="p">{</span><span class="s1">'float'</span><span class="p">:</span> <span class="s1">'</span><span class="si">{: 0.3f}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P_outstar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.000  0.143  0.143  0.143  0.143  0.143  0.143  0.143]
 [ 0.000  1.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 0.000  0.000  1.000  0.000  0.000  0.000  0.000  0.000]
 [ 0.000  0.000  0.000  1.000  0.000  0.000  0.000  0.000]
 [ 0.000  0.000  0.000  0.000  1.000  0.000  0.000  0.000]
 [ 0.000  0.000  0.000  0.000  0.000  1.000  0.000  0.000]
 [ 0.000  0.000  0.000  0.000  0.000  0.000  1.000  0.000]
 [ 0.000  0.000  0.000  0.000  0.000  0.000  0.000  1.000]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.85</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
<span class="n">Q_outstar</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P_outstar</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q_outstar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.019  0.140  0.140  0.140  0.140  0.140  0.140  0.140]
 [ 0.019  0.869  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.019  0.019  0.869  0.019  0.019  0.019  0.019  0.019]
 [ 0.019  0.019  0.019  0.869  0.019  0.019  0.019  0.019]
 [ 0.019  0.019  0.019  0.019  0.869  0.019  0.019  0.019]
 [ 0.019  0.019  0.019  0.019  0.019  0.869  0.019  0.019]
 [ 0.019  0.019  0.019  0.019  0.019  0.019  0.869  0.019]
 [ 0.019  0.019  0.019  0.019  0.019  0.019  0.019  0.869]]
</pre></div>
</div>
</div>
</div>
<p>While it is tempting to guess that <span class="math notranslate nohighlight">\(1\)</span> is the most central node of the network, no edge actually points to it. In this case, the center of the star has a low PageRank value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_outstar</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.019  0.140  0.140  0.140  0.140  0.140  0.140  0.140]
</pre></div>
</div>
</div>
</div>
<p>We then try a star with edges pointing in.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">G_instar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G_instar</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">G_instar</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_instar</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> 
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a2444432691b17ff6593a677e80f8c88415d65748ccba7a1b3b861b173f655f2.png" src="../Images/d644ec50527bfb74965fed98a96f2683.png" data-original-src="https://mmids-textbook.github.io/_images/a2444432691b17ff6593a677e80f8c88415d65748ccba7a1b3b861b173f655f2.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_instar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_instar</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_instar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">P_instar</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A_instar</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P_instar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">Q_instar</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P_instar</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q_instar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]]
</pre></div>
</div>
</div>
</div>
<p>In this case, the center of the star does indeed have a high PageRank value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_instar</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>A notion of centrality for undirected graphs</strong> We can apply PageRank<span class="math notranslate nohighlight">\(\idx{PageRank}\xdi\)</span> in the undirected case as well.</p>
<p>Consider random walk on the undirected graph <span class="math notranslate nohighlight">\(G\)</span>. That is, at every step, we pick a neighbor of the current state uniformly at random. If needed, add a self-loop to any isolated vertex. Then the transition matrix is</p>
<div class="math notranslate nohighlight">
\[
P = D^{-1} A,
\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the degree matrix and <span class="math notranslate nohighlight">\(A\)</span> is the adjacency matrix. A stationary distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_1,\ldots,\pi_n)^T\)</span> is a row vector satisfying in this case</p>
<div class="math notranslate nohighlight">
\[
\pi_i
= \sum_{j=1}^n \pi_j p_{j,i}
= \sum_{j \in N(i)} \pi_j \frac{1}{\delta(j)}.
\]</div>
<p>We already know the solution to this system of equations. In the connected case without damping, the unique stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in V.
\]</div>
<p>In words, the centrality of a node is directly proportional to its degree, i.e., how many neighbors it has. Up to the scaling factor, this is known as <a class="reference external" href="https://en.wikipedia.org/wiki/Centrality#Degree_centrality">degree centrality</a><span class="math notranslate nohighlight">\(\idx{degree centrality}\xdi\)</span>.</p>
<p>For a general undirected graph that may not be connected, we can use a damping factor to enforce irreducibility. We add a small probability at each step of landing at a uniformly chosen node. That is, we define the transition matrix</p>
<div class="math notranslate nohighlight">
\[
Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, 
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span> known as the damping factor. We define the PageRank vector <span class="math notranslate nohighlight">\(\mathbf{PR}\)</span> as the unique stationary distribution of <span class="math notranslate nohighlight">\(Q = (q_{i,j})_{i,j=1}^n\)</span>, that is, the solution to</p>
<div class="math notranslate nohighlight">
\[
\mathrm{PR}_i
= \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i},
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{PR} \geq 0\)</span></p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \mathrm{PR}_i = 1.
\]</div>
<p><strong>NUMERICAL CORNER:</strong> We revisit the star example in the undirected case.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">G_star</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">G_star</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
    
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_star</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> 
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c7f2056453edc830e099413bfdb90e0047d36da8030c2b8abf8be4198801e5fc.png" src="../Images/2d0d2764fb0069741155c9c91833c00b.png" data-original-src="https://mmids-textbook.github.io/_images/c7f2056453edc830e099413bfdb90e0047d36da8030c2b8abf8be4198801e5fc.png"/>
</div>
</div>
<p>We first compute the PageRank vector without damping. Here the random walk is periodic (Why?) so power iteration may fail (Try it!). Instead, we use a small amount of damping and increase the number of iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_star</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_star</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_star</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 1 1 1 1 1 1 1]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_star</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.999</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.500  0.071  0.071  0.071  0.071  0.071  0.071  0.071]
</pre></div>
</div>
</div>
</div>
<p>The PageRank value for the center node is indeed roughly <span class="math notranslate nohighlight">\(7\)</span> times larger than the other ones, as can be expected from the ratio of their degrees.</p>
<p>We try again with more damping. This time the ratio of PageRank values is not quite the same as the ratio of degrees, but the center node continues to have a higher value than the other nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_star</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.470  0.076  0.076  0.076  0.076  0.076  0.076  0.076]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> There are many other centrality measures besides PageRank, such as betweenness centrality, closeness centrality, and eigenvector centrality. Ask your favorite AI chatbot to explain these measures and discuss their similarities and differences with PageRank. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="personalized-pagerank">
<h2><span class="section-number">7.5.3. </span>Personalized PageRank<a class="headerlink" href="#personalized-pagerank" title="Link to this heading">#</a></h2>
<p>We return to <a class="reference external" href="https://mathworld.wolfram.com">MathWorld</a> dataset. Recall that each page of MathWorld concerns a particular mathematical concept. In a section entitled “SEE ALSO”, other related mathematical concepts are listed with a link to their MathWorld page. Our goal is to identify “central” vertices in the resulting graph.</p>
<p><strong>Figure:</strong> Platonic solids (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Platonic solids" src="../Images/4393ac7f9684a7e62d5f82de15668d37.png" data-original-src="https://mmids-textbook.github.io/_images/small-platonic_solids.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We load the dataset again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data_edges</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'mathworld-adjacency.csv'</span><span class="p">)</span>
<span class="n">data_edges</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>from</th>
      <th>to</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>47</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>404</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>2721</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The second file contains the titles of the pages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data_titles</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'mathworld-titles.csv'</span><span class="p">)</span>
<span class="n">data_titles</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Alexander's Horned Sphere</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Exotic Sphere</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Antoine's Horned Sphere</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Flat</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Poincaré Manifold</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We construct the graph by adding the edges one by one. We first convert <code class="docutils literal notranslate"><span class="pre">df_edges</span></code> into a NumPy array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">edgelist</span> <span class="o">=</span> <span class="n">data_edges</span><span class="p">[[</span><span class="s1">'from'</span><span class="p">,</span><span class="s1">'to'</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">edgelist</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[    0     2]
 [    1    47]
 [    1   404]
 ...
 [12361 12306]
 [12361 12310]
 [12361 12360]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">12362</span>
<span class="n">G_mw</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">empty_graph</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">create_using</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">edgelist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">G_mw</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">edgelist</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">edgelist</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>To apply PageRank, we construct the adjacency matric of the graph. We also define a vector of title pages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_mw</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_mw</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">titles_mw</span> <span class="o">=</span> <span class="n">data_titles</span><span class="p">[</span><span class="s1">'title'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">pr_mw</span> <span class="o">=</span> <span class="n">pagerank</span><span class="p">(</span><span class="n">A_mw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.argsort.html"><code class="docutils literal notranslate"><span class="pre">numpy.argsort</span></code></a> to identify the pages with highest scores. We apply it to <code class="docutils literal notranslate"><span class="pre">-pr_mw</span></code> to sort from the highest to lowest value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">top_pages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">pr_mw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The top 25 topics are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">titles_mw</span><span class="p">[</span><span class="n">top_pages</span><span class="p">[:</span><span class="mi">25</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['Sphere' 'Circle' 'Prime Number' 'Aleksandrov-Čech Cohomology'
 'Centroid Hexagon' 'Group' 'Fourier Transform' 'Tree' 'Splitting Field'
 'Archimedean Solid' 'Normal Distribution' 'Integer Sequence Primes'
 'Perimeter Polynomial' 'Polygon' 'Finite Group' 'Large Number'
 'Riemann Zeta Function' 'Chebyshev Approximation Formula' 'Vector' 'Ring'
 'Fibonacci Number' 'Conic Section' 'Fourier Series' 'Derivative'
 'Gamma Function']
</pre></div>
</div>
</div>
</div>
<p>We indeed get a list of central concepts in mathematics – including several we have encountered previously such as <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">Distribution</span></code>, <code class="docutils literal notranslate"><span class="pre">Tree</span></code>, <code class="docutils literal notranslate"><span class="pre">Vector</span></code> or <code class="docutils literal notranslate"><span class="pre">Derivative</span></code>.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>There is a variant of PageRank, referred to as Personalized PageRank (PPR)<span class="math notranslate nohighlight">\(\idx{Personalized PageRank}\xdi\)</span>, which aims to tailor the outcome to specific interests. This is accomplished from a simple change to the algorithm. When teleporting, rather than jumping to a uniformly random page, we instead jump to an arbitrary distribution which is meant to capture some specific interests. In the context of the web for instance, this distribution might be uniform over someone’s bookmarks.</p>
<p>We adapt <code class="docutils literal notranslate"><span class="pre">pagerank</span></code> as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">ppr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">mu</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">@</span> <span class="n">Q</span>
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> To test PPR, consider the distribution concentrated on a single topic <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">Distribution</span></code>. This is topic number <code class="docutils literal notranslate"><span class="pre">1270</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">titles_mw</span> <span class="o">==</span> <span class="s1">'Normal Distribution'</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1270
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">mu</span><span class="p">[</span><span class="mi">1270</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>We now run PPR and list the top 25 pages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ppr_mw</span> <span class="o">=</span> <span class="n">ppr</span><span class="p">(</span><span class="n">A_mw</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
<span class="n">top_pers_pages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">ppr_mw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The top 25 topics are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">titles_mw</span><span class="p">[</span><span class="n">top_pers_pages</span><span class="p">[:</span><span class="mi">25</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['Normal Distribution' 'Pearson System' 'Logit Transformation' 'z-Score'
 'Erf' 'Central Limit Theorem' 'Bivariate Normal Distribution'
 'Normal Ratio Distribution' 'Normal Sum Distribution'
 'Normal Distribution Function' 'Gaussian Function'
 'Standard Normal Distribution' 'Normal Product Distribution'
 'Binomial Distribution' 'Tetrachoric Function' 'Ratio Distribution'
 'Kolmogorov-Smirnov Test' 'Box-Muller Transformation' 'Galton Board'
 'Fisher-Behrens Problem' 'Erfc' 'Normal Difference Distribution'
 'Half-Normal Distribution' 'Inverse Gaussian Distribution'
 'Error Function Distribution']
</pre></div>
</div>
</div>
</div>
<p>This indeed returns various statistical concepts, particularly related to the normal dsitribution.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The PageRank algorithm has been adapted for various applications beyond web search, such as ranking scientific papers, analyzing social networks, and even ranking sports teams [<a class="reference external" href="https://arxiv.org/abs/1407.5107">Gle</a>]. Ask your favorite AI chatbot to discuss some of these applications and how the PageRank algorithm is modified for each case. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Consider a random walk on the following graph:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
</pre></div>
</div>
<p>What is the transition matrix for this random walk?</p>
<p>a)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1/3 &amp; 1/3 &amp; 1/3 \\ 1/3 &amp; 1/3 &amp; 1/3 \\ 1/3 &amp; 1/3 &amp; 1/3 \end{bmatrix}
\end{split}\]</div>
<p>b)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 0 &amp; 1/2 &amp; 1/2 \\ 1/2 &amp; 0 &amp; 1/2 \\ 1/2 &amp; 1/2 &amp; 0 \end{bmatrix}
\end{split}\]</div>
<p>c)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<p>d)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \end{bmatrix}
\end{split}\]</div>
<p><strong>2</strong> In a random walk on a directed graph, the transition probability from vertex <span class="math notranslate nohighlight">\(i\)</span> to vertex <span class="math notranslate nohighlight">\(j\)</span> is given by:</p>
<p>a) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^-(j)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^-(i)\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^+(j)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^+(i)\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^-(i)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^-(i)\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^+(i)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^+(i)\)</span></p>
<p><strong>3</strong> The transition matrix <span class="math notranslate nohighlight">\(P\)</span> of a random walk on a directed graph can be expressed in terms of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> as:</p>
<p>a) <span class="math notranslate nohighlight">\(P = AD^{-1}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(P = A^TD^{-1}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(P = D^{-1}A\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(P = D^{-1}A^T\)</span></p>
<p><strong>4</strong> In a random walk on an undirected graph, the stationary distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> satisfies:</p>
<p>a) <span class="math notranslate nohighlight">\(\pi_i = \frac{\delta^+(i)}{\sum_{j \in V} \delta^+(j)}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\pi_i = \frac{1}{\delta(i)}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\pi_i = \frac{1}{|V|}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\pi_i = \frac{\delta(i)}{\sum_{j \in V} \delta(j)}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p><strong>5</strong> Personalized PageRank differs from standard PageRank in that:</p>
<p>a) It considers the user’s browsing history</p>
<p>b) It jumps to a non-uniform distribution when teleporting</p>
<p>c) It uses a different damping factor</p>
<p>d) It only considers a subset of the graph</p>
<p>Answer for 1: b. Justification: Each node has a degree of 2, and the probability of transitioning to each neighbor is 1/2.</p>
<p>Answer for 2: d. Justification: The text states, “In words, at each step, we choose an outgoing edge from the current state uniformly at random,” which corresponds to the transition probability <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^+(i)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^+(i)\)</span>, where <span class="math notranslate nohighlight">\(\delta^+(i)\)</span> is the out-degree of vertex <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(N^+(i)\)</span> is the set of vertices with an edge from <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Answer for 3: c. Justification: The text states, “The transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> satisfying the conditions of the definition above is <span class="math notranslate nohighlight">\(P = D^{-1}A\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the out-degree matrix.”</p>
<p>Answer for 4: d. Justification: The text states, “In the connected case without damping, the unique stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span> is given by <span class="math notranslate nohighlight">\(\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \forall i \in V\)</span>.”</p>
<p>Answer for 5: b. Justification: The text states, “When teleporting, rather than jumping to a uniformly random page, we instead jump to an arbitrary distribution <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> which is meant to capture some specific interests.”</p>
</section>
&#13;

<h2><span class="section-number">7.5.1. </span>Random walk on a graph<a class="headerlink" href="#random-walk-on-a-graph" title="Link to this heading">#</a></h2>
<p>We first specialize the theory of the previous section to random walks on graphs. We start with the case of digraphs. The undirected case leads to useful simplifications.</p>
<p><strong>Directed case</strong> We first define a random walk on a digraph.</p>
<p><strong>DEFINITION</strong> <strong>(Random Walk on a Digraph)</strong> <span class="math notranslate nohighlight">\(\idx{random walk on a graph}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a directed graph. If a vertex does not have an outgoing edge (i.e., an edge with it as its source), add a self-loop to it. A random walk on <span class="math notranslate nohighlight">\(G\)</span> is a time-homogeneous Markov chain <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> with state space <span class="math notranslate nohighlight">\(\mathcal{S} = V\)</span> and transition probabilities</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta^+(i)},
\qquad
\forall i \in V, j \in N^+(i),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta^+(i)\)</span> is the out-degree of <span class="math notranslate nohighlight">\(i\)</span>, i.e., the number of outgoing edges and <span class="math notranslate nohighlight">\(N^+(i) = \{j \in V:(i,j) \in E\}\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In words, at each step, we choose an outgoing edge from the current state uniformly at random. Choosing a self-loop (i.e., an edge of the form <span class="math notranslate nohighlight">\((i,i)\)</span>) means staying where we are.</p>
<p>Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a digraph with <span class="math notranslate nohighlight">\(n = |V|\)</span> vertices. Without loss of generality, we let the vertex set be <span class="math notranslate nohighlight">\([n] = \{1,\ldots,n\}\)</span>. The adjacency matrix of <span class="math notranslate nohighlight">\(G\)</span> is denoted as <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j}\)</span>. We define the out-degree matrix <span class="math notranslate nohighlight">\(D\)</span> as the diagonal matrix with diagonal entries <span class="math notranslate nohighlight">\(\delta^+(i)\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>. That is,</p>
<div class="math notranslate nohighlight">
\[
D = \mathrm{diag}(A \mathbf{1}).
\]</div>
<p><strong>LEMMA</strong> <strong>(Transition Matrix in Terms of Adjacency)</strong> <span class="math notranslate nohighlight">\(\idx{transition matrix in terms of adjacency}\xdi\)</span> The transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> satisfying the conditions of the definition above is</p>
<div class="math notranslate nohighlight">
\[
P = D^{-1} A.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The formula follows immediately from the definition. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We specialize irreducibility to the case of random walk on a digraph.</p>
<p><strong>LEMMA</strong> <strong>(Irreducibility)</strong> <span class="math notranslate nohighlight">\(\idx{irreducibility lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a digraph. Random walk on <span class="math notranslate nohighlight">\(G\)</span> is irreducible if and only if <span class="math notranslate nohighlight">\(G\)</span> is strongly connected. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Simply note that the transition graph of the walk is <span class="math notranslate nohighlight">\(G\)</span> itself. We have seen previously that irreducibility is equivalent to the transition graph being strongly connected. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>In the undirected case, more structure emerges as we detail next.</p>
<p><strong>Undirected case</strong> Specializing the previous definitions and observations to undirected graphs, we get the following.</p>
<p>It will be convenient to allow self-loops, i.e., entry <span class="math notranslate nohighlight">\(a_{i,i}\)</span> of the adjacency matrix can be <span class="math notranslate nohighlight">\(1\)</span> for some <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>DEFINITION</strong> <strong>(Random Walk on a Graph)</strong> <span class="math notranslate nohighlight">\(\idx{random walk on a graph}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a graph. If a vertex is isolated, add a self-loop to it. A random walk on <span class="math notranslate nohighlight">\(G\)</span> is a time-homogeneous Markov chain <span class="math notranslate nohighlight">\((X_t)_{t \geq 0}\)</span> with state space <span class="math notranslate nohighlight">\(\mathcal{S} = V\)</span> and transition probabilities</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta(i)},
\qquad
\forall i \in V, j \in N(i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta(i)\)</span> is the degree of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(N(i) = \{j \in V: \{i,j\} \in E\}\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>As we have seen previously, the transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> satisfying the conditions of the definition above is <span class="math notranslate nohighlight">\(P = D^{-1} A\)</span>, where <span class="math notranslate nohighlight">\(D = \mathrm{diag}(A \mathbf{1})\)</span> is the degree matrix.</p>
<p>For instance, we have previously derived the transition matrix for random walk on the Petersen graph.</p>
<p>We specialize irreducibility to the case of random walk on a graph.</p>
<p><strong>LEMMA</strong> <strong>(Irreducibility)</strong> <span class="math notranslate nohighlight">\(\idx{irreducibility lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a graph. Random walk on <span class="math notranslate nohighlight">\(G\)</span> is irreducible if and only if <span class="math notranslate nohighlight">\(G\)</span> is connected. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> We only prove one direction. Suppose <span class="math notranslate nohighlight">\(G\)</span> is connected. Then between any two vertices <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> there is a sequence of vertices <span class="math notranslate nohighlight">\(z_0 = i, z_1, \ldots, z_r = j\)</span> such that <span class="math notranslate nohighlight">\(\{z_{\ell-1},z_\ell\} \in E\)</span> for all <span class="math notranslate nohighlight">\(\ell = 1,\ldots,r\)</span>. In particular, <span class="math notranslate nohighlight">\(a_{z_{\ell-1},z_\ell} &gt; 0\)</span> which implies <span class="math notranslate nohighlight">\(p_{z_{\ell-1},z_\ell} &gt; 0\)</span>. That proves irreducibility. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>By the previous lemma and the <em>Existence of a Stationary Distribution</em>, provided <span class="math notranslate nohighlight">\(G\)</span> is connected, it has a unique stationary distribution. It turns out to be straightforward to compute it as we see in the next subsection.</p>
<p><strong>Reversible chains</strong> A Markov chain is said to be reversible if it satisfies the detailed balance conditions.</p>
<p><strong>DEFINITION</strong> <strong>(Reversibility)</strong> <span class="math notranslate nohighlight">\(\idx{reversible}\xdi\)</span> A transition matrix <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span> is reversible with respect to a probability distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i=1}^n\)</span> if it satisfies the so-called detailed balance conditions</p>
<div class="math notranslate nohighlight">
\[
\pi_i p_{i,j} = \pi_j p_{j,i}, \qquad \forall i,j. 
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The next theorem explains why this definition is useful to us.</p>
<p><strong>THEOREM</strong> <strong>(Reversibility and Stationarity)</strong> <span class="math notranslate nohighlight">\(\idx{reversibility and stationarity theorem}\xdi\)</span>  Let <span class="math notranslate nohighlight">\(P = (p_{i,j})_{i,j=1}^n\)</span> be a transition matrix reversible with respect to a probability distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i=1}^n\)</span>. Then <span class="math notranslate nohighlight">\(\bpi\)</span> is a stationary distribution of <span class="math notranslate nohighlight">\(P\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> Just check the definition.</p>
<p><em>Proof:</em> For any <span class="math notranslate nohighlight">\(j\)</span>, by the definition of reversibility,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i} \pi_i p_{i,j}
= \sum_{i} \pi_j p_{j,i}
= \pi_j \sum_{i}  p_{j,i}
= \pi_j,
\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(P\)</span> is stochastic in the last equality. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We return to random walk on a graph. We show that it is reversible and derive the stationary dsitribution.</p>
<p><strong>THEOREM</strong> <strong>(Stationary Distribution on a Graph)</strong> <span class="math notranslate nohighlight">\(\idx{stationary distribution on a graph}\xdi\)</span> Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a graph. Assume further that <span class="math notranslate nohighlight">\(G\)</span> is connected. Then the unique stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in V.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We prove this in two parts. We first argue that <span class="math notranslate nohighlight">\(\bpi = (\pi_i)_{i \in V}\)</span> is indeed a probability distribution. Then we show that the transition matrix <span class="math notranslate nohighlight">\(P\)</span> is reversible with respect to <span class="math notranslate nohighlight">\(\bpi\)</span>.</p>
<p><em>Proof:</em> We first show that <span class="math notranslate nohighlight">\(\bpi = (\pi_v)_{v \in V}\)</span> is a probability distribution. Its entries are non-negative by definition. Further</p>
<div class="math notranslate nohighlight">
\[
\sum_{i \in V} \pi_i
= \sum_{i \in V}\frac{\delta(i)}{\sum_{i \in V} \delta(i)}
= \frac{\sum_{i \in V} \delta(i)}{\sum_{i \in V} \delta(i)}
= 1.
\]</div>
<p>It remains to establish reversibility. For any <span class="math notranslate nohighlight">\(i, j\)</span>, by definition,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi_i p_{i,j}
&amp;= \frac{\delta(i)}{\sum_{i \in V} \delta(i)} \frac{a_{i,j}}{\sum_{k} a_{i,k}}\\
&amp;= \frac{\delta(i)}{\sum_{i \in V} \delta(i)} \frac{a_{i,j}}{\delta(i)}\\
&amp;= \frac{1}{\sum_{i \in V} \delta(i)} a_{i,j}.
\end{align*}\]</div>
<p>Changing the roles of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> gives the same expression since <span class="math notranslate nohighlight">\(a_{j,i} = a_{i,j}\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
&#13;

<h2><span class="section-number">7.5.2. </span>PageRank<a class="headerlink" href="#pagerank" title="Link to this heading">#</a></h2>
<p>One is often interested in identifying central nodes in a network. Intuitively, they should correspond to entities (e.g., individuals or webpages depending on the network) that are particularly influential or authoritative. There are many ways of uncovering such nodes. Formally one defines a notion of <a class="reference external" href="https://en.wikipedia.org/wiki/Centrality">node centrality</a><span class="math notranslate nohighlight">\(\idx{node centrality}\xdi\)</span>, which ranks nodes by importance. Here we focus on one important such notion, PageRank. We will see that it is closely related to random walks on graphs.</p>
<p><strong>A notion of centrality for directed graphs</strong> We start with the directed case.</p>
<p>Let <span class="math notranslate nohighlight">\(G = (V,E)\)</span> be a digraph on <span class="math notranslate nohighlight">\(n\)</span> vertices. We seek to associate a measure of importance to each vertex. We will denote this (row) vector by</p>
<div class="math notranslate nohighlight">
\[
\mathbf{PR}
= (\mathrm{PR}_1, \ldots, \mathrm{PR}_n)^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{PR}\)</span> stands for PageRank<span class="math notranslate nohighlight">\(\idx{PageRank}\xdi\)</span>.</p>
<p>We posit that each vertex has a certain amount of influence associated to it and that it distributes that influence equally among the neighbors it points to. We seek a (row) vector <span class="math notranslate nohighlight">\(\mathbf{z} = (z_1,\ldots,z_n)^T\)</span> that satisfies an equation of the form</p>
<div class="math notranslate nohighlight">
\[
z_i 
= \sum_{j \in N^-(i)} z_j \frac{1}{\delta^+(j)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta^+(j) = |N^+(j)|\)</span> is the out-degree of <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(N^-(i)\)</span> is the set of vertices <span class="math notranslate nohighlight">\(j\)</span> with an edge <span class="math notranslate nohighlight">\((j,i)\)</span>. Observe that we explicitly take into account the direction of the edges. We think of an edge <span class="math notranslate nohighlight">\((j,i)\)</span> as an indication that <span class="math notranslate nohighlight">\(j\)</span> values <span class="math notranslate nohighlight">\(i\)</span>.</p>
<ul class="simple">
<li><p>On the web for instance, a link towards a page indicates that the destination page has information of value. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/PageRank">Wikipedia</a>:</p></li>
</ul>
<blockquote>
<div><p>PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.</p>
</div></blockquote>
<ul class="simple">
<li><p>On X (formerly known as Twitter), following an account is an indication that the latter is of interest.</p></li>
</ul>
<p>We have already encountered this set of equations. Consider random walk on the directed graph <span class="math notranslate nohighlight">\(G\)</span> (where a self-loop is added to each vertex without an outgoing edge). That is, at every step, we pick an outgoing edge of the current state uniformly at random. Then the transition matrix is</p>
<div class="math notranslate nohighlight">
\[
P = D^{-1} A,
\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the diagonal matrix with the out-degrees on its diagonal.</p>
<p>A stationary distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_1,\ldots,\pi_n)^T\)</span> is a row vector satisfying in this case</p>
<div class="math notranslate nohighlight">
\[
\pi_i
= \sum_{j=1}^n \pi_j p_{j,i}
= \sum_{j \in N^-(i)} \pi_j \frac{1}{\delta^+(j)}.
\]</div>
<p>So <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span>.</p>
<p>If the graph <span class="math notranslate nohighlight">\(G\)</span> is strongly connected, we know that there is a unique stationary distribution by the <em>Existence of a Stationary Distribution</em>. In many real-world digraphs, however, that assumption is not satisfied. To ensure that a meaningful solution can still be found, we modify the walk slightly.</p>
<p>To make the walk irreducible, we add a small probability at each step of landing at a uniformly chosen node. This is sometimes referred to as <em>teleporting</em>. That is, we define the transition matrix</p>
<div class="math notranslate nohighlight">
\[
Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, 
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span> known as the damping factor (or teleporting parameter). A typical choice is <span class="math notranslate nohighlight">\(\alpha = 0.85\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\frac{1}{n} \mathbf{1} \mathbf{1}^T\)</span> is a stochastic matrix. Indeed,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \mathbf{1} \mathbf{1}^T \mathbf{1}
= \frac{1}{n} \mathbf{1} n
= \mathbf{1}.
\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(Q\)</span> is a stochastic matrix (as a convex combination of stochastic matrices).</p>
<p>Moreover, <span class="math notranslate nohighlight">\(Q\)</span> is clearly irreducible since it is strictly positive. That is, for any <span class="math notranslate nohighlight">\(x, y \in [n]\)</span>, one can reach <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span> in a single step</p>
<div class="math notranslate nohighlight">
\[
Q_{x,y} = \alpha P_{x,y} + (1-\alpha) \frac{1}{n} &gt; 0.
\]</div>
<p>This also holds for <span class="math notranslate nohighlight">\(x = y\)</span> so the chain is lazy.</p>
<p>Finally, we define <span class="math notranslate nohighlight">\(\mathbf{PR}\)</span> as the unique stationary distribution of <span class="math notranslate nohighlight">\(Q = (q_{i,j})_{i,j=1}^n\)</span>, that is, the solution to</p>
<div class="math notranslate nohighlight">
\[
\mathrm{PR}_i
= \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i},
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{PR} \geq 0\)</span></p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \mathrm{PR}_i = 1.
\]</div>
<p>Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/PageRank">Wikipedia</a>:</p>
<blockquote>
<div><p>The formula uses a model of a random surfer who reaches their target site after several clicks, then switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. It can be understood as a Markov chain in which the states are pages, and the transitions are the links between pages – all of which are all equally probable.</p>
</div></blockquote>
<p>Here is an implementation of the PageRank algorithm. We will need a function that takes as input an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> and returns the corresponding transition matrix <span class="math notranslate nohighlight">\(P\)</span>. Some vertices have no outgoing links. To avoid dividing by <span class="math notranslate nohighlight">\(0\)</span>, we add a self-loop to <em>all vertices with out-degree <span class="math notranslate nohighlight">\(0\)</span></em>. We <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.fill_diagonal.html"><code class="docutils literal notranslate"><span class="pre">numpy.fill_diagonal</span></code></a> for this purpose. (The boolean values in <code class="docutils literal notranslate"><span class="pre">sinks</span></code> below automatically convert to <code class="docutils literal notranslate"><span class="pre">0.0</span></code> and <code class="docutils literal notranslate"><span class="pre">1.0</span></code> when used in the numerical matrix context. So this effectively adds self-loops only to sink nodes – nodes that had no outgoing edges now have a single outgoing edge pointing back to themselves.)</p>
<p>Also, because the adjacency matrix and the vector of out-degrees have different shapes, we turn <code class="docutils literal notranslate"><span class="pre">out_deg</span></code> into a column vector using <a class="reference external" href="https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis"><code class="docutils literal notranslate"><span class="pre">numpy.newaxis</span></code></a> to ensure that the division is <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html#broadcastable-arrays">done one column at a time</a>. (There are many ways of doing this, <a class="reference external" href="https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array">but some are slower than others</a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">transition_from_adjacency</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">sinks</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">==</span> <span class="mf">0.</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">sinks</span><span class="p">)</span>
    <span class="n">out_deg</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">/</span> <span class="n">out_deg</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">P</span>
</pre></div>
</div>
</div>
</div>
<p>The following function adds the damping factor. Here <code class="docutils literal notranslate"><span class="pre">mu</span></code> will be the uniform distribution. It gets added (after scaling by <code class="docutils literal notranslate"><span class="pre">1-alpha</span></code>) one row at a time to <code class="docutils literal notranslate"><span class="pre">P</span></code> (again after scaling by <code class="docutils literal notranslate"><span class="pre">alpha</span></code>). This time we do not need to reshape <code class="docutils literal notranslate"><span class="pre">mu</span></code> as the sum is done one row at a time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">add_damping</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">P</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu</span>
    <span class="k">return</span> <span class="n">Q</span>
</pre></div>
</div>
</div>
</div>
<p>When computing PageRank, we multiply from the left.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">pagerank</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">mu</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">@</span> <span class="n">Q</span>
    
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> Let’s try a star with edges pointing out. Along the way, we check that our functions work how we expect them to.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">G_outstar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">G_outstar</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_outstar</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> 
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/660932284bae2affdddf5662aeec676a1cf3903af81ef0cecb91e479b6585e26.png" src="../Images/ff153b67326802af34ceb69f23f4985b.png" data-original-src="https://mmids-textbook.github.io/_images/660932284bae2affdddf5662aeec676a1cf3903af81ef0cecb91e479b6585e26.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_outstar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_outstar</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_outstar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 1 1 1 1 1 1 1]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<p>We compute the matrices <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>. We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html"><code class="docutils literal notranslate"><span class="pre">numpy.set_printoptions</span></code></a> to condense the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">P_outstar</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A_outstar</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">formatter</span><span class="o">=</span><span class="p">{</span><span class="s1">'float'</span><span class="p">:</span> <span class="s1">'</span><span class="si">{: 0.3f}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P_outstar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.000  0.143  0.143  0.143  0.143  0.143  0.143  0.143]
 [ 0.000  1.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 0.000  0.000  1.000  0.000  0.000  0.000  0.000  0.000]
 [ 0.000  0.000  0.000  1.000  0.000  0.000  0.000  0.000]
 [ 0.000  0.000  0.000  0.000  1.000  0.000  0.000  0.000]
 [ 0.000  0.000  0.000  0.000  0.000  1.000  0.000  0.000]
 [ 0.000  0.000  0.000  0.000  0.000  0.000  1.000  0.000]
 [ 0.000  0.000  0.000  0.000  0.000  0.000  0.000  1.000]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.85</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
<span class="n">Q_outstar</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P_outstar</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q_outstar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.019  0.140  0.140  0.140  0.140  0.140  0.140  0.140]
 [ 0.019  0.869  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.019  0.019  0.869  0.019  0.019  0.019  0.019  0.019]
 [ 0.019  0.019  0.019  0.869  0.019  0.019  0.019  0.019]
 [ 0.019  0.019  0.019  0.019  0.869  0.019  0.019  0.019]
 [ 0.019  0.019  0.019  0.019  0.019  0.869  0.019  0.019]
 [ 0.019  0.019  0.019  0.019  0.019  0.019  0.869  0.019]
 [ 0.019  0.019  0.019  0.019  0.019  0.019  0.019  0.869]]
</pre></div>
</div>
</div>
</div>
<p>While it is tempting to guess that <span class="math notranslate nohighlight">\(1\)</span> is the most central node of the network, no edge actually points to it. In this case, the center of the star has a low PageRank value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_outstar</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.019  0.140  0.140  0.140  0.140  0.140  0.140  0.140]
</pre></div>
</div>
</div>
</div>
<p>We then try a star with edges pointing in.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">G_instar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G_instar</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">G_instar</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_instar</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> 
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a2444432691b17ff6593a677e80f8c88415d65748ccba7a1b3b861b173f655f2.png" src="../Images/d644ec50527bfb74965fed98a96f2683.png" data-original-src="https://mmids-textbook.github.io/_images/a2444432691b17ff6593a677e80f8c88415d65748ccba7a1b3b861b173f655f2.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_instar</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_instar</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_instar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">P_instar</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A_instar</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">P_instar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]
 [ 1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">Q_instar</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P_instar</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q_instar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
 [ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]]
</pre></div>
</div>
</div>
</div>
<p>In this case, the center of the star does indeed have a high PageRank value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_instar</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.869  0.019  0.019  0.019  0.019  0.019  0.019  0.019]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>A notion of centrality for undirected graphs</strong> We can apply PageRank<span class="math notranslate nohighlight">\(\idx{PageRank}\xdi\)</span> in the undirected case as well.</p>
<p>Consider random walk on the undirected graph <span class="math notranslate nohighlight">\(G\)</span>. That is, at every step, we pick a neighbor of the current state uniformly at random. If needed, add a self-loop to any isolated vertex. Then the transition matrix is</p>
<div class="math notranslate nohighlight">
\[
P = D^{-1} A,
\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the degree matrix and <span class="math notranslate nohighlight">\(A\)</span> is the adjacency matrix. A stationary distribution <span class="math notranslate nohighlight">\(\bpi = (\pi_1,\ldots,\pi_n)^T\)</span> is a row vector satisfying in this case</p>
<div class="math notranslate nohighlight">
\[
\pi_i
= \sum_{j=1}^n \pi_j p_{j,i}
= \sum_{j \in N(i)} \pi_j \frac{1}{\delta(j)}.
\]</div>
<p>We already know the solution to this system of equations. In the connected case without damping, the unique stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in V.
\]</div>
<p>In words, the centrality of a node is directly proportional to its degree, i.e., how many neighbors it has. Up to the scaling factor, this is known as <a class="reference external" href="https://en.wikipedia.org/wiki/Centrality#Degree_centrality">degree centrality</a><span class="math notranslate nohighlight">\(\idx{degree centrality}\xdi\)</span>.</p>
<p>For a general undirected graph that may not be connected, we can use a damping factor to enforce irreducibility. We add a small probability at each step of landing at a uniformly chosen node. That is, we define the transition matrix</p>
<div class="math notranslate nohighlight">
\[
Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, 
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span> known as the damping factor. We define the PageRank vector <span class="math notranslate nohighlight">\(\mathbf{PR}\)</span> as the unique stationary distribution of <span class="math notranslate nohighlight">\(Q = (q_{i,j})_{i,j=1}^n\)</span>, that is, the solution to</p>
<div class="math notranslate nohighlight">
\[
\mathrm{PR}_i
= \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i},
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{PR} \geq 0\)</span></p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \mathrm{PR}_i = 1.
\]</div>
<p><strong>NUMERICAL CORNER:</strong> We revisit the star example in the undirected case.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">G_star</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">G_star</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
    
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_star</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> 
                 <span class="n">node_color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c7f2056453edc830e099413bfdb90e0047d36da8030c2b8abf8be4198801e5fc.png" src="../Images/2d0d2764fb0069741155c9c91833c00b.png" data-original-src="https://mmids-textbook.github.io/_images/c7f2056453edc830e099413bfdb90e0047d36da8030c2b8abf8be4198801e5fc.png"/>
</div>
</div>
<p>We first compute the PageRank vector without damping. Here the random walk is periodic (Why?) so power iteration may fail (Try it!). Instead, we use a small amount of damping and increase the number of iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_star</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_star</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_star</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0 1 1 1 1 1 1 1]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_star</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.999</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.500  0.071  0.071  0.071  0.071  0.071  0.071  0.071]
</pre></div>
</div>
</div>
</div>
<p>The PageRank value for the center node is indeed roughly <span class="math notranslate nohighlight">\(7\)</span> times larger than the other ones, as can be expected from the ratio of their degrees.</p>
<p>We try again with more damping. This time the ratio of PageRank values is not quite the same as the ratio of degrees, but the center node continues to have a higher value than the other nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">pagerank</span><span class="p">(</span><span class="n">A_star</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.470  0.076  0.076  0.076  0.076  0.076  0.076  0.076]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> There are many other centrality measures besides PageRank, such as betweenness centrality, closeness centrality, and eigenvector centrality. Ask your favorite AI chatbot to explain these measures and discuss their similarities and differences with PageRank. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
&#13;

<h2><span class="section-number">7.5.3. </span>Personalized PageRank<a class="headerlink" href="#personalized-pagerank" title="Link to this heading">#</a></h2>
<p>We return to <a class="reference external" href="https://mathworld.wolfram.com">MathWorld</a> dataset. Recall that each page of MathWorld concerns a particular mathematical concept. In a section entitled “SEE ALSO”, other related mathematical concepts are listed with a link to their MathWorld page. Our goal is to identify “central” vertices in the resulting graph.</p>
<p><strong>Figure:</strong> Platonic solids (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Platonic solids" src="../Images/4393ac7f9684a7e62d5f82de15668d37.png" data-original-src="https://mmids-textbook.github.io/_images/small-platonic_solids.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We load the dataset again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data_edges</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'mathworld-adjacency.csv'</span><span class="p">)</span>
<span class="n">data_edges</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>from</th>
      <th>to</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>47</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>404</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>2721</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The second file contains the titles of the pages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data_titles</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'mathworld-titles.csv'</span><span class="p">)</span>
<span class="n">data_titles</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Alexander's Horned Sphere</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Exotic Sphere</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Antoine's Horned Sphere</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Flat</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Poincaré Manifold</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We construct the graph by adding the edges one by one. We first convert <code class="docutils literal notranslate"><span class="pre">df_edges</span></code> into a NumPy array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">edgelist</span> <span class="o">=</span> <span class="n">data_edges</span><span class="p">[[</span><span class="s1">'from'</span><span class="p">,</span><span class="s1">'to'</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">edgelist</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[    0     2]
 [    1    47]
 [    1   404]
 ...
 [12361 12306]
 [12361 12310]
 [12361 12360]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span> <span class="o">=</span> <span class="mi">12362</span>
<span class="n">G_mw</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">empty_graph</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">create_using</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">edgelist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">G_mw</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">edgelist</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">edgelist</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>To apply PageRank, we construct the adjacency matric of the graph. We also define a vector of title pages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A_mw</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G_mw</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">titles_mw</span> <span class="o">=</span> <span class="n">data_titles</span><span class="p">[</span><span class="s1">'title'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">pr_mw</span> <span class="o">=</span> <span class="n">pagerank</span><span class="p">(</span><span class="n">A_mw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.argsort.html"><code class="docutils literal notranslate"><span class="pre">numpy.argsort</span></code></a> to identify the pages with highest scores. We apply it to <code class="docutils literal notranslate"><span class="pre">-pr_mw</span></code> to sort from the highest to lowest value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">top_pages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">pr_mw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The top 25 topics are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">titles_mw</span><span class="p">[</span><span class="n">top_pages</span><span class="p">[:</span><span class="mi">25</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['Sphere' 'Circle' 'Prime Number' 'Aleksandrov-Čech Cohomology'
 'Centroid Hexagon' 'Group' 'Fourier Transform' 'Tree' 'Splitting Field'
 'Archimedean Solid' 'Normal Distribution' 'Integer Sequence Primes'
 'Perimeter Polynomial' 'Polygon' 'Finite Group' 'Large Number'
 'Riemann Zeta Function' 'Chebyshev Approximation Formula' 'Vector' 'Ring'
 'Fibonacci Number' 'Conic Section' 'Fourier Series' 'Derivative'
 'Gamma Function']
</pre></div>
</div>
</div>
</div>
<p>We indeed get a list of central concepts in mathematics – including several we have encountered previously such as <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">Distribution</span></code>, <code class="docutils literal notranslate"><span class="pre">Tree</span></code>, <code class="docutils literal notranslate"><span class="pre">Vector</span></code> or <code class="docutils literal notranslate"><span class="pre">Derivative</span></code>.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>There is a variant of PageRank, referred to as Personalized PageRank (PPR)<span class="math notranslate nohighlight">\(\idx{Personalized PageRank}\xdi\)</span>, which aims to tailor the outcome to specific interests. This is accomplished from a simple change to the algorithm. When teleporting, rather than jumping to a uniformly random page, we instead jump to an arbitrary distribution which is meant to capture some specific interests. In the context of the web for instance, this distribution might be uniform over someone’s bookmarks.</p>
<p>We adapt <code class="docutils literal notranslate"><span class="pre">pagerank</span></code> as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">ppr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">transition_from_adjacency</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">add_damping</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">mu</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">@</span> <span class="n">Q</span>
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> To test PPR, consider the distribution concentrated on a single topic <code class="docutils literal notranslate"><span class="pre">Normal</span> <span class="pre">Distribution</span></code>. This is topic number <code class="docutils literal notranslate"><span class="pre">1270</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">titles_mw</span> <span class="o">==</span> <span class="s1">'Normal Distribution'</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1270
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">mu</span><span class="p">[</span><span class="mi">1270</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>We now run PPR and list the top 25 pages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ppr_mw</span> <span class="o">=</span> <span class="n">ppr</span><span class="p">(</span><span class="n">A_mw</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
<span class="n">top_pers_pages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">ppr_mw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The top 25 topics are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">titles_mw</span><span class="p">[</span><span class="n">top_pers_pages</span><span class="p">[:</span><span class="mi">25</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['Normal Distribution' 'Pearson System' 'Logit Transformation' 'z-Score'
 'Erf' 'Central Limit Theorem' 'Bivariate Normal Distribution'
 'Normal Ratio Distribution' 'Normal Sum Distribution'
 'Normal Distribution Function' 'Gaussian Function'
 'Standard Normal Distribution' 'Normal Product Distribution'
 'Binomial Distribution' 'Tetrachoric Function' 'Ratio Distribution'
 'Kolmogorov-Smirnov Test' 'Box-Muller Transformation' 'Galton Board'
 'Fisher-Behrens Problem' 'Erfc' 'Normal Difference Distribution'
 'Half-Normal Distribution' 'Inverse Gaussian Distribution'
 'Error Function Distribution']
</pre></div>
</div>
</div>
</div>
<p>This indeed returns various statistical concepts, particularly related to the normal dsitribution.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The PageRank algorithm has been adapted for various applications beyond web search, such as ranking scientific papers, analyzing social networks, and even ranking sports teams [<a class="reference external" href="https://arxiv.org/abs/1407.5107">Gle</a>]. Ask your favorite AI chatbot to discuss some of these applications and how the PageRank algorithm is modified for each case. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Consider a random walk on the following graph:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
</pre></div>
</div>
<p>What is the transition matrix for this random walk?</p>
<p>a)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1/3 &amp; 1/3 &amp; 1/3 \\ 1/3 &amp; 1/3 &amp; 1/3 \\ 1/3 &amp; 1/3 &amp; 1/3 \end{bmatrix}
\end{split}\]</div>
<p>b)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 0 &amp; 1/2 &amp; 1/2 \\ 1/2 &amp; 0 &amp; 1/2 \\ 1/2 &amp; 1/2 &amp; 0 \end{bmatrix}
\end{split}\]</div>
<p>c)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<p>d)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \end{bmatrix}
\end{split}\]</div>
<p><strong>2</strong> In a random walk on a directed graph, the transition probability from vertex <span class="math notranslate nohighlight">\(i\)</span> to vertex <span class="math notranslate nohighlight">\(j\)</span> is given by:</p>
<p>a) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^-(j)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^-(i)\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^+(j)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^+(i)\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^-(i)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^-(i)\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^+(i)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^+(i)\)</span></p>
<p><strong>3</strong> The transition matrix <span class="math notranslate nohighlight">\(P\)</span> of a random walk on a directed graph can be expressed in terms of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> as:</p>
<p>a) <span class="math notranslate nohighlight">\(P = AD^{-1}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(P = A^TD^{-1}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(P = D^{-1}A\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(P = D^{-1}A^T\)</span></p>
<p><strong>4</strong> In a random walk on an undirected graph, the stationary distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> satisfies:</p>
<p>a) <span class="math notranslate nohighlight">\(\pi_i = \frac{\delta^+(i)}{\sum_{j \in V} \delta^+(j)}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\pi_i = \frac{1}{\delta(i)}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\pi_i = \frac{1}{|V|}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\pi_i = \frac{\delta(i)}{\sum_{j \in V} \delta(j)}\)</span> for all <span class="math notranslate nohighlight">\(i \in V\)</span></p>
<p><strong>5</strong> Personalized PageRank differs from standard PageRank in that:</p>
<p>a) It considers the user’s browsing history</p>
<p>b) It jumps to a non-uniform distribution when teleporting</p>
<p>c) It uses a different damping factor</p>
<p>d) It only considers a subset of the graph</p>
<p>Answer for 1: b. Justification: Each node has a degree of 2, and the probability of transitioning to each neighbor is 1/2.</p>
<p>Answer for 2: d. Justification: The text states, “In words, at each step, we choose an outgoing edge from the current state uniformly at random,” which corresponds to the transition probability <span class="math notranslate nohighlight">\(p_{i,j} = \frac{1}{\delta^+(i)}\)</span> for all <span class="math notranslate nohighlight">\(j \in N^+(i)\)</span>, where <span class="math notranslate nohighlight">\(\delta^+(i)\)</span> is the out-degree of vertex <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(N^+(i)\)</span> is the set of vertices with an edge from <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Answer for 3: c. Justification: The text states, “The transition matrix of random walk on <span class="math notranslate nohighlight">\(G\)</span> satisfying the conditions of the definition above is <span class="math notranslate nohighlight">\(P = D^{-1}A\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the out-degree matrix.”</p>
<p>Answer for 4: d. Justification: The text states, “In the connected case without damping, the unique stationary distribution of random walk on <span class="math notranslate nohighlight">\(G\)</span> is given by <span class="math notranslate nohighlight">\(\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \forall i \in V\)</span>.”</p>
<p>Answer for 5: b. Justification: The text states, “When teleporting, rather than jumping to a uniformly random page, we instead jump to an arbitrary distribution <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> which is meant to capture some specific interests.”</p>
    
</body>
</html>