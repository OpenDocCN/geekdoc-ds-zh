- en: What problems fit to GPU?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://enccs.github.io/gpu-programming/3-gpu-problems/](https://enccs.github.io/gpu-programming/3-gpu-problems/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*[GPU programming: why, when and how?](../)* **   What problems fit to GPU?'
  prefs: []
  type: TYPE_NORMAL
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/3-gpu-problems.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs: []
  type: TYPE_NORMAL
- en: What are the strengths and weaknesses of GPUs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes a particular problem suitable for GPU-porting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are GPUs so ubiquitous in machine learning applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Get a feeling for the type of use cases that GPUs excel at.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 10 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are GPUs good for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Answer from [Stack Exchange](https://scicomp.stackexchange.com/questions/943/what-kinds-of-problems-lend-themselves-well-to-gpu-computing):'
  prefs: []
  type: TYPE_NORMAL
- en: '*From a metaphorical point of view, the GPU can be seen as a person lying on
    a bed of nails. The person lying on top is the data and in the base of each nail
    there is a processor, so the nail is actually an arrow pointing from processor
    to memory. All nails are in a regular pattern, like a grid. If the body is well
    spread, it feels good (performance is good), if the body only touches some spots
    of the nail bed, then the pain is bad (bad performance).*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GPU computing is well-suited to problems that involve large amounts of data
    parallelism. Specifically, you can expect good performance on GPUs for:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large-scale matrix and vector operations**: Common in machine learning, scientific
    computing, and image processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourier transforms**: Also common in machine learning, scientific computing,
    and image processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo simulations**: Used across finance, physics, and other fields
    to simulate complex systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Molecular dynamics simulations**: Used in chemistry, biochemistry and physics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational fluid dynamics**: Used in engineering, physics, and other fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** and **computer vision algorithms**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big data analytics**: Clustering, classification, regression, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphics rendering**: Original use-case for GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are GPUs not good for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not all programming problems can efficiently leverage the parallelism offered
    by GPUs. Some types of problems that do not fit well on a GPU include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential tasks**: Problems that require a series of dependent steps, where
    each step relies on the outcome of the previous step, are not well-suited for
    parallel processing. Examples include recursive algorithms, certain dynamic programming
    problems, and some graph traversal algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-grained branching**: GPUs perform best when the code being executed
    across different threads follows a similar control flow. When there is extensive
    branching (i.e., many `if` statements) within a kernel or algorithm, performance
    may suffer due to the divergence in execution paths among the GPU threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low arithmetic intensity**: GPUs excel at performing a large number of mathematical
    operations quickly. If a problem has low arithmetic intensity (i.e., a low ratio
    of arithmetic operations to memory accesses), the GPU may not be able to efficiently
    utilize its computational power, leading to underperformance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small data sets**: If the problem involves a small data set that does not
    require significant parallelism, using a GPU may not result in noticeable performance
    gains. In such cases, the overhead of transferring data between the CPU and GPU,
    and the time spent initializing the GPU, may outweigh any potential benefits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited parallelism**: Some algorithms have inherent limitations on the degree
    of parallelism that can be achieved. In these cases, using a GPU may not lead
    to significant performance improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory-bound problems**: GPUs generally have less memory available compared
    to CPUs, and their memory bandwidth can be a limiting factor. If a problem requires
    a large amount of memory or involves memory-intensive operations, it may not be
    well-suited for a GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of GPU acceleration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give a flavor of what type of performance gains we can achieve by porting
    a calculations to a GPU (if we’re lucky!), let’s look at a few case examples.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of array size
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the case of matrix multiplication in the Julia language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How much faster do you think the GPU version is compared to running on a single
    CPU core?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia automatically parallelises matrix multiplication over available CPU cores.
    Will the GPU version be faster than running on 64 cores?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the size of the array affect how much the performance improves?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: 'Example results from running on LUMI (MI250X AMD GPU, 64-core AMD Trento CPUs):'
  prefs: []
  type: TYPE_NORMAL
- en: GPU acceleration for matrix multiply in Julia
  prefs: []
  type: TYPE_NORMAL
- en: '| Matrix size | 1 CPU core | 64 CPU cores | 1 GPU | GPU speedup |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (512, 512) | 5.472 ms | 517.722 μs | 115.805 μs | ~47x / ~5x |'
  prefs: []
  type: TYPE_TB
- en: '| (1024, 1024) | 43.364 ms | 2.929 ms | 173.316 μs | ~250x / ~17x |'
  prefs: []
  type: TYPE_TB
- en: '| (2048, 2048) | 344.364 ms | 30.081 ms | 866.348 μs | ~400x / ~35x |'
  prefs: []
  type: TYPE_TB
- en: '| (4096, 4096) | 3.221 s | 159.563 ms | 5.910 ms | ~550x / ~27x |'
  prefs: []
  type: TYPE_TB
- en: Electronic structure calculations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[VASP](https://www.vasp.at/) is a popular software package used for electronic
    structure calculations. The figures below show the speedup observed in a recent
    benchmark study on the [VASP Power Profiles on NVIDIA A100 GPUs](https://ieeexplore.ieee.org/document/10820603),
    which was conducted on the Perlmutter system at NERSC. An analysis of total energy
    usage demonstrated that VASP’s power usage varies significantly with different
    workloads, more so than with parallel concurrency. Additionally, power capping
    GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads
    with less than a 10% performance loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_parallel_efficiency.png](../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel efficiency of VASP on seven test cases representing diverse VASP production
    workloads and ensuring a comprehensive coverage of various code paths, elements,
    and problem sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_energy_consumption.jpg](../Images/6fa5e8533100766ad3399b8e454b4d19.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Left) Power usage of seven representative VASP workloads. The horizontal axis
    shows number of nodes used, and vertical axis shows high power mode per node.
    (Right) Power consumed per GPU when running VASP under four different power caps:
    400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied
    to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied
    cap. The dashed horizontal line represents applied power cap. Each benchmark was
    run with a node count optimizing runtime while remaining above 70% parallel efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Computational Chemistry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A great deal of computational resources are spent in Quantum Chemical calculations
    which involve the solution of the Hartree-Fock eigenvalue problem, which requires
    the diagonalization of the Fock matrix whose elements are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma
    \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma
    \beta) \right ],\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term is related to the one electron contributions and the second
    term is related to the electron repulsion integrals (ERIs), in parenthesis, weighted
    by the by the density matrix \(D_{\gamma \delta}\). One of the most expensive
    parts in the solution of the Hartree-Fock equations is the processing (digestion)
    of the ERIs, one algorithm to do this task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/algorithms.svg](../Images/8315333e7eb919a0358c1a5aad382eaf.png)](../_images/algorithms.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm for processing ERIs [see [JCTC, 17, 7486, (2021)](https://doi.org/10.1021/acs.jctc.1c00720)
    for details]
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is suitable for GPUs as it involves many arithmetic operations.
    In addition to this, there are symmetries and properties of the integrals that
    could be used to rearrange the loops in an efficient manner that fit GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Humanities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A brief introduction into some of the work that is being done in the humanities
    that can benefit from utilizing GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Language models and NLP (natural language processing)**'
  prefs: []
  type: TYPE_NORMAL
- en: With the recent popularity of ChatGPT, the use of language models has come into
    the mainstream, however such models have been used in the humanities many years
    already. One of the biggest goals of humanities researchers is working with textual
    data which has increased exponentially over recent years due to the rise in social
    media. Analyzing such textual data to gain insights into questions of sociology,
    linguistics and various other fields have become increasingly reliant on using
    language models. Along with language models, the need for GPU access has become
    essential.
  prefs: []
  type: TYPE_NORMAL
- en: '**Archeology**'
  prefs: []
  type: TYPE_NORMAL
- en: The field of archeology also makes use of GPUs in their 3D modelling and rendering
    work. The biggest problem with archeological sites is that once they are excavated,
    they are destroyed, so any researchers who aren’t present at the site, would lose
    valuable insights into how it looked when it was found. However, with recent developments
    in technology and accessibility to high-performance computing, they are able to
    generate extremely detailed renderings of the excavation sites which act as a
    way to preserve the site for future researchers to gain critical insights and
    contribute to the research.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cognitive Science**'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be
    invaluable in studies that delve into human behavior or population dynamics. MCMC
    sampling allows researchers to simulate and analyze complex systems by iteratively
    sampling from a Markov chain, enabling the exploration of high-dimensional parameter
    spaces. This method is particularly useful when studying human behavior, as it
    can capture the inherent randomness and interdependencies that characterize social
    systems. By leveraging MCMC sampling, researchers can gain insights into various
    aspects of human behavior, such as decision-making, social interactions, and the
    spread of information or diseases within populations.
  prefs: []
  type: TYPE_NORMAL
- en: By offloading the computational workload to GPUs, researchers can experience
    substantial speedup in the execution of MCMC algorithms. This speedup allows for
    more extensive exploration of parameter spaces and facilitates the analysis of
    larger datasets, leading to more accurate and detailed insights into human behavior
    or population dynamics. Examples of studies done using these methods can be found
    at the [Center for Humanities Computing Aarhus](https://chc.au.dk/) (CHCAA) and
    [Interacting Minds Centre](https://interactingminds.au.dk/) (IMC) at Aarhus University.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discussion
  prefs: []
  type: TYPE_NORMAL
- en: What type of problems have you used GPUs for?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large was the performance boost?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good and bad use cases for GPU porting
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following computational tasks is likely to gain the least performance
    benefit from being ported to a GPU?
  prefs: []
  type: TYPE_NORMAL
- en: Training a large, deep neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing a Monte Carlo simulation with a large number of independent trials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing an algorithm with heavy use of recursion and frequent branching.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing a large image with a convolutional filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: The right answer is option 3\. GPUs do not handle recursion and branching as
    effectively as more data-heavy algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: GPUs excel in processing tasks with high data parallelism, such as large-scale
    matrix operations, Fourier transforms, and big data analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs struggle with sequential tasks, problems with extensive control flow divergence,
    low arithmetic intensity tasks, small data sets, and memory-bound problems. [Previous](../2-gpu-ecosystem/
    "The GPU hardware and software ecosystem") [Next](../4-gpu-concepts/ "GPU programming
    concepts")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2023-2024, The contributors.
  prefs: []
  type: TYPE_NORMAL
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  prefs: []
  type: TYPE_NORMAL
- en: What are the strengths and weaknesses of GPUs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes a particular problem suitable for GPU-porting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are GPUs so ubiquitous in machine learning applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Get a feeling for the type of use cases that GPUs excel at.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 10 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are GPUs good for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Answer from [Stack Exchange](https://scicomp.stackexchange.com/questions/943/what-kinds-of-problems-lend-themselves-well-to-gpu-computing):'
  prefs: []
  type: TYPE_NORMAL
- en: '*From a metaphorical point of view, the GPU can be seen as a person lying on
    a bed of nails. The person lying on top is the data and in the base of each nail
    there is a processor, so the nail is actually an arrow pointing from processor
    to memory. All nails are in a regular pattern, like a grid. If the body is well
    spread, it feels good (performance is good), if the body only touches some spots
    of the nail bed, then the pain is bad (bad performance).*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GPU computing is well-suited to problems that involve large amounts of data
    parallelism. Specifically, you can expect good performance on GPUs for:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large-scale matrix and vector operations**: Common in machine learning, scientific
    computing, and image processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourier transforms**: Also common in machine learning, scientific computing,
    and image processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo simulations**: Used across finance, physics, and other fields
    to simulate complex systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Molecular dynamics simulations**: Used in chemistry, biochemistry and physics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational fluid dynamics**: Used in engineering, physics, and other fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** and **computer vision algorithms**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big data analytics**: Clustering, classification, regression, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphics rendering**: Original use-case for GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are GPUs not good for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not all programming problems can efficiently leverage the parallelism offered
    by GPUs. Some types of problems that do not fit well on a GPU include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential tasks**: Problems that require a series of dependent steps, where
    each step relies on the outcome of the previous step, are not well-suited for
    parallel processing. Examples include recursive algorithms, certain dynamic programming
    problems, and some graph traversal algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-grained branching**: GPUs perform best when the code being executed
    across different threads follows a similar control flow. When there is extensive
    branching (i.e., many `if` statements) within a kernel or algorithm, performance
    may suffer due to the divergence in execution paths among the GPU threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low arithmetic intensity**: GPUs excel at performing a large number of mathematical
    operations quickly. If a problem has low arithmetic intensity (i.e., a low ratio
    of arithmetic operations to memory accesses), the GPU may not be able to efficiently
    utilize its computational power, leading to underperformance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small data sets**: If the problem involves a small data set that does not
    require significant parallelism, using a GPU may not result in noticeable performance
    gains. In such cases, the overhead of transferring data between the CPU and GPU,
    and the time spent initializing the GPU, may outweigh any potential benefits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited parallelism**: Some algorithms have inherent limitations on the degree
    of parallelism that can be achieved. In these cases, using a GPU may not lead
    to significant performance improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory-bound problems**: GPUs generally have less memory available compared
    to CPUs, and their memory bandwidth can be a limiting factor. If a problem requires
    a large amount of memory or involves memory-intensive operations, it may not be
    well-suited for a GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of GPU acceleration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give a flavor of what type of performance gains we can achieve by porting
    a calculations to a GPU (if we’re lucky!), let’s look at a few case examples.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of array size
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the case of matrix multiplication in the Julia language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How much faster do you think the GPU version is compared to running on a single
    CPU core?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia automatically parallelises matrix multiplication over available CPU cores.
    Will the GPU version be faster than running on 64 cores?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the size of the array affect how much the performance improves?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: 'Example results from running on LUMI (MI250X AMD GPU, 64-core AMD Trento CPUs):'
  prefs: []
  type: TYPE_NORMAL
- en: GPU acceleration for matrix multiply in Julia
  prefs: []
  type: TYPE_NORMAL
- en: '| Matrix size | 1 CPU core | 64 CPU cores | 1 GPU | GPU speedup |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (512, 512) | 5.472 ms | 517.722 μs | 115.805 μs | ~47x / ~5x |'
  prefs: []
  type: TYPE_TB
- en: '| (1024, 1024) | 43.364 ms | 2.929 ms | 173.316 μs | ~250x / ~17x |'
  prefs: []
  type: TYPE_TB
- en: '| (2048, 2048) | 344.364 ms | 30.081 ms | 866.348 μs | ~400x / ~35x |'
  prefs: []
  type: TYPE_TB
- en: '| (4096, 4096) | 3.221 s | 159.563 ms | 5.910 ms | ~550x / ~27x |'
  prefs: []
  type: TYPE_TB
- en: Electronic structure calculations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[VASP](https://www.vasp.at/) is a popular software package used for electronic
    structure calculations. The figures below show the speedup observed in a recent
    benchmark study on the [VASP Power Profiles on NVIDIA A100 GPUs](https://ieeexplore.ieee.org/document/10820603),
    which was conducted on the Perlmutter system at NERSC. An analysis of total energy
    usage demonstrated that VASP’s power usage varies significantly with different
    workloads, more so than with parallel concurrency. Additionally, power capping
    GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads
    with less than a 10% performance loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_parallel_efficiency.png](../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel efficiency of VASP on seven test cases representing diverse VASP production
    workloads and ensuring a comprehensive coverage of various code paths, elements,
    and problem sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_energy_consumption.jpg](../Images/6fa5e8533100766ad3399b8e454b4d19.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Left) Power usage of seven representative VASP workloads. The horizontal axis
    shows number of nodes used, and vertical axis shows high power mode per node.
    (Right) Power consumed per GPU when running VASP under four different power caps:
    400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied
    to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied
    cap. The dashed horizontal line represents applied power cap. Each benchmark was
    run with a node count optimizing runtime while remaining above 70% parallel efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Computational Chemistry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A great deal of computational resources are spent in Quantum Chemical calculations
    which involve the solution of the Hartree-Fock eigenvalue problem, which requires
    the diagonalization of the Fock matrix whose elements are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma
    \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma
    \beta) \right ],\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term is related to the one electron contributions and the second
    term is related to the electron repulsion integrals (ERIs), in parenthesis, weighted
    by the by the density matrix \(D_{\gamma \delta}\). One of the most expensive
    parts in the solution of the Hartree-Fock equations is the processing (digestion)
    of the ERIs, one algorithm to do this task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/algorithms.svg](../Images/8315333e7eb919a0358c1a5aad382eaf.png)](../_images/algorithms.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm for processing ERIs [see [JCTC, 17, 7486, (2021)](https://doi.org/10.1021/acs.jctc.1c00720)
    for details]
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is suitable for GPUs as it involves many arithmetic operations.
    In addition to this, there are symmetries and properties of the integrals that
    could be used to rearrange the loops in an efficient manner that fit GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Humanities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A brief introduction into some of the work that is being done in the humanities
    that can benefit from utilizing GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Language models and NLP (natural language processing)**'
  prefs: []
  type: TYPE_NORMAL
- en: With the recent popularity of ChatGPT, the use of language models has come into
    the mainstream, however such models have been used in the humanities many years
    already. One of the biggest goals of humanities researchers is working with textual
    data which has increased exponentially over recent years due to the rise in social
    media. Analyzing such textual data to gain insights into questions of sociology,
    linguistics and various other fields have become increasingly reliant on using
    language models. Along with language models, the need for GPU access has become
    essential.
  prefs: []
  type: TYPE_NORMAL
- en: '**Archeology**'
  prefs: []
  type: TYPE_NORMAL
- en: The field of archeology also makes use of GPUs in their 3D modelling and rendering
    work. The biggest problem with archeological sites is that once they are excavated,
    they are destroyed, so any researchers who aren’t present at the site, would lose
    valuable insights into how it looked when it was found. However, with recent developments
    in technology and accessibility to high-performance computing, they are able to
    generate extremely detailed renderings of the excavation sites which act as a
    way to preserve the site for future researchers to gain critical insights and
    contribute to the research.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cognitive Science**'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be
    invaluable in studies that delve into human behavior or population dynamics. MCMC
    sampling allows researchers to simulate and analyze complex systems by iteratively
    sampling from a Markov chain, enabling the exploration of high-dimensional parameter
    spaces. This method is particularly useful when studying human behavior, as it
    can capture the inherent randomness and interdependencies that characterize social
    systems. By leveraging MCMC sampling, researchers can gain insights into various
    aspects of human behavior, such as decision-making, social interactions, and the
    spread of information or diseases within populations.
  prefs: []
  type: TYPE_NORMAL
- en: By offloading the computational workload to GPUs, researchers can experience
    substantial speedup in the execution of MCMC algorithms. This speedup allows for
    more extensive exploration of parameter spaces and facilitates the analysis of
    larger datasets, leading to more accurate and detailed insights into human behavior
    or population dynamics. Examples of studies done using these methods can be found
    at the [Center for Humanities Computing Aarhus](https://chc.au.dk/) (CHCAA) and
    [Interacting Minds Centre](https://interactingminds.au.dk/) (IMC) at Aarhus University.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discussion
  prefs: []
  type: TYPE_NORMAL
- en: What type of problems have you used GPUs for?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large was the performance boost?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good and bad use cases for GPU porting
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following computational tasks is likely to gain the least performance
    benefit from being ported to a GPU?
  prefs: []
  type: TYPE_NORMAL
- en: Training a large, deep neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing a Monte Carlo simulation with a large number of independent trials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing an algorithm with heavy use of recursion and frequent branching.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing a large image with a convolutional filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: The right answer is option 3\. GPUs do not handle recursion and branching as
    effectively as more data-heavy algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: GPUs excel in processing tasks with high data parallelism, such as large-scale
    matrix operations, Fourier transforms, and big data analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs struggle with sequential tasks, problems with extensive control flow divergence,
    low arithmetic intensity tasks, small data sets, and memory-bound problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are GPUs good for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Answer from [Stack Exchange](https://scicomp.stackexchange.com/questions/943/what-kinds-of-problems-lend-themselves-well-to-gpu-computing):'
  prefs: []
  type: TYPE_NORMAL
- en: '*From a metaphorical point of view, the GPU can be seen as a person lying on
    a bed of nails. The person lying on top is the data and in the base of each nail
    there is a processor, so the nail is actually an arrow pointing from processor
    to memory. All nails are in a regular pattern, like a grid. If the body is well
    spread, it feels good (performance is good), if the body only touches some spots
    of the nail bed, then the pain is bad (bad performance).*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GPU computing is well-suited to problems that involve large amounts of data
    parallelism. Specifically, you can expect good performance on GPUs for:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large-scale matrix and vector operations**: Common in machine learning, scientific
    computing, and image processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourier transforms**: Also common in machine learning, scientific computing,
    and image processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo simulations**: Used across finance, physics, and other fields
    to simulate complex systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Molecular dynamics simulations**: Used in chemistry, biochemistry and physics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational fluid dynamics**: Used in engineering, physics, and other fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** and **computer vision algorithms**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big data analytics**: Clustering, classification, regression, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphics rendering**: Original use-case for GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are GPUs not good for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not all programming problems can efficiently leverage the parallelism offered
    by GPUs. Some types of problems that do not fit well on a GPU include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential tasks**: Problems that require a series of dependent steps, where
    each step relies on the outcome of the previous step, are not well-suited for
    parallel processing. Examples include recursive algorithms, certain dynamic programming
    problems, and some graph traversal algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-grained branching**: GPUs perform best when the code being executed
    across different threads follows a similar control flow. When there is extensive
    branching (i.e., many `if` statements) within a kernel or algorithm, performance
    may suffer due to the divergence in execution paths among the GPU threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low arithmetic intensity**: GPUs excel at performing a large number of mathematical
    operations quickly. If a problem has low arithmetic intensity (i.e., a low ratio
    of arithmetic operations to memory accesses), the GPU may not be able to efficiently
    utilize its computational power, leading to underperformance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small data sets**: If the problem involves a small data set that does not
    require significant parallelism, using a GPU may not result in noticeable performance
    gains. In such cases, the overhead of transferring data between the CPU and GPU,
    and the time spent initializing the GPU, may outweigh any potential benefits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited parallelism**: Some algorithms have inherent limitations on the degree
    of parallelism that can be achieved. In these cases, using a GPU may not lead
    to significant performance improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory-bound problems**: GPUs generally have less memory available compared
    to CPUs, and their memory bandwidth can be a limiting factor. If a problem requires
    a large amount of memory or involves memory-intensive operations, it may not be
    well-suited for a GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of GPU acceleration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give a flavor of what type of performance gains we can achieve by porting
    a calculations to a GPU (if we’re lucky!), let’s look at a few case examples.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of array size
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the case of matrix multiplication in the Julia language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How much faster do you think the GPU version is compared to running on a single
    CPU core?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia automatically parallelises matrix multiplication over available CPU cores.
    Will the GPU version be faster than running on 64 cores?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the size of the array affect how much the performance improves?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: 'Example results from running on LUMI (MI250X AMD GPU, 64-core AMD Trento CPUs):'
  prefs: []
  type: TYPE_NORMAL
- en: GPU acceleration for matrix multiply in Julia
  prefs: []
  type: TYPE_NORMAL
- en: '| Matrix size | 1 CPU core | 64 CPU cores | 1 GPU | GPU speedup |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (512, 512) | 5.472 ms | 517.722 μs | 115.805 μs | ~47x / ~5x |'
  prefs: []
  type: TYPE_TB
- en: '| (1024, 1024) | 43.364 ms | 2.929 ms | 173.316 μs | ~250x / ~17x |'
  prefs: []
  type: TYPE_TB
- en: '| (2048, 2048) | 344.364 ms | 30.081 ms | 866.348 μs | ~400x / ~35x |'
  prefs: []
  type: TYPE_TB
- en: '| (4096, 4096) | 3.221 s | 159.563 ms | 5.910 ms | ~550x / ~27x |'
  prefs: []
  type: TYPE_TB
- en: Electronic structure calculations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[VASP](https://www.vasp.at/) is a popular software package used for electronic
    structure calculations. The figures below show the speedup observed in a recent
    benchmark study on the [VASP Power Profiles on NVIDIA A100 GPUs](https://ieeexplore.ieee.org/document/10820603),
    which was conducted on the Perlmutter system at NERSC. An analysis of total energy
    usage demonstrated that VASP’s power usage varies significantly with different
    workloads, more so than with parallel concurrency. Additionally, power capping
    GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads
    with less than a 10% performance loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_parallel_efficiency.png](../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel efficiency of VASP on seven test cases representing diverse VASP production
    workloads and ensuring a comprehensive coverage of various code paths, elements,
    and problem sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_energy_consumption.jpg](../Images/6fa5e8533100766ad3399b8e454b4d19.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Left) Power usage of seven representative VASP workloads. The horizontal axis
    shows number of nodes used, and vertical axis shows high power mode per node.
    (Right) Power consumed per GPU when running VASP under four different power caps:
    400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied
    to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied
    cap. The dashed horizontal line represents applied power cap. Each benchmark was
    run with a node count optimizing runtime while remaining above 70% parallel efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Computational Chemistry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A great deal of computational resources are spent in Quantum Chemical calculations
    which involve the solution of the Hartree-Fock eigenvalue problem, which requires
    the diagonalization of the Fock matrix whose elements are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma
    \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma
    \beta) \right ],\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term is related to the one electron contributions and the second
    term is related to the electron repulsion integrals (ERIs), in parenthesis, weighted
    by the by the density matrix \(D_{\gamma \delta}\). One of the most expensive
    parts in the solution of the Hartree-Fock equations is the processing (digestion)
    of the ERIs, one algorithm to do this task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/algorithms.svg](../Images/8315333e7eb919a0358c1a5aad382eaf.png)](../_images/algorithms.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm for processing ERIs [see [JCTC, 17, 7486, (2021)](https://doi.org/10.1021/acs.jctc.1c00720)
    for details]
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is suitable for GPUs as it involves many arithmetic operations.
    In addition to this, there are symmetries and properties of the integrals that
    could be used to rearrange the loops in an efficient manner that fit GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Humanities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A brief introduction into some of the work that is being done in the humanities
    that can benefit from utilizing GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Language models and NLP (natural language processing)**'
  prefs: []
  type: TYPE_NORMAL
- en: With the recent popularity of ChatGPT, the use of language models has come into
    the mainstream, however such models have been used in the humanities many years
    already. One of the biggest goals of humanities researchers is working with textual
    data which has increased exponentially over recent years due to the rise in social
    media. Analyzing such textual data to gain insights into questions of sociology,
    linguistics and various other fields have become increasingly reliant on using
    language models. Along with language models, the need for GPU access has become
    essential.
  prefs: []
  type: TYPE_NORMAL
- en: '**Archeology**'
  prefs: []
  type: TYPE_NORMAL
- en: The field of archeology also makes use of GPUs in their 3D modelling and rendering
    work. The biggest problem with archeological sites is that once they are excavated,
    they are destroyed, so any researchers who aren’t present at the site, would lose
    valuable insights into how it looked when it was found. However, with recent developments
    in technology and accessibility to high-performance computing, they are able to
    generate extremely detailed renderings of the excavation sites which act as a
    way to preserve the site for future researchers to gain critical insights and
    contribute to the research.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cognitive Science**'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be
    invaluable in studies that delve into human behavior or population dynamics. MCMC
    sampling allows researchers to simulate and analyze complex systems by iteratively
    sampling from a Markov chain, enabling the exploration of high-dimensional parameter
    spaces. This method is particularly useful when studying human behavior, as it
    can capture the inherent randomness and interdependencies that characterize social
    systems. By leveraging MCMC sampling, researchers can gain insights into various
    aspects of human behavior, such as decision-making, social interactions, and the
    spread of information or diseases within populations.
  prefs: []
  type: TYPE_NORMAL
- en: By offloading the computational workload to GPUs, researchers can experience
    substantial speedup in the execution of MCMC algorithms. This speedup allows for
    more extensive exploration of parameter spaces and facilitates the analysis of
    larger datasets, leading to more accurate and detailed insights into human behavior
    or population dynamics. Examples of studies done using these methods can be found
    at the [Center for Humanities Computing Aarhus](https://chc.au.dk/) (CHCAA) and
    [Interacting Minds Centre](https://interactingminds.au.dk/) (IMC) at Aarhus University.
  prefs: []
  type: TYPE_NORMAL
- en: Electronic structure calculations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[VASP](https://www.vasp.at/) is a popular software package used for electronic
    structure calculations. The figures below show the speedup observed in a recent
    benchmark study on the [VASP Power Profiles on NVIDIA A100 GPUs](https://ieeexplore.ieee.org/document/10820603),
    which was conducted on the Perlmutter system at NERSC. An analysis of total energy
    usage demonstrated that VASP’s power usage varies significantly with different
    workloads, more so than with parallel concurrency. Additionally, power capping
    GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads
    with less than a 10% performance loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_parallel_efficiency.png](../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel efficiency of VASP on seven test cases representing diverse VASP production
    workloads and ensuring a comprehensive coverage of various code paths, elements,
    and problem sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/vasp_energy_consumption.jpg](../Images/6fa5e8533100766ad3399b8e454b4d19.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Left) Power usage of seven representative VASP workloads. The horizontal axis
    shows number of nodes used, and vertical axis shows high power mode per node.
    (Right) Power consumed per GPU when running VASP under four different power caps:
    400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied
    to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied
    cap. The dashed horizontal line represents applied power cap. Each benchmark was
    run with a node count optimizing runtime while remaining above 70% parallel efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Computational Chemistry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A great deal of computational resources are spent in Quantum Chemical calculations
    which involve the solution of the Hartree-Fock eigenvalue problem, which requires
    the diagonalization of the Fock matrix whose elements are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma
    \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma
    \beta) \right ],\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term is related to the one electron contributions and the second
    term is related to the electron repulsion integrals (ERIs), in parenthesis, weighted
    by the by the density matrix \(D_{\gamma \delta}\). One of the most expensive
    parts in the solution of the Hartree-Fock equations is the processing (digestion)
    of the ERIs, one algorithm to do this task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/algorithms.svg](../Images/8315333e7eb919a0358c1a5aad382eaf.png)](../_images/algorithms.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm for processing ERIs [see [JCTC, 17, 7486, (2021)](https://doi.org/10.1021/acs.jctc.1c00720)
    for details]
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is suitable for GPUs as it involves many arithmetic operations.
    In addition to this, there are symmetries and properties of the integrals that
    could be used to rearrange the loops in an efficient manner that fit GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Humanities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A brief introduction into some of the work that is being done in the humanities
    that can benefit from utilizing GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Language models and NLP (natural language processing)**'
  prefs: []
  type: TYPE_NORMAL
- en: With the recent popularity of ChatGPT, the use of language models has come into
    the mainstream, however such models have been used in the humanities many years
    already. One of the biggest goals of humanities researchers is working with textual
    data which has increased exponentially over recent years due to the rise in social
    media. Analyzing such textual data to gain insights into questions of sociology,
    linguistics and various other fields have become increasingly reliant on using
    language models. Along with language models, the need for GPU access has become
    essential.
  prefs: []
  type: TYPE_NORMAL
- en: '**Archeology**'
  prefs: []
  type: TYPE_NORMAL
- en: The field of archeology also makes use of GPUs in their 3D modelling and rendering
    work. The biggest problem with archeological sites is that once they are excavated,
    they are destroyed, so any researchers who aren’t present at the site, would lose
    valuable insights into how it looked when it was found. However, with recent developments
    in technology and accessibility to high-performance computing, they are able to
    generate extremely detailed renderings of the excavation sites which act as a
    way to preserve the site for future researchers to gain critical insights and
    contribute to the research.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cognitive Science**'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be
    invaluable in studies that delve into human behavior or population dynamics. MCMC
    sampling allows researchers to simulate and analyze complex systems by iteratively
    sampling from a Markov chain, enabling the exploration of high-dimensional parameter
    spaces. This method is particularly useful when studying human behavior, as it
    can capture the inherent randomness and interdependencies that characterize social
    systems. By leveraging MCMC sampling, researchers can gain insights into various
    aspects of human behavior, such as decision-making, social interactions, and the
    spread of information or diseases within populations.
  prefs: []
  type: TYPE_NORMAL
- en: By offloading the computational workload to GPUs, researchers can experience
    substantial speedup in the execution of MCMC algorithms. This speedup allows for
    more extensive exploration of parameter spaces and facilitates the analysis of
    larger datasets, leading to more accurate and detailed insights into human behavior
    or population dynamics. Examples of studies done using these methods can be found
    at the [Center for Humanities Computing Aarhus](https://chc.au.dk/) (CHCAA) and
    [Interacting Minds Centre](https://interactingminds.au.dk/) (IMC) at Aarhus University.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discussion
  prefs: []
  type: TYPE_NORMAL
- en: What type of problems have you used GPUs for?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large was the performance boost?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good and bad use cases for GPU porting
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following computational tasks is likely to gain the least performance
    benefit from being ported to a GPU?
  prefs: []
  type: TYPE_NORMAL
- en: Training a large, deep neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing a Monte Carlo simulation with a large number of independent trials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing an algorithm with heavy use of recursion and frequent branching.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing a large image with a convolutional filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: The right answer is option 3\. GPUs do not handle recursion and branching as
    effectively as more data-heavy algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: GPUs excel in processing tasks with high data parallelism, such as large-scale
    matrix operations, Fourier transforms, and big data analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs struggle with sequential tasks, problems with extensive control flow divergence,
    low arithmetic intensity tasks, small data sets, and memory-bound problems.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
