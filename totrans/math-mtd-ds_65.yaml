- en: '8.4\. Building blocks of AI 2: stochastic gradient descent#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html](https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Having shown how to compute the gradient, we can now apply gradient descent
    to fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: To get the full gradient, we consider \(n\) samples \((\mathbf{x}_i,y_i)\),
    \(i=1,\ldots,n\). At this point, we make the dependence on \((\mathbf{x}_i, y_i)\)
    explicit. The loss function can be taken as the average of the individual sample
    contributions, so the gradient is obtained by linearity
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla \left(\frac{1}{n} \sum_{i=1}^n f_{\mathbf{x}_i,y_i}(\mathbf{w})\right)
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}), \]
  prefs: []
  type: TYPE_NORMAL
- en: where each term can be computed separately by the procedure above.
  prefs: []
  type: TYPE_NORMAL
- en: We can then apply gradient decent. We start from an arbitrary \(\mathbf{w}^{0}\)
    and update as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \left(\frac{1}{n} \sum_{i=1}^n
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t})\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: In a large dataset, computing the sum over all samples may be prohibitively
    expensive. We present a popular alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1\. Algorithm[#](#algorithm "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
    (SGD)\(\idx{stochastic gradient descent}\xdi\), a variant of gradient descent,
    we pick a sample \(I_t\) uniformly at random in \(\{1,\ldots,n\}\) and update
    as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: More generally, in the so-called mini-batch version of SGD, we pick instead
    a uniformly random sub-sample \(\mathcal{B}_t \subseteq \{1,\ldots,n\}\) of size
    \(b\) without replacement (i.e., all sub-samples of that size have the same probability
    of being picked)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t}
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}). \]
  prefs: []
  type: TYPE_NORMAL
- en: The key observation about the two stochastic updates above is that, in expectation,
    they perform a step of gradient descent. That turns out to be enough and it has
    computational advantages.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Fix a batch size \(1 \leq b \leq n\) and and an arbitrary vector
    of parameters \(\mathbf{w}\). Let \(\mathcal{B} \subseteq \{1,\ldots,n\}\) be
    a uniformly random sub-sample of size \(b\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Because \(\mathcal{B}\) is picked uniformly at random (without replacement),
    for any sub-sample \(B \subseteq \{1,\ldots,n\}\) of size \(b\) without repeats'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[\mathcal{B} = B] = \frac{1}{\binom{n}{b}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So, summing over all such sub-samples, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla
    f_{\mathbf{x}_i,y_i}(\mathbf{w})\right] &= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B}
    = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{B
    \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i
    \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})
    \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Computing the internal sum requires a combinatorial argument. Indeed, \(\sum_{B
    \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\) counts the number of ways that
    \(i\) can be picked in a sub-sample of size \(b\) without repeats. That is \(\binom{n-1}{b-1}\),
    which is the number of ways of picking the remaining \(b-1\) elements of \(B\)
    from the other \(n-1\) possible elements. By definition of the binomial coefficient
    and the properties of factorials,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\binom{n-1}{b-1}}{b \binom{n}{b}} = \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b
    \frac{n!}{b! (n-b)!}} = \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!} = \frac{1}{n}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Plugging back gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first illustration, we return to logistic regression\(\idx{logistic regression}\xdi\).
    Recall that the input data is of the form \(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots,
    n\}\) where \(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in
    \mathbb{R}^d\) are the features and \(b_i \in \{0,1\}\) is the label. As before
    we use a matrix representation: \(A \in \mathbb{R}^{n \times d}\) has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). We
    want to solve the minimization problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the loss is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The gradient was previously computed as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= - \frac{1}{n} \sum_{i=1}^n
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i\\
    &= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: For the mini-batch version of SGD, we pick a random sub-sample \(\mathcal{B}_t
    \subseteq \{1,\ldots,n\}\) of size \(B\) and take the step
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t}
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We modify our previous code for logistic regression. The only change is to pick
    a random mini-batch which can be fed to the descent update sub-routine as dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We analyze a dataset from [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)],
    which can be downloaded [here](https://web.stanford.edu/~hastie/ElemStatLearn/data.html).
    Quoting [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/), Section 4.4.2]'
  prefs: []
  type: TYPE_NORMAL
- en: The data […] are a subset of the Coronary Risk-Factor Study (CORIS) baseline
    survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw
    et al., 1983). The aim of the study was to establish the intensity of ischemic
    heart disease risk factors in that high-incidence region. The data represent white
    males between 15 and 64, and the response variable is the presence or absence
    of myocardial infarction (MI) at the time of the survey (the overall prevalence
    of MI was 5.1% in this region). There are 160 cases in our data set, and a sample
    of 302 controls. These data are described in more detail in Hastie and Tibshirani
    (1987).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We load the data, which we slightly reformatted and look at a summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|  | sbp | tobacco | ldl | adiposity | typea | obesity | alcohol | age | chd
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 160.0 | 12.00 | 5.73 | 23.11 | 49.0 | 25.30 | 97.20 | 52.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 144.0 | 0.01 | 4.41 | 28.61 | 55.0 | 28.87 | 2.06 | 63.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 118.0 | 0.08 | 3.48 | 32.28 | 52.0 | 29.14 | 3.81 | 46.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 170.0 | 7.50 | 6.41 | 38.03 | 51.0 | 31.99 | 24.26 | 58.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 134.0 | 13.60 | 3.50 | 27.78 | 60.0 | 25.99 | 57.34 | 49.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: Our goal to predict `chd`, which stands for coronary heart disease, based on
    the other variables (which are briefly described [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)).
    We use logistic regression again.
  prefs: []
  type: TYPE_NORMAL
- en: We first construct the data matrices. We only use three of the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We try mini-batch SGD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The outcome is harder to vizualize. To get a sense of how accurate the result
    is, we compare our predictions to the true labels. By prediction, let us say that
    we mean that we predict label \(1\) whenever \(\sigma(\boldsymbol{\alpha}^T \mathbf{x})
    > 1/2\). We try this on the training set. (A better approach would be to split
    the data into training and testing sets, but we will not do this here.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.2\. Example: multinomial logistic regression[#](#example-multinomial-logistic-regression
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We give a concrete example of progressive functions and of the application of
    backpropagation and SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a classifier \(h\) takes an input in \(\mathbb{R}^d\) and predicts
    one of \(K\) possible labels. It will be convenient for reasons that will become
    clear below to use [one-hot encoding](https://en.wikipedia.org/wiki/One-hot)\(\idx{one-hot
    encoding}\xdi\) of the labels. That is, we encode label \(i\) as the \(K\)-dimensional
    vector \(\mathbf{e}_i\). Here, as usual, \(\mathbf{e}_i\) the standard basis of
    \(\mathbb{R}^K\), i.e., the vector with a \(1\) in entry \(i\) and a \(0\) elsewhere.
    Furthermore, we allow the output of the classifier to be a probability distribution
    over the labels \(\{1,\ldots,K\}\), that is, a vector in
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Delta_K = \left\{ (p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k =
    1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Observe that \(\mathbf{e}_i\) can itself be thought of as a probability distribution,
    one that assigns probability one to \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Background on multinomial logistic regression** We use [multinomial logistic
    regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\(\idx{multinomial
    logistic regression}\xdi\) to learn a classifier over \(K\) labels. In multinomial
    logistic regression, we once again use an affine function of the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we have \(K\) functions that output a score associated to each label.
    We then transform these scores into a probability distribution over the \(K\)
    labels. There are many ways of doing this. A standard approach is the [softmax
    function](https://en.wikipedia.org/wiki/Softmax_function)\(\idx{softmax}\xdi\)
    \(\bgamma = (\gamma_1,\ldots,\gamma_K)\): for \(\mathbf{z} \in \mathbb{R}^K\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \gamma_i(\mathbf{z}) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \quad i=1,\ldots,K.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: To explain the name, observe that the larger inputs are mapped to larger probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, since a probability distribution must sum to \(1\), it is determined
    by the probabilities assigned to the first \(K-1\) labels. In other words, we
    could drop the score associated to the last label. But the keep the notation simple,
    we will not do this here.
  prefs: []
  type: TYPE_NORMAL
- en: For each \(k\), we have a regression function
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^d w^{(k)}_{j} x_{j} = \mathbf{x}_1^T \mathbf{w}^{(k)}, \quad k=1,\ldots,K
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\) are the parameters
    with \(\mathbf{w}^{(k)} \in \mathbb{R}^{d}\) and \(\mathbf{x} \in \mathbb{R}^d\)
    is the input. A constant term can be included by adding an additional entry \(1\)
    to \(\mathbf{x}\). As we did in the linear regression case, we assume that this
    pre-processing has been performed previously. To simplify the notation, we let
    \(\mathcal{W} \in \mathbb{R}^{K \times d}\) as the matrix with rows \((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\).
  prefs: []
  type: TYPE_NORMAL
- en: The output of the classifier is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bfh(\mathbf{w}) &= \bgamma\left(\mathcal{W} \mathbf{x}\right),
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for \(i=1,\ldots,K\), where \(\bgamma\) is the softmax function. Note that the
    latter has no associated parameter.
  prefs: []
  type: TYPE_NORMAL
- en: It remains to define a loss function. To quantify the fit, it is natural to
    use a notion of distance between probability measures, here between the output
    \(\mathbf{h}(\mathbf{w}) \in \Delta_K\) and the correct label \(\mathbf{y} \in
    \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq \Delta_K\). There are many such
    measures. In multinomial logistic regression, we use the Kullback-Leibler divergence,
    which we have encountered in the context of maximum likelihood estimation. Recall
    that, for two probability distributions \(\mathbf{p}, \mathbf{q} \in \Delta_K\),
    it is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where it will suffice to restrict ourselves to the case \(\mathbf{q} > \mathbf{0}\)
    and where we use the convention \(0 \log 0 = 0\) (so that terms with \(p_i = 0\)
    contribute \(0\) to the sum). Notice that \(\mathbf{p} = \mathbf{q}\) implies
    \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) = 0\). We proved previously that \(\mathrm{KL}(\mathbf{p}
    \| \mathbf{q}) \geq 0\), a result known as *Gibbs’ inequality*.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the loss function, we use the identity \(\log\frac{\alpha}{\beta}
    = \log \alpha - \log \beta\) to re-write
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w})) &= \sum_{i=1}^K
    y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\ &= \sum_{i=1}^K y_i \log y_i - \sum_{i=1}^K
    y_i \log h_{i}(\mathbf{w}), \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bfh = (h_{1},\ldots,h_{K})\). Notice that the first term on right-hand
    side does not depend on \(\mathbf{w}\). Hence we can ignore it when optimizing
    \(\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))\). The remaining term is
  prefs: []
  type: TYPE_NORMAL
- en: \[ H(\mathbf{y}, \bfh(\mathbf{w})) = - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We use it to define our loss function. That is, we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_{i}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Finally,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfh(\mathbf{w}))\\ &= H(\mathbf{y},
    \bfh(\mathbf{w}))\\ &= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
    &= - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**Computing the gradient** We apply the forward and backpropagation steps from
    the previous section. We then use the resulting recursions to derive an analytical
    formula for the gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: The forward pass starts with the initialization \(\mathbf{z}_0 := \mathbf{x}\).
    The forward layer loop has two steps. Set \(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)
    equal to \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\). First we
    compute
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0) &:=\begin{pmatrix} A_0 &
    B_0 \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'where we defined \(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\) as the matrix
    with rows \((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\). We have
    previously computed the Jacobian:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A_0 = \mathbb{A}_{K}[\mathbf{w}_0] = \mathcal{W}_0 = \begin{pmatrix}
    (\mathbf{w}^{(1)}_0)^T\\ \vdots\\ (\mathbf{w}^{(K)}_0)^T \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ B_0 = \mathbb{B}_{K}[\mathbf{z}_0] = I_{K\times K} \otimes \mathbf{z}_0^T
    = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_0^T & \cdots & \mathbf{e}_{K}\mathbf{z}_0^T
    \end{pmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: In the second step of the forward layer loop, we compute
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1) =
    \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1) = J_{\bgamma}(\mathbf{z}_1).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So we need to compute the Jacobian of \(\bgamma\). We divide this computation
    into two cases. When \(1 \leq i = j \leq K\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (A_1)_{ii} &= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)
    - e^{z_{1,i}}\left(e^{z_{1,i}}\right)} {\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
    &= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule).
  prefs: []
  type: TYPE_NORMAL
- en: When \(1 \leq i, j \leq K\) with \(i \neq j\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (A_1)_{ij} &= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)} {\left(\sum_{k=1}^{K}
    e^{z_{1,k}}\right)^2}\\ &= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In matrix form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bgamma}(\mathbf{z}_1) = A_1 = \mathrm{diag}(\bgamma(\mathbf{z}_1)) -
    \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: The Jacobian of the loss function is
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i}
    \right]^T = -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
    = - (\mathbf{y}\oslash\hat{\mathbf{y}})^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\oslash\) is the Hadamard division (i.e., element-wise division).
  prefs: []
  type: TYPE_NORMAL
- en: We summarize the whole procedure next.
  prefs: []
  type: TYPE_NORMAL
- en: '*Initialization:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\mathbf{z}_0 := \mathbf{x}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Forward layer loop:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ \begin{pmatrix} A_0 & B_0 \end{pmatrix} &:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
    = \begin{pmatrix} \mathbb{A}_{K}[\mathbf{w}_0] & \mathbb{B}_{K}[\mathbf{z}_0]
    \end{pmatrix} \end{align*}\]\[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2
    &:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1)
    = \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} z_3 &:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
    \mathbf{p}_2 &:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2) = -\left(\frac{y_1}{z_{2,1}},
    \ldots, \frac{y_K}{z_{2,K}}\right) = - \mathbf{y} \oslash \mathbf{z}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Backward layer loop:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_{1} &:= A_1^T \mathbf{p}_{2} \end{align*}\]\[\begin{align*}
    \mathbf{q}_{0} &:= B_0^T \mathbf{p}_{1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0, \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\mathbf{w} := \mathbf{w}_0\).
  prefs: []
  type: TYPE_NORMAL
- en: Explicit formulas can be derived from the previous recursion.
  prefs: []
  type: TYPE_NORMAL
- en: We first compute \(\mathbf{p}_1\). We use the *Properties of the Hadamard Product*.
    We get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_1 &= A_1^T \mathbf{p}_{2}\\ &= [\mathrm{diag}(\bgamma(\mathbf{z}_1))
    - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T [- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
    &= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))
    + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
    &= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\ &= \bgamma(\mathbf{z}_1)
    - \mathbf{y}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\sum_{k=1}^{K} y_k = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: It remains to compute \(\mathbf{q}_0\). We have by parts (e) and (f) of the
    *Properties of the Kronecker Product*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{q}_{0} = B_0^T \mathbf{p}_{1} &= (I_{K\times K} \otimes
    \mathbf{z}_0^T)^T (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= ( I_{K\times K} \otimes
    \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= (\bgamma(\mathbf{z}_1)
    - \mathbf{y}) \otimes \mathbf{z}_0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, replacing \(\mathbf{z}_0 = \mathbf{x}\) and \(\mathbf{z}_1 = \mathcal{W}
    \mathbf{x}\), the gradient is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0 = (\bgamma\left(\mathcal{W} \mathbf{x}\right)
    - \mathbf{y}) \otimes \mathbf{x}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown that the objective function \(f(\mathbf{w})\) is convex in \(\mathbf{w}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We will use the Fashion-MNIST dataset. This example is
    inspired by [these](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)
    [tutorials](https://www.tensorflow.org/tutorials/keras/classification). We first
    check for the availability of GPUs and load the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We used [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader),
    which provides utilities to load the data in batches for training. We took mini-batches
    of size `BATCH_SIZE = 32` and we apply a random permutation of the samples on
    every pass over the training data (with the option `shuffle=True`). The function
    [`torch.manual_seed()`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html)
    is used to set the global seed for PyTorch operations (e.g., weight initialization).
    The shuffling in `DataLoader` uses its own separate random number generator, which
    we initialize with [`torch.Generator()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator)
    and [`manual_seed()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed).
    (You can tell from the fact that `seed=42` that Claude explained that one to me…)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain the lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: \(\ddagger\)
  prefs: []
  type: TYPE_NORMAL
- en: 'We implement multinomial logistic regression to learn a classifier for the
    Fashion-MNIST data. In PyTorch, composition of functions can be achieved with
    [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).
    Our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)
    layer turns each input image into a vector of size \(784\) (where \(784 = 28^2\)
    is the number of pixels in each image). After the flattening, we have an affine
    map from \(\mathbb{R}^{784}\) to \(\mathbb{R}^{10}\). Note that there is no need
    to pre-process the inputs by adding \(1\)s. A constant term (or “bias variable”)
    is automatically added by PyTorch (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).
    The final output is \(10\)-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are ready to run an optimization method of our choice on the loss
    function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)
    available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)
    for a brief explanation of many common optimizers.) Here we use SGD as the optimizer.
    A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).
    The loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy),
    as implemented by [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),
    which first takes the softmax and expects the labels to be the actual class labels
    rather than their one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We implement special functions for training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: An epoch is one training iteration where all samples are iterated once (in a
    randomly shuffled order). In the interest of time, we train for 10 epochs only.
    But it does better if you train it longer (try it!). On each pass, we compute
    the output of the current model, use `backward()` to obtain the gradient, and
    then perform a descent update with `step()`. We also have to reset the gradients
    first (otherwise they add up by default).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Because of the issue of [overfitting](https://en.wikipedia.org/wiki/Overfitting),
    we use the *test* images to assess the performance of the final classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To make a prediction, we take a [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    of the output of our model. Recall that it is implicitly included in `torch.nn.CrossEntropyLoss`,
    but is not actually part of `model`. (Note that the softmax itself has no parameter.)
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, we do this for each test image. We use [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html)
    to concatenate a sequence of tensors into a single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The result for the first test image is shown below. To make a prediction, we
    choose the label with the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The truth is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Above, `next(iter(test_loader))` loads the first batch of test images. (See
    [here](https://docs.python.org/3/tutorial/classes.html#iterators) for background
    on iterators in Python.)
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In stochastic gradient descent (SGD), how is the gradient estimated at
    each iteration?'
  prefs: []
  type: TYPE_NORMAL
- en: a) By computing the gradient over the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: b) By using the gradient from the previous iteration.
  prefs: []
  type: TYPE_NORMAL
- en: c) By randomly selecting a subset of sample and computing their gradient.
  prefs: []
  type: TYPE_NORMAL
- en: d) By averaging the gradients of all samples in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** What is the key advantage of using mini-batch SGD over standard SGD?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It guarantees faster convergence to the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: b) It reduces the variance of the gradient estimate at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: c) It eliminates the need for computing gradients altogether.
  prefs: []
  type: TYPE_NORMAL
- en: d) It increases the computational cost per iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which of the following statements is true about the update step in stochastic
    gradient descent?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is always equal to the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is always in the opposite direction of the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is, on average, equal to the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: d) It has no relationship to the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In multinomial logistic regression, what is the role of the softmax function
    \(\boldsymbol{\gamma}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To compute the gradient of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: b) To normalize the input features.
  prefs: []
  type: TYPE_NORMAL
- en: c) To transform scores into a probability distribution over labels.
  prefs: []
  type: TYPE_NORMAL
- en: d) To update the model parameters during gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What is the Kullback-Leibler (KL) divergence used for in multinomial
    logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To measure the distance between the predicted probabilities and the true
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: b) To normalize the input features.
  prefs: []
  type: TYPE_NORMAL
- en: c) To update the model parameters during gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: d) To compute the gradient of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: The text states that in SGD, “we pick a sample
    uniformly at random in \(\{1, ..., n\}\) and update as follows \(\mathbf{w}^{t+1}
    = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text implies that mini-batch SGD reduces
    the variance of the gradient estimate compared to standard SGD, which only uses
    a single sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text proves a lemma stating that “in expectation,
    they [stochastic updates] perform a step of gradient descent.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text defines the softmax function and states
    that it is used to “transform these scores into a probability distribution over
    the labels.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: a. Justification: The text introduces the KL divergence as a
    “notion of distance between probability measures” and uses it to define the loss
    function in multinomial logistic regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1\. Algorithm[#](#algorithm "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
    (SGD)\(\idx{stochastic gradient descent}\xdi\), a variant of gradient descent,
    we pick a sample \(I_t\) uniformly at random in \(\{1,\ldots,n\}\) and update
    as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: More generally, in the so-called mini-batch version of SGD, we pick instead
    a uniformly random sub-sample \(\mathcal{B}_t \subseteq \{1,\ldots,n\}\) of size
    \(b\) without replacement (i.e., all sub-samples of that size have the same probability
    of being picked)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t}
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}). \]
  prefs: []
  type: TYPE_NORMAL
- en: The key observation about the two stochastic updates above is that, in expectation,
    they perform a step of gradient descent. That turns out to be enough and it has
    computational advantages.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Fix a batch size \(1 \leq b \leq n\) and and an arbitrary vector
    of parameters \(\mathbf{w}\). Let \(\mathcal{B} \subseteq \{1,\ldots,n\}\) be
    a uniformly random sub-sample of size \(b\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Because \(\mathcal{B}\) is picked uniformly at random (without replacement),
    for any sub-sample \(B \subseteq \{1,\ldots,n\}\) of size \(b\) without repeats'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[\mathcal{B} = B] = \frac{1}{\binom{n}{b}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So, summing over all such sub-samples, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla
    f_{\mathbf{x}_i,y_i}(\mathbf{w})\right] &= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B}
    = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{B
    \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i
    \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})
    \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Computing the internal sum requires a combinatorial argument. Indeed, \(\sum_{B
    \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\) counts the number of ways that
    \(i\) can be picked in a sub-sample of size \(b\) without repeats. That is \(\binom{n-1}{b-1}\),
    which is the number of ways of picking the remaining \(b-1\) elements of \(B\)
    from the other \(n-1\) possible elements. By definition of the binomial coefficient
    and the properties of factorials,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\binom{n-1}{b-1}}{b \binom{n}{b}} = \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b
    \frac{n!}{b! (n-b)!}} = \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!} = \frac{1}{n}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Plugging back gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first illustration, we return to logistic regression\(\idx{logistic regression}\xdi\).
    Recall that the input data is of the form \(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots,
    n\}\) where \(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in
    \mathbb{R}^d\) are the features and \(b_i \in \{0,1\}\) is the label. As before
    we use a matrix representation: \(A \in \mathbb{R}^{n \times d}\) has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). We
    want to solve the minimization problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the loss is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The gradient was previously computed as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= - \frac{1}{n} \sum_{i=1}^n
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i\\
    &= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: For the mini-batch version of SGD, we pick a random sub-sample \(\mathcal{B}_t
    \subseteq \{1,\ldots,n\}\) of size \(B\) and take the step
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t}
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We modify our previous code for logistic regression. The only change is to pick
    a random mini-batch which can be fed to the descent update sub-routine as dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We analyze a dataset from [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)],
    which can be downloaded [here](https://web.stanford.edu/~hastie/ElemStatLearn/data.html).
    Quoting [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/), Section 4.4.2]'
  prefs: []
  type: TYPE_NORMAL
- en: The data […] are a subset of the Coronary Risk-Factor Study (CORIS) baseline
    survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw
    et al., 1983). The aim of the study was to establish the intensity of ischemic
    heart disease risk factors in that high-incidence region. The data represent white
    males between 15 and 64, and the response variable is the presence or absence
    of myocardial infarction (MI) at the time of the survey (the overall prevalence
    of MI was 5.1% in this region). There are 160 cases in our data set, and a sample
    of 302 controls. These data are described in more detail in Hastie and Tibshirani
    (1987).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We load the data, which we slightly reformatted and look at a summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '|  | sbp | tobacco | ldl | adiposity | typea | obesity | alcohol | age | chd
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 160.0 | 12.00 | 5.73 | 23.11 | 49.0 | 25.30 | 97.20 | 52.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 144.0 | 0.01 | 4.41 | 28.61 | 55.0 | 28.87 | 2.06 | 63.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 118.0 | 0.08 | 3.48 | 32.28 | 52.0 | 29.14 | 3.81 | 46.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 170.0 | 7.50 | 6.41 | 38.03 | 51.0 | 31.99 | 24.26 | 58.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 134.0 | 13.60 | 3.50 | 27.78 | 60.0 | 25.99 | 57.34 | 49.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: Our goal to predict `chd`, which stands for coronary heart disease, based on
    the other variables (which are briefly described [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)).
    We use logistic regression again.
  prefs: []
  type: TYPE_NORMAL
- en: We first construct the data matrices. We only use three of the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We try mini-batch SGD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The outcome is harder to vizualize. To get a sense of how accurate the result
    is, we compare our predictions to the true labels. By prediction, let us say that
    we mean that we predict label \(1\) whenever \(\sigma(\boldsymbol{\alpha}^T \mathbf{x})
    > 1/2\). We try this on the training set. (A better approach would be to split
    the data into training and testing sets, but we will not do this here.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.2\. Example: multinomial logistic regression[#](#example-multinomial-logistic-regression
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We give a concrete example of progressive functions and of the application of
    backpropagation and SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a classifier \(h\) takes an input in \(\mathbb{R}^d\) and predicts
    one of \(K\) possible labels. It will be convenient for reasons that will become
    clear below to use [one-hot encoding](https://en.wikipedia.org/wiki/One-hot)\(\idx{one-hot
    encoding}\xdi\) of the labels. That is, we encode label \(i\) as the \(K\)-dimensional
    vector \(\mathbf{e}_i\). Here, as usual, \(\mathbf{e}_i\) the standard basis of
    \(\mathbb{R}^K\), i.e., the vector with a \(1\) in entry \(i\) and a \(0\) elsewhere.
    Furthermore, we allow the output of the classifier to be a probability distribution
    over the labels \(\{1,\ldots,K\}\), that is, a vector in
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Delta_K = \left\{ (p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k =
    1 \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Observe that \(\mathbf{e}_i\) can itself be thought of as a probability distribution,
    one that assigns probability one to \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Background on multinomial logistic regression** We use [multinomial logistic
    regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\(\idx{multinomial
    logistic regression}\xdi\) to learn a classifier over \(K\) labels. In multinomial
    logistic regression, we once again use an affine function of the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we have \(K\) functions that output a score associated to each label.
    We then transform these scores into a probability distribution over the \(K\)
    labels. There are many ways of doing this. A standard approach is the [softmax
    function](https://en.wikipedia.org/wiki/Softmax_function)\(\idx{softmax}\xdi\)
    \(\bgamma = (\gamma_1,\ldots,\gamma_K)\): for \(\mathbf{z} \in \mathbb{R}^K\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \gamma_i(\mathbf{z}) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \quad i=1,\ldots,K.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: To explain the name, observe that the larger inputs are mapped to larger probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, since a probability distribution must sum to \(1\), it is determined
    by the probabilities assigned to the first \(K-1\) labels. In other words, we
    could drop the score associated to the last label. But the keep the notation simple,
    we will not do this here.
  prefs: []
  type: TYPE_NORMAL
- en: For each \(k\), we have a regression function
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^d w^{(k)}_{j} x_{j} = \mathbf{x}_1^T \mathbf{w}^{(k)}, \quad k=1,\ldots,K
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\) are the parameters
    with \(\mathbf{w}^{(k)} \in \mathbb{R}^{d}\) and \(\mathbf{x} \in \mathbb{R}^d\)
    is the input. A constant term can be included by adding an additional entry \(1\)
    to \(\mathbf{x}\). As we did in the linear regression case, we assume that this
    pre-processing has been performed previously. To simplify the notation, we let
    \(\mathcal{W} \in \mathbb{R}^{K \times d}\) as the matrix with rows \((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\).
  prefs: []
  type: TYPE_NORMAL
- en: The output of the classifier is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bfh(\mathbf{w}) &= \bgamma\left(\mathcal{W} \mathbf{x}\right),
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for \(i=1,\ldots,K\), where \(\bgamma\) is the softmax function. Note that the
    latter has no associated parameter.
  prefs: []
  type: TYPE_NORMAL
- en: It remains to define a loss function. To quantify the fit, it is natural to
    use a notion of distance between probability measures, here between the output
    \(\mathbf{h}(\mathbf{w}) \in \Delta_K\) and the correct label \(\mathbf{y} \in
    \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq \Delta_K\). There are many such
    measures. In multinomial logistic regression, we use the Kullback-Leibler divergence,
    which we have encountered in the context of maximum likelihood estimation. Recall
    that, for two probability distributions \(\mathbf{p}, \mathbf{q} \in \Delta_K\),
    it is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where it will suffice to restrict ourselves to the case \(\mathbf{q} > \mathbf{0}\)
    and where we use the convention \(0 \log 0 = 0\) (so that terms with \(p_i = 0\)
    contribute \(0\) to the sum). Notice that \(\mathbf{p} = \mathbf{q}\) implies
    \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) = 0\). We proved previously that \(\mathrm{KL}(\mathbf{p}
    \| \mathbf{q}) \geq 0\), a result known as *Gibbs’ inequality*.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the loss function, we use the identity \(\log\frac{\alpha}{\beta}
    = \log \alpha - \log \beta\) to re-write
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w})) &= \sum_{i=1}^K
    y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\ &= \sum_{i=1}^K y_i \log y_i - \sum_{i=1}^K
    y_i \log h_{i}(\mathbf{w}), \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bfh = (h_{1},\ldots,h_{K})\). Notice that the first term on right-hand
    side does not depend on \(\mathbf{w}\). Hence we can ignore it when optimizing
    \(\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))\). The remaining term is
  prefs: []
  type: TYPE_NORMAL
- en: \[ H(\mathbf{y}, \bfh(\mathbf{w})) = - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We use it to define our loss function. That is, we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_{i}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Finally,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfh(\mathbf{w}))\\ &= H(\mathbf{y},
    \bfh(\mathbf{w}))\\ &= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
    &= - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**Computing the gradient** We apply the forward and backpropagation steps from
    the previous section. We then use the resulting recursions to derive an analytical
    formula for the gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: The forward pass starts with the initialization \(\mathbf{z}_0 := \mathbf{x}\).
    The forward layer loop has two steps. Set \(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)
    equal to \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\). First we
    compute
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0) &:=\begin{pmatrix} A_0 &
    B_0 \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'where we defined \(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\) as the matrix
    with rows \((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\). We have
    previously computed the Jacobian:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A_0 = \mathbb{A}_{K}[\mathbf{w}_0] = \mathcal{W}_0 = \begin{pmatrix}
    (\mathbf{w}^{(1)}_0)^T\\ \vdots\\ (\mathbf{w}^{(K)}_0)^T \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ B_0 = \mathbb{B}_{K}[\mathbf{z}_0] = I_{K\times K} \otimes \mathbf{z}_0^T
    = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_0^T & \cdots & \mathbf{e}_{K}\mathbf{z}_0^T
    \end{pmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: In the second step of the forward layer loop, we compute
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1) =
    \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1) = J_{\bgamma}(\mathbf{z}_1).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So we need to compute the Jacobian of \(\bgamma\). We divide this computation
    into two cases. When \(1 \leq i = j \leq K\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (A_1)_{ii} &= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)
    - e^{z_{1,i}}\left(e^{z_{1,i}}\right)} {\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
    &= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule).
  prefs: []
  type: TYPE_NORMAL
- en: When \(1 \leq i, j \leq K\) with \(i \neq j\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (A_1)_{ij} &= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)} {\left(\sum_{k=1}^{K}
    e^{z_{1,k}}\right)^2}\\ &= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In matrix form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bgamma}(\mathbf{z}_1) = A_1 = \mathrm{diag}(\bgamma(\mathbf{z}_1)) -
    \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: The Jacobian of the loss function is
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i}
    \right]^T = -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
    = - (\mathbf{y}\oslash\hat{\mathbf{y}})^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\oslash\) is the Hadamard division (i.e., element-wise division).
  prefs: []
  type: TYPE_NORMAL
- en: We summarize the whole procedure next.
  prefs: []
  type: TYPE_NORMAL
- en: '*Initialization:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\mathbf{z}_0 := \mathbf{x}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Forward layer loop:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ \begin{pmatrix} A_0 & B_0 \end{pmatrix} &:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
    = \begin{pmatrix} \mathbb{A}_{K}[\mathbf{w}_0] & \mathbb{B}_{K}[\mathbf{z}_0]
    \end{pmatrix} \end{align*}\]\[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2
    &:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1)
    = \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} z_3 &:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
    \mathbf{p}_2 &:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2) = -\left(\frac{y_1}{z_{2,1}},
    \ldots, \frac{y_K}{z_{2,K}}\right) = - \mathbf{y} \oslash \mathbf{z}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Backward layer loop:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_{1} &:= A_1^T \mathbf{p}_{2} \end{align*}\]\[\begin{align*}
    \mathbf{q}_{0} &:= B_0^T \mathbf{p}_{1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0, \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that \(\mathbf{w} := \mathbf{w}_0\).
  prefs: []
  type: TYPE_NORMAL
- en: Explicit formulas can be derived from the previous recursion.
  prefs: []
  type: TYPE_NORMAL
- en: We first compute \(\mathbf{p}_1\). We use the *Properties of the Hadamard Product*.
    We get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_1 &= A_1^T \mathbf{p}_{2}\\ &= [\mathrm{diag}(\bgamma(\mathbf{z}_1))
    - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T [- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
    &= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))
    + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
    &= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\ &= \bgamma(\mathbf{z}_1)
    - \mathbf{y}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\sum_{k=1}^{K} y_k = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: It remains to compute \(\mathbf{q}_0\). We have by parts (e) and (f) of the
    *Properties of the Kronecker Product*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{q}_{0} = B_0^T \mathbf{p}_{1} &= (I_{K\times K} \otimes
    \mathbf{z}_0^T)^T (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= ( I_{K\times K} \otimes
    \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= (\bgamma(\mathbf{z}_1)
    - \mathbf{y}) \otimes \mathbf{z}_0. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Finally, replacing \(\mathbf{z}_0 = \mathbf{x}\) and \(\mathbf{z}_1 = \mathcal{W}
    \mathbf{x}\), the gradient is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0 = (\bgamma\left(\mathcal{W} \mathbf{x}\right)
    - \mathbf{y}) \otimes \mathbf{x}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown that the objective function \(f(\mathbf{w})\) is convex in \(\mathbf{w}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We will use the Fashion-MNIST dataset. This example is
    inspired by [these](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)
    [tutorials](https://www.tensorflow.org/tutorials/keras/classification). We first
    check for the availability of GPUs and load the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We used [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader),
    which provides utilities to load the data in batches for training. We took mini-batches
    of size `BATCH_SIZE = 32` and we apply a random permutation of the samples on
    every pass over the training data (with the option `shuffle=True`). The function
    [`torch.manual_seed()`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html)
    is used to set the global seed for PyTorch operations (e.g., weight initialization).
    The shuffling in `DataLoader` uses its own separate random number generator, which
    we initialize with [`torch.Generator()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator)
    and [`manual_seed()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed).
    (You can tell from the fact that `seed=42` that Claude explained that one to me…)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain the lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: \(\ddagger\)
  prefs: []
  type: TYPE_NORMAL
- en: 'We implement multinomial logistic regression to learn a classifier for the
    Fashion-MNIST data. In PyTorch, composition of functions can be achieved with
    [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).
    Our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)
    layer turns each input image into a vector of size \(784\) (where \(784 = 28^2\)
    is the number of pixels in each image). After the flattening, we have an affine
    map from \(\mathbb{R}^{784}\) to \(\mathbb{R}^{10}\). Note that there is no need
    to pre-process the inputs by adding \(1\)s. A constant term (or “bias variable”)
    is automatically added by PyTorch (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).
    The final output is \(10\)-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are ready to run an optimization method of our choice on the loss
    function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)
    available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)
    for a brief explanation of many common optimizers.) Here we use SGD as the optimizer.
    A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).
    The loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy),
    as implemented by [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),
    which first takes the softmax and expects the labels to be the actual class labels
    rather than their one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We implement special functions for training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: An epoch is one training iteration where all samples are iterated once (in a
    randomly shuffled order). In the interest of time, we train for 10 epochs only.
    But it does better if you train it longer (try it!). On each pass, we compute
    the output of the current model, use `backward()` to obtain the gradient, and
    then perform a descent update with `step()`. We also have to reset the gradients
    first (otherwise they add up by default).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Because of the issue of [overfitting](https://en.wikipedia.org/wiki/Overfitting),
    we use the *test* images to assess the performance of the final classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: To make a prediction, we take a [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    of the output of our model. Recall that it is implicitly included in `torch.nn.CrossEntropyLoss`,
    but is not actually part of `model`. (Note that the softmax itself has no parameter.)
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, we do this for each test image. We use [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html)
    to concatenate a sequence of tensors into a single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The result for the first test image is shown below. To make a prediction, we
    choose the label with the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The truth is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Above, `next(iter(test_loader))` loads the first batch of test images. (See
    [here](https://docs.python.org/3/tutorial/classes.html#iterators) for background
    on iterators in Python.)
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In stochastic gradient descent (SGD), how is the gradient estimated at
    each iteration?'
  prefs: []
  type: TYPE_NORMAL
- en: a) By computing the gradient over the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: b) By using the gradient from the previous iteration.
  prefs: []
  type: TYPE_NORMAL
- en: c) By randomly selecting a subset of sample and computing their gradient.
  prefs: []
  type: TYPE_NORMAL
- en: d) By averaging the gradients of all samples in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** What is the key advantage of using mini-batch SGD over standard SGD?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It guarantees faster convergence to the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: b) It reduces the variance of the gradient estimate at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: c) It eliminates the need for computing gradients altogether.
  prefs: []
  type: TYPE_NORMAL
- en: d) It increases the computational cost per iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which of the following statements is true about the update step in stochastic
    gradient descent?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is always equal to the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is always in the opposite direction of the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is, on average, equal to the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: d) It has no relationship to the full gradient descent update.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In multinomial logistic regression, what is the role of the softmax function
    \(\boldsymbol{\gamma}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To compute the gradient of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: b) To normalize the input features.
  prefs: []
  type: TYPE_NORMAL
- en: c) To transform scores into a probability distribution over labels.
  prefs: []
  type: TYPE_NORMAL
- en: d) To update the model parameters during gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What is the Kullback-Leibler (KL) divergence used for in multinomial
    logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To measure the distance between the predicted probabilities and the true
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: b) To normalize the input features.
  prefs: []
  type: TYPE_NORMAL
- en: c) To update the model parameters during gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: d) To compute the gradient of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: The text states that in SGD, “we pick a sample
    uniformly at random in \(\{1, ..., n\}\) and update as follows \(\mathbf{w}^{t+1}
    = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text implies that mini-batch SGD reduces
    the variance of the gradient estimate compared to standard SGD, which only uses
    a single sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text proves a lemma stating that “in expectation,
    they [stochastic updates] perform a step of gradient descent.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text defines the softmax function and states
    that it is used to “transform these scores into a probability distribution over
    the labels.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: a. Justification: The text introduces the KL divergence as a
    “notion of distance between probability measures” and uses it to define the loss
    function in multinomial logistic regression.'
  prefs: []
  type: TYPE_NORMAL
