- en: '8.2\. Background: Jacobian, chain rule, and a brief introduction to automatic
    differentiation#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.2\. 背景：雅可比、链式法则以及自动微分的简要介绍#
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/02_chain/roch-mmids-nn-chain.html](https://mmids-textbook.github.io/chap08_nn/02_chain/roch-mmids-nn-chain.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap08_nn/02_chain/roch-mmids-nn-chain.html](https://mmids-textbook.github.io/chap08_nn/02_chain/roch-mmids-nn-chain.html)
- en: We introduce the Jacobian of a vector-valued function of several variables as
    well as the *Chain Rule* for this more general setting. We also give a brief introduction
    to automatic differentiation. We begin with some additional matrix algebra.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入多个变量的向量值函数的雅可比以及在此更一般设置下的**链式法则**。我们还简要介绍了自动微分。我们首先介绍一些额外的矩阵代数。
- en: '8.2.1\. More matrix algebra: Hadamard and Kronecker products[#](#more-matrix-algebra-hadamard-and-kronecker-products
    "Link to this heading")'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.1\. 更多的矩阵代数：Hadamard 和 Kronecker 积[#](#more-matrix-algebra-hadamard-and-kronecker-products
    "链接到这个标题")
- en: First, we introduce the Hadamard product\(\idx{Hadamard product}\xdi\) and division\(\idx{Hadamard
    division}\xdi\). The Hadamard product of two matrices (or vectors) of the same
    dimension, \(A = (a_{i,j})_{i \in [n], j \in [m]}, B = (b_{i,j})_{i\in [n], j
    \in [m]} \in \mathbb{R}^{n \times m}\), is defined as the element-wise product
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍Hadamard积\(\idx{Hadamard 积}\xdi\)和除法\(\idx{Hadamard 除法}\xdi\)。相同维度的两个矩阵（或向量）的Hadamard积，\(A
    = (a_{i,j})_{i \in [n], j \in [m]}, B = (b_{i,j})_{i\in [n], j \in [m]} \in \mathbb{R}^{n
    \times m}\)，定义为逐元素乘积
- en: \[ A \odot B = (a_{i,j} b_{i,j})_{i\in [n], j \in [m]}. \]
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \odot B = (a_{i,j} b_{i,j})_{i\in [n], j \in [m]}. \]
- en: Similarly the Hadamard division is defined as the element-wise division
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Hadamard 除法定义为逐元素除法
- en: \[ A \oslash B = (a_{i,j} / b_{i,j})_{i\in [n], j \in [m]} \]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \oslash B = (a_{i,j} / b_{i,j})_{i\in [n], j \in [m]} \]
- en: where we assume that \(b_{i,j} \neq 0\) for all \(i,j\).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们假设对于所有的 \(i,j\)，都有 \(b_{i,j} \neq 0\)。
- en: '**EXAMPLE:** As an illustrative example,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 作为一个说明性的例子，'
- en: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \odot \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \times 0 & 2 \times 5\\ 3
    \times 6 & 4 \times 7\\ \end{bmatrix} = \begin{bmatrix} 0 & 10\\ 18 & 28\\ \end{bmatrix}.
    \end{split}\]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \odot \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \times 0 & 2 \times 5\\ 3
    \times 6 & 4 \times 7\\ \end{bmatrix} = \begin{bmatrix} 0 & 10\\ 18 & 28\\ \end{bmatrix}.
    \end{split}\]
- en: \(\lhd\)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: Recall that \(\mathbf{1}\) is the all-one vector and that, for \(\mathbf{x}
    = (x_1,\ldots,x_n) \in \mathbb{R}^n\), \(\mathrm{diag}(\mathbf{x}) \in \mathbb{R}^{n
    \times n}\) is the diagonal matrix with diagonal entries \(x_1,\ldots,x_n\).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，\(\mathbf{1}\) 是全一向量，并且对于 \(\mathbf{x} = (x_1,\ldots,x_n) \in \mathbb{R}^n\)，\(\mathrm{diag}(\mathbf{x})
    \in \mathbb{R}^{n \times n}\) 是对角线元素为 \(x_1,\ldots,x_n\) 的对角矩阵。
- en: '**LEMMA** **(Properties of the Hadamard Product)** \(\idx{properties of the
    Hadamard product}\xdi\) Let \(\mathbf{a} = (a_1,\ldots,a_n), \mathbf{b} = (b_1,\ldots,b_n),
    \mathbf{c} = (c_1,\ldots,c_n) \in \mathbb{R}^n\). Then the following hold:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(Hadamard 积的性质)** \(\idx{Hadamard 积的性质}\xdi\) 设 \(\mathbf{a} = (a_1,\ldots,a_n),
    \mathbf{b} = (b_1,\ldots,b_n), \mathbf{c} = (c_1,\ldots,c_n) \in \mathbb{R}^n\)。那么以下性质成立：'
- en: a) \(\mathrm{diag}(\mathbf{a}) \, \mathbf{b} = \mathrm{diag}(\mathbf{a} \odot
    \mathbf{b})\);
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathrm{diag}(\mathbf{a}) \, \mathbf{b} = \mathrm{diag}(\mathbf{a} \odot
    \mathbf{b})\);
- en: b) \(\mathbf{a}^T(\mathbf{b} \odot \mathbf{c}) = \mathbf{1}^T(\mathbf{a} \odot
    \mathbf{b} \odot \mathbf{c})\)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{a}^T(\mathbf{b} \odot \mathbf{c}) = \mathbf{1}^T(\mathbf{a} \odot
    \mathbf{b} \odot \mathbf{c})\)
- en: 'and, provided \(a_i \neq 0\) for all \(i\), the following hold as well:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，当 \(a_i \neq 0\) 对于所有的 \(i\) 成立时，以下性质也成立：
- en: c) \(\mathrm{diag}(\mathbf{a}) \, (\mathbf{b} \oslash \mathbf{a}) = \mathrm{diag}(\mathbf{b})\);
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathrm{diag}(\mathbf{a}) \, (\mathbf{b} \oslash \mathbf{a}) = \mathrm{diag}(\mathbf{b})\);
- en: d) \(\mathbf{a}^T \, (\mathbf{b} \oslash \mathbf{a}) = \mathbf{1}^T \mathbf{b}\).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbf{a}^T \, (\mathbf{b} \oslash \mathbf{a}) = \mathbf{1}^T \mathbf{b}\).
- en: \(\flat\)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* a) The product of a diagonal matrix and a vector produces a new vector
    whose original coordinates are multiplied by the corresponding diagonal entry.
    That proves the first claim.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：** a) 对角矩阵与向量的乘积产生一个新的向量，其原始坐标被相应的对角线元素相乘。这证明了第一个命题。'
- en: b) We have
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: b) 我们有
- en: \[ \mathbf{a}^T \, (\mathbf{b} \odot \mathbf{c}) = \sum_{i=1}^n a_i (b_i c_i)
    = \mathbf{1}^T (\mathbf{a} \odot \mathbf{b} \odot \mathbf{c}). \]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{a}^T \, (\mathbf{b} \odot \mathbf{c}) = \sum_{i=1}^n a_i (b_i c_i)
    = \mathbf{1}^T (\mathbf{a} \odot \mathbf{b} \odot \mathbf{c}). \]
- en: c) and d) follow respectively from a) and b).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: c) 和 d) 分别由 a) 和 b) 推出。
- en: \(\square\)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: Second, we introduce the Kronecker product\(\idx{Kronecker product}\xdi\). Let
    \(A = (a_{i,j})_{i \in [n], j \in [m]} \in \mathbb{R}^{n \times m}\) and \(B =
    (b_{i,j})_{i \in [p], j \in [q]} \in \mathbb{R}^{p \times q}\) be arbitrary matrices.
    Their Kronecker product, denoted \(A \otimes B \in \mathbb{R}^{np \times mq}\),
    is the following matrix in block form
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们介绍克罗内克积\(\idx{克罗内克积}\xdi\)。设 \(A = (a_{i,j})_{i \in [n], j \in [m]} \in
    \mathbb{R}^{n \times m}\) 和 \(B = (b_{i,j})_{i \in [p], j \in [q]} \in \mathbb{R}^{p
    \times q}\) 是任意矩阵。它们的克罗内克积，记作 \(A \otimes B \in \mathbb{R}^{np \times mq}\)，是以下形式的矩阵
- en: \[\begin{split} A \otimes B = \begin{pmatrix} a_{1,1} B & \cdots & a_{1,m} B
    \\ \vdots & \ddots & \vdots \\ a_{n,1} B & \cdots & a_{n,m} B \end{pmatrix}. \end{split}\]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} a_{1,1} B & \cdots & a_{1,m} B
    \\ \vdots & \ddots & \vdots \\ a_{n,1} B & \cdots & a_{n,m} B \end{pmatrix}. \end{split}\]
- en: '**EXAMPLE:** Here is a simple illustrative example from [Wikipedia](https://en.wikipedia.org/wiki/Kronecker_product#Examples):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：以下是一个来自 [维基百科](https://en.wikipedia.org/wiki/Kronecker_product#Examples)
    的简单说明性示例：'
- en: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \otimes \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} & 2 \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \\ 3
    \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} & 4 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 5 & 0 & 10 \\ 6 &
    7 & 12 & 14 \\ 0 & 15 & 0 & 20 \\ 18 & 21 & 24 & 28 \end{bmatrix}. \end{split}\]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \otimes \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} & 2 \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \\ 3
    \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} & 4 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 5 & 0 & 10 \\ 6 &
    7 & 12 & 14 \\ 0 & 15 & 0 & 20 \\ 18 & 21 & 24 & 28 \end{bmatrix}. \end{split}\]
- en: \(\lhd\)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Outer product)** \(\idx{outer product}\xdi\) Here is another
    example we have encoutered previously, the outer product of two vectors \(\mathbf{u}
    = (u_1,\ldots,u_n) \in \mathbb{R}^n\) and \(\mathbf{v} = (v_1,\ldots, v_m) \in
    \mathbb{R}^m\). Recall that the outer product is defined in block form as the
    \(n \times m\) matrix'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：**（外积）** \(\idx{外积}\xdi\) 这是另一个我们之前遇到的例子，两个向量 \(\mathbf{u} = (u_1,\ldots,u_n)
    \in \mathbb{R}^n\) 和 \(\mathbf{v} = (v_1,\ldots, v_m) \in \mathbb{R}^m\) 的外积。回忆一下，外积按块形式定义为
    \(n \times m\) 矩阵'
- en: \[ \mathbf{u} \mathbf{v}^T = \begin{pmatrix} v_1 \mathbf{u} & \cdots & v_m \mathbf{u}
    \end{pmatrix} = \mathbf{v}^T \otimes \mathbf{u}. \]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{u} \mathbf{v}^T = \begin{pmatrix} v_1 \mathbf{u} & \cdots & v_m \mathbf{u}
    \end{pmatrix} = \mathbf{v}^T \otimes \mathbf{u}. \]
- en: Equivalently,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 等价地，
- en: \[\begin{split} \mathbf{u} \mathbf{v}^T = \begin{pmatrix} u_1 \mathbf{v}^T\\
    \vdots\\ u_n \mathbf{v}^T \end{pmatrix} = \mathbf{u} \otimes \mathbf{v}^T. \end{split}\]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u} \mathbf{v}^T = \begin{pmatrix} u_1 \mathbf{v}^T\\
    \vdots\\ u_n \mathbf{v}^T \end{pmatrix} = \mathbf{u} \otimes \mathbf{v}^T. \end{split}\]
- en: \(\lhd\)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(continued)** In the previous example the Kronecker product
    turned out to be commutative (i.e., we had \(\mathbf{v}^T \otimes \mathbf{u} =
    \mathbf{u} \otimes \mathbf{v}^T\)). This is not the case in general. Going back
    to the first example above, note that'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：**（续）** 在前面的例子中，克罗内克积是可交换的（即，我们有 \(\mathbf{v}^T \otimes \mathbf{u} =
    \mathbf{u} \otimes \mathbf{v}^T\)）。在一般情况下并非如此。回到上面的第一个例子，请注意'
- en: \[\begin{split} \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \otimes \begin{bmatrix}
    1 & 2 \\ 3 & 4 \\ \end{bmatrix} = \begin{bmatrix} 0 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} & 5 \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \\ 6
    \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} & 7 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 0 & 5 & 10 \\ 0 &
    0 & 15 & 20 \\ 6 & 12 & 7 & 14 \\ 18 & 24 & 21 & 28 \end{bmatrix}. \end{split}\]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \otimes \begin{bmatrix}
    1 & 2 \\ 3 & 4 \\ \end{bmatrix} = \begin{bmatrix} 0 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} & 5 \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \\ 6
    \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} & 7 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 0 & 5 & 10 \\ 0 &
    0 & 15 & 20 \\ 6 & 12 & 7 & 14 \\ 18 & 24 & 21 & 28 \end{bmatrix}. \end{split}\]
- en: You can check that this is different from what we obtained in the opposite order.
    \(\lhd\)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以检查，这与我们按相反顺序得到的结果不同。\(\lhd\)
- en: The proof of the following lemma is left as an exercise.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下引理的证明留给读者作为练习。
- en: '**LEMMA** **(Properties of the Kronecker Product)** \(\idx{properties of the
    Kronecker Product}\xdi\) The Kronecker product has the following properties:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（克罗内克积的性质）** \(\idx{克罗内克积的性质}\xdi\) 克罗内克积具有以下性质：'
- en: a) If \(B, C\) are matrices of the same dimension
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: a) 如果 \(B, C\) 是相同维度的矩阵
- en: \[ A \otimes (B + C) = A \otimes B + A \otimes C \quad \text{and}\quad (B +
    C) \otimes A = B \otimes A + C \otimes A. \]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \otimes (B + C) = A \otimes B + A \otimes C \quad \text{并且}\quad (B + C)
    \otimes A = B \otimes A + C \otimes A. \]
- en: b) If \(A, B, C, D\) are matrices of such size that one can form the matrix
    products \(AC\) and \(BD\), then
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: b) 如果 \(A, B, C, D\) 是可以形成矩阵乘积 \(AC\) 和 \(BD\) 的矩阵，那么
- en: \[ (A \otimes B)\,(C \otimes D) = (AC) \otimes (BD). \]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)\,(C \otimes D) = (AC) \otimes (BD). \]
- en: c) If \(A, C\) are matrices of the same dimension and \(B, D\) are matrices
    of the same dimension, then
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: c) 如果 \(A, C\) 是相同维度的矩阵，且 \(B, D\) 是相同维度的矩阵，那么
- en: \[ (A \otimes B)\odot(C \otimes D) = (A\odot C) \otimes (B\odot D). \]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)\odot(C \otimes D) = (A\odot C) \otimes (B\odot D). \]
- en: d) If \(A,B\) are invertible, then
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: d) 如果 \(A,B\) 是可逆的，那么
- en: \[ (A \otimes B)^{-1} = A^{-1} \otimes B^{-1}. \]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)^{-1} = A^{-1} \otimes B^{-1}. \]
- en: e) The transpose of \(A \otimes B\) is
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: e) \(A \otimes B\) 的转置是
- en: \[ (A \otimes B)^T = A^T \otimes B^T. \]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)^T = A^T \otimes B^T. \]
- en: f) If \(\mathbf{u}\) is a column vector and \(A, B\) are matrices of such size
    that one can form the matrix product \(AB\), then
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: f) 如果 \(\mathbf{u}\) 是一个列向量，且 \(A, B\) 是可以形成矩阵乘积 \(AB\) 的矩阵，那么
- en: \[ (\mathbf{u} \otimes A) B = \mathbf{u} \otimes (AB) \quad\text{and}\quad (A
    \otimes \mathbf{u}) B = (AB) \otimes \mathbf{u}. \]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{u} \otimes A) B = \mathbf{u} \otimes (AB) \quad\text{并且}\quad (A
    \otimes \mathbf{u}) B = (AB) \otimes \mathbf{u}. \]
- en: Similarly,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，
- en: \[ A \,(\mathbf{u}^T \otimes B) = \mathbf{u}^T \otimes (AB) \quad\text{and}\quad
    A \,(B \otimes \mathbf{u}^T) = (AB) \otimes \mathbf{u}^T. \]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \,(\mathbf{u}^T \otimes B) = \mathbf{u}^T \otimes (AB) \quad\text{并且}\quad
    A \,(B \otimes \mathbf{u}^T) = (AB) \otimes \mathbf{u}^T. \]
- en: \(\flat\)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: 8.2.2\. Jacobian[#](#jacobian "Link to this heading")
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.2\. 雅可比矩阵[#](#jacobian "链接到这个标题")
- en: Recall that the derivative of a function of a real variable is the rate of change
    of the function with respect to the change in the variable. A different way to
    put this is that \(f'(x)\) is the slope of the tangent line to \(f\) at \(x\).
    Formally, one can approximate \(f(x)\) by a linear function in the neighborhood
    of \(x\) as follows
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，实变函数的导数是函数相对于变量变化的速率。另一种说法是 \(f'(x)\) 是 \(f\) 在 \(x\) 处的切线斜率。形式上，可以在 \(x\)
    的邻域内通过以下线性函数来近似 \(f(x)\)
- en: \[ f(x + h) = f(x) + f'(x) h + r(h), \]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x + h) = f(x) + f'(x) h + r(h), \]
- en: where \(r(h)\) is negligible compared to \(h\) in the sense that
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(r(h)\) 相对于 \(h\) 是可以忽略的，
- en: \[ \lim_{h\to 0} \frac{r(h)}{h} = 0. \]
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{h\to 0} \frac{r(h)}{h} = 0. \]
- en: Indeed, define
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，定义
- en: \[ r(h) = f(x + h) - f(x) - f'(x) h. \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r(h) = f(x + h) - f(x) - f'(x) h. \]
- en: Then by definition of the derivative
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后根据导数的定义
- en: \[ \lim_{h\to 0} \frac{r(h)}{h} = \lim_{h\to 0} \frac{f(x + h) - f(x) - f'(x)
    h}{h} = \lim_{h\to 0} \left[\frac{f(x + h) - f(x)}{h} - f'(x) \right] = 0. \]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{h\to 0} \frac{r(h)}{h} = \lim_{h\to 0} \frac{f(x + h) - f(x) - f'(x)
    h}{h} = \lim_{h\to 0} \left[\frac{f(x + h) - f(x)}{h} - f'(x) \right] = 0. \]
- en: 'For vector-valued functions, we have the following generalization. Let \(\mathbf{f}
    = (f_1, \ldots, f_m) : D \to \mathbb{R}^m\) where \(D \subseteq \mathbb{R}^d\)
    and let \(\mathbf{x} \in D\) be an interior point of \(D\). We say that \(\mathbf{f}\)
    is diffentiable\(\idx{diffentiable}\xdi\) at \(\mathbf{x}\) if there exists a
    matrix \(A \in \mathbb{R}^{m \times d}\) such that'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '对于向量值函数，我们有以下推广。设 \(\mathbf{f} = (f_1, \ldots, f_m) : D \to \mathbb{R}^m\)，其中
    \(D \subseteq \mathbb{R}^d\)，并且设 \(\mathbf{x} \in D\) 是 \(D\) 的一个内点。我们说 \(\mathbf{f}\)
    在 \(\mathbf{x}\) 处可微\(\idx{可微}\xdi\)，如果存在一个矩阵 \(A \in \mathbb{R}^{m \times d}\)
    使得'
- en: \[ \mathbf{f}(\mathbf{x}+\mathbf{h}) = \mathbf{f}(\mathbf{x}) + A \mathbf{h}
    + \mathbf{r}(\mathbf{h}) \]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}+\mathbf{h}) = \mathbf{f}(\mathbf{x}) + A \mathbf{h}
    + \mathbf{r}(\mathbf{h}) \]
- en: where
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[ \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{r}(\mathbf{h})\|_2}{\|\mathbf{h}\|_2}
    = 0. \]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{r}(\mathbf{h})\|_2}{\|\mathbf{h}\|_2}
    = 0. \]
- en: The matrix \(\mathbf{f}'(\mathbf{x}) = A\) is called the differential\(\idx{differential}\xdi\)
    of \(\mathbf{f}\) at \(\mathbf{x}\), and we see that the affine map \(\mathbf{f}(\mathbf{x})
    + A \mathbf{h}\) provides an approximation of \(\mathbf{f}\) in the neighborhood
    of \(\mathbf{x}\).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(\mathbf{f}'(\mathbf{x}) = A\) 被称为 \(\mathbf{f}\) 在 \(\mathbf{x}\) 处的微分\(\idx{微分}\xdi\)，并且我们看到仿射映射
    \(\mathbf{f}(\mathbf{x}) + A \mathbf{h}\) 提供了 \(\mathbf{f}\) 在 \(\mathbf{x}\)
    附近的近似。
- en: We will not derive the full theory here. In the case where each component of
    \(\mathbf{f}\) has continuous partial derivatives in a neighborhood of \(\mathbf{x}\),
    then the differential exists and is equal to the Jacobian, as defined next.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会推导出完整的理论。在 \(\mathbf{f}\) 的每个分量在 \(\mathbf{x}\) 的邻域内具有连续偏导数的情形下，微分存在且等于定义如下的雅可比矩阵。
- en: '**DEFINITION** **(Jacobian)** \(\idx{Jacobian}\xdi\) Let \(\mathbf{f} = (f_1,
    \ldots, f_m) : D \to \mathbb{R}^m\) where \(D \subseteq \mathbb{R}^d\) and let
    \(\mathbf{x}_0 \in D\) be an interior point of \(D\) where \(\frac{\partial f_j
    (\mathbf{x}_0)}{\partial x_i}\) exists for all \(i, j\). The Jacobian of \(\mathbf{f}\)
    at \(\mathbf{x}_0\) is the \(m \times d\) matrix'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(雅可比)** \(\idx{雅可比}\xdi\) 设 \(\mathbf{f} = (f_1, \ldots, f_m) : D
    \to \mathbb{R}^m\) 其中 \(D \subseteq \mathbb{R}^d\)，并且设 \(\mathbf{x}_0 \in D\)
    是 \(D\) 的一个内部点，其中对于所有 \(i, j\)，\(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\)
    存在。在 \(\mathbf{x}_0\) 处 \(\mathbf{f}\) 的雅可比是 \(m \times d\) 矩阵'
- en: \[\begin{split} J_{\mathbf{f}}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial
    f_1 (\mathbf{x}_0)}{\partial x_1} & \ldots & \frac{\partial f_1 (\mathbf{x}_0)}{\partial
    x_d}\\ \vdots & \ddots & \vdots\\ \frac{\partial f_m (\mathbf{x}_0)}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0)}{\partial x_d} \end{pmatrix}.
    \end{split}\]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{f}}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial
    f_1 (\mathbf{x}_0)}{\partial x_1} & \ldots & \frac{\partial f_1 (\mathbf{x}_0)}{\partial
    x_d}\\ \vdots & \ddots & \vdots\\ \frac{\partial f_m (\mathbf{x}_0)}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0)}{\partial x_d} \end{pmatrix}.
    \end{split}\]
- en: \(\natural\)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: '**THEOREM** **(Differential and Jacobian)** \(\idx{differential and Jacobian
    theorem}\xdi\) Let \(\mathbf{f} = (f_1, \ldots, f_m) : D \to \mathbb{R}^m\) where
    \(D \subseteq \mathbb{R}^d\) and let \(\mathbf{x}_0 \in D\) be an interior point
    of \(D\). Assume that \(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\) exists
    and is continous is an open ball around \(\mathbf{x}_0\) for all \(i, j\). Then
    the differential at \(\mathbf{x}_0\) is equal to the Jacobian of \(\mathbf{f}\)
    at \(\mathbf{x}_0\). \(\sharp\)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(微分和雅可比)** \(\idx{微分和雅可比定理}\xdi\) 设 \(\mathbf{f} = (f_1, \ldots, f_m)
    : D \to \mathbb{R}^m\) 其中 \(D \subseteq \mathbb{R}^d\)，并且设 \(\mathbf{x}_0 \in
    D\) 是 \(D\) 的一个内部点。假设对于所有 \(i, j\)，\(\frac{\partial f_j (\mathbf{x}_0)}{\partial
    x_i}\) 存在并且在一个以 \(\mathbf{x}_0\) 为中心的开球内是连续的。那么在 \(\mathbf{x}_0\) 处的微分等于 \(\mathbf{f}\)
    在 \(\mathbf{x}_0\) 处的雅可比。 \(\sharp\)'
- en: Recall that for any \(A, B\) for which \(AB\) is well-defined it holds that
    \(\|A B \|_F \leq \|A\|_F \|B\|_F\). This applies in particular when \(B\) is
    a column vector, in which case \(\|B\|_F\) is its Euclidean norm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，对于任何 \(A, B\)，当 \(AB\) 定义良好时，有 \(\|A B \|_F \leq \|A\|_F \|B\|_F\)。这特别适用于
    \(B\) 是一个列向量时，此时 \(\|B\|_F\) 是其欧几里得范数。
- en: '*Proof:* By the *Mean Value Theorem*, for each \(i\), there is \(\xi_{\mathbf{h},i}
    \in (0,1)\) such that'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：根据**平均值定理**，对于每个 \(i\)，存在 \(\xi_{\mathbf{h},i} \in (0,1)\) 使得'
- en: \[ f_i(\mathbf{x}_0+\mathbf{h}) = f_i(\mathbf{x}_0) + \nabla f_i(\mathbf{x}_0
    + \xi_{\mathbf{h},i} \mathbf{h})^T \mathbf{h}. \]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_i(\mathbf{x}_0+\mathbf{h}) = f_i(\mathbf{x}_0) + \nabla f_i(\mathbf{x}_0
    + \xi_{\mathbf{h},i} \mathbf{h})^T \mathbf{h}. \]
- en: Define
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: \[\begin{split} \tilde{J}(\mathbf{h}) = \begin{pmatrix} \frac{\partial f_1 (\mathbf{x}_0
    + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_1} & \ldots & \frac{\partial f_1
    (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_d}\\ \vdots & \ddots
    & \vdots\\ \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_d} \end{pmatrix}. \end{split}\]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \tilde{J}(\mathbf{h}) = \begin{pmatrix} \frac{\partial f_1 (\mathbf{x}_0
    + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_1} & \ldots & \frac{\partial f_1
    (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_d}\\ \vdots & \ddots
    & \vdots\\ \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_d} \end{pmatrix}. \end{split}\]
- en: Hence we have
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们有
- en: \[ \mathbf{f}(\mathbf{x}_0+\mathbf{h}) - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \tilde{J}(\mathbf{h}) \,\mathbf{h} - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}.
    \]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}_0+\mathbf{h}) - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \tilde{J}(\mathbf{h}) \,\mathbf{h} - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}.
    \]
- en: Taking a limit as \(\mathbf{h}\) goes to \(0\), we get
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\mathbf{h}\) 趋向于 \(0\) 时取极限，我们得到
- en: \[\begin{align*} \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{f}(\mathbf{x}_0+\mathbf{h})
    - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}
    &= \lim_{\mathbf{h} \to 0} \frac{\|\left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
    &\leq \lim_{\mathbf{h} \to 0} \frac{\|\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\|_F
    \|\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\ &= 0, \end{align*}\]
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{f}(\mathbf{x}_0+\mathbf{h})
    - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}
    &= \lim_{\mathbf{h} \to 0} \frac{\|\left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
    &\leq \lim_{\mathbf{h} \to 0} \frac{\|\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\|_F
    \|\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\ &= 0, \end{align*}\]
- en: by continuity of the partial derivatives. \(\square\)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过偏导数的连续性。\(\square\)
- en: '**EXAMPLE:** An example of a vector-valued function is'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 向量值函数的一个例子是'
- en: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} g_1(x_1,x_2)\\ g_2(x_1,x_2)\\
    g_3(x_1,x_2) \end{pmatrix} = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2 \end{pmatrix}.
    \end{split}\]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} g_1(x_1,x_2)\\ g_2(x_1,x_2)\\
    g_3(x_1,x_2) \end{pmatrix} = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2 \end{pmatrix}.
    \end{split}\]
- en: Its Jacobian is
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其 Jacobian 是
- en: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} \frac{\partial g_1
    (x_1, x_2)}{\partial x_1} & \frac{\partial g_1 (x_1, x_2)}{\partial x_2}\\ \frac{\partial
    g_2 (x_1, x_2)}{\partial x_1} & \frac{\partial g_2 (x_1, x_2)}{\partial x_2}\\
    \frac{\partial g_3 (x_1, x_2)}{\partial x_1} & \frac{\partial g_3 (x_1, x_2)}{\partial
    x_2} \end{pmatrix} = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}.
    \end{split}\]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} \frac{\partial g_1
    (x_1, x_2)}{\partial x_1} & \frac{\partial g_1 (x_1, x_2)}{\partial x_2}\\ \frac{\partial
    g_2 (x_1, x_2)}{\partial x_1} & \frac{\partial g_2 (x_1, x_2)}{\partial x_2}\\
    \frac{\partial g_3 (x_1, x_2)}{\partial x_1} & \frac{\partial g_3 (x_1, x_2)}{\partial
    x_2} \end{pmatrix} = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}.
    \end{split}\]
- en: \(\lhd\)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Gradient and Jacobian)** For a continuously differentiable
    real-valued function \(f : D \to \mathbb{R}\), the Jacobian reduces to the row
    vector'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（梯度与 Jacobian）** 对于一个连续可微的实值函数 \(f : D \to \mathbb{R}\)，其 Jacobian
    简化为行向量'
- en: \[ J_{f}(\mathbf{x}_0) = \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1},
    \ldots, \frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)^T = \nabla f(\mathbf{x}_0)^T
    \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{f}(\mathbf{x}_0) = \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1},
    \ldots, \frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)^T = \nabla f(\mathbf{x}_0)^T
    \]
- en: where \(\nabla f(\mathbf{x}_0)\) is the gradient of \(f\) at \(\mathbf{x}_0\).
    \(\lhd\)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\nabla f(\mathbf{x}_0)\) 是 \(f\) 在 \(\mathbf{x}_0\) 处的梯度。\(\lhd\)
- en: '**EXAMPLE:** **(Hessian and Jacobian)** For a twice continuously differentiable
    real-valued function \(f : D \to \mathbb{R}\), the Jacobian of its gradient is'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（Hessian 和 Jacobian）** 对于一个二阶连续可微的实值函数 \(f : D \to \mathbb{R}\)，其梯度的
    Jacobian 是'
- en: \[\begin{split} J_{\nabla f}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial^2
    f(\mathbf{x}_0)}{\partial x_1^2} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_d \partial x_1}\\ \vdots & \ddots & \vdots\\ \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_1 \partial x_d} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
    \end{pmatrix}, \end{split}\]
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\nabla f}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial^2
    f(\mathbf{x}_0)}{\partial x_1^2} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_d \partial x_1}\\ \vdots & \ddots & \vdots\\ \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_1 \partial x_d} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
    \end{pmatrix}, \end{split}\]
- en: that is, the Hessian (tranposed, but that makes no difference; why?) of \(f\)
    at \(\mathbf{x}_0\). \(\lhd\)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 即，\(f\) 在 \(\mathbf{x}_0\) 处的 Hessian（转置的，但这没有关系；为什么？）。\(\lhd\)
- en: '**EXAMPLE:** **(Parametric Curve and Jacobian)** Consider the parametric curve
    \(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t)) \in \mathbb{R}^d\) for \(t\) in some
    closed interval of \(\mathbb{R}\). Assume that \(\mathbf{g}(t)\) is continuously
    differentiable at \(t\), that is, each of its component is.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（参数曲线和 Jacobian）** 考虑参数曲线 \(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t))
    \in \mathbb{R}^d\)，其中 \(t\) 在 \(\mathbb{R}\) 的某个闭区间内。假设 \(\mathbf{g}(t)\) 在 \(t\)
    处连续可微，即其每个分量都是。'
- en: Then
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[\begin{split} J_{\mathbf{g}}(t) = \begin{pmatrix} g_1'(t)\\ \vdots\\ g_m'(t)
    \end{pmatrix} = \mathbf{g}'(t). \end{split}\]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(t) = \begin{pmatrix} g_1'(t)\\ \vdots\\ g_m'(t)
    \end{pmatrix} = \mathbf{g}'(t). \end{split}\]
- en: \(\lhd\)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Affine Map)** Let \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{m \times
    d}\) and \(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\). Define the vector-valued
    function \(\mathbf{f} = (f_1, \ldots, f_m) : \mathbb{R}^d \to \mathbb{R}^m\) as'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（仿射映射）** 设 \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{m \times d}\) 和
    \(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\)。定义向量值函数 \(\mathbf{f} = (f_1,
    \ldots, f_m) : \mathbb{R}^d \to \mathbb{R}^m\) 为'
- en: \[ \mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}. \]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}. \]
- en: This is an affine map. Note in particular that, in the case \(\mathbf{b} = \mathbf{0}\)
    of a linear map,
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个仿射映射。特别地，在线性映射的情况下，当 \(\mathbf{b} = \mathbf{0}\) 时，
- en: \[ \mathbf{f}(\mathbf{x} + \mathbf{y}) = A(\mathbf{x} + \mathbf{y}) = A\mathbf{x}
    + A\mathbf{y} = \mathbf{f}(\mathbf{x}) + \mathbf{f}(\mathbf{y}). \]
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x} + \mathbf{y}) = A(\mathbf{x} + \mathbf{y}) = A\mathbf{x}
    + A\mathbf{y} = \mathbf{f}(\mathbf{x}) + \mathbf{f}(\mathbf{y}). \]
- en: Denote the rows of \(A\) by \(\boldsymbol{\alpha}_1^T,\ldots,\boldsymbol{\alpha}_m^T\).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 用 \(\boldsymbol{\alpha}_1^T,\ldots,\boldsymbol{\alpha}_m^T\) 表示 \(A\) 的行。
- en: We compute the Jacobian of \(\mathbf{f}\) at \(\mathbf{x}\). Note that
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 \(\mathbf{f}\) 在 \(\mathbf{x}\) 处的 Jacobian。注意，
- en: \[\begin{align*} \frac{\partial f_i (\mathbf{x})}{\partial x_j} &= \frac{\partial}{\partial
    x_j}[\boldsymbol{\alpha}_i^T \mathbf{x} + b_i]\\ &= \frac{\partial}{\partial x_j}\left[\sum_{\ell=1}^m
    a_{i,\ell} x_{\ell} + b_i\right]\\ &= a_{i,j}. \end{align*}\]
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f_i (\mathbf{x})}{\partial x_j} &= \frac{\partial}{\partial
    x_j}[\boldsymbol{\alpha}_i^T \mathbf{x} + b_i]\\ &= \frac{\partial}{\partial x_j}\left[\sum_{\ell=1}^m
    a_{i,\ell} x_{\ell} + b_i\right]\\ &= a_{i,j}. \end{align*}\]
- en: So
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以
- en: \[ J_{\mathbf{f}}(\mathbf{x}) = A. \]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{f}}(\mathbf{x}) = A. \]
- en: \(\lhd\)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: The following important example is a less straightforward application of the
    Jacobian.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下重要的例子是雅可比的一个不太直接的应用。
- en: It will be useful to introduce the vectorization \(\mathrm{vec}(A) \in \mathbb{R}^{nm}\)
    of a matrix \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) as the vector
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 引入矩阵 \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) 的向量化 \(\mathrm{vec}(A)
    \in \mathbb{R}^{nm}\) 作为向量是有用的，作为
- en: \[ \mathrm{vec}(A) = (a_{1,1},\ldots,a_{n,1},a_{1,2},\ldots,a_{n,2},\ldots,a_{1,m},\ldots,a_{n,m}).
    \]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{vec}(A) = (a_{1,1},\ldots,a_{n,1},a_{1,2},\ldots,a_{n,2},\ldots,a_{1,m},\ldots,a_{n,m}).
    \]
- en: That is, it is obtained by stacking the columns of \(A\) on top of each other.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是通过将 \(A\) 的列堆叠在一起得到的。
- en: '**EXAMPLE:** **(Jacobian of a Linear Map with Respect to its Matrix)** We take
    a different tack on the previous example. In data science applications, it will
    be useful to compute the Jacobian of a linear map \(X \mathbf{z}\) – with respect
    to the matrix \(X \in \mathbb{R}^{n \times m}\). Specifically, for a fixed \(\mathbf{z}
    \in \mathbb{R}^{m}\), letting \((\mathbf{x}^{(i)})^T\) be the \(i\)-th row of
    \(X\) we define the function'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(关于其矩阵的线性映射的雅可比)** 我们对上一个例子采取不同的方法。在数据科学应用中，计算线性映射 \(X \mathbf{z}\)
    的雅可比（相对于矩阵 \(X \in \mathbb{R}^{n \times m}\)）将是有用的。具体来说，对于固定的 \(\mathbf{z} \in
    \mathbb{R}^{m}\)，令 \((\mathbf{x}^{(i)})^T\) 为 \(X\) 的第 \(i\) 行，我们定义函数'
- en: \[\begin{split} \mathbf{f}(\mathbf{x}) = X \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\
    \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T
    \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T \mathbf{z} \end{pmatrix} \end{split}\]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{f}(\mathbf{x}) = X \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\
    \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T
    \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T \mathbf{z} \end{pmatrix} \end{split}\]
- en: where \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\).
- en: To compute the Jacobian, let us look at its columns that correspond to the variables
    in \(\mathbf{x}^{(k)}\), that is, columns \(\alpha_k = (k-1) m + 1\) to \(\beta_k
    = k m\). Note that only the \(k\)-th component of \(\mathbf{f}\) depends on \(\mathbf{x}^{(k)}\),
    so the rows \(\neq k\) of \(J_{\mathbf{f}}(\mathbf{x})\) are \(0\) for the corresponding
    columns.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算雅可比，让我们看看与 \(\mathbf{x}^{(k)}\) 中的变量相对应的列，即列 \(\alpha_k = (k-1) m + 1\)
    到 \(\beta_k = k m\)。注意，只有 \(\mathbf{f}\) 的第 \(k\) 个分量依赖于 \(\mathbf{x}^{(k)}\)，所以
    \(J_{\mathbf{f}}(\mathbf{x})\) 的行 \(\neq k\) 对应的列是 \(0\)。
- en: Row \(k\) on the other hand is \(\mathbf{z}^T\) from our previous formula for
    the gradient of an affine map. Hence one way to write the columns \(\alpha_k\)
    to \(\beta_k\) of \(J_{\mathbf{f}}(\mathbf{x})\) is \(\mathbf{e}_k \mathbf{z}^T\),
    where here \(\mathbf{e}_k \in \mathbb{R}^{n}\) is the \(k\)-th standard basis
    vector of \(\mathbb{R}^{n}\).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，行 \(k\) 是我们之前公式中仿射映射梯度的公式中的 \(\mathbf{z}^T\)。因此，\(J_{\mathbf{f}}(\mathbf{x})\)
    的列 \(\alpha_k\) 到 \(\beta_k\) 可以写成 \(\mathbf{e}_k \mathbf{z}^T\)，其中这里的 \(\mathbf{e}_k
    \in \mathbb{R}^{n}\) 是 \(\mathbb{R}^{n}\) 的第 \(k\) 个标准基向量。
- en: So \(J_{\mathbf{f}}(\mathbf{x})\) can be written in block form as
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(J_{\mathbf{f}}(\mathbf{x})\) 可以写成块形式：
- en: '\[ J_{\mathbf{f}}(\mathbf{x}) = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}^T &
    \cdots & \mathbf{e}_{n}\mathbf{z}^T \end{pmatrix} = I_{n\times n} \otimes \mathbf{z}^T
    =: \mathbb{B}_{n}[\mathbf{z}], \]'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ J_{\mathbf{f}}(\mathbf{x}) = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}^T &
    \cdots & \mathbf{e}_{n}\mathbf{z}^T \end{pmatrix} = I_{n\times n} \otimes \mathbf{z}^T
    =: \mathbb{B}_{n}[\mathbf{z}], \]'
- en: where the last equality is a definition. \(\lhd\)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最后一个等式是一个定义。 \(\lhd\)
- en: We will need one more wrinkle.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个额外的细节。
- en: '**EXAMPLE:** **(Jacobian of a Linear Map with Respect to its Input and Matrix)**
    Consider again the linear map \(X \mathbf{z}\) – this time as a function of *both*
    the matrix \(X \in \mathbb{R}^{n \times m}\) and the vector \(\mathbf{z} \in \mathbb{R}^{m}\).
    That is, letting again \((\mathbf{x}^{(i)})^T\) be the \(i\)-th row of \(X\),
    we define the function'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(关于其输入和矩阵的线性映射的雅可比)** 再次考虑线性映射 \(X \mathbf{z}\) - 这次是作为矩阵 \(X \in
    \mathbb{R}^{n \times m}\) 和向量 \(\mathbf{z} \in \mathbb{R}^{m}\) 的函数。也就是说，再次令 \((\mathbf{x}^{(i)})^T\)
    为 \(X\) 的第 \(i\) 行，我们定义函数'
- en: \[\begin{split} \mathbf{g}(\mathbf{z}, \mathbf{x}) = X \mathbf{z} = \begin{pmatrix}
    (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z}
    = \begin{pmatrix} (\mathbf{x}^{(1)})^T \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T
    \mathbf{z} \end{pmatrix} \end{split}\]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{g}(\mathbf{z}, \mathbf{x}) = X \mathbf{z} = \begin{pmatrix}
    (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z}
    = \begin{pmatrix} (\mathbf{x}^{(1)})^T \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T
    \mathbf{z} \end{pmatrix} \end{split}\]
- en: where as before \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots,
    \mathbf{x}^{(n)})\).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述 \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\)。
- en: To compute the Jacobian, we think of it as a block matrix and use the two previous
    examples. The columns of \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) corresponding
    to the variables in \(\mathbf{z}\), that is, columns \(1\) to \(m\), are
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算雅可比矩阵，我们将其视为一个分块矩阵，并使用前两个例子。\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) 中对应于
    \(\mathbf{z}\) 中的变量的列，即列 \(1\) 到 \(m\)，是
- en: '\[\begin{split} X = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T
    \end{pmatrix} =: \mathbb{A}_{n}[\mathbf{x}]. \end{split}\]'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{split} X = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T
    \end{pmatrix} =: \mathbb{A}_{n}[\mathbf{x}]. \end{split}\]'
- en: 'The columns of \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) corresponding to
    the variables in \(\mathbf{x}\), that is, columns \(m + 1\) to \(m + nm\), are
    the matrix \(\mathbb{B}_{n}[\mathbf{z}]\). Note that, in both \(\mathbb{A}_{n}[\mathbf{x}]\)
    and \(\mathbb{B}_{n}[\mathbf{z}]\), the subscript \(n\) indicates the number of
    rows of the matrix. The number of columns is determined by \(n\) and the size
    of the input vector:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) 的列对应于 \(\mathbf{x}\) 中的变量，即列 \(m
    + 1\) 到 \(m + nm\)，是矩阵 \(\mathbb{B}_{n}[\mathbf{z}]\)。注意，在 \(\mathbb{A}_{n}[\mathbf{x}]\)
    和 \(\mathbb{B}_{n}[\mathbf{z}]\) 中，下标 \(n\) 表示矩阵的行数。列数由 \(n\) 和输入向量的尺寸决定：
- en: the length of \(\mathbf{x}\) divided by \(n\) for \(\mathbb{A}_{n}[\mathbf{x}]\);
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{A}_{n}[\mathbf{x}]\) 的长度除以 \(n\)；
- en: the length of \(\mathbf{z}\) multiplied by \(n\) for \(\mathbb{B}_{n}[\mathbf{z}]\).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{B}_{n}[\mathbf{z}]\) 的长度乘以 \(n\)。
- en: So \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) can be written in block form as
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) 可以写成分块形式
- en: \[ J_{\mathbf{f}}(\mathbf{z}, \mathbf{x}) = \begin{pmatrix} \mathbb{A}_{n}[\mathbf{x}]
    & \mathbb{B}_{n}[\mathbf{z}] \end{pmatrix}. \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{f}}(\mathbf{z}, \mathbf{x}) = \begin{pmatrix} \mathbb{A}_{n}[\mathbf{x}]
    & \mathbb{B}_{n}[\mathbf{z}] \end{pmatrix}. \]
- en: \(\lhd\)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Elementwise Function)** Let \(f : D \to \mathbb{R}\), with
    \(D \subseteq \mathbb{R}\), be a continuously differentiable real-valued function
    of a single variable. For \(n \geq 2\), consider applying \(f\) to each entry
    of a vector \(\mathbf{x} \in \mathbb{R}^n\), that is, let \(\mathbf{f} : D^n \to
    \mathbb{R}^n\) with'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(逐元素函数)** 设 \(f : D \to \mathbb{R}\)，其中 \(D \subseteq \mathbb{R}\)，是一个关于单变量的连续可微实值函数。对于
    \(n \geq 2\)，考虑将 \(f\) 应用到向量 \(\mathbf{x} \in \mathbb{R}^n\) 的每个元素上，即令 \(\mathbf{f}
    : D^n \to \mathbb{R}^n\)，具有'
- en: \[ \mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}), \ldots, f_n(\mathbf{x})) = (f(x_1),
    \ldots, f(x_n)). \]
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}), \ldots, f_n(\mathbf{x})) = (f(x_1),
    \ldots, f(x_n)). \]
- en: The Jacobian of \(\mathbf{f}\) can be computed from \(f'\), the derivative of
    the single-variable case. Indeed, letting \(\mathbf{x} = (x_1,\ldots,x_n)\) be
    such that \(x_i\) is an interior point of \(D\) for all \(i\),
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \(\mathbf{f}\) 的雅可比矩阵可以从单变量情况的导数 \(f'\) 计算得出。确实，令 \(\mathbf{x} = (x_1,\ldots,x_n)\)
    使得对于所有 \(i\)，\(x_i\) 是 \(D\) 的内点，
- en: \[ \frac{\partial f_j(\mathbf{x})}{\partial x_j} = f'(x_j), \]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f_j(\mathbf{x})}{\partial x_j} = f'(x_j), \]
- en: while for \(\ell \neq j\)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\ell \neq j\) 时
- en: \[ \frac{\partial f_\ell(\mathbf{x})}{\partial x_j} =0, \]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f_\ell(\mathbf{x})}{\partial x_j} =0, \]
- en: as \(f_\ell(\mathbf{x})\) does not in fact depend on \(x_j\). In other words,
    the \(j\)-th column of the Jacobian is \(f'(x_j) \,\mathbf{e}_j\), where again
    \(\mathbf{e}_{j}\) is the \(j\)-th standard basis vector in \(\mathbb{R}^{n}\).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(f_\ell(\mathbf{x})\) 实际上并不依赖于 \(x_j\)。换句话说，雅可比矩阵的第 \(j\) 列是 \(f'(x_j) \,\mathbf{e}_j\)，其中再次
    \(\mathbf{e}_{j}\) 是 \(\mathbb{R}^{n}\) 中的第 \(j\) 个标准基向量。
- en: So \(J_{\mathbf{f}}(\mathbf{x})\) is the diagonal matrix with diagonal entries
    \(f'(x_j)\), \(j=1, \ldots, n\), which we denote by
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 \(J_{\mathbf{f}}(\mathbf{x})\) 是对角矩阵，其对角元素为 \(f'(x_j)\)，\(j=1, \ldots, n\)，我们用
- en: \[ J_{\mathbf{f}}(\mathbf{x}) = \mathrm{diag}(f'(x_1),\ldots,f'(x_n)). \]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{f}}(\mathbf{x}) = \mathrm{diag}(f'(x_1),\ldots,f'(x_n)). \]
- en: \(\lhd\)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: 8.2.3\. Generalization of the Chain Rule[#](#generalization-of-the-chain-rule
    "Link to this heading")
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.3\. 链式法则的推广[#](#generalization-of-the-chain-rule "链接到本标题")
- en: As we have seen, functions are often obtained from the composition of simpler
    ones. We will use the vector notation \(\mathbf{h} = \mathbf{g} \circ \mathbf{f}\)
    for the function \(\mathbf{h}(\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))\).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，函数通常是通过简单函数的复合得到的。我们将使用向量符号 \(\mathbf{h} = \mathbf{g} \circ \mathbf{f}\)
    表示函数 \(\mathbf{h}(\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))\)。
- en: '**LEMMA** **(Composition of Continuous Functions)** \(\idx{composition of continuous
    functions lemma}\xdi\) Let \(\mathbf{f} : D_1 \to \mathbb{R}^m\), where \(D_1
    \subseteq \mathbb{R}^d\), and let \(\mathbf{g} : D_2 \to \mathbb{R}^p\), where
    \(D_2 \subseteq \mathbb{R}^m\). Assume that \(\mathbf{f}\) is continuous at \(\mathbf{x}_0\)
    and that \(\mathbf{g}\) is continuous \(\mathbf{f}(\mathbf{x}_0)\). Then \(\mathbf{g}
    \circ \mathbf{f}\) is continuous at \(\mathbf{x}_0\). \(\flat\)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(连续函数的复合)** \(\idx{连续函数复合引理}\xdi\) 设 \(\mathbf{f} : D_1 \to \mathbb{R}^m\)，其中
    \(D_1 \subseteq \mathbb{R}^d\)，以及设 \(\mathbf{g} : D_2 \to \mathbb{R}^p\)，其中 \(D_2
    \subseteq \mathbb{R}^m\)。假设 \(\mathbf{f}\) 在 \(\mathbf{x}_0\) 处连续，且 \(\mathbf{g}\)
    在 \(\mathbf{f}(\mathbf{x}_0)\) 处连续。那么 \(\mathbf{g} \circ \mathbf{f}\) 在 \(\mathbf{x}_0\)
    处连续。 \(\flat\)'
- en: The *Chain Rule* gives a formula for the Jacobian of a composition.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**链式法则** 给出了复合函数雅可比的公式。'
- en: '**THEOREM** **(Chain Rule)** \(\idx{chain rule}\xdi\) Let \(\mathbf{f} : D_1
    \to \mathbb{R}^m\), where \(D_1 \subseteq \mathbb{R}^d\), and let \(\mathbf{g}
    : D_2 \to \mathbb{R}^p\), where \(D_2 \subseteq \mathbb{R}^m\). Assume that \(\mathbf{f}\)
    is continuously differentiable at \(\mathbf{x}_0\), an interior point of \(D_1\),
    and that \(\mathbf{g}\) is continuously differentiable at \(\mathbf{f}(\mathbf{x}_0)\),
    an interior point of \(D_2\). Then'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(链式法则)** \(\idx{链式法则}\xdi\) 设 \(\mathbf{f} : D_1 \to \mathbb{R}^m\)，其中
    \(D_1 \subseteq \mathbb{R}^d\)，以及设 \(\mathbf{g} : D_2 \to \mathbb{R}^p\)，其中 \(D_2
    \subseteq \mathbb{R}^m\)。假设 \(\mathbf{f}\) 在 \(D_1\) 的内点 \(\mathbf{x}_0\) 处连续可微，且
    \(\mathbf{g}\) 在 \(D_2\) 的内点 \(\mathbf{f}(\mathbf{x}_0)\) 处连续可微。那么'
- en: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \,J_{\mathbf{f}}(\mathbf{x}_0) \]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \,J_{\mathbf{f}}(\mathbf{x}_0) \]
- en: as a product of matrices. \(\sharp\)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作为矩阵的乘积。 \(\sharp\)
- en: Intuitively, the Jacobian provides a linear approximation of the function in
    the neighborhood of a point. The composition of linear maps corresponds to the
    product of the associated matrices. Similarly, the Jacobian of a composition is
    the product of the Jacobians.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，雅可比提供了函数在点附近的线性近似。线性映射的复合对应于相关矩阵的乘积。同样，复合函数的雅可比是雅可比矩阵的乘积。
- en: '*Proof:* To avoid confusion, we think of \(\mathbf{f}\) and \(\mathbf{g}\)
    as being functions of variables with different names, specifically,'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 为了避免混淆，我们将 \(\mathbf{f}\) 和 \(\mathbf{g}\) 视为具有不同名称的变量函数，具体来说，'
- en: \[ \mathbf{f}(\mathbf{x}) = (f_1(x_1,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_d))
    \]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}) = (f_1(x_1,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_d))
    \]
- en: and
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[ \mathbf{g}(\mathbf{y}) = (g_1(y_1,\ldots,y_m),\ldots,g_p(y_1,\ldots,y_m)).
    \]
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{g}(\mathbf{y}) = (g_1(y_1,\ldots,y_m),\ldots,g_p(y_1,\ldots,y_m)).
    \]
- en: We apply the *Chain Rule* for a real-valued function over a parametric vector
    curve. That is, we think of
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将链式法则应用于参数向量曲线上的实值函数。也就是说，我们考虑
- en: \[ h_i(\mathbf{x}) = g_i(\mathbf{f}(\mathbf{x})) = g_i(f_1(x_1,\ldots,x_j,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_j,\ldots,x_d))
    \]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h_i(\mathbf{x}) = g_i(\mathbf{f}(\mathbf{x})) = g_i(f_1(x_1,\ldots,x_j,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_j,\ldots,x_d))
    \]
- en: as *a function of \(x_j\) only* with all other \(x_i\)s fixed.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 仅作为 \(x_j\) 的函数，其他所有 \(x_i\) 均固定。
- en: We get that
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到
- en: \[ \frac{\partial h_i(\mathbf{x}_0)}{\partial x_j} = \sum_{k=1}^m \frac{\partial
    g_i(\mathbf{f}(\mathbf{x}_0))} {\partial y_k} \frac{\partial f_k(\mathbf{x}_0)}{\partial
    x_j} \]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial h_i(\mathbf{x}_0)}{\partial x_j} = \sum_{k=1}^m \frac{\partial
    g_i(\mathbf{f}(\mathbf{x}_0))} {\partial y_k} \frac{\partial f_k(\mathbf{x}_0)}{\partial
    x_j} \]
- en: where, as before, the notation \(\frac{\partial g_i} {\partial y_k}\) indicates
    the partial derivative of \(g_i\) with respect to its \(k\)-th component. In matrix
    form, the claim follows. \(\square\)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，与之前一样，符号 \(\frac{\partial g_i} {\partial y_k}\) 表示 \(g_i\) 对其第 \(k\) 个分量的偏导数。以矩阵形式，该命题成立。
    \(\square\)
- en: '**EXAMPLE:** **(Affine Map continued)** Let \(A \in \mathbb{R}^{m \times d}\)
    and \(\mathbf{b} \in \mathbb{R}^{m}\). Define again the vector-valued function
    \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\) as \(\mathbf{f}(\mathbf{x}) = A
    \mathbf{x} + \mathbf{b}\). In addition, for \(C \in \mathbb{R}^{p \times m}\)
    and \(\mathbf{d} \in \mathbb{R}^{p}\), define \(\mathbf{g} : \mathbb{R}^m \to
    \mathbb{R}^p\) as \(\mathbf{g}(\mathbf{y}) = C \mathbf{y} + \mathbf{d}\).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(仿射映射继续)** 设 \(A \in \mathbb{R}^{m \times d}\) 和 \(\mathbf{b} \in
    \mathbb{R}^{m}\)。再次定义向量值函数 \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\) 为 \(\mathbf{f}(\mathbf{x})
    = A \mathbf{x} + \mathbf{b}\)。此外，对于 \(C \in \mathbb{R}^{p \times m}\) 和 \(\mathbf{d}
    \in \mathbb{R}^{p}\)，定义 \(\mathbf{g} : \mathbb{R}^m \to \mathbb{R}^p\) 为 \(\mathbf{g}(\mathbf{y})
    = C \mathbf{y} + \mathbf{d}\)。'
- en: Then
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}))
    \,J_{\mathbf{f}}(\mathbf{x}) = C A, \]
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}))
    \,J_{\mathbf{f}}(\mathbf{x}) = C A, \]
- en: for all \(\mathbf{x} \in \mathbb{R}^d\).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(\mathbf{x} \in \mathbb{R}^d\)。
- en: This is consistent with the observation that
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这与观察结果一致
- en: \[ \mathbf{g} \circ \mathbf{f} (\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))
    = C( A\mathbf{x} + \mathbf{b} ) + \mathbf{d} = CA \mathbf{x} + (C\mathbf{b} +
    \mathbf{d}). \]
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{g} \circ \mathbf{f} (\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))
    = C( A\mathbf{x} + \mathbf{b} ) + \mathbf{d} = CA \mathbf{x} + (C\mathbf{b} +
    \mathbf{d}). \]
- en: \(\lhd\)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** Suppose we want to compute the gradient of the function'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 假设我们想要计算函数的梯度'
- en: \[ f(x_1, x_2) = 3 x_1^2 + x_2 + \exp(x_1 x_2). \]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x_1, x_2) = 3 x_1^2 + x_2 + \exp(x_1 x_2). \]
- en: We could apply the *Chain Rule* directly, but to illustrate the perspective
    that is coming up, we think of \(f\) as a composition of “simpler” vector-valued
    functions. Specifically, let
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接应用**链式法则**，但为了说明即将出现的观点，我们将 \(f\) 视为“更简单”的向量值函数的复合。具体来说，让
- en: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2
    \end{pmatrix} \qquad h(y_1,y_2,y_3) = y_1 + y_2 + \exp(y_3). \end{split}\]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2
    \end{pmatrix} \qquad h(y_1,y_2,y_3) = y_1 + y_2 + \exp(y_3). \end{split}\]
- en: Then \(f(x_1, x_2) = h(\mathbf{g}(x_1, x_2)) = h \circ \mathbf{g}(x_1, x_2)\).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 \(f(x_1, x_2) = h(\mathbf{g}(x_1, x_2)) = h \circ \mathbf{g}(x_1, x_2)\).
- en: By the *Chain Rule*, we can compute the gradient of \(f\) by first computing
    the Jacobians of \(\mathbf{g}\) and \(h\). We have already computed the Jacobian
    of \(\mathbf{g}\)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**链式法则**，我们可以通过首先计算 \(\mathbf{g}\) 和 \(h\) 的雅可比矩阵来计算 \(f\) 的梯度。我们已经计算了 \(\mathbf{g}\)
    的雅可比矩阵
- en: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\
    x_2 & x_1 \end{pmatrix}. \end{split}\]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\
    x_2 & x_1 \end{pmatrix}. \end{split}\]
- en: The Jacobian of \(h\) is
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: \(h\) 的雅可比矩阵为
- en: \[ J_h(y_1, y_2, y_3) = \begin{pmatrix} \frac{\partial h(y_1, y_2, y_3)}{\partial
    y_1} & \frac{\partial h(y_1, y_2, y_3)}{\partial y_2} & \frac{\partial h(y_1,
    y_2, y_3)}{\partial y_3} \end{pmatrix} = \begin{pmatrix} 1 & 1 & \exp(y_3) \end{pmatrix}.
    \]
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_h(y_1, y_2, y_3) = \begin{pmatrix} \frac{\partial h(y_1, y_2, y_3)}{\partial
    y_1} & \frac{\partial h(y_1, y_2, y_3)}{\partial y_2} & \frac{\partial h(y_1,
    y_2, y_3)}{\partial y_3} \end{pmatrix} = \begin{pmatrix} 1 & 1 & \exp(y_3) \end{pmatrix}.
    \]
- en: The *Chain Rule* stipulates that
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**链式法则**规定'
- en: \[\begin{align*} \nabla f(x_1, x_2)^T &= J_f(x_1, x_2)\\ &= J_h(\mathbf{g}(x_1,x_2))
    \, J_{\mathbf{g}}(x_1, x_2)\\ &= \begin{pmatrix} 1 & 1 & \exp(g_3(x_1, x_2)) \end{pmatrix}
    \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}\\ &= \begin{pmatrix}
    1 & 1 & \exp(x_1 x_2) \end{pmatrix} \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 &
    x_1 \end{pmatrix}\\ &= \begin{pmatrix} 6 x_1 + x_2 \exp(x_1 x_2) & 1 + x_1 \exp(x_1
    x_2) \end{pmatrix}. \end{align*}\]
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f(x_1, x_2)^T &= J_f(x_1, x_2)\\ &= J_h(\mathbf{g}(x_1,x_2))
    \, J_{\mathbf{g}}(x_1, x_2)\\ &= \begin{pmatrix} 1 & 1 & \exp(g_3(x_1, x_2)) \end{pmatrix}
    \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}\\ &= \begin{pmatrix}
    1 & 1 & \exp(x_1 x_2) \end{pmatrix} \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 &
    x_1 \end{pmatrix}\\ &= \begin{pmatrix} 6 x_1 + x_2 \exp(x_1 x_2) & 1 + x_1 \exp(x_1
    x_2) \end{pmatrix}. \end{align*}\]
- en: You can check directly (i.e., without the composition) that this is indeed the
    correct gradient (transposed).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接检查（即，不进行复合）这确实是正确的梯度（转置）。
- en: Alternatively, it is instructive to “expand” the *Chain Rule* as we did in its
    proof. Specifically,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，像我们在其证明中所做的那样“展开”**链式法则**是有益的。具体来说，
- en: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_1} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_1}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_1}\\ &= 1 \cdot 6x_1 + 1 \cdot
    0 + \exp(g_3(x_1, x_2)) \cdot x_2\\ &= 6 x_1 + x_2 \exp(x_1 x_2). \end{align*}\]
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_1} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_1}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_1}\\ &= 1 \cdot 6x_1 + 1 \cdot
    0 + \exp(g_3(x_1, x_2)) \cdot x_2\\ &= 6 x_1 + x_2 \exp(x_1 x_2). \end{align*}\]
- en: Note that this corresponds to multiplying \(J_h(\mathbf{g}(x_1,x_2))\) by the
    first column of \(J_{\mathbf{g}}(x_1, x_2)\).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这相当于将 \(J_h(\mathbf{g}(x_1,x_2))\) 乘以 \(J_{\mathbf{g}}(x_1, x_2)\) 的第一列。
- en: Similarly
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地
- en: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_2} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_2}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_2}\\ &= 1 \cdot 0 + 1 \cdot 1 +
    \exp(g_3(x_1, x_2)) \cdot x_1\\ &= 1 + x_1 \exp(x_1 x_2). \end{align*}\]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_2} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_2}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_2}\\ &= 1 \cdot 0 + 1 \cdot 1 +
    \exp(g_3(x_1, x_2)) \cdot x_1\\ &= 1 + x_1 \exp(x_1 x_2). \end{align*}\]
- en: This corresponds to multiplying \(J_h(\mathbf{g}(x_1,x_2))\) by the second column
    of \(J_{\mathbf{g}}(x_1, x_2)\). \(\lhd\)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于将 \(J_h(\mathbf{g}(x_1,x_2))\) 乘以 \(J_{\mathbf{g}}(x_1, x_2)\) 的第二列。 \(\lhd\)
- en: '**CHAT & LEARN** The Jacobian determinant has important applications in change
    of variables for multivariable integrals. Ask your favorite AI chatbot to explain
    this application and provide an example of using the Jacobian determinant in a
    change of variables for a double integral. \(\ddagger\)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 雅可比行列式在多元积分变量变换中有着重要的应用。请你的心仪 AI 聊天机器人解释这一应用，并提供一个使用雅可比行列式在双重积分变量变换中的例子。
    \(\ddagger\)'
- en: 8.2.4\. Brief introduction to automatic differentiation in PyTorch[#](#brief-introduction-to-automatic-differentiation-in-pytorch
    "Link to this heading")
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.4\. PyTorch中自动微分简介[#](#brief-introduction-to-automatic-differentiation-in-pytorch
    "链接到本标题")
- en: We illustrate the use of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\(\idx{automatic
    differentiation}\xdi\) to compute gradients in PyTorch.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何在 PyTorch 中使用 [自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)\(\idx{自动微分}\xdi\)
    来计算梯度。
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '引用 [维基百科](https://en.wikipedia.org/wiki/Automatic_differentiation):'
- en: In mathematics and computer algebra, automatic differentiation (AD), also called
    algorithmic differentiation or computational differentiation, is a set of techniques
    to numerically evaluate the derivative of a function specified by a computer program.
    AD exploits the fact that every computer program, no matter how complicated, executes
    a sequence of elementary arithmetic operations (addition, subtraction, multiplication,
    division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying
    the chain rule repeatedly to these operations, derivatives of arbitrary order
    can be computed automatically, accurately to working precision, and using at most
    a small constant factor more arithmetic operations than the original program.
    Automatic differentiation is distinct from symbolic differentiation and numerical
    differentiation (the method of finite differences). Symbolic differentiation can
    lead to inefficient code and faces the difficulty of converting a computer program
    into a single expression, while numerical differentiation can introduce round-off
    errors in the discretization process and cancellation.
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在数学和计算机代数中，自动微分（AD），也称为算法微分或计算微分，是一组用于数值评估由计算机程序指定的函数导数的技巧。AD利用了这样一个事实：无论计算机程序多么复杂，它都执行一系列基本的算术运算（加法、减法、乘法、除法等）和基本函数（指数、对数、正弦、余弦等）。通过将这些运算反复应用链式法则，可以自动计算任意阶的导数，精确到工作精度，并且使用的算术运算次数最多只比原始程序多一个很小的常数因子。自动微分与符号微分和数值微分（有限差分法）不同。符号微分可能导致代码效率低下，并且面临将计算机程序转换为单个表达式的困难，而数值微分可能在离散化过程中引入舍入误差和消去。
- en: '**Automatic differentiation in PyTorch** We will use [PyTorch](https://pytorch.org/tutorials/).
    It uses [tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\(\idx{tensor}\xdi\),
    which in many ways behave similarly to NumPy arrays. See [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)
    for a quick introduction. We first initialize the tensors. Here each tensor corresponds
    to a single real variable. With the option [`requires_grad=True`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad),
    we indicate that these are variables with respect to which a gradient will be
    taken later. We initialize the tensors at the values where the derivatives will
    be computed. If derivatives need to be computed at different values, we need to
    repeat this process. The function [`.backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html)
    computes the gradient using backpropagation, to which we will return later. The
    partial derivatives are accessed with [`.grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch中的自动微分** 我们将使用[PyTorch](https://pytorch.org/tutorials/)。它使用[tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\(\idx{tensor}\xdi\)，在许多方面与NumPy数组的行为相似。参见[这里](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)以获得快速介绍。我们首先初始化张量。在这里，每个张量对应一个单独的实变量。使用选项`requires_grad=True`，我们指示这些是以后将计算梯度的变量。我们初始化张量到将计算导数的值。如果需要在不同的值处计算导数，我们需要重复此过程。函数`[.backward()]`使用反向传播计算梯度，我们将在后面返回。偏导数可以通过`[.grad]`访问。'
- en: '**NUMERICAL CORNER:** This is better understood through an example.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 这可以通过一个例子更好地理解。'
- en: '[PRE0]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We define the function. Note that we use [`torch.exp`](https://pytorch.org/docs/stable/generated/torch.exp.html),
    the PyTorch implementation of the (element-wise) exponential function. Moreover,
    as in NumPy, PyTorch allows the use of `**` for [taking a power](https://pytorch.org/docs/stable/generated/torch.pow.html).
    [Here](https://pytorch.org/docs/stable/name_inference.html) is a list of operations
    on tensors in PyTorch.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义函数。请注意，我们使用`torch.exp`，这是PyTorch实现的（逐元素）指数函数。此外，与NumPy一样，PyTorch允许使用`**`来[取幂](https://pytorch.org/docs/stable/generated/torch.pow.html)。[这里](https://pytorch.org/docs/stable/name_inference.html)是PyTorch中张量操作列表。
- en: '[PRE1]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The input parameters can also be vectors, which allows to consider functions
    of large numbers of variables. Here we use [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum)
    for taking a sum of the arguments.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输入参数也可以是向量，这允许考虑大量变量的函数。在这里，我们使用 `torch.sum` 来取参数的和。
- en: '[PRE3]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here is another typical example in a data science context.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据科学环境中另一个典型的例子。
- en: '[PRE5]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain how to compute a second
    derivative using PyTorch (it’s bit tricky). Ask for code that you can apply to
    the previous examples. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 请你的AI聊天机器人解释如何使用PyTorch计算二阶导数（有点棘手）。请提供可以应用于先前示例的代码。 ([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Implementing gradient descent in PyTorch** Rather than explicitly specifying
    the gradient function, we could use PyTorch to compute it automatically. This
    is done next. Note that the descent update is done within [`with torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html),
    which ensures that the update operation itself is not tracked for gradient computation.
    Here the input `x0` as well as the output `xk.numpy(force=True)` are NumPy arrays.
    The function [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    converts a PyTorch tensor to a NumPy array (see the documentation for an explanation
    of the `force=True` option). Also, quoting ChatGPT:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**在PyTorch中实现梯度下降** 而不是明确指定梯度函数，我们可以使用PyTorch自动计算它。接下来将这样做。注意，下降更新是在 `with
    torch.no_grad()` 中完成的（[torch.no_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html)），这确保更新操作本身不被跟踪用于梯度计算。在这里，输入
    `x0` 以及输出 `xk.numpy(force=True)` 都是NumPy数组。函数 `torch.Tensor.numpy()` 将PyTorch张量转换为NumPy数组（有关
    `force=True` 选项的解释，请参阅文档）。此外，引用ChatGPT：'
- en: In the given code, `.item()` is used to extract the scalar value from a tensor.
    In PyTorch, when you perform operations on tensors, you get back tensors as results,
    even if the result is a single scalar value. `.item()` is used to extract this
    scalar value from the tensor.
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在给定的代码中，`.item()` 用于从一个张量中提取标量值。在PyTorch中，当你对张量执行操作时，你会得到张量作为结果，即使结果是一个单一的标量值。`.item()`
    用于从张量中提取这个标量值。
- en: '[PRE7]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**NUMERICAL CORNER:** We revisit a previous example.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**：我们回顾一个先前的例子。'
- en: '[PRE8]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: \(\unlhd\)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** The section briefly mentions that automatic differentiation
    is distinct from symbolic differentiation and numerical differentiation. Ask your
    favorite AI chatbot to explain in more detail the differences between these three
    methods of computing derivatives. \(\ddagger\)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 简要介绍了自动微分与符号微分和数值微分不同。请你的AI聊天机器人详细解释这三种计算导数方法的区别。 \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** Let \(A \in \mathbb{R}^{n \times m}\) and \(B \in \mathbb{R}^{p \times
    q}\). What are the dimensions of the Kronecker product \(A \otimes B\)?'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 设 \(A \in \mathbb{R}^{n \times m}\) 和 \(B \in \mathbb{R}^{p \times q}\)。Kronecker积
    \(A \otimes B\) 的维度是什么？'
- en: a) \(n \times m\)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(n \times m\)
- en: b) \(p \times q\)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(p \times q\)
- en: c) \(np \times mq\)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(np \times mq\)
- en: d) \(nq \times mp\)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(nq \times mp\)
- en: '**2** If \(f: \mathbb{R}^d \rightarrow \mathbb{R}^m\) is a continuously differentiable
    function, what is its Jacobian \(J_f(x_0)\) at an interior point \(x_0\) of its
    domain?'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 如果 \(f: \mathbb{R}^d \rightarrow \mathbb{R}^m\) 是一个连续可微的函数，那么在其定义域内部点
    \(x_0\) 处的雅可比矩阵 \(J_f(x_0)\) 是什么？'
- en: a) A scalar representing the rate of change of \(f\) at \(x_0\).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: a) 一个表示 \(f\) 在 \(x_0\) 处的变化率的标量。
- en: b) A vector in \(\mathbb{R}^m\) representing the direction of steepest ascent
    of \(f\) at \(x_0\).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: b) 一个在 \(\mathbb{R}^m\) 中的向量，表示 \(f\) 在 \(x_0\) 处的最速上升方向。
- en: c) An \(m \times d\) matrix of partial derivatives of the component functions
    of \(f\) at \(x_0\).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: c) 一个 \(m \times d\) 的矩阵，表示 \(f\) 在 \(x_0\) 处的分量函数的偏导数。
- en: d) The Hessian matrix of \(f\) at \(x_0\).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(f\) 在 \(x_0\) 处的Hessian矩阵。
- en: '**3** In the context of the Chain Rule, if \(f: \mathbb{R}^2 \to \mathbb{R}^3\)
    and \(g: \mathbb{R}^3 \to \mathbb{R}\), what is the dimension of the Jacobian
    matrix \(J_{g \circ f}(x)\)?'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 在链式法则的背景下，如果 \(f: \mathbb{R}^2 \to \mathbb{R}^3\) 和 \(g: \mathbb{R}^3
    \to \mathbb{R}\)，那么雅可比矩阵 \(J_{g \circ f}(x)\) 的维度是多少？'
- en: a) \(3 \times 2\)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(3 \times 2\)
- en: b) \(1 \times 3\)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(1 \times 3\)
- en: c) \(2 \times 3\)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(2 \times 3\)
- en: d) \(1 \times 2\)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(1 \times 2\)
- en: '**4** Let \(\mathbf{f} : D_1 \to \mathbb{R}^m\), where \(D_1 \subseteq \mathbb{R}^d\),
    and let \(\mathbf{g} : D_2 \to \mathbb{R}^p\), where \(D_2 \subseteq \mathbb{R}^m\).
    Assume that \(\mathbf{f}\) is continuously differentiable at \(\mathbf{x}_0\),
    an interior point of \(D_1\), and that \(\mathbf{g}\) is continuously differentiable
    at \(\mathbf{f}(\mathbf{x}_0)\), an interior point of \(D_2\). Which of the following
    is correct according to the Chain Rule?'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 让 \(\mathbf{f} : D_1 \to \mathbb{R}^m\)，其中 \(D_1 \subseteq \mathbb{R}^d\)，并且让
    \(\mathbf{g} : D_2 \to \mathbb{R}^p\)，其中 \(D_2 \subseteq \mathbb{R}^m\)。假设 \(\mathbf{f}\)
    在 \(D_1\) 的内点 \(\mathbf{x}_0\) 处连续可微，且 \(\mathbf{g}\) 在 \(D_2\) 的内点 \(\mathbf{f}(\mathbf{x}_0)\)
    处连续可微。根据链式法则，以下哪个是正确的？'
- en: a) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{x}_0)
    \, J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))\)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{x}_0)
    \, J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))\)
- en: b) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \, J_{\mathbf{f}}(\mathbf{x}_0)\)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \, J_{\mathbf{f}}(\mathbf{x}_0)\)
- en: c) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))
    \, J_{\mathbf{g}}(\mathbf{x}_0)\)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))
    \, J_{\mathbf{g}}(\mathbf{x}_0)\)
- en: d) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{x}_0)
    \, J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))\)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{x}_0)
    \, J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))\)
- en: '**5** Let \(A \in \mathbb{R}^{m \times d}\) and \(\mathbf{b} \in \mathbb{R}^{m}\).
    Define the vector-valued function \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\)
    as \(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\). What is the Jacobian
    of \(\mathbf{f}\) at \(\mathbf{x}_0\)?'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 设 \(A \in \mathbb{R}^{m \times d}\) 和 \(\mathbf{b} \in \mathbb{R}^{m}\)。定义向量值函数
    \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\) 为 \(\mathbf{f}(\mathbf{x}) = A
    \mathbf{x} + \mathbf{b}\)。\(\mathbf{f}\) 在 \(\mathbf{x}_0\) 处的雅可比矩阵是什么？'
- en: a) \(J_{\mathbf{f}}(\mathbf{x}_0) = A^T\)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(J_{\mathbf{f}}(\mathbf{x}_0) = A^T\)
- en: b) \(J_{\mathbf{f}}(\mathbf{x}_0) = A \mathbf{x}_0 + \mathbf{b}\)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(J_{\mathbf{f}}(\mathbf{x}_0) = A \mathbf{x}_0 + \mathbf{b}\)
- en: c) \(J_{\mathbf{f}}(\mathbf{x}_0) = A\)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(J_{\mathbf{f}}(\mathbf{x}_0) = A\)
- en: d) \(J_{\mathbf{f}}(\mathbf{x}_0) = \mathbf{b}\)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(J_{\mathbf{f}}(\mathbf{x}_0) = \mathbf{b}\)
- en: 'Answer for 1: c. Justification: The text defines the Kronecker product as a
    matrix in block form with dimensions \(np \times mq\).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 1题的答案：c. 理由：文本定义了Kronecker积为一个具有 \(np \times mq\) 维度的分块矩阵。
- en: 'Answer for 2: c. Justification: The text defines the Jacobian of a vector-valued
    function as a matrix of partial derivatives.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 2题的答案：c. 理由：文本定义了向量值函数的雅可比矩阵为偏导数矩阵。
- en: 'Answer for 3: d. Justification: The composition \(g \circ f\) maps \(\mathbb{R}^2
    \to \mathbb{R}\), hence the Jacobian matrix \(J_{g \circ f}(x)\) is \(1 \times
    2\).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 3题的答案：d. 理由：复合函数 \(g \circ f\) 将 \(\mathbb{R}^2 \to \mathbb{R}\) 映射，因此雅可比矩阵
    \(J_{g \circ f}(x)\) 是 \(1 \times 2\)。
- en: 'Answer for 4: b. Justification: The text states: “The Chain Rule gives a formula
    for the Jacobian of a composition. […] Assume that \(\mathbf{f}\) is continuously
    differentiable at \(\mathbf{x}_0\), an interior point of \(D_1\), and that \(\mathbf{g}\)
    is continuously differentiable at \(\mathbf{f}(\mathbf{x}_0)\), an interior point
    of \(D_2\). Then'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 4题的答案：b. 理由：文本中提到：“链式法则给出了复合函数雅可比矩阵的公式。[…] 假设 \(\mathbf{f}\) 在 \(D_1\) 的内点 \(\mathbf{x}_0\)
    处连续可微，且 \(\mathbf{g}\) 在 \(D_2\) 的内点 \(\mathbf{f}(\mathbf{x}_0)\) 处连续可微。那么
- en: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \,J_{\mathbf{f}}(\mathbf{x}_0) \]
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \,J_{\mathbf{f}}(\mathbf{x}_0) \]
- en: as a product of matrices.”
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 作为矩阵的乘积。”
- en: 'Answer for 5: c. Justification: The text states: “Let \(A \in \mathbb{R}^{m
    \times d}\) and \(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\). Define the
    vector-valued function \(\mathbf{f} = (f_1, \ldots, f_m) : \mathbb{R}^d \to \mathbb{R}^m\)
    as \(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\). […] So \(J_{\mathbf{f}}(\mathbf{x})
    = A.\)”'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '5题的答案：c. 理由：文本中提到：“设 \(A \in \mathbb{R}^{m \times d}\) 和 \(\mathbf{b} = (b_1,\ldots,b_m)
    \in \mathbb{R}^{m}\)。定义向量值函数 \(\mathbf{f} = (f_1, \ldots, f_m) : \mathbb{R}^d
    \to \mathbb{R}^m\) 为 \(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)。[…]
    因此 \(J_{\mathbf{f}}(\mathbf{x}) = A.\)” '
- en: '8.2.1\. More matrix algebra: Hadamard and Kronecker products[#](#more-matrix-algebra-hadamard-and-kronecker-products
    "Link to this heading")'
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.1\. 更多的矩阵代数：Hadamard 和 Kronecker 积[#](#more-matrix-algebra-hadamard-and-kronecker-products
    "链接到这个标题")
- en: First, we introduce the Hadamard product\(\idx{Hadamard product}\xdi\) and division\(\idx{Hadamard
    division}\xdi\). The Hadamard product of two matrices (or vectors) of the same
    dimension, \(A = (a_{i,j})_{i \in [n], j \in [m]}, B = (b_{i,j})_{i\in [n], j
    \in [m]} \in \mathbb{R}^{n \times m}\), is defined as the element-wise product
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们引入Hadamard积\(\idx{Hadamard product}\xdi\) 和除法\(\idx{Hadamard division}\xdi\).
    两个相同维度的矩阵（或向量）的Hadamard积定义为元素乘积
- en: \[ A \odot B = (a_{i,j} b_{i,j})_{i\in [n], j \in [m]}. \]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \odot B = (a_{i,j} b_{i,j})_{i\in [n], j \in [m]}. \]
- en: Similarly the Hadamard division is defined as the element-wise division
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Hadamard除法定义为元素除法
- en: \[ A \oslash B = (a_{i,j} / b_{i,j})_{i\in [n], j \in [m]} \]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \oslash B = (a_{i,j} / b_{i,j})_{i\in [n], j \in [m]} \]
- en: where we assume that \(b_{i,j} \neq 0\) for all \(i,j\).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们假设对于所有 \(i,j\)，\(b_{i,j} \neq 0\)。
- en: '**EXAMPLE:** As an illustrative example,'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** 作为一个说明性的例子，'
- en: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \odot \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \times 0 & 2 \times 5\\ 3
    \times 6 & 4 \times 7\\ \end{bmatrix} = \begin{bmatrix} 0 & 10\\ 18 & 28\\ \end{bmatrix}.
    \end{split}\]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \odot \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \times 0 & 2 \times 5\\ 3
    \times 6 & 4 \times 7\\ \end{bmatrix} = \begin{bmatrix} 0 & 10\\ 18 & 28\\ \end{bmatrix}.
    \end{split}\]
- en: \(\lhd\)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: Recall that \(\mathbf{1}\) is the all-one vector and that, for \(\mathbf{x}
    = (x_1,\ldots,x_n) \in \mathbb{R}^n\), \(\mathrm{diag}(\mathbf{x}) \in \mathbb{R}^{n
    \times n}\) is the diagonal matrix with diagonal entries \(x_1,\ldots,x_n\).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，\(\mathbf{1}\) 是全一向量，并且对于 \(\mathbf{x} = (x_1,\ldots,x_n) \in \mathbb{R}^n\)，\(\mathrm{diag}(\mathbf{x})
    \in \mathbb{R}^{n \times n}\) 是对角线元素为 \(x_1,\ldots,x_n\) 的对角矩阵。
- en: '**LEMMA** **(Properties of the Hadamard Product)** \(\idx{properties of the
    Hadamard product}\xdi\) Let \(\mathbf{a} = (a_1,\ldots,a_n), \mathbf{b} = (b_1,\ldots,b_n),
    \mathbf{c} = (c_1,\ldots,c_n) \in \mathbb{R}^n\). Then the following hold:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(Hadamard 积的性质)** \(\idx{Hadamard product properties}\xdi\) 设 \(\mathbf{a}
    = (a_1,\ldots,a_n), \mathbf{b} = (b_1,\ldots,b_n), \mathbf{c} = (c_1,\ldots,c_n)
    \in \mathbb{R}^n\). 则以下成立：'
- en: a) \(\mathrm{diag}(\mathbf{a}) \, \mathbf{b} = \mathrm{diag}(\mathbf{a} \odot
    \mathbf{b})\);
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathrm{diag}(\mathbf{a}) \, \mathbf{b} = \mathrm{diag}(\mathbf{a} \odot
    \mathbf{b})\);
- en: b) \(\mathbf{a}^T(\mathbf{b} \odot \mathbf{c}) = \mathbf{1}^T(\mathbf{a} \odot
    \mathbf{b} \odot \mathbf{c})\)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{a}^T(\mathbf{b} \odot \mathbf{c}) = \mathbf{1}^T(\mathbf{a} \odot
    \mathbf{b} \odot \mathbf{c})\)
- en: 'and, provided \(a_i \neq 0\) for all \(i\), the following hold as well:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，假设对于所有 \(i\)，\(a_i \neq 0\)，以下也成立：
- en: c) \(\mathrm{diag}(\mathbf{a}) \, (\mathbf{b} \oslash \mathbf{a}) = \mathrm{diag}(\mathbf{b})\);
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathrm{diag}(\mathbf{a}) \, (\mathbf{b} \oslash \mathbf{a}) = \mathrm{diag}(\mathbf{b})\);
- en: d) \(\mathbf{a}^T \, (\mathbf{b} \oslash \mathbf{a}) = \mathbf{1}^T \mathbf{b}\).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbf{a}^T \, (\mathbf{b} \oslash \mathbf{a}) = \mathbf{1}^T \mathbf{b}\).
- en: \(\flat\)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* a) The product of a diagonal matrix and a vector produces a new vector
    whose original coordinates are multiplied by the corresponding diagonal entry.
    That proves the first claim.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* a) 对角矩阵与向量的乘积产生一个新的向量，其原始坐标乘以相应的对角线元素。这证明了第一个命题。'
- en: b) We have
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: b) 我们有
- en: \[ \mathbf{a}^T \, (\mathbf{b} \odot \mathbf{c}) = \sum_{i=1}^n a_i (b_i c_i)
    = \mathbf{1}^T (\mathbf{a} \odot \mathbf{b} \odot \mathbf{c}). \]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{a}^T \, (\mathbf{b} \odot \mathbf{c}) = \sum_{i=1}^n a_i (b_i c_i)
    = \mathbf{1}^T (\mathbf{a} \odot \mathbf{b} \odot \mathbf{c}). \]
- en: c) and d) follow respectively from a) and b).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: c) 和 d) 分别由 a) 和 b) 得出。
- en: \(\square\)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: Second, we introduce the Kronecker product\(\idx{Kronecker product}\xdi\). Let
    \(A = (a_{i,j})_{i \in [n], j \in [m]} \in \mathbb{R}^{n \times m}\) and \(B =
    (b_{i,j})_{i \in [p], j \in [q]} \in \mathbb{R}^{p \times q}\) be arbitrary matrices.
    Their Kronecker product, denoted \(A \otimes B \in \mathbb{R}^{np \times mq}\),
    is the following matrix in block form
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们引入Kronecker积\(\idx{Kronecker product}\xdi\). 设 \(A = (a_{i,j})_{i \in [n],
    j \in [m]} \in \mathbb{R}^{n \times m}\) 和 \(B = (b_{i,j})_{i \in [p], j \in [q]}
    \in \mathbb{R}^{p \times q}\) 是任意矩阵。它们的Kronecker积，记为 \(A \otimes B \in \mathbb{R}^{np
    \times mq}\)，是以下形式的矩阵
- en: \[\begin{split} A \otimes B = \begin{pmatrix} a_{1,1} B & \cdots & a_{1,m} B
    \\ \vdots & \ddots & \vdots \\ a_{n,1} B & \cdots & a_{n,m} B \end{pmatrix}. \end{split}\]
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} a_{1,1} B & \cdots & a_{1,m} B
    \\ \vdots & \ddots & \vdots \\ a_{n,1} B & \cdots & a_{n,m} B \end{pmatrix}. \end{split}\]
- en: '**EXAMPLE:** Here is a simple illustrative example from [Wikipedia](https://en.wikipedia.org/wiki/Kronecker_product#Examples):'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** 这里是一个来自 [维基百科](https://en.wikipedia.org/wiki/Kronecker_product#Examples)
    的简单说明性例子：'
- en: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \otimes \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} & 2 \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \\ 3
    \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} & 4 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 5 & 0 & 10 \\ 6 &
    7 & 12 & 14 \\ 0 & 15 & 0 & 20 \\ 18 & 21 & 24 & 28 \end{bmatrix}. \end{split}\]
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \otimes \begin{bmatrix}
    0 & 5 \\ 6 & 7 \\ \end{bmatrix} = \begin{bmatrix} 1 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} & 2 \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \\ 3
    \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} & 4 \begin{bmatrix} 0 & 5 \\ 6
    & 7 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 5 & 0 & 10 \\ 6 &
    7 & 12 & 14 \\ 0 & 15 & 0 & 20 \\ 18 & 21 & 24 & 28 \end{bmatrix}. \end{split}\]
- en: \(\lhd\)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Outer product)** \(\idx{outer product}\xdi\) Here is another
    example we have encoutered previously, the outer product of two vectors \(\mathbf{u}
    = (u_1,\ldots,u_n) \in \mathbb{R}^n\) and \(\mathbf{v} = (v_1,\ldots, v_m) \in
    \mathbb{R}^m\). Recall that the outer product is defined in block form as the
    \(n \times m\) matrix'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**例题** **(外积)** \(\idx{外积}\xdi\) 这里是另一个我们之前遇到的例子，两个向量 \(\mathbf{u} = (u_1,\ldots,u_n)
    \in \mathbb{R}^n\) 和 \(\mathbf{v} = (v_1,\ldots, v_m) \in \mathbb{R}^m\) 的外积。回忆一下，外积以块形式定义为
    \(n \times m\) 矩阵'
- en: \[ \mathbf{u} \mathbf{v}^T = \begin{pmatrix} v_1 \mathbf{u} & \cdots & v_m \mathbf{u}
    \end{pmatrix} = \mathbf{v}^T \otimes \mathbf{u}. \]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{u} \mathbf{v}^T = \begin{pmatrix} v_1 \mathbf{u} & \cdots & v_m \mathbf{u}
    \end{pmatrix} = \mathbf{v}^T \otimes \mathbf{u}. \]
- en: Equivalently,
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 相当于，
- en: \[\begin{split} \mathbf{u} \mathbf{v}^T = \begin{pmatrix} u_1 \mathbf{v}^T\\
    \vdots\\ u_n \mathbf{v}^T \end{pmatrix} = \mathbf{u} \otimes \mathbf{v}^T. \end{split}\]
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u} \mathbf{v}^T = \begin{pmatrix} u_1 \mathbf{v}^T\\
    \vdots\\ u_n \mathbf{v}^T \end{pmatrix} = \mathbf{u} \otimes \mathbf{v}^T. \end{split}\]
- en: \(\lhd\)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(continued)** In the previous example the Kronecker product
    turned out to be commutative (i.e., we had \(\mathbf{v}^T \otimes \mathbf{u} =
    \mathbf{u} \otimes \mathbf{v}^T\)). This is not the case in general. Going back
    to the first example above, note that'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**例题** **(继续)** 在前面的例子中，克罗内克积是可交换的（即，我们有 \(\mathbf{v}^T \otimes \mathbf{u}
    = \mathbf{u} \otimes \mathbf{v}^T\)）。在一般情况下并不总是这样。回到上面的第一个例子，注意'
- en: \[\begin{split} \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \otimes \begin{bmatrix}
    1 & 2 \\ 3 & 4 \\ \end{bmatrix} = \begin{bmatrix} 0 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} & 5 \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \\ 6
    \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} & 7 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 0 & 5 & 10 \\ 0 &
    0 & 15 & 20 \\ 6 & 12 & 7 & 14 \\ 18 & 24 & 21 & 28 \end{bmatrix}. \end{split}\]
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{bmatrix} 0 & 5 \\ 6 & 7 \\ \end{bmatrix} \otimes \begin{bmatrix}
    1 & 2 \\ 3 & 4 \\ \end{bmatrix} = \begin{bmatrix} 0 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} & 5 \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} \\ 6
    \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ \end{bmatrix} & 7 \begin{bmatrix} 1 & 2 \\ 3
    & 4 \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} 0 & 0 & 5 & 10 \\ 0 &
    0 & 15 & 20 \\ 6 & 12 & 7 & 14 \\ 18 & 24 & 21 & 28 \end{bmatrix}. \end{split}\]
- en: You can check that this is different from what we obtained in the opposite order.
    \(\lhd\)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以检查这和我们之前相反顺序得到的结果是不同的。\(\lhd\)
- en: The proof of the following lemma is left as an exercise.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下引理的证明留给读者作为练习。
- en: '**LEMMA** **(Properties of the Kronecker Product)** \(\idx{properties of the
    Kronecker Product}\xdi\) The Kronecker product has the following properties:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(克罗内克积的性质)** \(\idx{克罗内克积的性质}\xdi\) 克罗内克积具有以下性质：'
- en: a) If \(B, C\) are matrices of the same dimension
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: a) 如果 \(B, C\) 是相同维度的矩阵
- en: \[ A \otimes (B + C) = A \otimes B + A \otimes C \quad \text{and}\quad (B +
    C) \otimes A = B \otimes A + C \otimes A. \]
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \otimes (B + C) = A \otimes B + A \otimes C \quad \text{和}\quad (B + C)
    \otimes A = B \otimes A + C \otimes A. \]
- en: b) If \(A, B, C, D\) are matrices of such size that one can form the matrix
    products \(AC\) and \(BD\), then
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: b) 如果 \(A, B, C, D\) 是可以形成矩阵乘积 \(AC\) 和 \(BD\) 的矩阵，那么
- en: \[ (A \otimes B)\,(C \otimes D) = (AC) \otimes (BD). \]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)\,(C \otimes D) = (AC) \otimes (BD). \]
- en: c) If \(A, C\) are matrices of the same dimension and \(B, D\) are matrices
    of the same dimension, then
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: c) 如果 \(A, C\) 是相同维度的矩阵，且 \(B, D\) 是相同维度的矩阵，那么
- en: \[ (A \otimes B)\odot(C \otimes D) = (A\odot C) \otimes (B\odot D). \]
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)\odot(C \otimes D) = (A\odot C) \otimes (B\odot D). \]
- en: d) If \(A,B\) are invertible, then
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: d) 如果 \(A,B\) 是可逆的，那么
- en: \[ (A \otimes B)^{-1} = A^{-1} \otimes B^{-1}. \]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)^{-1} = A^{-1} \otimes B^{-1}. \]
- en: e) The transpose of \(A \otimes B\) is
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: e) \(A \otimes B\) 的转置是
- en: \[ (A \otimes B)^T = A^T \otimes B^T. \]
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A \otimes B)^T = A^T \otimes B^T. \]
- en: f) If \(\mathbf{u}\) is a column vector and \(A, B\) are matrices of such size
    that one can form the matrix product \(AB\), then
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: f) 如果 \(\mathbf{u}\) 是一个列向量，且 \(A, B\) 是可以形成矩阵乘积 \(AB\) 的矩阵，那么
- en: \[ (\mathbf{u} \otimes A) B = \mathbf{u} \otimes (AB) \quad\text{and}\quad (A
    \otimes \mathbf{u}) B = (AB) \otimes \mathbf{u}. \]
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{u} \otimes A) B = \mathbf{u} \otimes (AB) \quad\text{和}\quad (A
    \otimes \mathbf{u}) B = (AB) \otimes \mathbf{u}. \]
- en: Similarly,
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，
- en: \[ A \,(\mathbf{u}^T \otimes B) = \mathbf{u}^T \otimes (AB) \quad\text{and}\quad
    A \,(B \otimes \mathbf{u}^T) = (AB) \otimes \mathbf{u}^T. \]
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \,(\mathbf{u}^T \otimes B) = \mathbf{u}^T \otimes (AB) \quad\text{和}\quad
    A \,(B \otimes \mathbf{u}^T) = (AB) \otimes \mathbf{u}^T. \]
- en: \(\flat\)
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: 8.2.2\. Jacobian[#](#jacobian "Link to this heading")
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.2\. 雅可比矩阵[#](#jacobian "链接到这个标题")
- en: Recall that the derivative of a function of a real variable is the rate of change
    of the function with respect to the change in the variable. A different way to
    put this is that \(f'(x)\) is the slope of the tangent line to \(f\) at \(x\).
    Formally, one can approximate \(f(x)\) by a linear function in the neighborhood
    of \(x\) as follows
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，实变函数的导数是函数相对于变量变化的改变率。另一种说法是 \(f'(x)\) 是 \(f\) 在 \(x\) 处的切线斜率。形式上，可以在 \(x\)
    的邻域内通过以下方式近似 \(f(x)\)
- en: \[ f(x + h) = f(x) + f'(x) h + r(h), \]
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x + h) = f(x) + f'(x) h + r(h), \]
- en: where \(r(h)\) is negligible compared to \(h\) in the sense that
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(r(h)\) 与 \(h\) 相比可以忽略不计
- en: \[ \lim_{h\to 0} \frac{r(h)}{h} = 0. \]
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{h\to 0} \frac{r(h)}{h} = 0. \]
- en: Indeed, define
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，定义
- en: \[ r(h) = f(x + h) - f(x) - f'(x) h. \]
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r(h) = f(x + h) - f(x) - f'(x) h. \]
- en: Then by definition of the derivative
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 然后根据导数的定义
- en: \[ \lim_{h\to 0} \frac{r(h)}{h} = \lim_{h\to 0} \frac{f(x + h) - f(x) - f'(x)
    h}{h} = \lim_{h\to 0} \left[\frac{f(x + h) - f(x)}{h} - f'(x) \right] = 0. \]
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{h\to 0} \frac{r(h)}{h} = \lim_{h\to 0} \frac{f(x + h) - f(x) - f'(x)
    h}{h} = \lim_{h\to 0} \left[\frac{f(x + h) - f(x)}{h} - f'(x) \right] = 0. \]
- en: 'For vector-valued functions, we have the following generalization. Let \(\mathbf{f}
    = (f_1, \ldots, f_m) : D \to \mathbb{R}^m\) where \(D \subseteq \mathbb{R}^d\)
    and let \(\mathbf{x} \in D\) be an interior point of \(D\). We say that \(\mathbf{f}\)
    is diffentiable\(\idx{diffentiable}\xdi\) at \(\mathbf{x}\) if there exists a
    matrix \(A \in \mathbb{R}^{m \times d}\) such that'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '对于向量值函数，我们有以下推广。设 \(\mathbf{f} = (f_1, \ldots, f_m) : D \to \mathbb{R}^m\)
    其中 \(D \subseteq \mathbb{R}^d\)，并且设 \(\mathbf{x} \in D\) 是 \(D\) 的一个内部点。我们说 \(\mathbf{f}\)
    在 \(\mathbf{x}\) 处是可微的\(\idx{可微}\xdi\)，如果存在一个矩阵 \(A \in \mathbb{R}^{m \times d}\)
    使得'
- en: \[ \mathbf{f}(\mathbf{x}+\mathbf{h}) = \mathbf{f}(\mathbf{x}) + A \mathbf{h}
    + \mathbf{r}(\mathbf{h}) \]
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}+\mathbf{h}) = \mathbf{f}(\mathbf{x}) + A \mathbf{h}
    + \mathbf{r}(\mathbf{h}) \]
- en: where
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[ \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{r}(\mathbf{h})\|_2}{\|\mathbf{h}\|_2}
    = 0. \]
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{r}(\mathbf{h})\|_2}{\|\mathbf{h}\|_2}
    = 0. \]
- en: The matrix \(\mathbf{f}'(\mathbf{x}) = A\) is called the differential\(\idx{differential}\xdi\)
    of \(\mathbf{f}\) at \(\mathbf{x}\), and we see that the affine map \(\mathbf{f}(\mathbf{x})
    + A \mathbf{h}\) provides an approximation of \(\mathbf{f}\) in the neighborhood
    of \(\mathbf{x}\).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(\mathbf{f}'(\mathbf{x}) = A\) 被称为 \(\mathbf{f}\) 在 \(\mathbf{x}\) 处的微分\(\idx{微分}\xdi\)，我们看到仿射映射
    \(\mathbf{f}(\mathbf{x}) + A \mathbf{h}\) 提供了在 \(\mathbf{x}\) 的邻域内对 \(\mathbf{f}\)
    的近似。
- en: We will not derive the full theory here. In the case where each component of
    \(\mathbf{f}\) has continuous partial derivatives in a neighborhood of \(\mathbf{x}\),
    then the differential exists and is equal to the Jacobian, as defined next.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会推导完整的理论。在 \(\mathbf{f}\) 的每个分量在 \(\mathbf{x}\) 的邻域内具有连续偏导数的情形下，微分存在，并且等于下一个定义的雅可比矩阵。
- en: '**DEFINITION** **(Jacobian)** \(\idx{Jacobian}\xdi\) Let \(\mathbf{f} = (f_1,
    \ldots, f_m) : D \to \mathbb{R}^m\) where \(D \subseteq \mathbb{R}^d\) and let
    \(\mathbf{x}_0 \in D\) be an interior point of \(D\) where \(\frac{\partial f_j
    (\mathbf{x}_0)}{\partial x_i}\) exists for all \(i, j\). The Jacobian of \(\mathbf{f}\)
    at \(\mathbf{x}_0\) is the \(m \times d\) matrix'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(雅可比矩阵)** \(\idx{雅可比}\xdi\) 设 \(\mathbf{f} = (f_1, \ldots, f_m) :
    D \to \mathbb{R}^m\) 其中 \(D \subseteq \mathbb{R}^d\)，并且设 \(\mathbf{x}_0 \in D\)
    是 \(D\) 的一个内部点，其中对于所有 \(i, j\)，\(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\)
    都存在。在 \(\mathbf{x}_0\) 处 \(\mathbf{f}\) 的雅可比矩阵是一个 \(m \times d\) 的矩阵'
- en: \[\begin{split} J_{\mathbf{f}}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial
    f_1 (\mathbf{x}_0)}{\partial x_1} & \ldots & \frac{\partial f_1 (\mathbf{x}_0)}{\partial
    x_d}\\ \vdots & \ddots & \vdots\\ \frac{\partial f_m (\mathbf{x}_0)}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0)}{\partial x_d} \end{pmatrix}.
    \end{split}\]
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{f}}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial
    f_1 (\mathbf{x}_0)}{\partial x_1} & \ldots & \frac{\partial f_1 (\mathbf{x}_0)}{\partial
    x_d}\\ \vdots & \ddots & \vdots\\ \frac{\partial f_m (\mathbf{x}_0)}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0)}{\partial x_d} \end{pmatrix}.
    \end{split}\]
- en: \(\natural\)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: '**THEOREM** **(Differential and Jacobian)** \(\idx{differential and Jacobian
    theorem}\xdi\) Let \(\mathbf{f} = (f_1, \ldots, f_m) : D \to \mathbb{R}^m\) where
    \(D \subseteq \mathbb{R}^d\) and let \(\mathbf{x}_0 \in D\) be an interior point
    of \(D\). Assume that \(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\) exists
    and is continous is an open ball around \(\mathbf{x}_0\) for all \(i, j\). Then
    the differential at \(\mathbf{x}_0\) is equal to the Jacobian of \(\mathbf{f}\)
    at \(\mathbf{x}_0\). \(\sharp\)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(微分和雅可比)** \(\idx{微分和雅可比定理}\xdi\) 设 \(\mathbf{f} = (f_1, \ldots, f_m)
    : D \to \mathbb{R}^m\) 其中 \(D \subseteq \mathbb{R}^d\)，并且设 \(\mathbf{x}_0 \in
    D\) 是 \(D\) 的一个内点。假设对于所有 \(i, j\)，\(\frac{\partial f_j (\mathbf{x}_0)}{\partial
    x_i}\) 存在且在 \(\mathbf{x}_0\) 附近的某个开球内是连续的。那么在 \(\mathbf{x}_0\) 处的微分等于 \(\mathbf{f}\)
    在 \(\mathbf{x}_0\) 处的雅可比矩阵。 \(\sharp\)'
- en: Recall that for any \(A, B\) for which \(AB\) is well-defined it holds that
    \(\|A B \|_F \leq \|A\|_F \|B\|_F\). This applies in particular when \(B\) is
    a column vector, in which case \(\|B\|_F\) is its Euclidean norm.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，对于任何 \(A, B\)，当 \(AB\) 被良好定义时，它都满足 \(\|A B \|_F \leq \|A\|_F \|B\|_F\)。这特别适用于
    \(B\) 是一个列向量时，在这种情况下，\(\|B\|_F\) 是其欧几里得范数。
- en: '*Proof:* By the *Mean Value Theorem*, for each \(i\), there is \(\xi_{\mathbf{h},i}
    \in (0,1)\) such that'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 通过**平均值定理**，对于每个 \(i\)，存在 \(\xi_{\mathbf{h},i} \in (0,1)\) 使得'
- en: \[ f_i(\mathbf{x}_0+\mathbf{h}) = f_i(\mathbf{x}_0) + \nabla f_i(\mathbf{x}_0
    + \xi_{\mathbf{h},i} \mathbf{h})^T \mathbf{h}. \]
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_i(\mathbf{x}_0+\mathbf{h}) = f_i(\mathbf{x}_0) + \nabla f_i(\mathbf{x}_0
    + \xi_{\mathbf{h},i} \mathbf{h})^T \mathbf{h}. \]
- en: Define
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: \[\begin{split} \tilde{J}(\mathbf{h}) = \begin{pmatrix} \frac{\partial f_1 (\mathbf{x}_0
    + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_1} & \ldots & \frac{\partial f_1
    (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_d}\\ \vdots & \ddots
    & \vdots\\ \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_d} \end{pmatrix}. \end{split}\]
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \tilde{J}(\mathbf{h}) = \begin{pmatrix} \frac{\partial f_1 (\mathbf{x}_0
    + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_1} & \ldots & \frac{\partial f_1
    (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_d}\\ \vdots & \ddots
    & \vdots\\ \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_1} & \ldots & \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial
    x_d} \end{pmatrix}. \end{split}\]
- en: Hence we have
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们有
- en: \[ \mathbf{f}(\mathbf{x}_0+\mathbf{h}) - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \tilde{J}(\mathbf{h}) \,\mathbf{h} - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}.
    \]
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}_0+\mathbf{h}) - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \tilde{J}(\mathbf{h}) \,\mathbf{h} - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
    = \left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}.
    \]
- en: Taking a limit as \(\mathbf{h}\) goes to \(0\), we get
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\mathbf{h}\) 趋向于 \(0\) 时取极限，我们得到
- en: \[\begin{align*} \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{f}(\mathbf{x}_0+\mathbf{h})
    - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}
    &= \lim_{\mathbf{h} \to 0} \frac{\|\left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
    &\leq \lim_{\mathbf{h} \to 0} \frac{\|\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\|_F
    \|\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\ &= 0, \end{align*}\]
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \lim_{\mathbf{h} \to 0} \frac{\|\mathbf{f}(\mathbf{x}_0+\mathbf{h})
    - \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}
    &= \lim_{\mathbf{h} \to 0} \frac{\|\left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
    &\leq \lim_{\mathbf{h} \to 0} \frac{\|\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\|_F
    \|\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\ &= 0, \end{align*}\]
- en: by continuity of the partial derivatives. \(\square\)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通过偏导数的连续性。 \(\square\)
- en: '**EXAMPLE:** An example of a vector-valued function is'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：一个向量值函数的例子是'
- en: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} g_1(x_1,x_2)\\ g_2(x_1,x_2)\\
    g_3(x_1,x_2) \end{pmatrix} = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2 \end{pmatrix}.
    \end{split}\]
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} g_1(x_1,x_2)\\ g_2(x_1,x_2)\\
    g_3(x_1,x_2) \end{pmatrix} = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2 \end{pmatrix}.
    \end{split}\]
- en: Its Jacobian is
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 它的雅可比矩阵是
- en: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} \frac{\partial g_1
    (x_1, x_2)}{\partial x_1} & \frac{\partial g_1 (x_1, x_2)}{\partial x_2}\\ \frac{\partial
    g_2 (x_1, x_2)}{\partial x_1} & \frac{\partial g_2 (x_1, x_2)}{\partial x_2}\\
    \frac{\partial g_3 (x_1, x_2)}{\partial x_1} & \frac{\partial g_3 (x_1, x_2)}{\partial
    x_2} \end{pmatrix} = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}.
    \end{split}\]
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} \frac{\partial g_1
    (x_1, x_2)}{\partial x_1} & \frac{\partial g_1 (x_1, x_2)}{\partial x_2}\\ \frac{\partial
    g_2 (x_1, x_2)}{\partial x_1} & \frac{\partial g_2 (x_1, x_2)}{\partial x_2}\\
    \frac{\partial g_3 (x_1, x_2)}{\partial x_1} & \frac{\partial g_3 (x_1, x_2)}{\partial
    x_2} \end{pmatrix} = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}.
    \end{split}\]
- en: \(\lhd\)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Gradient and Jacobian)** For a continuously differentiable
    real-valued function \(f : D \to \mathbb{R}\), the Jacobian reduces to the row
    vector'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（梯度与雅可比矩阵）** 对于一个连续可微的实值函数 \(f : D \to \mathbb{R}\)，雅可比矩阵简化为行向量'
- en: \[ J_{f}(\mathbf{x}_0) = \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1},
    \ldots, \frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)^T = \nabla f(\mathbf{x}_0)^T
    \]
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{f}(\mathbf{x}_0) = \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1},
    \ldots, \frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)^T = \nabla f(\mathbf{x}_0)^T
    \]
- en: where \(\nabla f(\mathbf{x}_0)\) is the gradient of \(f\) at \(\mathbf{x}_0\).
    \(\lhd\)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\nabla f(\mathbf{x}_0)\) 是 \(f\) 在 \(\mathbf{x}_0\) 处的梯度。\(\lhd\)
- en: '**EXAMPLE:** **(Hessian and Jacobian)** For a twice continuously differentiable
    real-valued function \(f : D \to \mathbb{R}\), the Jacobian of its gradient is'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（Hessian与雅可比矩阵）** 对于一个二阶连续可微的实值函数 \(f : D \to \mathbb{R}\)，其梯度的雅可比矩阵为'
- en: \[\begin{split} J_{\nabla f}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial^2
    f(\mathbf{x}_0)}{\partial x_1^2} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_d \partial x_1}\\ \vdots & \ddots & \vdots\\ \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_1 \partial x_d} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
    \end{pmatrix}, \end{split}\]
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\nabla f}(\mathbf{x}_0) = \begin{pmatrix} \frac{\partial^2
    f(\mathbf{x}_0)}{\partial x_1^2} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_d \partial x_1}\\ \vdots & \ddots & \vdots\\ \frac{\partial^2 f(\mathbf{x}_0)}{\partial
    x_1 \partial x_d} & \cdots & \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
    \end{pmatrix}, \end{split}\]
- en: that is, the Hessian (tranposed, but that makes no difference; why?) of \(f\)
    at \(\mathbf{x}_0\). \(\lhd\)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 即，\(f\) 在 \(\mathbf{x}_0\) 处的Hessian（转置，但这没有关系；为什么？）。\(\lhd\)
- en: '**EXAMPLE:** **(Parametric Curve and Jacobian)** Consider the parametric curve
    \(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t)) \in \mathbb{R}^d\) for \(t\) in some
    closed interval of \(\mathbb{R}\). Assume that \(\mathbf{g}(t)\) is continuously
    differentiable at \(t\), that is, each of its component is.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（参数曲线与雅可比矩阵）** 考虑参数曲线 \(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t))
    \in \mathbb{R}^d\)，其中 \(t\) 在 \(\mathbb{R}\) 的某个闭区间内。假设 \(\mathbf{g}(t)\) 在 \(t\)
    处连续可微，即其每个分量都是。'
- en: Then
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[\begin{split} J_{\mathbf{g}}(t) = \begin{pmatrix} g_1'(t)\\ \vdots\\ g_m'(t)
    \end{pmatrix} = \mathbf{g}'(t). \end{split}\]
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(t) = \begin{pmatrix} g_1'(t)\\ \vdots\\ g_m'(t)
    \end{pmatrix} = \mathbf{g}'(t). \end{split}\]
- en: \(\lhd\)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Affine Map)** Let \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{m \times
    d}\) and \(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\). Define the vector-valued
    function \(\mathbf{f} = (f_1, \ldots, f_m) : \mathbb{R}^d \to \mathbb{R}^m\) as'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（仿射映射）** 设 \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{m \times d}\) 和
    \(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\)。定义向量值函数 \(\mathbf{f} = (f_1,
    \ldots, f_m) : \mathbb{R}^d \to \mathbb{R}^m\) 为'
- en: \[ \mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}. \]
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}. \]
- en: This is an affine map. Note in particular that, in the case \(\mathbf{b} = \mathbf{0}\)
    of a linear map,
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个仿射映射。特别地，在线性映射的情况下，当 \(\mathbf{b} = \mathbf{0}\) 时，
- en: \[ \mathbf{f}(\mathbf{x} + \mathbf{y}) = A(\mathbf{x} + \mathbf{y}) = A\mathbf{x}
    + A\mathbf{y} = \mathbf{f}(\mathbf{x}) + \mathbf{f}(\mathbf{y}). \]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x} + \mathbf{y}) = A(\mathbf{x} + \mathbf{y}) = A\mathbf{x}
    + A\mathbf{y} = \mathbf{f}(\mathbf{x}) + \mathbf{f}(\mathbf{y}). \]
- en: Denote the rows of \(A\) by \(\boldsymbol{\alpha}_1^T,\ldots,\boldsymbol{\alpha}_m^T\).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 用 \(\boldsymbol{\alpha}_1^T,\ldots,\boldsymbol{\alpha}_m^T\) 表示 \(A\) 的行。
- en: We compute the Jacobian of \(\mathbf{f}\) at \(\mathbf{x}\). Note that
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 \(\mathbf{f}\) 在 \(\mathbf{x}\) 处的雅可比矩阵。注意，
- en: \[\begin{align*} \frac{\partial f_i (\mathbf{x})}{\partial x_j} &= \frac{\partial}{\partial
    x_j}[\boldsymbol{\alpha}_i^T \mathbf{x} + b_i]\\ &= \frac{\partial}{\partial x_j}\left[\sum_{\ell=1}^m
    a_{i,\ell} x_{\ell} + b_i\right]\\ &= a_{i,j}. \end{align*}\]
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f_i (\mathbf{x})}{\partial x_j} &= \frac{\partial}{\partial
    x_j}[\boldsymbol{\alpha}_i^T \mathbf{x} + b_i]\\ &= \frac{\partial}{\partial x_j}\left[\sum_{\ell=1}^m
    a_{i,\ell} x_{\ell} + b_i\right]\\ &= a_{i,j}. \end{align*}\]
- en: So
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 所以
- en: \[ J_{\mathbf{f}}(\mathbf{x}) = A. \]
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{f}}(\mathbf{x}) = A. \]
- en: \(\lhd\)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: The following important example is a less straightforward application of the
    Jacobian.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 以下重要示例是雅可比矩阵的一个不那么直接的应用。
- en: It will be useful to introduce the vectorization \(\mathrm{vec}(A) \in \mathbb{R}^{nm}\)
    of a matrix \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) as the vector
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵 \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) 的向量化 \(\mathrm{vec}(A)
    \in \mathbb{R}^{nm}\) 介绍为向量将是有用的。
- en: \[ \mathrm{vec}(A) = (a_{1,1},\ldots,a_{n,1},a_{1,2},\ldots,a_{n,2},\ldots,a_{1,m},\ldots,a_{n,m}).
    \]
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{vec}(A) = (a_{1,1},\ldots,a_{n,1},a_{1,2},\ldots,a_{n,2},\ldots,a_{1,m},\ldots,a_{n,m}).
    \]
- en: That is, it is obtained by stacking the columns of \(A\) on top of each other.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是通过将 \(A\) 的列堆叠在一起得到的。
- en: '**EXAMPLE:** **(Jacobian of a Linear Map with Respect to its Matrix)** We take
    a different tack on the previous example. In data science applications, it will
    be useful to compute the Jacobian of a linear map \(X \mathbf{z}\) – with respect
    to the matrix \(X \in \mathbb{R}^{n \times m}\). Specifically, for a fixed \(\mathbf{z}
    \in \mathbb{R}^{m}\), letting \((\mathbf{x}^{(i)})^T\) be the \(i\)-th row of
    \(X\) we define the function'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(关于其矩阵的线性映射的雅可比矩阵)** 我们对前面的例子采取不同的方法。在数据科学应用中，计算线性映射 \(X \mathbf{z}\)
    的雅可比矩阵——相对于矩阵 \(X \in \mathbb{R}^{n \times m}\) 将是有用的。具体来说，对于固定的 \(\mathbf{z}
    \in \mathbb{R}^{m}\)，令 \((\mathbf{x}^{(i)})^T\) 为 \(X\) 的第 \(i\) 行，我们定义函数'
- en: \[\begin{split} \mathbf{f}(\mathbf{x}) = X \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\
    \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T
    \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T \mathbf{z} \end{pmatrix} \end{split}\]
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{f}(\mathbf{x}) = X \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\
    \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z} = \begin{pmatrix} (\mathbf{x}^{(1)})^T
    \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T \mathbf{z} \end{pmatrix} \end{split}\]
- en: where \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\).
- en: To compute the Jacobian, let us look at its columns that correspond to the variables
    in \(\mathbf{x}^{(k)}\), that is, columns \(\alpha_k = (k-1) m + 1\) to \(\beta_k
    = k m\). Note that only the \(k\)-th component of \(\mathbf{f}\) depends on \(\mathbf{x}^{(k)}\),
    so the rows \(\neq k\) of \(J_{\mathbf{f}}(\mathbf{x})\) are \(0\) for the corresponding
    columns.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算雅可比矩阵，让我们看看与 \(\mathbf{x}^{(k)}\) 中的变量相对应的列，即列 \(\alpha_k = (k-1) m + 1\)
    到 \(\beta_k = k m\)。注意，只有 \(\mathbf{f}\) 的第 \(k\) 个分量依赖于 \(\mathbf{x}^{(k)}\)，所以
    \(J_{\mathbf{f}}(\mathbf{x})\) 的行 \(\neq k\) 对应的列是 \(0\)。
- en: Row \(k\) on the other hand is \(\mathbf{z}^T\) from our previous formula for
    the gradient of an affine map. Hence one way to write the columns \(\alpha_k\)
    to \(\beta_k\) of \(J_{\mathbf{f}}(\mathbf{x})\) is \(\mathbf{e}_k \mathbf{z}^T\),
    where here \(\mathbf{e}_k \in \mathbb{R}^{n}\) is the \(k\)-th standard basis
    vector of \(\mathbb{R}^{n}\).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，行 \(k\) 是我们之前公式中关于仿射映射梯度的公式中的 \(\mathbf{z}^T\)。因此，一种写 \(J_{\mathbf{f}}(\mathbf{x})\)
    的列 \(\alpha_k\) 到 \(\beta_k\) 的方法是 \(\mathbf{e}_k \mathbf{z}^T\)，其中这里的 \(\mathbf{e}_k
    \in \mathbb{R}^{n}\) 是 \(\mathbb{R}^{n}\) 的第 \(k\) 个标准基向量。
- en: So \(J_{\mathbf{f}}(\mathbf{x})\) can be written in block form as
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 \(J_{\mathbf{f}}(\mathbf{x})\) 可以写成块形式。
- en: '\[ J_{\mathbf{f}}(\mathbf{x}) = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}^T &
    \cdots & \mathbf{e}_{n}\mathbf{z}^T \end{pmatrix} = I_{n\times n} \otimes \mathbf{z}^T
    =: \mathbb{B}_{n}[\mathbf{z}], \]'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ J_{\mathbf{f}}(\mathbf{x}) = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}^T &
    \cdots & \mathbf{e}_{n}\mathbf{z}^T \end{pmatrix} = I_{n\times n} \otimes \mathbf{z}^T
    =: \mathbb{B}_{n}[\mathbf{z}], \]'
- en: where the last equality is a definition. \(\lhd\)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最后一个等式是一个定义。 \(\lhd\)
- en: We will need one more wrinkle.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个额外的技巧。
- en: '**EXAMPLE:** **(Jacobian of a Linear Map with Respect to its Input and Matrix)**
    Consider again the linear map \(X \mathbf{z}\) – this time as a function of *both*
    the matrix \(X \in \mathbb{R}^{n \times m}\) and the vector \(\mathbf{z} \in \mathbb{R}^{m}\).
    That is, letting again \((\mathbf{x}^{(i)})^T\) be the \(i\)-th row of \(X\),
    we define the function'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(关于输入和矩阵的线性映射的雅可比矩阵)** 再次考虑线性映射 \(X \mathbf{z}\)——这次作为矩阵 \(X \in
    \mathbb{R}^{n \times m}\) 和向量 \(\mathbf{z} \in \mathbb{R}^{m}\) 的函数。也就是说，再次令 \((\mathbf{x}^{(i)})^T\)
    为 \(X\) 的第 \(i\) 行，我们定义函数'
- en: \[\begin{split} \mathbf{g}(\mathbf{z}, \mathbf{x}) = X \mathbf{z} = \begin{pmatrix}
    (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z}
    = \begin{pmatrix} (\mathbf{x}^{(1)})^T \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T
    \mathbf{z} \end{pmatrix} \end{split}\]
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{g}(\mathbf{z}, \mathbf{x}) = X \mathbf{z} = \begin{pmatrix}
    (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T \end{pmatrix} \mathbf{z}
    = \begin{pmatrix} (\mathbf{x}^{(1)})^T \mathbf{z} \\ \vdots\\ (\mathbf{x}^{(n)})^T
    \mathbf{z} \end{pmatrix} \end{split}\]
- en: where as before \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots,
    \mathbf{x}^{(n)})\).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，如前所述 \(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\)。
- en: To compute the Jacobian, we think of it as a block matrix and use the two previous
    examples. The columns of \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) corresponding
    to the variables in \(\mathbf{z}\), that is, columns \(1\) to \(m\), are
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算雅可比矩阵，我们将其视为一个分块矩阵，并使用前两个例子。\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) 对应于
    \(\mathbf{z}\) 中的变量的列，即 \(1\) 到 \(m\) 的列，是
- en: '\[\begin{split} X = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T
    \end{pmatrix} =: \mathbb{A}_{n}[\mathbf{x}]. \end{split}\]'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{split} X = \begin{pmatrix} (\mathbf{x}^{(1)})^T\\ \vdots\\ (\mathbf{x}^{(n)})^T
    \end{pmatrix} =: \mathbb{A}_{n}[\mathbf{x}]. \end{split}\]'
- en: 'The columns of \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) corresponding to
    the variables in \(\mathbf{x}\), that is, columns \(m + 1\) to \(m + nm\), are
    the matrix \(\mathbb{B}_{n}[\mathbf{z}]\). Note that, in both \(\mathbb{A}_{n}[\mathbf{x}]\)
    and \(\mathbb{B}_{n}[\mathbf{z}]\), the subscript \(n\) indicates the number of
    rows of the matrix. The number of columns is determined by \(n\) and the size
    of the input vector:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) 对应于 \(\mathbf{x}\) 中的变量的列，即 \(m +
    1\) 到 \(m + nm\) 的列，是矩阵 \(\mathbb{B}_{n}[\mathbf{z}]\)。注意，在 \(\mathbb{A}_{n}[\mathbf{x}]\)
    和 \(\mathbb{B}_{n}[\mathbf{z}]\) 中，下标 \(n\) 表示矩阵的行数。列数由 \(n\) 和输入向量的尺寸决定：
- en: the length of \(\mathbf{x}\) divided by \(n\) for \(\mathbb{A}_{n}[\mathbf{x}]\);
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{A}_{n}[\mathbf{x}]\) 的长度除以 \(n\)；
- en: the length of \(\mathbf{z}\) multiplied by \(n\) for \(\mathbb{B}_{n}[\mathbf{z}]\).
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathbb{B}_{n}[\mathbf{z}]\) 的长度乘以 \(n\)。
- en: So \(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) can be written in block form as
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\) 可以写成分块形式
- en: \[ J_{\mathbf{f}}(\mathbf{z}, \mathbf{x}) = \begin{pmatrix} \mathbb{A}_{n}[\mathbf{x}]
    & \mathbb{B}_{n}[\mathbf{z}] \end{pmatrix}. \]
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{f}}(\mathbf{z}, \mathbf{x}) = \begin{pmatrix} \mathbb{A}_{n}[\mathbf{x}]
    & \mathbb{B}_{n}[\mathbf{z}] \end{pmatrix}. \]
- en: \(\lhd\)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** **(Elementwise Function)** Let \(f : D \to \mathbb{R}\), with
    \(D \subseteq \mathbb{R}\), be a continuously differentiable real-valued function
    of a single variable. For \(n \geq 2\), consider applying \(f\) to each entry
    of a vector \(\mathbf{x} \in \mathbb{R}^n\), that is, let \(\mathbf{f} : D^n \to
    \mathbb{R}^n\) with'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（逐元素函数）** 设 \(f : D \to \mathbb{R}\)，其中 \(D \subseteq \mathbb{R}\)，是一个关于单变量的连续可微实值函数。对于
    \(n \geq 2\)，考虑将 \(f\) 应用到向量 \(\mathbf{x} \in \mathbb{R}^n\) 的每个元素上，即设 \(\mathbf{f}
    : D^n \to \mathbb{R}^n\)，使得'
- en: \[ \mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}), \ldots, f_n(\mathbf{x})) = (f(x_1),
    \ldots, f(x_n)). \]
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}), \ldots, f_n(\mathbf{x})) = (f(x_1),
    \ldots, f(x_n)). \]
- en: The Jacobian of \(\mathbf{f}\) can be computed from \(f'\), the derivative of
    the single-variable case. Indeed, letting \(\mathbf{x} = (x_1,\ldots,x_n)\) be
    such that \(x_i\) is an interior point of \(D\) for all \(i\),
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: \(\mathbf{f}\) 的雅可比矩阵可以从单变量情况的导数 \(f'\) 计算得出。实际上，让 \(\mathbf{x} = (x_1,\ldots,x_n)\)
    使得对于所有 \(i\)，\(x_i\) 是 \(D\) 的内点，
- en: \[ \frac{\partial f_j(\mathbf{x})}{\partial x_j} = f'(x_j), \]
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f_j(\mathbf{x})}{\partial x_j} = f'(x_j), \]
- en: while for \(\ell \neq j\)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\ell \neq j\)
- en: \[ \frac{\partial f_\ell(\mathbf{x})}{\partial x_j} =0, \]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f_\ell(\mathbf{x})}{\partial x_j} =0, \]
- en: as \(f_\ell(\mathbf{x})\) does not in fact depend on \(x_j\). In other words,
    the \(j\)-th column of the Jacobian is \(f'(x_j) \,\mathbf{e}_j\), where again
    \(\mathbf{e}_{j}\) is the \(j\)-th standard basis vector in \(\mathbb{R}^{n}\).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(f_\ell(\mathbf{x})\) 实际上不依赖于 \(x_j\)。换句话说，雅可比矩阵的第 \(j\) 列是 \(f'(x_j) \,\mathbf{e}_j\)，其中再次
    \(\mathbf{e}_{j}\) 是 \(\mathbb{R}^{n}\) 中的第 \(j\) 个标准基向量。
- en: So \(J_{\mathbf{f}}(\mathbf{x})\) is the diagonal matrix with diagonal entries
    \(f'(x_j)\), \(j=1, \ldots, n\), which we denote by
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(J_{\mathbf{f}}(\mathbf{x})\) 是对角矩阵，其对角元素是 \(f'(x_j)\)，\(j=1, \ldots, n\)，我们用
- en: \[ J_{\mathbf{f}}(\mathbf{x}) = \mathrm{diag}(f'(x_1),\ldots,f'(x_n)). \]
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{f}}(\mathbf{x}) = \mathrm{diag}(f'(x_1),\ldots,f'(x_n)). \]
- en: \(\lhd\)
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: 8.2.3\. Generalization of the Chain Rule[#](#generalization-of-the-chain-rule
    "Link to this heading")
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.3\. 链式法则的推广[#](#generalization-of-the-chain-rule "链接到本标题")
- en: As we have seen, functions are often obtained from the composition of simpler
    ones. We will use the vector notation \(\mathbf{h} = \mathbf{g} \circ \mathbf{f}\)
    for the function \(\mathbf{h}(\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))\).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，函数通常是通过更简单函数的组合得到的。我们将使用向量表示法 \(\mathbf{h} = \mathbf{g} \circ \mathbf{f}\)
    来表示函数 \(\mathbf{h}(\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))\)。
- en: '**LEMMA** **(Composition of Continuous Functions)** \(\idx{composition of continuous
    functions lemma}\xdi\) Let \(\mathbf{f} : D_1 \to \mathbb{R}^m\), where \(D_1
    \subseteq \mathbb{R}^d\), and let \(\mathbf{g} : D_2 \to \mathbb{R}^p\), where
    \(D_2 \subseteq \mathbb{R}^m\). Assume that \(\mathbf{f}\) is continuous at \(\mathbf{x}_0\)
    and that \(\mathbf{g}\) is continuous \(\mathbf{f}(\mathbf{x}_0)\). Then \(\mathbf{g}
    \circ \mathbf{f}\) is continuous at \(\mathbf{x}_0\). \(\flat\)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（连续函数的复合）** \(\idx{连续函数复合引理}\xdi\) 设 \(\mathbf{f} : D_1 \to \mathbb{R}^m\)，其中
    \(D_1 \subseteq \mathbb{R}^d\)，设 \(\mathbf{g} : D_2 \to \mathbb{R}^p\)，其中 \(D_2
    \subseteq \mathbb{R}^m\)。假设 \(\mathbf{f}\) 在 \(\mathbf{x}_0\) 处连续，且 \(\mathbf{g}\)
    在 \(\mathbf{f}(\mathbf{x}_0)\) 处连续。那么 \(\mathbf{g} \circ \mathbf{f}\) 在 \(\mathbf{x}_0\)
    处连续。 \(\flat\)'
- en: The *Chain Rule* gives a formula for the Jacobian of a composition.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**链式法则** 给出了复合函数雅可比矩阵的公式。'
- en: '**THEOREM** **(Chain Rule)** \(\idx{chain rule}\xdi\) Let \(\mathbf{f} : D_1
    \to \mathbb{R}^m\), where \(D_1 \subseteq \mathbb{R}^d\), and let \(\mathbf{g}
    : D_2 \to \mathbb{R}^p\), where \(D_2 \subseteq \mathbb{R}^m\). Assume that \(\mathbf{f}\)
    is continuously differentiable at \(\mathbf{x}_0\), an interior point of \(D_1\),
    and that \(\mathbf{g}\) is continuously differentiable at \(\mathbf{f}(\mathbf{x}_0)\),
    an interior point of \(D_2\). Then'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（链式法则）** \(\idx{链式法则}\xdi\) 设 \(\mathbf{f} : D_1 \to \mathbb{R}^m\)，其中
    \(D_1 \subseteq \mathbb{R}^d\)，设 \(\mathbf{g} : D_2 \to \mathbb{R}^p\)，其中 \(D_2
    \subseteq \mathbb{R}^m\)。假设 \(\mathbf{f}\) 在 \(D_1\) 的内点 \(\mathbf{x}_0\) 处可连续微分，且
    \(\mathbf{g}\) 在 \(D_2\) 的内点 \(\mathbf{f}(\mathbf{x}_0)\) 处可连续微分。那么'
- en: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \,J_{\mathbf{f}}(\mathbf{x}_0) \]
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \,J_{\mathbf{f}}(\mathbf{x}_0) \]
- en: as a product of matrices. \(\sharp\)
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 作为矩阵的乘积。 \(\sharp\)
- en: Intuitively, the Jacobian provides a linear approximation of the function in
    the neighborhood of a point. The composition of linear maps corresponds to the
    product of the associated matrices. Similarly, the Jacobian of a composition is
    the product of the Jacobians.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，雅可比矩阵提供了函数在一点邻域内的线性近似。线性映射的复合对应于相关矩阵的乘积。同样，复合函数的雅可比矩阵是雅可比矩阵的乘积。
- en: '*Proof:* To avoid confusion, we think of \(\mathbf{f}\) and \(\mathbf{g}\)
    as being functions of variables with different names, specifically,'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：** 为了避免混淆，我们考虑 \(\mathbf{f}\) 和 \(\mathbf{g}\) 是变量名称不同的函数，具体来说，'
- en: \[ \mathbf{f}(\mathbf{x}) = (f_1(x_1,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_d))
    \]
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{f}(\mathbf{x}) = (f_1(x_1,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_d))
    \]
- en: and
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbf{g}(\mathbf{y}) = (g_1(y_1,\ldots,y_m),\ldots,g_p(y_1,\ldots,y_m)).
    \]
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{g}(\mathbf{y}) = (g_1(y_1,\ldots,y_m),\ldots,g_p(y_1,\ldots,y_m)).
    \]
- en: We apply the *Chain Rule* for a real-valued function over a parametric vector
    curve. That is, we think of
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用实值函数在参数向量曲线上的**链式法则**。也就是说，我们考虑
- en: \[ h_i(\mathbf{x}) = g_i(\mathbf{f}(\mathbf{x})) = g_i(f_1(x_1,\ldots,x_j,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_j,\ldots,x_d))
    \]
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h_i(\mathbf{x}) = g_i(\mathbf{f}(\mathbf{x})) = g_i(f_1(x_1,\ldots,x_j,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_j,\ldots,x_d))
    \]
- en: as *a function of \(x_j\) only* with all other \(x_i\)s fixed.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 仅作为 \(x_j\) 的函数，其他所有 \(x_i\) 均固定。
- en: We get that
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到
- en: \[ \frac{\partial h_i(\mathbf{x}_0)}{\partial x_j} = \sum_{k=1}^m \frac{\partial
    g_i(\mathbf{f}(\mathbf{x}_0))} {\partial y_k} \frac{\partial f_k(\mathbf{x}_0)}{\partial
    x_j} \]
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial h_i(\mathbf{x}_0)}{\partial x_j} = \sum_{k=1}^m \frac{\partial
    g_i(\mathbf{f}(\mathbf{x}_0))} {\partial y_k} \frac{\partial f_k(\mathbf{x}_0)}{\partial
    x_j} \]
- en: where, as before, the notation \(\frac{\partial g_i} {\partial y_k}\) indicates
    the partial derivative of \(g_i\) with respect to its \(k\)-th component. In matrix
    form, the claim follows. \(\square\)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，与之前一样，符号 \(\frac{\partial g_i} {\partial y_k}\) 表示 \(g_i\) 对其第 \(k\) 个分量的偏导数。在矩阵形式中，该命题成立。
    \(\square\)
- en: '**EXAMPLE:** **(Affine Map continued)** Let \(A \in \mathbb{R}^{m \times d}\)
    and \(\mathbf{b} \in \mathbb{R}^{m}\). Define again the vector-valued function
    \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\) as \(\mathbf{f}(\mathbf{x}) = A
    \mathbf{x} + \mathbf{b}\). In addition, for \(C \in \mathbb{R}^{p \times m}\)
    and \(\mathbf{d} \in \mathbb{R}^{p}\), define \(\mathbf{g} : \mathbb{R}^m \to
    \mathbb{R}^p\) as \(\mathbf{g}(\mathbf{y}) = C \mathbf{y} + \mathbf{d}\).'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '**例：** **（仿射映射继续）** 设 \(A \in \mathbb{R}^{m \times d}\) 和 \(\mathbf{b} \in
    \mathbb{R}^{m}\)。再次定义向量值函数 \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\) 为 \(\mathbf{f}(\mathbf{x})
    = A \mathbf{x} + \mathbf{b}\)。此外，对于 \(C \in \mathbb{R}^{p \times m}\) 和 \(\mathbf{d}
    \in \mathbb{R}^{p}\)，定义 \(\mathbf{g} : \mathbb{R}^m \to \mathbb{R}^p\) 为 \(\mathbf{g}(\mathbf{y})
    = C \mathbf{y} + \mathbf{d}\)。'
- en: Then
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}))
    \,J_{\mathbf{f}}(\mathbf{x}) = C A, \]
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}))
    \,J_{\mathbf{f}}(\mathbf{x}) = C A, \]
- en: for all \(\mathbf{x} \in \mathbb{R}^d\).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(\mathbf{x} \in \mathbb{R}^d\).
- en: This is consistent with the observation that
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这与观察一致
- en: \[ \mathbf{g} \circ \mathbf{f} (\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))
    = C( A\mathbf{x} + \mathbf{b} ) + \mathbf{d} = CA \mathbf{x} + (C\mathbf{b} +
    \mathbf{d}). \]
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{g} \circ \mathbf{f} (\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))
    = C( A\mathbf{x} + \mathbf{b} ) + \mathbf{d} = CA \mathbf{x} + (C\mathbf{b} +
    \mathbf{d}). \]
- en: \(\lhd\)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** Suppose we want to compute the gradient of the function'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 假设我们想要计算函数的梯度'
- en: \[ f(x_1, x_2) = 3 x_1^2 + x_2 + \exp(x_1 x_2). \]
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x_1, x_2) = 3 x_1^2 + x_2 + \exp(x_1 x_2). \]
- en: We could apply the *Chain Rule* directly, but to illustrate the perspective
    that is coming up, we think of \(f\) as a composition of “simpler” vector-valued
    functions. Specifically, let
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接应用链式法则，但为了说明即将出现的观点，我们将 \(f\) 视为“更简单”的向量值函数的组合。具体来说，设
- en: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2
    \end{pmatrix} \qquad h(y_1,y_2,y_3) = y_1 + y_2 + \exp(y_3). \end{split}\]
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{g}(x_1,x_2) = \begin{pmatrix} 3 x_1^2\\ x_2\\ x_1 x_2
    \end{pmatrix} \qquad h(y_1,y_2,y_3) = y_1 + y_2 + \exp(y_3). \end{split}\]
- en: Then \(f(x_1, x_2) = h(\mathbf{g}(x_1, x_2)) = h \circ \mathbf{g}(x_1, x_2)\).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 \(f(x_1, x_2) = h(\mathbf{g}(x_1, x_2)) = h \circ \mathbf{g}(x_1, x_2)\).
- en: By the *Chain Rule*, we can compute the gradient of \(f\) by first computing
    the Jacobians of \(\mathbf{g}\) and \(h\). We have already computed the Jacobian
    of \(\mathbf{g}\)
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 根据链式法则，我们可以通过首先计算 \(\mathbf{g}\) 和 \(h\) 的雅可比矩阵来计算 \(f\) 的梯度。我们已计算了 \(\mathbf{g}\)
    的雅可比矩阵
- en: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\
    x_2 & x_1 \end{pmatrix}. \end{split}\]
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(x_1, x_2) = \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\
    x_2 & x_1 \end{pmatrix}. \end{split}\]
- en: The Jacobian of \(h\) is
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: \(h\) 的雅可比矩阵是
- en: \[ J_h(y_1, y_2, y_3) = \begin{pmatrix} \frac{\partial h(y_1, y_2, y_3)}{\partial
    y_1} & \frac{\partial h(y_1, y_2, y_3)}{\partial y_2} & \frac{\partial h(y_1,
    y_2, y_3)}{\partial y_3} \end{pmatrix} = \begin{pmatrix} 1 & 1 & \exp(y_3) \end{pmatrix}.
    \]
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_h(y_1, y_2, y_3) = \begin{pmatrix} \frac{\partial h(y_1, y_2, y_3)}{\partial
    y_1} & \frac{\partial h(y_1, y_2, y_3)}{\partial y_2} & \frac{\partial h(y_1,
    y_2, y_3)}{\partial y_3} \end{pmatrix} = \begin{pmatrix} 1 & 1 & \exp(y_3) \end{pmatrix}.
    \]
- en: The *Chain Rule* stipulates that
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则规定
- en: \[\begin{align*} \nabla f(x_1, x_2)^T &= J_f(x_1, x_2)\\ &= J_h(\mathbf{g}(x_1,x_2))
    \, J_{\mathbf{g}}(x_1, x_2)\\ &= \begin{pmatrix} 1 & 1 & \exp(g_3(x_1, x_2)) \end{pmatrix}
    \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}\\ &= \begin{pmatrix}
    1 & 1 & \exp(x_1 x_2) \end{pmatrix} \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 &
    x_1 \end{pmatrix}\\ &= \begin{pmatrix} 6 x_1 + x_2 \exp(x_1 x_2) & 1 + x_1 \exp(x_1
    x_2) \end{pmatrix}. \end{align*}\]
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f(x_1, x_2)^T &= J_f(x_1, x_2)\\ &= J_h(\mathbf{g}(x_1,x_2))
    \, J_{\mathbf{g}}(x_1, x_2)\\ &= \begin{pmatrix} 1 & 1 & \exp(g_3(x_1, x_2)) \end{pmatrix}
    \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 & x_1 \end{pmatrix}\\ &= \begin{pmatrix}
    1 & 1 & \exp(x_1 x_2) \end{pmatrix} \begin{pmatrix} 6 x_1 & 0\\ 0 & 1\\ x_2 &
    x_1 \end{pmatrix}\\ &= \begin{pmatrix} 6 x_1 + x_2 \exp(x_1 x_2) & 1 + x_1 \exp(x_1
    x_2) \end{pmatrix}. \end{align*}\]
- en: You can check directly (i.e., without the composition) that this is indeed the
    correct gradient (transposed).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接检查（即，无需组合）这确实是正确的梯度（转置）。
- en: Alternatively, it is instructive to “expand” the *Chain Rule* as we did in its
    proof. Specifically,
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，像我们在其证明中所做的那样，“展开”链式法则是有益的。具体来说，
- en: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_1} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_1}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_1}\\ &= 1 \cdot 6x_1 + 1 \cdot
    0 + \exp(g_3(x_1, x_2)) \cdot x_2\\ &= 6 x_1 + x_2 \exp(x_1 x_2). \end{align*}\]
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_1} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_1}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_1} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_1}\\ &= 1 \cdot 6x_1 + 1 \cdot
    0 + \exp(g_3(x_1, x_2)) \cdot x_2\\ &= 6 x_1 + x_2 \exp(x_1 x_2). \end{align*}\]
- en: Note that this corresponds to multiplying \(J_h(\mathbf{g}(x_1,x_2))\) by the
    first column of \(J_{\mathbf{g}}(x_1, x_2)\).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这对应于将 \(J_h(\mathbf{g}(x_1,x_2))\) 乘以 \(J_{\mathbf{g}}(x_1, x_2)\) 的第一列。
- en: Similarly
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 同样
- en: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_2} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_2}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_2}\\ &= 1 \cdot 0 + 1 \cdot 1 +
    \exp(g_3(x_1, x_2)) \cdot x_1\\ &= 1 + x_1 \exp(x_1 x_2). \end{align*}\]
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f (x_1, x_2)}{\partial x_2} &= \sum_{i=1}^3
    \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1,
    x_2)}{\partial x_2}\\ &= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1}
    \frac{\partial g_1 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_2} + \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial
    y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_2}\\ &= 1 \cdot 0 + 1 \cdot 1 +
    \exp(g_3(x_1, x_2)) \cdot x_1\\ &= 1 + x_1 \exp(x_1 x_2). \end{align*}\]
- en: This corresponds to multiplying \(J_h(\mathbf{g}(x_1,x_2))\) by the second column
    of \(J_{\mathbf{g}}(x_1, x_2)\). \(\lhd\)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于将 \(J_h(\mathbf{g}(x_1,x_2))\) 乘以 \(J_{\mathbf{g}}(x_1, x_2)\) 的第二列。 \(\lhd\)
- en: '**CHAT & LEARN** The Jacobian determinant has important applications in change
    of variables for multivariable integrals. Ask your favorite AI chatbot to explain
    this application and provide an example of using the Jacobian determinant in a
    change of variables for a double integral. \(\ddagger\)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 雅可比行列式在多元积分变量变换中有着重要的应用。请你的心仪 AI 聊天机器人解释这一应用，并提供一个使用雅可比行列式在双重积分变量变换中的例子。
    \(\ddagger\)'
- en: 8.2.4\. Brief introduction to automatic differentiation in PyTorch[#](#brief-introduction-to-automatic-differentiation-in-pytorch
    "Link to this heading")
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.4\. PyTorch 中自动微分的简要介绍[#](#brief-introduction-to-automatic-differentiation-in-pytorch
    "链接到这个标题")
- en: We illustrate the use of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\(\idx{automatic
    differentiation}\xdi\) to compute gradients in PyTorch.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了在 PyTorch 中使用 [自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)\(\idx{自动微分}\xdi\)
    来计算梯度的方法。
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation):'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '引用 [维基百科](https://en.wikipedia.org/wiki/Automatic_differentiation):'
- en: In mathematics and computer algebra, automatic differentiation (AD), also called
    algorithmic differentiation or computational differentiation, is a set of techniques
    to numerically evaluate the derivative of a function specified by a computer program.
    AD exploits the fact that every computer program, no matter how complicated, executes
    a sequence of elementary arithmetic operations (addition, subtraction, multiplication,
    division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying
    the chain rule repeatedly to these operations, derivatives of arbitrary order
    can be computed automatically, accurately to working precision, and using at most
    a small constant factor more arithmetic operations than the original program.
    Automatic differentiation is distinct from symbolic differentiation and numerical
    differentiation (the method of finite differences). Symbolic differentiation can
    lead to inefficient code and faces the difficulty of converting a computer program
    into a single expression, while numerical differentiation can introduce round-off
    errors in the discretization process and cancellation.
  id: totrans-435
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在数学和计算机代数中，自动微分（AD），也称为算法微分或计算微分，是一组用于数值评估由计算机程序指定的函数导数的技巧。AD 利用这样一个事实：无论计算机程序多么复杂，它都执行一系列基本的算术运算（加法、减法、乘法、除法等）和基本函数（指数、对数、正弦、余弦等）。通过将这些运算反复应用链式法则，可以自动计算任意阶的导数，精确到工作精度，并且使用的算术运算次数最多只比原始程序多一个很小的常数因子。自动微分与符号微分和数值微分（有限差分法）不同。符号微分可能导致代码效率低下，并且面临将计算机程序转换为单个表达式的困难，而数值微分可能在离散化过程中引入舍入误差和消去。
- en: '**Automatic differentiation in PyTorch** We will use [PyTorch](https://pytorch.org/tutorials/).
    It uses [tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\(\idx{tensor}\xdi\),
    which in many ways behave similarly to NumPy arrays. See [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)
    for a quick introduction. We first initialize the tensors. Here each tensor corresponds
    to a single real variable. With the option [`requires_grad=True`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad),
    we indicate that these are variables with respect to which a gradient will be
    taken later. We initialize the tensors at the values where the derivatives will
    be computed. If derivatives need to be computed at different values, we need to
    repeat this process. The function [`.backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html)
    computes the gradient using backpropagation, to which we will return later. The
    partial derivatives are accessed with [`.grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html).'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch 中的自动微分** 我们将使用 [PyTorch](https://pytorch.org/tutorials/)。它使用 [张量](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\(\idx{tensor}\xdi\)，在许多方面与
    NumPy 数组的行为相似。见[这里](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)的快速介绍。我们首先初始化张量。这里每个张量对应一个单独的实变量。通过选项`[`requires_grad=True`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad)，我们表明这些是之后将计算梯度的变量。我们初始化张量到将计算导数的值。如果需要在不同的值处计算导数，我们需要重复此过程。函数`[``.backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html)`使用反向传播计算梯度，我们稍后会回到这一点。偏导数可以通过`[``.grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html)`访问。'
- en: '**NUMERICAL CORNER:** This is better understood through an example.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 这通过一个例子更容易理解。'
- en: '[PRE12]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We define the function. Note that we use [`torch.exp`](https://pytorch.org/docs/stable/generated/torch.exp.html),
    the PyTorch implementation of the (element-wise) exponential function. Moreover,
    as in NumPy, PyTorch allows the use of `**` for [taking a power](https://pytorch.org/docs/stable/generated/torch.pow.html).
    [Here](https://pytorch.org/docs/stable/name_inference.html) is a list of operations
    on tensors in PyTorch.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了函数。请注意，我们使用了`[`torch.exp`](https://pytorch.org/docs/stable/generated/torch.exp.html)，这是
    PyTorch 对（逐元素）指数函数的实现。此外，与 NumPy 类似，PyTorch 允许使用 `**` 来进行[取幂](https://pytorch.org/docs/stable/generated/torch.pow.html)。[这里](https://pytorch.org/docs/stable/name_inference.html)是
    PyTorch 中张量操作的列表。
- en: '[PRE13]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The input parameters can also be vectors, which allows to consider functions
    of large numbers of variables. Here we use [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum)
    for taking a sum of the arguments.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 输入参数也可以是向量，这允许考虑大量变量的函数。这里我们使用`[`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum)`来对参数求和。
- en: '[PRE15]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here is another typical example in a data science context.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学背景下，这里还有一个典型的例子。
- en: '[PRE17]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain how to compute a second
    derivative using PyTorch (it’s bit tricky). Ask for code that you can apply to
    the previous examples. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习** 向你喜欢的 AI 聊天机器人解释如何使用 PyTorch 计算二阶导数（有点棘手）。请求代码，你可以将其应用于前面的例子。([在
    Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Implementing gradient descent in PyTorch** Rather than explicitly specifying
    the gradient function, we could use PyTorch to compute it automatically. This
    is done next. Note that the descent update is done within [`with torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html),
    which ensures that the update operation itself is not tracked for gradient computation.
    Here the input `x0` as well as the output `xk.numpy(force=True)` are NumPy arrays.
    The function [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    converts a PyTorch tensor to a NumPy array (see the documentation for an explanation
    of the `force=True` option). Also, quoting ChatGPT:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '**在 PyTorch 中实现梯度下降** 我们可以不用显式指定梯度函数，而是使用 PyTorch 自动计算它。接下来将这样做。注意，下降更新是在 `with
    torch.no_grad()` 内部完成的，这确保更新操作本身不被跟踪用于梯度计算。这里输入 `x0` 以及输出 `xk.numpy(force=True)`
    都是 NumPy 数组。函数 `torch.Tensor.numpy()` 将 PyTorch 张量转换为 NumPy 数组（有关 `force=True`
    选项的解释，请参阅文档）。此外，引用 ChatGPT：'
- en: In the given code, `.item()` is used to extract the scalar value from a tensor.
    In PyTorch, when you perform operations on tensors, you get back tensors as results,
    even if the result is a single scalar value. `.item()` is used to extract this
    scalar value from the tensor.
  id: totrans-451
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在给定的代码中，`.item()` 用于从张量中提取标量值。在 PyTorch 中，当你对张量执行操作时，你会得到张量作为结果，即使结果是一个单一的标量值。`.item()`
    用于从张量中提取这个标量值。
- en: '[PRE19]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**NUMERICAL CORNER:** We revisit a previous example.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角** 我们回顾一个先前的例子。'
- en: '[PRE20]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: \(\unlhd\)
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** The section briefly mentions that automatic differentiation
    is distinct from symbolic differentiation and numerical differentiation. Ask your
    favorite AI chatbot to explain in more detail the differences between these three
    methods of computing derivatives. \(\ddagger\)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 本节简要提到自动微分与符号微分和数值微分不同。请你的首选 AI 聊天机器人详细解释这三种计算导数方法的区别。 \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由 Claude、Gemini 和 ChatGPT 协助)*'
- en: '**1** Let \(A \in \mathbb{R}^{n \times m}\) and \(B \in \mathbb{R}^{p \times
    q}\). What are the dimensions of the Kronecker product \(A \otimes B\)?'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 设 \(A \in \mathbb{R}^{n \times m}\) 和 \(B \in \mathbb{R}^{p \times q}\)。Kronecker
    积 \(A \otimes B\) 的维度是什么？'
- en: a) \(n \times m\)
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(n \times m\)
- en: b) \(p \times q\)
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(p \times q\)
- en: c) \(np \times mq\)
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(np \times mq\)
- en: d) \(nq \times mp\)
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(nq \times mp\)
- en: '**2** If \(f: \mathbb{R}^d \rightarrow \mathbb{R}^m\) is a continuously differentiable
    function, what is its Jacobian \(J_f(x_0)\) at an interior point \(x_0\) of its
    domain?'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 如果 \(f: \mathbb{R}^d \rightarrow \mathbb{R}^m\) 是一个在定义域内点 \(x_0\) 处连续可微的函数，那么它在
    \(x_0\) 处的雅可比 \(J_f(x_0)\) 是什么？'
- en: a) A scalar representing the rate of change of \(f\) at \(x_0\).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: a) 表示 \(f\) 在 \(x_0\) 处变化率的标量。
- en: b) A vector in \(\mathbb{R}^m\) representing the direction of steepest ascent
    of \(f\) at \(x_0\).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: b) 在 \(x_0\) 处 \(f\) 的最速上升方向的 \(\mathbb{R}^m\) 向量。
- en: c) An \(m \times d\) matrix of partial derivatives of the component functions
    of \(f\) at \(x_0\).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(f\) 在 \(x_0\) 处的分量函数的偏导数构成的 \(m \times d\) 矩阵。
- en: d) The Hessian matrix of \(f\) at \(x_0\).
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(f\) 在 \(x_0\) 处的 Hessian 矩阵。
- en: '**3** In the context of the Chain Rule, if \(f: \mathbb{R}^2 \to \mathbb{R}^3\)
    and \(g: \mathbb{R}^3 \to \mathbb{R}\), what is the dimension of the Jacobian
    matrix \(J_{g \circ f}(x)\)?'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 在链式法则的背景下，如果 \(f: \mathbb{R}^2 \to \mathbb{R}^3\) 且 \(g: \mathbb{R}^3
    \to \mathbb{R}\)，那么 \(J_{g \circ f}(x)\) 的雅可比矩阵的维度是多少？'
- en: a) \(3 \times 2\)
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(3 \times 2\)
- en: b) \(1 \times 3\)
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(1 \times 3\)
- en: c) \(2 \times 3\)
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(2 \times 3\)
- en: d) \(1 \times 2\)
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(1 \times 2\)
- en: '**4** Let \(\mathbf{f} : D_1 \to \mathbb{R}^m\), where \(D_1 \subseteq \mathbb{R}^d\),
    and let \(\mathbf{g} : D_2 \to \mathbb{R}^p\), where \(D_2 \subseteq \mathbb{R}^m\).
    Assume that \(\mathbf{f}\) is continuously differentiable at \(\mathbf{x}_0\),
    an interior point of \(D_1\), and that \(\mathbf{g}\) is continuously differentiable
    at \(\mathbf{f}(\mathbf{x}_0)\), an interior point of \(D_2\). Which of the following
    is correct according to the Chain Rule?'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 设 \(\mathbf{f} : D_1 \to \mathbb{R}^m\)，其中 \(D_1 \subseteq \mathbb{R}^d\)，设
    \(\mathbf{g} : D_2 \to \mathbb{R}^p\)，其中 \(D_2 \subseteq \mathbb{R}^m\)。假设 \(\mathbf{f}\)
    在 \(D_1\) 的内点 \(\mathbf{x}_0\) 处连续可微，且 \(\mathbf{g}\) 在 \(\mathbf{f}(\mathbf{x}_0)\)
    处，即 \(D_2\) 的内点处连续可微。根据链式法则，以下哪个是正确的？'
- en: a) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{x}_0)
    \, J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))\)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{x}_0)
    \, J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))\)
- en: b) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \, J_{\mathbf{f}}(\mathbf{x}_0)\)
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \, J_{\mathbf{f}}(\mathbf{x}_0)\)
- en: c) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))
    \, J_{\mathbf{g}}(\mathbf{x}_0)\)
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))
    \, J_{\mathbf{g}}(\mathbf{x}_0)\)
- en: d) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{x}_0)
    \, J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))\)
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{x}_0)
    \, J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))\)
- en: '**5** Let \(A \in \mathbb{R}^{m \times d}\) and \(\mathbf{b} \in \mathbb{R}^{m}\).
    Define the vector-valued function \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\)
    as \(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\). What is the Jacobian
    of \(\mathbf{f}\) at \(\mathbf{x}_0\)?'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 设 \(A \in \mathbb{R}^{m \times d}\) 和 \(\mathbf{b} \in \mathbb{R}^{m}\)。定义向量值函数
    \(\mathbf{f} : \mathbb{R}^d \to \mathbb{R}^m\) 为 \(\mathbf{f}(\mathbf{x}) = A
    \mathbf{x} + \mathbf{b}\)。\(\mathbf{f}\) 在 \(\mathbf{x}_0\) 处的雅可比矩阵是什么？'
- en: a) \(J_{\mathbf{f}}(\mathbf{x}_0) = A^T\)
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(J_{\mathbf{f}}(\mathbf{x}_0) = A^T\)
- en: b) \(J_{\mathbf{f}}(\mathbf{x}_0) = A \mathbf{x}_0 + \mathbf{b}\)
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(J_{\mathbf{f}}(\mathbf{x}_0) = A \mathbf{x}_0 + \mathbf{b}\)
- en: c) \(J_{\mathbf{f}}(\mathbf{x}_0) = A\)
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(J_{\mathbf{f}}(\mathbf{x}_0) = A\)
- en: d) \(J_{\mathbf{f}}(\mathbf{x}_0) = \mathbf{b}\)
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(J_{\mathbf{f}}(\mathbf{x}_0) = \mathbf{b}\)
- en: 'Answer for 1: c. Justification: The text defines the Kronecker product as a
    matrix in block form with dimensions \(np \times mq\).'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 1题的答案：c. 理由：文本将克罗内克积定义为具有 \(np \times mq\) 维度的分块矩阵。
- en: 'Answer for 2: c. Justification: The text defines the Jacobian of a vector-valued
    function as a matrix of partial derivatives.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 2题的答案：c. 理由：文本将向量值函数的雅可比定义为偏导数矩阵。
- en: 'Answer for 3: d. Justification: The composition \(g \circ f\) maps \(\mathbb{R}^2
    \to \mathbb{R}\), hence the Jacobian matrix \(J_{g \circ f}(x)\) is \(1 \times
    2\).'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 3题的答案：d. 理由：复合函数 \(g \circ f\) 将 \(\mathbb{R}^2 \to \mathbb{R}\) 映射，因此雅可比矩阵
    \(J_{g \circ f}(x)\) 是 \(1 \times 2\)。
- en: 'Answer for 4: b. Justification: The text states: “The Chain Rule gives a formula
    for the Jacobian of a composition. […] Assume that \(\mathbf{f}\) is continuously
    differentiable at \(\mathbf{x}_0\), an interior point of \(D_1\), and that \(\mathbf{g}\)
    is continuously differentiable at \(\mathbf{f}(\mathbf{x}_0)\), an interior point
    of \(D_2\). Then'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 4题的答案：b. 理由：文本中提到：“链式法则给出了复合函数雅可比的公式。[……]假设 \(\mathbf{f}\) 在 \(D_1\) 的内部点 \(\mathbf{x}_0\)
    处连续可微，且 \(\mathbf{g}\) 在 \(\mathbf{f}(\mathbf{x}_0)\) 处，即 \(D_2\) 的内部点处连续可微。那么”
- en: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \,J_{\mathbf{f}}(\mathbf{x}_0) \]
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
    \, J_{\mathbf{f}}(\mathbf{x}_0) \]
- en: as a product of matrices.”
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 作为矩阵的产物。”
- en: 'Answer for 5: c. Justification: The text states: “Let \(A \in \mathbb{R}^{m
    \times d}\) and \(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\). Define the
    vector-valued function \(\mathbf{f} = (f_1, \ldots, f_m) : \mathbb{R}^d \to \mathbb{R}^m\)
    as \(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\). […] So \(J_{\mathbf{f}}(\mathbf{x})
    = A.\)”'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '5题的答案：c. 理由：文本中提到：“设 \(A \in \mathbb{R}^{m \times d}\) 和 \(\mathbf{b} = (b_1,\ldots,b_m)
    \in \mathbb{R}^{m}\)。定义向量值函数 \(\mathbf{f} = (f_1, \ldots, f_m) : \mathbb{R}^d
    \to \mathbb{R}^m\) 为 \(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)。\[
    \text{所以 } J_{\mathbf{f}}(\mathbf{x}) = A.\]”'
