<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>4.4. Power iteration#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>4.4. Power iteration#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html">https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html</a></blockquote>

<p>There is in general <a class="reference external" href="https://math.stackexchange.com/questions/2582300/what-does-the-author-mean-by-no-method-exists-for-exactly-computing-the-eigenva">no exact method</a> for computing SVDs. Instead we must rely on iterative methods, that is, methods that progressively approach the solution. We describe in this section the power iteration method. This method is behind an effective numerical approach for computing SVDs.</p>
<p>The focus here is on numerical methods and we will not spend much time computing SVDs by hand. But note that the connection between the SVD and the spectral decomposition of <span class="math notranslate nohighlight">\(A^T A\)</span> can be used for this purpose on small examples.</p>
<section id="key-lemma">
<h2><span class="section-number">4.4.1. </span>Key lemma<a class="headerlink" href="#key-lemma" title="Link to this heading">#</a></h2>
<p>We now derive the main idea behind an algorithm to compute singular vectors. Let <span class="math notranslate nohighlight">\(U \Sigma V^T\)</span> be a (compact) SVD of <span class="math notranslate nohighlight">\(A\)</span>. Because of the orthogonality of <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, the powers of <span class="math notranslate nohighlight">\(A^T A\)</span> have a simple representation. Indeed</p>
<div class="math notranslate nohighlight">
\[
B = A^T A
= (U \Sigma V^T)^T (U \Sigma V^T)
= V \Sigma^T U^T U \Sigma V^T
= V \Sigma^T \Sigma V^T.
\]</div>
<p>Note that this formula is closely related to our previously uncovered connection between the SVD and the spectral decomposition of <span class="math notranslate nohighlight">\(A^T A\)</span> – although it is not quite a spectral decomposition of <span class="math notranslate nohighlight">\(A^T A\)</span> since <span class="math notranslate nohighlight">\(V\)</span> is not orthogonal.</p>
<p>Iterating,</p>
<div class="math notranslate nohighlight">
\[
B^2 
= (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T)
= V (\Sigma^T \Sigma)^2 V^T,
\]</div>
<p>and, for general <span class="math notranslate nohighlight">\(k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
B^{k}
= V (\Sigma^T \Sigma)^{k} V^T.
\]</div>
<p>Hence, defining</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\widetilde{\Sigma}
= \Sigma^T \Sigma
= \begin{pmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; \sigma_r^2
\end{pmatrix},
\end{split}\]</div>
<p>we see that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\widetilde{\Sigma}^k
= \begin{pmatrix}
\sigma_1^{2k} &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \sigma_2^{2k} &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; \sigma_r^{2k}
\end{pmatrix}.
\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\(\sigma_1 &gt; \sigma_2, \ldots, \sigma_r\)</span>, which is typically the case with real datasets, we get that <span class="math notranslate nohighlight">\(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)</span> when <span class="math notranslate nohighlight">\(k\)</span> is large. Then, we get the approximation</p>
<div class="math notranslate nohighlight">
\[
B^{k}
=
\sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
\approx
\sigma_1^{2k} \mathbf{v}_1 \mathbf{v}_1^T.
\]</div>
<p>Finally, we arrive at:</p>
<p><strong>LEMMA</strong> <strong>(Power Iteration)</strong> <span class="math notranslate nohighlight">\(\idx{power iteration lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be a matrix and let <span class="math notranslate nohighlight">\(U \Sigma V^T\)</span> be a (compact) SVD of <span class="math notranslate nohighlight">\(A\)</span> such that <span class="math notranslate nohighlight">\(\sigma_1 &gt; \sigma_2 &gt; 0\)</span>. Define <span class="math notranslate nohighlight">\(B = A^T A\)</span> and assume that <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^m\)</span> is a vector satisfying <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &gt; 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1
\]</div>
<p>as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>. If instead <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit is <span class="math notranslate nohighlight">\(- \mathbf{v}_1\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> We use the approximation above and divide by the norm to get a unit norm vector in the direction of <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>.</p>
<p><em>Proof:</em> We have</p>
<div class="math notranslate nohighlight">
\[
B^{k}\mathbf{x}
=
\sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \mathbf{x}
=
\sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
\]</div>
<p>So</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|}
&amp;= 
\sum_{j=1}^r \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|}\\
&amp;= 
\mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|}\right\}
+ \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|}\right\}.
\end{align*}\]</div>
<p>This goes to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> as <span class="math notranslate nohighlight">\(k\to +\infty\)</span> if the expression in the first curly brackets goes to <span class="math notranslate nohighlight">\(1\)</span> and the one in the second curly brackets goes to <span class="math notranslate nohighlight">\(0\)</span>. We prove this in the next claim.</p>
<p><strong>LEMMA</strong> As <span class="math notranslate nohighlight">\(k\to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|} \to 1
\qquad
\text{and}
\qquad
\frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|} \to 0, 
\ 
j = 2,\ldots,r.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Because the <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span>s are an orthonormal basis,</p>
<div class="math notranslate nohighlight">
\[
\|B^{k}\mathbf{x}\|^2
= 
\sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})\right]^2
=
\sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
\]</div>
<p>So, as <span class="math notranslate nohighlight">\(k\to +\infty\)</span>, using the fact that <span class="math notranslate nohighlight">\(\mathbf{v}_1^T \mathbf{x} = \langle \mathbf{v}_1, \mathbf{x} \rangle \neq 0\)</span> by assumption</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T \mathbf{x})^2}
&amp;=
1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k} (\mathbf{v}_1^T \mathbf{x})^2}\\
&amp;=
1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k} \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\
&amp;\to 
1,
\end{align*}\]</div>
<p>since <span class="math notranslate nohighlight">\(\sigma_j &lt; \sigma_1\)</span> for all <span class="math notranslate nohighlight">\(j =2,\ldots,r\)</span>. That implies the first part of the claim by taking a square root and using <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &gt; 0\)</span>. The second part of the claim follows essentially from the same argument. <span class="math notranslate nohighlight">\(\square\)</span> <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> We revisit the example</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{pmatrix}
1 &amp; 0\\
-1 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>We previously compute its SVD and found that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{v}_1 
= \begin{pmatrix}
1\\
0
\end{pmatrix}.
\end{split}\]</div>
<p>This time we use the <em>Power Iteration Lemma</em>. Here</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B = A^T A 
= \begin{pmatrix}
2 &amp; 0\\
0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>Taking powers of this matrix is easy</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B^k = \begin{pmatrix}
2^k &amp; 0\\
0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>Let’s choose an arbitrary initial vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, say <span class="math notranslate nohighlight">\((-1, 2)\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B^k \mathbf{x}
= \begin{pmatrix}
-2^k\\
0
\end{pmatrix}
\quad
\text{and}
\quad
\|B^k \mathbf{x}\| = 2^k.
\end{split}\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
-1\\
0
\end{pmatrix} 
=
- \mathbf{v}_1,
\end{split}\]</div>
<p>as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>. In fact, in this case, convergence occurs after one step. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>The argument leading to the <em>Power Iteration Lemma</em> also holds more generally for the eigenvectors of positive semidefinite matrices. Let <span class="math notranslate nohighlight">\(A\)</span> be a symmetric, positive semidefinite matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{d \times d}\)</span>. By the <em>Spectral Theorem</em>, it has an eigenvector decomposition</p>
<div class="math notranslate nohighlight">
\[
A
= Q \Lambda Q^T
= \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T
\]</div>
<p>where further <span class="math notranslate nohighlight">\(0 \leq \lambda_d \leq \cdots \leq \lambda_1\)</span> by the <em>Characterization of Positive Semidefiniteness</em>. Because of the orthogonality of <span class="math notranslate nohighlight">\(Q\)</span>, the powers of <span class="math notranslate nohighlight">\(A\)</span> have a simple representation. The square gives</p>
<div class="math notranslate nohighlight">
\[
A^2 
= (Q \Lambda Q^T) (Q \Lambda Q^T)
= Q \Lambda^2 Q^T.
\]</div>
<p>Repeating, we obtain</p>
<div class="math notranslate nohighlight">
\[
A^{k}
= Q \Lambda^{k} Q^T.
\]</div>
<p>This leads to the following:</p>
<p><strong>LEMMA</strong> <strong>(Power Iteration)</strong> <span class="math notranslate nohighlight">\(\idx{power iteration lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A\)</span> be a symmetric, positive semindefinite matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{d \times d}\)</span> with eigenvector decomposition <span class="math notranslate nohighlight">\(A= Q \Lambda Q^T\)</span> where the eigenvalues satisfy <span class="math notranslate nohighlight">\(0 \leq \lambda_d \leq \cdots \leq \lambda_2 &lt; \lambda_1\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> is a vector such that <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &gt; 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1
\]</div>
<p>as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>. If instead <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit is <span class="math notranslate nohighlight">\(- \mathbf{q}_1\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>The proof is similar to the case of singular vectors.</p>
</section>
<section id="computing-the-top-singular-vector">
<h2><span class="section-number">4.4.2. </span>Computing the top singular vector<a class="headerlink" href="#computing-the-top-singular-vector" title="Link to this heading">#</a></h2>
<p>Power iteration gives us a way to compute <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> – at least approximately if we use a large enough <span class="math notranslate nohighlight">\(k\)</span>. But how do we find an appropriate vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, as required by the <em>Power Iteration Lemma</em>? It turns out that a random vector will do. For instance, let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be an <span class="math notranslate nohighlight">\(m\)</span>-dimensional spherical Gaussian with mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. Then, <span class="math notranslate nohighlight">\(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X} \rangle = 0] = 0\)</span>.</p>
<p>We implement the algorithm suggested by the <em>Power Iteration Lemma</em>. That is, we compute <span class="math notranslate nohighlight">\(B^{k} \mathbf{x}\)</span>, then normalize it. To obtain the corresponding singular value and left singular vector, we use that <span class="math notranslate nohighlight">\(\sigma_1 = \|A \mathbf{v}_1\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">topsing</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">B</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">v</span> <span class="o">/</span> <span class="n">s</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We will apply it to our previous two-cluster example. The necessary functions are in <a class="reference external" href="https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py">mmids.py</a>, which is available on the <a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main">GitHub of the book</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png" src="../Images/e66fa4144461a2ee2155a730c0c7df77.png" data-original-src="https://mmids-textbook.github.io/_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png"/>
</div>
</div>
<p>Let’s compute the top singular vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">topsing</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.99257882  0.10164805  0.01581003  0.03202184  0.02075852  0.02798115
 -0.02920916 -0.028189   -0.0166094  -0.00648726]
</pre></div>
</div>
</div>
</div>
<p>This is approximately <span class="math notranslate nohighlight">\(-\mathbf{e}_1\)</span>. We get roughly the same answer (possibly up to sign) from Python’s <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.svd</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vh</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vh</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.99257882  0.10164803  0.01581003  0.03202184  0.02075851  0.02798112
 -0.02920917 -0.028189   -0.01660938 -0.00648724]
</pre></div>
</div>
</div>
</div>
<p>Recall that, when we applied <span class="math notranslate nohighlight">\(k\)</span>-means clustering to this example with <span class="math notranslate nohighlight">\(d=1000\)</span> dimension, we obtained a very poor clustering.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>99423.42794703908
99423.42794703908
99423.42794703908
99423.42794703908
99423.42794703908
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png" src="../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png" data-original-src="https://mmids-textbook.github.io/_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png"/>
</div>
</div>
<p>Let’s try again, but after projecting on the top singular vector. Recall that this corresponds to finding the best one-dimensional approximating subspace. The projection can be computed using the truncated SVD <span class="math notranslate nohighlight">\(Z= U_{(1)} \Sigma_{(1)} V_{(1)}^T\)</span>. We can interpret the rows of <span class="math notranslate nohighlight">\(U_{(1)} \Sigma_{(1)}\)</span> as the coefficients of each data point in the basis <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. We will work in that basis. We need one small hack: because our implementation of <span class="math notranslate nohighlight">\(k\)</span>-means clustering expects data points in at least <span class="math notranslate nohighlight">\(2\)</span> dimension, we add a column of <span class="math notranslate nohighlight">\(0\)</span>’s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">topsing</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">Xproj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">u</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xproj</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xproj</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png" src="../Images/d0d37edca421b5badf55590966b6d81a.png" data-original-src="https://mmids-textbook.github.io/_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png"/>
</div>
</div>
<p>There is a small – yet noticeable – gap around 0. We run <span class="math notranslate nohighlight">\(k\)</span>-means clustering on the projected data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">Xproj</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1779.020119584778
514.1899426112672
514.1899426112672
514.1899426112672
514.1899426112672
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png" src="../Images/5ffbd56fc722edd92c57dc038954354a.png" data-original-src="https://mmids-textbook.github.io/_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png"/>
</div>
</div>
<p>Much better. We give a more formal explanation of this outcome in a subsequent section. In essence, quoting [BHK, Section 7.5.1]:</p>
<blockquote>
<div><p>[…] let’s understand the central advantage of doing the projection to [the top <span class="math notranslate nohighlight">\(k\)</span> right singular vectors]. It is simply that for any reasonable (unknown) clustering of data points, the projection brings data points closer to their cluster centers.</p>
</div></blockquote>
<p>Finally, looking at the top right singular vector (or its first ten entries for lack of space), we see that it does align quite well (but not perfectly) with the first dimension.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-0.55564563 -0.02433674  0.02193487 -0.0333936  -0.00445505 -0.00243003
  0.02576056  0.02523275 -0.00682153  0.02524646]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> There are other methods to compute the SVD. Ask your favorite AI chatbot about randomized algorithms for the SVD. What are their advantages in terms of computational efficiency for large matrices? <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In the power iteration lemma for the positive semidefinite case, what happens when the initial vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> satisfies <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &lt; 0\)</span>?</p>
<p>a) The iteration converges to <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span>.</p>
<p>b) The iteration converges to <span class="math notranslate nohighlight">\(-\mathbf{q}_1\)</span>.</p>
<p>c) The iteration does not converge.</p>
<p>d) The iteration converges to a random eigenvector.</p>
<p><strong>2</strong> In the power iteration lemma for the SVD case, what is the convergence result for a random vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>.</p>
<p>b) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> or <span class="math notranslate nohighlight">\(-\mathbf{v}_1\)</span>.</p>
<p>c) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\sigma_1\)</span>.</p>
<p>d) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> does not converge.</p>
<p><strong>3</strong> Suppose you apply the power iteration method to a matrix <span class="math notranslate nohighlight">\(A\)</span> and obtain a vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>. How can you compute the corresponding singular value <span class="math notranslate nohighlight">\(\sigma\)</span> and left singular vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\sigma = \|A\mathbf{v}\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = A\mathbf{v}/\sigma\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\sigma = \|A^T\mathbf{v}\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = A^T\mathbf{v}/\sigma\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\sigma = \|\mathbf{v}\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = \mathbf{v}/\sigma\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\sigma = 1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = A\mathbf{v}\)</span></p>
<p><strong>4</strong> What is required for the initial vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in the power iteration method to ensure convergence to the top eigenvector?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> must be a zero vector.</p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> must be orthogonal to the top eigenvector.</p>
<p>c) <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\)</span>.</p>
<p>d) <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> must be the top eigenvector itself.</p>
<p><strong>5</strong> What does the truncated SVD <span class="math notranslate nohighlight">\(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\)</span> correspond to? [Uses Section 4.8.2.1.]</p>
<p>a) The best one-dimensional approximating subspace</p>
<p>b) The best two-dimensional approximating subspace</p>
<p>c) The projection of the data onto the top singular vector</p>
<p>d) The projection of the data onto the top two singular vectors</p>
<p>Answer for 1: b. Justification: The lemma states that if <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit of <span class="math notranslate nohighlight">\(A^k \mathbf{x} / \|A^k \mathbf{x}\|\)</span> is <span class="math notranslate nohighlight">\(-\mathbf{q}_1\)</span>.</p>
<p>Answer for 2: b. Justification: The lemma states that if <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>, and if <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit is <span class="math notranslate nohighlight">\(-\mathbf{v}_1\)</span>.</p>
<p>Answer for 3: a. Justification: The text provides these formulas in the “Numerical Corner” section.</p>
<p>Answer for 4: c. Justification: The key lemma states that convergence is ensured if <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is such that <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\)</span>.</p>
<p>Answer for 5: d. Justification: The text states that “projecting on the top two singular vectors… corresponds to finding the best two-dimensional approximating subspace. The projection can be computed using the truncated SVD <span class="math notranslate nohighlight">\(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\)</span>.”</p>
</section>
&#13;

<h2><span class="section-number">4.4.1. </span>Key lemma<a class="headerlink" href="#key-lemma" title="Link to this heading">#</a></h2>
<p>We now derive the main idea behind an algorithm to compute singular vectors. Let <span class="math notranslate nohighlight">\(U \Sigma V^T\)</span> be a (compact) SVD of <span class="math notranslate nohighlight">\(A\)</span>. Because of the orthogonality of <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, the powers of <span class="math notranslate nohighlight">\(A^T A\)</span> have a simple representation. Indeed</p>
<div class="math notranslate nohighlight">
\[
B = A^T A
= (U \Sigma V^T)^T (U \Sigma V^T)
= V \Sigma^T U^T U \Sigma V^T
= V \Sigma^T \Sigma V^T.
\]</div>
<p>Note that this formula is closely related to our previously uncovered connection between the SVD and the spectral decomposition of <span class="math notranslate nohighlight">\(A^T A\)</span> – although it is not quite a spectral decomposition of <span class="math notranslate nohighlight">\(A^T A\)</span> since <span class="math notranslate nohighlight">\(V\)</span> is not orthogonal.</p>
<p>Iterating,</p>
<div class="math notranslate nohighlight">
\[
B^2 
= (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T)
= V (\Sigma^T \Sigma)^2 V^T,
\]</div>
<p>and, for general <span class="math notranslate nohighlight">\(k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
B^{k}
= V (\Sigma^T \Sigma)^{k} V^T.
\]</div>
<p>Hence, defining</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\widetilde{\Sigma}
= \Sigma^T \Sigma
= \begin{pmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; \sigma_r^2
\end{pmatrix},
\end{split}\]</div>
<p>we see that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\widetilde{\Sigma}^k
= \begin{pmatrix}
\sigma_1^{2k} &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \sigma_2^{2k} &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; \sigma_r^{2k}
\end{pmatrix}.
\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\(\sigma_1 &gt; \sigma_2, \ldots, \sigma_r\)</span>, which is typically the case with real datasets, we get that <span class="math notranslate nohighlight">\(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)</span> when <span class="math notranslate nohighlight">\(k\)</span> is large. Then, we get the approximation</p>
<div class="math notranslate nohighlight">
\[
B^{k}
=
\sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
\approx
\sigma_1^{2k} \mathbf{v}_1 \mathbf{v}_1^T.
\]</div>
<p>Finally, we arrive at:</p>
<p><strong>LEMMA</strong> <strong>(Power Iteration)</strong> <span class="math notranslate nohighlight">\(\idx{power iteration lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be a matrix and let <span class="math notranslate nohighlight">\(U \Sigma V^T\)</span> be a (compact) SVD of <span class="math notranslate nohighlight">\(A\)</span> such that <span class="math notranslate nohighlight">\(\sigma_1 &gt; \sigma_2 &gt; 0\)</span>. Define <span class="math notranslate nohighlight">\(B = A^T A\)</span> and assume that <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^m\)</span> is a vector satisfying <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &gt; 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1
\]</div>
<p>as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>. If instead <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit is <span class="math notranslate nohighlight">\(- \mathbf{v}_1\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> We use the approximation above and divide by the norm to get a unit norm vector in the direction of <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>.</p>
<p><em>Proof:</em> We have</p>
<div class="math notranslate nohighlight">
\[
B^{k}\mathbf{x}
=
\sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \mathbf{x}
=
\sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
\]</div>
<p>So</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|}
&amp;= 
\sum_{j=1}^r \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|}\\
&amp;= 
\mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|}\right\}
+ \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|}\right\}.
\end{align*}\]</div>
<p>This goes to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> as <span class="math notranslate nohighlight">\(k\to +\infty\)</span> if the expression in the first curly brackets goes to <span class="math notranslate nohighlight">\(1\)</span> and the one in the second curly brackets goes to <span class="math notranslate nohighlight">\(0\)</span>. We prove this in the next claim.</p>
<p><strong>LEMMA</strong> As <span class="math notranslate nohighlight">\(k\to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|} \to 1
\qquad
\text{and}
\qquad
\frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})}
{\|B^{k} \mathbf{x}\|} \to 0, 
\ 
j = 2,\ldots,r.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Because the <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span>s are an orthonormal basis,</p>
<div class="math notranslate nohighlight">
\[
\|B^{k}\mathbf{x}\|^2
= 
\sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})\right]^2
=
\sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
\]</div>
<p>So, as <span class="math notranslate nohighlight">\(k\to +\infty\)</span>, using the fact that <span class="math notranslate nohighlight">\(\mathbf{v}_1^T \mathbf{x} = \langle \mathbf{v}_1, \mathbf{x} \rangle \neq 0\)</span> by assumption</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T \mathbf{x})^2}
&amp;=
1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k} (\mathbf{v}_1^T \mathbf{x})^2}\\
&amp;=
1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k} \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\
&amp;\to 
1,
\end{align*}\]</div>
<p>since <span class="math notranslate nohighlight">\(\sigma_j &lt; \sigma_1\)</span> for all <span class="math notranslate nohighlight">\(j =2,\ldots,r\)</span>. That implies the first part of the claim by taking a square root and using <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &gt; 0\)</span>. The second part of the claim follows essentially from the same argument. <span class="math notranslate nohighlight">\(\square\)</span> <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> We revisit the example</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{pmatrix}
1 &amp; 0\\
-1 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>We previously compute its SVD and found that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{v}_1 
= \begin{pmatrix}
1\\
0
\end{pmatrix}.
\end{split}\]</div>
<p>This time we use the <em>Power Iteration Lemma</em>. Here</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B = A^T A 
= \begin{pmatrix}
2 &amp; 0\\
0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>Taking powers of this matrix is easy</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B^k = \begin{pmatrix}
2^k &amp; 0\\
0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>Let’s choose an arbitrary initial vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, say <span class="math notranslate nohighlight">\((-1, 2)\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B^k \mathbf{x}
= \begin{pmatrix}
-2^k\\
0
\end{pmatrix}
\quad
\text{and}
\quad
\|B^k \mathbf{x}\| = 2^k.
\end{split}\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
-1\\
0
\end{pmatrix} 
=
- \mathbf{v}_1,
\end{split}\]</div>
<p>as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>. In fact, in this case, convergence occurs after one step. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>The argument leading to the <em>Power Iteration Lemma</em> also holds more generally for the eigenvectors of positive semidefinite matrices. Let <span class="math notranslate nohighlight">\(A\)</span> be a symmetric, positive semidefinite matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{d \times d}\)</span>. By the <em>Spectral Theorem</em>, it has an eigenvector decomposition</p>
<div class="math notranslate nohighlight">
\[
A
= Q \Lambda Q^T
= \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T
\]</div>
<p>where further <span class="math notranslate nohighlight">\(0 \leq \lambda_d \leq \cdots \leq \lambda_1\)</span> by the <em>Characterization of Positive Semidefiniteness</em>. Because of the orthogonality of <span class="math notranslate nohighlight">\(Q\)</span>, the powers of <span class="math notranslate nohighlight">\(A\)</span> have a simple representation. The square gives</p>
<div class="math notranslate nohighlight">
\[
A^2 
= (Q \Lambda Q^T) (Q \Lambda Q^T)
= Q \Lambda^2 Q^T.
\]</div>
<p>Repeating, we obtain</p>
<div class="math notranslate nohighlight">
\[
A^{k}
= Q \Lambda^{k} Q^T.
\]</div>
<p>This leads to the following:</p>
<p><strong>LEMMA</strong> <strong>(Power Iteration)</strong> <span class="math notranslate nohighlight">\(\idx{power iteration lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A\)</span> be a symmetric, positive semindefinite matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{d \times d}\)</span> with eigenvector decomposition <span class="math notranslate nohighlight">\(A= Q \Lambda Q^T\)</span> where the eigenvalues satisfy <span class="math notranslate nohighlight">\(0 \leq \lambda_d \leq \cdots \leq \lambda_2 &lt; \lambda_1\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> is a vector such that <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &gt; 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1
\]</div>
<p>as <span class="math notranslate nohighlight">\(k \to +\infty\)</span>. If instead <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit is <span class="math notranslate nohighlight">\(- \mathbf{q}_1\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>The proof is similar to the case of singular vectors.</p>
&#13;

<h2><span class="section-number">4.4.2. </span>Computing the top singular vector<a class="headerlink" href="#computing-the-top-singular-vector" title="Link to this heading">#</a></h2>
<p>Power iteration gives us a way to compute <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> – at least approximately if we use a large enough <span class="math notranslate nohighlight">\(k\)</span>. But how do we find an appropriate vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, as required by the <em>Power Iteration Lemma</em>? It turns out that a random vector will do. For instance, let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be an <span class="math notranslate nohighlight">\(m\)</span>-dimensional spherical Gaussian with mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. Then, <span class="math notranslate nohighlight">\(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X} \rangle = 0] = 0\)</span>.</p>
<p>We implement the algorithm suggested by the <em>Power Iteration Lemma</em>. That is, we compute <span class="math notranslate nohighlight">\(B^{k} \mathbf{x}\)</span>, then normalize it. To obtain the corresponding singular value and left singular vector, we use that <span class="math notranslate nohighlight">\(\sigma_1 = \|A \mathbf{v}_1\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">topsing</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">B</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">v</span> <span class="o">/</span> <span class="n">s</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We will apply it to our previous two-cluster example. The necessary functions are in <a class="reference external" href="https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py">mmids.py</a>, which is available on the <a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main">GitHub of the book</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png" src="../Images/e66fa4144461a2ee2155a730c0c7df77.png" data-original-src="https://mmids-textbook.github.io/_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png"/>
</div>
</div>
<p>Let’s compute the top singular vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">topsing</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.99257882  0.10164805  0.01581003  0.03202184  0.02075852  0.02798115
 -0.02920916 -0.028189   -0.0166094  -0.00648726]
</pre></div>
</div>
</div>
</div>
<p>This is approximately <span class="math notranslate nohighlight">\(-\mathbf{e}_1\)</span>. We get roughly the same answer (possibly up to sign) from Python’s <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.svd</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vh</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vh</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[ 0.99257882  0.10164803  0.01581003  0.03202184  0.02075851  0.02798112
 -0.02920917 -0.028189   -0.01660938 -0.00648724]
</pre></div>
</div>
</div>
</div>
<p>Recall that, when we applied <span class="math notranslate nohighlight">\(k\)</span>-means clustering to this example with <span class="math notranslate nohighlight">\(d=1000\)</span> dimension, we obtained a very poor clustering.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>99423.42794703908
99423.42794703908
99423.42794703908
99423.42794703908
99423.42794703908
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png" src="../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png" data-original-src="https://mmids-textbook.github.io/_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png"/>
</div>
</div>
<p>Let’s try again, but after projecting on the top singular vector. Recall that this corresponds to finding the best one-dimensional approximating subspace. The projection can be computed using the truncated SVD <span class="math notranslate nohighlight">\(Z= U_{(1)} \Sigma_{(1)} V_{(1)}^T\)</span>. We can interpret the rows of <span class="math notranslate nohighlight">\(U_{(1)} \Sigma_{(1)}\)</span> as the coefficients of each data point in the basis <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. We will work in that basis. We need one small hack: because our implementation of <span class="math notranslate nohighlight">\(k\)</span>-means clustering expects data points in at least <span class="math notranslate nohighlight">\(2\)</span> dimension, we add a column of <span class="math notranslate nohighlight">\(0\)</span>’s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">topsing</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">Xproj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">u</span><span class="o">*</span><span class="n">s</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xproj</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xproj</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png" src="../Images/d0d37edca421b5badf55590966b6d81a.png" data-original-src="https://mmids-textbook.github.io/_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png"/>
</div>
</div>
<p>There is a small – yet noticeable – gap around 0. We run <span class="math notranslate nohighlight">\(k\)</span>-means clustering on the projected data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">Xproj</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1779.020119584778
514.1899426112672
514.1899426112672
514.1899426112672
514.1899426112672
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png" src="../Images/5ffbd56fc722edd92c57dc038954354a.png" data-original-src="https://mmids-textbook.github.io/_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png"/>
</div>
</div>
<p>Much better. We give a more formal explanation of this outcome in a subsequent section. In essence, quoting [BHK, Section 7.5.1]:</p>
<blockquote>
<div><p>[…] let’s understand the central advantage of doing the projection to [the top <span class="math notranslate nohighlight">\(k\)</span> right singular vectors]. It is simply that for any reasonable (unknown) clustering of data points, the projection brings data points closer to their cluster centers.</p>
</div></blockquote>
<p>Finally, looking at the top right singular vector (or its first ten entries for lack of space), we see that it does align quite well (but not perfectly) with the first dimension.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-0.55564563 -0.02433674  0.02193487 -0.0333936  -0.00445505 -0.00243003
  0.02576056  0.02523275 -0.00682153  0.02524646]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> There are other methods to compute the SVD. Ask your favorite AI chatbot about randomized algorithms for the SVD. What are their advantages in terms of computational efficiency for large matrices? <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In the power iteration lemma for the positive semidefinite case, what happens when the initial vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> satisfies <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &lt; 0\)</span>?</p>
<p>a) The iteration converges to <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span>.</p>
<p>b) The iteration converges to <span class="math notranslate nohighlight">\(-\mathbf{q}_1\)</span>.</p>
<p>c) The iteration does not converge.</p>
<p>d) The iteration converges to a random eigenvector.</p>
<p><strong>2</strong> In the power iteration lemma for the SVD case, what is the convergence result for a random vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>.</p>
<p>b) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> or <span class="math notranslate nohighlight">\(-\mathbf{v}_1\)</span>.</p>
<p>c) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\sigma_1\)</span>.</p>
<p>d) <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> does not converge.</p>
<p><strong>3</strong> Suppose you apply the power iteration method to a matrix <span class="math notranslate nohighlight">\(A\)</span> and obtain a vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>. How can you compute the corresponding singular value <span class="math notranslate nohighlight">\(\sigma\)</span> and left singular vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\sigma = \|A\mathbf{v}\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = A\mathbf{v}/\sigma\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\sigma = \|A^T\mathbf{v}\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = A^T\mathbf{v}/\sigma\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\sigma = \|\mathbf{v}\|\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = \mathbf{v}/\sigma\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\sigma = 1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u} = A\mathbf{v}\)</span></p>
<p><strong>4</strong> What is required for the initial vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in the power iteration method to ensure convergence to the top eigenvector?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> must be a zero vector.</p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> must be orthogonal to the top eigenvector.</p>
<p>c) <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\)</span>.</p>
<p>d) <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> must be the top eigenvector itself.</p>
<p><strong>5</strong> What does the truncated SVD <span class="math notranslate nohighlight">\(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\)</span> correspond to? [Uses Section 4.8.2.1.]</p>
<p>a) The best one-dimensional approximating subspace</p>
<p>b) The best two-dimensional approximating subspace</p>
<p>c) The projection of the data onto the top singular vector</p>
<p>d) The projection of the data onto the top two singular vectors</p>
<p>Answer for 1: b. Justification: The lemma states that if <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit of <span class="math notranslate nohighlight">\(A^k \mathbf{x} / \|A^k \mathbf{x}\|\)</span> is <span class="math notranslate nohighlight">\(-\mathbf{q}_1\)</span>.</p>
<p>Answer for 2: b. Justification: The lemma states that if <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(B^k \mathbf{x} / \|B^k \mathbf{x}\|\)</span> converges to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>, and if <span class="math notranslate nohighlight">\(\langle \mathbf{v}_1, \mathbf{x} \rangle &lt; 0\)</span>, then the limit is <span class="math notranslate nohighlight">\(-\mathbf{v}_1\)</span>.</p>
<p>Answer for 3: a. Justification: The text provides these formulas in the “Numerical Corner” section.</p>
<p>Answer for 4: c. Justification: The key lemma states that convergence is ensured if <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is such that <span class="math notranslate nohighlight">\(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\)</span>.</p>
<p>Answer for 5: d. Justification: The text states that “projecting on the top two singular vectors… corresponds to finding the best two-dimensional approximating subspace. The projection can be computed using the truncated SVD <span class="math notranslate nohighlight">\(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\)</span>.”</p>
    
</body>
</html>