<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.5. Building blocks of AI 3: neural networks#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.5. Building blocks of AI 3: neural networks#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap08_nn/05_nn/roch-mmids-nn-nn.html">https://mmids-textbook.github.io/chap08_nn/05_nn/roch-mmids-nn-nn.html</a></blockquote>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Today's paper shows that it is possible to implement John Von Neumann's claim: "With 4 parameters I can ﬁt an elephant, and with 5 I can make him wiggle his trunk"<br/><br/>Paper here: <a href="https://t.co/SvVrLuRFNy">https://t.co/SvVrLuRFNy</a> <a href="https://t.co/VG37439vE7">pic.twitter.com/VG37439vE7</a></p>— Fermat's Library (@fermatslibrary) <a href="https://twitter.com/fermatslibrary/status/965971333422120962?ref_src=twsrc%5Etfw">February 20, 2018</a></blockquote> <p>In this section, we introduce neural networks. Unlike the previous examples we encountered, this one is not convex. Based on the theory we developed in Chapter 3, finding a local minimizer is the best we can hope for in general from descent methods. Yet, in many application settings, stochastic gradient descent (and some variants) have proven very effective at computing a good model to fit the data. Why that is remains an open question.</p>
<p>We describe the basic setup and apply it to classification on the Fashion-MNIST dataset. As we will see, we will get an improvement over multinomial logistic regression (with some help from a different optimization method). We use a particular architecture referred as a multilayer perceptron (MLP)<span class="math notranslate nohighlight">\(\idx{multilayer perceptron}\xdi\)</span>. These are a special class of progressive functions.</p>
<section id="multilayer-perceptron">
<h2><span class="section-number">8.5.1. </span>Multilayer perceptron<a class="headerlink" href="#multilayer-perceptron" title="Link to this heading">#</a></h2>
<p>Each of the main layers of a feedforward neural network<span class="math notranslate nohighlight">\(\idx{neural network}\xdi\)</span> has two components, an affine map and a nonlinear activation function<span class="math notranslate nohighlight">\(\idx{activation function}\xdi\)</span>. For the latter, we restrict ourselves here to the sigmoid function<span class="math notranslate nohighlight">\(\idx{sigmoid}\xdi\)</span> (although there are many <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">other choices of activation functions</a>).</p>
<p>The Jacobian of the elementwise version of the sigmoid function (which we will need later on)</p>
<div class="math notranslate nohighlight">
\[
\bsigma(\mathbf{t})
= (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n}(\mathbf{t}))
:= (\sigma(t_1),\ldots,\sigma(t_{n})),
\]</div>
<p>as a function of several variables can be computed from <span class="math notranslate nohighlight">\(\sigma'\)</span>, i.e., the derivative of the single-variable case. Indeed, we have seen in a previous example that <span class="math notranslate nohighlight">\(J_{\bsigma}(\mathbf{t})\)</span> is the diagonal matrix with diagonal entries</p>
<div class="math notranslate nohighlight">
\[
\sigma'(t_j) = \frac{e^{-t_j}}{(1 + e^{-t_j})^2}
= \sigma(t_j) (1 - \sigma(t_j)), \qquad j=1, \ldots, n,
\]</div>
<p>which we denote</p>
<div class="math notranslate nohighlight">
\[
J_{\bsigma}(\mathbf{t})
= \mathrm{diag}(\bsigma'(\mathbf{t}))
= \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1} - \bsigma(\mathbf{t}))),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bsigma'(\mathbf{t}) = (\sigma'(t_1), \ldots,\sigma'(t_{n}))\)</span> and <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is the all-one vector.</p>
<p>We consider an arbitrary number of layers<span class="math notranslate nohighlight">\(\idx{layer}\xdi\)</span> <span class="math notranslate nohighlight">\(L+2\)</span>. As a special case of progressive functions<span class="math notranslate nohighlight">\(\idx{progressive function}\xdi\)</span>, hidden layer<span class="math notranslate nohighlight">\(\idx{hidden layer}\xdi\)</span> <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,L\)</span>, is defined by a continuously differentiable function <span class="math notranslate nohighlight">\(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)</span> which takes two vector-valued inputs: a vector <span class="math notranslate nohighlight">\(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)</span> fed from the <span class="math notranslate nohighlight">\((i-1)\)</span>-st layer and a vector <span class="math notranslate nohighlight">\(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)</span> of parameters specific to the <span class="math notranslate nohighlight">\(i\)</span>-th layer</p>
<div class="math notranslate nohighlight">
\[
\bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}} \to \mathbb{R}^{n_{i}}.
\]</div>
<p>The output <span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span> of <span class="math notranslate nohighlight">\(\bfg_{i-1}\)</span> is a vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{n_{i}}\)</span> which is passed to the <span class="math notranslate nohighlight">\((i+1)\)</span>-st layer as input. Each component of <span class="math notranslate nohighlight">\(\bfg_{i-1}\)</span> is referred to as a neuron. Here <span class="math notranslate nohighlight">\(r_{i-1} = n_{i} n_{i-1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_{i-1} = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\)</span> are the parameters with <span class="math notranslate nohighlight">\(\mathbf{w}^{(k)}_{i-1} \in \mathbb{R}^{n_{i-1}}\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>. Specifically, <span class="math notranslate nohighlight">\(\bfg_{i-1}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1})
= \bsigma\left(\mathcal{W}_{i-1} \mathbf{z}_{i-1}\right)
= \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j} z_{i-1,j}\right),
\ldots,
\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j} z_{i-1,j}\right)\right)
\]</div>
<p>where we define <span class="math notranslate nohighlight">\(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\)</span> as the matrix with rows <span class="math notranslate nohighlight">\((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\)</span>. As we have done previously, to keep the notation simple, we ignore the constant term (or “bias variable”<span class="math notranslate nohighlight">\(\idx{bias variable}\xdi\)</span>) in the linear combinations of <span class="math notranslate nohighlight">\(\mathbf{z}_{i-1}\)</span>. It can be incorporated by adding a <span class="math notranslate nohighlight">\(1\)</span> input to each neuron as illustrated below. We will not detail this complication here.</p>
<p><strong>Figure:</strong> Each component of <span class="math notranslate nohighlight">\(g_i\)</span> is referred to as a neuron (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png">Source</a>)</p>
<p><img alt="Neuron" src="../Images/228200787298b2f894943501d2a42428.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/800px-ArtificialNeuronModel_english.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>The input layer<span class="math notranslate nohighlight">\(\idx{input layer}\xdi\)</span> is <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span>, which we refer to as layer <span class="math notranslate nohighlight">\(0\)</span>, so that <span class="math notranslate nohighlight">\(n_0 = d\)</span>.</p>
<p>Similarly to multinomial logistic regression, layer <span class="math notranslate nohighlight">\(L+1\)</span> (i.e., the output layer<span class="math notranslate nohighlight">\(\idx{output layer}\xdi\)</span>) is the softmax function</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L}) 
= \bgamma(\mathcal{W}_L \mathbf{z}_{L}),
\]</div>
<p>but this time we compose it with a linear transformation. We implicitly assume that <span class="math notranslate nohighlight">\(\bgamma\)</span> has <span class="math notranslate nohighlight">\(K\)</span> outputs and, in particular, we have that <span class="math notranslate nohighlight">\(n_{L+1} = K\)</span>.</p>
<p>So the output of the classifier with parameters <span class="math notranslate nohighlight">\(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\)</span> on input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\bfh(\mathbf{w})
= \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1), \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}).
\]</div>
<p>What makes this an MLP is that, on each layer, all outputs feed into the input of the next layer. In graphical terms, the edges between consecutive layers form a complete bipartite graph.</p>
<p><img alt="Feedforward neural network (with help from Claude; inspired by (Source))" src="../Images/271cf3797d7e5cd7669d7f17ae49618b.png" data-original-src="https://mmids-textbook.github.io/_images/feedforward.png"/></p>
<p>We again use cross-entropy<span class="math notranslate nohighlight">\(\idx{cross-entropy}\xdi\)</span> as the loss function (although there are many <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#loss-functions">other choices of loss functions</a>). That is, we set</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}})
= H(\mathbf{y}, \hat{\mathbf{y}})
= - \sum_{i=1}^K y_i \log \hat{y}_i.
\]</div>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w})
= \ell(\mathbf{h}(\mathbf{w})).
\]</div>
</section>
<section id="a-first-example">
<h2><span class="section-number">8.5.2. </span>A first example<a class="headerlink" href="#a-first-example" title="Link to this heading">#</a></h2>
<p>Before detailing a general algorithm for computing the gradient, we adapt our first progressive example to this setting to illustrate the main ideas. Suppose <span class="math notranslate nohighlight">\(d=3\)</span>, <span class="math notranslate nohighlight">\(L=1\)</span>, <span class="math notranslate nohighlight">\(n_1 = n_2 = 2\)</span>, and <span class="math notranslate nohighlight">\(K = 2\)</span>, that is, we have one hidden layer and our output is 2-dimensional. Fix a data sample <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\)</span>. For <span class="math notranslate nohighlight">\(i=0, 1\)</span>, we use the notation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{W}_{0}
=
\begin{pmatrix}
w_0 &amp; w_1 &amp; w_2\\
w_3 &amp; w_4 &amp; w_5
\end{pmatrix}
\quad
\text{and}
\quad
\mathcal{W}_{1}
=
\begin{pmatrix}
w_6 &amp; w_7\\
w_8 &amp; w_9
\end{pmatrix}
\end{split}\]</div>
<p>and let</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}})
= H(\mathbf{y}, \hat{\mathbf{y}})
= - y_1 \log \hat{y}_1 - y_2 \log \hat{y}_2.
\]</div>
<p>The layer functions are as follows</p>
<div class="math notranslate nohighlight">
\[
\bfg_0(\mathbf{z}_0, \mathbf{w}_0)
= \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
\quad\text{with}\quad
\mathbf{w}_0
= (w_0, w_1, w_2, w_3, w_4, w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\bfg_1(\mathbf{z}_1, \mathbf{w}_1)
= \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})
\quad\text{with}\quad
\mathbf{w}_1
= (w_6, w_7, w_8, w_9).
\]</div>
<p>We seek to compute the gradient of</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\mathbf{w})
&amp;= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
&amp;= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x}))\\
&amp;= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ 
&amp; \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Z
&amp;= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ 
&amp; \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)).
\end{align*}\]</div>
<p>In the forward phase, we compute <span class="math notranslate nohighlight">\(f\)</span> itself and the requisite Jacobians:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\mathbf{z}_0 := \mathbf{x}\\ 
&amp; = (x_1, x_2, x_3)\\
&amp;\mathbf{z}_1 := \bfg_0(\mathbf{z}_0, \mathbf{w}_0)
= \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
&amp;= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4 x_2 + w_5 x_3) \end{pmatrix}\\ 
&amp;J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := 
J_{\bsigma}(\mathcal{W}_{0} \mathbf{z}_{0})
\begin{pmatrix}
\mathbb{A}_{2}[\mathbf{w}_0] &amp;
\mathbb{B}_{2}[\mathbf{z}_0]
\end{pmatrix}\\
&amp;= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
\begin{pmatrix}
\mathcal{W}_{0} &amp;
I_{2\times 2} \otimes \mathbf{z}_0^T
\end{pmatrix}\\
&amp;= \begin{pmatrix}
\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} &amp;
\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \otimes \mathbf{z}_0^T
\end{pmatrix},
\end{align*}\]</div>
<p>where we used the <em>Chain Rule</em> to compute the Jacobian of <span class="math notranslate nohighlight">\(J_{\bfg_0}\)</span>.</p>
<p>Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
= \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\
&amp;= \begin{pmatrix}
Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\
Z^{-1} \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))
\end{pmatrix}\\
&amp;J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):=
J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
\begin{pmatrix}
\mathbb{A}_{2}[\mathbf{w}_1] &amp;
\mathbb{B}_{2}[\mathbf{z}_1]
\end{pmatrix}\\
&amp;= 
[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
\begin{pmatrix}
\mathcal{W}_{1} &amp;
I_{2\times 2} \otimes \mathbf{z}_1^T
\end{pmatrix}\\
&amp;= 
\begin{pmatrix}
[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1} &amp;
[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T
\end{pmatrix},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> was introduced previously and we used the expression for <span class="math notranslate nohighlight">\(J_{\bgamma}\)</span> from the previous subsection.</p>
<p>Finally</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) 
= H(\mathbf{y}, \hat{\mathbf{y}})\\
&amp;= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ 
&amp; \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\
&amp;J_{\ell}(\hat{\mathbf{y}})
= - (\mathbf{y} \oslash \hat{\mathbf{y}})^T.
\end{align*}\]</div>
<p>We first compute the partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\)</span>. For this step, we think of <span class="math notranslate nohighlight">\(f\)</span> as the composition of <span class="math notranslate nohighlight">\(\ell(\mathbf{z}_2)\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{z}_2\)</span> and <span class="math notranslate nohighlight">\(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \bgamma(\mathcal{W}_1 \mathbf{z}_1)\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span>. Here <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> does not depend on <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> and therefore can be considered fixed for this calculation. By the <em>Chain Rule</em> and the <em>Properties of the Kronecker Product (f)</em>, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_7} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_8} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_9}
\end{pmatrix}\\
&amp;= - (\mathbf{y} \oslash \hat{\mathbf{y}})^T \left\{
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \right\}\\
&amp;= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}))^T
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\
&amp;= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\otimes \mathbf{z}_1^T\\
&amp;= (\hat{\mathbf{y}} - \mathbf{y})^T
\otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
\end{align*}\]</div>
<p>where we used the <em>Properties of the Hadamard Product</em> and the fact that <span class="math notranslate nohighlight">\(\hat{\mathbf{y}} = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\)</span> similarly to a calculation we did in the multinomial logistic regression setting.</p>
<p>To compute the partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\)</span>, we first need to compute partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{z}_1 = (z_{1,1}, z_{1,2})\)</span> since <span class="math notranslate nohighlight">\(f\)</span> depends on <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span> through it. For this calculation, we think again of <span class="math notranslate nohighlight">\(f\)</span> as the composition <span class="math notranslate nohighlight">\(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\)</span>, but this time our focus is on the variables <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span>. This is almost identical to the previous calculation, except that we use the block of <span class="math notranslate nohighlight">\(J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1)\)</span> corresponding to the partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> (i.e., the “<span class="math notranslate nohighlight">\(A\)</span>” block). We obtain</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}} &amp;
\frac{\partial f(\mathbf{w})}{\partial z_{1,2}}
\end{pmatrix}\\
&amp;= - (\mathbf{y} \oslash \hat{\mathbf{y}})^T \left\{
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1} \right\}\\
&amp;= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T 
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\
&amp;= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\mathcal{W}_{1}
\end{align*}\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}},
\frac{\partial f(\mathbf{w})}{\partial z_{1,2}}\right)\)</span> is called an adjoint.</p>
<p>We now compute the gradient of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\)</span>. For this step, we think of <span class="math notranslate nohighlight">\(f\)</span> as the composition of <span class="math notranslate nohighlight">\(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> and <span class="math notranslate nohighlight">\(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span>. Here <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}_0\)</span> do not depend on <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span> and therefore can be considered fixed for this calculation. By the <em>Chain Rule</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_1} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_2} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_3} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_4} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_5}
\end{pmatrix}\\
&amp;= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\mathcal{W}_{1}
\left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \otimes \mathbf{z}_0^T\right]\\
&amp;= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes \mathbf{z}_0^T\\
&amp;= [(\hat{\mathbf{y}} - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{x}))] \otimes \mathbf{x}^T\\
&amp;= [(\hat{\mathbf{y}} - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
\end{align*}\]</div>
<p>where we used <em>Properties of the Kronecker Product (f)</em> on the second to last line and our expression for the derivative of the sigmoid function on the last line.</p>
<p>To sum up,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(\mathbf{w})^T
&amp;= \begin{pmatrix}
[(\hat{\mathbf{y}} - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T &amp;
(\hat{\mathbf{y}} - \mathbf{y})^T
\otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
\end{pmatrix}
\end{align*}\]</div>
<p><strong>NUMERICAL CORNER:</strong> We return to the concrete example from the previous section. We re-write the gradient as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(\mathbf{w})^T
&amp;= \begin{pmatrix}
[(\mathbf{z}_2 - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))] \otimes \mathbf{z}_0^T &amp;
(\mathbf{z}_2 - \mathbf{y})^T
\otimes \mathbf{z}_1^T
\end{pmatrix}.
\end{align*}\]</div>
<p>We will use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.sigmoid</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.softmax</span></code></a> for the sigmoid and softmax functions respectively. We also use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.dot.html"><code class="docutils literal notranslate"><span class="pre">torch.dot</span></code></a> for the inner product (i.e., dot product) of two vectors (as tensors) and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diag.html"><code class="docutils literal notranslate"><span class="pre">torch.diag</span></code></a> for the creation of a diagonal matrix with specified entries on its diagonal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">W0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">z0</span> <span class="o">=</span> <span class="n">x</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">W0</span> <span class="o">@</span> <span class="n">z0</span><span class="p">)</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">W1</span> <span class="o">@</span> <span class="n">z1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">z2</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">z0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([ 1.,  0., -1.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([0.7311, 0.7311], grad_fn=&lt;SigmoidBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([0.1881, 0.8119], grad_fn=&lt;SoftmaxBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor(0.2084, grad_fn=&lt;NegBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>We compute the gradient <span class="math notranslate nohighlight">\(\nabla f(\mathbf{w})\)</span> using AD.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W0</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[-0.1110, -0.0000,  0.1110],
        [ 0.0370,  0.0000, -0.0370]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[ 0.1375,  0.1375],
        [-0.1375, -0.1375]])
</pre></div>
</div>
</div>
</div>
<p>We use our formulas to confirm that they match these results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">grad_W0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">kron</span><span class="p">((</span><span class="n">z2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">z1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">z1</span><span class="p">)),</span> <span class="n">z0</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">grad_W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">kron</span><span class="p">((</span><span class="n">z2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">z1</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">grad_W0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[-0.1110, -0.0000,  0.1110,  0.0370,  0.0000, -0.0370]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[ 0.1375,  0.1375, -0.1375, -0.1375]])
</pre></div>
</div>
</div>
</div>
<p>The results match with the AD output.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="computing-the-gradient">
<h2><span class="section-number">8.5.3. </span>Computing the gradient<a class="headerlink" href="#computing-the-gradient" title="Link to this heading">#</a></h2>
<p>We now detail how to compute the gradient of <span class="math notranslate nohighlight">\(f(\mathbf{w})\)</span> for a general MLP. In the forward loop, we first set <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span> and then we compute for <span class="math notranslate nohighlight">\(i = 0,1,\ldots,L-1\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{i+1} 
&amp;:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i)
= \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right)\\
\begin{pmatrix}
A_i &amp; B_i
\end{pmatrix}
&amp;:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
\end{align*}\]</div>
<p>To compute the Jacobian of <span class="math notranslate nohighlight">\(\bfg_i\)</span>, we use the <em>Chain Rule</em> on the composition <span class="math notranslate nohighlight">\(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)</span>
where we define <span class="math notranslate nohighlight">\(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\)</span>. That is,</p>
<div class="math notranslate nohighlight">
\[
J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
= J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i).
\]</div>
<p>In our analysis of multinomial logistic regression, we computed the Jacobian of <span class="math notranslate nohighlight">\(\bfk_i\)</span>. We obtained</p>
<div class="math notranslate nohighlight">
\[
J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)
= \begin{pmatrix}
\mathbb{A}_{n_{i+1}}[\mathbf{w}_i] &amp; \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
\end{pmatrix}.
\]</div>
<p>Recall that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
=
\mathcal{W}_i.
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathbb{B}_{n_{i+1}}[\mathbf{z}_i] 
= \begin{pmatrix}
\mathbf{e}_1 \mathbf{z}_i^T
&amp; \cdots &amp; \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T
\end{pmatrix}
= I_{n_{i+1}\times n_{i+1}} \otimes \mathbf{z}_i^T,
\]</div>
<p>where here <span class="math notranslate nohighlight">\(\mathbf{e}_{j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th standard basis vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{n_{i+1}}\)</span>.</p>
<p>From a previous calculation, the Jacobian of</p>
<div class="math notranslate nohighlight">
\[
\bsigma(\mathbf{t})
= (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
:= (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})),
\]</div>
<p>is</p>
<div class="math notranslate nohighlight">
\[
J_{\bsigma}(\mathbf{t})
= \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1} - \bsigma(\mathbf{t}))).
\]</div>
<p>Combining the previous formulas, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
&amp;= J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\
&amp;= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \begin{pmatrix}
\mathbb{A}_{n_{i+1}}[\mathbf{w}_i] &amp; \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
\end{pmatrix}\\
&amp;=\begin{pmatrix}
\widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &amp; \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
\end{pmatrix}
\end{align*}\]</div>
<p>where we define</p>
<div class="math notranslate nohighlight">
\[
\widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \mathcal{W}_i,
\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] 
&amp;= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
\left(I_{n_{i+1}\times n_{i+1}} \otimes \mathbf{z}_i^T\right)\\
&amp;= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \otimes \mathbf{z}_i^T,
\end{align*}\]</div>
<p>where we used the <em>Properties of the Kronecker Product (f)</em>.</p>
<p>For layer <span class="math notranslate nohighlight">\(L+1\)</span> (i.e., the output layer), we have previously computed the Jacobian of the softmax function composed with a linear transformation. We get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\mathbf{z}_{L+1}
:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
&amp;= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\
&amp;\begin{pmatrix} A_{L} &amp; B_{L} \end{pmatrix}
:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
&amp;= 
\begin{pmatrix}
[\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \mathcal{W}_{L} &amp;
[\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T
\end{pmatrix}\\
&amp;=: \begin{pmatrix}
\widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&amp;
\widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]
\end{pmatrix}
\end{align*}\]</div>
<p>Also, as in the multinomial logistic regression case, the loss and gradient of the loss are</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{L+2}
&amp;:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K y_i \log z_{L+1,i}\\
\mathbf{q}_{L+1}
&amp;:= \nabla \ell(\mathbf{z}_{L+1})
= \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
\end{align*}\]</div>
<p><em>Initialization:</em> <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span></p>
<p><em>Forward loop:</em> For <span class="math notranslate nohighlight">\(i = 0,1,\ldots,L-1\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{i+1} &amp;:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i \mathbf{z}_i \right)\\
\begin{pmatrix}
A_i &amp; B_i
\end{pmatrix}
&amp;:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
=\begin{pmatrix}
\widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &amp; \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
\end{pmatrix}
\end{align*}\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{L+1}
&amp;:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\
\begin{pmatrix} A_{L} &amp; B_{L} \end{pmatrix}
&amp;:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})
= \begin{pmatrix}
\widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&amp;
\widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]
\end{pmatrix}.
\end{align*}\]</div>
<p><em>Loss:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{L+2}
&amp;:= \ell(\mathbf{z}_{L+1})\\
\mathbf{p}_{L+1}
&amp;:= \nabla {\ell}(\mathbf{z}_{L+1})
= \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
\end{align*}\]</div>
<p><em>Backward loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1}
&amp;= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T \mathbf{p}_{L+1}\\
\mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1}
&amp;= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T \mathbf{p}_{L+1}
\end{align*}\]</div>
<p>and for <span class="math notranslate nohighlight">\(i = L-1,L-2,\ldots,1,0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_{i} 
&amp;:= A_i^T \mathbf{p}_{i+1}
= \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T \mathbf{p}_{i+1}\\
\mathbf{q}_{i} 
&amp;:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T \mathbf{p}_{i+1}
\end{align*}\]</div>
<p><em>Output:</em></p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
\]</div>
<!--ONLINE ONLY

**CHAT & LEARN** Via ChatGPT, in the words of Yoda (in case that helps you), the overall algorithm can be summarized as follows:

> Backward, the path of learning must go. Begin at the end, we do. At the output layer of the network, error we find. How much wrong the answers are, it shows. Calculate we must, the difference between what expected we were and what received we have.
> 
> Propagate this error back, we shall, through each layer preceding. Update each weight we must, according to its contribution to the error. Small adjustments to the weights, guided by the Force of the gradient, and the learning rate, it is.
>
> Learn from mistakes, the network does. Adjust it must, until right, the outputs are. Repeat this process we will, layer by layer, neuron by neuron, until learned enough, the network has. Train it does, iterate it must, learn it shall.
>
> This, the way of backpropagation is. Simple, yet profound.

$\ddagger$
--><p><strong>NUMERICAL CORNER:</strong> We implement the training of a neural network in PyTorch. We use the Fashion-MNIST dataset again. We first load it again. We also check for the availability of GPUs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                      <span class="k">else</span> <span class="p">(</span><span class="s1">'mps'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                            <span class="k">else</span> <span class="s1">'cpu'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Using device:'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Using device: mps
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">:</span> <span class="c1"># device-specific seeding and settings</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for multi-GPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">elif</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'mps'</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># MPS-specific seeding</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">g</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                               <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We construct a two-layer model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>                      <span class="c1"># Flatten the input</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>            <span class="c1"># First Linear layer with 32 nodes</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>                      <span class="c1"># Sigmoid activation function</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                  <span class="c1"># Second Linear layer with 10 nodes (output layer)</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As we did for multinomial logistic regression, we use the SGD optimizer and the cross-entropy loss (which in PyTorch includes the softmax function and expects labels to be actual class labels rather than one-hot encoding).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We train for 10 epochs.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>On the test data, we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 64.0% accuracy
</pre></div>
</div>
</div>
</div>
<p>Disappointingly, this is significantly less accurate than what we obtained using multinomial logistic regression. It turns out that using a different optimizer gives much better results.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 87.1% accuracy
</pre></div>
</div>
</div>
</div>
<p><strong>CHAT &amp; LEARN</strong> We mentioned that there are many optimizers available in PyTorch besides SGD and Adam. Ask your favorite AI chatbot to explain and implement a different optimizer, such as Adagrad or RMSprop, for the MLP. Compare the results with those obtained using SGD and Adam. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Open In Colab</a>) <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Regularization techniques are often used to prevent overfitting in neural networks. Ask your favorite AI chatbot about <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> regularization, dropout, and early stopping. Discuss how these techniques can be incorporated into the training process and their effects on the learned model. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> What is the role of the sigmoid function in a multilayer perceptron (MLP)?</p>
<p>a) It is used as the loss function for training the MLP.</p>
<p>b) It is used as the nonlinear activation function in each layer of the MLP.</p>
<p>c) It is used to compute the gradient of the loss function with respect to the weights.</p>
<p>d) It is used to initialize the weights of the MLP.</p>
<p><strong>2</strong> In an MLP, what is the purpose of the softmax function in the output layer?</p>
<p>a) To introduce nonlinearity into the model.</p>
<p>b) To normalize the outputs into a probability distribution.</p>
<p>c) To compute the gradient of the loss function.</p>
<p>d) To reduce the dimensionality of the output.</p>
<p><strong>3</strong> What is the Jacobian matrix of the elementwise sigmoid function <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}(\mathbf{t}) = (\sigma(t_1), \dots, \sigma(t_n))\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot (1-\boldsymbol{\sigma}(\mathbf{t}))\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}) \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1 - \boldsymbol{\sigma}(\mathbf{t}))^T\)</span></p>
<p><strong>4</strong> In the forward phase of computing the gradient of the loss function in an MLP, what is the output of the <span class="math notranslate nohighlight">\(i\)</span>-th hidden layer?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i \mathbf{z}_i)\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i \mathbf{z}_i)\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i \boldsymbol{\sigma}(\mathbf{z}_i)\)</span></p>
<p><strong>5</strong> What is the output of the backward loop in computing the gradient of the loss function in an MLP?</p>
<p>a) The gradient of the loss function with respect to the activations of each layer.</p>
<p>b) The gradient of the loss function with respect to the weights of each layer.</p>
<p>c) The updated weights of the MLP.</p>
<p>d) The loss function value.</p>
<p>Answer for 1: b. Justification: The text states that “Each of the main layers of a feedforward neural network has two components, an affine map and a nonlinear activation function. For the latter, we restrict ourselves here to the sigmoid function.”</p>
<p>Answer for 2: b. Justification: The text states that the softmax function is used in the output layer to produce a probability distribution over the possible classes.</p>
<p>Answer for 3: c. Justification: The text states that the Jacobian of the elementwise sigmoid function is:</p>
<div class="math notranslate nohighlight">
\[
J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t) \odot (1 - \boldsymbol{\sigma}(t)))
\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> denotes the Hadamard (elementwise) product.</p>
<p>Answer for 4: a. Justification: The text defines the output of the <span class="math notranslate nohighlight">\(i\)</span>-th hidden layer as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i \mathbf{z}_i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}\)</span> is the sigmoid activation function and <span class="math notranslate nohighlight">\(\mathcal{W}_i\)</span> is the weight matrix for the <span class="math notranslate nohighlight">\(i\)</span>-th layer.</p>
<p>Answer for 5: b. Justification: The text states that the output of the backward loop is the gradient of the loss function with respect to the weights of each layer:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i, \mathbf{w}_i]^T \mathbf{p}_{i+1}\)</span> is the gradient with respect to the weights of the <span class="math notranslate nohighlight">\(i\)</span>-th layer.</p>
</section>
&#13;

<h2><span class="section-number">8.5.1. </span>Multilayer perceptron<a class="headerlink" href="#multilayer-perceptron" title="Link to this heading">#</a></h2>
<p>Each of the main layers of a feedforward neural network<span class="math notranslate nohighlight">\(\idx{neural network}\xdi\)</span> has two components, an affine map and a nonlinear activation function<span class="math notranslate nohighlight">\(\idx{activation function}\xdi\)</span>. For the latter, we restrict ourselves here to the sigmoid function<span class="math notranslate nohighlight">\(\idx{sigmoid}\xdi\)</span> (although there are many <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">other choices of activation functions</a>).</p>
<p>The Jacobian of the elementwise version of the sigmoid function (which we will need later on)</p>
<div class="math notranslate nohighlight">
\[
\bsigma(\mathbf{t})
= (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n}(\mathbf{t}))
:= (\sigma(t_1),\ldots,\sigma(t_{n})),
\]</div>
<p>as a function of several variables can be computed from <span class="math notranslate nohighlight">\(\sigma'\)</span>, i.e., the derivative of the single-variable case. Indeed, we have seen in a previous example that <span class="math notranslate nohighlight">\(J_{\bsigma}(\mathbf{t})\)</span> is the diagonal matrix with diagonal entries</p>
<div class="math notranslate nohighlight">
\[
\sigma'(t_j) = \frac{e^{-t_j}}{(1 + e^{-t_j})^2}
= \sigma(t_j) (1 - \sigma(t_j)), \qquad j=1, \ldots, n,
\]</div>
<p>which we denote</p>
<div class="math notranslate nohighlight">
\[
J_{\bsigma}(\mathbf{t})
= \mathrm{diag}(\bsigma'(\mathbf{t}))
= \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1} - \bsigma(\mathbf{t}))),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bsigma'(\mathbf{t}) = (\sigma'(t_1), \ldots,\sigma'(t_{n}))\)</span> and <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is the all-one vector.</p>
<p>We consider an arbitrary number of layers<span class="math notranslate nohighlight">\(\idx{layer}\xdi\)</span> <span class="math notranslate nohighlight">\(L+2\)</span>. As a special case of progressive functions<span class="math notranslate nohighlight">\(\idx{progressive function}\xdi\)</span>, hidden layer<span class="math notranslate nohighlight">\(\idx{hidden layer}\xdi\)</span> <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,L\)</span>, is defined by a continuously differentiable function <span class="math notranslate nohighlight">\(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)</span> which takes two vector-valued inputs: a vector <span class="math notranslate nohighlight">\(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)</span> fed from the <span class="math notranslate nohighlight">\((i-1)\)</span>-st layer and a vector <span class="math notranslate nohighlight">\(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)</span> of parameters specific to the <span class="math notranslate nohighlight">\(i\)</span>-th layer</p>
<div class="math notranslate nohighlight">
\[
\bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}} \to \mathbb{R}^{n_{i}}.
\]</div>
<p>The output <span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span> of <span class="math notranslate nohighlight">\(\bfg_{i-1}\)</span> is a vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{n_{i}}\)</span> which is passed to the <span class="math notranslate nohighlight">\((i+1)\)</span>-st layer as input. Each component of <span class="math notranslate nohighlight">\(\bfg_{i-1}\)</span> is referred to as a neuron. Here <span class="math notranslate nohighlight">\(r_{i-1} = n_{i} n_{i-1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_{i-1} = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\)</span> are the parameters with <span class="math notranslate nohighlight">\(\mathbf{w}^{(k)}_{i-1} \in \mathbb{R}^{n_{i-1}}\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>. Specifically, <span class="math notranslate nohighlight">\(\bfg_{i-1}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1})
= \bsigma\left(\mathcal{W}_{i-1} \mathbf{z}_{i-1}\right)
= \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j} z_{i-1,j}\right),
\ldots,
\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j} z_{i-1,j}\right)\right)
\]</div>
<p>where we define <span class="math notranslate nohighlight">\(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\)</span> as the matrix with rows <span class="math notranslate nohighlight">\((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\)</span>. As we have done previously, to keep the notation simple, we ignore the constant term (or “bias variable”<span class="math notranslate nohighlight">\(\idx{bias variable}\xdi\)</span>) in the linear combinations of <span class="math notranslate nohighlight">\(\mathbf{z}_{i-1}\)</span>. It can be incorporated by adding a <span class="math notranslate nohighlight">\(1\)</span> input to each neuron as illustrated below. We will not detail this complication here.</p>
<p><strong>Figure:</strong> Each component of <span class="math notranslate nohighlight">\(g_i\)</span> is referred to as a neuron (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png">Source</a>)</p>
<p><img alt="Neuron" src="../Images/228200787298b2f894943501d2a42428.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/800px-ArtificialNeuronModel_english.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>The input layer<span class="math notranslate nohighlight">\(\idx{input layer}\xdi\)</span> is <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span>, which we refer to as layer <span class="math notranslate nohighlight">\(0\)</span>, so that <span class="math notranslate nohighlight">\(n_0 = d\)</span>.</p>
<p>Similarly to multinomial logistic regression, layer <span class="math notranslate nohighlight">\(L+1\)</span> (i.e., the output layer<span class="math notranslate nohighlight">\(\idx{output layer}\xdi\)</span>) is the softmax function</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L}) 
= \bgamma(\mathcal{W}_L \mathbf{z}_{L}),
\]</div>
<p>but this time we compose it with a linear transformation. We implicitly assume that <span class="math notranslate nohighlight">\(\bgamma\)</span> has <span class="math notranslate nohighlight">\(K\)</span> outputs and, in particular, we have that <span class="math notranslate nohighlight">\(n_{L+1} = K\)</span>.</p>
<p>So the output of the classifier with parameters <span class="math notranslate nohighlight">\(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\)</span> on input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\bfh(\mathbf{w})
= \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1), \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}).
\]</div>
<p>What makes this an MLP is that, on each layer, all outputs feed into the input of the next layer. In graphical terms, the edges between consecutive layers form a complete bipartite graph.</p>
<p><img alt="Feedforward neural network (with help from Claude; inspired by (Source))" src="../Images/271cf3797d7e5cd7669d7f17ae49618b.png" data-original-src="https://mmids-textbook.github.io/_images/feedforward.png"/></p>
<p>We again use cross-entropy<span class="math notranslate nohighlight">\(\idx{cross-entropy}\xdi\)</span> as the loss function (although there are many <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#loss-functions">other choices of loss functions</a>). That is, we set</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}})
= H(\mathbf{y}, \hat{\mathbf{y}})
= - \sum_{i=1}^K y_i \log \hat{y}_i.
\]</div>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{w})
= \ell(\mathbf{h}(\mathbf{w})).
\]</div>
&#13;

<h2><span class="section-number">8.5.2. </span>A first example<a class="headerlink" href="#a-first-example" title="Link to this heading">#</a></h2>
<p>Before detailing a general algorithm for computing the gradient, we adapt our first progressive example to this setting to illustrate the main ideas. Suppose <span class="math notranslate nohighlight">\(d=3\)</span>, <span class="math notranslate nohighlight">\(L=1\)</span>, <span class="math notranslate nohighlight">\(n_1 = n_2 = 2\)</span>, and <span class="math notranslate nohighlight">\(K = 2\)</span>, that is, we have one hidden layer and our output is 2-dimensional. Fix a data sample <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\)</span>. For <span class="math notranslate nohighlight">\(i=0, 1\)</span>, we use the notation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{W}_{0}
=
\begin{pmatrix}
w_0 &amp; w_1 &amp; w_2\\
w_3 &amp; w_4 &amp; w_5
\end{pmatrix}
\quad
\text{and}
\quad
\mathcal{W}_{1}
=
\begin{pmatrix}
w_6 &amp; w_7\\
w_8 &amp; w_9
\end{pmatrix}
\end{split}\]</div>
<p>and let</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}})
= H(\mathbf{y}, \hat{\mathbf{y}})
= - y_1 \log \hat{y}_1 - y_2 \log \hat{y}_2.
\]</div>
<p>The layer functions are as follows</p>
<div class="math notranslate nohighlight">
\[
\bfg_0(\mathbf{z}_0, \mathbf{w}_0)
= \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
\quad\text{with}\quad
\mathbf{w}_0
= (w_0, w_1, w_2, w_3, w_4, w_5)
\]</div>
<div class="math notranslate nohighlight">
\[
\bfg_1(\mathbf{z}_1, \mathbf{w}_1)
= \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})
\quad\text{with}\quad
\mathbf{w}_1
= (w_6, w_7, w_8, w_9).
\]</div>
<p>We seek to compute the gradient of</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\mathbf{w})
&amp;= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
&amp;= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x}))\\
&amp;= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ 
&amp; \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]
\end{align*}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Z
&amp;= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ 
&amp; \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)).
\end{align*}\]</div>
<p>In the forward phase, we compute <span class="math notranslate nohighlight">\(f\)</span> itself and the requisite Jacobians:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\mathbf{z}_0 := \mathbf{x}\\ 
&amp; = (x_1, x_2, x_3)\\
&amp;\mathbf{z}_1 := \bfg_0(\mathbf{z}_0, \mathbf{w}_0)
= \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
&amp;= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4 x_2 + w_5 x_3) \end{pmatrix}\\ 
&amp;J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := 
J_{\bsigma}(\mathcal{W}_{0} \mathbf{z}_{0})
\begin{pmatrix}
\mathbb{A}_{2}[\mathbf{w}_0] &amp;
\mathbb{B}_{2}[\mathbf{z}_0]
\end{pmatrix}\\
&amp;= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
\begin{pmatrix}
\mathcal{W}_{0} &amp;
I_{2\times 2} \otimes \mathbf{z}_0^T
\end{pmatrix}\\
&amp;= \begin{pmatrix}
\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} &amp;
\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \otimes \mathbf{z}_0^T
\end{pmatrix},
\end{align*}\]</div>
<p>where we used the <em>Chain Rule</em> to compute the Jacobian of <span class="math notranslate nohighlight">\(J_{\bfg_0}\)</span>.</p>
<p>Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
= \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\
&amp;= \begin{pmatrix}
Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\
Z^{-1} \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))
\end{pmatrix}\\
&amp;J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):=
J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
\begin{pmatrix}
\mathbb{A}_{2}[\mathbf{w}_1] &amp;
\mathbb{B}_{2}[\mathbf{z}_1]
\end{pmatrix}\\
&amp;= 
[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
\begin{pmatrix}
\mathcal{W}_{1} &amp;
I_{2\times 2} \otimes \mathbf{z}_1^T
\end{pmatrix}\\
&amp;= 
\begin{pmatrix}
[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1} &amp;
[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T
\end{pmatrix},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> was introduced previously and we used the expression for <span class="math notranslate nohighlight">\(J_{\bgamma}\)</span> from the previous subsection.</p>
<p>Finally</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) 
= H(\mathbf{y}, \hat{\mathbf{y}})\\
&amp;= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ 
&amp; \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\
&amp;J_{\ell}(\hat{\mathbf{y}})
= - (\mathbf{y} \oslash \hat{\mathbf{y}})^T.
\end{align*}\]</div>
<p>We first compute the partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\)</span>. For this step, we think of <span class="math notranslate nohighlight">\(f\)</span> as the composition of <span class="math notranslate nohighlight">\(\ell(\mathbf{z}_2)\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{z}_2\)</span> and <span class="math notranslate nohighlight">\(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \bgamma(\mathcal{W}_1 \mathbf{z}_1)\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span>. Here <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> does not depend on <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> and therefore can be considered fixed for this calculation. By the <em>Chain Rule</em> and the <em>Properties of the Kronecker Product (f)</em>, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_7} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_8} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_9}
\end{pmatrix}\\
&amp;= - (\mathbf{y} \oslash \hat{\mathbf{y}})^T \left\{
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \right\}\\
&amp;= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}))^T
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\
&amp;= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\otimes \mathbf{z}_1^T\\
&amp;= (\hat{\mathbf{y}} - \mathbf{y})^T
\otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
\end{align*}\]</div>
<p>where we used the <em>Properties of the Hadamard Product</em> and the fact that <span class="math notranslate nohighlight">\(\hat{\mathbf{y}} = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\)</span> similarly to a calculation we did in the multinomial logistic regression setting.</p>
<p>To compute the partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\)</span>, we first need to compute partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{z}_1 = (z_{1,1}, z_{1,2})\)</span> since <span class="math notranslate nohighlight">\(f\)</span> depends on <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span> through it. For this calculation, we think again of <span class="math notranslate nohighlight">\(f\)</span> as the composition <span class="math notranslate nohighlight">\(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\)</span>, but this time our focus is on the variables <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span>. This is almost identical to the previous calculation, except that we use the block of <span class="math notranslate nohighlight">\(J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1)\)</span> corresponding to the partial derivatives with respect to <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> (i.e., the “<span class="math notranslate nohighlight">\(A\)</span>” block). We obtain</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}} &amp;
\frac{\partial f(\mathbf{w})}{\partial z_{1,2}}
\end{pmatrix}\\
&amp;= - (\mathbf{y} \oslash \hat{\mathbf{y}})^T \left\{
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1} \right\}\\
&amp;= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T 
\,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\
&amp;= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\mathcal{W}_{1}
\end{align*}\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}},
\frac{\partial f(\mathbf{w})}{\partial z_{1,2}}\right)\)</span> is called an adjoint.</p>
<p>We now compute the gradient of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\)</span>. For this step, we think of <span class="math notranslate nohighlight">\(f\)</span> as the composition of <span class="math notranslate nohighlight">\(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> and <span class="math notranslate nohighlight">\(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span>. Here <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}_0\)</span> do not depend on <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span> and therefore can be considered fixed for this calculation. By the <em>Chain Rule</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_1} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_2} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_3} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_4} &amp;
\frac{\partial f(\mathbf{w})}{\partial w_5}
\end{pmatrix}\\
&amp;= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\mathcal{W}_{1}
\left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \otimes \mathbf{z}_0^T\right]\\
&amp;= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes \mathbf{z}_0^T\\
&amp;= [(\hat{\mathbf{y}} - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{x}))] \otimes \mathbf{x}^T\\
&amp;= [(\hat{\mathbf{y}} - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
\end{align*}\]</div>
<p>where we used <em>Properties of the Kronecker Product (f)</em> on the second to last line and our expression for the derivative of the sigmoid function on the last line.</p>
<p>To sum up,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(\mathbf{w})^T
&amp;= \begin{pmatrix}
[(\hat{\mathbf{y}} - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T &amp;
(\hat{\mathbf{y}} - \mathbf{y})^T
\otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
\end{pmatrix}
\end{align*}\]</div>
<p><strong>NUMERICAL CORNER:</strong> We return to the concrete example from the previous section. We re-write the gradient as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(\mathbf{w})^T
&amp;= \begin{pmatrix}
[(\mathbf{z}_2 - \mathbf{y})^T
\mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))] \otimes \mathbf{z}_0^T &amp;
(\mathbf{z}_2 - \mathbf{y})^T
\otimes \mathbf{z}_1^T
\end{pmatrix}.
\end{align*}\]</div>
<p>We will use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.sigmoid</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.softmax</span></code></a> for the sigmoid and softmax functions respectively. We also use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.dot.html"><code class="docutils literal notranslate"><span class="pre">torch.dot</span></code></a> for the inner product (i.e., dot product) of two vectors (as tensors) and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diag.html"><code class="docutils literal notranslate"><span class="pre">torch.diag</span></code></a> for the creation of a diagonal matrix with specified entries on its diagonal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">W0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">z0</span> <span class="o">=</span> <span class="n">x</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">W0</span> <span class="o">@</span> <span class="n">z0</span><span class="p">)</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">W1</span> <span class="o">@</span> <span class="n">z1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">z2</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">z0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([ 1.,  0., -1.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([0.7311, 0.7311], grad_fn=&lt;SigmoidBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([0.1881, 0.8119], grad_fn=&lt;SoftmaxBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor(0.2084, grad_fn=&lt;NegBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>We compute the gradient <span class="math notranslate nohighlight">\(\nabla f(\mathbf{w})\)</span> using AD.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W0</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[-0.1110, -0.0000,  0.1110],
        [ 0.0370,  0.0000, -0.0370]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[ 0.1375,  0.1375],
        [-0.1375, -0.1375]])
</pre></div>
</div>
</div>
</div>
<p>We use our formulas to confirm that they match these results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">grad_W0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">kron</span><span class="p">((</span><span class="n">z2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">z1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">z1</span><span class="p">)),</span> <span class="n">z0</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">grad_W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">kron</span><span class="p">((</span><span class="n">z2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">z1</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">grad_W0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[-0.1110, -0.0000,  0.1110,  0.0370,  0.0000, -0.0370]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[ 0.1375,  0.1375, -0.1375, -0.1375]])
</pre></div>
</div>
</div>
</div>
<p>The results match with the AD output.</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
&#13;

<h2><span class="section-number">8.5.3. </span>Computing the gradient<a class="headerlink" href="#computing-the-gradient" title="Link to this heading">#</a></h2>
<p>We now detail how to compute the gradient of <span class="math notranslate nohighlight">\(f(\mathbf{w})\)</span> for a general MLP. In the forward loop, we first set <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span> and then we compute for <span class="math notranslate nohighlight">\(i = 0,1,\ldots,L-1\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{i+1} 
&amp;:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i)
= \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right)\\
\begin{pmatrix}
A_i &amp; B_i
\end{pmatrix}
&amp;:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
\end{align*}\]</div>
<p>To compute the Jacobian of <span class="math notranslate nohighlight">\(\bfg_i\)</span>, we use the <em>Chain Rule</em> on the composition <span class="math notranslate nohighlight">\(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)</span>
where we define <span class="math notranslate nohighlight">\(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\)</span>. That is,</p>
<div class="math notranslate nohighlight">
\[
J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
= J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i).
\]</div>
<p>In our analysis of multinomial logistic regression, we computed the Jacobian of <span class="math notranslate nohighlight">\(\bfk_i\)</span>. We obtained</p>
<div class="math notranslate nohighlight">
\[
J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)
= \begin{pmatrix}
\mathbb{A}_{n_{i+1}}[\mathbf{w}_i] &amp; \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
\end{pmatrix}.
\]</div>
<p>Recall that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
=
\mathcal{W}_i.
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathbb{B}_{n_{i+1}}[\mathbf{z}_i] 
= \begin{pmatrix}
\mathbf{e}_1 \mathbf{z}_i^T
&amp; \cdots &amp; \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T
\end{pmatrix}
= I_{n_{i+1}\times n_{i+1}} \otimes \mathbf{z}_i^T,
\]</div>
<p>where here <span class="math notranslate nohighlight">\(\mathbf{e}_{j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th standard basis vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{n_{i+1}}\)</span>.</p>
<p>From a previous calculation, the Jacobian of</p>
<div class="math notranslate nohighlight">
\[
\bsigma(\mathbf{t})
= (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
:= (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})),
\]</div>
<p>is</p>
<div class="math notranslate nohighlight">
\[
J_{\bsigma}(\mathbf{t})
= \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1} - \bsigma(\mathbf{t}))).
\]</div>
<p>Combining the previous formulas, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
&amp;= J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\
&amp;= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \begin{pmatrix}
\mathbb{A}_{n_{i+1}}[\mathbf{w}_i] &amp; \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
\end{pmatrix}\\
&amp;=\begin{pmatrix}
\widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &amp; \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
\end{pmatrix}
\end{align*}\]</div>
<p>where we define</p>
<div class="math notranslate nohighlight">
\[
\widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \mathcal{W}_i,
\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] 
&amp;= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
\left(I_{n_{i+1}\times n_{i+1}} \otimes \mathbf{z}_i^T\right)\\
&amp;= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \otimes \mathbf{z}_i^T,
\end{align*}\]</div>
<p>where we used the <em>Properties of the Kronecker Product (f)</em>.</p>
<p>For layer <span class="math notranslate nohighlight">\(L+1\)</span> (i.e., the output layer), we have previously computed the Jacobian of the softmax function composed with a linear transformation. We get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\mathbf{z}_{L+1}
:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
&amp;= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\
&amp;\begin{pmatrix} A_{L} &amp; B_{L} \end{pmatrix}
:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
&amp;= 
\begin{pmatrix}
[\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \mathcal{W}_{L} &amp;
[\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T
\end{pmatrix}\\
&amp;=: \begin{pmatrix}
\widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&amp;
\widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]
\end{pmatrix}
\end{align*}\]</div>
<p>Also, as in the multinomial logistic regression case, the loss and gradient of the loss are</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{L+2}
&amp;:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K y_i \log z_{L+1,i}\\
\mathbf{q}_{L+1}
&amp;:= \nabla \ell(\mathbf{z}_{L+1})
= \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
\end{align*}\]</div>
<p><em>Initialization:</em> <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span></p>
<p><em>Forward loop:</em> For <span class="math notranslate nohighlight">\(i = 0,1,\ldots,L-1\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{i+1} &amp;:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i \mathbf{z}_i \right)\\
\begin{pmatrix}
A_i &amp; B_i
\end{pmatrix}
&amp;:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
=\begin{pmatrix}
\widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &amp; \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
\end{pmatrix}
\end{align*}\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{L+1}
&amp;:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\
\begin{pmatrix} A_{L} &amp; B_{L} \end{pmatrix}
&amp;:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})
= \begin{pmatrix}
\widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&amp;
\widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]
\end{pmatrix}.
\end{align*}\]</div>
<p><em>Loss:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{L+2}
&amp;:= \ell(\mathbf{z}_{L+1})\\
\mathbf{p}_{L+1}
&amp;:= \nabla {\ell}(\mathbf{z}_{L+1})
= \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
\end{align*}\]</div>
<p><em>Backward loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1}
&amp;= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T \mathbf{p}_{L+1}\\
\mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1}
&amp;= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T \mathbf{p}_{L+1}
\end{align*}\]</div>
<p>and for <span class="math notranslate nohighlight">\(i = L-1,L-2,\ldots,1,0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_{i} 
&amp;:= A_i^T \mathbf{p}_{i+1}
= \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T \mathbf{p}_{i+1}\\
\mathbf{q}_{i} 
&amp;:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T \mathbf{p}_{i+1}
\end{align*}\]</div>
<p><em>Output:</em></p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
\]</div>
<!--ONLINE ONLY

**CHAT & LEARN** Via ChatGPT, in the words of Yoda (in case that helps you), the overall algorithm can be summarized as follows:

> Backward, the path of learning must go. Begin at the end, we do. At the output layer of the network, error we find. How much wrong the answers are, it shows. Calculate we must, the difference between what expected we were and what received we have.
> 
> Propagate this error back, we shall, through each layer preceding. Update each weight we must, according to its contribution to the error. Small adjustments to the weights, guided by the Force of the gradient, and the learning rate, it is.
>
> Learn from mistakes, the network does. Adjust it must, until right, the outputs are. Repeat this process we will, layer by layer, neuron by neuron, until learned enough, the network has. Train it does, iterate it must, learn it shall.
>
> This, the way of backpropagation is. Simple, yet profound.

$\ddagger$
--><p><strong>NUMERICAL CORNER:</strong> We implement the training of a neural network in PyTorch. We use the Fashion-MNIST dataset again. We first load it again. We also check for the availability of GPUs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                      <span class="k">else</span> <span class="p">(</span><span class="s1">'mps'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                            <span class="k">else</span> <span class="s1">'cpu'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Using device:'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Using device: mps
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">:</span> <span class="c1"># device-specific seeding and settings</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for multi-GPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">elif</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'mps'</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># MPS-specific seeding</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">g</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                               <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We construct a two-layer model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>                      <span class="c1"># Flatten the input</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>            <span class="c1"># First Linear layer with 32 nodes</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>                      <span class="c1"># Sigmoid activation function</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                  <span class="c1"># Second Linear layer with 10 nodes (output layer)</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As we did for multinomial logistic regression, we use the SGD optimizer and the cross-entropy loss (which in PyTorch includes the softmax function and expects labels to be actual class labels rather than one-hot encoding).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We train for 10 epochs.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>On the test data, we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 64.0% accuracy
</pre></div>
</div>
</div>
</div>
<p>Disappointingly, this is significantly less accurate than what we obtained using multinomial logistic regression. It turns out that using a different optimizer gives much better results.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>  
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">mmids</span><span class="o">.</span><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 87.1% accuracy
</pre></div>
</div>
</div>
</div>
<p><strong>CHAT &amp; LEARN</strong> We mentioned that there are many optimizers available in PyTorch besides SGD and Adam. Ask your favorite AI chatbot to explain and implement a different optimizer, such as Adagrad or RMSprop, for the MLP. Compare the results with those obtained using SGD and Adam. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Open In Colab</a>) <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Regularization techniques are often used to prevent overfitting in neural networks. Ask your favorite AI chatbot about <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> regularization, dropout, and early stopping. Discuss how these techniques can be incorporated into the training process and their effects on the learned model. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> What is the role of the sigmoid function in a multilayer perceptron (MLP)?</p>
<p>a) It is used as the loss function for training the MLP.</p>
<p>b) It is used as the nonlinear activation function in each layer of the MLP.</p>
<p>c) It is used to compute the gradient of the loss function with respect to the weights.</p>
<p>d) It is used to initialize the weights of the MLP.</p>
<p><strong>2</strong> In an MLP, what is the purpose of the softmax function in the output layer?</p>
<p>a) To introduce nonlinearity into the model.</p>
<p>b) To normalize the outputs into a probability distribution.</p>
<p>c) To compute the gradient of the loss function.</p>
<p>d) To reduce the dimensionality of the output.</p>
<p><strong>3</strong> What is the Jacobian matrix of the elementwise sigmoid function <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}(\mathbf{t}) = (\sigma(t_1), \dots, \sigma(t_n))\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot (1-\boldsymbol{\sigma}(\mathbf{t}))\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}) \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1 - \boldsymbol{\sigma}(\mathbf{t}))^T\)</span></p>
<p><strong>4</strong> In the forward phase of computing the gradient of the loss function in an MLP, what is the output of the <span class="math notranslate nohighlight">\(i\)</span>-th hidden layer?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i \mathbf{z}_i)\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i \mathbf{z}_i)\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i \boldsymbol{\sigma}(\mathbf{z}_i)\)</span></p>
<p><strong>5</strong> What is the output of the backward loop in computing the gradient of the loss function in an MLP?</p>
<p>a) The gradient of the loss function with respect to the activations of each layer.</p>
<p>b) The gradient of the loss function with respect to the weights of each layer.</p>
<p>c) The updated weights of the MLP.</p>
<p>d) The loss function value.</p>
<p>Answer for 1: b. Justification: The text states that “Each of the main layers of a feedforward neural network has two components, an affine map and a nonlinear activation function. For the latter, we restrict ourselves here to the sigmoid function.”</p>
<p>Answer for 2: b. Justification: The text states that the softmax function is used in the output layer to produce a probability distribution over the possible classes.</p>
<p>Answer for 3: c. Justification: The text states that the Jacobian of the elementwise sigmoid function is:</p>
<div class="math notranslate nohighlight">
\[
J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t) \odot (1 - \boldsymbol{\sigma}(t)))
\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> denotes the Hadamard (elementwise) product.</p>
<p>Answer for 4: a. Justification: The text defines the output of the <span class="math notranslate nohighlight">\(i\)</span>-th hidden layer as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i \mathbf{z}_i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}\)</span> is the sigmoid activation function and <span class="math notranslate nohighlight">\(\mathcal{W}_i\)</span> is the weight matrix for the <span class="math notranslate nohighlight">\(i\)</span>-th layer.</p>
<p>Answer for 5: b. Justification: The text states that the output of the backward loop is the gradient of the loss function with respect to the weights of each layer:</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i, \mathbf{w}_i]^T \mathbf{p}_{i+1}\)</span> is the gradient with respect to the weights of the <span class="math notranslate nohighlight">\(i\)</span>-th layer.</p>
    
</body>
</html>