- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SAMPLING AND CONFIDENCE
  prefs: []
  type: TYPE_NORMAL
- en: Recall that inferential statistics involves making inferences about a **population**
    of examples by analyzing a randomly chosen subset of that population. This subset
    is called a **sample**.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling is important because it is often not possible to observe the entire
    population of interest. A physician cannot count the number of a species of bacterium
    in a patient's blood stream, but it is possible to measure the population in a
    small sample of the patient's blood, and from that to infer characteristics of
    the total population. If you wanted to know the average weight of eighteen-year-old
    Americans, you could try and round them all up, put them on a very large scale,
    and then divide by the number of people. Alternatively, you could round up 50
    randomly chose eighteen-year-olds, compute their mean weight, and assume that
    their mean weight was a reasonable estimate of the mean weight of the entire population
    of eighteen-year-olds.
  prefs: []
  type: TYPE_NORMAL
- en: The correspondence between the sample and the population of interest is of overriding
    importance. If the sample is not representative of the population, no amount of
    fancy mathematics will lead to valid inferences. A sample of 50 women or 50 Asian-Americans
    or 50 football players cannot be used to make valid inferences about the average
    weight of the population of all eighteen-year-olds in America.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focus on **probability sampling**. With probability sampling,
    each member of the population of interest has some nonzero probability of being
    included in the sample. In a **simple random sample**, each member of the population
    has an equal chance of being chosen for the sample. In **stratified sampling**,
    the population is first partitioned into subgroups, and then the sample is built
    by randomly sampling from each subgroup. Stratified sampling can be used to increase
    the probability that a sample is representative of the population as a whole.
    For example, ensuring that the fraction of men and women in a sample matches the
    fraction of men and women in the population increases the probability that that
    the mean weight of the sample, the **sample mean**, will be a good estimate of
    the mean weight of the whole population, the **population mean**.
  prefs: []
  type: TYPE_NORMAL
- en: The code in the chapter assumes the following import statements
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 19.1 Sampling the Boston Marathon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each year since 1897, athletes (mostly runners, but since 1975 there has been
    a wheelchair division) have gathered in Massachusetts to participate in the Boston
    Marathon.[^(133)](#c19-fn-0001) In recent years, around 20,000 hardy souls per
    year have successfully taken on the `42.195` km (`26` mile, `385` yard) course.
  prefs: []
  type: TYPE_NORMAL
- en: A file containing data from the 2012 race is available on the website associated
    with this book. The file `bm_results2012.csv` is in a comma-separated format,
    and contains the name, gender,[^(134)](#c19-fn-0002) age, division, country, and
    time for each participant. [Figure 19-1](#c19-fig-0001) contains the first few
    lines of the contents of the file.
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0001.jpg](../images/c19-fig-0001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-1](#c19-fig-0001a) The first few lines in `bm_results2012.csv`'
  prefs: []
  type: TYPE_NORMAL
- en: Since complete data about the results of each race is easily available, there
    is no pragmatic need to using sampling to derive statistics about a race. However,
    it is pedagogically useful to compare statistical estimates derived from samples
    to the actual value being estimated.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Figure 19-2](#c19-fig-0002) produces the plot shown in [Figure
    19-3](#c19-fig-0003). The function `get_BM_data` reads data from a file containing
    information about each of the competitors in the race. It returns the data in
    a dictionary with six elements. Each key describes the type of data (e.g., `'name'`
    or `'gender'`) contained in the elements of a list associated with that key. For
    example, `data['time']` is a list of floats containing the finishing time of each
    competitor, `data['name'][i]` is the name of the `i`^(th) competitor, and `data['time'][i]`
    is the finishing time of the `i`^(th) competitor. The function `make_hist` produces
    a visual representation of the finishing times. (In Chapter 23, we will look at
    a Python module, Pandas, that could be used to simplify a lot of the code in this
    chapter, including `get_BM_data` and `make_hist`.)
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0002.jpg](../images/c19-fig-0002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-2](#c19-fig-0002a) Read data and produce plot of Boston Marathon'
  prefs: []
  type: TYPE_NORMAL
- en: The code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: produces the plot in [Figure 19-3](#c19-fig-0003).
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0003.jpg](../images/c19-fig-0003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-3](#c19-fig-0003a) Boston Marathon finishing times'
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of finishing times resembles a normal distribution but is clearly
    not normal because of the fat tail on the right.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's pretend that we don't have access to the data about all competitors,
    and instead want to estimate some statistics about the finishing times of the
    entire field by sampling a small number of randomly chosen competitors.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Figure 19-4](#c19-fig-0004) creates a simple random sample of the
    elements of `times`, and then uses that sample to estimate the mean and standard
    deviation of `times`. The function `sample_times` uses `random.sample(times, num_examples)`
    to extract the sample. The invocation of `random.sample` returns a list of size
    `num_examples` of randomly chosen distinct elements from the list `times`. After
    extracting the sample, `sample_times` produces a histogram showing the distribution
    of values in the sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0004.jpg](../images/c19-fig-0004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-4](#c19-fig-0004a) Sampling finishing times'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 19-5](#c19-fig-0005) shows, the distribution of the sample is much
    farther from normal than the distribution from which it was drawn. This is not
    surprising, given the small sample size. What's more surprising is that despite
    the small sample size (`40` out of about `21,000`) the estimated mean differs
    from the population mean by around 3`%`. Did we get lucky, or is there reason
    to expect that the estimate of the mean will be pretty good? To put it another
    way, can we express in a quantitative way how much confidence we should have in
    our estimate?
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0005.jpg](../images/c19-fig-0005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-5](#c19-fig-0005a) Analyzing a small sample'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in Chapters 17 and 18, it is often useful to provide a confidence
    interval and confidence level to indicate the reliability of the estimate. Given
    a single sample (of any size) drawn from a larger population, the best estimate
    of the mean of the population is the mean of the sample. Estimating the width
    of the confidence interval required to achieve a desired confidence level is trickier.
    It depends, in part, upon the size of the sample.
  prefs: []
  type: TYPE_NORMAL
- en: It's easy to understand why the size of the sample is important. The law of
    large numbers tells us that as the sample size grows, the distribution of the
    values of the sample is more likely to resemble the distribution of the population
    from which the sample is drawn. Consequently, as the sample size grows, the sample
    mean and the sample standard deviation are likely to be closer to the population
    mean and population standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: So, bigger is better, but how big is big enough? That depends upon the variance
    of the population. The higher the variance, the more samples are needed. Consider
    two normal distributions, one with a mean of `0` and standard deviation of `1`,
    and the other with a mean of `0` and a standard deviation of `100`. If we were
    to select one randomly chosen element from one of these distributions and use
    it to estimate the mean of the distribution, the probability of that estimate
    being within any desired accuracy, ∈, of the true mean (`0`), would be equal to
    the area under the probability density function between −∈and ∈ (see Section 17.4.1).
    The code in [Figure 19-6](#c19-fig-0006) computes and prints these probabilities
    for ∈ = 3 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0006.jpg](../images/c19-fig-0006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-6](#c19-fig-0006a) Effect of variance on estimate of mean'
  prefs: []
  type: TYPE_NORMAL
- en: When the code in [Figure 19-6](#c19-fig-0006) is run, it prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The code in [Figure 19-7](#c19-fig-0007) plots the mean of each of `1000` samples
    of size `40` from two normal distributions. Again, each distribution has a mean
    of `0`, but one has a standard deviation of `1` and the other a standard deviation
    of `100`.
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0007.jpg](../images/c19-fig-0007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-7](#c19-fig-0007a) Compute and plot sample means'
  prefs: []
  type: TYPE_NORMAL
- en: The left side of [Figure 19-8](#c19-fig-0008) shows the mean of each sample.
    As expected, when the population standard deviation is 1, the sample means are
    all near the population mean of `0,` which is why no distinct circles are visible—they
    are so dense that they merge into what appears to be a bar. In contrast, when
    the standard deviation of the population is `100`, the sample means are scattered
    in a hard-to-discern pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0008.jpg](../images/c19-fig-0008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-8](#c19-fig-0008a) Sample means'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when we look at a histogram of the means when the standard deviation
    is `100`, the right side of [Figure 19-8](#c19-fig-0008), something important
    emerges: the means form a distribution that resembles a normal distribution centered
    around `0`. That the right side of [Figure 19-8](#c19-fig-0008) looks the way
    it does is not an accident. It is a consequence of the central limit theorem,
    the most famous theorem in all of probability and statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: 19.2 The Central Limit Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central limit theorem explains why it is possible to use a single sample
    drawn from a population to estimate the variability of the means of a set of hypothetical
    samples drawn from the same population.
  prefs: []
  type: TYPE_NORMAL
- en: A version of the **central limit theorem** (**CLT** to its friends) was first
    published by Laplace in 1810, and then refined by Poisson in the 1820s. But the
    CLT as we know it today is a product of work done by a sequence of prominent mathematicians
    in the first half of the twentieth century.
  prefs: []
  type: TYPE_NORMAL
- en: Despite (or maybe because of) the impressive list of mathematicians who have
    worked on it, the CLT is really quite simple. It says that
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of sufficiently large samples drawn from the same population, the
    means of the samples (the sample means) will be approximately normally distributed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This normal distribution will have a mean close to the mean of the population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance (computed using `numpy.var`) of the sample means will be close
    to the variance of the population divided by the sample size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at an example of the CLT in action. Imagine that you had a die with
    the property that that each roll would yield a random real number between 0 and
    5\. The code in [Figure 19-9](#c19-fig-0009) simulates rolling such a die many
    times, prints the mean and variance (the function `variance` is defined in Figure
    17-8), and then plots a histogram showing the probability of ranges of numbers
    getting rolled. It also simulates rolling `100` dice many times and plots (on
    the same figure) a histogram of the mean value of those `100` dice. The `hatch`
    keyword argument is used to visually distinguish one histogram from the other.
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0009.jpg](../images/c19-fig-0009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-9](#c19-fig-0009a) Estimating the mean of a continuous die'
  prefs: []
  type: TYPE_NORMAL
- en: The `weights` keyword is bound to an array of the same length as the first argument
    to `hist`, and is used to assign a weight to each element in the first argument.
    In the resulting histogram, each value in a bin contributes its associated weight
    towards the bin count (instead of the usual `1`). In this example, we use `weights`
    to scale the y values to the relative (rather than absolute) size of each bin.
    Therefore, for each bin, the value on the y-axis is the probability of the mean
    falling within that bin.
  prefs: []
  type: TYPE_NORMAL
- en: When run, the code produced the plot in [Figure 19-10](#c19-fig-0010), and printed,
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0010.jpg](../images/c19-fig-0010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-10](#c19-fig-0010a) An illustration of the CLT'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In each case the mean was quite close to the expected mean of `2.5`. Since our
    die is fair, the probability distribution for one die is almost perfectly uniform,[^(135)](#c19-fn-0003)
    i.e., very far from normal. However, when we look at the average value of `100`
    dice, the distribution is almost perfectly normal, with the peak including the
    expected mean. Furthermore, the variance of the mean of the `100` rolls is close
    to the variance of the value of a single roll divided by `100`. All is as predicted
    by the CLT.
  prefs: []
  type: TYPE_NORMAL
- en: It's nice that the CLT seems to work, but what good is it? Perhaps it could
    prove useful in winning bar bets for those who drink in particularly nerdy bars.
    However, the primary value of the CLT is that it allows us to compute confidence
    levels and intervals even when the underlying population distribution is not normal.
    When we looked at confidence intervals in Section 17.4.2, we pointed out that
    the empirical rule is based on assumptions about the nature of the space being
    sampled. We assumed that
  prefs: []
  type: TYPE_NORMAL
- en: The mean estimation error is 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution of the errors in the estimates is normal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When these assumptions hold, the empirical rule for normal distributions provides
    a handy way to estimate confidence intervals and levels given the mean and standard
    deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's return to the Boston Marathon example. The code in [Figure 19-11](#c19-fig-0011),
    which produced the plot in [Figure 19-12](#c19-fig-0012), draws 200 simple random
    samples for each of a variety of sample sizes. For each sample size, it computes
    the mean of each of the 200 samples; it then computes the mean and standard deviation
    of those means. Since the CLT tells us that the sample means will be normally
    distributed, we can use the standard deviation and the empirical rule to compute
    a `95%` confidence interval for each sample size.
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0011.jpg](../images/c19-fig-0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-11](#c19-fig-0011a) Produce plot with error bars'
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0012.jpg](../images/c19-fig-0012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-12](#c19-fig-0012a) Estimates of finishing times with error bars'
  prefs: []
  type: TYPE_NORMAL
- en: As the plot in [Figure 19-12](#c19-fig-0012) shows, all of the estimates are
    reasonably close to the actual population mean. Notice, however, that the error
    in the estimated mean does not decrease monotonically with the size of the samples—the
    estimate using `700` examples happens to be worse than the estimate using `50`
    examples. What does change monotonically with the sample size is our confidence
    in our estimate of the mean. As the sample size grows from `100` to `1500`, the
    confidence interval decreases from about `±15` to about `±2.5`. This is important.
    It's not good enough to get lucky and happen to get a good estimate. We need to
    know how much confidence to have in our estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 19.3 Standard Error of the Mean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We just saw that if we chose 200 random samples of `1,500` competitors, we could,
    with `95%` confidence, estimate the mean finishing time within a range of about
    five minutes. We did this using the standard deviation of the sample means. Unfortunately,
    since this involves using more total examples (`200`*`1500 = 300,000`) than there
    were competitors, it doesn't seem like a useful result. We would have been better
    off computing the actual mean directly using the entire population. What we need
    is a way to estimate a confidence interval using a single example. Enter the concept
    of the **standard error** **of the mean** (**SE** or **SEM**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The SEM for a sample of size `n` is the standard deviation of the means of
    an infinite number of samples of size `n` drawn from the same population. Unsurprisingly,
    it depends upon both `n` and σ, the standard deviation of the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-5001.jpg](../images/c19-fig-5001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-13](#c19-fig-0014) compares the SEM for the sample sizes used in
    [Figure 19-12](#c19-fig-0012) to the standard deviation of the means of the 200
    samples we generated for each sample size.'
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0013.jpg](../images/c19-fig-0013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-13](#c19-fig-0014a) Standard error of the mean'
  prefs: []
  type: TYPE_NORMAL
- en: The actual standard deviations of the means of our 200 samples closely tracks
    the SE. Notice that both the SEM and the SD drop rapidly at the start and then
    more slowly as the sample size gets large. This is because the value depends upon
    the square root of the sample size. In other words, to cut the standard deviation
    in half, we need to quadruple the sample size.
  prefs: []
  type: TYPE_NORMAL
- en: Alas, if all we have is a single sample, we don't know the standard deviation
    of the population. Typically, we assume that the standard deviation of the sample,
    the sample standard deviation, is a reasonable proxy for the standard deviation
    of the population. This will be the case when the population distribution is not
    terribly skewed.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Figure 19-14](#c19-fig-0015) creates `100` samples of various sizes
    from the Boston Marathon data, and compares the mean standard deviation of the
    samples of each size to the standard deviation of the population. It produces
    the plot in [Figure 19-15](#c19-fig-0016).
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0014.jpg](../images/c19-fig-0014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-14](#c19-fig-0015a) Sample standard deviation vs. population standard
    deviation'
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0015.jpg](../images/c19-fig-0015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-15](#c19-fig-0016a) Sample standard deviations'
  prefs: []
  type: TYPE_NORMAL
- en: By the time the sample size reaches `100`, the difference between the sample
    standard deviation and the population standard deviation is relatively small (about
    1.2% of the actual mean finishing time).
  prefs: []
  type: TYPE_NORMAL
- en: In practice, people usually use the sample standard deviation in place of the
    (usually unknown) population standard deviation to estimate the SE. If the sample
    size is large enough,[^(136)](#c19-fn-0004) and the population distribution is
    not too far from normal, it is safe to use this estimate to compute confidence
    intervals using the empirical rule.
  prefs: []
  type: TYPE_NORMAL
- en: What does this imply? If we take a single sample of say 200 runners, we can
  prefs: []
  type: TYPE_NORMAL
- en: Compute the mean and standard deviation of that sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the standard deviation of that sample to estimate the SE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the estimated SE to generate confidence intervals around the sample mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code in [Figure 19-16](#c19-fig-0017) does this `10,000` times and then
    prints the fraction of times the sample mean is more than `1.96` estimated SEs
    from the population mean. (Recall that for a normal distribution `95%` of the
    data falls within `1.96` standard deviations of the mean.)
  prefs: []
  type: TYPE_NORMAL
- en: '![c19-fig-0016.jpg](../images/c19-fig-0016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 19-16](#c19-fig-0017a) Estimating the population mean 10,000 times'
  prefs: []
  type: TYPE_NORMAL
- en: When the code is run it prints,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: That is pretty much what the theory predicts. Score one for the CLT!
  prefs: []
  type: TYPE_NORMAL
- en: 19.4 Terms Introduced in Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sample size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: probability sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: simple random sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stratified sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sample mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: population mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: central limit theorem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: standard error (SE, SEM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
