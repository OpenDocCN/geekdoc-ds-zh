<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Memory Latency</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Memory Latency</h1>
<blockquote>原文：<a href="https://en.algorithmica.org/hpc/cpu-cache/latency/">https://en.algorithmica.org/hpc/cpu-cache/latency/</a></blockquote><div id="search"><input id="search-bar" type="search" placeholder="Search this book…" oninput="search()"/><div id="search-count"/><div id="search-results"/></div><header><div class="info"/></header><article><p>Despite that <a href="../bandwidth">bandwidth</a> is a more complicated concept, it is much easier to observe and measure than latency: you can simply execute a long series of independent read or write queries, and the scheduler, having access to them in advance, reorders and overlaps them, hiding their latency and maximizing the total throughput.</p><p>To measure <em>latency</em>, we need to design an experiment where the CPU can’t cheat by knowing the memory locations we will request in advance. One way to ensure this is to generate a random permutation of size $N$ that corresponds to a cycle and then repeatedly follow the permutation:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">p</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">q</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// generating a random permutation
</span></span></span><span class="line"><span class="cl"><span class="c1"/><span class="n">iota</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span> <span class="o">+</span> <span class="n">N</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">random_shuffle</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span> <span class="o">+</span> <span class="n">N</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// this permutation may contain multiple cycles,
</span></span></span><span class="line"><span class="cl"><span class="c1">// so instead we use it to construct another permutation with a single cycle
</span></span></span><span class="line"><span class="cl"><span class="c1"/><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">t</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
</span></span></code></pre></div><p>Compared to linear iteration, it is <em>much</em> slower — by multiple orders of magnitude — to visit all elements of an array this way. Not only does it make <a href="/hpc/simd">SIMD</a> impossible, but it also <a href="/hpc/pipelining">stalls the pipeline</a>, creating a large traffic jam of instructions, all waiting for a single piece of data to be fetched from the memory.</p><p>This performance anti-pattern is known as <em>pointer chasing</em>, and it is very frequent in data structures, especially those written high-level languages that use lots of heap-allocated objects and pointers to them necessary for dynamic typing.</p><p><figure><img src="../Images/679a67a9cc2e67d6e8233f2aef54db53.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/latency-throughput.svg"/><figcaption/></figure></p><p>When talking about latency, it makes more sense to use cycles or nanoseconds rather than throughput units, so we replace this graph with its reciprocal:</p><p><figure><img src="../Images/37ff0412d79dea6330b801c37cc61f52.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/permutation-latency.svg"/><figcaption/></figure></p><p>Note that the cliffs on both graphs aren’t as distinctive as they were for the bandwidth. This is because we still have some chance of hitting the previous layer of cache even if the array can’t fit into it entirely.</p><span class="anchor" id="theoretical-latency"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/cpu-cache/latency/#theoretical-latency">#</a>Theoretical Latency</h3><p>More formally, if there are $k$ levels in the cache hierarchy with sizes $s_i$ and latencies $l_i$, then, instead of being equal to the slowest access, their expected latency will be:</p>$$
E[L] = \frac{
s_1 \cdot l_1
+ (s_2 - s_1) \cdot l_2
% + (s_3 - s_2) \cdot l_3
+ \ldots
+ (N - s_k) \cdot l_{RAM}
}{N}
$$
If we abstract away from all that happens before the slowest cache layer, we can reduce the formula to just this:
$$
E[L] = \frac{N \cdot l_{last} - C}{N} = l_{last} - \frac{C}{N}
$$
As $N$ increases, the expected latency slowly approaches $l_{last}$, and if you squint hard enough, the graph of the throughput (reciprocal latency) should roughly look like if it is composed of a few transposed and scaled hyperbolas:
$$
\begin{aligned}
E[L]^{-1} &amp;= \frac{1}{l_{last} - \frac{C}{N}}
\\ &amp;= \frac{N}{N \cdot l_{last} - C}
\\ &amp;= \frac{1}{l_{last}} \cdot \frac{N + \frac{C}{l_{last}} - \frac{C}{l_{last}}}{N - \frac{C}{l_{last}}}
\\ &amp;= \frac{1}{l_{last}} \cdot \left(\frac{1}{N \cdot \frac{l_{last}}{C} - 1} + 1\right)
\\ &amp;= \frac{1}{k \cdot (x - x_0)} + y_0
\end{aligned}
$$<p>To get the actual latency numbers, we can iteratively apply the first formula to deduce $l_1$, then $l_2$, and so on. Or just look at the values right before the cliff — they should be within 10-15% of the true latency.</p><p>There are more direct ways to measure latency, including the use of <a href="../bandwidth">non-temporal reads</a>, but this benchmark is more representable of practical access patterns.</p><span class="anchor" id="frequency-scaling"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/cpu-cache/latency/#frequency-scaling">#</a>Frequency Scaling</h3><p>Similar to bandwidth, the latency of all CPU caches proportionally scales with its clock frequency, while the RAM does not. We can also observe this difference if we change the frequency by turning turbo boost on.</p><p><figure><img src="../Images/52a8679535465aa0773ba3893cbcc533.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/permutation-boost.svg"/><figcaption/></figure></p><p>The graph starts making more sense if we plot it as a relative speedup.</p><p><figure><img src="../Images/3448a82210f51d78cfa7ee4a30cc4db3.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/permutation-boost-speedup.svg"/><figcaption/></figure></p><p>You would expect 2x rates for array sizes that fit into CPU cache entirely, but then roughly equal for arrays stored in RAM. But this is not quite what is happening: there is a small, fixed-latency delay on lower clocked run even for RAM accesses. This happens because the CPU first has to check its cache before dispatching a read query to the main memory — to save RAM bandwidth for other processes that potentially need it.</p><p>Memory latency is also slightly affected by some details of the <a href="../paging">virtual memory implementation</a> and <a href="../mlp">RAM-specific timings</a>, which we will discuss later.</p></article><div class="nextprev"><div class="left"><a href="https://en.algorithmica.org/hpc/cpu-cache/bandwidth/" id="prev-article">← Memory Bandwidth</a></div><div class="right"><a href="https://en.algorithmica.org/hpc/cpu-cache/cache-lines/" id="next-article">Cache Lines →</a></div></div>    
</body>
</html>