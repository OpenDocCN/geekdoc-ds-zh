- en: Chapter 9 Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 聚类
- en: 原文：[https://datasciencebook.ca/clustering.html](https://datasciencebook.ca/clustering.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://datasciencebook.ca/clustering.html](https://datasciencebook.ca/clustering.html)
- en: 9.1 Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 概述
- en: As part of exploratory data analysis, it is often helpful to see if there are
    meaningful subgroups (or *clusters*) in the data. This grouping can be used for
    many purposes, such as generating new questions or improving predictive analyses.
    This chapter provides an introduction to clustering using the K-means algorithm,
    including techniques to choose the number of clusters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为探索性数据分析的一部分，查看数据中是否存在有意义的子组（或*聚类*）通常很有帮助。这种分组可以用于许多目的，例如提出新问题或改进预测分析。本章介绍了使用K-means算法进行聚类的方法，包括选择聚类数量的技术。
- en: 9.2 Chapter learning objectives
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 章节学习目标
- en: 'By the end of the chapter, readers will be able to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，读者将能够做到以下几项：
- en: Describe a situation in which clustering is an appropriate technique to use,
    and what insight it might extract from the data.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述一个聚类是适当技术的情况，以及它可能从数据中提取的见解。
- en: Explain the K-means clustering algorithm.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释K-means聚类算法。
- en: Interpret the output of a K-means analysis.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释K-means分析的结果。
- en: Differentiate between clustering, classification, and regression.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分聚类、分类和回归。
- en: Identify when it is necessary to scale variables before clustering, and do this
    using R.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定何时在聚类之前需要对变量进行缩放，并使用R来完成此操作。
- en: Perform K-means clustering in R using `tidymodels` workflows.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tidymodels`工作流程在R中执行K-means聚类。
- en: Use the elbow method to choose the number of clusters for K-means.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用肘部方法选择K-means的聚类数量。
- en: Visualize the output of K-means clustering in R using colored scatter plots.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R中的彩色散点图可视化K-means聚类的结果。
- en: Describe the advantages, limitations and assumptions of the K-means clustering
    algorithm.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述K-means聚类算法的优点、局限性和假设。
- en: 9.3 Clustering
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 聚类
- en: Clustering is a data analysis technique involving separating a data set into
    subgroups of related data. For example, we might use clustering to separate a
    data set of documents into groups that correspond to topics, a data set of human
    genetic information into groups that correspond to ancestral subpopulations, or
    a data set of online customers into groups that correspond to purchasing behaviors.
    Once the data are separated, we can, for example, use the subgroups to generate
    new questions about the data and follow up with a predictive modeling exercise.
    In this course, clustering will be used only for exploratory analysis, i.e., uncovering
    patterns in the data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种数据分析技术，涉及将数据集分成相关数据的子组。例如，我们可能使用聚类将文档数据集分成与主题相对应的组，将人类遗传信息数据集分成与祖先亚种群相对应的组，或将在线客户数据集分成与购买行为相对应的组。一旦数据被分离，例如，我们可以使用子组来提出关于数据的新问题，并随后进行预测建模练习。在本课程中，聚类仅用于探索性分析，即揭示数据中的模式。
- en: Note that clustering is a fundamentally different kind of task than classification
    or regression. In particular, both classification and regression are *supervised
    tasks* where there is a *response variable* (a category label or value), and we
    have examples of past data with labels/values that help us predict those of future
    data. By contrast, clustering is an *unsupervised task*, as we are trying to understand
    and examine the structure of data without any response variable labels or values
    to help us. This approach has both advantages and disadvantages. Clustering requires
    no additional annotation or input on the data. For example, while it would be
    nearly impossible to annotate all the articles on Wikipedia with human-made topic
    labels, we can cluster the articles without this information to find groupings
    corresponding to topics automatically. However, given that there is no response
    variable, it is not as easy to evaluate the “quality” of a clustering. With classification,
    we can use a test data set to assess prediction performance. In clustering, there
    is not a single good choice for evaluation. In this book, we will use visualization
    to ascertain the quality of a clustering, and leave rigorous evaluation for more
    advanced courses.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，聚类是一个与分类或回归根本不同的任务。特别是，分类和回归都是**监督任务**，其中有一个**响应变量**（一个类别标签或值），并且我们有带有标签/值的过去数据的例子，这有助于我们预测未来数据的标签/值。相比之下，聚类是一个**无监督任务**，因为我们试图理解和检查数据的结构，而没有任何响应变量标签或值来帮助我们。这种方法既有优点也有缺点。聚类不需要对数据进行额外的注释或输入。例如，虽然几乎不可能用人工主题标签注释维基百科上的所有文章，但我们可以在没有这些信息的情况下聚类文章，以自动找到与主题相对应的分组。然而，由于没有响应变量，评估聚类的“质量”并不容易。在分类中，我们可以使用测试数据集来评估预测性能。在聚类中，没有单一的好的评估选择。在这本书中，我们将使用可视化来确定聚类的质量，并将严格的评估留给更高级的课程。
- en: As in the case of classification, there are many possible methods that we could
    use to cluster our observations to look for subgroups. In this book, we will focus
    on the widely used K-means algorithm ([Lloyd 1982](#ref-kmeans)). In your future
    studies, you might encounter hierarchical clustering, principal component analysis,
    multidimensional scaling, and more; see the additional resources section at the
    end of this chapter for where to begin learning more about these other methods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在分类的情况下，有许多可能的方法我们可以用来聚类我们的观察结果以寻找子组。在这本书中，我们将重点关注广泛使用的K-means算法([Lloyd 1982](#ref-kmeans))。在你的未来研究中，你可能会遇到层次聚类、主成分分析、多维尺度分析等；请参阅本章末尾的附加资源部分，了解如何开始学习这些其他方法。
- en: '**Note:** There are also so-called *semisupervised* tasks, where only some
    of the data come with response variable labels/values, but the vast majority don’t.
    The goal is to try to uncover underlying structure in the data that allows one
    to guess the missing labels. This sort of task is beneficial, for example, when
    one has an unlabeled data set that is too large to manually label, but one is
    willing to provide a few informative example labels as a “seed” to guess the labels
    for all the data.'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**还存在所谓的**半监督**任务，其中只有部分数据带有响应变量标签/值，但绝大多数没有。目标是尝试揭示数据中的潜在结构，以便能够猜测缺失的标签。这种任务在例如，当一个人有一个未标记的数据集太大而无法手动标记，但愿意提供一些信息性示例标签作为“种子”来猜测所有数据的标签时是有益的。'
- en: 9.4 An illustrative example
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 一个示例
- en: In this chapter we will focus on a data set from [the `palmerpenguins` R package](https://allisonhorst.github.io/palmerpenguins/)
    ([Horst, Hill, and Gorman 2020](#ref-palmerpenguins)). This data set was collected
    by Dr. Kristen Gorman and the Palmer Station, Antarctica Long Term Ecological
    Research Site, and includes measurements for adult penguins (Figure [9.1](clustering.html#fig:09-penguins))
    found near there ([Gorman, Williams, and Fraser 2014](#ref-penguinpaper)). Our
    goal will be to use two variables—penguin bill and flipper length, both in millimeters—to
    determine whether there are distinct types of penguins in our data. Understanding
    this might help us with species discovery and classification in a data-driven
    way. Note that we have reduced the size of the data set to 18 observations and
    2 variables; this will help us make clear visualizations that illustrate how clustering
    works for learning purposes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注来自[Palmerpenguins R包](https://allisonhorst.github.io/palmerpenguins/)（[Horst,
    Hill, and Gorman 2020](#ref-palmerpenguins)）的数据集。这个数据集由Kristen Gorman博士和南极洲Palmer站长期生态研究站收集，包括在该地区发现的成年企鹅的测量数据（图[9.1](clustering.html#fig:09-penguins)）([Gorman,
    Williams, and Fraser 2014](#ref-penguinpaper)）。我们的目标将是使用两个变量——企鹅喙和鳍长，两者都以毫米为单位——来确定我们的数据中是否存在不同的企鹅类型。了解这一点可能有助于我们以数据驱动的方式发现和分类物种。请注意，我们已经将数据集的大小减少到18个观测值和2个变量；这将帮助我们制作清晰的视觉图表，说明聚类如何用于学习目的。
- en: '![A Gentoo penguin.](../Images/587e65c59e3cbf110b7dc3fa162adf40.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![一只 Gentoo 企鹅。](../Images/587e65c59e3cbf110b7dc3fa162adf40.png)'
- en: 'Figure 9.1: A Gentoo penguin.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：一只 Gentoo 企鹅。
- en: Before we get started, we will load the `tidyverse` metapackage as well as set
    a random seed. This will ensure we have access to the functions we need and that
    our analysis will be reproducible. As we will learn in more detail later in the
    chapter, setting the seed here is important because the K-means clustering algorithm
    uses randomness when choosing a starting position for each cluster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们将加载`tidyverse`元包并设置一个随机种子。这将确保我们能够访问所需的函数，并且我们的分析将是可重复的。正如我们将在本章后面更详细地学习的那样，在这里设置种子很重要，因为K-means聚类算法在为每个簇选择起始位置时使用随机性。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we can load and preview the `penguins` data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以加载并预览`penguins`数据。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will begin by using a version of the data that we have standardized, `penguins_standardized`,
    to illustrate how K-means clustering works (recall standardization from Chapter
    [5](classification1.html#classification1)). Later in this chapter, we will return
    to the original `penguins` data to see how to include standardization automatically
    in the clustering pipeline.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用一个标准化的数据版本，即`penguins_standardized`，来展示K-means聚类是如何工作的（回忆第[5](classification1.html#classification1)章中的标准化）。在本章的后面部分，我们将回到原始的`penguins`数据，看看如何将标准化自动包含到聚类流程中。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we can create a scatter plot using this data set to see if we can detect
    subtypes or groups in our data set.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用这个数据集创建一个散点图，看看我们是否能在数据集中检测到亚型或组。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Scatter plot of standardized bill length versus standardized flipper length.](../Images/1b4fb65d637c9162fd8bac2616286adf.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![标准化喙长度与标准化鳍长度的散点图。](../Images/1b4fb65d637c9162fd8bac2616286adf.png)'
- en: 'Figure 9.2: Scatter plot of standardized bill length versus standardized flipper
    length.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：标准化喙长度与标准化鳍长度的散点图。
- en: 'Based on the visualization in Figure [9.2](clustering.html#fig:10-toy-example-plot),
    we might suspect there are a few subtypes of penguins within our data set. We
    can see roughly 3 groups of observations in Figure [9.2](clustering.html#fig:10-toy-example-plot),
    including:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图[9.2](clustering.html#fig:10-toy-example-plot)中的可视化，我们可能会怀疑在我们的数据集中存在几种企鹅亚型。我们可以从图[9.2](clustering.html#fig:10-toy-example-plot)中看到大约3个观测值组，包括：
- en: a small flipper and bill length group,
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个小鳍和喙长度组，
- en: a small flipper length, but large bill length group, and
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个小鳍长度但大喙长度组，
- en: a large flipper and bill length group.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个大鳍和喙长度组。
- en: Data visualization is a great tool to give us a rough sense of such patterns
    when we have a small number of variables. But if we are to group data—and select
    the number of groups—as part of a reproducible analysis, we need something a bit
    more automated. Additionally, finding groups via visualization becomes more difficult
    as we increase the number of variables we consider when clustering. The way to
    rigorously separate the data into groups is to use a clustering algorithm. In
    this chapter, we will focus on the *K-means* algorithm, a widely used and often
    very effective clustering method, combined with the *elbow method* for selecting
    the number of clusters. This procedure will separate the data into groups; Figure
    [9.3](clustering.html#fig:10-toy-example-clustering) shows these groups denoted
    by colored scatter points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是当我们只有少量变量时，给我们一个这种模式的大致感觉的伟大工具。但是，如果我们要将数据分组——并选择组数——作为可重复分析的一部分，我们需要一些更自动化的东西。此外，随着我们在聚类时考虑的变量数量的增加，通过可视化找到分组变得更加困难。将数据严格分离成组的方法是使用聚类算法。在本章中，我们将重点关注*K-means*算法，这是一种广泛使用且通常非常有效的聚类方法，结合*肘部方法*来选择聚类数量。这个程序将数据分离成组；图[9.3](clustering.html#fig:10-toy-example-clustering)显示了这些用彩色散点表示的组。
- en: '![Scatter plot of standardized bill length versus standardized flipper length
    with colored groups.](../Images/193770128dda6dec7f1c0874f6a92578.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![标准化喙长与标准化鳍长散点图，分组用颜色表示。](../Images/193770128dda6dec7f1c0874f6a92578.png)'
- en: 'Figure 9.3: Scatter plot of standardized bill length versus standardized flipper
    length with colored groups.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：标准化喙长与标准化鳍长散点图，分组用颜色表示。
- en: 'What are the labels for these groups? Unfortunately, we don’t have any. K-means,
    like almost all clustering algorithms, just outputs meaningless “cluster labels”
    that are typically whole numbers: 1, 2, 3, etc. But in a simple case like this,
    where we can easily visualize the clusters on a scatter plot, we can give human-made
    labels to the groups using their positions on the plot:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组的标签是什么？不幸的是，我们没有任何标签。K-means，就像几乎所有的聚类算法一样，只输出无意义的“聚类标签”，通常是整数：1，2，3等。但在这种简单的情况下，我们可以很容易地在散点图上可视化聚类，我们可以使用它们在图上的位置来给这些组赋予人为的标签：
- en: small flipper length and small bill length (orange cluster),
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短鳍长和短喙长（橙色聚类），
- en: small flipper length and large bill length (blue cluster).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短鳍长和长喙长（蓝色聚类）。
- en: and large flipper length and large bill length (yellow cluster).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及长鳍长和长喙长（黄色聚类）。
- en: Once we have made these determinations, we can use them to inform our species
    classifications or ask further questions about our data. For example, we might
    be interested in understanding the relationship between flipper length and bill
    length, and that relationship may differ depending on the type of penguin we have.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们做出了这些决定，我们就可以使用它们来告知我们的物种分类或对我们的数据提出进一步的问题。例如，我们可能对了解鳍长与喙长之间的关系感兴趣，而这种关系可能取决于我们拥有的企鹅类型。
- en: 9.5 K-means
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 K-means
- en: 9.5.1 Measuring cluster quality
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 测量聚类质量
- en: The K-means algorithm is a procedure that groups data into K clusters. It starts
    with an initial clustering of the data, and then iteratively improves it by making
    adjustments to the assignment of data to clusters until it cannot improve any
    further. But how do we measure the “quality” of a clustering, and what does it
    mean to improve it? In K-means clustering, we measure the quality of a cluster
    by its *within-cluster sum-of-squared-distances* (WSSD). Computing this involves
    two steps. First, we find the cluster centers by computing the mean of each variable
    over data points in the cluster. For example, suppose we have a cluster containing
    four observations, and we are using two variables, \(x\) and \(y\), to cluster
    the data. Then we would compute the coordinates, \(\mu_x\) and \(\mu_y\), of the
    cluster center via
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法是一种将数据分组到K个聚类的过程。它从数据的初始聚类开始，然后通过调整数据到聚类的分配来迭代地改进它，直到不能再进一步改进。但是，我们如何衡量聚类的“质量”，以及改进它意味着什么？在K-means聚类中，我们通过聚类的*内部平方和距离总和*（WSSD）来衡量聚类质量。计算这一过程涉及两个步骤。首先，我们通过计算聚类中每个变量的均值来找到聚类中心。例如，假设我们有一个包含四个观察值的聚类，我们使用两个变量\(x\)和\(y\)来聚类数据。然后，我们会计算聚类中心的坐标\(\mu_x\)和\(\mu_y\)：
- en: \[\mu_x = \frac{1}{4}(x_1+x_2+x_3+x_4) \quad \mu_y = \frac{1}{4}(y_1+y_2+y_3+y_4).\]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mu_x = \frac{1}{4}(x_1+x_2+x_3+x_4) \quad \mu_y = \frac{1}{4}(y_1+y_2+y_3+y_4).\]
- en: In the first cluster from the example, there are 4 data points. These are shown
    with their cluster center (standardized flipper length -0.35, standardized bill
    length 0.99) highlighted in Figure [9.4](clustering.html#fig:10-toy-example-clus1-center).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例的第一个集群中，有4个数据点。这些点及其集群中心（标准化鳍长-0.35，标准化喙长0.99）在图[9.4](clustering.html#fig:10-toy-example-clus1-center)中突出显示。
- en: '![Cluster 1 from the penguins_standardized data set example. Observations are
    small blue points, with the cluster center highlighted as a large blue point with
    a black outline.](../Images/f02afd23291ef58592f7296f2f23bac7.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![企鹅标准化数据集示例中的集群1。观测值以小蓝色点表示，集群中心以大蓝色点加黑色轮廓突出显示](../Images/f02afd23291ef58592f7296f2f23bac7.png)'
- en: 'Figure 9.4: Cluster 1 from the `penguins_standardized` data set example. Observations
    are small blue points, with the cluster center highlighted as a large blue point
    with a black outline.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：企鹅`标准化`数据集示例中的集群1。观测值以小蓝色点表示，集群中心以大蓝色点加黑色轮廓突出显示。
- en: The second step in computing the WSSD is to add up the squared distance between
    each point in the cluster and the cluster center. We use the straight-line / Euclidean
    distance formula that we learned about in Chapter [5](classification1.html#classification1).
    In the 4-observation cluster example above, we would compute the WSSD \(S^2\)
    via
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 计算WSSD的第二步是将集群中每个点到集群中心的平方距离相加。我们使用我们在第[5](classification1.html#classification1)章中学到的直线/欧几里得距离公式。在上面的4观测值集群示例中，我们会通过以下方式计算WSSD
    \(S^2\)：
- en: \[\begin{align*} S^2 = \left((x_1 - \mu_x)^2 + (y_1 - \mu_y)^2\right) + \left((x_2
    - \mu_x)^2 + (y_2 - \mu_y)^2\right) + \\ \left((x_3 - \mu_x)^2 + (y_3 - \mu_y)^2\right)
    + \left((x_4 - \mu_x)^2 + (y_4 - \mu_y)^2\right). \end{align*}\]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} S^2 = \left((x_1 - \mu_x)^2 + (y_1 - \mu_y)^2\right) + \left((x_2
    - \mu_x)^2 + (y_2 - \mu_y)^2\right) + \\ \left((x_3 - \mu_x)^2 + (y_3 - \mu_y)^2\right)
    + \left((x_4 - \mu_x)^2 + (y_4 - \mu_y)^2\right). \end{align*}\]
- en: These distances are denoted by lines in Figure [9.5](clustering.html#fig:10-toy-example-clus1-dists)
    for the first cluster of the penguin data example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些距离在图[9.5](clustering.html#fig:10-toy-example-clus1-dists)中用线条表示，这是企鹅数据示例的第一个集群。
- en: '![Cluster 1 from the penguins_standardized data set example. Observations are
    small blue points, with the cluster center highlighted as a large blue point with
    a black outline. The distances from the observations to the cluster center are
    represented as black lines.](../Images/51a0c7cde00f207637200c30905c629d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![企鹅标准化数据集示例中的集群1。观测值以小蓝色点表示，集群中心以大蓝色点加黑色轮廓突出显示。观测值到集群中心的距离以黑色线条表示](../Images/51a0c7cde00f207637200c30905c629d.png)'
- en: 'Figure 9.5: Cluster 1 from the `penguins_standardized` data set example. Observations
    are small blue points, with the cluster center highlighted as a large blue point
    with a black outline. The distances from the observations to the cluster center
    are represented as black lines.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：企鹅`标准化`数据集示例中的集群1。观测值以小蓝色点表示，集群中心以大蓝色点加黑色轮廓突出显示。观测值到集群中心的距离以黑色线条表示。
- en: The larger the value of \(S^2\), the more spread out the cluster is, since large
    \(S^2\) means that points are far from the cluster center. Note, however, that
    “large” is relative to *both* the scale of the variables for clustering *and*
    the number of points in the cluster. A cluster where points are very close to
    the center might still have a large \(S^2\) if there are many data points in the
    cluster.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \(S^2\)的值越大，集群就越分散，因为大的\(S^2\)意味着点离集群中心很远。然而，请注意，“大”是相对于聚类变量的规模*以及*集群中点的数量而言的。如果一个集群中的点非常接近中心，但如果集群中有许多数据点，它仍然可能有一个大的\(S^2\)。
- en: After we have calculated the WSSD for all the clusters, we sum them together
    to get the *total WSSD*. For our example, this means adding up all the squared
    distances for the 18 observations. These distances are denoted by black lines
    in Figure [9.6](clustering.html#fig:10-toy-example-all-clus-dists).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们计算了所有集群的WSSD之后，我们将它们相加以得到*总WSSD*。对于我们的示例，这意味着将18个观测值的平方距离相加。这些距离在图[9.6](clustering.html#fig:10-toy-example-all-clus-dists)中以黑色线条表示。
- en: '![All clusters from the penguins_standardized data set example. Observations
    are small orange, blue, and yellow points with cluster centers denoted by larger
    points with a black outline. The distances from the observations to each of the
    respective cluster centers are represented as black lines.](../Images/012947f886c403f6d4173d7ae9f56027.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![`penguins_standardized`数据集示例中的所有聚类。观测值用小橙色、蓝色和黄色点表示，聚类中心由黑色轮廓的大点表示。观测值到各自聚类中心的距离用黑色线条表示。](../Images/012947f886c403f6d4173d7ae9f56027.png)'
- en: 'Figure 9.6: All clusters from the `penguins_standardized` data set example.
    Observations are small orange, blue, and yellow points with cluster centers denoted
    by larger points with a black outline. The distances from the observations to
    each of the respective cluster centers are represented as black lines.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：`penguins_standardized`数据集示例中的所有聚类。观测值用小橙色、蓝色和黄色点表示，聚类中心由黑色轮廓的大点表示。观测值到各自聚类中心的距离用黑色线条表示。
- en: Since K-means uses the straight-line distance to measure the quality of a clustering,
    it is limited to clustering based on quantitative variables. However, note that
    there are variants of the K-means algorithm, as well as other clustering algorithms
    entirely, that use other distance metrics to allow for non-quantitative data to
    be clustered. These are beyond the scope of this book.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于K-means使用直线距离来衡量聚类的质量，因此它仅限于基于定量变量的聚类。然而，请注意，K-means算法的变体以及其他一些聚类算法完全使用其他距离度量，以便对非定量数据进行聚类。这些内容超出了本书的范围。
- en: 9.5.2 The clustering algorithm
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 聚类算法
- en: We begin the K-means algorithm by picking K, and randomly assigning a roughly
    equal number of observations to each of the K clusters. An example random initialization
    is shown in Figure [9.7](clustering.html#fig:10-toy-kmeans-init).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始K-means算法时选择K，并将大致相等数量的观测值随机分配给K个聚类中的每一个。一个示例随机初始化如图[9.7](clustering.html#fig:10-toy-kmeans-init)所示。
- en: '![Random initialization of labels.](../Images/e352ef7b753d70195dd99c27ad8471b4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![标签的随机初始化。](../Images/e352ef7b753d70195dd99c27ad8471b4.png)'
- en: 'Figure 9.7: Random initialization of labels.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：标签的随机初始化。
- en: 'Then K-means consists of two major steps that attempt to minimize the sum of
    WSSDs over all the clusters, i.e., the *total WSSD*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，K-means由两个主要步骤组成，这两个步骤试图最小化所有聚类上的WSSD总和，即*总WSSD*：
- en: '**Center update:** Compute the center of each cluster.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**中心更新：** 计算每个聚类的中心。'
- en: '**Label update:** Reassign each data point to the cluster with the nearest
    center.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标签更新：** 将每个数据点重新分配到最近的中心所在的聚类。'
- en: These two steps are repeated until the cluster assignments no longer change.
    We show what the first four iterations of K-means would look like in Figure [9.8](clustering.html#fig:10-toy-kmeans-iter).
    There each pair of plots in each row corresponds to an iteration, where the left
    figure in the pair depicts the center update, and the right figure in the pair
    depicts the label update (i.e., the reassignment of data to clusters).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤会重复进行，直到聚类分配不再改变。我们在图[9.8](clustering.html#fig:10-toy-kmeans-iter)中展示了K-means的前四次迭代的样子。在每一行中，每对图表对应一个迭代，其中这对图表中的左图描述了中心更新，而这对图表中的右图描述了标签更新（即数据重新分配到聚类中）。
- en: '![First four iterations of K-means clustering on the penguins_standardized
    example data set. Each pair of plots corresponds to an iteration. Within the pair,
    the first plot depicts the center update, and the second plot depicts the reassignment
    of data to clusters. Cluster centers are indicated by larger points that are outlined
    in black.](../Images/aa0c7fab33c23e145b2dc2b7451fc95a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![在`penguins_standardized`示例数据集上进行的K-means聚类的前四次迭代。每一对图表对应一个迭代。在每一对图表中，第一个图表描述了中心更新，第二个图表描述了数据重新分配到聚类中。聚类中心由黑色轮廓的大点表示。](../Images/aa0c7fab33c23e145b2dc2b7451fc95a.png)'
- en: 'Figure 9.8: First four iterations of K-means clustering on the `penguins_standardized`
    example data set. Each pair of plots corresponds to an iteration. Within the pair,
    the first plot depicts the center update, and the second plot depicts the reassignment
    of data to clusters. Cluster centers are indicated by larger points that are outlined
    in black.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：在`penguins_standardized`示例数据集上进行的K-means聚类的前四次迭代。每一对图表对应一个迭代。在每一对图表中，第一个图表描述了中心更新，第二个图表描述了数据重新分配到聚类中。聚类中心由黑色轮廓的大点表示。
- en: Note that at this point, we can terminate the algorithm since none of the assignments
    changed in the fourth iteration; both the centers and labels will remain the same
    from this point onward.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个时候，我们可以终止算法，因为第四次迭代中没有任何分配发生变化；从这一点开始，中心和标签都将保持不变。
- en: '**Note:** Is K-means *guaranteed* to stop at some point, or could it iterate
    forever? As it turns out, thankfully, the answer is that K-means is guaranteed
    to stop after *some* number of iterations. For the interested reader, the logic
    for this has three steps: (1) both the label update and the center update decrease
    total WSSD in each iteration, (2) the total WSSD is always greater than or equal
    to 0, and (3) there are only a finite number of possible ways to assign the data
    to clusters. So at some point, the total WSSD must stop decreasing, which means
    none of the assignments are changing, and the algorithm terminates.'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** K-means是否**保证**在某一点停止，或者可能会无限迭代？实际上，幸运的是，答案是K-means保证在*某些*迭代次数后停止。对于感兴趣的读者，这个逻辑有三个步骤：（1）标签更新和中心更新在每个迭代中都会减少总WSSD，（2）总WSSD始终大于或等于0，（3）将数据分配到聚类的可能方式只有有限种。所以，在某个点上，总WSSD必须停止减少，这意味着没有任何分配在改变，算法终止。'
- en: 9.5.3 Random restarts
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.3 随机重启
- en: Unlike the classification and regression models we studied in previous chapters,
    K-means can get “stuck” in a bad solution. For example, Figure [9.9](clustering.html#fig:10-toy-kmeans-bad-init)
    illustrates an unlucky random initialization by K-means.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前几章中研究的分类和回归模型不同，K-means可能会陷入一个不良的解。例如，图[9.9](clustering.html#fig:10-toy-kmeans-bad-init)展示了K-means不幸的随机初始化。
- en: '![Random initialization of labels.](../Images/2e3267fca078082988a01ee94f3b45f1.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![标签的随机初始化。](../Images/2e3267fca078082988a01ee94f3b45f1.png)'
- en: 'Figure 9.9: Random initialization of labels.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：标签的随机初始化。
- en: Figure [9.10](clustering.html#fig:10-toy-kmeans-bad-iter) shows what the iterations
    of K-means would look like with the unlucky random initialization shown in Figure
    [9.9](clustering.html#fig:10-toy-kmeans-bad-init).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9.10](clustering.html#fig:10-toy-kmeans-bad-iter)显示了K-means迭代的外观，其中包含图[9.9](clustering.html#fig:10-toy-kmeans-bad-init)中显示的不幸随机初始化。
- en: '![First five iterations of K-means clustering on the penguins_standardized
    example data set with a poor random initialization. Each pair of plots corresponds
    to an iteration. Within the pair, the first plot depicts the center update, and
    the second plot depicts the reassignment of data to clusters. Cluster centers
    are indicated by larger points that are outlined in black.](../Images/8696cc629e0ebbd8fdb26406de1a95a8.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![penguins_standardized示例数据集上K-means聚类的第一次五次迭代，具有较差的随机初始化。每对图对应一个迭代。在每对图中，第一个图描述了中心更新，第二个图描述了数据重新分配到聚类。聚类中心由黑色轮廓的大点表示。](../Images/8696cc629e0ebbd8fdb26406de1a95a8.png)'
- en: 'Figure 9.10: First five iterations of K-means clustering on the `penguins_standardized`
    example data set with a poor random initialization. Each pair of plots corresponds
    to an iteration. Within the pair, the first plot depicts the center update, and
    the second plot depicts the reassignment of data to clusters. Cluster centers
    are indicated by larger points that are outlined in black.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：penguins_standardized示例数据集上K-means聚类的第一次五次迭代，具有较差的随机初始化。每对图对应一个迭代。在每对图中，第一个图描述了中心更新，第二个图描述了数据重新分配到聚类。聚类中心由黑色轮廓的大点表示。
- en: This looks like a relatively bad clustering of the data, but K-means cannot
    improve it. To solve this problem when clustering data using K-means, we should
    randomly re-initialize the labels a few times, run K-means for each initialization,
    and pick the clustering that has the lowest final total WSSD.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像是数据的一个相对较差的聚类，但K-means无法改进它。在使用K-means聚类数据时解决此问题，我们应该随机重新初始化标签几次，为每次初始化运行K-means，并选择具有最低最终总WSSD的聚类。
- en: 9.5.4 Choosing K
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.4 选择K
- en: In order to cluster data using K-means, we also have to pick the number of clusters,
    K. But unlike in classification, we have no response variable and cannot perform
    cross-validation with some measure of model prediction error. Further, if K is
    chosen too small, then multiple clusters get grouped together; if K is too large,
    then clusters get subdivided. In both cases, we will potentially miss interesting
    structure in the data. Figure [9.11](clustering.html#fig:10-toy-kmeans-vary-k)
    illustrates the impact of K on K-means clustering of our penguin flipper and bill
    length data by showing the different clusterings for K’s ranging from 1 to 9.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用K-means聚类数据，我们还必须选择聚类数量K。但在分类中不同，我们没有响应变量，也不能使用交叉验证和一些模型预测误差的度量来执行。此外，如果K选择得太小，那么多个聚类会被分组在一起；如果K太大，那么聚类会被细分。在这两种情况下，我们可能会错过数据中的有趣结构。图[9.11](clustering.html#fig:10-toy-kmeans-vary-k)通过显示K从1到9的不同聚类来说明了K对我们企鹅鳍长和喙长数据的K-means聚类的影响。
- en: '![Clustering of the penguin data for K clusters ranging from 1 to 9\. Cluster
    centers are indicated by larger points that are outlined in black.](../Images/d220603f7561e4813be2c04b7dd738ee.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![K从1到9的企鹅数据聚类图。聚类中心由黑色轮廓的大点表示。](../Images/d220603f7561e4813be2c04b7dd738ee.png)'
- en: 'Figure 9.11: Clustering of the penguin data for K clusters ranging from 1 to
    9\. Cluster centers are indicated by larger points that are outlined in black.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：K从1到9的企鹅数据聚类图。聚类中心由黑色轮廓的大点表示。
- en: If we set K less than 3, then the clustering merges separate groups of data;
    this causes a large total WSSD, since the cluster center is not close to any of
    the data in the cluster. On the other hand, if we set K greater than 3, the clustering
    subdivides subgroups of data; this does indeed still decrease the total WSSD,
    but by only a *diminishing amount*. If we plot the total WSSD versus the number
    of clusters, we see that the decrease in total WSSD levels off (or forms an “elbow
    shape”) when we reach roughly the right number of clusters (Figure [9.12](clustering.html#fig:10-toy-kmeans-elbow)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将K设置为小于3，那么聚类将合并分离的数据组；这会导致较大的总WSSD，因为聚类中心不接近聚类中的任何数据点。另一方面，如果我们设置K大于3，聚类将细分数据子组；这确实仍然会减少总WSSD，但只是减少的幅度很小。如果我们绘制总WSSD与聚类数量的关系图，我们会看到当达到大约正确的聚类数量时，总WSSD的减少趋于平稳（或形成“肘部形状”）（见图[9.12](clustering.html#fig:10-toy-kmeans-elbow)）。
- en: '![Total WSSD for K clusters ranging from 1 to 9.](../Images/5eeece9ba6160364818c383b43971119.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![K从1到9的K聚类总WSSD图](../Images/5eeece9ba6160364818c383b43971119.png)'
- en: 'Figure 9.12: Total WSSD for K clusters ranging from 1 to 9.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：K从1到9的总WSSD。
- en: 9.6 K-means in R
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 R中的K-means
- en: We can perform K-means clustering in R using a `tidymodels` workflow similar
    to those in the earlier classification and regression chapters. We will begin
    by loading the `tidyclust` library, which contains the necessary functionality.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与早期分类和回归章节中类似的`tidymodels`工作流程在R中执行K-means聚类。我们首先加载包含必要功能的`tidyclust`库。
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Returning to the original (unstandardized) `penguins` data, recall that K-means
    clustering uses straight-line distance to decide which points are similar to each
    other. Therefore, the *scale* of each of the variables in the data will influence
    which cluster data points end up being assigned. Variables with a large scale
    will have a much larger effect on deciding cluster assignment than variables with
    a small scale. To address this problem, we need to create a recipe that standardizes
    our data before clustering using the `step_scale` and `step_center` preprocessing
    steps. Standardization will ensure that each variable has a mean of 0 and standard
    deviation of 1 prior to clustering. We will designate that all variables are to
    be used in clustering via the model formula `~ .`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到原始（未标准化的）`penguins`数据，回想一下K-means聚类使用直线距离来决定哪些点彼此相似。因此，数据中每个变量的*尺度*将影响哪些聚类数据点被分配。尺度大的变量在决定聚类分配时比尺度小的变量有更大的影响。为了解决这个问题，我们需要创建一个配方，在聚类之前使用`step_scale`和`step_center`预处理步骤标准化我们的数据。标准化将确保在聚类之前每个变量的平均值是0，标准差是1。我们将指定所有变量都通过模型公式`~
    .`用于聚类。
- en: '**Note:** Recipes were originally designed specifically for *predictive* data
    analysis problems—like classification and regression—not clustering problems.
    So the functions in R that we use to construct recipes are a little bit awkward
    in the setting of clustering In particular, we will have to treat “predictors”
    here as if it meant “variables to be used in clustering”. So the model formula
    `~ .` specifies that all variables are “predictors”, i.e., all variables should
    be used for clustering. Similarly, when we use the `all_predictors()` function
    in the preprocessing steps, we really mean “apply this step to all variables used
    for clustering.”'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**配方最初是为特定的*预测性*数据分析问题设计的——如分类和回归，而不是聚类问题。因此，我们在R中使用来构建配方的函数在聚类环境中有点不自然。特别是，我们将在这里将“predictors”视为“用于聚类的变量”。因此，模型公式`~
    .`指定所有变量都是“predictors”，即所有变量都应用于聚类。同样，当我们使用`all_predictors()`函数在预处理步骤中时，我们真正意味着“将此步骤应用于用于聚类的所有变量。”'
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To indicate that we are performing K-means clustering, we will use the `k_means`
    model specification. We will use the `num_clusters` argument to specify the number
    of clusters (here we choose K = 3), and specify that we are using the `"stats"`
    engine.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表明我们正在进行K-means聚类，我们将使用`k_means`模型规范。我们将使用`num_clusters`参数来指定聚类数（在这里我们选择K
    = 3），并指定我们使用的是`"stats"`引擎。
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To actually run the K-means clustering, we combine the recipe and model specification
    in a workflow, and use the `fit` function. Note that the K-means algorithm uses
    a random initialization of assignments; but since we set the random seed earlier,
    the clustering will be reproducible.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要实际运行K-means聚类，我们将配方和模型规范组合在一个工作流程中，并使用`fit`函数。请注意，K-means算法使用随机初始化的分配；但由于我们之前设置了随机种子，聚类将是可重复的。
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see above, the fit object has a lot of information that can be used
    to visualize the clusters, pick K, and evaluate the total WSSD. Let’s start by
    visualizing the clusters as a colored scatter plot! In order to do that, we first
    need to augment our original data frame with the cluster assignments. We can achieve
    this using the `augment` function from `tidyclust`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，拟合对象包含大量可用于可视化聚类、选择K值和评估总WSSD的信息。让我们先从将聚类可视化为一个彩色散点图开始！为了做到这一点，我们首先需要使用`tidyclust`中的`augment`函数增强我们的原始数据框。我们可以通过这种方式实现。
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that we have the cluster assignments included in the `clustered_data` tidy
    data frame, we can visualize them as shown in Figure [9.13](clustering.html#fig:10-plot-clusters-2).
    Note that we are plotting the *un-standardized* data here; if we for some reason
    wanted to visualize the *standardized* data from the recipe, we would need to
    use the `bake` function to obtain that first.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将聚类分配包含在`clustered_data`整洁数据框中，我们可以像图[9.13](clustering.html#fig:10-plot-clusters-2)中所示的那样可视化它们。请注意，我们在这里绘制的是*未标准化的*数据；如果我们出于某种原因想要可视化配方中的*标准化*数据，我们需要使用`bake`函数来首先获取它。
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![The data colored by the cluster assignments returned by K-means.](../Images/deaa07481c71253f123210823524aa90.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![由K-means返回的聚类分配着色的数据。](../Images/deaa07481c71253f123210823524aa90.png)'
- en: 'Figure 9.13: The data colored by the cluster assignments returned by K-means.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13：由K-means返回的聚类分配着色的数据。
- en: As mentioned above, we also need to select K by finding where the “elbow” occurs
    in the plot of total WSSD versus the number of clusters. We can obtain the total
    WSSD (`tot.withinss`) from our clustering with 3 clusters using the `glance` function.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们还需要通过在总WSSD与聚类数量之间的图中找到“肘部”来选择K值。我们可以使用`glance`函数从具有3个聚类的聚类中获得总WSSD（`tot.withinss`）。
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To calculate the total WSSD for a variety of Ks, we will create a data frame
    with a column named `num_clusters` with rows containing each value of K we want
    to run K-means with (here, 1 to 9).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算各种K值的总WSSD，我们将创建一个包含名为`num_clusters`的列的数据框，其中包含我们想要运行K-means的每个K值的行（在这里，1到9）。
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Then we construct our model specification again, this time specifying that we
    want to tune the `num_clusters` parameter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们再次构建我们的模型规范，这次指定我们想要调整`num_clusters`参数。
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We combine the recipe and specification in a workflow, and then use the `tune_cluster`
    function to run K-means on each of the different settings of `num_clusters`. The
    `grid` argument controls which values of K we want to try—in this case, the values
    from 1 to 9 that are stored in the `penguin_clust_ks` data frame. We set the `resamples`
    argument to `apparent(penguins)` to tell K-means to run on the whole data set
    for each value of `num_clusters`. Finally, we collect the results using the `collect_metrics`
    function.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在流程中将配方和规范结合起来，然后使用 `tune_cluster` 函数在 `num_clusters` 的不同设置上运行 K-means。`grid`
    参数控制我们想要尝试的 K 的值——在这种情况下，存储在 `penguin_clust_ks` 数据框中的 1 到 9 的值。我们将 `resamples`
    参数设置为 `apparent(penguins)`，以告诉 K-means 对每个 `num_clusters` 的值在整个数据集上运行。最后，我们使用
    `collect_metrics` 函数收集结果。
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The total WSSD results correspond to the `mean` column when the `.metric` variable
    is equal to `sse_within_total`. We can obtain a tidy data frame with this information
    using `filter` and `mutate`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 总 WSSD 结果对应于当 `.metric` 变量等于 `sse_within_total` 时的 `mean` 列。我们可以使用 `filter`
    和 `mutate` 获得包含此信息的整洁数据框。
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that we have `total_WSSD` and `num_clusters` as columns in a data frame,
    we can make a line plot (Figure [9.14](clustering.html#fig:10-plot-choose-k))
    and search for the “elbow” to find which value of K to use.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有 `total_WSSD` 和 `num_clusters` 作为数据框中的列，我们可以制作一个折线图（图 [9.14](clustering.html#fig:10-plot-choose-k)）并寻找“肘部”以确定使用哪个
    K 的值。
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![A plot showing the total WSSD versus the number of clusters.](../Images/da50d2295bc82d58a7747896b7bf800a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![显示总 WSSD 与簇数关系的图。](../Images/da50d2295bc82d58a7747896b7bf800a.png)'
- en: 'Figure 9.14: A plot showing the total WSSD versus the number of clusters.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：显示总 WSSD 与簇数关系的图。
- en: 'It looks like 3 clusters is the right choice for this data. But why is there
    a “bump” in the total WSSD plot here? Shouldn’t total WSSD always decrease as
    we add more clusters? Technically yes, but remember: K-means can get “stuck” in
    a bad solution. Unfortunately, for K = 8 we had an unlucky initialization and
    found a bad clustering! We can help prevent finding a bad clustering by trying
    a few different random initializations via the `nstart` argument in the model
    specification. Here we will try using 10 restarts.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来对于这些数据，3 个簇是合适的选择。但为什么在总 WSSD 图中这里会有一个“峰值”？总 WSSD 不应该随着我们添加更多簇而总是减少吗？从技术上讲是的，但请记住：K-means
    可能会陷入一个不良的解。不幸的是，对于 K = 8，我们有一个不幸的初始化，发现了一个不良的聚类！我们可以通过在模型指定中的 `nstart` 参数尝试几个不同的随机初始化来帮助防止找到不良聚类。在这里，我们将尝试使用
    10 次重启。
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now if we rerun the same workflow with the new model specification, K-means
    clustering will be performed `nstart = 10` times for each value of K. The `collect_metrics`
    function will then pick the best clustering of the 10 runs for each value of K,
    and report the results for that best clustering. Figure [9.15](clustering.html#fig:10-choose-k-nstart)
    shows the resulting total WSSD plot from using 10 restarts; the bump is gone and
    the total WSSD decreases as expected. The more times we perform K-means clustering,
    the more likely we are to find a good clustering (if one exists). What value should
    you choose for `nstart`? The answer is that it depends on many factors: the size
    and characteristics of your data set, as well as how powerful your computer is.
    The larger the `nstart` value the better from an analysis perspective, but there
    is a trade-off that doing many clusterings could take a long time. So this is
    something that needs to be balanced.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们用新的模型指定重新运行相同的流程，K-means 聚类将针对每个 K 的值执行 `nstart = 10` 次。然后，`collect_metrics`
    函数将为每个 K 的值选择 10 次运行中的最佳聚类，并报告该最佳聚类的结果。图 [9.15](clustering.html#fig:10-choose-k-nstart)
    显示了使用 10 次重启得到的总 WSSD 图；峰值消失了，总 WSSD 如预期那样减少。我们执行 K-means 聚类的次数越多，找到良好聚类（如果存在的话）的可能性就越大。你应该为
    `nstart` 选择什么值呢？答案是这取决于许多因素：数据集的大小和特征，以及你的计算机有多强大。从分析的角度来看，`nstart` 值越大越好，但这也存在一个权衡，即进行多次聚类可能会花费很长时间。所以这是需要权衡的事情。
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![A plot showing the total WSSD versus the number of clusters when K-means
    is run with 10 restarts.](../Images/498f76cd33ff5f37dd4d0de5ae978fa1.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![当 K-means 使用 10 次重启运行时，显示总 WSSD 与簇数关系的图。](../Images/498f76cd33ff5f37dd4d0de5ae978fa1.png)'
- en: 'Figure 9.15: A plot showing the total WSSD versus the number of clusters when
    K-means is run with 10 restarts.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：当 K-means 使用 10 次重启运行时，显示总 WSSD 与簇数关系的图。
- en: 9.7 Exercises
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 练习
- en: Practice exercises for the material covered in this chapter can be found in
    the accompanying [worksheets repository](https://worksheets.datasciencebook.ca)
    in the “Clustering” row. You can launch an interactive version of the worksheet
    in your browser by clicking the “launch binder” button. You can also preview a
    non-interactive version of the worksheet by clicking “view worksheet.” If you
    instead decide to download the worksheet and run it on your own machine, make
    sure to follow the instructions for computer setup found in Chapter [13](setup.html#setup).
    This will ensure that the automated feedback and guidance that the worksheets
    provide will function as intended.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的练习题可以在配套的[工作表存储库](https://worksheets.datasciencebook.ca)中找到，位于“聚类”行。您可以通过点击“启动绑定器”按钮在浏览器中启动工作表的交互式版本。您还可以通过点击“查看工作表”预览非交互式版本的工作表。如果您决定下载工作表并在自己的机器上运行，请确保遵循第[13](setup.html#setup)章中找到的计算机设置说明。这将确保工作表提供的自动反馈和指导能够按预期工作。
- en: 9.8 Additional resources
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.8 其他资源
- en: Chapter 10 of *An Introduction to Statistical Learning* ([James et al. 2013](#ref-james2013introduction))
    provides a great next stop in the process of learning about clustering and unsupervised
    learning in general. In the realm of clustering specifically, it provides a great
    companion introduction to K-means, but also covers *hierarchical* clustering for
    when you expect there to be subgroups, and then subgroups within subgroups, etc.,
    in your data. In the realm of more general unsupervised learning, it covers *principal
    components analysis (PCA)*, which is a very popular technique for reducing the
    number of predictors in a data set.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*An Introduction to Statistical Learning* ([James et al. 2013](#ref-james2013introduction))的第10章是学习聚类和一般无监督学习过程的绝佳下一站。在聚类的特定领域，它提供了对K-means的绝佳介绍，同时也涵盖了当你预期数据中存在子组，以及子组内的子组等时，如何进行*层次聚类*。在更广泛的无监督学习领域，它涵盖了*主成分分析（PCA）*，这是一种非常流行的减少数据集中预测变量数量的技术。'
- en: References
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Gorman, Kristen, Tony Williams, and William Fraser. 2014\. “Ecological Sexual
    Dimorphism and Environmental Variability Within a Community of Antarctic Penguins
    (Genus Pygoscelis).” *PLoS ONE* 9 (3).Horst, Allison, Alison Hill, and Kristen
    Gorman. 2020\. *palmerpenguins: Palmer Archipelago Penguin Data*. [https://allisonhorst.github.io/palmerpenguins/](https://allisonhorst.github.io/palmerpenguins/).James,
    Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013\. *An Introduction
    to Statistical Learning*. 1st ed. Springer. [https://www.statlearning.com/](https://www.statlearning.com/).Lloyd,
    Stuart. 1982\. “Least Square Quantization in PCM.” *IEEE Transactions on Information
    Theory* 28 (2): 129–37.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gorman, Kristen, Tony Williams, 和 William Fraser. 2014. “南极企鹅（Pygoscelis属）群体中的生态性性别二态性和环境可变性。”
    *PLoS ONE* 9 (3)。Horst, Allison, Alison Hill, 和 Kristen Gorman. 2020. *palmerpenguins:
    Palmer Archipelago Penguin Data*。[https://allisonhorst.github.io/palmerpenguins/](https://allisonhorst.github.io/palmerpenguins/)。James,
    Gareth, Daniela Witten, Trevor Hastie, 和 Robert Tibshirani. 2013. *An Introduction
    to Statistical Learning*。第1版。Springer。[https://www.statlearning.com/](https://www.statlearning.com/)。Lloyd,
    Stuart. 1982. “PCM中的最小二乘量化。” *IEEE Transactions on Information Theory* 28 (2):
    129–37。'
