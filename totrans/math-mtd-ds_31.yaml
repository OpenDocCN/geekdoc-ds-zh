- en: '4.5\. Application: principal components analysis#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html](https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We discuss an application to principal components analysis and revisit our genetic
    dataset from ealier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1\. Dimensionality reduction via principal components analysis (PCA)[#](#dimensionality-reduction-via-principal-components-analysis-pca
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Principal components analysis (PCA)\(\idx{principal components analysis}\xdi\)
    is a commonly used dimensionality reduction approach that is closely related to
    what we described in the previous sections. We formalize the connection.
  prefs: []
  type: TYPE_NORMAL
- en: '*The data matrix:* In PCA we are given \(n\) data points \(\mathbf{x}_1,\ldots,\mathbf{x}_n
    \in \mathbb{R}^p\) with \(p\) features (i.e., coordinates). We denote the components
    of \(\mathbf{x}_i\) as \((x_{i1},\ldots,x_{ip})\). As usual, we stack them up
    into a matrix \(X\) whose \(i\)-th row is \(\mathbf{x}_i^T\).'
  prefs: []
  type: TYPE_NORMAL
- en: The first step of PCA is to center the data, i.e., we assume that\(\idx{mean
    centering}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Put differently, the empirical mean of each column is \(0\). Quoting [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)
    (and this will become clearer below):'
  prefs: []
  type: TYPE_NORMAL
- en: Mean subtraction (a.k.a. “mean centering”) is necessary for performing classical
    PCA to ensure that the first principal component describes the direction of maximum
    variance. If mean subtraction is not performed, the first principal component
    might instead correspond more or less to the mean of the data. A mean of zero
    is needed for finding a basis that minimizes the mean square error of the approximation
    of the data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An optional step is to divide each column by the square root of its [sample
    variance](https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance), i.e.,
    assume that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p. \]
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in a previous chapter, this is particularly important when the
    features are measured in different units to ensure that their variability can
    be meaningfully compared.
  prefs: []
  type: TYPE_NORMAL
- en: '*The first principal component:* The first principal component is the linear
    combination of the features'
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip} \]
  prefs: []
  type: TYPE_NORMAL
- en: with largest sample variance. For this to make sense, we need to constrain the
    \(\phi_{j1}\)s. Specifically, we require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^p \phi_{j1}^2 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: The \(\phi_{j1}\)s are referred to as the *loadings* and the \(t_{i1}\)s are
    referred to as the *scores*.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we seek to solve
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\
    :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the fact that the \(t_{i1}\)s are centered
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{1}{n} \sum_{i=1}^n t_{i1} &= \frac{1}{n} \sum_{i=1}^n
    [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\ &= \phi_{11} \frac{1}{n} \sum_{i=1}^n
    x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n x_{ip}\\ &= 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: to compute their sample variance as the mean of their square
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p
    \phi_{j1} x_{ij}\right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\) and \(\mathbf{t}_1
    = (t_{11},\ldots,t_{n1})\). Then for all \(i\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1, \]
  prefs: []
  type: TYPE_NORMAL
- en: or in vector form
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{t}_1 = X \boldsymbol{\phi}_1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Also
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \|\mathbf{t}_1\|^2 = \frac{1}{n-1}
    \|X \boldsymbol{\phi}_1\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting the maximization problem above in vector form,
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}, \]'
  prefs: []
  type: TYPE_NORMAL
- en: we see that we have already encountered this problem (up to the factor of \(1/(n-1)\)
    which does not affect the solution). The solution is to take \(\boldsymbol{\phi}_1\)
    to be the top right singular vector of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)).
    As we know this is equivalent to computing the top eigenvector of the matrix \(\frac{1}{n-1}
    X^T X\), which is the sample covariance matrix of the data (accounting for the
    fact that the data is already centered).
  prefs: []
  type: TYPE_NORMAL
- en: '*The second principal component:* The second principal component is the linear
    combination of the features'
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{i2} = \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip} \]
  prefs: []
  type: TYPE_NORMAL
- en: with largest sample variance that is also uncorrelated with the first principal
    component, in the sense that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The next lemma shows how to deal with this condition. Again, we also require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^p \phi_{j2}^2 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: As before, let \(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\) and \(\mathbf{t}_2
    = (t_{12},\ldots,t_{n2})\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Uncorrelated Principal Components)** Assume \(X \neq \mathbf{0}\).
    Let \(t_{i1}\), \(t_{i2}\), \(\boldsymbol{\phi}_1\), \(\boldsymbol{\phi}_2\) be
    as above (where, in particular, \(\boldsymbol{\phi}_1\) is a top right singular
    vector of \(X\)). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: holds if and only if
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The condition'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we dropped the \(1/(n-1)\) factor as it does not play any role. Using
    that \(\mathbf{t}_1 = X \boldsymbol{\phi}_1\), and similarly, \(\mathbf{t}_2 =
    X \boldsymbol{\phi}_2\), this is in turn equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(\boldsymbol{\phi}_1\) can be chosen as a top right singular vector
    in an SVD of \(X\), it follows from the *SVD Relations* that \(X^T X \boldsymbol{\phi}_1
    = \sigma_1^2 \boldsymbol{\phi}_1\), where \(\sigma_1\) is the singular value associated
    to \(\boldsymbol{\phi}_1\). Since \(X \neq 0\), \(\sigma_1 > 0\). Plugging this
    in the inner product on the left hand side above, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
    &= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\ &= (X \boldsymbol{\phi}_2)^T
    (X \boldsymbol{\phi}_1)\\ &= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
    &= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\ &= \langle \boldsymbol{\phi}_2,
    \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\ &= \sigma_1^2 \langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(\sigma_1 \neq 0\), this is \(0\) if and only if \(\langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle = 0\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we can write the maximization problem for the second principal
    component in matrix form as
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2
    = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: Again, we see that we have encountered this problem before. The solution is
    to take \(\boldsymbol{\phi}_2\) to be a second right singular vector in an SVD
    of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)). Again, this is equivalent to computing
    the second eigenvector of the sample covariance matrix \(\frac{1}{n-1} X^T X\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Further principal components:* We can proceed in a similar fashion and define
    further principal components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To quote [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations):'
  prefs: []
  type: TYPE_NORMAL
- en: PCA essentially rotates the set of points around their mean in order to align
    with the principal components. This moves as much of the variance as possible
    (using an orthogonal transformation) into the first few dimensions. The values
    in the remaining dimensions, therefore, tend to be small and may be dropped with
    minimal loss of information […] PCA is often used in this manner for dimensionality
    reduction. PCA has the distinction of being the optimal orthogonal transformation
    for keeping the subspace that has largest “variance” […]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Formally, let
  prefs: []
  type: TYPE_NORMAL
- en: \[ X = U \Sigma V^T \]
  prefs: []
  type: TYPE_NORMAL
- en: be the SVD of the data matrix \(X\). The principal component transformation,
    truncated at the \(\ell\)-th component, is
  prefs: []
  type: TYPE_NORMAL
- en: \[ T = X V_{(\ell)}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(T\) is the matrix whose columns are the vectors \(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\).
    Recall that \(V_{(\ell)}\) is the matrix made of the first \(k\) columns of \(V\).
  prefs: []
  type: TYPE_NORMAL
- en: Then, using the orthonormality of the right singular vectors,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} T = U \Sigma V^T V_{(\ell)} = U \Sigma \begin{bmatrix} I_{\ell
    \times \ell}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U \begin{bmatrix}\Sigma_{(\ell)}\\
    \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U_{(\ell)} \Sigma_{(\ell)}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, the vector \(\mathbf{t}_i\) is the left singular vector \(\mathbf{u}_i\)
    scaled by the corresponding singular value \(\sigma_i\).
  prefs: []
  type: TYPE_NORMAL
- en: Having established a formal connection between PCA and SVD, we implement PCA
    using the SVD algorithm [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html).
    We perform mean centering (now is the time to read that quote about the importance
    of mean centering again), but not the optional standardization. We use the fact
    that, in NumPy, subtracting a matrix by a vector whose dimension matches the number
    of columns performs row-wise subtraction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We apply it to the Gaussian Mixture Model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Plotting the result, we see that PCA does succeed in finding the main direction
    of variation. Note tha gap in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png](../Images/0ad434c45a764ec31a02581fa80c1fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Note however that the first two principal components in fact “capture more noise”
    than what can be seen in the orginal first two coordinates, a form of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**TRY IT!** Compute the first two right singular vectors \(\mathbf{v}_1\) and
    \(\mathbf{v}_2\) of \(X\) after mean centering. Do they align well with the first
    and second standard basis vectors \(\mathbf{e}_1\) and \(\mathbf{e}_2\)? Why or
    why not? ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We return to our motivating example. We apply PCA to our genetic dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Viruses (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Viruses](../Images/03664e03f12993cb6edd6189d59b79e1.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We load the dataset again. Recall that it contains \(1642\)
    strains and lives in a \(317\)-dimensional space.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Our goal is to find a “good” low-dimensional representation of the data. We
    work with ten dimensions using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We plot the first two principal components, and see what appears to be some
    potential structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png](../Images/f60d8d2f8537bfd4a76822ba9228e316.png)'
  prefs: []
  type: TYPE_IMG
- en: There seems to be some reasonably well-defined clusters in this projection.
    We use \(k\)-means to identiy clusters. We take advantage of the implementation
    in scikit-learn, [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).
    By default, it finds \(8\) clusters. The clusters can be extracted from the attribute
    `labels_`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To further reveal the structure, we look at our the clusters spread out over
    the years. That information is in a separate file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '|  | strain | length | country | year | lon | lat | date |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | AB434107 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/02/25
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | AB434108 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/03/01
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | CY000113 | 1762 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/29 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | CY000209 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/17 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | CY000217 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/02/26 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For each cluster, we plot how many of its data points come from a specific year.
    Each cluster has a different color.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png](../Images/1decff267b5cfeee392565099c5a1864.png)'
  prefs: []
  type: TYPE_IMG
- en: Remarkably, we see that each cluster comes mostly from one year or two consecutive
    ones. In other words, the clustering in this low-dimensional projection captures
    some true underlying structure that is not explicitly in the genetic data on which
    it is computed.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the first two principal components, we color the points on the
    scatterplot by year. (We use [`legend_elements()`](https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements)
    for automatic legend creation.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png](../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png)'
  prefs: []
  type: TYPE_IMG
- en: To some extent, one can “see” the virus evolving from year to year. The \(x\)-axis
    in particular seems to correlate strongly with the year, in the sense that samples
    from later years tend to be towards one side of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: To further quantify this observation, we use [`numpy.corrcoef`](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)
    to compute the correlation coefficients between the year and the first \(10\)
    principal components.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, we see that the first three or four principal components correlate well
    with the year.
  prefs: []
  type: TYPE_NORMAL
- en: Using [related techniques](https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8),
    one can also identify which mutations distinguish different epidemics (i.e., years).
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the difference between
    principal components analysis (PCA) and linear discriminant analysis (LDA). \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the goal of principal components analysis (PCA)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To find clusters in the data.
  prefs: []
  type: TYPE_NORMAL
- en: b) To find a low-dimensional representation of the data that captures the maximum
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: c) To find the mean of each feature in the data.
  prefs: []
  type: TYPE_NORMAL
- en: d) To find the correlation between features in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Formally, the first principal component is the linear combination of
    features \(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\) that solves which optimization
    problem?'
  prefs: []
  type: TYPE_NORMAL
- en: 'a) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: 'b) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: 'c) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: 'd) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**3** What is the relationship between the loadings in PCA and the singular
    vectors of the data matrix?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The loadings are the left singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: b) The loadings are the right singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: c) The loadings are the singular values.
  prefs: []
  type: TYPE_NORMAL
- en: d) There is no direct relationship between loadings and singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the dimensionality of the matrix \(T\) in the principal component
    transformation \(T = XV^{(l)}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(n \times p\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(n \times l\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(l \times p\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(p \times l\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What is the purpose of centering the data in PCA?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To make the calculations easier.
  prefs: []
  type: TYPE_NORMAL
- en: b) To ensure the first principal component describes the direction of maximum
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: c) To normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: d) To remove outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that “Principal components
    analysis (PCA) is a commonly used dimensionality reduction approach” and that
    “The first principal component is the linear combination of the features … with
    largest sample variance.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states that “Formally, we seek to
    solve \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: The text explains that the solution to the
    PCA optimization problem is to take the loadings to be the top right singular
    vector of the data matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The matrix \(T\) contains the scores of the
    data points on the first \(l\) principal components. Since there are \(n\) data
    points and \(l\) principal components, the dimensionality of \(T\) is \(n \times
    l\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text mentions that “Mean subtraction (a.k.a.
    ‘mean centering’) is necessary for performing classical PCA to ensure that the
    first principal component describes the direction of maximum variance.”'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1\. Dimensionality reduction via principal components analysis (PCA)[#](#dimensionality-reduction-via-principal-components-analysis-pca
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Principal components analysis (PCA)\(\idx{principal components analysis}\xdi\)
    is a commonly used dimensionality reduction approach that is closely related to
    what we described in the previous sections. We formalize the connection.
  prefs: []
  type: TYPE_NORMAL
- en: '*The data matrix:* In PCA we are given \(n\) data points \(\mathbf{x}_1,\ldots,\mathbf{x}_n
    \in \mathbb{R}^p\) with \(p\) features (i.e., coordinates). We denote the components
    of \(\mathbf{x}_i\) as \((x_{i1},\ldots,x_{ip})\). As usual, we stack them up
    into a matrix \(X\) whose \(i\)-th row is \(\mathbf{x}_i^T\).'
  prefs: []
  type: TYPE_NORMAL
- en: The first step of PCA is to center the data, i.e., we assume that\(\idx{mean
    centering}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Put differently, the empirical mean of each column is \(0\). Quoting [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)
    (and this will become clearer below):'
  prefs: []
  type: TYPE_NORMAL
- en: Mean subtraction (a.k.a. “mean centering”) is necessary for performing classical
    PCA to ensure that the first principal component describes the direction of maximum
    variance. If mean subtraction is not performed, the first principal component
    might instead correspond more or less to the mean of the data. A mean of zero
    is needed for finding a basis that minimizes the mean square error of the approximation
    of the data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An optional step is to divide each column by the square root of its [sample
    variance](https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance), i.e.,
    assume that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p. \]
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in a previous chapter, this is particularly important when the
    features are measured in different units to ensure that their variability can
    be meaningfully compared.
  prefs: []
  type: TYPE_NORMAL
- en: '*The first principal component:* The first principal component is the linear
    combination of the features'
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip} \]
  prefs: []
  type: TYPE_NORMAL
- en: with largest sample variance. For this to make sense, we need to constrain the
    \(\phi_{j1}\)s. Specifically, we require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^p \phi_{j1}^2 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: The \(\phi_{j1}\)s are referred to as the *loadings* and the \(t_{i1}\)s are
    referred to as the *scores*.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we seek to solve
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\
    :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the fact that the \(t_{i1}\)s are centered
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{1}{n} \sum_{i=1}^n t_{i1} &= \frac{1}{n} \sum_{i=1}^n
    [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\ &= \phi_{11} \frac{1}{n} \sum_{i=1}^n
    x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n x_{ip}\\ &= 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: to compute their sample variance as the mean of their square
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p
    \phi_{j1} x_{ij}\right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\) and \(\mathbf{t}_1
    = (t_{11},\ldots,t_{n1})\). Then for all \(i\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1, \]
  prefs: []
  type: TYPE_NORMAL
- en: or in vector form
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{t}_1 = X \boldsymbol{\phi}_1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Also
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \|\mathbf{t}_1\|^2 = \frac{1}{n-1}
    \|X \boldsymbol{\phi}_1\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting the maximization problem above in vector form,
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}, \]'
  prefs: []
  type: TYPE_NORMAL
- en: we see that we have already encountered this problem (up to the factor of \(1/(n-1)\)
    which does not affect the solution). The solution is to take \(\boldsymbol{\phi}_1\)
    to be the top right singular vector of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)).
    As we know this is equivalent to computing the top eigenvector of the matrix \(\frac{1}{n-1}
    X^T X\), which is the sample covariance matrix of the data (accounting for the
    fact that the data is already centered).
  prefs: []
  type: TYPE_NORMAL
- en: '*The second principal component:* The second principal component is the linear
    combination of the features'
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{i2} = \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip} \]
  prefs: []
  type: TYPE_NORMAL
- en: with largest sample variance that is also uncorrelated with the first principal
    component, in the sense that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The next lemma shows how to deal with this condition. Again, we also require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^p \phi_{j2}^2 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: As before, let \(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\) and \(\mathbf{t}_2
    = (t_{12},\ldots,t_{n2})\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Uncorrelated Principal Components)** Assume \(X \neq \mathbf{0}\).
    Let \(t_{i1}\), \(t_{i2}\), \(\boldsymbol{\phi}_1\), \(\boldsymbol{\phi}_2\) be
    as above (where, in particular, \(\boldsymbol{\phi}_1\) is a top right singular
    vector of \(X\)). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: holds if and only if
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The condition'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we dropped the \(1/(n-1)\) factor as it does not play any role. Using
    that \(\mathbf{t}_1 = X \boldsymbol{\phi}_1\), and similarly, \(\mathbf{t}_2 =
    X \boldsymbol{\phi}_2\), this is in turn equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(\boldsymbol{\phi}_1\) can be chosen as a top right singular vector
    in an SVD of \(X\), it follows from the *SVD Relations* that \(X^T X \boldsymbol{\phi}_1
    = \sigma_1^2 \boldsymbol{\phi}_1\), where \(\sigma_1\) is the singular value associated
    to \(\boldsymbol{\phi}_1\). Since \(X \neq 0\), \(\sigma_1 > 0\). Plugging this
    in the inner product on the left hand side above, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
    &= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\ &= (X \boldsymbol{\phi}_2)^T
    (X \boldsymbol{\phi}_1)\\ &= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
    &= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\ &= \langle \boldsymbol{\phi}_2,
    \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\ &= \sigma_1^2 \langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(\sigma_1 \neq 0\), this is \(0\) if and only if \(\langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle = 0\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we can write the maximization problem for the second principal
    component in matrix form as
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2
    = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: Again, we see that we have encountered this problem before. The solution is
    to take \(\boldsymbol{\phi}_2\) to be a second right singular vector in an SVD
    of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)). Again, this is equivalent to computing
    the second eigenvector of the sample covariance matrix \(\frac{1}{n-1} X^T X\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Further principal components:* We can proceed in a similar fashion and define
    further principal components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To quote [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations):'
  prefs: []
  type: TYPE_NORMAL
- en: PCA essentially rotates the set of points around their mean in order to align
    with the principal components. This moves as much of the variance as possible
    (using an orthogonal transformation) into the first few dimensions. The values
    in the remaining dimensions, therefore, tend to be small and may be dropped with
    minimal loss of information […] PCA is often used in this manner for dimensionality
    reduction. PCA has the distinction of being the optimal orthogonal transformation
    for keeping the subspace that has largest “variance” […]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Formally, let
  prefs: []
  type: TYPE_NORMAL
- en: \[ X = U \Sigma V^T \]
  prefs: []
  type: TYPE_NORMAL
- en: be the SVD of the data matrix \(X\). The principal component transformation,
    truncated at the \(\ell\)-th component, is
  prefs: []
  type: TYPE_NORMAL
- en: \[ T = X V_{(\ell)}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(T\) is the matrix whose columns are the vectors \(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\).
    Recall that \(V_{(\ell)}\) is the matrix made of the first \(k\) columns of \(V\).
  prefs: []
  type: TYPE_NORMAL
- en: Then, using the orthonormality of the right singular vectors,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} T = U \Sigma V^T V_{(\ell)} = U \Sigma \begin{bmatrix} I_{\ell
    \times \ell}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U \begin{bmatrix}\Sigma_{(\ell)}\\
    \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U_{(\ell)} \Sigma_{(\ell)}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, the vector \(\mathbf{t}_i\) is the left singular vector \(\mathbf{u}_i\)
    scaled by the corresponding singular value \(\sigma_i\).
  prefs: []
  type: TYPE_NORMAL
- en: Having established a formal connection between PCA and SVD, we implement PCA
    using the SVD algorithm [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html).
    We perform mean centering (now is the time to read that quote about the importance
    of mean centering again), but not the optional standardization. We use the fact
    that, in NumPy, subtracting a matrix by a vector whose dimension matches the number
    of columns performs row-wise subtraction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We apply it to the Gaussian Mixture Model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Plotting the result, we see that PCA does succeed in finding the main direction
    of variation. Note tha gap in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png](../Images/0ad434c45a764ec31a02581fa80c1fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Note however that the first two principal components in fact “capture more noise”
    than what can be seen in the orginal first two coordinates, a form of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**TRY IT!** Compute the first two right singular vectors \(\mathbf{v}_1\) and
    \(\mathbf{v}_2\) of \(X\) after mean centering. Do they align well with the first
    and second standard basis vectors \(\mathbf{e}_1\) and \(\mathbf{e}_2\)? Why or
    why not? ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We return to our motivating example. We apply PCA to our genetic dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Viruses (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Viruses](../Images/03664e03f12993cb6edd6189d59b79e1.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We load the dataset again. Recall that it contains \(1642\)
    strains and lives in a \(317\)-dimensional space.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Our goal is to find a “good” low-dimensional representation of the data. We
    work with ten dimensions using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We plot the first two principal components, and see what appears to be some
    potential structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png](../Images/f60d8d2f8537bfd4a76822ba9228e316.png)'
  prefs: []
  type: TYPE_IMG
- en: There seems to be some reasonably well-defined clusters in this projection.
    We use \(k\)-means to identiy clusters. We take advantage of the implementation
    in scikit-learn, [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).
    By default, it finds \(8\) clusters. The clusters can be extracted from the attribute
    `labels_`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To further reveal the structure, we look at our the clusters spread out over
    the years. That information is in a separate file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|  | strain | length | country | year | lon | lat | date |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | AB434107 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/02/25
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | AB434108 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/03/01
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | CY000113 | 1762 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/29 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | CY000209 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/17 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | CY000217 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/02/26 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: For each cluster, we plot how many of its data points come from a specific year.
    Each cluster has a different color.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png](../Images/1decff267b5cfeee392565099c5a1864.png)'
  prefs: []
  type: TYPE_IMG
- en: Remarkably, we see that each cluster comes mostly from one year or two consecutive
    ones. In other words, the clustering in this low-dimensional projection captures
    some true underlying structure that is not explicitly in the genetic data on which
    it is computed.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the first two principal components, we color the points on the
    scatterplot by year. (We use [`legend_elements()`](https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements)
    for automatic legend creation.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png](../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png)'
  prefs: []
  type: TYPE_IMG
- en: To some extent, one can “see” the virus evolving from year to year. The \(x\)-axis
    in particular seems to correlate strongly with the year, in the sense that samples
    from later years tend to be towards one side of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: To further quantify this observation, we use [`numpy.corrcoef`](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)
    to compute the correlation coefficients between the year and the first \(10\)
    principal components.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, we see that the first three or four principal components correlate well
    with the year.
  prefs: []
  type: TYPE_NORMAL
- en: Using [related techniques](https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8),
    one can also identify which mutations distinguish different epidemics (i.e., years).
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the difference between
    principal components analysis (PCA) and linear discriminant analysis (LDA). \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the goal of principal components analysis (PCA)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To find clusters in the data.
  prefs: []
  type: TYPE_NORMAL
- en: b) To find a low-dimensional representation of the data that captures the maximum
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: c) To find the mean of each feature in the data.
  prefs: []
  type: TYPE_NORMAL
- en: d) To find the correlation between features in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Formally, the first principal component is the linear combination of
    features \(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\) that solves which optimization
    problem?'
  prefs: []
  type: TYPE_NORMAL
- en: 'a) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: 'b) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: 'c) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: 'd) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**3** What is the relationship between the loadings in PCA and the singular
    vectors of the data matrix?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The loadings are the left singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: b) The loadings are the right singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: c) The loadings are the singular values.
  prefs: []
  type: TYPE_NORMAL
- en: d) There is no direct relationship between loadings and singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the dimensionality of the matrix \(T\) in the principal component
    transformation \(T = XV^{(l)}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(n \times p\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(n \times l\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(l \times p\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(p \times l\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What is the purpose of centering the data in PCA?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To make the calculations easier.
  prefs: []
  type: TYPE_NORMAL
- en: b) To ensure the first principal component describes the direction of maximum
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: c) To normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: d) To remove outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that “Principal components
    analysis (PCA) is a commonly used dimensionality reduction approach” and that
    “The first principal component is the linear combination of the features … with
    largest sample variance.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states that “Formally, we seek to
    solve \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: The text explains that the solution to the
    PCA optimization problem is to take the loadings to be the top right singular
    vector of the data matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The matrix \(T\) contains the scores of the
    data points on the first \(l\) principal components. Since there are \(n\) data
    points and \(l\) principal components, the dimensionality of \(T\) is \(n \times
    l\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text mentions that “Mean subtraction (a.k.a.
    ‘mean centering’) is necessary for performing classical PCA to ensure that the
    first principal component describes the direction of maximum variance.”'
  prefs: []
  type: TYPE_NORMAL
