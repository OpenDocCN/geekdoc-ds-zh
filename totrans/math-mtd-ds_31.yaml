- en: '4.5\. Application: principal components analysis#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.5. 应用：主成分分析#
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html](https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html](https://mmids-textbook.github.io/chap04_svd/05_pca/roch-mmids-svd-pca.html)
- en: We discuss an application to principal components analysis and revisit our genetic
    dataset from ealier in the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了主成分分析的应用，并回顾了本章早期提到的我们的基因数据集。
- en: 4.5.1\. Dimensionality reduction via principal components analysis (PCA)[#](#dimensionality-reduction-via-principal-components-analysis-pca
    "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5.1. 通过主成分分析（PCA）进行降维\[#](#dimensionality-reduction-via-principal-components-analysis-pca
    "链接到这个标题")
- en: Principal components analysis (PCA)\(\idx{principal components analysis}\xdi\)
    is a commonly used dimensionality reduction approach that is closely related to
    what we described in the previous sections. We formalize the connection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）\(\idx{主成分分析}\xdi\) 是一种常用的降维方法，与我们前几节中描述的方法密切相关。我们正式化这种联系。
- en: '*The data matrix:* In PCA we are given \(n\) data points \(\mathbf{x}_1,\ldots,\mathbf{x}_n
    \in \mathbb{R}^p\) with \(p\) features (i.e., coordinates). We denote the components
    of \(\mathbf{x}_i\) as \((x_{i1},\ldots,x_{ip})\). As usual, we stack them up
    into a matrix \(X\) whose \(i\)-th row is \(\mathbf{x}_i^T\).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据矩阵:* 在PCA中，我们给出了 \(n\) 个数据点 \(\mathbf{x}_1,\ldots,\mathbf{x}_n \in \mathbb{R}^p\)，其中
    \(p\) 是特征（即坐标）。我们用 \((x_{i1},\ldots,x_{ip})\) 表示 \(\mathbf{x}_i\) 的分量。像往常一样，我们将它们堆叠成一个矩阵
    \(X\)，其第 \(i\) 行是 \(\mathbf{x}_i^T\)。'
- en: The first step of PCA is to center the data, i.e., we assume that\(\idx{mean
    centering}\xdi\)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）的第一步是对数据进行中心化，即我们假设\(\idx{均值中心化}\xdi\)
- en: \[ \frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p \]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p \]
- en: 'Put differently, the empirical mean of each column is \(0\). Quoting [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)
    (and this will become clearer below):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每一列的样本均值是 \(0\)。引用 [维基百科](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)（以下内容将变得更加清晰）：
- en: Mean subtraction (a.k.a. “mean centering”) is necessary for performing classical
    PCA to ensure that the first principal component describes the direction of maximum
    variance. If mean subtraction is not performed, the first principal component
    might instead correspond more or less to the mean of the data. A mean of zero
    is needed for finding a basis that minimizes the mean square error of the approximation
    of the data.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 均值减法（也称为“均值中心化”）是执行经典PCA所必需的，以确保第一个主成分描述的是最大方差的方向。如果不执行均值减法，第一个主成分可能更多地或更少地对应于数据的均值。为了找到最小化数据近似均方误差的基，需要零均值。
- en: An optional step is to divide each column by the square root of its [sample
    variance](https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance), i.e.,
    assume that
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可选步骤是将每一列除以其 [样本方差](https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance)，即假设
- en: \[ \frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p. \]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p. \]
- en: As we mentioned in a previous chapter, this is particularly important when the
    features are measured in different units to ensure that their variability can
    be meaningfully compared.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中提到的，当特征以不同的单位测量时，这一点尤为重要，以确保它们的变异性可以有意义地比较。
- en: '*The first principal component:* The first principal component is the linear
    combination of the features'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一个主成分:* 第一个主成分是特征的线性组合'
- en: \[ t_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip} \]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \[ t_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip} \]
- en: with largest sample variance. For this to make sense, we need to constrain the
    \(\phi_{j1}\)s. Specifically, we require
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最大的样本方差。为了使这一点有意义，我们需要约束 \(\phi_{j1}\)s。具体来说，我们要求
- en: \[ \sum_{j=1}^p \phi_{j1}^2 = 1. \]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^p \phi_{j1}^2 = 1. \]
- en: The \(\phi_{j1}\)s are referred to as the *loadings* and the \(t_{i1}\)s are
    referred to as the *scores*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \(\phi_{j1}\)s 被称为 *载荷*，而 \(t_{i1}\)s 被称为 *得分*。
- en: Formally, we seek to solve
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们试图解决
- en: \[ \max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\
    :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\}, \]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\
    :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\}, \]
- en: where we used the fact that the \(t_{i1}\)s are centered
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(t_{i1}\)s 是中心化的这一事实
- en: \[\begin{align*} \frac{1}{n} \sum_{i=1}^n t_{i1} &= \frac{1}{n} \sum_{i=1}^n
    [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\ &= \phi_{11} \frac{1}{n} \sum_{i=1}^n
    x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n x_{ip}\\ &= 0, \end{align*}\]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{1}{n} \sum_{i=1}^n t_{i1} &= \frac{1}{n} \sum_{i=1}^n
    [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\ &= \phi_{11} \frac{1}{n} \sum_{i=1}^n
    x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n x_{ip}\\ &= 0, \end{align*}\]
- en: to compute their sample variance as the mean of their square
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算它们的样本方差作为它们的平方的平均值
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p
    \phi_{j1} x_{ij}\right)^2. \]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p
    \phi_{j1} x_{ij}\right)^2. \]
- en: Let \(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\) and \(\mathbf{t}_1
    = (t_{11},\ldots,t_{n1})\). Then for all \(i\)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\) 和 \(\mathbf{t}_1 =
    (t_{11},\ldots,t_{n1})\)。那么对于所有 \(i\)
- en: \[ t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1, \]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \[ t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1, \]
- en: or in vector form
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 或者以向量形式
- en: \[ \mathbf{t}_1 = X \boldsymbol{\phi}_1. \]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{t}_1 = X \boldsymbol{\phi}_1. \]
- en: Also
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 也
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \|\mathbf{t}_1\|^2 = \frac{1}{n-1}
    \|X \boldsymbol{\phi}_1\|^2. \]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \|\mathbf{t}_1\|^2 = \frac{1}{n-1}
    \|X \boldsymbol{\phi}_1\|^2. \]
- en: Rewriting the maximization problem above in vector form,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述最大化问题重新写成向量形式，
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}, \]'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}, \]'
- en: we see that we have already encountered this problem (up to the factor of \(1/(n-1)\)
    which does not affect the solution). The solution is to take \(\boldsymbol{\phi}_1\)
    to be the top right singular vector of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)).
    As we know this is equivalent to computing the top eigenvector of the matrix \(\frac{1}{n-1}
    X^T X\), which is the sample covariance matrix of the data (accounting for the
    fact that the data is already centered).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们已经遇到过这个问题（直到 \(1/(n-1)\) 的因子，它不影响解）。解决方案是将 \(\boldsymbol{\phi}_1\) 取为
    \(\frac{1}{\sqrt{n-1}}X\) 的右上奇异向量（或者简单地 \(X\)）。正如我们所知，这相当于计算矩阵 \(\frac{1}{n-1}
    X^T X\) 的最大特征向量，这是数据的样本协方差矩阵（考虑到数据已经中心化）。
- en: '*The second principal component:* The second principal component is the linear
    combination of the features'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二个主成分:* 第二个主成分是特征的线性组合'
- en: \[ t_{i2} = \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip} \]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \[ t_{i2} = \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip} \]
- en: with largest sample variance that is also uncorrelated with the first principal
    component, in the sense that
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最大的样本方差，并且与第一个主成分不相关，即
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0. \]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0. \]
- en: The next lemma shows how to deal with this condition. Again, we also require
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个引理展示了如何处理这个条件。同样，我们还需要
- en: \[ \sum_{j=1}^p \phi_{j2}^2 = 1. \]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^p \phi_{j2}^2 = 1. \]
- en: As before, let \(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\) and \(\mathbf{t}_2
    = (t_{12},\ldots,t_{n2})\).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，设 \(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\) 和 \(\mathbf{t}_2
    = (t_{12},\ldots,t_{n2})\)。
- en: '**LEMMA** **(Uncorrelated Principal Components)** Assume \(X \neq \mathbf{0}\).
    Let \(t_{i1}\), \(t_{i2}\), \(\boldsymbol{\phi}_1\), \(\boldsymbol{\phi}_2\) be
    as above (where, in particular, \(\boldsymbol{\phi}_1\) is a top right singular
    vector of \(X\)). Then'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(不相关主成分)** 假设 \(X \neq \mathbf{0}\)。令 \(t_{i1}\)，\(t_{i2}\)，\(\boldsymbol{\phi}_1\)，\(\boldsymbol{\phi}_2\)
    如上所述（特别是，\(\boldsymbol{\phi}_1\) 是 \(X\) 的右上奇异向量）。那么'
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0. \]
- en: holds if and only if
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 成立当且仅当
- en: \[ \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0. \]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0. \]
- en: \(\flat\)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* The condition'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 条件'
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
- en: is equivalent to
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 等价于
- en: \[ \langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0, \]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0, \]
- en: where we dropped the \(1/(n-1)\) factor as it does not play any role. Using
    that \(\mathbf{t}_1 = X \boldsymbol{\phi}_1\), and similarly, \(\mathbf{t}_2 =
    X \boldsymbol{\phi}_2\), this is in turn equivalent to
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们省略了 \(1/(n-1)\) 的因子，因为它不起任何作用。使用 \(\mathbf{t}_1 = X \boldsymbol{\phi}_1\)，以及类似地，\(\mathbf{t}_2
    = X \boldsymbol{\phi}_2\)，这反过来又相当于
- en: \[ \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0. \]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0. \]
- en: Because \(\boldsymbol{\phi}_1\) can be chosen as a top right singular vector
    in an SVD of \(X\), it follows from the *SVD Relations* that \(X^T X \boldsymbol{\phi}_1
    = \sigma_1^2 \boldsymbol{\phi}_1\), where \(\sigma_1\) is the singular value associated
    to \(\boldsymbol{\phi}_1\). Since \(X \neq 0\), \(\sigma_1 > 0\). Plugging this
    in the inner product on the left hand side above, we get
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\boldsymbol{\phi}_1\) 可以选择为 \(X\) 的奇异值分解中的右上奇异向量，根据 *SVD 关系*，有 \(X^T X
    \boldsymbol{\phi}_1 = \sigma_1^2 \boldsymbol{\phi}_1\)，其中 \(\sigma_1\) 是与 \(\boldsymbol{\phi}_1\)
    相关的奇异值。由于 \(X \neq 0\)，\(\sigma_1 > 0\)。将此代入上面左边的内积中，我们得到
- en: \[\begin{align*} \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
    &= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\ &= (X \boldsymbol{\phi}_2)^T
    (X \boldsymbol{\phi}_1)\\ &= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
    &= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\ &= \langle \boldsymbol{\phi}_2,
    \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\ &= \sigma_1^2 \langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle. \end{align*}\]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
    &= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\ &= (X \boldsymbol{\phi}_2)^T
    (X \boldsymbol{\phi}_1)\\ &= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
    &= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\ &= \langle \boldsymbol{\phi}_2,
    \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\ &= \sigma_1^2 \langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle. \end{align*}\]
- en: Because \(\sigma_1 \neq 0\), this is \(0\) if and only if \(\langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle = 0\). \(\square\)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\sigma_1 \neq 0\)，这只有在 \(\langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2
    \rangle = 0\) 时才成立。 \(\square\)
- en: As a result, we can write the maximization problem for the second principal
    component in matrix form as
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将第二个主成分的最大化问题写成矩阵形式，
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2
    = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}. \]'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2
    = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}. \]'
- en: Again, we see that we have encountered this problem before. The solution is
    to take \(\boldsymbol{\phi}_2\) to be a second right singular vector in an SVD
    of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)). Again, this is equivalent to computing
    the second eigenvector of the sample covariance matrix \(\frac{1}{n-1} X^T X\).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们看到我们之前遇到过这个问题。解决方案是将 \(\boldsymbol{\phi}_2\) 取为 \(\frac{1}{\sqrt{n-1}}X\)（或简单地
    \(X\)）的奇异值分解中的第二个右奇异向量。再次，这相当于计算样本协方差矩阵 \(\frac{1}{n-1} X^T X\) 的第二个特征向量。
- en: '*Further principal components:* We can proceed in a similar fashion and define
    further principal components.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*进一步的主成分*：我们可以以类似的方式继续进行，并定义进一步的主成分。'
- en: 'To quote [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 引用[维基百科](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)：
- en: PCA essentially rotates the set of points around their mean in order to align
    with the principal components. This moves as much of the variance as possible
    (using an orthogonal transformation) into the first few dimensions. The values
    in the remaining dimensions, therefore, tend to be small and may be dropped with
    minimal loss of information […] PCA is often used in this manner for dimensionality
    reduction. PCA has the distinction of being the optimal orthogonal transformation
    for keeping the subspace that has largest “variance” […]
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）本质上是在点的均值周围旋转点集，以与主成分对齐。这尽可能地将方差（使用正交变换）移动到前几个维度。因此，剩余维度的值往往很小，并且可以最小损失信息地删除
    [...] PCA通常以这种方式用于降维。PCA的特点是它是保持具有最大“方差”子空间的最佳正交变换 [...]
- en: Formally, let
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，设
- en: \[ X = U \Sigma V^T \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ X = U \Sigma V^T \]
- en: be the SVD of the data matrix \(X\). The principal component transformation,
    truncated at the \(\ell\)-th component, is
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是数据矩阵 \(X\) 的奇异值分解。截断到第 \(\ell\) 个成分的主成分变换是
- en: \[ T = X V_{(\ell)}, \]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[ T = X V_{(\ell)}, \]
- en: where \(T\) is the matrix whose columns are the vectors \(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\).
    Recall that \(V_{(\ell)}\) is the matrix made of the first \(k\) columns of \(V\).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(T\) 是列向量 \(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\) 的矩阵。回忆一下，\(V_{(\ell)}\)
    是由 \(V\) 的前 \(k\) 列组成的矩阵。
- en: Then, using the orthonormality of the right singular vectors,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，利用右奇异向量的正交性，
- en: \[\begin{split} T = U \Sigma V^T V_{(\ell)} = U \Sigma \begin{bmatrix} I_{\ell
    \times \ell}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U \begin{bmatrix}\Sigma_{(\ell)}\\
    \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U_{(\ell)} \Sigma_{(\ell)}. \end{split}\]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} T = U \Sigma V^T V_{(\ell)} = U \Sigma \begin{bmatrix} I_{\ell
    \times \ell}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U \begin{bmatrix}\Sigma_{(\ell)}\\
    \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U_{(\ell)} \Sigma_{(\ell)}. \end{split}\]
- en: Put differently, the vector \(\mathbf{t}_i\) is the left singular vector \(\mathbf{u}_i\)
    scaled by the corresponding singular value \(\sigma_i\).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，向量 \(\mathbf{t}_i\) 是由相应的奇异值 \(\sigma_i\) 缩放的左奇异向量 \(\mathbf{u}_i\)。
- en: Having established a formal connection between PCA and SVD, we implement PCA
    using the SVD algorithm [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html).
    We perform mean centering (now is the time to read that quote about the importance
    of mean centering again), but not the optional standardization. We use the fact
    that, in NumPy, subtracting a matrix by a vector whose dimension matches the number
    of columns performs row-wise subtraction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了 PCA 和 SVD 之间的正式联系后，我们使用 SVD 算法实现 PCA，即 `numpy.linalg.svd`。我们执行均值中心化（现在是时候再次阅读关于均值中心化重要性的那段引言了），但不进行可选的标准化。我们利用在
    NumPy 中，从矩阵中减去一个与列数匹配的向量执行行减法的事实。
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**NUMERICAL CORNER:** We apply it to the Gaussian Mixture Model.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们将其应用于高斯混合模型。'
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Plotting the result, we see that PCA does succeed in finding the main direction
    of variation. Note tha gap in the middle.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制结果，我们看到 PCA 确实成功地找到了主要的变化方向。注意中间的差距。
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png](../Images/0ad434c45a764ec31a02581fa80c1fd3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png](../Images/0ad434c45a764ec31a02581fa80c1fd3.png)'
- en: Note however that the first two principal components in fact “capture more noise”
    than what can be seen in the orginal first two coordinates, a form of overfitting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，前两个主成分实际上“捕获了比原始前两个坐标中可见的更多噪声”，这是一种过度拟合的形式。
- en: '**TRY IT!** Compute the first two right singular vectors \(\mathbf{v}_1\) and
    \(\mathbf{v}_2\) of \(X\) after mean centering. Do they align well with the first
    and second standard basis vectors \(\mathbf{e}_1\) and \(\mathbf{e}_2\)? Why or
    why not? ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试一下！** 计算均值中心化后 \(X\) 的前两个右奇异向量 \(\mathbf{v}_1\) 和 \(\mathbf{v}_2\)。它们是否与第一个和第二个标准基向量
    \(\mathbf{e}_1\) 和 \(\mathbf{e}_2\) 对齐得很好？为什么或为什么不？([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
- en: \(\unlhd\)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: We return to our motivating example. We apply PCA to our genetic dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到我们的动机示例。我们对我们的遗传数据集应用 PCA。
- en: '**Figure:** Viruses (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**图**: 病毒 (*来源:* 使用 [Midjourney](https://www.midjourney.com/) 制作)'
- en: '![Viruses](../Images/03664e03f12993cb6edd6189d59b79e1.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![病毒](../Images/03664e03f12993cb6edd6189d59b79e1.png)'
- en: \(\bowtie\)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**NUMERICAL CORNER:** We load the dataset again. Recall that it contains \(1642\)
    strains and lives in a \(317\)-dimensional space.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们再次加载数据集。回想一下，它包含 \(1642\) 个菌株，存在于一个 \(317\) 维的空间中。'
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our goal is to find a “good” low-dimensional representation of the data. We
    work with ten dimensions using PCA.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到数据的一个“良好”的低维表示。我们使用 PCA 在十个维度上工作。
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We plot the first two principal components, and see what appears to be some
    potential structure.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制前两个主成分，并看到似乎有一些潜在的结构。
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png](../Images/f60d8d2f8537bfd4a76822ba9228e316.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png](../Images/f60d8d2f8537bfd4a76822ba9228e316.png)'
- en: There seems to be some reasonably well-defined clusters in this projection.
    We use \(k\)-means to identiy clusters. We take advantage of the implementation
    in scikit-learn, [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).
    By default, it finds \(8\) clusters. The clusters can be extracted from the attribute
    `labels_`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个投影中似乎有一些定义合理的簇。我们使用 \(k\)-means 来识别簇。我们利用 scikit-learn 中的实现，[`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)。默认情况下，它找到
    \(8\) 个簇。簇可以从属性 `labels_` 中提取。
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To further reveal the structure, we look at our the clusters spread out over
    the years. That information is in a separate file.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步揭示结构，我们查看我们的簇在年份上的分布。这些信息在另一个文件中。
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|  | strain | length | country | year | lon | lat | date |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 菌株 | 长度 | 国家 | 年份 | 经度 | 纬度 | 日期 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | AB434107 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/02/25
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 0 | AB434107 | 1701 | 日本 | 2002 | 137.215474 | 35.584176 | 2002/02/25 |'
- en: '| 1 | AB434108 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/03/01
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1 | AB434108 | 1701 | 日本 | 2002 | 137.215474 | 35.584176 | 2002/03/01 |'
- en: '| 2 | CY000113 | 1762 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/29 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 2 | CY000113 | 1762 | 美国 | 2002 | -73.940000 | 40.670000 | 2002/01/29 |'
- en: '| 3 | CY000209 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/17 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 3 | CY000209 | 1760 | 美国 | 2002 | -73.940000 | 40.670000 | 2002/01/17 |'
- en: '| 4 | CY000217 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/02/26 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 4 | CY000217 | 1760 | 美国 | 2002 | -73.940000 | 40.670000 | 2002/02/26 |'
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For each cluster, we plot how many of its data points come from a specific year.
    Each cluster has a different color.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个簇，我们绘制其数据点中有多少来自特定年份。每个簇都有不同的颜色。
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png](../Images/1decff267b5cfeee392565099c5a1864.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png](../Images/1decff267b5cfeee392565099c5a1864.png)'
- en: Remarkably, we see that each cluster comes mostly from one year or two consecutive
    ones. In other words, the clustering in this low-dimensional projection captures
    some true underlying structure that is not explicitly in the genetic data on which
    it is computed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们看到每个簇主要来自一年或两年连续的年份。换句话说，在这个低维投影中的聚类捕捉到了一些真正的潜在结构，而这些结构并没有明确地体现在计算它的遗传数据中。
- en: Going back to the first two principal components, we color the points on the
    scatterplot by year. (We use [`legend_elements()`](https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements)
    for automatic legend creation.)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回到前两个主成分，我们根据年份给散点图上的点着色。（我们使用[`legend_elements()`](https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements)来自动创建图例。）
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png](../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png](../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png)'
- en: To some extent, one can “see” the virus evolving from year to year. The \(x\)-axis
    in particular seems to correlate strongly with the year, in the sense that samples
    from later years tend to be towards one side of the plot.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定程度上，可以看到病毒从一年到一年地进化。特别是，x轴似乎与年份有很强的相关性，即后来的年份的样本倾向于图表的一侧。
- en: To further quantify this observation, we use [`numpy.corrcoef`](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)
    to compute the correlation coefficients between the year and the first \(10\)
    principal components.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步量化这一观察结果，我们使用[`numpy.corrcoef`](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)来计算年份和前10个主成分之间的相关系数。
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Indeed, we see that the first three or four principal components correlate well
    with the year.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，我们看到前三个或四个主成分与年份的相关性很好。
- en: Using [related techniques](https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8),
    one can also identify which mutations distinguish different epidemics (i.e., years).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[相关技术](https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8)，也可以确定哪些突变区分了不同的流行病（即，年份）。
- en: \(\unlhd\)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the difference between
    principal components analysis (PCA) and linear discriminant analysis (LDA). \(\ddagger\)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向您最喜欢的AI聊天机器人询问主成分分析（PCA）和线性判别分析（LDA）之间的区别。 \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude、Gemini和ChatGPT协助)*'
- en: '**1** What is the goal of principal components analysis (PCA)?'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 主成分分析（PCA）的目标是什么？'
- en: a) To find clusters in the data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: a) 在数据中找到簇。
- en: b) To find a low-dimensional representation of the data that captures the maximum
    variance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: b) 找到数据中特征的最大方差的低维表示。
- en: c) To find the mean of each feature in the data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: c) 找到数据中每个特征的平均值。
- en: d) To find the correlation between features in the data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: d) 找到数据中特征之间的相关性。
- en: '**2** Formally, the first principal component is the linear combination of
    features \(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\) that solves which optimization
    problem?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 形式上，第一个主成分是特征 \(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\) 的线性组合，它解决了哪个优化问题？'
- en: 'a) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
- en: 'b) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
- en: 'c) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
- en: 'd) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
- en: '**3** What is the relationship between the loadings in PCA and the singular
    vectors of the data matrix?'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** PCA中的载荷与数据矩阵的奇异向量之间的关系是什么？'
- en: a) The loadings are the left singular vectors.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: a) 载荷是左奇异向量。
- en: b) The loadings are the right singular vectors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: b) 载荷是右奇异向量。
- en: c) The loadings are the singular values.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: c) 载荷是奇异值。
- en: d) There is no direct relationship between loadings and singular vectors.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: d) 载荷与奇异向量之间没有直接关系。
- en: '**4** What is the dimensionality of the matrix \(T\) in the principal component
    transformation \(T = XV^{(l)}\)?'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 主成分变换 \(T = XV^{(l)}\) 中的矩阵 \(T\) 的维度是多少？'
- en: a) \(n \times p\)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(n \times p\)
- en: b) \(n \times l\)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(n \times l\)
- en: c) \(l \times p\)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(l \times p\)
- en: d) \(p \times l\)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(p \times l\)
- en: '**5** What is the purpose of centering the data in PCA?'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** PCA中数据中心化的目的是什么？'
- en: a) To make the calculations easier.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: a) 为了使计算更简单。
- en: b) To ensure the first principal component describes the direction of maximum
    variance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: b) 确保第一个主成分描述最大方差的方向。
- en: c) To normalize the data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: c) 为了归一化数据。
- en: d) To remove outliers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: d) 为了去除异常值。
- en: 'Answer for 1: b. Justification: The text states that “Principal components
    analysis (PCA) is a commonly used dimensionality reduction approach” and that
    “The first principal component is the linear combination of the features … with
    largest sample variance.”'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 1的答案：b. 理由：文本指出，“主成分分析（PCA）是一种常用的降维方法”，并且“第一个主成分是特征的最大样本方差的线性组合……”
- en: 'Answer for 2: a. Justification: The text states that “Formally, we seek to
    solve \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\).”'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '2的答案：a. 理由：文本指出，“形式上，我们寻求解决 \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2
    : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)。”'
- en: 'Answer for 3: b. Justification: The text explains that the solution to the
    PCA optimization problem is to take the loadings to be the top right singular
    vector of the data matrix.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 3的答案：b. 理由：文本解释说，PCA优化问题的解是将载荷取为数据矩阵右上角的奇异向量。
- en: 'Answer for 4: b. Justification: The matrix \(T\) contains the scores of the
    data points on the first \(l\) principal components. Since there are \(n\) data
    points and \(l\) principal components, the dimensionality of \(T\) is \(n \times
    l\).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 4的答案：b. 理由：矩阵 \(T\) 包含数据点在第一个 \(l\) 个主成分上的得分。由于有 \(n\) 个数据点和 \(l\) 个主成分，\(T\)
    的维度是 \(n \times l\)。
- en: 'Answer for 5: b. Justification: The text mentions that “Mean subtraction (a.k.a.
    ‘mean centering’) is necessary for performing classical PCA to ensure that the
    first principal component describes the direction of maximum variance.”'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 5的答案：b. 理由：文本提到，“均值减法（也称为‘均值中心化’）对于执行经典PCA是必要的，以确保第一个主成分描述最大方差的方向。”
- en: 4.5.1\. Dimensionality reduction via principal components analysis (PCA)[#](#dimensionality-reduction-via-principal-components-analysis-pca
    "Link to this heading")
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5.1\. 通过主成分分析（PCA）进行降维[#](#dimensionality-reduction-via-principal-components-analysis-pca
    "链接到这个标题")
- en: Principal components analysis (PCA)\(\idx{principal components analysis}\xdi\)
    is a commonly used dimensionality reduction approach that is closely related to
    what we described in the previous sections. We formalize the connection.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）\(\idx{principal components analysis}\xdi\) 是一种常用的降维方法，它与我们在前几节中描述的内容密切相关。我们正式化这种联系。
- en: '*The data matrix:* In PCA we are given \(n\) data points \(\mathbf{x}_1,\ldots,\mathbf{x}_n
    \in \mathbb{R}^p\) with \(p\) features (i.e., coordinates). We denote the components
    of \(\mathbf{x}_i\) as \((x_{i1},\ldots,x_{ip})\). As usual, we stack them up
    into a matrix \(X\) whose \(i\)-th row is \(\mathbf{x}_i^T\).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据矩阵：* 在PCA中，我们给出了 \(n\) 个数据点 \(\mathbf{x}_1,\ldots,\mathbf{x}_n \in \mathbb{R}^p\)，其中
    \(p\) 是特征（即坐标）。我们用 \((x_{i1},\ldots,x_{ip})\) 表示 \(\mathbf{x}_i\) 的分量。像往常一样，我们将它们堆叠成一个矩阵
    \(X\)，其第 \(i\) 行是 \(\mathbf{x}_i^T\)。'
- en: The first step of PCA is to center the data, i.e., we assume that\(\idx{mean
    centering}\xdi\)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）的第一步是对数据进行中心化，即我们假设\(\idx{均值中心化}\xdi\)
- en: \[ \frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p \]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n} \sum_{i=1}^n x_{ij} = 0, \qquad \forall j=1,\ldots,p \]
- en: 'Put differently, the empirical mean of each column is \(0\). Quoting [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)
    (and this will become clearer below):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每一列的经验均值是 \(0\)。引用 [维基百科](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)（以下将变得更加清晰）：
- en: Mean subtraction (a.k.a. “mean centering”) is necessary for performing classical
    PCA to ensure that the first principal component describes the direction of maximum
    variance. If mean subtraction is not performed, the first principal component
    might instead correspond more or less to the mean of the data. A mean of zero
    is needed for finding a basis that minimizes the mean square error of the approximation
    of the data.
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 均值减法（也称为“均值中心化”）对于执行经典PCA是必要的，以确保第一个主成分描述的是最大方差的方向。如果没有执行均值减法，第一个主成分可能更多地或更少地对应于数据的均值。为了找到最小化数据近似均方误差的基，需要零均值。
- en: An optional step is to divide each column by the square root of its [sample
    variance](https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance), i.e.,
    assume that
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 可选步骤是将每一列除以其样本方差的平方根，即假设
- en: \[ \frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p. \]
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n x_{ij}^2 = 1, \qquad \forall j=1,\ldots,p. \]
- en: As we mentioned in a previous chapter, this is particularly important when the
    features are measured in different units to ensure that their variability can
    be meaningfully compared.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中提到的，当特征以不同的单位测量时，这一点尤为重要，以确保它们的变异性可以有意义地比较。
- en: '*The first principal component:* The first principal component is the linear
    combination of the features'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一主成分:* 第一主成分是特征向量的线性组合'
- en: \[ t_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip} \]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[ t_{i1} = \phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip} \]
- en: with largest sample variance. For this to make sense, we need to constrain the
    \(\phi_{j1}\)s. Specifically, we require
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最大的样本方差。为了使这一点有意义，我们需要约束 \(\phi_{j1}\)s。具体来说，我们要求
- en: \[ \sum_{j=1}^p \phi_{j1}^2 = 1. \]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^p \phi_{j1}^2 = 1. \]
- en: The \(\phi_{j1}\)s are referred to as the *loadings* and the \(t_{i1}\)s are
    referred to as the *scores*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: \(\phi_{j1}\)s 被称为 *载荷*，而 \(t_{i1}\)s 被称为 *得分*。
- en: Formally, we seek to solve
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们试图解决
- en: \[ \max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\
    :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\}, \]
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max\left\{\frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p \phi_{j1} x_{ij}\right)^2\
    :\ \sum_{j=1}^p \phi_{j1}^2 = 1\right\}, \]
- en: where we used the fact that the \(t_{i1}\)s are centered
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(t_{i1}\)s 已经中心化的事实
- en: \[\begin{align*} \frac{1}{n} \sum_{i=1}^n t_{i1} &= \frac{1}{n} \sum_{i=1}^n
    [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\ &= \phi_{11} \frac{1}{n} \sum_{i=1}^n
    x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n x_{ip}\\ &= 0, \end{align*}\]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{1}{n} \sum_{i=1}^n t_{i1} &= \frac{1}{n} \sum_{i=1}^n
    [\phi_{11} x_{i1} + \cdots + \phi_{p1} x_{ip}]\\ &= \phi_{11} \frac{1}{n} \sum_{i=1}^n
    x_{i1} + \cdots + \phi_{p1} \frac{1}{n} \sum_{i=1}^n x_{ip}\\ &= 0, \end{align*}\]
- en: to compute their sample variance as the mean of their square
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 计算它们的样本方差作为它们平方的平均值
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p
    \phi_{j1} x_{ij}\right)^2. \]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \sum_{i=1}^n \left(\sum_{j=1}^p
    \phi_{j1} x_{ij}\right)^2. \]
- en: Let \(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\) and \(\mathbf{t}_1
    = (t_{11},\ldots,t_{n1})\). Then for all \(i\)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\boldsymbol{\phi}_1 = (\phi_{11},\ldots,\phi_{p1})\) 和 \(\mathbf{t}_1 =
    (t_{11},\ldots,t_{n1})\)。然后对于所有的 \(i\)
- en: \[ t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1, \]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[ t_{i1} = \mathbf{x}_i^T \boldsymbol{\phi}_1, \]
- en: or in vector form
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 或者以向量形式表示
- en: \[ \mathbf{t}_1 = X \boldsymbol{\phi}_1. \]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{t}_1 = X \boldsymbol{\phi}_1. \]
- en: Also
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外
- en: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \|\mathbf{t}_1\|^2 = \frac{1}{n-1}
    \|X \boldsymbol{\phi}_1\|^2. \]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1}\sum_{i=1}^n t_{i1}^2 = \frac{1}{n-1} \|\mathbf{t}_1\|^2 = \frac{1}{n-1}
    \|X \boldsymbol{\phi}_1\|^2. \]
- en: Rewriting the maximization problem above in vector form,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述最大化问题重新写成向量形式，
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}, \]'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}, \]'
- en: we see that we have already encountered this problem (up to the factor of \(1/(n-1)\)
    which does not affect the solution). The solution is to take \(\boldsymbol{\phi}_1\)
    to be the top right singular vector of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)).
    As we know this is equivalent to computing the top eigenvector of the matrix \(\frac{1}{n-1}
    X^T X\), which is the sample covariance matrix of the data (accounting for the
    fact that the data is already centered).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们已经遇到过这个问题（直到 \(1/(n-1)\) 的因子，它不影响解）。解决方案是将 \(\boldsymbol{\phi}_1\) 取为
    \(\frac{1}{\sqrt{n-1}}X\) 的右上角奇异向量（或者简单地取 \(X\)）。正如我们所知，这等价于计算矩阵 \(\frac{1}{n-1}
    X^T X\) 的最大特征向量，这是数据的样本协方差矩阵（考虑到数据已经中心化）。
- en: '*The second principal component:* The second principal component is the linear
    combination of the features'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二个主成分:* 第二个主成分是特征的线性组合'
- en: \[ t_{i2} = \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip} \]
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: \[ t_{i2} = \phi_{12} x_{i1} + \cdots + \phi_{p2} x_{ip} \]
- en: with largest sample variance that is also uncorrelated with the first principal
    component, in the sense that
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最大的样本方差，并且与第一个主成分不相关，即
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0. \]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0. \]
- en: The next lemma shows how to deal with this condition. Again, we also require
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个引理展示了如何处理这个条件。同样，我们还需要
- en: \[ \sum_{j=1}^p \phi_{j2}^2 = 1. \]
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^p \phi_{j2}^2 = 1. \]
- en: As before, let \(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\) and \(\mathbf{t}_2
    = (t_{12},\ldots,t_{n2})\).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，设 \(\boldsymbol{\phi}_2 = (\phi_{12},\ldots,\phi_{p2})\) 和 \(\mathbf{t}_2
    = (t_{12},\ldots,t_{n2})\)。
- en: '**LEMMA** **(Uncorrelated Principal Components)** Assume \(X \neq \mathbf{0}\).
    Let \(t_{i1}\), \(t_{i2}\), \(\boldsymbol{\phi}_1\), \(\boldsymbol{\phi}_2\) be
    as above (where, in particular, \(\boldsymbol{\phi}_1\) is a top right singular
    vector of \(X\)). Then'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(不相关的主成分)** 假设 \(X \neq \mathbf{0}\)。设 \(t_{i1}\)，\(t_{i2}\)，\(\boldsymbol{\phi}_1\)，\(\boldsymbol{\phi}_2\)
    如上所述（特别是，\(\boldsymbol{\phi}_1\) 是 \(X\) 的右上角奇异向量）。那么'
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
- en: holds if and only if
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 成立当且仅当
- en: \[ \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0. \]
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0. \]
- en: \(\flat\)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* The condition'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 条件'
- en: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n-1} \sum_{i=1}^n t_{i1} t_{i2} = 0 \]
- en: is equivalent to
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 等价于
- en: \[ \langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0, \]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle \mathbf{t}_1, \mathbf{t}_2 \rangle = 0, \]
- en: where we dropped the \(1/(n-1)\) factor as it does not play any role. Using
    that \(\mathbf{t}_1 = X \boldsymbol{\phi}_1\), and similarly, \(\mathbf{t}_2 =
    X \boldsymbol{\phi}_2\), this is in turn equivalent to
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们省略了 \(1/(n-1)\) 的因子，因为它不起任何作用。使用 \(\mathbf{t}_1 = X \boldsymbol{\phi}_1\)，以及类似地，\(\mathbf{t}_2
    = X \boldsymbol{\phi}_2\)，这反过来又相当于
- en: \[ \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0. \]
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle = 0. \]
- en: Because \(\boldsymbol{\phi}_1\) can be chosen as a top right singular vector
    in an SVD of \(X\), it follows from the *SVD Relations* that \(X^T X \boldsymbol{\phi}_1
    = \sigma_1^2 \boldsymbol{\phi}_1\), where \(\sigma_1\) is the singular value associated
    to \(\boldsymbol{\phi}_1\). Since \(X \neq 0\), \(\sigma_1 > 0\). Plugging this
    in the inner product on the left hand side above, we get
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\boldsymbol{\phi}_1\) 可以被选作 \(X\) 的奇异值分解中的右上角奇异向量，根据 *奇异值分解关系*，有 \(X^T
    X \boldsymbol{\phi}_1 = \sigma_1^2 \boldsymbol{\phi}_1\)，其中 \(\sigma_1\) 是与 \(\boldsymbol{\phi}_1\)
    相关的奇异值。由于 \(X \neq 0\)，\(\sigma_1 > 0\)。将此代入上述内积的左侧，我们得到
- en: \[\begin{align*} \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
    &= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\ &= (X \boldsymbol{\phi}_2)^T
    (X \boldsymbol{\phi}_1)\\ &= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
    &= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\ &= \langle \boldsymbol{\phi}_2,
    \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\ &= \sigma_1^2 \langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle. \end{align*}\]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \langle X \boldsymbol{\phi}_1, X \boldsymbol{\phi}_2 \rangle
    &= \langle X \boldsymbol{\phi}_2, X \boldsymbol{\phi}_1 \rangle\\ &= (X \boldsymbol{\phi}_2)^T
    (X \boldsymbol{\phi}_1)\\ &= \boldsymbol{\phi}_2^T X^T X \boldsymbol{\phi}_1\\
    &= \boldsymbol{\phi}_2^T (\sigma_1^2 \boldsymbol{\phi}_1)\\ &= \langle \boldsymbol{\phi}_2,
    \sigma_1^2 \boldsymbol{\phi}_1 \rangle\\ &= \sigma_1^2 \langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle. \end{align*}\]
- en: Because \(\sigma_1 \neq 0\), this is \(0\) if and only if \(\langle \boldsymbol{\phi}_1,
    \boldsymbol{\phi}_2 \rangle = 0\). \(\square\)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\sigma_1 \neq 0\)，这只有在 \(\langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2
    \rangle = 0\) 时才成立。 \(\square\)
- en: As a result, we can write the maximization problem for the second principal
    component in matrix form as
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将第二个主成分的最大化问题写成矩阵形式
- en: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2
    = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}. \]'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \max\left\{\frac{1}{n-1} \|X \boldsymbol{\phi}_2\|^2 : \|\boldsymbol{\phi}_2\|^2
    = 1, \langle \boldsymbol{\phi}_1, \boldsymbol{\phi}_2 \rangle = 0\right\}. \]'
- en: Again, we see that we have encountered this problem before. The solution is
    to take \(\boldsymbol{\phi}_2\) to be a second right singular vector in an SVD
    of \(\frac{1}{\sqrt{n-1}}X\) (or simply \(X\)). Again, this is equivalent to computing
    the second eigenvector of the sample covariance matrix \(\frac{1}{n-1} X^T X\).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们看到我们之前遇到过这个问题。解决方案是将 \(\boldsymbol{\phi}_2\) 设为 \(\frac{1}{\sqrt{n-1}}X\)（或简单地
    \(X\)）的SVD中的第二个右奇异向量。同样，这相当于计算样本协方差矩阵 \(\frac{1}{n-1} X^T X\) 的第二个特征向量。
- en: '*Further principal components:* We can proceed in a similar fashion and define
    further principal components.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*进一步的主成分分析：* 我们可以以类似的方式继续进行，并定义进一步的主成分。'
- en: 'To quote [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 引用 [维基百科](https://en.wikipedia.org/wiki/Principal_component_analysis#Further_considerations)：
- en: PCA essentially rotates the set of points around their mean in order to align
    with the principal components. This moves as much of the variance as possible
    (using an orthogonal transformation) into the first few dimensions. The values
    in the remaining dimensions, therefore, tend to be small and may be dropped with
    minimal loss of information […] PCA is often used in this manner for dimensionality
    reduction. PCA has the distinction of being the optimal orthogonal transformation
    for keeping the subspace that has largest “variance” […]
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PCA本质上是在点的均值周围旋转点集，以与主成分对齐。这尽可能地将方差（使用正交变换）移动到前几个维度。因此，剩余维度的值往往很小，并且可以最小损失信息地删除
    [...] PCA通常以这种方式用于降维。PCA的特点是它是保持具有最大“方差”子空间的最佳正交变换 [...]
- en: Formally, let
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，让
- en: \[ X = U \Sigma V^T \]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: \[ X = U \Sigma V^T \]
- en: be the SVD of the data matrix \(X\). The principal component transformation,
    truncated at the \(\ell\)-th component, is
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 成为数据矩阵 \(X\) 的SVD。截断到第 \(\ell\) 个成分的主成分变换是
- en: \[ T = X V_{(\ell)}, \]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: \[ T = X V_{(\ell)}, \]
- en: where \(T\) is the matrix whose columns are the vectors \(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\).
    Recall that \(V_{(\ell)}\) is the matrix made of the first \(k\) columns of \(V\).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(T\) 是其列是向量 \(\mathbf{t}_1,\ldots,\mathbf{t}_\ell\) 的矩阵。回想一下，\(V_{(\ell)}\)
    是由 \(V\) 的前 \(k\) 列组成的矩阵。
- en: Then, using the orthonormality of the right singular vectors,
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，利用右奇异向量的正交性，
- en: \[\begin{split} T = U \Sigma V^T V_{(\ell)} = U \Sigma \begin{bmatrix} I_{\ell
    \times \ell}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U \begin{bmatrix}\Sigma_{(\ell)}\\
    \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U_{(\ell)} \Sigma_{(\ell)}. \end{split}\]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} T = U \Sigma V^T V_{(\ell)} = U \Sigma \begin{bmatrix} I_{\ell
    \times \ell}\\ \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U \begin{bmatrix}\Sigma_{(\ell)}\\
    \mathbf{0}_{(p-\ell)\times \ell}\end{bmatrix} = U_{(\ell)} \Sigma_{(\ell)}. \end{split}\]
- en: Put differently, the vector \(\mathbf{t}_i\) is the left singular vector \(\mathbf{u}_i\)
    scaled by the corresponding singular value \(\sigma_i\).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，向量 \(\mathbf{t}_i\) 是由相应的奇异值 \(\sigma_i\) 缩放的左奇异向量 \(\mathbf{u}_i\)。
- en: Having established a formal connection between PCA and SVD, we implement PCA
    using the SVD algorithm [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html).
    We perform mean centering (now is the time to read that quote about the importance
    of mean centering again), but not the optional standardization. We use the fact
    that, in NumPy, subtracting a matrix by a vector whose dimension matches the number
    of columns performs row-wise subtraction.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了PCA和SVD之间的正式联系后，我们使用SVD算法 `numpy.linalg.svd` 实现PCA。我们执行均值中心（现在是时候再次阅读关于均值中心重要性的那段引言了），但不进行可选的标准化。我们使用在NumPy中，从矩阵中减去一个与列数匹配的向量执行行减法的事实。
- en: '[PRE13]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**NUMERICAL CORNER:** We apply it to the Gaussian Mixture Model.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角：** 我们将其应用于高斯混合模型。'
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Plotting the result, we see that PCA does succeed in finding the main direction
    of variation. Note tha gap in the middle.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制结果，我们看到PCA确实成功地找到了主要变化方向。注意中间的差距。
- en: '[PRE15]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png](../Images/0ad434c45a764ec31a02581fa80c1fd3.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/e21a6d33ce608db4641ce1f34dc3031add52c5563b99fd4f50d6ff957ec36eba.png](../Images/0ad434c45a764ec31a02581fa80c1fd3.png)'
- en: Note however that the first two principal components in fact “capture more noise”
    than what can be seen in the orginal first two coordinates, a form of overfitting.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，前两个主成分实际上“捕捉到的噪声”比原始前两个坐标中看到的要多，这是一种过度拟合的形式。
- en: '**TRY IT!** Compute the first two right singular vectors \(\mathbf{v}_1\) and
    \(\mathbf{v}_2\) of \(X\) after mean centering. Do they align well with the first
    and second standard basis vectors \(\mathbf{e}_1\) and \(\mathbf{e}_2\)? Why or
    why not? ([Open in Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试！** 计算均值中心化后的 \(X\) 的前两个右奇异向量 \(\mathbf{v}_1\) 和 \(\mathbf{v}_2\)。它们是否与第一个和第二个标准基向量
    \(\mathbf{e}_1\) 和 \(\mathbf{e}_2\) 对齐得很好？为什么或为什么不？([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
- en: \(\unlhd\)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: We return to our motivating example. We apply PCA to our genetic dataset.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到我们的动机示例。我们对我们的遗传数据集应用 PCA。
- en: '**Figure:** Viruses (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：病毒 (*来源：使用 [Midjourney](https://www.midjourney.com/))**'
- en: '![Viruses](../Images/03664e03f12993cb6edd6189d59b79e1.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![病毒](../Images/03664e03f12993cb6edd6189d59b79e1.png)'
- en: \(\bowtie\)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**NUMERICAL CORNER:** We load the dataset again. Recall that it contains \(1642\)
    strains and lives in a \(317\)-dimensional space.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角：** 我们再次加载数据集。回想一下，它包含 \(1642\) 个菌株，存在于 \(317\) 维空间中。'
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our goal is to find a “good” low-dimensional representation of the data. We
    work with ten dimensions using PCA.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到数据的一个“良好”的低维表示。我们使用 PCA 在十个维度上工作。
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We plot the first two principal components, and see what appears to be some
    potential structure.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制前两个主成分，并看到似乎有一些潜在的结构。
- en: '[PRE18]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png](../Images/f60d8d2f8537bfd4a76822ba9228e316.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/ef753944f279930b676622662784f66b76d3187b6b2b63bb750df3d541ae7660.png](../Images/f60d8d2f8537bfd4a76822ba9228e316.png)'
- en: There seems to be some reasonably well-defined clusters in this projection.
    We use \(k\)-means to identiy clusters. We take advantage of the implementation
    in scikit-learn, [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).
    By default, it finds \(8\) clusters. The clusters can be extracted from the attribute
    `labels_`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个投影中似乎有一些相当明确定义的簇。我们使用 \(k\)-means 来识别簇。我们利用 scikit-learn 中的实现，[`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)。默认情况下，它找到
    \(8\) 个簇。簇可以从属性 `labels_` 中提取。
- en: '[PRE19]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To further reveal the structure, we look at our the clusters spread out over
    the years. That information is in a separate file.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步揭示结构，我们查看这些簇在年份上的分布。这些信息在另一个文件中。
- en: '[PRE20]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|  | strain | length | country | year | lon | lat | date |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | 应力 | 长度 | 国家 | 年份 | 经度 | 纬度 | 日期 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | AB434107 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/02/25
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 0 | AB434107 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/02/25
    |'
- en: '| 1 | AB434108 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/03/01
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 1 | AB434108 | 1701 | Japan | 2002 | 137.215474 | 35.584176 | 2002/03/01
    |'
- en: '| 2 | CY000113 | 1762 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/29 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 2 | CY000113 | 1762 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/29 |'
- en: '| 3 | CY000209 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/17 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 3 | CY000209 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/01/17 |'
- en: '| 4 | CY000217 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/02/26 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 4 | CY000217 | 1760 | USA | 2002 | -73.940000 | 40.670000 | 2002/02/26 |'
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: For each cluster, we plot how many of its data points come from a specific year.
    Each cluster has a different color.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个簇，我们绘制其数据点来自特定年份的数量。每个簇都有不同的颜色。
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png](../Images/1decff267b5cfeee392565099c5a1864.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/2cf7b5038d28508b6fc0e79888fbe7b827014b9d6f399ccb64da3b94b189f151.png](../Images/1decff267b5cfeee392565099c5a1864.png)'
- en: Remarkably, we see that each cluster comes mostly from one year or two consecutive
    ones. In other words, the clustering in this low-dimensional projection captures
    some true underlying structure that is not explicitly in the genetic data on which
    it is computed.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 非常引人注目的是，我们发现每个簇主要来自一年或两年连续的年份。换句话说，在这个低维投影中的聚类捕捉到了一些真正的潜在结构，而这些结构并没有明确地体现在计算它的遗传数据中。
- en: Going back to the first two principal components, we color the points on the
    scatterplot by year. (We use [`legend_elements()`](https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements)
    for automatic legend creation.)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 回到前两个主成分，我们在散点图上根据年份给点着色。（我们使用 `legend_elements()` [链接](https://matplotlib.org/stable/api/collections_api.html#matplotlib.collections.PathCollection.legend_elements)
    来自动创建图例。）
- en: '[PRE23]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png](../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/38586396ddc40a870a6c73ddce993f3b89e660d10f7e481f7b65e2e8a2b0ddba.png](../Images/387a5aa4a48d8cbbb7a6fc0b790ffc10.png)'
- en: To some extent, one can “see” the virus evolving from year to year. The \(x\)-axis
    in particular seems to correlate strongly with the year, in the sense that samples
    from later years tend to be towards one side of the plot.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定程度上，人们可以“看到”病毒逐年演变。特别是，\(x\) 轴似乎与年份有很强的相关性，即后期年份的样本倾向于图表的一侧。
- en: To further quantify this observation, we use [`numpy.corrcoef`](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)
    to compute the correlation coefficients between the year and the first \(10\)
    principal components.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步量化这一观察结果，我们使用 `numpy.corrcoef`（[链接](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)）来计算年份与第一个
    \(10\) 个主成分之间的相关系数。
- en: '[PRE24]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Indeed, we see that the first three or four principal components correlate well
    with the year.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们看到前三个或四个主成分与年份的相关性很好。
- en: Using [related techniques](https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8),
    one can also identify which mutations distinguish different epidemics (i.e., years).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [相关技术](https://bmcgenet.biomedcentral.com/articles/10.1186/1471-2156-11-94/figures/8)，人们还可以确定哪些突变区分了不同的流行病（即，年份）。
- en: \(\unlhd\)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the difference between
    principal components analysis (PCA) and linear discriminant analysis (LDA). \(\ddagger\)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向你最喜欢的AI聊天机器人询问主成分分析（PCA）和线性判别分析（LDA）之间的区别。 \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(有Claude、Gemini和ChatGPT的帮助)*'
- en: '**1** What is the goal of principal components analysis (PCA)?'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 主成分分析（PCA）的目标是什么？'
- en: a) To find clusters in the data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: a) 在数据中找到聚类。
- en: b) To find a low-dimensional representation of the data that captures the maximum
    variance.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: b) 找到数据的低维表示，该表示能够捕捉到最大的方差。
- en: c) To find the mean of each feature in the data.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: c) 找到数据中每个特征的均值。
- en: d) To find the correlation between features in the data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: d) 找到数据中特征之间的相关性。
- en: '**2** Formally, the first principal component is the linear combination of
    features \(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\) that solves which optimization
    problem?'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 形式上，第一个主成分是特征 \(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\) 的线性组合，它解决了哪个优化问题？'
- en: 'a) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
- en: 'b) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\)'
- en: 'c) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
- en: 'd) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\min \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    \leq 1\right\}\)'
- en: '**3** What is the relationship between the loadings in PCA and the singular
    vectors of the data matrix?'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** PCA中的加载量与数据矩阵的奇异向量之间的关系是什么？'
- en: a) The loadings are the left singular vectors.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: a) 加载量是左奇异向量。
- en: b) The loadings are the right singular vectors.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: b) 加载量是右奇异向量。
- en: c) The loadings are the singular values.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: c) 加载量是奇异值。
- en: d) There is no direct relationship between loadings and singular vectors.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: d) 加载量和奇异向量之间没有直接关系。
- en: '**4** What is the dimensionality of the matrix \(T\) in the principal component
    transformation \(T = XV^{(l)}\)?'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 主成分变换 \(T = XV^{(l)}\) 中的矩阵 \(T\) 的维度是多少？'
- en: a) \(n \times p\)
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(n \times p\)
- en: b) \(n \times l\)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(n \times l\)
- en: c) \(l \times p\)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(l \times p\)
- en: d) \(p \times l\)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(p \times l\)
- en: '**5** What is the purpose of centering the data in PCA?'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** PCA中数据中心化的目的是什么？'
- en: a) To make the calculations easier.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: a) 为了使计算更容易。
- en: b) To ensure the first principal component describes the direction of maximum
    variance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: b) 确保第一个主成分描述了最大方差的方向。
- en: c) To normalize the data.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: c) 为了归一化数据。
- en: d) To remove outliers.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: d) 去除异常值。
- en: 'Answer for 1: b. Justification: The text states that “Principal components
    analysis (PCA) is a commonly used dimensionality reduction approach” and that
    “The first principal component is the linear combination of the features … with
    largest sample variance.”'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 答案1：b. 证明：文本中提到“主成分分析（PCA）是一种常用的降维方法”，并且“第一个主成分是特征 … 的线性组合，具有最大的样本方差。”
- en: 'Answer for 2: a. Justification: The text states that “Formally, we seek to
    solve \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2
    = 1\right\}\).”'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '答案2：a. 证明：文本中提到“形式上，我们寻求解决 \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2
    : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\)。”'
- en: 'Answer for 3: b. Justification: The text explains that the solution to the
    PCA optimization problem is to take the loadings to be the top right singular
    vector of the data matrix.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 答案3：b. 证明：文本解释说，PCA优化问题的解是取数据矩阵的右上角奇异向量作为负载量。
- en: 'Answer for 4: b. Justification: The matrix \(T\) contains the scores of the
    data points on the first \(l\) principal components. Since there are \(n\) data
    points and \(l\) principal components, the dimensionality of \(T\) is \(n \times
    l\).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：b. 证明：矩阵 \(T\) 包含了数据点在第一个 \(l\) 个主成分上的得分。由于有 \(n\) 个数据点，\(l\) 个主成分，因此 \(T\)
    的维度是 \(n \times l\)。
- en: 'Answer for 5: b. Justification: The text mentions that “Mean subtraction (a.k.a.
    ‘mean centering’) is necessary for performing classical PCA to ensure that the
    first principal component describes the direction of maximum variance.”'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：b. 证明：文本提到“减去均值（也称为‘均值中心化’）对于执行经典PCA是必要的，以确保第一个主成分描述了最大方差的方向。”
