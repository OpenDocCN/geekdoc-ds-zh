<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>2.4. QR decomposition and Householder transformations#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>2.4. QR decomposition and Householder transformations#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap02_ls/04_qr/roch-mmids-ls-qr.html">https://mmids-textbook.github.io/chap02_ls/04_qr/roch-mmids-ls-qr.html</a></blockquote>

<p>We have some business left over from previous sections: constructing orthonormal bases. We go over the Gram-Schimidt algorithm below. Through a matrix factorization perspective, we give an alternative way to solve the linear least squares problem.</p>
<section id="matrix-form-of-gram-schmidt">
<h2><span class="section-number">2.4.1. </span>Matrix form of Gram-Schmidt<a class="headerlink" href="#matrix-form-of-gram-schmidt" title="Link to this heading">#</a></h2>
<p>In this subsection, we prove the <em>Gram-Schmidt Theorem</em> and introduce a fruitful matrix perspective.</p>
<p><strong>Gram-Schmidt algorithm</strong> Let <span class="math notranslate nohighlight">\(\mathbf{a}_1,\ldots,\mathbf{a}_m\)</span> be linearly independent. We use the Gram-Schmidt algorithm<span class="math notranslate nohighlight">\(\idx{Gram-Schmidt algorithm}\xdi\)</span> to obtain an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\)</span>. The process takes advantage of the properties of the orthogonal projection derived above. In essence we add the vectors <span class="math notranslate nohighlight">\(\mathbf{a}_i\)</span> one by one, but only after taking out their orthogonal projection on the previously included vectors. The outcome spans the same subspace and the <em>Orthogonal Projection Theorem</em> ensures orthogonality.</p>
<p><em>Proof idea:</em> <em>(Gram-Schmidt)</em> Suppose first that <span class="math notranslate nohighlight">\(m=1\)</span>. In that case, all that needs to be done is to divide <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span> by its norm to obtain a unit vector whose span is the same as <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span>, that is, we set <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)</span>.</p>
<p>Suppose now that <span class="math notranslate nohighlight">\(m=2\)</span>. We first let <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)</span> as in the previous case. Then we subtract from <span class="math notranslate nohighlight">\(\mathbf{a}_2\)</span> its projection on <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span>, that is, we set <span class="math notranslate nohighlight">\(\mathbf{v}_2 = \mathbf{a}_2 - \langle \mathbf{q}_1, \mathbf{a}_2 \rangle \,\mathbf{q}_1\)</span>. It is easily checked that <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> is orthogonal to <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span> (see the proof of the <em>Orthogonal Projection Theorem</em> for a similar calculation). Moreover, because <span class="math notranslate nohighlight">\(\mathbf{a}_2\)</span> is a linear combination of <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span>, we have <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\mathbf{v}_2)
= \mathrm{span}(\mathbf{a}_1,\mathbf{a}_2)\)</span>. It remains to divide by the norm of the resulting vector: <span class="math notranslate nohighlight">\(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|}\)</span>.</p>
<p>For general <span class="math notranslate nohighlight">\(m\)</span>, we proceed similarly but project onto the subspace spanned by the previously added vectors at each step.</p>
<p><img alt="Gram–Schmidt process (with help from ChatGPT; inspired by Source)" src="../Images/357bfa05be44b654c5858a352b39044e.png" data-original-src="https://mmids-textbook.github.io/_images/gram-schmidt.png"/></p>
<p><em>Proof:</em> <em>(Gram-Schmidt)</em> The first step of the induction is described above. Then the general inductive step is the following. Assume that we have constructed orthonormal vectors <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
U_{j-1} := 
\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = 
\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}).
\]</div>
<p><em>Constructing <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>:</em> By the <em>Properties of Orthonormal Lists</em>, <span class="math notranslate nohighlight">\(\{\mathbf{q}\}_{i=1}^{j-1}\)</span> is an independent list and therefore forms an orthonormal basis for <span class="math notranslate nohighlight">\(U_{j-1}\)</span>. So we can compute the orthogonal projection of <span class="math notranslate nohighlight">\(\mathbf{a}_j\)</span> on <span class="math notranslate nohighlight">\(U_{j-1}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_{U_{j-1}}\mathbf{a}_j
= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i,
\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(r_{ij} = \langle \mathbf{q}_i , \mathbf{a}_j\rangle\)</span>. And we set</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_j 
= \mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j
= \mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i
\quad
\text{and}
\quad
\mathbf{q}_j 
= \frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}.
\]</div>
<p>The last step is possible because:</p>
<p><strong>LEMMA</strong> <span class="math notranslate nohighlight">\(\|\mathbf{v}_j\| &gt; 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Indeed otherwise <span class="math notranslate nohighlight">\(\mathbf{a}_j\)</span>
would be equal to its projection <span class="math notranslate nohighlight">\(\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)</span> which would contradict linear independence of the <span class="math notranslate nohighlight">\(\mathbf{a}_k\)</span>’s. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span> is of unit norm by construction. It is also orthogonal to <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})\)</span> by the definition of <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span> and the <em>Orthogonal Projection Theorem</em>. So <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)</span> form an orthonormal list.</p>
<p><em>Pushing the induction through:</em> It remains to prove that <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\)</span>. Because by induction <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = 
\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)</span>, all we have to prove are the following two claims.</p>
<p><strong>LEMMA</strong> <span class="math notranslate nohighlight">\(\mathbf{q}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By construction,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{q}_j 
= \frac{1}{\|\mathbf{v}_j\|} \left\{\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\right\}
= \frac{1}{\|\mathbf{v}_j\|} \mathbf{a}_j + \frac{1}{\|\mathbf{v}_j\|} \mathrm{proj}_{U_{j-1}}\mathbf{a}_j.
\]</div>
<p>By definition of the orthogonal projection,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in U_{j-1}= \mathrm{span} (\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}) \subseteq \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j}).
\]</div>
<p>Hence we have written <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span> as a linear combination of vectors in <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)</span>. That proves the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>LEMMA</strong> <span class="math notranslate nohighlight">\(\mathbf{a}_j \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Unrolling the calculations above, <span class="math notranslate nohighlight">\(\mathbf{a}_j\)</span> can be re-written as the following linear combination of <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{a}_j
&amp;= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \mathbf{v}_j\\
&amp;= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{v}_j\| \mathbf{q}_j\\
&amp;= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\| \mathbf{q}_j\\
&amp;= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i
+ \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\| \,\mathbf{q}_j\\
&amp;= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i
+ r_{jj} \,\mathbf{q}_j,
\end{align*}\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(r_{jj} = \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\| = \|\mathbf{v}_j\|\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Hence <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)</span> forms an orthonormal list with <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)</span>. So induction goes through. That concludes the proof of the theorem. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We implement the Gram-Schmidt algorithm in Python. For reasons that will become clear in the next subsection, we output both the <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>’s and <span class="math notranslate nohighlight">\(r_{ij}\)</span>’s, each in matrix form. Here we use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html"><code class="docutils literal notranslate"><span class="pre">numpy.dot</span></code></a> to compute inner products.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">))</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="p">):</span>
            <span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">A</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
            <span class="n">v</span> <span class="o">-=</span> <span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">Q</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>
        <span class="n">R</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">Q</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">/</span><span class="n">R</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> Let’s try a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1. 0.]
 [0. 1.]
 [1. 1.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.70710678 -0.40824829]
 [ 0.          0.81649658]
 [ 0.70710678  0.40824829]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1.41421356 0.70710678]
 [0.         1.22474487]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Matrix form</strong> Let <span class="math notranslate nohighlight">\(\mathbf{a}_1,\ldots,\mathbf{a}_m \in \mathbb{R}^n\)</span> be linearly independent. Above, we presented the Gram-Schmidt algorithm to obtain an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\)</span>. We revisit it in matrix form.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A =
\begin{pmatrix}
| &amp;  &amp; | \\
\mathbf{a}_1 &amp; \ldots &amp; \mathbf{a}_m \\
| &amp;  &amp; | 
\end{pmatrix}
\quad
\text{and}
\quad
Q =
\begin{pmatrix}
| &amp;  &amp; | \\
\mathbf{q}_1 &amp; \ldots &amp; \mathbf{q}_m \\
| &amp;  &amp; | 
\end{pmatrix}.
\end{split}\]</div>
<p>Recalling that, for all <span class="math notranslate nohighlight">\(j\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{a}_j
= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
\]</div>
<p>the output of the Gram-Schmidt algorithm can be written in the following compact form, known as a <a class="reference external" href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a><span class="math notranslate nohighlight">\(\idx{QR decomposition}\xdi\)</span>,</p>
<div class="math notranslate nohighlight">
\[
A = QR
\]</div>
<p>where column <span class="math notranslate nohighlight">\(i\)</span> of the <span class="math notranslate nohighlight">\(m \times m\)</span> matrix <span class="math notranslate nohighlight">\(R\)</span> contains the coefficients of the linear combination of the <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>’s that produce <span class="math notranslate nohighlight">\(\mathbf{a}_i\)</span>.</p>
<p>By the proof of the <em>Gram-Schmidt Theorem</em>, <span class="math notranslate nohighlight">\(\mathbf{a}_i \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_i)\)</span>. So column <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(R\)</span> has only zeros below the diagonal. Hence <span class="math notranslate nohighlight">\(R\)</span> has a special structure we have previously encountered: it is upper triangular. The proof also established that the diagonal elements of <span class="math notranslate nohighlight">\(R\)</span> are strictly positive.</p>
<p><strong>DEFINITION</strong> <strong>(Triangular matrix)</strong> <span class="math notranslate nohighlight">\(\idx{triangular matrix}\xdi\)</span> A matrix <span class="math notranslate nohighlight">\(R = (r_{ij})_{i,j} \in \mathbb{R}^{n \times m}\)</span> is upper-triangular if all entries below the diagonal are zero, that is, if <span class="math notranslate nohighlight">\(i &gt; j\)</span> implies <span class="math notranslate nohighlight">\(r_{ij} = 0\)</span>. Similarly, a lower-triangular matrix has zeros above the diagonal. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>An upper-triangular matrix looks like this</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R = \begin{bmatrix}
r_{1,1} &amp; r_{1,2} &amp; r_{1,3} &amp; \ldots &amp; r_{1,n} \\
0        &amp; r_{2,2} &amp; r_{2,3} &amp; \ldots &amp; r_{2,n} \\
        &amp; 0        &amp; \ddots &amp; \ddots &amp; \vdots  \\
        &amp;         &amp; \ddots       &amp; \ddots &amp; r_{n-1,n} \\
0       &amp;         &amp;        &amp;  0      &amp; r_{n,n}
\end{bmatrix}.
\end{split}\]</div>
<p><strong>Remarks:</strong></p>
<p>a) If the input vectors <span class="math notranslate nohighlight">\(\mathbf{a}_1,\ldots,\mathbf{a}_m\)</span> are not linearly independent (in which case we say that the matrix <span class="math notranslate nohighlight">\(A\)</span> is rank-deficient), the Gram-Schmidt algorithm will fail. Indeed, at some point we will have that <span class="math notranslate nohighlight">\(\mathbf{a}_j \in U_{j-1}\)</span> and the normalization of <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span> will not be possible. In that case, one can instead use a technique called <a class="reference external" href="https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting">column pivoting</a>, which we will not describe.</p>
<p>b) The QR decomposition we have derived here is technically called a reduced QR decomposition. In a full QR decomposition<span class="math notranslate nohighlight">\(\idx{full QR decomposition}\xdi\)</span>, the matrix <span class="math notranslate nohighlight">\(Q\)</span> is square and orthogonal. In other words, the columns of such a <span class="math notranslate nohighlight">\(Q\)</span> form an orthonormal basis of the full space <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Let <span class="math notranslate nohighlight">\(A = Q_1 R_1\)</span> be a reduced QR decomposition, as obtained through the Gram-Schmidt algorithm. Then the columns of <span class="math notranslate nohighlight">\(Q_1\)</span> form an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{col}(A)\)</span> and can be completed into an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> by adding further vectors <span class="math notranslate nohighlight">\(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\)</span>. Let <span class="math notranslate nohighlight">\(Q_2\)</span> be the matrix with columns <span class="math notranslate nohighlight">\(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\)</span>. Then a full QR decomposition of <span class="math notranslate nohighlight">\(A\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q
=
\begin{pmatrix}
Q_1 &amp; Q_2
\end{pmatrix}
\qquad
R
=
\begin{pmatrix}
R_1\\ 
\mathbf{0}_{(n-m)\times m}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{0}_{(n-m)\times m}\)</span> is the all-zero matrix of size <span class="math notranslate nohighlight">\((n-m)\times m\)</span>. A numerical method for computing a full QR decomposition is presented in a later subsection.</p>
<p>c) The Gram-Schmidt algorithm is appealing geometrically, but it is known to have numerical issues. Other methods exist for computing QR decompositions with better numerical properties. We discuss such a method in a later subsection. (See that same subsection for an example where the <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>’s produced by Gram-Schmidt are far from orthogonal.)</p>
</section>
<section id="least-squares-via-qr">
<h2><span class="section-number">2.4.2. </span>Least squares via QR<a class="headerlink" href="#least-squares-via-qr" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix with linearly independent columns and let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> be a vector. Recall that a solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> to the linear least squares problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2
\]</div>
<p>satisfies the normal equations</p>
<div class="math notranslate nohighlight">
\[
A^T A \mathbf{x}^* = A^T \mathbf{b}.
\]</div>
<p><strong>Solving the normal equations</strong> In a first linear algebra course, one learns how to solve linear systems such as the normal equations. For this task a common approach is Gaussian elimination, or row reduction. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination">Wikipedia</a>:</p>
<blockquote>
<div><p>To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. […] Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. […] The process of row reduction […] can be divided into two parts. The first part (sometimes called forward elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.</p>
</div></blockquote>
<p><strong>Figure:</strong> An example of Gaussian elimination (<a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination">Source</a>)</p>
<p><img alt="Gaussian elimination" src="../Images/dc18b77fc722fe6abe5812c927848a72.png" data-original-src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65d92f997de9f7ad787b95d08fcd25dca828dd93"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>We will not go over Gaussian elimination here. In this subsection, we develop an alternative approach to solving the normal equations using the QR decomposition. We will need one component of Gaussian elimination, back substitution<span class="math notranslate nohighlight">\(\idx{back substitution}\xdi\)</span>. It is based on the observation that triangular systems of equations are straightforward to solve. We start with an example.</p>
<p><strong>EXAMPLE:</strong> Here is a concrete example of back substitution. Consider the system <span class="math notranslate nohighlight">\(R \mathbf{x} = \mathbf{b}\)</span> with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R
=
\begin{pmatrix}
2 &amp; -1 &amp; 2\\
0 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 2
\end{pmatrix}
\qquad
\mathbf{b}
=
\begin{pmatrix}
0\\
-2\\
0
\end{pmatrix}.
\end{split}\]</div>
<p>That corresponds to the linear equations</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;2 x_1 - x_2 + 2x_3 = 0\\
&amp;x_2 + x_3 = -2\\
&amp;2 x_3 = 0
\end{align*}\]</div>
<p>The third equation gives <span class="math notranslate nohighlight">\(x_3 = 0/2 = 0\)</span>. Plugging into the second one, we get <span class="math notranslate nohighlight">\(x_2 = -2 - x_3 = -2\)</span>. Plugging into the first one, we finally have <span class="math notranslate nohighlight">\(x_1 = (x_2 - 2 x_3)/2 = -1\)</span>. So the solution is <span class="math notranslate nohighlight">\(\mathbf{x} = (-1,-2,0)\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>In general, solving a triangular system of equations works as follows. Let <span class="math notranslate nohighlight">\(R = (r_{i,j})_{i,j} \in \mathbb{R}^{m \times m}\)</span> be upper-triangular and let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^m\)</span> be the left-hand vector, i.e., we want to solve the system</p>
<div class="math notranslate nohighlight">
\[
R \mathbf{x} = \mathbf{b}.
\]</div>
<p>Starting from the last row of the system, <span class="math notranslate nohighlight">\(r_{m,m} x_m = b_m\)</span> or <span class="math notranslate nohighlight">\(x_m = b_m/r_{m,m}\)</span>, assuming that <span class="math notranslate nohighlight">\(r_{m,m} \neq 0\)</span>. Moving to the second-to-last row, <span class="math notranslate nohighlight">\(r_{m-1,m-1} x_{m-1} + r_{m-1,m} x_m = b_{m-1}\)</span> or <span class="math notranslate nohighlight">\(x_{m-1} = (b_{m-1} - r_{m-1,m} x_m)/r_{m-1,m-1}\)</span>, assuming that <span class="math notranslate nohighlight">\(r_{m-1,m-1} \neq 0\)</span>. And so on. This procedure is known as <a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution">back substitution</a>.</p>
<p>Analogously, in the lower triangular case <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{m \times m}\)</span>, we have <a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution">forward substitution</a><span class="math notranslate nohighlight">\(\idx{forward substitution}\xdi\)</span>. These procedures implicitly define an inverse for <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(L\)</span> <em>when the diagonal elements are all non-zero</em>. We will not write them down explicitly here.</p>
<p>We implement back substitution in Python. In our naive implementation, we assume that the diagonal entries are not zero, which will suffice for our purposes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">backsubs</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">m</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">m</span><span class="p">]))</span><span class="o">/</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Forward substitution is implemented similarly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]))</span><span class="o">/</span><span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Using QR</strong> We show how to solve the normal equations via the QR decomposition.</p>
<p>1- Construct an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{col}(A)\)</span> through a QR decomposition</p>
<div class="math notranslate nohighlight">
\[
A = QR.
\]</div>
<p>2- Form the orthogonal projection matrix</p>
<div class="math notranslate nohighlight">
\[
P = Q Q^T.
\]</div>
<p>3- Apply the projection to <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> and observe that, by the proof of the <em>Normal Equations</em>, <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[
A \mathbf{x}^* = Q Q^T \mathbf{b}. 
\]</div>
<p>4- Plug in the QR decomposition for <span class="math notranslate nohighlight">\(A\)</span> to get</p>
<div class="math notranslate nohighlight">
\[
QR \mathbf{x}^* = Q Q^T \mathbf{b}. 
\]</div>
<p>5- Multiply both sides by <span class="math notranslate nohighlight">\(Q^T\)</span> and use <span class="math notranslate nohighlight">\(Q^T Q = I_{m \times m}\)</span></p>
<div class="math notranslate nohighlight">
\[
R \mathbf{x}^* = Q^T \mathbf{b}. 
\]</div>
<p>6- Solving this system for <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is straightforward because <span class="math notranslate nohighlight">\(R\)</span> is upper triangular via back substitution.</p>
<p><strong>THEOREM</strong> <strong>(Least Squares via QR)</strong> <span class="math notranslate nohighlight">\(\idx{least squares via QR}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix with linearly independent columns, let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> be a vector,
and let <span class="math notranslate nohighlight">\(A = QR\)</span> be a QR decomposition of <span class="math notranslate nohighlight">\(A\)</span>. The solution to the linear least-squares problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2.
\]</div>
<p>satisfies</p>
<div class="math notranslate nohighlight">
\[
R \mathbf{x}^* = Q^T \mathbf{b}. 
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Note that, in reality, we do not need to form the matrix <span class="math notranslate nohighlight">\(Q Q^T\)</span>.</p>
<p>We implement the QR approach to least squares.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">backsubs</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We return to our simple overdetermined system example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.66666667 0.66666667]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="householder-transformations">
<h2><span class="section-number">2.4.3. </span>Householder transformations<a class="headerlink" href="#householder-transformations" title="Link to this heading">#</a></h2>
<p>While the Gram-Schmidt algorithm gives a natural way to compute a (reduced) QR decomposition, there are many other numerical algorithms for this purpose. Some have better numerical behavior, specifically in terms of how they handle roundoff error. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Round-off_error">Wikipedia</a>:</p>
<blockquote>
<div><p>A roundoff error, also called rounding error, is the difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic. Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. […] When a sequence of calculations with an input involving roundoff error are made, errors may accumulate, sometimes dominating the calculation.</p>
</div></blockquote>
<p>We will not prove this here, but the following method based on Householder reflections is numerically more <a class="reference external" href="https://en.wikipedia.org/wiki/Numerical_stability#Stability_in_numerical_linear_algebra">stable</a>.</p>
<p>Recall that a square matrix <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{m\times m}\)</span> is orthogonal if <span class="math notranslate nohighlight">\(Q^T Q = Q Q^T = I_{m \times m}\)</span>. In words, the matrix inverse of <span class="math notranslate nohighlight">\(Q\)</span> is its transpose. This is equivalent to the columns of <span class="math notranslate nohighlight">\(Q\)</span> forming an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> (why?).</p>
<p>It can be shown that the product of two orthogonal matrices <span class="math notranslate nohighlight">\(Q_1\)</span> and <span class="math notranslate nohighlight">\(Q_2\)</span> is also orthogonal. (Try it!)</p>
<p>An important property of orthogonal matrices is that they preserve inner products: if <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{m\times m}\)</span> is orthogonal, then for any <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^m\)</span></p>
<div class="math notranslate nohighlight">
\[
\langle Q \mathbf{x}, Q \mathbf{y} \rangle 
= (Q \mathbf{x})^T  Q \mathbf{y}
= \mathbf{x}^T Q^T Q \mathbf{y}
= \mathbf{x}^T \mathbf{y}
= \langle \mathbf{x}, \mathbf{y} \rangle.
\]</div>
<p>In particular, orthogonal matrices preserve norms and angles.</p>
<!--ONLINE

Recall that the angle $\theta$ between $\mathbf{x}$ and $\mathbf{y}$ satisfies

$$
\cos \theta
= \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|}.
$$
--><p><strong>Reflections</strong> One such family of transformations are reflections.</p>
<p><strong>DEFINITION</strong> <strong>(Hyperplane)</strong> <span class="math notranslate nohighlight">\(\idx{hyperplane}\xdi\)</span> A hyperplane <span class="math notranslate nohighlight">\(W\)</span> is a linear subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> of dimension <span class="math notranslate nohighlight">\(m-1\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>DEFINITION</strong> <strong>(Householder Reflection)</strong> <span class="math notranslate nohighlight">\(\idx{Householder reflection}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^m\)</span> be a unit vector and let <span class="math notranslate nohighlight">\(W\)</span> be the hyperplane orthogonal to it. The reflection across <span class="math notranslate nohighlight">\(W\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
H = I_{m \times m} - 2 \mathbf{z} \mathbf{z}^T.
\]</div>
<p>This is referred to as a Householder reflection. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In words, we subtract twice the projection onto <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, as depicted below.</p>
<p><img alt="Householder reflection. (with the help of Claude; inspired by (Source))" src="../Images/7c283ce7dd76917f07050963bca3ee2b.png" data-original-src="https://mmids-textbook.github.io/_images/householder.png"/></p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(H = I_{m\times m} - 2\mathbf{z}\mathbf{z}^T\)</span> be a Householder reflection. Then <span class="math notranslate nohighlight">\(H\)</span> is an orthogonal matrix. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> We check the definition:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
H^T H 
&amp;= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)^T (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)\\
&amp;= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T) (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)\\
&amp;= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T\mathbf{z}\mathbf{z}^T\\
&amp;= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T
\end{align*}\]</div>
<p>which is equal to <span class="math notranslate nohighlight">\(I_{m\times m}\)</span>. The calculation for <span class="math notranslate nohighlight">\(H H^T\)</span> is the same.<span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>QR decomposition by introducing zeros</strong> We return to QR decompositions. One way to construct a (full) QR decomposition of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> is to find a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(H_1, \ldots, H_m\)</span> that triangularize <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[
H_m \cdots H_2 H_1 A = R
\]</div>
<p>for an upper-triangular matrix <span class="math notranslate nohighlight">\(R\)</span>. Indeed, by the properties of orthogonal matrices, we then have</p>
<div class="math notranslate nohighlight">
\[
A 
= H_1^T H_2^T \cdots H_m^T H_m \cdots H_2 H_1 A
= H_1^T H_2^T \cdots H_m^T R
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q = H_1^T H_2^T \cdots H_m^T\)</span> is itself orthogonal as a product of orthogonal matrices. So to proceed we need to identify orthogonal matrices that have the effect of introducing zeros below the diagonal, as illustrated below:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_2 H_1 A = 
\begin{pmatrix}
\times &amp; \times &amp; \times &amp; \times &amp; \times\\
0 &amp; \times &amp; \times &amp; \times &amp; \times\\
0 &amp; 0 &amp; \times &amp; \times &amp; \times\\
0 &amp; 0 &amp; \boxed{\times} &amp; \times &amp; \times\\
0 &amp; 0 &amp; \boxed{\times} &amp; \times &amp; \times\\
0 &amp; 0 &amp; \boxed{\times} &amp; \times &amp; \times\\
\end{pmatrix}.
\end{split}\]</div>
<p>It turns out that a well-chosen Householder reflection does the trick. Let <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> be the first column of <span class="math notranslate nohighlight">\(A\)</span> and take</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}
\quad
\text{and}
\quad
H_1 = I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{e}_1^{(n)}\)</span> is the first vector in the canonical basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. As depicted below, this choice sends <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}  
= 
\begin{pmatrix}
\|\mathbf{y}_1\|\\
0 \\
\vdots \\
0
\end{pmatrix}.
\end{split}\]</div>
<p>(It is clear that if <span class="math notranslate nohighlight">\(H_1 \mathbf{y}_1\)</span> is proportional to <span class="math notranslate nohighlight">\(\mathbf{e}_1^{(n)}\)</span>, than it can only be <span class="math notranslate nohighlight">\(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)</span> or <span class="math notranslate nohighlight">\(-\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)</span>. Prove it!)</p>
<p><img alt="Introducing zeros by Householder reflection (with the help of Claude; inspired by (Source))" src="../Images/19bf9b4f1f98bc25a2715d91da61fc5b.png" data-original-src="https://mmids-textbook.github.io/_images/spiegelebene.png"/></p>
<p><strong>LEMMA</strong> <strong>(Householder)</strong> <span class="math notranslate nohighlight">\(\idx{Householder lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span>, <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> and <span class="math notranslate nohighlight">\(H_1\)</span> be as above. Then</p>
<div class="math notranslate nohighlight">
\[
H_1 \mathbf{y}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> The proof by picture is in the figure above.</p>
<p><em>Proof:</em> Note that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2
&amp;= (\|\mathbf{y}_1\| - y_{1,1})^2
+ \sum_{j=2}^n y_{1,j}^2\\
&amp;= \|\mathbf{y}_1\|^2 -2 \|\mathbf{y}_1\| y_{1,1} + y_{1,1}^2 + \sum_{j=2}^n y_{1,j}^2\\
&amp;= 2(\|\mathbf{y}_1\|^2 - \|\mathbf{y}_1\| y_{1,1})
\end{align*}\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2 \mathbf{z}_1 \mathbf{z}_1^T \mathbf{y}_1
&amp;= 2 \mathbf{z}_1 \frac{\|\mathbf{y}_1\| \,(\mathbf{e}_1^{(n)})^T \mathbf{y}_1 - \mathbf{y}_1^T \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}\\
&amp;= 2 \frac{\|\mathbf{y}_1\| y_{1,1} - \|\mathbf{y}_1\|^2}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2} (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)\\
&amp;= - (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)
\end{align*}\]</div>
<p>where we used the previous equation. Hence</p>
<div class="math notranslate nohighlight">
\[
H_1 \mathbf{y}_1
= (I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T) \,\mathbf{y}_1
= \mathbf{y}_1 + (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) = \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}.
\]</div>
<p>That establishes the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The upshot is that multiplying <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(H_1\)</span> introduces zeros below the diagonal in the first column. To see this, recall that one interpretation of the matrix-matrix product is that each column of the second matrix gets multiplied by the first one. By the <em>Householder Lemma</em>, applying <span class="math notranslate nohighlight">\(H_1\)</span> to <span class="math notranslate nohighlight">\(A\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
H_1 A
= \begin{pmatrix}
H_1 \mathbf{y}_1 &amp; H_1 A_{\cdot,2} &amp; \cdots &amp; H_1 A_{\cdot,m} 
\end{pmatrix}
= \begin{pmatrix}
\|\mathbf{y}_1\| \mathbf{e}_1^{(n)} &amp; H_1 A_{\cdot,2} &amp; \cdots &amp; H_1 A_{\cdot,m}
\end{pmatrix}
\]</div>
<p>So the first column is now proportional to <span class="math notranslate nohighlight">\(\mathbf{e}_1\)</span>, which has zeros in all but the first element. (What should we do if <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> is already equal to <span class="math notranslate nohighlight">\(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)</span>?)</p>
<p>It turns that there is another choice of Householder reflection. Indeed, it can be shown that</p>
<div class="math notranslate nohighlight">
\[
\tilde{\mathbf{z}}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1}{\| \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1\|}
\quad
\text{and}
\quad
\tilde{H}_1 = I_{n\times n} - 2\tilde{\mathbf{z}}_1 \tilde{\mathbf{z}}_1^T
\]</div>
<p>are such that <span class="math notranslate nohighlight">\(\tilde{H}_1 \mathbf{y}_1 = - \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}\)</span> (try it!).</p>
<p><strong>Putting everything together</strong> We have shown how to introduce zeros below the diagonal in the first column of a matrix. To introduce zeros in the second column below the diagonal we use a block matrix. Recall that
if <span class="math notranslate nohighlight">\(A_{ij} \in \mathbb{R}^{n_i \times m_j}\)</span> and <span class="math notranslate nohighlight">\(B_{ij} \in \mathbb{R}^{m_i \times p_j}\)</span> for <span class="math notranslate nohighlight">\(i,j = 1, 2\)</span>, then we have the following formula</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
A_{11} &amp; A_{12}\\
A_{21} &amp; A_{22}
\end{pmatrix}
\begin{pmatrix}
B_{11} &amp; B_{12}\\
B_{21} &amp; B_{22}
\end{pmatrix}
=
\begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} &amp; A_{11} B_{12} + A_{12} B_{22}\\
A_{21} B_{11} + A_{22} B_{21} &amp; A_{21} B_{12} + A_{22} B_{22}
\end{pmatrix}.
\end{split}\]</div>
<p>Now consider the following block matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_2
= 
\begin{pmatrix}
1 &amp; \mathbf{0} \\
\mathbf{0} &amp; F_2
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(F_2\)</span> is the following Householder reflection. Write the second column of <span class="math notranslate nohighlight">\(H_1 A\)</span> as <span class="math notranslate nohighlight">\((y^{(2)}, \mathbf{y}_2)\)</span>. That is, <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span> are the entries <span class="math notranslate nohighlight">\(2,\ldots, n\)</span> of that column. Define</p>
<div class="math notranslate nohighlight">
\[
F_2 = I_{(n-1) \times (n-1)} - 2 \mathbf{z}_2 \mathbf{z}_2^T
\quad
\text{with}
\quad
\mathbf{z}_2 
= \frac{\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2}{\|\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2\|}
\]</div>
<p>where now <span class="math notranslate nohighlight">\(\mathbf{e}_1^{(n-1)} \in \mathbb{R}^{n-1}\)</span>. By the <em>Householder Lemma</em>, we have <span class="math notranslate nohighlight">\(F_2 \mathbf{y}_2 = \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}\)</span>. It can be shown that <span class="math notranslate nohighlight">\(\mathbf{y}_2 \neq \mathbf{0}\)</span> when the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent. (Try it!)</p>
<p>Applying <span class="math notranslate nohighlight">\(H_2\)</span> to <span class="math notranslate nohighlight">\(H_1 A\)</span> preserves the first row and column, and introduces zeros under the diagonal in the second column. To see this, first re-write <span class="math notranslate nohighlight">\(H_1 A\)</span> in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_1 A
= 
\begin{pmatrix}
\|\mathbf{y}_1\|  &amp; \mathbf{g}_2^T \\
\mathbf{0} &amp; G_2
\end{pmatrix}
\end{split}\]</div>
<p>where we used our previous observation about the first column of <span class="math notranslate nohighlight">\(H_1 A\)</span> and where <span class="math notranslate nohighlight">\(\mathbf{g}_2 \in \mathbb{R}^{m-1}\)</span>, <span class="math notranslate nohighlight">\(G_2 \in \mathbb{R}^{(n-1)\times (m-1)}\)</span>. One important point to note: the first column of <span class="math notranslate nohighlight">\(G_2\)</span> is equal to <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span>. Now multiply by <span class="math notranslate nohighlight">\(H_2\)</span> to get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_2 H_1 A
= 
\begin{pmatrix}
1 &amp; \mathbf{0} \\
\mathbf{0} &amp; F_2
\end{pmatrix}
\begin{pmatrix}
\|\mathbf{y}_1\|  &amp; \mathbf{g}_2^T \\
\mathbf{0} &amp; G_2
\end{pmatrix}
= 
\begin{pmatrix}
\|\mathbf{y}_1\| &amp; \mathbf{g}_2^T \\
\mathbf{0} &amp; F_2 G_2
\end{pmatrix}.
\end{split}\]</div>
<p>Computing the block <span class="math notranslate nohighlight">\(F_2 G_2\)</span> column by column we get</p>
<div class="math notranslate nohighlight">
\[
F_2 G_2
= \begin{pmatrix}
F_2 \mathbf{y}_2 &amp; F_2 (G_2)_{\cdot,2} &amp; \cdots &amp; F_2 (G_2)_{\cdot,m-1} 
\end{pmatrix}
= \begin{pmatrix}
\|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)} &amp; F_2 (G_2)_{\cdot,2} &amp; \cdots &amp; F_2 (G_2)_{\cdot,m-1}, 
\end{pmatrix}
\]</div>
<p>where <span class="math notranslate nohighlight">\((G_2)_{\cdot,j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th column of <span class="math notranslate nohighlight">\(G_2\)</span>. So the second column of <span class="math notranslate nohighlight">\(H_2 H_1 A\)</span> has zeros in all but the first two elements.</p>
<p>And so on. At Step <span class="math notranslate nohighlight">\(k\)</span>, we split the <span class="math notranslate nohighlight">\(k\)</span>-th column of <span class="math notranslate nohighlight">\(H_{k-1} \cdots H_1 A\)</span> into its first <span class="math notranslate nohighlight">\(k-1\)</span> and last <span class="math notranslate nohighlight">\(n-k+1\)</span> entries <span class="math notranslate nohighlight">\((\mathbf{y}^{(k)}, \mathbf{y}_k)\)</span> and form the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_k
= 
\begin{pmatrix}
I_{(k-1)\times (k-1)} &amp; \mathbf{0} \\
\mathbf{0} &amp; F_k
\end{pmatrix}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
F_k = I_{(n-k+1) \times (n-k+1)} - 2 \mathbf{z}_k \mathbf{z}_k^T
\quad
\text{with}
\quad
\mathbf{z}_k 
= \frac{\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k}{\|\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k\|}.
\]</div>
<p>This time the first <span class="math notranslate nohighlight">\(k-1\)</span> rows and columns are preserved, while zeros are introduced under the diagonal of the <span class="math notranslate nohighlight">\(k\)</span>-th column. We omit the details (try it!).</p>
<p>We implement the procedure above in Python. We will need the following function. For <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>, let the sign of <span class="math notranslate nohighlight">\(\alpha\)</span> be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{sign}(\alpha)
= 
\begin{cases}
1 &amp; \text{if $\alpha &gt; 0$}\\
0 &amp; \text{if $\alpha = 0$}\\
-1 &amp; \text{if $\alpha &lt; 0$}
\end{cases}
\end{split}\]</div>
<p>In Python, this is done using the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.sign.html"><code class="docutils literal notranslate"><span class="pre">numpy.sign</span></code></a>.</p>
<p>The following function constructs the upper triangular matrix <span class="math notranslate nohighlight">\(R\)</span> by iteratively modifying the relevant block of <span class="math notranslate nohighlight">\(A\)</span>. On the other hand, computing the matrix <span class="math notranslate nohighlight">\(Q\)</span> actually requires extra computational work that is often not needed. We saw that, in the context of the least-squares problem, we really only need to compute <span class="math notranslate nohighlight">\(Q^T \mathbf{b}\)</span> for some input vector <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. This can be done at the same time that <span class="math notranslate nohighlight">\(R\)</span> is constructed, as follows. The key point to note is that <span class="math notranslate nohighlight">\(Q^T \mathbf{b} = H_m \cdots H_1 \mathbf{b}\)</span>.</p>
<p>In our implementation of <code class="docutils literal notranslate"><span class="pre">householder</span></code>, we use both reflections defined above. We will not prove this here, but the particular choice made has good numerical properties. Quoting [TB, Lecture 10] (where <span class="math notranslate nohighlight">\(H^+\)</span> refers to the hyperplane used for the reflection when <span class="math notranslate nohighlight">\(z=1\)</span>):</p>
<blockquote>
<div><p>Mathematically, either choice of sign is satisfactory. However, this is a case where numerical stability – insensitivity to rounding errors – dictates that one choice should be taken rather than the other. For numerical stability, it is desirable to reflect <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to the vector <span class="math notranslate nohighlight">\(z \|\mathbf{x}\| \mathbf{e}_1\)</span> that is not too close to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> itself. […] Suppose that [in the figure above] the angle between <span class="math notranslate nohighlight">\(H^+\)</span> and the <span class="math notranslate nohighlight">\(\mathbf{e}_1\)</span> axis is very small. Then the vector <span class="math notranslate nohighlight">\(\mathbf{v} = \|\mathbf{x}\| \mathbf{e}_1 - \mathbf{x}\)</span> is much smaller than <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> or <span class="math notranslate nohighlight">\(\|\mathbf{x}\| \mathbf{e}_1\)</span>. Thus the calculation of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> represents a subtraction of nearby quantities and will tend to suffer from cancellation errors.</p>
</div></blockquote>
<p>We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.outer.html"><code class="docutils literal notranslate"><span class="pre">numpy.outer</span></code></a> to compute <span class="math notranslate nohighlight">\(\mathbf{z} \mathbf{z}^T\)</span>, which is referred to as an outer product. See <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.copy.html">here</a> for an explanation of <code class="docutils literal notranslate"><span class="pre">numpy.copy</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">householder</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Qtb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    
        <span class="n">y</span> <span class="o">=</span> <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">]</span>
        <span class="n">e1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
        <span class="n">e1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">e1</span> <span class="o">+</span> <span class="n">y</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">/</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        
        <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">:</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">:</span><span class="n">m</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span> <span class="o">@</span> <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">:</span><span class="n">m</span><span class="p">]</span>
        <span class="n">Qtb</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qtb</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span> <span class="o">@</span> <span class="n">Qtb</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">R</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">],</span> <span class="n">Qtb</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We return to our overdetermined system example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="n">R</span><span class="p">,</span> <span class="n">Qtb</span> <span class="o">=</span> <span class="n">householder</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">backsubs</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Qtb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.66666667 0.66666667]
</pre></div>
</div>
</div>
</div>
<p>One advantage of the Householder approach is that it produces a matrix <span class="math notranslate nohighlight">\(Q\)</span> with very good orthogonality, i.e., <span class="math notranslate nohighlight">\(Q^T Q \approx I\)</span>. We give a quick example below comparing Gram-Schmidt and Householder. (The choice of matrix <span class="math notranslate nohighlight">\(A\)</span> will become clearer when we discuss the singular value decomposition in a later chapter.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">U</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)))</span>
<span class="n">V</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)))</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">S</span> <span class="o">@</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span>

<span class="n">Qgs</span><span class="p">,</span> <span class="n">Rgs</span> <span class="o">=</span> <span class="n">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">Qgs</span> <span class="o">@</span> <span class="n">Rgs</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Qgs</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Qgs</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1.4369568046009742e-16
19.745599060592102
</pre></div>
</div>
</div>
</div>
<p>As you can see above, the <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(R\)</span> factors produced by the Gram-Schmidt algorithm do have the property that <span class="math notranslate nohighlight">\(QR \approx A\)</span>. However, <span class="math notranslate nohighlight">\(Q\)</span> is far from orthogonal. (Recall that <code class="docutils literal notranslate"><span class="pre">LA.norm</span></code> computes the Frobenius norm introduced previously.)</p>
<p>On the other hand, Householder reflections perform much better in that respect as we show next. Here we use the implementation of Householder transformations in <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.qr.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.qr</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">Qhh</span><span class="p">,</span> <span class="n">Rhh</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">Qhh</span> <span class="o">@</span> <span class="n">Rhh</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Qhh</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Qhh</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>4.739138228891714e-16
5.33506987519293e-15
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following matrices is upper triangular?</p>
<p>a) <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 2 \\ 0 &amp; 3 \end{pmatrix}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 0 \\ 2 &amp; 3 \end{pmatrix}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span></p>
<p><strong>2</strong> What is the output of the Gram-Schmidt algorithm when applied to a set of linearly independent vectors?</p>
<p>a) An orthogonal basis for the subspace spanned by the vectors.</p>
<p>b) An orthonormal basis for the subspace spanned by the vectors.</p>
<p>c) A set of linearly dependent vectors.</p>
<p>d) A single vector that is orthogonal to all the input vectors.</p>
<p><strong>3</strong> What is the dimension of a hyperplane in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(m\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(m - 1\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(m - 2\)</span></p>
<p>d) 1</p>
<p><strong>4</strong> Which of the following is true about a Householder reflection?</p>
<p>a) It is a reflection across a hyperplane orthogonal to a unit vector.</p>
<p>b) It is a reflection across a unit vector.</p>
<p>c) It is a rotation around a hyperplane orthogonal to a unit vector.</p>
<p>d) It is a rotation around a unit vector.</p>
<p><strong>5</strong> How can a sequence of Householder reflections be used to compute a QR decomposition of a matrix <span class="math notranslate nohighlight">\(A\)</span>?</p>
<p>a) By iteratively introducing zeros above the diagonal of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>b) By iteratively introducing zeros below the diagonal of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>c) By iteratively introducing zeros on the diagonal of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>d) By iteratively introducing zeros in the entire matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>Answer for 1: a. Justification: The text defines an upper triangular matrix as one with all entries below the diagonal equal to zero.</p>
<p>Answer for 2: b. Justification: The text states that the Gram-Schmidt algorithm “produces an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{span}(a_1, \dots, a_m)\)</span>.”</p>
<p>Answer for 3: b. Justification: The text defines a hyperplane as a linear subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> of dimension <span class="math notranslate nohighlight">\(m - 1\)</span>.</p>
<p>Answer for 4: a. Justification: The text defines a Householder reflection as follows: “Let <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^m\)</span> be a unit vector and let <span class="math notranslate nohighlight">\(W\)</span> be the hyperplane orthogonal to it. The reflection across <span class="math notranslate nohighlight">\(W\)</span> is given by <span class="math notranslate nohighlight">\(H = I_{m \times m} - 2\mathbf{z}\mathbf{z}^T\)</span>.”</p>
<p>Answer for 5: b. Justification: The text states, “One way to construct a (full) QR decomposition of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> is to find a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(H_1,\ldots,H_m\)</span> that triangularize <span class="math notranslate nohighlight">\(A\)</span>: <span class="math notranslate nohighlight">\(H_m \cdots H_2H_1A = R\)</span> for an upper-triangular matrix <span class="math notranslate nohighlight">\(R\)</span>.”</p>
</section>
&#13;

<h2><span class="section-number">2.4.1. </span>Matrix form of Gram-Schmidt<a class="headerlink" href="#matrix-form-of-gram-schmidt" title="Link to this heading">#</a></h2>
<p>In this subsection, we prove the <em>Gram-Schmidt Theorem</em> and introduce a fruitful matrix perspective.</p>
<p><strong>Gram-Schmidt algorithm</strong> Let <span class="math notranslate nohighlight">\(\mathbf{a}_1,\ldots,\mathbf{a}_m\)</span> be linearly independent. We use the Gram-Schmidt algorithm<span class="math notranslate nohighlight">\(\idx{Gram-Schmidt algorithm}\xdi\)</span> to obtain an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\)</span>. The process takes advantage of the properties of the orthogonal projection derived above. In essence we add the vectors <span class="math notranslate nohighlight">\(\mathbf{a}_i\)</span> one by one, but only after taking out their orthogonal projection on the previously included vectors. The outcome spans the same subspace and the <em>Orthogonal Projection Theorem</em> ensures orthogonality.</p>
<p><em>Proof idea:</em> <em>(Gram-Schmidt)</em> Suppose first that <span class="math notranslate nohighlight">\(m=1\)</span>. In that case, all that needs to be done is to divide <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span> by its norm to obtain a unit vector whose span is the same as <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span>, that is, we set <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)</span>.</p>
<p>Suppose now that <span class="math notranslate nohighlight">\(m=2\)</span>. We first let <span class="math notranslate nohighlight">\(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)</span> as in the previous case. Then we subtract from <span class="math notranslate nohighlight">\(\mathbf{a}_2\)</span> its projection on <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span>, that is, we set <span class="math notranslate nohighlight">\(\mathbf{v}_2 = \mathbf{a}_2 - \langle \mathbf{q}_1, \mathbf{a}_2 \rangle \,\mathbf{q}_1\)</span>. It is easily checked that <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> is orthogonal to <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span> (see the proof of the <em>Orthogonal Projection Theorem</em> for a similar calculation). Moreover, because <span class="math notranslate nohighlight">\(\mathbf{a}_2\)</span> is a linear combination of <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span>, we have <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\mathbf{v}_2)
= \mathrm{span}(\mathbf{a}_1,\mathbf{a}_2)\)</span>. It remains to divide by the norm of the resulting vector: <span class="math notranslate nohighlight">\(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|}\)</span>.</p>
<p>For general <span class="math notranslate nohighlight">\(m\)</span>, we proceed similarly but project onto the subspace spanned by the previously added vectors at each step.</p>
<p><img alt="Gram–Schmidt process (with help from ChatGPT; inspired by Source)" src="../Images/357bfa05be44b654c5858a352b39044e.png" data-original-src="https://mmids-textbook.github.io/_images/gram-schmidt.png"/></p>
<p><em>Proof:</em> <em>(Gram-Schmidt)</em> The first step of the induction is described above. Then the general inductive step is the following. Assume that we have constructed orthonormal vectors <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
U_{j-1} := 
\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = 
\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}).
\]</div>
<p><em>Constructing <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>:</em> By the <em>Properties of Orthonormal Lists</em>, <span class="math notranslate nohighlight">\(\{\mathbf{q}\}_{i=1}^{j-1}\)</span> is an independent list and therefore forms an orthonormal basis for <span class="math notranslate nohighlight">\(U_{j-1}\)</span>. So we can compute the orthogonal projection of <span class="math notranslate nohighlight">\(\mathbf{a}_j\)</span> on <span class="math notranslate nohighlight">\(U_{j-1}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_{U_{j-1}}\mathbf{a}_j
= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i,
\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(r_{ij} = \langle \mathbf{q}_i , \mathbf{a}_j\rangle\)</span>. And we set</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_j 
= \mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j
= \mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i
\quad
\text{and}
\quad
\mathbf{q}_j 
= \frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}.
\]</div>
<p>The last step is possible because:</p>
<p><strong>LEMMA</strong> <span class="math notranslate nohighlight">\(\|\mathbf{v}_j\| &gt; 0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Indeed otherwise <span class="math notranslate nohighlight">\(\mathbf{a}_j\)</span>
would be equal to its projection <span class="math notranslate nohighlight">\(\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)</span> which would contradict linear independence of the <span class="math notranslate nohighlight">\(\mathbf{a}_k\)</span>’s. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span> is of unit norm by construction. It is also orthogonal to <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})\)</span> by the definition of <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span> and the <em>Orthogonal Projection Theorem</em>. So <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)</span> form an orthonormal list.</p>
<p><em>Pushing the induction through:</em> It remains to prove that <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\)</span>. Because by induction <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = 
\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)</span>, all we have to prove are the following two claims.</p>
<p><strong>LEMMA</strong> <span class="math notranslate nohighlight">\(\mathbf{q}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> By construction,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{q}_j 
= \frac{1}{\|\mathbf{v}_j\|} \left\{\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\right\}
= \frac{1}{\|\mathbf{v}_j\|} \mathbf{a}_j + \frac{1}{\|\mathbf{v}_j\|} \mathrm{proj}_{U_{j-1}}\mathbf{a}_j.
\]</div>
<p>By definition of the orthogonal projection,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in U_{j-1}= \mathrm{span} (\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}) \subseteq \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j}).
\]</div>
<p>Hence we have written <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span> as a linear combination of vectors in <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)</span>. That proves the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>LEMMA</strong> <span class="math notranslate nohighlight">\(\mathbf{a}_j \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Unrolling the calculations above, <span class="math notranslate nohighlight">\(\mathbf{a}_j\)</span> can be re-written as the following linear combination of <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{a}_j
&amp;= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \mathbf{v}_j\\
&amp;= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{v}_j\| \mathbf{q}_j\\
&amp;= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\| \mathbf{q}_j\\
&amp;= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i
+ \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\| \,\mathbf{q}_j\\
&amp;= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i
+ r_{jj} \,\mathbf{q}_j,
\end{align*}\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(r_{jj} = \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\| = \|\mathbf{v}_j\|\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Hence <span class="math notranslate nohighlight">\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)</span> forms an orthonormal list with <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)</span>. So induction goes through. That concludes the proof of the theorem. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We implement the Gram-Schmidt algorithm in Python. For reasons that will become clear in the next subsection, we output both the <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>’s and <span class="math notranslate nohighlight">\(r_{ij}\)</span>’s, each in matrix form. Here we use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html"><code class="docutils literal notranslate"><span class="pre">numpy.dot</span></code></a> to compute inner products.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">))</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="p">):</span>
            <span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">A</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
            <span class="n">v</span> <span class="o">-=</span> <span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">Q</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>
        <span class="n">R</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">Q</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">/</span><span class="n">R</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> Let’s try a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1. 0.]
 [0. 1.]
 [1. 1.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[ 0.70710678 -0.40824829]
 [ 0.          0.81649658]
 [ 0.70710678  0.40824829]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1.41421356 0.70710678]
 [0.         1.22474487]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Matrix form</strong> Let <span class="math notranslate nohighlight">\(\mathbf{a}_1,\ldots,\mathbf{a}_m \in \mathbb{R}^n\)</span> be linearly independent. Above, we presented the Gram-Schmidt algorithm to obtain an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\)</span>. We revisit it in matrix form.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A =
\begin{pmatrix}
| &amp;  &amp; | \\
\mathbf{a}_1 &amp; \ldots &amp; \mathbf{a}_m \\
| &amp;  &amp; | 
\end{pmatrix}
\quad
\text{and}
\quad
Q =
\begin{pmatrix}
| &amp;  &amp; | \\
\mathbf{q}_1 &amp; \ldots &amp; \mathbf{q}_m \\
| &amp;  &amp; | 
\end{pmatrix}.
\end{split}\]</div>
<p>Recalling that, for all <span class="math notranslate nohighlight">\(j\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{a}_j
= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
\]</div>
<p>the output of the Gram-Schmidt algorithm can be written in the following compact form, known as a <a class="reference external" href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a><span class="math notranslate nohighlight">\(\idx{QR decomposition}\xdi\)</span>,</p>
<div class="math notranslate nohighlight">
\[
A = QR
\]</div>
<p>where column <span class="math notranslate nohighlight">\(i\)</span> of the <span class="math notranslate nohighlight">\(m \times m\)</span> matrix <span class="math notranslate nohighlight">\(R\)</span> contains the coefficients of the linear combination of the <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>’s that produce <span class="math notranslate nohighlight">\(\mathbf{a}_i\)</span>.</p>
<p>By the proof of the <em>Gram-Schmidt Theorem</em>, <span class="math notranslate nohighlight">\(\mathbf{a}_i \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_i)\)</span>. So column <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(R\)</span> has only zeros below the diagonal. Hence <span class="math notranslate nohighlight">\(R\)</span> has a special structure we have previously encountered: it is upper triangular. The proof also established that the diagonal elements of <span class="math notranslate nohighlight">\(R\)</span> are strictly positive.</p>
<p><strong>DEFINITION</strong> <strong>(Triangular matrix)</strong> <span class="math notranslate nohighlight">\(\idx{triangular matrix}\xdi\)</span> A matrix <span class="math notranslate nohighlight">\(R = (r_{ij})_{i,j} \in \mathbb{R}^{n \times m}\)</span> is upper-triangular if all entries below the diagonal are zero, that is, if <span class="math notranslate nohighlight">\(i &gt; j\)</span> implies <span class="math notranslate nohighlight">\(r_{ij} = 0\)</span>. Similarly, a lower-triangular matrix has zeros above the diagonal. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>An upper-triangular matrix looks like this</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R = \begin{bmatrix}
r_{1,1} &amp; r_{1,2} &amp; r_{1,3} &amp; \ldots &amp; r_{1,n} \\
0        &amp; r_{2,2} &amp; r_{2,3} &amp; \ldots &amp; r_{2,n} \\
        &amp; 0        &amp; \ddots &amp; \ddots &amp; \vdots  \\
        &amp;         &amp; \ddots       &amp; \ddots &amp; r_{n-1,n} \\
0       &amp;         &amp;        &amp;  0      &amp; r_{n,n}
\end{bmatrix}.
\end{split}\]</div>
<p><strong>Remarks:</strong></p>
<p>a) If the input vectors <span class="math notranslate nohighlight">\(\mathbf{a}_1,\ldots,\mathbf{a}_m\)</span> are not linearly independent (in which case we say that the matrix <span class="math notranslate nohighlight">\(A\)</span> is rank-deficient), the Gram-Schmidt algorithm will fail. Indeed, at some point we will have that <span class="math notranslate nohighlight">\(\mathbf{a}_j \in U_{j-1}\)</span> and the normalization of <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span> will not be possible. In that case, one can instead use a technique called <a class="reference external" href="https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting">column pivoting</a>, which we will not describe.</p>
<p>b) The QR decomposition we have derived here is technically called a reduced QR decomposition. In a full QR decomposition<span class="math notranslate nohighlight">\(\idx{full QR decomposition}\xdi\)</span>, the matrix <span class="math notranslate nohighlight">\(Q\)</span> is square and orthogonal. In other words, the columns of such a <span class="math notranslate nohighlight">\(Q\)</span> form an orthonormal basis of the full space <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Let <span class="math notranslate nohighlight">\(A = Q_1 R_1\)</span> be a reduced QR decomposition, as obtained through the Gram-Schmidt algorithm. Then the columns of <span class="math notranslate nohighlight">\(Q_1\)</span> form an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{col}(A)\)</span> and can be completed into an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> by adding further vectors <span class="math notranslate nohighlight">\(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\)</span>. Let <span class="math notranslate nohighlight">\(Q_2\)</span> be the matrix with columns <span class="math notranslate nohighlight">\(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\)</span>. Then a full QR decomposition of <span class="math notranslate nohighlight">\(A\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q
=
\begin{pmatrix}
Q_1 &amp; Q_2
\end{pmatrix}
\qquad
R
=
\begin{pmatrix}
R_1\\ 
\mathbf{0}_{(n-m)\times m}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{0}_{(n-m)\times m}\)</span> is the all-zero matrix of size <span class="math notranslate nohighlight">\((n-m)\times m\)</span>. A numerical method for computing a full QR decomposition is presented in a later subsection.</p>
<p>c) The Gram-Schmidt algorithm is appealing geometrically, but it is known to have numerical issues. Other methods exist for computing QR decompositions with better numerical properties. We discuss such a method in a later subsection. (See that same subsection for an example where the <span class="math notranslate nohighlight">\(\mathbf{q}_j\)</span>’s produced by Gram-Schmidt are far from orthogonal.)</p>
&#13;

<h2><span class="section-number">2.4.2. </span>Least squares via QR<a class="headerlink" href="#least-squares-via-qr" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix with linearly independent columns and let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> be a vector. Recall that a solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> to the linear least squares problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2
\]</div>
<p>satisfies the normal equations</p>
<div class="math notranslate nohighlight">
\[
A^T A \mathbf{x}^* = A^T \mathbf{b}.
\]</div>
<p><strong>Solving the normal equations</strong> In a first linear algebra course, one learns how to solve linear systems such as the normal equations. For this task a common approach is Gaussian elimination, or row reduction. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination">Wikipedia</a>:</p>
<blockquote>
<div><p>To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. […] Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. […] The process of row reduction […] can be divided into two parts. The first part (sometimes called forward elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.</p>
</div></blockquote>
<p><strong>Figure:</strong> An example of Gaussian elimination (<a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination">Source</a>)</p>
<p><img alt="Gaussian elimination" src="../Images/dc18b77fc722fe6abe5812c927848a72.png" data-original-src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65d92f997de9f7ad787b95d08fcd25dca828dd93"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>We will not go over Gaussian elimination here. In this subsection, we develop an alternative approach to solving the normal equations using the QR decomposition. We will need one component of Gaussian elimination, back substitution<span class="math notranslate nohighlight">\(\idx{back substitution}\xdi\)</span>. It is based on the observation that triangular systems of equations are straightforward to solve. We start with an example.</p>
<p><strong>EXAMPLE:</strong> Here is a concrete example of back substitution. Consider the system <span class="math notranslate nohighlight">\(R \mathbf{x} = \mathbf{b}\)</span> with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R
=
\begin{pmatrix}
2 &amp; -1 &amp; 2\\
0 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 2
\end{pmatrix}
\qquad
\mathbf{b}
=
\begin{pmatrix}
0\\
-2\\
0
\end{pmatrix}.
\end{split}\]</div>
<p>That corresponds to the linear equations</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;2 x_1 - x_2 + 2x_3 = 0\\
&amp;x_2 + x_3 = -2\\
&amp;2 x_3 = 0
\end{align*}\]</div>
<p>The third equation gives <span class="math notranslate nohighlight">\(x_3 = 0/2 = 0\)</span>. Plugging into the second one, we get <span class="math notranslate nohighlight">\(x_2 = -2 - x_3 = -2\)</span>. Plugging into the first one, we finally have <span class="math notranslate nohighlight">\(x_1 = (x_2 - 2 x_3)/2 = -1\)</span>. So the solution is <span class="math notranslate nohighlight">\(\mathbf{x} = (-1,-2,0)\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>In general, solving a triangular system of equations works as follows. Let <span class="math notranslate nohighlight">\(R = (r_{i,j})_{i,j} \in \mathbb{R}^{m \times m}\)</span> be upper-triangular and let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^m\)</span> be the left-hand vector, i.e., we want to solve the system</p>
<div class="math notranslate nohighlight">
\[
R \mathbf{x} = \mathbf{b}.
\]</div>
<p>Starting from the last row of the system, <span class="math notranslate nohighlight">\(r_{m,m} x_m = b_m\)</span> or <span class="math notranslate nohighlight">\(x_m = b_m/r_{m,m}\)</span>, assuming that <span class="math notranslate nohighlight">\(r_{m,m} \neq 0\)</span>. Moving to the second-to-last row, <span class="math notranslate nohighlight">\(r_{m-1,m-1} x_{m-1} + r_{m-1,m} x_m = b_{m-1}\)</span> or <span class="math notranslate nohighlight">\(x_{m-1} = (b_{m-1} - r_{m-1,m} x_m)/r_{m-1,m-1}\)</span>, assuming that <span class="math notranslate nohighlight">\(r_{m-1,m-1} \neq 0\)</span>. And so on. This procedure is known as <a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution">back substitution</a>.</p>
<p>Analogously, in the lower triangular case <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{m \times m}\)</span>, we have <a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution">forward substitution</a><span class="math notranslate nohighlight">\(\idx{forward substitution}\xdi\)</span>. These procedures implicitly define an inverse for <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(L\)</span> <em>when the diagonal elements are all non-zero</em>. We will not write them down explicitly here.</p>
<p>We implement back substitution in Python. In our naive implementation, we assume that the diagonal entries are not zero, which will suffice for our purposes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">backsubs</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">m</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">m</span><span class="p">]))</span><span class="o">/</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Forward substitution is implemented similarly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]))</span><span class="o">/</span><span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Using QR</strong> We show how to solve the normal equations via the QR decomposition.</p>
<p>1- Construct an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{col}(A)\)</span> through a QR decomposition</p>
<div class="math notranslate nohighlight">
\[
A = QR.
\]</div>
<p>2- Form the orthogonal projection matrix</p>
<div class="math notranslate nohighlight">
\[
P = Q Q^T.
\]</div>
<p>3- Apply the projection to <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> and observe that, by the proof of the <em>Normal Equations</em>, <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[
A \mathbf{x}^* = Q Q^T \mathbf{b}. 
\]</div>
<p>4- Plug in the QR decomposition for <span class="math notranslate nohighlight">\(A\)</span> to get</p>
<div class="math notranslate nohighlight">
\[
QR \mathbf{x}^* = Q Q^T \mathbf{b}. 
\]</div>
<p>5- Multiply both sides by <span class="math notranslate nohighlight">\(Q^T\)</span> and use <span class="math notranslate nohighlight">\(Q^T Q = I_{m \times m}\)</span></p>
<div class="math notranslate nohighlight">
\[
R \mathbf{x}^* = Q^T \mathbf{b}. 
\]</div>
<p>6- Solving this system for <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is straightforward because <span class="math notranslate nohighlight">\(R\)</span> is upper triangular via back substitution.</p>
<p><strong>THEOREM</strong> <strong>(Least Squares via QR)</strong> <span class="math notranslate nohighlight">\(\idx{least squares via QR}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> be an <span class="math notranslate nohighlight">\(n\times m\)</span> matrix with linearly independent columns, let <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span> be a vector,
and let <span class="math notranslate nohighlight">\(A = QR\)</span> be a QR decomposition of <span class="math notranslate nohighlight">\(A\)</span>. The solution to the linear least-squares problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2.
\]</div>
<p>satisfies</p>
<div class="math notranslate nohighlight">
\[
R \mathbf{x}^* = Q^T \mathbf{b}. 
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Note that, in reality, we do not need to form the matrix <span class="math notranslate nohighlight">\(Q Q^T\)</span>.</p>
<p>We implement the QR approach to least squares.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">backsubs</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We return to our simple overdetermined system example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ls_by_qr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.66666667 0.66666667]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
&#13;

<h2><span class="section-number">2.4.3. </span>Householder transformations<a class="headerlink" href="#householder-transformations" title="Link to this heading">#</a></h2>
<p>While the Gram-Schmidt algorithm gives a natural way to compute a (reduced) QR decomposition, there are many other numerical algorithms for this purpose. Some have better numerical behavior, specifically in terms of how they handle roundoff error. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Round-off_error">Wikipedia</a>:</p>
<blockquote>
<div><p>A roundoff error, also called rounding error, is the difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic. Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. […] When a sequence of calculations with an input involving roundoff error are made, errors may accumulate, sometimes dominating the calculation.</p>
</div></blockquote>
<p>We will not prove this here, but the following method based on Householder reflections is numerically more <a class="reference external" href="https://en.wikipedia.org/wiki/Numerical_stability#Stability_in_numerical_linear_algebra">stable</a>.</p>
<p>Recall that a square matrix <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{m\times m}\)</span> is orthogonal if <span class="math notranslate nohighlight">\(Q^T Q = Q Q^T = I_{m \times m}\)</span>. In words, the matrix inverse of <span class="math notranslate nohighlight">\(Q\)</span> is its transpose. This is equivalent to the columns of <span class="math notranslate nohighlight">\(Q\)</span> forming an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> (why?).</p>
<p>It can be shown that the product of two orthogonal matrices <span class="math notranslate nohighlight">\(Q_1\)</span> and <span class="math notranslate nohighlight">\(Q_2\)</span> is also orthogonal. (Try it!)</p>
<p>An important property of orthogonal matrices is that they preserve inner products: if <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{m\times m}\)</span> is orthogonal, then for any <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^m\)</span></p>
<div class="math notranslate nohighlight">
\[
\langle Q \mathbf{x}, Q \mathbf{y} \rangle 
= (Q \mathbf{x})^T  Q \mathbf{y}
= \mathbf{x}^T Q^T Q \mathbf{y}
= \mathbf{x}^T \mathbf{y}
= \langle \mathbf{x}, \mathbf{y} \rangle.
\]</div>
<p>In particular, orthogonal matrices preserve norms and angles.</p>
<!--ONLINE

Recall that the angle $\theta$ between $\mathbf{x}$ and $\mathbf{y}$ satisfies

$$
\cos \theta
= \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|}.
$$
--><p><strong>Reflections</strong> One such family of transformations are reflections.</p>
<p><strong>DEFINITION</strong> <strong>(Hyperplane)</strong> <span class="math notranslate nohighlight">\(\idx{hyperplane}\xdi\)</span> A hyperplane <span class="math notranslate nohighlight">\(W\)</span> is a linear subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> of dimension <span class="math notranslate nohighlight">\(m-1\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>DEFINITION</strong> <strong>(Householder Reflection)</strong> <span class="math notranslate nohighlight">\(\idx{Householder reflection}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^m\)</span> be a unit vector and let <span class="math notranslate nohighlight">\(W\)</span> be the hyperplane orthogonal to it. The reflection across <span class="math notranslate nohighlight">\(W\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
H = I_{m \times m} - 2 \mathbf{z} \mathbf{z}^T.
\]</div>
<p>This is referred to as a Householder reflection. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In words, we subtract twice the projection onto <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, as depicted below.</p>
<p><img alt="Householder reflection. (with the help of Claude; inspired by (Source))" src="../Images/7c283ce7dd76917f07050963bca3ee2b.png" data-original-src="https://mmids-textbook.github.io/_images/householder.png"/></p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(H = I_{m\times m} - 2\mathbf{z}\mathbf{z}^T\)</span> be a Householder reflection. Then <span class="math notranslate nohighlight">\(H\)</span> is an orthogonal matrix. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> We check the definition:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
H^T H 
&amp;= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)^T (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)\\
&amp;= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T) (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)\\
&amp;= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T\mathbf{z}\mathbf{z}^T\\
&amp;= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T
\end{align*}\]</div>
<p>which is equal to <span class="math notranslate nohighlight">\(I_{m\times m}\)</span>. The calculation for <span class="math notranslate nohighlight">\(H H^T\)</span> is the same.<span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>QR decomposition by introducing zeros</strong> We return to QR decompositions. One way to construct a (full) QR decomposition of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> is to find a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(H_1, \ldots, H_m\)</span> that triangularize <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[
H_m \cdots H_2 H_1 A = R
\]</div>
<p>for an upper-triangular matrix <span class="math notranslate nohighlight">\(R\)</span>. Indeed, by the properties of orthogonal matrices, we then have</p>
<div class="math notranslate nohighlight">
\[
A 
= H_1^T H_2^T \cdots H_m^T H_m \cdots H_2 H_1 A
= H_1^T H_2^T \cdots H_m^T R
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q = H_1^T H_2^T \cdots H_m^T\)</span> is itself orthogonal as a product of orthogonal matrices. So to proceed we need to identify orthogonal matrices that have the effect of introducing zeros below the diagonal, as illustrated below:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_2 H_1 A = 
\begin{pmatrix}
\times &amp; \times &amp; \times &amp; \times &amp; \times\\
0 &amp; \times &amp; \times &amp; \times &amp; \times\\
0 &amp; 0 &amp; \times &amp; \times &amp; \times\\
0 &amp; 0 &amp; \boxed{\times} &amp; \times &amp; \times\\
0 &amp; 0 &amp; \boxed{\times} &amp; \times &amp; \times\\
0 &amp; 0 &amp; \boxed{\times} &amp; \times &amp; \times\\
\end{pmatrix}.
\end{split}\]</div>
<p>It turns out that a well-chosen Householder reflection does the trick. Let <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> be the first column of <span class="math notranslate nohighlight">\(A\)</span> and take</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}
\quad
\text{and}
\quad
H_1 = I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{e}_1^{(n)}\)</span> is the first vector in the canonical basis of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. As depicted below, this choice sends <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}  
= 
\begin{pmatrix}
\|\mathbf{y}_1\|\\
0 \\
\vdots \\
0
\end{pmatrix}.
\end{split}\]</div>
<p>(It is clear that if <span class="math notranslate nohighlight">\(H_1 \mathbf{y}_1\)</span> is proportional to <span class="math notranslate nohighlight">\(\mathbf{e}_1^{(n)}\)</span>, than it can only be <span class="math notranslate nohighlight">\(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)</span> or <span class="math notranslate nohighlight">\(-\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)</span>. Prove it!)</p>
<p><img alt="Introducing zeros by Householder reflection (with the help of Claude; inspired by (Source))" src="../Images/19bf9b4f1f98bc25a2715d91da61fc5b.png" data-original-src="https://mmids-textbook.github.io/_images/spiegelebene.png"/></p>
<p><strong>LEMMA</strong> <strong>(Householder)</strong> <span class="math notranslate nohighlight">\(\idx{Householder lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span>, <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> and <span class="math notranslate nohighlight">\(H_1\)</span> be as above. Then</p>
<div class="math notranslate nohighlight">
\[
H_1 \mathbf{y}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> The proof by picture is in the figure above.</p>
<p><em>Proof:</em> Note that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2
&amp;= (\|\mathbf{y}_1\| - y_{1,1})^2
+ \sum_{j=2}^n y_{1,j}^2\\
&amp;= \|\mathbf{y}_1\|^2 -2 \|\mathbf{y}_1\| y_{1,1} + y_{1,1}^2 + \sum_{j=2}^n y_{1,j}^2\\
&amp;= 2(\|\mathbf{y}_1\|^2 - \|\mathbf{y}_1\| y_{1,1})
\end{align*}\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
2 \mathbf{z}_1 \mathbf{z}_1^T \mathbf{y}_1
&amp;= 2 \mathbf{z}_1 \frac{\|\mathbf{y}_1\| \,(\mathbf{e}_1^{(n)})^T \mathbf{y}_1 - \mathbf{y}_1^T \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}\\
&amp;= 2 \frac{\|\mathbf{y}_1\| y_{1,1} - \|\mathbf{y}_1\|^2}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2} (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)\\
&amp;= - (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)
\end{align*}\]</div>
<p>where we used the previous equation. Hence</p>
<div class="math notranslate nohighlight">
\[
H_1 \mathbf{y}_1
= (I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T) \,\mathbf{y}_1
= \mathbf{y}_1 + (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) = \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}.
\]</div>
<p>That establishes the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The upshot is that multiplying <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(H_1\)</span> introduces zeros below the diagonal in the first column. To see this, recall that one interpretation of the matrix-matrix product is that each column of the second matrix gets multiplied by the first one. By the <em>Householder Lemma</em>, applying <span class="math notranslate nohighlight">\(H_1\)</span> to <span class="math notranslate nohighlight">\(A\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
H_1 A
= \begin{pmatrix}
H_1 \mathbf{y}_1 &amp; H_1 A_{\cdot,2} &amp; \cdots &amp; H_1 A_{\cdot,m} 
\end{pmatrix}
= \begin{pmatrix}
\|\mathbf{y}_1\| \mathbf{e}_1^{(n)} &amp; H_1 A_{\cdot,2} &amp; \cdots &amp; H_1 A_{\cdot,m}
\end{pmatrix}
\]</div>
<p>So the first column is now proportional to <span class="math notranslate nohighlight">\(\mathbf{e}_1\)</span>, which has zeros in all but the first element. (What should we do if <span class="math notranslate nohighlight">\(\mathbf{y}_1\)</span> is already equal to <span class="math notranslate nohighlight">\(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)</span>?)</p>
<p>It turns that there is another choice of Householder reflection. Indeed, it can be shown that</p>
<div class="math notranslate nohighlight">
\[
\tilde{\mathbf{z}}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1}{\| \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1\|}
\quad
\text{and}
\quad
\tilde{H}_1 = I_{n\times n} - 2\tilde{\mathbf{z}}_1 \tilde{\mathbf{z}}_1^T
\]</div>
<p>are such that <span class="math notranslate nohighlight">\(\tilde{H}_1 \mathbf{y}_1 = - \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}\)</span> (try it!).</p>
<p><strong>Putting everything together</strong> We have shown how to introduce zeros below the diagonal in the first column of a matrix. To introduce zeros in the second column below the diagonal we use a block matrix. Recall that
if <span class="math notranslate nohighlight">\(A_{ij} \in \mathbb{R}^{n_i \times m_j}\)</span> and <span class="math notranslate nohighlight">\(B_{ij} \in \mathbb{R}^{m_i \times p_j}\)</span> for <span class="math notranslate nohighlight">\(i,j = 1, 2\)</span>, then we have the following formula</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
A_{11} &amp; A_{12}\\
A_{21} &amp; A_{22}
\end{pmatrix}
\begin{pmatrix}
B_{11} &amp; B_{12}\\
B_{21} &amp; B_{22}
\end{pmatrix}
=
\begin{pmatrix}
A_{11} B_{11} + A_{12} B_{21} &amp; A_{11} B_{12} + A_{12} B_{22}\\
A_{21} B_{11} + A_{22} B_{21} &amp; A_{21} B_{12} + A_{22} B_{22}
\end{pmatrix}.
\end{split}\]</div>
<p>Now consider the following block matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_2
= 
\begin{pmatrix}
1 &amp; \mathbf{0} \\
\mathbf{0} &amp; F_2
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(F_2\)</span> is the following Householder reflection. Write the second column of <span class="math notranslate nohighlight">\(H_1 A\)</span> as <span class="math notranslate nohighlight">\((y^{(2)}, \mathbf{y}_2)\)</span>. That is, <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span> are the entries <span class="math notranslate nohighlight">\(2,\ldots, n\)</span> of that column. Define</p>
<div class="math notranslate nohighlight">
\[
F_2 = I_{(n-1) \times (n-1)} - 2 \mathbf{z}_2 \mathbf{z}_2^T
\quad
\text{with}
\quad
\mathbf{z}_2 
= \frac{\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2}{\|\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2\|}
\]</div>
<p>where now <span class="math notranslate nohighlight">\(\mathbf{e}_1^{(n-1)} \in \mathbb{R}^{n-1}\)</span>. By the <em>Householder Lemma</em>, we have <span class="math notranslate nohighlight">\(F_2 \mathbf{y}_2 = \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}\)</span>. It can be shown that <span class="math notranslate nohighlight">\(\mathbf{y}_2 \neq \mathbf{0}\)</span> when the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent. (Try it!)</p>
<p>Applying <span class="math notranslate nohighlight">\(H_2\)</span> to <span class="math notranslate nohighlight">\(H_1 A\)</span> preserves the first row and column, and introduces zeros under the diagonal in the second column. To see this, first re-write <span class="math notranslate nohighlight">\(H_1 A\)</span> in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_1 A
= 
\begin{pmatrix}
\|\mathbf{y}_1\|  &amp; \mathbf{g}_2^T \\
\mathbf{0} &amp; G_2
\end{pmatrix}
\end{split}\]</div>
<p>where we used our previous observation about the first column of <span class="math notranslate nohighlight">\(H_1 A\)</span> and where <span class="math notranslate nohighlight">\(\mathbf{g}_2 \in \mathbb{R}^{m-1}\)</span>, <span class="math notranslate nohighlight">\(G_2 \in \mathbb{R}^{(n-1)\times (m-1)}\)</span>. One important point to note: the first column of <span class="math notranslate nohighlight">\(G_2\)</span> is equal to <span class="math notranslate nohighlight">\(\mathbf{y}_2\)</span>. Now multiply by <span class="math notranslate nohighlight">\(H_2\)</span> to get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_2 H_1 A
= 
\begin{pmatrix}
1 &amp; \mathbf{0} \\
\mathbf{0} &amp; F_2
\end{pmatrix}
\begin{pmatrix}
\|\mathbf{y}_1\|  &amp; \mathbf{g}_2^T \\
\mathbf{0} &amp; G_2
\end{pmatrix}
= 
\begin{pmatrix}
\|\mathbf{y}_1\| &amp; \mathbf{g}_2^T \\
\mathbf{0} &amp; F_2 G_2
\end{pmatrix}.
\end{split}\]</div>
<p>Computing the block <span class="math notranslate nohighlight">\(F_2 G_2\)</span> column by column we get</p>
<div class="math notranslate nohighlight">
\[
F_2 G_2
= \begin{pmatrix}
F_2 \mathbf{y}_2 &amp; F_2 (G_2)_{\cdot,2} &amp; \cdots &amp; F_2 (G_2)_{\cdot,m-1} 
\end{pmatrix}
= \begin{pmatrix}
\|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)} &amp; F_2 (G_2)_{\cdot,2} &amp; \cdots &amp; F_2 (G_2)_{\cdot,m-1}, 
\end{pmatrix}
\]</div>
<p>where <span class="math notranslate nohighlight">\((G_2)_{\cdot,j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th column of <span class="math notranslate nohighlight">\(G_2\)</span>. So the second column of <span class="math notranslate nohighlight">\(H_2 H_1 A\)</span> has zeros in all but the first two elements.</p>
<p>And so on. At Step <span class="math notranslate nohighlight">\(k\)</span>, we split the <span class="math notranslate nohighlight">\(k\)</span>-th column of <span class="math notranslate nohighlight">\(H_{k-1} \cdots H_1 A\)</span> into its first <span class="math notranslate nohighlight">\(k-1\)</span> and last <span class="math notranslate nohighlight">\(n-k+1\)</span> entries <span class="math notranslate nohighlight">\((\mathbf{y}^{(k)}, \mathbf{y}_k)\)</span> and form the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_k
= 
\begin{pmatrix}
I_{(k-1)\times (k-1)} &amp; \mathbf{0} \\
\mathbf{0} &amp; F_k
\end{pmatrix}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
F_k = I_{(n-k+1) \times (n-k+1)} - 2 \mathbf{z}_k \mathbf{z}_k^T
\quad
\text{with}
\quad
\mathbf{z}_k 
= \frac{\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k}{\|\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k\|}.
\]</div>
<p>This time the first <span class="math notranslate nohighlight">\(k-1\)</span> rows and columns are preserved, while zeros are introduced under the diagonal of the <span class="math notranslate nohighlight">\(k\)</span>-th column. We omit the details (try it!).</p>
<p>We implement the procedure above in Python. We will need the following function. For <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>, let the sign of <span class="math notranslate nohighlight">\(\alpha\)</span> be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{sign}(\alpha)
= 
\begin{cases}
1 &amp; \text{if $\alpha &gt; 0$}\\
0 &amp; \text{if $\alpha = 0$}\\
-1 &amp; \text{if $\alpha &lt; 0$}
\end{cases}
\end{split}\]</div>
<p>In Python, this is done using the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.sign.html"><code class="docutils literal notranslate"><span class="pre">numpy.sign</span></code></a>.</p>
<p>The following function constructs the upper triangular matrix <span class="math notranslate nohighlight">\(R\)</span> by iteratively modifying the relevant block of <span class="math notranslate nohighlight">\(A\)</span>. On the other hand, computing the matrix <span class="math notranslate nohighlight">\(Q\)</span> actually requires extra computational work that is often not needed. We saw that, in the context of the least-squares problem, we really only need to compute <span class="math notranslate nohighlight">\(Q^T \mathbf{b}\)</span> for some input vector <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. This can be done at the same time that <span class="math notranslate nohighlight">\(R\)</span> is constructed, as follows. The key point to note is that <span class="math notranslate nohighlight">\(Q^T \mathbf{b} = H_m \cdots H_1 \mathbf{b}\)</span>.</p>
<p>In our implementation of <code class="docutils literal notranslate"><span class="pre">householder</span></code>, we use both reflections defined above. We will not prove this here, but the particular choice made has good numerical properties. Quoting [TB, Lecture 10] (where <span class="math notranslate nohighlight">\(H^+\)</span> refers to the hyperplane used for the reflection when <span class="math notranslate nohighlight">\(z=1\)</span>):</p>
<blockquote>
<div><p>Mathematically, either choice of sign is satisfactory. However, this is a case where numerical stability – insensitivity to rounding errors – dictates that one choice should be taken rather than the other. For numerical stability, it is desirable to reflect <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to the vector <span class="math notranslate nohighlight">\(z \|\mathbf{x}\| \mathbf{e}_1\)</span> that is not too close to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> itself. […] Suppose that [in the figure above] the angle between <span class="math notranslate nohighlight">\(H^+\)</span> and the <span class="math notranslate nohighlight">\(\mathbf{e}_1\)</span> axis is very small. Then the vector <span class="math notranslate nohighlight">\(\mathbf{v} = \|\mathbf{x}\| \mathbf{e}_1 - \mathbf{x}\)</span> is much smaller than <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> or <span class="math notranslate nohighlight">\(\|\mathbf{x}\| \mathbf{e}_1\)</span>. Thus the calculation of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> represents a subtraction of nearby quantities and will tend to suffer from cancellation errors.</p>
</div></blockquote>
<p>We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.outer.html"><code class="docutils literal notranslate"><span class="pre">numpy.outer</span></code></a> to compute <span class="math notranslate nohighlight">\(\mathbf{z} \mathbf{z}^T\)</span>, which is referred to as an outer product. See <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.copy.html">here</a> for an explanation of <code class="docutils literal notranslate"><span class="pre">numpy.copy</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">householder</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Qtb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    
        <span class="n">y</span> <span class="o">=</span> <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">]</span>
        <span class="n">e1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
        <span class="n">e1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">e1</span> <span class="o">+</span> <span class="n">y</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">/</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        
        <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">:</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">:</span><span class="n">m</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span> <span class="o">@</span> <span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">:</span><span class="n">m</span><span class="p">]</span>
        <span class="n">Qtb</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qtb</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span> <span class="o">@</span> <span class="n">Qtb</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">R</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">],</span> <span class="n">Qtb</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We return to our overdetermined system example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="n">R</span><span class="p">,</span> <span class="n">Qtb</span> <span class="o">=</span> <span class="n">householder</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">backsubs</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Qtb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.66666667 0.66666667]
</pre></div>
</div>
</div>
</div>
<p>One advantage of the Householder approach is that it produces a matrix <span class="math notranslate nohighlight">\(Q\)</span> with very good orthogonality, i.e., <span class="math notranslate nohighlight">\(Q^T Q \approx I\)</span>. We give a quick example below comparing Gram-Schmidt and Householder. (The choice of matrix <span class="math notranslate nohighlight">\(A\)</span> will become clearer when we discuss the singular value decomposition in a later chapter.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">U</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)))</span>
<span class="n">V</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)))</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">S</span> <span class="o">@</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span>

<span class="n">Qgs</span><span class="p">,</span> <span class="n">Rgs</span> <span class="o">=</span> <span class="n">gramschmidt</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">Qgs</span> <span class="o">@</span> <span class="n">Rgs</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Qgs</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Qgs</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1.4369568046009742e-16
19.745599060592102
</pre></div>
</div>
</div>
</div>
<p>As you can see above, the <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(R\)</span> factors produced by the Gram-Schmidt algorithm do have the property that <span class="math notranslate nohighlight">\(QR \approx A\)</span>. However, <span class="math notranslate nohighlight">\(Q\)</span> is far from orthogonal. (Recall that <code class="docutils literal notranslate"><span class="pre">LA.norm</span></code> computes the Frobenius norm introduced previously.)</p>
<p>On the other hand, Householder reflections perform much better in that respect as we show next. Here we use the implementation of Householder transformations in <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.qr.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.qr</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">Qhh</span><span class="p">,</span> <span class="n">Rhh</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">Qhh</span> <span class="o">@</span> <span class="n">Rhh</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Qhh</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Qhh</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>4.739138228891714e-16
5.33506987519293e-15
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following matrices is upper triangular?</p>
<p>a) <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 2 \\ 0 &amp; 3 \end{pmatrix}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 0 \\ 2 &amp; 3 \end{pmatrix}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span></p>
<p><strong>2</strong> What is the output of the Gram-Schmidt algorithm when applied to a set of linearly independent vectors?</p>
<p>a) An orthogonal basis for the subspace spanned by the vectors.</p>
<p>b) An orthonormal basis for the subspace spanned by the vectors.</p>
<p>c) A set of linearly dependent vectors.</p>
<p>d) A single vector that is orthogonal to all the input vectors.</p>
<p><strong>3</strong> What is the dimension of a hyperplane in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(m\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(m - 1\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(m - 2\)</span></p>
<p>d) 1</p>
<p><strong>4</strong> Which of the following is true about a Householder reflection?</p>
<p>a) It is a reflection across a hyperplane orthogonal to a unit vector.</p>
<p>b) It is a reflection across a unit vector.</p>
<p>c) It is a rotation around a hyperplane orthogonal to a unit vector.</p>
<p>d) It is a rotation around a unit vector.</p>
<p><strong>5</strong> How can a sequence of Householder reflections be used to compute a QR decomposition of a matrix <span class="math notranslate nohighlight">\(A\)</span>?</p>
<p>a) By iteratively introducing zeros above the diagonal of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>b) By iteratively introducing zeros below the diagonal of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>c) By iteratively introducing zeros on the diagonal of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>d) By iteratively introducing zeros in the entire matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>Answer for 1: a. Justification: The text defines an upper triangular matrix as one with all entries below the diagonal equal to zero.</p>
<p>Answer for 2: b. Justification: The text states that the Gram-Schmidt algorithm “produces an orthonormal basis of <span class="math notranslate nohighlight">\(\mathrm{span}(a_1, \dots, a_m)\)</span>.”</p>
<p>Answer for 3: b. Justification: The text defines a hyperplane as a linear subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> of dimension <span class="math notranslate nohighlight">\(m - 1\)</span>.</p>
<p>Answer for 4: a. Justification: The text defines a Householder reflection as follows: “Let <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^m\)</span> be a unit vector and let <span class="math notranslate nohighlight">\(W\)</span> be the hyperplane orthogonal to it. The reflection across <span class="math notranslate nohighlight">\(W\)</span> is given by <span class="math notranslate nohighlight">\(H = I_{m \times m} - 2\mathbf{z}\mathbf{z}^T\)</span>.”</p>
<p>Answer for 5: b. Justification: The text states, “One way to construct a (full) QR decomposition of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> is to find a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(H_1,\ldots,H_m\)</span> that triangularize <span class="math notranslate nohighlight">\(A\)</span>: <span class="math notranslate nohighlight">\(H_m \cdots H_2H_1A = R\)</span> for an upper-triangular matrix <span class="math notranslate nohighlight">\(R\)</span>.”</p>
    
</body>
</html>