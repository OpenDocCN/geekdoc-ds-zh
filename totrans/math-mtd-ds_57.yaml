- en: '7.5\. Application: random walks on graphs and PageRank#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap07_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html](https://mmids-textbook.github.io/chap07_rwmc/05_pagerank/roch-mmids-rwmc-pagerank.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we mentioned earlier in this chapter, a powerful way to extract information
    about the structure of a network is to analyze the behavior of a walker randomly
    “diffusing” on it.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1\. Random walk on a graph[#](#random-walk-on-a-graph "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first specialize the theory of the previous section to random walks on graphs.
    We start with the case of digraphs. The undirected case leads to useful simplifications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Directed case** We first define a random walk on a digraph.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Random Walk on a Digraph)** \(\idx{random walk on a graph}\xdi\)
    Let \(G = (V,E)\) be a directed graph. If a vertex does not have an outgoing edge
    (i.e., an edge with it as its source), add a self-loop to it. A random walk on
    \(G\) is a time-homogeneous Markov chain \((X_t)_{t \geq 0}\) with state space
    \(\mathcal{S} = V\) and transition probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta^+(i)}, \qquad \forall
    i \in V, j \in N^+(i), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\delta^+(i)\) is the out-degree of \(i\), i.e., the number of outgoing
    edges and \(N^+(i) = \{j \in V:(i,j) \in E\}\). \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: In words, at each step, we choose an outgoing edge from the current state uniformly
    at random. Choosing a self-loop (i.e., an edge of the form \((i,i)\)) means staying
    where we are.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(G = (V,E)\) be a digraph with \(n = |V|\) vertices. Without loss of generality,
    we let the vertex set be \([n] = \{1,\ldots,n\}\). The adjacency matrix of \(G\)
    is denoted as \(A = (a_{i,j})_{i,j}\). We define the out-degree matrix \(D\) as
    the diagonal matrix with diagonal entries \(\delta^+(i)\), \(i=1,\ldots,n\). That
    is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ D = \mathrm{diag}(A \mathbf{1}). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Transition Matrix in Terms of Adjacency)** \(\idx{transition matrix
    in terms of adjacency}\xdi\) The transition matrix of random walk on \(G\) satisfying
    the conditions of the definition above is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = D^{-1} A. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The formula follows immediately from the definition. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: We specialize irreducibility to the case of random walk on a digraph.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Irreducibility)** \(\idx{irreducibility lemma}\xdi\) Let \(G =
    (V,E)\) be a digraph. Random walk on \(G\) is irreducible if and only if \(G\)
    is strongly connected. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Simply note that the transition graph of the walk is \(G\) itself.
    We have seen previously that irreducibility is equivalent to the transition graph
    being strongly connected. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: In the undirected case, more structure emerges as we detail next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Undirected case** Specializing the previous definitions and observations
    to undirected graphs, we get the following.'
  prefs: []
  type: TYPE_NORMAL
- en: It will be convenient to allow self-loops, i.e., entry \(a_{i,i}\) of the adjacency
    matrix can be \(1\) for some \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Random Walk on a Graph)** \(\idx{random walk on a graph}\xdi\)
    Let \(G = (V,E)\) be a graph. If a vertex is isolated, add a self-loop to it.
    A random walk on \(G\) is a time-homogeneous Markov chain \((X_t)_{t \geq 0}\)
    with state space \(\mathcal{S} = V\) and transition probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta(i)}, \qquad \forall
    i \in V, j \in N(i) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(\delta(i)\) is the degree of \(i\) and \(N(i) = \{j \in V: \{i,j\}
    \in E\}\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen previously, the transition matrix of random walk on \(G\) satisfying
    the conditions of the definition above is \(P = D^{-1} A\), where \(D = \mathrm{diag}(A
    \mathbf{1})\) is the degree matrix.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we have previously derived the transition matrix for random walk
    on the Petersen graph.
  prefs: []
  type: TYPE_NORMAL
- en: We specialize irreducibility to the case of random walk on a graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Irreducibility)** \(\idx{irreducibility lemma}\xdi\) Let \(G =
    (V,E)\) be a graph. Random walk on \(G\) is irreducible if and only if \(G\) is
    connected. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We only prove one direction. Suppose \(G\) is connected. Then between
    any two vertices \(i\) and \(j\) there is a sequence of vertices \(z_0 = i, z_1,
    \ldots, z_r = j\) such that \(\{z_{\ell-1},z_\ell\} \in E\) for all \(\ell = 1,\ldots,r\).
    In particular, \(a_{z_{\ell-1},z_\ell} > 0\) which implies \(p_{z_{\ell-1},z_\ell}
    > 0\). That proves irreducibility. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: By the previous lemma and the *Existence of a Stationary Distribution*, provided
    \(G\) is connected, it has a unique stationary distribution. It turns out to be
    straightforward to compute it as we see in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reversible chains** A Markov chain is said to be reversible if it satisfies
    the detailed balance conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Reversibility)** \(\idx{reversible}\xdi\) A transition matrix
    \(P = (p_{i,j})_{i,j=1}^n\) is reversible with respect to a probability distribution
    \(\bpi = (\pi_i)_{i=1}^n\) if it satisfies the so-called detailed balance conditions'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i p_{i,j} = \pi_j p_{j,i}, \qquad \forall i,j. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: The next theorem explains why this definition is useful to us.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Reversibility and Stationarity)** \(\idx{reversibility and stationarity
    theorem}\xdi\) Let \(P = (p_{i,j})_{i,j=1}^n\) be a transition matrix reversible
    with respect to a probability distribution \(\bpi = (\pi_i)_{i=1}^n\). Then \(\bpi\)
    is a stationary distribution of \(P\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Just check the definition.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For any \(j\), by the definition of reversibility,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i} \pi_i p_{i,j} = \sum_{i} \pi_j p_{j,i} = \pi_j \sum_{i} p_{j,i}
    = \pi_j, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(P\) is stochastic in the last equality. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We return to random walk on a graph. We show that it is reversible and derive
    the stationary dsitribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Stationary Distribution on a Graph)** \(\idx{stationary distribution
    on a graph}\xdi\) Let \(G = (V,E)\) be a graph. Assume further that \(G\) is connected.
    Then the unique stationary distribution of random walk on \(G\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in
    V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We prove this in two parts. We first argue that \(\bpi = (\pi_i)_{i
    \in V}\) is indeed a probability distribution. Then we show that the transition
    matrix \(P\) is reversible with respect to \(\bpi\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We first show that \(\bpi = (\pi_v)_{v \in V}\) is a probability distribution.
    Its entries are non-negative by definition. Further'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i \in V} \pi_i = \sum_{i \in V}\frac{\delta(i)}{\sum_{i \in V} \delta(i)}
    = \frac{\sum_{i \in V} \delta(i)}{\sum_{i \in V} \delta(i)} = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: It remains to establish reversibility. For any \(i, j\), by definition,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_i p_{i,j} &= \frac{\delta(i)}{\sum_{i \in V} \delta(i)}
    \frac{a_{i,j}}{\sum_{k} a_{i,k}}\\ &= \frac{\delta(i)}{\sum_{i \in V} \delta(i)}
    \frac{a_{i,j}}{\delta(i)}\\ &= \frac{1}{\sum_{i \in V} \delta(i)} a_{i,j}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Changing the roles of \(i\) and \(j\) gives the same expression since \(a_{j,i}
    = a_{i,j}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2\. PageRank[#](#pagerank "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One is often interested in identifying central nodes in a network. Intuitively,
    they should correspond to entities (e.g., individuals or webpages depending on
    the network) that are particularly influential or authoritative. There are many
    ways of uncovering such nodes. Formally one defines a notion of [node centrality](https://en.wikipedia.org/wiki/Centrality)\(\idx{node
    centrality}\xdi\), which ranks nodes by importance. Here we focus on one important
    such notion, PageRank. We will see that it is closely related to random walks
    on graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '**A notion of centrality for directed graphs** We start with the directed case.'
  prefs: []
  type: TYPE_NORMAL
- en: Let \(G = (V,E)\) be a digraph on \(n\) vertices. We seek to associate a measure
    of importance to each vertex. We will denote this (row) vector by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{PR} = (\mathrm{PR}_1, \ldots, \mathrm{PR}_n)^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathrm{PR}\) stands for PageRank\(\idx{PageRank}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: We posit that each vertex has a certain amount of influence associated to it
    and that it distributes that influence equally among the neighbors it points to.
    We seek a (row) vector \(\mathbf{z} = (z_1,\ldots,z_n)^T\) that satisfies an equation
    of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ z_i = \sum_{j \in N^-(i)} z_j \frac{1}{\delta^+(j)}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\delta^+(j) = |N^+(j)|\) is the out-degree of \(j\) and \(N^-(i)\) is
    the set of vertices \(j\) with an edge \((j,i)\). Observe that we explicitly take
    into account the direction of the edges. We think of an edge \((j,i)\) as an indication
    that \(j\) values \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the web for instance, a link towards a page indicates that the destination
    page has information of value. Quoting [Wikipedia](https://en.wikipedia.org/wiki/PageRank):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PageRank works by counting the number and quality of links to a page to determine
    a rough estimate of how important the website is. The underlying assumption is
    that more important websites are likely to receive more links from other websites.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On X (formerly known as Twitter), following an account is an indication that
    the latter is of interest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already encountered this set of equations. Consider random walk on the
    directed graph \(G\) (where a self-loop is added to each vertex without an outgoing
    edge). That is, at every step, we pick an outgoing edge of the current state uniformly
    at random. Then the transition matrix is
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = D^{-1} A, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(D\) is the diagonal matrix with the out-degrees on its diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: A stationary distribution \(\bpi = (\pi_1,\ldots,\pi_n)^T\) is a row vector
    satisfying in this case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \sum_{j=1}^n \pi_j p_{j,i} = \sum_{j \in N^-(i)} \pi_j \frac{1}{\delta^+(j)}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(\mathbf{z}\) is stationary distribution of random walk on \(G\).
  prefs: []
  type: TYPE_NORMAL
- en: If the graph \(G\) is strongly connected, we know that there is a unique stationary
    distribution by the *Existence of a Stationary Distribution*. In many real-world
    digraphs, however, that assumption is not satisfied. To ensure that a meaningful
    solution can still be found, we modify the walk slightly.
  prefs: []
  type: TYPE_NORMAL
- en: To make the walk irreducible, we add a small probability at each step of landing
    at a uniformly chosen node. This is sometimes referred to as *teleporting*. That
    is, we define the transition matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\alpha \in (0,1)\) known as the damping factor (or teleporting parameter).
    A typical choice is \(\alpha = 0.85\).
  prefs: []
  type: TYPE_NORMAL
- en: Note that \(\frac{1}{n} \mathbf{1} \mathbf{1}^T\) is a stochastic matrix. Indeed,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \mathbf{1} \mathbf{1}^T \mathbf{1} = \frac{1}{n} \mathbf{1} n
    = \mathbf{1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(Q\) is a stochastic matrix (as a convex combination of stochastic matrices).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(Q\) is clearly irreducible since it is strictly positive. That is,
    for any \(x, y \in [n]\), one can reach \(y\) from \(x\) in a single step
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q_{x,y} = \alpha P_{x,y} + (1-\alpha) \frac{1}{n} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: This also holds for \(x = y\) so the chain is lazy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we define \(\mathbf{PR}\) as the unique stationary distribution of
    \(Q = (q_{i,j})_{i,j=1}^n\), that is, the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{PR}_i = \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i}, \]
  prefs: []
  type: TYPE_NORMAL
- en: with \(\mathbf{PR} \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \mathrm{PR}_i = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/PageRank):'
  prefs: []
  type: TYPE_NORMAL
- en: The formula uses a model of a random surfer who reaches their target site after
    several clicks, then switches to a random page. The PageRank value of a page reflects
    the chance that the random surfer will land on that page by clicking on a link.
    It can be understood as a Markov chain in which the states are pages, and the
    transitions are the links between pages – all of which are all equally probable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here is an implementation of the PageRank algorithm. We will need a function
    that takes as input an adjacency matrix \(A\) and returns the corresponding transition
    matrix \(P\). Some vertices have no outgoing links. To avoid dividing by \(0\),
    we add a self-loop to *all vertices with out-degree \(0\)*. We [`numpy.fill_diagonal`](https://numpy.org/doc/stable/reference/generated/numpy.fill_diagonal.html)
    for this purpose. (The boolean values in `sinks` below automatically convert to
    `0.0` and `1.0` when used in the numerical matrix context. So this effectively
    adds self-loops only to sink nodes – nodes that had no outgoing edges now have
    a single outgoing edge pointing back to themselves.)
  prefs: []
  type: TYPE_NORMAL
- en: Also, because the adjacency matrix and the vector of out-degrees have different
    shapes, we turn `out_deg` into a column vector using [`numpy.newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis)
    to ensure that the division is [done one column at a time](https://numpy.org/doc/stable/user/basics.broadcasting.html#broadcastable-arrays).
    (There are many ways of doing this, [but some are slower than others](https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array).)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The following function adds the damping factor. Here `mu` will be the uniform
    distribution. It gets added (after scaling by `1-alpha`) one row at a time to
    `P` (again after scaling by `alpha`). This time we do not need to reshape `mu`
    as the sum is done one row at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When computing PageRank, we multiply from the left.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Let’s try a star with edges pointing out. Along the way,
    we check that our functions work how we expect them to.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/660932284bae2affdddf5662aeec676a1cf3903af81ef0cecb91e479b6585e26.png](../Images/ff153b67326802af34ceb69f23f4985b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We compute the matrices \(P\) and \(Q\). We use [`numpy.set_printoptions`](https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html)
    to condense the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: While it is tempting to guess that \(1\) is the most central node of the network,
    no edge actually points to it. In this case, the center of the star has a low
    PageRank value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We then try a star with edges pointing in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/a2444432691b17ff6593a677e80f8c88415d65748ccba7a1b3b861b173f655f2.png](../Images/d644ec50527bfb74965fed98a96f2683.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the center of the star does indeed have a high PageRank value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**A notion of centrality for undirected graphs** We can apply PageRank\(\idx{PageRank}\xdi\)
    in the undirected case as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider random walk on the undirected graph \(G\). That is, at every step,
    we pick a neighbor of the current state uniformly at random. If needed, add a
    self-loop to any isolated vertex. Then the transition matrix is
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = D^{-1} A, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(D\) is the degree matrix and \(A\) is the adjacency matrix. A stationary
    distribution \(\bpi = (\pi_1,\ldots,\pi_n)^T\) is a row vector satisfying in this
    case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \sum_{j=1}^n \pi_j p_{j,i} = \sum_{j \in N(i)} \pi_j \frac{1}{\delta(j)}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We already know the solution to this system of equations. In the connected case
    without damping, the unique stationary distribution of random walk on \(G\) is
    given by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in
    V. \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, the centrality of a node is directly proportional to its degree, i.e.,
    how many neighbors it has. Up to the scaling factor, this is known as [degree
    centrality](https://en.wikipedia.org/wiki/Centrality#Degree_centrality)\(\idx{degree
    centrality}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: For a general undirected graph that may not be connected, we can use a damping
    factor to enforce irreducibility. We add a small probability at each step of landing
    at a uniformly chosen node. That is, we define the transition matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\alpha \in (0,1)\) known as the damping factor. We define the PageRank
    vector \(\mathbf{PR}\) as the unique stationary distribution of \(Q = (q_{i,j})_{i,j=1}^n\),
    that is, the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{PR}_i = \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i}, \]
  prefs: []
  type: TYPE_NORMAL
- en: with \(\mathbf{PR} \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \mathrm{PR}_i = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We revisit the star example in the undirected case.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/c7f2056453edc830e099413bfdb90e0047d36da8030c2b8abf8be4198801e5fc.png](../Images/2d0d2764fb0069741155c9c91833c00b.png)'
  prefs: []
  type: TYPE_IMG
- en: We first compute the PageRank vector without damping. Here the random walk is
    periodic (Why?) so power iteration may fail (Try it!). Instead, we use a small
    amount of damping and increase the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The PageRank value for the center node is indeed roughly \(7\) times larger
    than the other ones, as can be expected from the ratio of their degrees.
  prefs: []
  type: TYPE_NORMAL
- en: We try again with more damping. This time the ratio of PageRank values is not
    quite the same as the ratio of degrees, but the center node continues to have
    a higher value than the other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** There are many other centrality measures besides PageRank,
    such as betweenness centrality, closeness centrality, and eigenvector centrality.
    Ask your favorite AI chatbot to explain these measures and discuss their similarities
    and differences with PageRank. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.3\. Personalized PageRank[#](#personalized-pagerank "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We return to [MathWorld](https://mathworld.wolfram.com) dataset. Recall that
    each page of MathWorld concerns a particular mathematical concept. In a section
    entitled “SEE ALSO”, other related mathematical concepts are listed with a link
    to their MathWorld page. Our goal is to identify “central” vertices in the resulting
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Platonic solids (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Platonic solids](../Images/4393ac7f9684a7e62d5f82de15668d37.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We load the dataset again.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|  | from | to |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 47 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 404 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 2721 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: The second file contains the titles of the pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '|  | title |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Alexander''s Horned Sphere |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Exotic Sphere |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Antoine''s Horned Sphere |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Flat |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Poincaré Manifold |'
  prefs: []
  type: TYPE_TB
- en: We construct the graph by adding the edges one by one. We first convert `df_edges`
    into a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: To apply PageRank, we construct the adjacency matric of the graph. We also define
    a vector of title pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We use [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)
    to identify the pages with highest scores. We apply it to `-pr_mw` to sort from
    the highest to lowest value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The top 25 topics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We indeed get a list of central concepts in mathematics – including several
    we have encountered previously such as `Normal Distribution`, `Tree`, `Vector`
    or `Derivative`.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: There is a variant of PageRank, referred to as Personalized PageRank (PPR)\(\idx{Personalized
    PageRank}\xdi\), which aims to tailor the outcome to specific interests. This
    is accomplished from a simple change to the algorithm. When teleporting, rather
    than jumping to a uniformly random page, we instead jump to an arbitrary distribution
    which is meant to capture some specific interests. In the context of the web for
    instance, this distribution might be uniform over someone’s bookmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We adapt `pagerank` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** To test PPR, consider the distribution concentrated on
    a single topic `Normal Distribution`. This is topic number `1270`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We now run PPR and list the top 25 pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The top 25 topics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This indeed returns various statistical concepts, particularly related to the
    normal dsitribution.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The PageRank algorithm has been adapted for various applications
    beyond web search, such as ranking scientific papers, analyzing social networks,
    and even ranking sports teams [[Gle](https://arxiv.org/abs/1407.5107)]. Ask your
    favorite AI chatbot to discuss some of these applications and how the PageRank
    algorithm is modified for each case. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Consider a random walk on the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: What is the transition matrix for this random walk?
  prefs: []
  type: TYPE_NORMAL
- en: a)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 &
    1/3 & 1/3 \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: b)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 0 & 1/2 & 1/2 \\ 1/2 & 0 & 1/2 \\ 1/2 & 1/2
    & 0 \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: c)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: d)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In a random walk on a directed graph, the transition probability from
    vertex \(i\) to vertex \(j\) is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(p_{i,j} = \frac{1}{\delta^-(j)}\) for all \(j \in N^-(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(p_{i,j} = \frac{1}{\delta^+(j)}\) for all \(j \in N^+(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(p_{i,j} = \frac{1}{\delta^-(i)}\) for all \(j \in N^-(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(p_{i,j} = \frac{1}{\delta^+(i)}\) for all \(j \in N^+(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** The transition matrix \(P\) of a random walk on a directed graph can
    be expressed in terms of the adjacency matrix \(A\) as:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(P = AD^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(P = A^TD^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(P = D^{-1}A\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(P = D^{-1}A^T\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In a random walk on an undirected graph, the stationary distribution
    \(\boldsymbol{\pi}\) satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\pi_i = \frac{\delta^+(i)}{\sum_{j \in V} \delta^+(j)}\) for all \(i \in
    V\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\pi_i = \frac{1}{\delta(i)}\) for all \(i \in V\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\pi_i = \frac{1}{|V|}\) for all \(i \in V\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\pi_i = \frac{\delta(i)}{\sum_{j \in V} \delta(j)}\) for all \(i \in V\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Personalized PageRank differs from standard PageRank in that:'
  prefs: []
  type: TYPE_NORMAL
- en: a) It considers the user’s browsing history
  prefs: []
  type: TYPE_NORMAL
- en: b) It jumps to a non-uniform distribution when teleporting
  prefs: []
  type: TYPE_NORMAL
- en: c) It uses a different damping factor
  prefs: []
  type: TYPE_NORMAL
- en: d) It only considers a subset of the graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: Each node has a degree of 2, and the probability
    of transitioning to each neighbor is 1/2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: d. Justification: The text states, “In words, at each step, we
    choose an outgoing edge from the current state uniformly at random,” which corresponds
    to the transition probability \(p_{i,j} = \frac{1}{\delta^+(i)}\) for all \(j
    \in N^+(i)\), where \(\delta^+(i)\) is the out-degree of vertex \(i\) and \(N^+(i)\)
    is the set of vertices with an edge from \(i\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states, “The transition matrix of
    random walk on \(G\) satisfying the conditions of the definition above is \(P
    = D^{-1}A\), where \(D\) is the out-degree matrix.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text states, “In the connected case without
    damping, the unique stationary distribution of random walk on \(G\) is given by
    \(\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \forall i \in V\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states, “When teleporting, rather
    than jumping to a uniformly random page, we instead jump to an arbitrary distribution
    \(\boldsymbol{\mu}\) which is meant to capture some specific interests.”'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1\. Random walk on a graph[#](#random-walk-on-a-graph "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first specialize the theory of the previous section to random walks on graphs.
    We start with the case of digraphs. The undirected case leads to useful simplifications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Directed case** We first define a random walk on a digraph.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Random Walk on a Digraph)** \(\idx{random walk on a graph}\xdi\)
    Let \(G = (V,E)\) be a directed graph. If a vertex does not have an outgoing edge
    (i.e., an edge with it as its source), add a self-loop to it. A random walk on
    \(G\) is a time-homogeneous Markov chain \((X_t)_{t \geq 0}\) with state space
    \(\mathcal{S} = V\) and transition probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta^+(i)}, \qquad \forall
    i \in V, j \in N^+(i), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\delta^+(i)\) is the out-degree of \(i\), i.e., the number of outgoing
    edges and \(N^+(i) = \{j \in V:(i,j) \in E\}\). \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: In words, at each step, we choose an outgoing edge from the current state uniformly
    at random. Choosing a self-loop (i.e., an edge of the form \((i,i)\)) means staying
    where we are.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(G = (V,E)\) be a digraph with \(n = |V|\) vertices. Without loss of generality,
    we let the vertex set be \([n] = \{1,\ldots,n\}\). The adjacency matrix of \(G\)
    is denoted as \(A = (a_{i,j})_{i,j}\). We define the out-degree matrix \(D\) as
    the diagonal matrix with diagonal entries \(\delta^+(i)\), \(i=1,\ldots,n\). That
    is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ D = \mathrm{diag}(A \mathbf{1}). \]
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Transition Matrix in Terms of Adjacency)** \(\idx{transition matrix
    in terms of adjacency}\xdi\) The transition matrix of random walk on \(G\) satisfying
    the conditions of the definition above is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = D^{-1} A. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The formula follows immediately from the definition. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: We specialize irreducibility to the case of random walk on a digraph.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Irreducibility)** \(\idx{irreducibility lemma}\xdi\) Let \(G =
    (V,E)\) be a digraph. Random walk on \(G\) is irreducible if and only if \(G\)
    is strongly connected. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Simply note that the transition graph of the walk is \(G\) itself.
    We have seen previously that irreducibility is equivalent to the transition graph
    being strongly connected. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: In the undirected case, more structure emerges as we detail next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Undirected case** Specializing the previous definitions and observations
    to undirected graphs, we get the following.'
  prefs: []
  type: TYPE_NORMAL
- en: It will be convenient to allow self-loops, i.e., entry \(a_{i,i}\) of the adjacency
    matrix can be \(1\) for some \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Random Walk on a Graph)** \(\idx{random walk on a graph}\xdi\)
    Let \(G = (V,E)\) be a graph. If a vertex is isolated, add a self-loop to it.
    A random walk on \(G\) is a time-homogeneous Markov chain \((X_t)_{t \geq 0}\)
    with state space \(\mathcal{S} = V\) and transition probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{1}{\delta(i)}, \qquad \forall
    i \in V, j \in N(i) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(\delta(i)\) is the degree of \(i\) and \(N(i) = \{j \in V: \{i,j\}
    \in E\}\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen previously, the transition matrix of random walk on \(G\) satisfying
    the conditions of the definition above is \(P = D^{-1} A\), where \(D = \mathrm{diag}(A
    \mathbf{1})\) is the degree matrix.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we have previously derived the transition matrix for random walk
    on the Petersen graph.
  prefs: []
  type: TYPE_NORMAL
- en: We specialize irreducibility to the case of random walk on a graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Irreducibility)** \(\idx{irreducibility lemma}\xdi\) Let \(G =
    (V,E)\) be a graph. Random walk on \(G\) is irreducible if and only if \(G\) is
    connected. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We only prove one direction. Suppose \(G\) is connected. Then between
    any two vertices \(i\) and \(j\) there is a sequence of vertices \(z_0 = i, z_1,
    \ldots, z_r = j\) such that \(\{z_{\ell-1},z_\ell\} \in E\) for all \(\ell = 1,\ldots,r\).
    In particular, \(a_{z_{\ell-1},z_\ell} > 0\) which implies \(p_{z_{\ell-1},z_\ell}
    > 0\). That proves irreducibility. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: By the previous lemma and the *Existence of a Stationary Distribution*, provided
    \(G\) is connected, it has a unique stationary distribution. It turns out to be
    straightforward to compute it as we see in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reversible chains** A Markov chain is said to be reversible if it satisfies
    the detailed balance conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Reversibility)** \(\idx{reversible}\xdi\) A transition matrix
    \(P = (p_{i,j})_{i,j=1}^n\) is reversible with respect to a probability distribution
    \(\bpi = (\pi_i)_{i=1}^n\) if it satisfies the so-called detailed balance conditions'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i p_{i,j} = \pi_j p_{j,i}, \qquad \forall i,j. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: The next theorem explains why this definition is useful to us.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Reversibility and Stationarity)** \(\idx{reversibility and stationarity
    theorem}\xdi\) Let \(P = (p_{i,j})_{i,j=1}^n\) be a transition matrix reversible
    with respect to a probability distribution \(\bpi = (\pi_i)_{i=1}^n\). Then \(\bpi\)
    is a stationary distribution of \(P\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Just check the definition.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For any \(j\), by the definition of reversibility,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i} \pi_i p_{i,j} = \sum_{i} \pi_j p_{j,i} = \pi_j \sum_{i} p_{j,i}
    = \pi_j, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(P\) is stochastic in the last equality. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We return to random walk on a graph. We show that it is reversible and derive
    the stationary dsitribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Stationary Distribution on a Graph)** \(\idx{stationary distribution
    on a graph}\xdi\) Let \(G = (V,E)\) be a graph. Assume further that \(G\) is connected.
    Then the unique stationary distribution of random walk on \(G\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in
    V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We prove this in two parts. We first argue that \(\bpi = (\pi_i)_{i
    \in V}\) is indeed a probability distribution. Then we show that the transition
    matrix \(P\) is reversible with respect to \(\bpi\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We first show that \(\bpi = (\pi_v)_{v \in V}\) is a probability distribution.
    Its entries are non-negative by definition. Further'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i \in V} \pi_i = \sum_{i \in V}\frac{\delta(i)}{\sum_{i \in V} \delta(i)}
    = \frac{\sum_{i \in V} \delta(i)}{\sum_{i \in V} \delta(i)} = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: It remains to establish reversibility. For any \(i, j\), by definition,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_i p_{i,j} &= \frac{\delta(i)}{\sum_{i \in V} \delta(i)}
    \frac{a_{i,j}}{\sum_{k} a_{i,k}}\\ &= \frac{\delta(i)}{\sum_{i \in V} \delta(i)}
    \frac{a_{i,j}}{\delta(i)}\\ &= \frac{1}{\sum_{i \in V} \delta(i)} a_{i,j}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Changing the roles of \(i\) and \(j\) gives the same expression since \(a_{j,i}
    = a_{i,j}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2\. PageRank[#](#pagerank "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One is often interested in identifying central nodes in a network. Intuitively,
    they should correspond to entities (e.g., individuals or webpages depending on
    the network) that are particularly influential or authoritative. There are many
    ways of uncovering such nodes. Formally one defines a notion of [node centrality](https://en.wikipedia.org/wiki/Centrality)\(\idx{node
    centrality}\xdi\), which ranks nodes by importance. Here we focus on one important
    such notion, PageRank. We will see that it is closely related to random walks
    on graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '**A notion of centrality for directed graphs** We start with the directed case.'
  prefs: []
  type: TYPE_NORMAL
- en: Let \(G = (V,E)\) be a digraph on \(n\) vertices. We seek to associate a measure
    of importance to each vertex. We will denote this (row) vector by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{PR} = (\mathrm{PR}_1, \ldots, \mathrm{PR}_n)^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathrm{PR}\) stands for PageRank\(\idx{PageRank}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: We posit that each vertex has a certain amount of influence associated to it
    and that it distributes that influence equally among the neighbors it points to.
    We seek a (row) vector \(\mathbf{z} = (z_1,\ldots,z_n)^T\) that satisfies an equation
    of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ z_i = \sum_{j \in N^-(i)} z_j \frac{1}{\delta^+(j)}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\delta^+(j) = |N^+(j)|\) is the out-degree of \(j\) and \(N^-(i)\) is
    the set of vertices \(j\) with an edge \((j,i)\). Observe that we explicitly take
    into account the direction of the edges. We think of an edge \((j,i)\) as an indication
    that \(j\) values \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the web for instance, a link towards a page indicates that the destination
    page has information of value. Quoting [Wikipedia](https://en.wikipedia.org/wiki/PageRank):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PageRank works by counting the number and quality of links to a page to determine
    a rough estimate of how important the website is. The underlying assumption is
    that more important websites are likely to receive more links from other websites.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On X (formerly known as Twitter), following an account is an indication that
    the latter is of interest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already encountered this set of equations. Consider random walk on the
    directed graph \(G\) (where a self-loop is added to each vertex without an outgoing
    edge). That is, at every step, we pick an outgoing edge of the current state uniformly
    at random. Then the transition matrix is
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = D^{-1} A, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(D\) is the diagonal matrix with the out-degrees on its diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: A stationary distribution \(\bpi = (\pi_1,\ldots,\pi_n)^T\) is a row vector
    satisfying in this case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \sum_{j=1}^n \pi_j p_{j,i} = \sum_{j \in N^-(i)} \pi_j \frac{1}{\delta^+(j)}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(\mathbf{z}\) is stationary distribution of random walk on \(G\).
  prefs: []
  type: TYPE_NORMAL
- en: If the graph \(G\) is strongly connected, we know that there is a unique stationary
    distribution by the *Existence of a Stationary Distribution*. In many real-world
    digraphs, however, that assumption is not satisfied. To ensure that a meaningful
    solution can still be found, we modify the walk slightly.
  prefs: []
  type: TYPE_NORMAL
- en: To make the walk irreducible, we add a small probability at each step of landing
    at a uniformly chosen node. This is sometimes referred to as *teleporting*. That
    is, we define the transition matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\alpha \in (0,1)\) known as the damping factor (or teleporting parameter).
    A typical choice is \(\alpha = 0.85\).
  prefs: []
  type: TYPE_NORMAL
- en: Note that \(\frac{1}{n} \mathbf{1} \mathbf{1}^T\) is a stochastic matrix. Indeed,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \mathbf{1} \mathbf{1}^T \mathbf{1} = \frac{1}{n} \mathbf{1} n
    = \mathbf{1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(Q\) is a stochastic matrix (as a convex combination of stochastic matrices).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(Q\) is clearly irreducible since it is strictly positive. That is,
    for any \(x, y \in [n]\), one can reach \(y\) from \(x\) in a single step
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q_{x,y} = \alpha P_{x,y} + (1-\alpha) \frac{1}{n} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: This also holds for \(x = y\) so the chain is lazy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we define \(\mathbf{PR}\) as the unique stationary distribution of
    \(Q = (q_{i,j})_{i,j=1}^n\), that is, the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{PR}_i = \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i}, \]
  prefs: []
  type: TYPE_NORMAL
- en: with \(\mathbf{PR} \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \mathrm{PR}_i = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/PageRank):'
  prefs: []
  type: TYPE_NORMAL
- en: The formula uses a model of a random surfer who reaches their target site after
    several clicks, then switches to a random page. The PageRank value of a page reflects
    the chance that the random surfer will land on that page by clicking on a link.
    It can be understood as a Markov chain in which the states are pages, and the
    transitions are the links between pages – all of which are all equally probable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here is an implementation of the PageRank algorithm. We will need a function
    that takes as input an adjacency matrix \(A\) and returns the corresponding transition
    matrix \(P\). Some vertices have no outgoing links. To avoid dividing by \(0\),
    we add a self-loop to *all vertices with out-degree \(0\)*. We [`numpy.fill_diagonal`](https://numpy.org/doc/stable/reference/generated/numpy.fill_diagonal.html)
    for this purpose. (The boolean values in `sinks` below automatically convert to
    `0.0` and `1.0` when used in the numerical matrix context. So this effectively
    adds self-loops only to sink nodes – nodes that had no outgoing edges now have
    a single outgoing edge pointing back to themselves.)
  prefs: []
  type: TYPE_NORMAL
- en: Also, because the adjacency matrix and the vector of out-degrees have different
    shapes, we turn `out_deg` into a column vector using [`numpy.newaxis`](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis)
    to ensure that the division is [done one column at a time](https://numpy.org/doc/stable/user/basics.broadcasting.html#broadcastable-arrays).
    (There are many ways of doing this, [but some are slower than others](https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array).)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The following function adds the damping factor. Here `mu` will be the uniform
    distribution. It gets added (after scaling by `1-alpha`) one row at a time to
    `P` (again after scaling by `alpha`). This time we do not need to reshape `mu`
    as the sum is done one row at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: When computing PageRank, we multiply from the left.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Let’s try a star with edges pointing out. Along the way,
    we check that our functions work how we expect them to.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/660932284bae2affdddf5662aeec676a1cf3903af81ef0cecb91e479b6585e26.png](../Images/ff153b67326802af34ceb69f23f4985b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We compute the matrices \(P\) and \(Q\). We use [`numpy.set_printoptions`](https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html)
    to condense the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: While it is tempting to guess that \(1\) is the most central node of the network,
    no edge actually points to it. In this case, the center of the star has a low
    PageRank value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We then try a star with edges pointing in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/a2444432691b17ff6593a677e80f8c88415d65748ccba7a1b3b861b173f655f2.png](../Images/d644ec50527bfb74965fed98a96f2683.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the center of the star does indeed have a high PageRank value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**A notion of centrality for undirected graphs** We can apply PageRank\(\idx{PageRank}\xdi\)
    in the undirected case as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider random walk on the undirected graph \(G\). That is, at every step,
    we pick a neighbor of the current state uniformly at random. If needed, add a
    self-loop to any isolated vertex. Then the transition matrix is
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = D^{-1} A, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(D\) is the degree matrix and \(A\) is the adjacency matrix. A stationary
    distribution \(\bpi = (\pi_1,\ldots,\pi_n)^T\) is a row vector satisfying in this
    case
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \sum_{j=1}^n \pi_j p_{j,i} = \sum_{j \in N(i)} \pi_j \frac{1}{\delta(j)}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: We already know the solution to this system of equations. In the connected case
    without damping, the unique stationary distribution of random walk on \(G\) is
    given by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in
    V. \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, the centrality of a node is directly proportional to its degree, i.e.,
    how many neighbors it has. Up to the scaling factor, this is known as [degree
    centrality](https://en.wikipedia.org/wiki/Centrality#Degree_centrality)\(\idx{degree
    centrality}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: For a general undirected graph that may not be connected, we can use a damping
    factor to enforce irreducibility. We add a small probability at each step of landing
    at a uniformly chosen node. That is, we define the transition matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q = \alpha P + (1-\alpha) \frac{1}{n} \mathbf{1} \mathbf{1}^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(\alpha \in (0,1)\) known as the damping factor. We define the PageRank
    vector \(\mathbf{PR}\) as the unique stationary distribution of \(Q = (q_{i,j})_{i,j=1}^n\),
    that is, the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{PR}_i = \sum_{j=1}^n \mathrm{PR}_j \, q_{j,i}, \]
  prefs: []
  type: TYPE_NORMAL
- en: with \(\mathbf{PR} \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \mathrm{PR}_i = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We revisit the star example in the undirected case.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/c7f2056453edc830e099413bfdb90e0047d36da8030c2b8abf8be4198801e5fc.png](../Images/2d0d2764fb0069741155c9c91833c00b.png)'
  prefs: []
  type: TYPE_IMG
- en: We first compute the PageRank vector without damping. Here the random walk is
    periodic (Why?) so power iteration may fail (Try it!). Instead, we use a small
    amount of damping and increase the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The PageRank value for the center node is indeed roughly \(7\) times larger
    than the other ones, as can be expected from the ratio of their degrees.
  prefs: []
  type: TYPE_NORMAL
- en: We try again with more damping. This time the ratio of PageRank values is not
    quite the same as the ratio of degrees, but the center node continues to have
    a higher value than the other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** There are many other centrality measures besides PageRank,
    such as betweenness centrality, closeness centrality, and eigenvector centrality.
    Ask your favorite AI chatbot to explain these measures and discuss their similarities
    and differences with PageRank. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.3\. Personalized PageRank[#](#personalized-pagerank "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We return to [MathWorld](https://mathworld.wolfram.com) dataset. Recall that
    each page of MathWorld concerns a particular mathematical concept. In a section
    entitled “SEE ALSO”, other related mathematical concepts are listed with a link
    to their MathWorld page. Our goal is to identify “central” vertices in the resulting
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Platonic solids (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Platonic solids](../Images/4393ac7f9684a7e62d5f82de15668d37.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We load the dataset again.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '|  | from | to |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 47 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 404 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 2721 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: The second file contains the titles of the pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '|  | title |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Alexander''s Horned Sphere |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Exotic Sphere |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Antoine''s Horned Sphere |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Flat |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Poincaré Manifold |'
  prefs: []
  type: TYPE_TB
- en: We construct the graph by adding the edges one by one. We first convert `df_edges`
    into a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: To apply PageRank, we construct the adjacency matric of the graph. We also define
    a vector of title pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We use [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)
    to identify the pages with highest scores. We apply it to `-pr_mw` to sort from
    the highest to lowest value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The top 25 topics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: We indeed get a list of central concepts in mathematics – including several
    we have encountered previously such as `Normal Distribution`, `Tree`, `Vector`
    or `Derivative`.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: There is a variant of PageRank, referred to as Personalized PageRank (PPR)\(\idx{Personalized
    PageRank}\xdi\), which aims to tailor the outcome to specific interests. This
    is accomplished from a simple change to the algorithm. When teleporting, rather
    than jumping to a uniformly random page, we instead jump to an arbitrary distribution
    which is meant to capture some specific interests. In the context of the web for
    instance, this distribution might be uniform over someone’s bookmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We adapt `pagerank` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** To test PPR, consider the distribution concentrated on
    a single topic `Normal Distribution`. This is topic number `1270`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: We now run PPR and list the top 25 pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The top 25 topics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: This indeed returns various statistical concepts, particularly related to the
    normal dsitribution.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The PageRank algorithm has been adapted for various applications
    beyond web search, such as ranking scientific papers, analyzing social networks,
    and even ranking sports teams [[Gle](https://arxiv.org/abs/1407.5107)]. Ask your
    favorite AI chatbot to discuss some of these applications and how the PageRank
    algorithm is modified for each case. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Consider a random walk on the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: What is the transition matrix for this random walk?
  prefs: []
  type: TYPE_NORMAL
- en: a)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 &
    1/3 & 1/3 \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: b)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 0 & 1/2 & 1/2 \\ 1/2 & 0 & 1/2 \\ 1/2 & 1/2
    & 0 \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: c)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: d)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In a random walk on a directed graph, the transition probability from
    vertex \(i\) to vertex \(j\) is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(p_{i,j} = \frac{1}{\delta^-(j)}\) for all \(j \in N^-(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(p_{i,j} = \frac{1}{\delta^+(j)}\) for all \(j \in N^+(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(p_{i,j} = \frac{1}{\delta^-(i)}\) for all \(j \in N^-(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(p_{i,j} = \frac{1}{\delta^+(i)}\) for all \(j \in N^+(i)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** The transition matrix \(P\) of a random walk on a directed graph can
    be expressed in terms of the adjacency matrix \(A\) as:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(P = AD^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(P = A^TD^{-1}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(P = D^{-1}A\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(P = D^{-1}A^T\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In a random walk on an undirected graph, the stationary distribution
    \(\boldsymbol{\pi}\) satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\pi_i = \frac{\delta^+(i)}{\sum_{j \in V} \delta^+(j)}\) for all \(i \in
    V\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\pi_i = \frac{1}{\delta(i)}\) for all \(i \in V\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\pi_i = \frac{1}{|V|}\) for all \(i \in V\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\pi_i = \frac{\delta(i)}{\sum_{j \in V} \delta(j)}\) for all \(i \in V\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Personalized PageRank differs from standard PageRank in that:'
  prefs: []
  type: TYPE_NORMAL
- en: a) It considers the user’s browsing history
  prefs: []
  type: TYPE_NORMAL
- en: b) It jumps to a non-uniform distribution when teleporting
  prefs: []
  type: TYPE_NORMAL
- en: c) It uses a different damping factor
  prefs: []
  type: TYPE_NORMAL
- en: d) It only considers a subset of the graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: Each node has a degree of 2, and the probability
    of transitioning to each neighbor is 1/2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: d. Justification: The text states, “In words, at each step, we
    choose an outgoing edge from the current state uniformly at random,” which corresponds
    to the transition probability \(p_{i,j} = \frac{1}{\delta^+(i)}\) for all \(j
    \in N^+(i)\), where \(\delta^+(i)\) is the out-degree of vertex \(i\) and \(N^+(i)\)
    is the set of vertices with an edge from \(i\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states, “The transition matrix of
    random walk on \(G\) satisfying the conditions of the definition above is \(P
    = D^{-1}A\), where \(D\) is the out-degree matrix.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text states, “In the connected case without
    damping, the unique stationary distribution of random walk on \(G\) is given by
    \(\pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \forall i \in V\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states, “When teleporting, rather
    than jumping to a uniformly random page, we instead jump to an arbitrary distribution
    \(\boldsymbol{\mu}\) which is meant to capture some specific interests.”'
  prefs: []
  type: TYPE_NORMAL
