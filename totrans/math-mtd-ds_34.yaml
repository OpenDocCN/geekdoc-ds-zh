- en: 4.8\. Online supplementary materials#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/supp/roch-mmids-svd-supp.html](https://mmids-textbook.github.io/chap04_svd/supp/roch-mmids-svd-supp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.1** The answer is \(\mathrm{rk}(A) = 2\). To see this, observe that
    the third column is the sum of the first two columns, so the column space is spanned
    by the first two columns. These two columns are linearly independent, so the dimension
    of the column space (i.e., the rank) is 2.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.3** The eigenvalues are \(\lambda_1 = 4\) and \(\lambda_2 = 2\). For
    \(\lambda_1 = 4\), solving \((A - 4I)\mathbf{x} = \mathbf{0}\) gives the eigenvector
    \(\mathbf{v}_1 = (1, 1)\). For \(\lambda_2 = 2\), solving \((A - 2I)\mathbf{x}
    = \mathbf{0}\) gives the eigenvector \(\mathbf{v}_2 = (1, -1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.5** Normalizing the eigenvectors to get an orthonormal basis: \(\mathbf{q}_1
    = \frac{1}{\sqrt{2}}(1, 1)\) and \(\mathbf{q}_2 = \frac{1}{\sqrt{2}}(1, -1)\).
    Then \(A = Q \Lambda Q^T\) where \(Q = (\mathbf{q}_1, \mathbf{q}_2)\) and \(\Lambda
    = \mathrm{diag}(4, 2)\). Explicitly,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.7**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.9** The rank of \(A\) is 1\. The second column is a multiple of the
    first column, so the column space is one-dimensional.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.11** The characteristic polynomial of \(A\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the eigenvalues are \(\lambda_1 = 1\) and \(\lambda_2 = 3\). For \(\lambda_1
    = 1\), we solve \((A - I)\mathbf{v} = \mathbf{0}\) to get \(\mathbf{v}_1 = \begin{pmatrix}
    1 \\ -1 \end{pmatrix}\). For \(\lambda_2 = 3\), we solve \((A - 3I)\mathbf{v}
    = 0\) to get \(\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.13** The columns of \(A\) are linearly dependent, since the second column
    is twice the first column. Hence, a basis for the column space of \(A\) is given
    by \(\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix} \right\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.15** The eigenvalues of \(A\) are \(\lambda_1 = 3\) and \(\lambda_2
    = -1\). Since \(A\) has a negative eigenvalue, it is not positive semidefinite.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.17** The Hessian of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues of the Hessian are \(\lambda_1 = 2\) and \(\lambda_2 = -2\).
    Since one eigenvalue is negative, the Hessian is not positive semidefinite, and
    \(f\) is not convex.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.19** The Hessian of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues of the Hessian are \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) and
    \(\lambda_2 = 0\), which are both nonnegative for all \(x, y\). Therefore, the
    Hessian is positive semidefinite, and \(f\) is convex.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.1** We have \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\). By
    the method described in the text, \(w_1\) is a unit eigenvector of \(A^TA = \begin{pmatrix}
    5 & 0 \\ 0 & 5 \end{pmatrix}\) corresponding to the largest eigenvalue. Thus,
    we can take \(\mathbf{w}_1 = (1, 0)\) or \(\mathbf{w}_1 = (0, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.3** We can take \(U = I_2\), \(\Sigma = A\), and \(V = I_2\). This is
    an SVD of \(A\) because \(U\) and \(V\) are orthogonal and \(\Sigma\) is diagonal.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.5** We have \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\).
    The characteristic polynomial of \(A^TA\) is \(\lambda^2 - 25\lambda = \lambda(\lambda
    - 25)\), so the eigenvalues are \(\lambda_1 = 25\) and \(\lambda_2 = 0\). An eigenvector
    corresponding to \(\lambda_1\) is \((1, 2)\), and an eigenvector corresponding
    to \(\lambda_2\) is \((-2, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.7** We have that the singular values of \(A\) are \(\sigma_1 = \sqrt{25}
    = 5\) and \(\sigma_2 = 0\). We can take \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1 = (1/\sqrt{5}, 2/\sqrt{5})\). Since
    the rank of \(A\) is 1, this gives a compact SVD of \(A\): \(A = U \Sigma V^T\)
    with \(U = \mathbf{u}_1\), \(\Sigma = (5)\), and \(V = \mathbf{v}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.9** From the full SVD of \(A\), we have that: An orthonormal basis for
    \(\mathrm{col}(A)\) is given by the first column of \(U\): \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\).
    An orthonormal basis for \(\mathrm{row}(A)\) is given by the first column of \(V\):
    \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\). An orthonormal basis for \(\mathrm{null}(A)\)
    is given by the second column of \(V\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\). An
    orthonormal basis for \(\mathrm{null}(A^T)\) is given by the second column of
    \(U\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.11** From its diagonal form, we see that \(\lambda_1 = 15\), \(\mathbf{q}_1
    = (1, 0)\); \(\lambda_2 = 7\), \(\mathbf{q}_2 = (0, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.13** By direct computation, \(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix}
    1 & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.15** By direct computation,'
  prefs: []
  type: TYPE_NORMAL
- en: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
  prefs: []
  type: TYPE_NORMAL
- en: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.17** The best approximating subspace of dimension \(k=1\) is the line
    spanned by the vector \(\mathbf{v}_1 = (1, 0)\). The sum of squared distances
    to this subspace is \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 =
    5 + 5 + 2 + 10 - 15 = 7.\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal entries are being raised to increasing powers, while the off-diagonal
    entries remain zero.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.7** We have \(A = Q \Lambda Q^T\), where \(Q\) is the matrix of normalized
    eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues. Then,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly, \(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.9** We have \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\).
    Let’s find the eigenvalues of the matrix \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 &
    5 \end{pmatrix}\). We solve the characteristic equation \(\det(A^TA - \lambda
    I) = 0\). First, let’s calculate the characteristic polynomial: \(\det(A^TA -
    \lambda I) = \det(\begin{pmatrix} 1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})=
    \lambda^2 - 6\lambda + 1\), so \((\lambda - 3)^2 - 8 = 0\) or \((\lambda - 3)^2
    = 8\) or \(\lambda = 3 \pm 2\sqrt{2}\). Therefore, the eigenvalues of \(A^TA\)
    are: \(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) and \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\). Hence, this matrix is positive semidefinite because its eigenvalues
    are 0 and 6, both of which are non-negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). The unit
    norm constraint requires that \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\).
    The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\). Uncorrelatedness requires that \(\frac{1}{n-1}\sum_{i=1}^n
    t_{i1}t_{i2} = 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.7** First, compute the mean of each column:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean-centered data matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\), \(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\). The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.11** The second principal component must be uncorrelated with the first,
    so its loading vector must be orthogonal to \(\varphi_1\). One such vector is
    \(\varphi_2 = (-0.6, 0.8)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.1** The Frobenius norm is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.7** The induced 2-norm of the difference between a matrix and its rank-1
    truncated SVD is equal to the second singular value. Therefore, using the SVD
    from before, we have \(\|A - A_1\|_2 = \sigma_2 = 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.9** The matrix \(A\) is singular (its determinant is zero), so it doesn’t
    have an inverse. Therefore, \(\|A^{-1}\|_2\) is undefined. Note that the induced
    2-norm of the pseudoinverse \(A^+\) is not the same as the induced 2-norm of the
    inverse (when it exists).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define the column space, row space, and rank of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the row rank equals the column rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply properties of matrix rank, such as \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\)
    and \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the Rank-Nullity Theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define eigenvalues and eigenvectors of a square matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that a symmetric matrix has at most d distinct eigenvalues, where d is
    the matrix size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Spectral Theorem for symmetric matrices and explain its implications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write the spectral decomposition of a symmetric matrix using outer products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine whether a symmetric matrix is positive semidefinite or positive definite
    based on its eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute eigenvalues and eigenvectors of symmetric matrices using programming
    tools like NumPy’s `linalg.eig`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the objective of the best approximating subspace problem and formulate
    it mathematically as a minimization of the sum of squared distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the best approximating subspace problem can be solved greedily by
    finding the best one-dimensional subspace, then the best one-dimensional subspace
    orthogonal to the first, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the singular value decomposition (SVD) of a matrix and describe the properties
    of the matrices involved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the existence of the SVD for any real matrix using the Spectral Theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the connection between the SVD of a matrix \(A\) and the spectral decompositions
    of \(A^T A\) and \(A A^T\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the SVD of simple matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the SVD of the data matrix to find the best k-dimensional approximating
    subspace to a set of data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the singular values as capturing the contributions of the right singular
    vectors to the fit of the approximating subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain low-dimensional representations of data points using the truncated SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinguish between full and compact forms of the SVD and convert between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the key lemma for power iteration in the positive semidefinite case and
    explain why it implies convergence to the top eigenvector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the power iteration lemma to the general case of singular value decomposition
    (SVD) and justify why repeated multiplication of A^T A with a random vector converges
    to the top right singular vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the corresponding top singular value and left singular vector given
    the converged top right singular vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the orthogonal iteration method for finding additional singular vectors
    beyond the top one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the power iteration method and orthogonal iteration to compute the SVD
    of a given matrix and use it to find the best low-dimensional subspace approximation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the power iteration method and orthogonal iteration in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define principal components and loadings in the context of PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express the objective of PCA as a constrained optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish the connection between PCA and singular value decomposition (SVD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement PCA using an SVD algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the results of PCA in the context of dimensionality reduction and
    data visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Frobenius norm and the induced 2-norm of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express the Frobenius norm and the induced 2-norm of a matrix in terms of its
    singular values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Eckart-Young theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the pseudoinverse of a matrix using the SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the pseudoinverse to solve least squares problems when the matrix has
    full column rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the pseudoinverse to find the least norm solution for underdetermined
    systems with full row rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the ridge regression problem as a regularized optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how ridge regression works by analyzing its solution in terms of the
    SVD of the design matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.8.2.1\. Computing more singular vectors[#](#computing-more-singular-vectors
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have shown how to compute the first singular vector. How do we compute more
    singular vectors? One approach is to first compute \(\mathbf{v}_1\) (or \(-\mathbf{v}_1\)),
    then find a vector \(\mathbf{y}\) orthogonal to it, and proceed as above. And
    then we repeat until we have all \(m\) right singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are often interested only in the top, say \(\ell < m\), singular vectors.
    An alternative approach in that case is to start with \(\ell\) random vectors
    and, first, find an orthonormal basis for the space they span. Then to quote [BHK,
    Section 3.7.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Then compute \(B\) times each of the basis vectors, and find an orthonormal
    basis for the space spanned by the resulting vectors. Intuitively, one has applied
    \(B\) to a subspace rather than a single vector. One repeatedly applies \(B\)
    to the subspace, calculating an orthonormal basis after each application to prevent
    the subspace collapsing to the one dimensional subspace spanned by the first singular
    vector.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will not prove here that this approach, known as orthogonal iteration, works.
    The proof is similar to that of the *Power Iteration Lemma*.
  prefs: []
  type: TYPE_NORMAL
- en: We implement this last algorithm. We will need our previous implementation of
    *Gram-Schimdt*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that above we avoided forming the matrix \(A^T A\). With a small number
    of iterations, that approach potentially requires fewer arithmetic operations
    overall and it allows to take advantage of the possible sparsity of \(A\) (i.e.
    the fact that it may have many zeros).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We apply it again to our two-cluster example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try again, but after projecting on the top two singular vectors. Recall
    that this corresponds to finding the best two-dimensional approximating subspace.
    The projection can be computed using the truncated SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\). We can interpret the rows of \(U_{(2)} \Sigma_{(2)}\) as the coefficients
    of each data point in the basis \(\mathbf{v}_1,\mathbf{v}_2\). We will work in
    that basis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, looking at the first two right singular vectors, we see that the first
    one does align quite well with the first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2.2\. Pseudoinverse[#](#pseudoinverse "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SVD leads to a natural generalization of the matrix inverse. First an observation.
    Recall that, to take the product of two square diagonal matrices, we simply multiply
    the corresponding diagonal entries. Let \(\Sigma \in \mathbb{R}^{r \times r}\)
    be a square diagonal matrix with diagonal entries \(\sigma_1,\ldots,\sigma_r\).
    If all diagonal entries are non-zero, then the matrix is invertible (since its
    columns then form a basis of the full space). The inverse of \(\Sigma\) in that
    case is simply the diagonal matrix \(\Sigma^{-1}\) with diagonal entries \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\).
    This can be confirmed by checking the definition of the inverse
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We are ready for our main definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Pseudoinverse)** Let \(A \in \mathbb{R}^{n \times m}\) be
    a matrix with compact SVD \(A = U \Sigma V^T\) and singular values \(\sigma_1
    \geq \cdots \geq \sigma_r > 0\). A pseudoinverse \(A^+ \in \mathbb{R}^{m \times
    n}\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = V \Sigma^{-1} U^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: While it is not obvious from the definition (why?), the pseudoinverse is in
    fact unique. To see that it is indeed a generalization of an inverse, we make
    a series of observations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 1:* Note that, using that \(U\) has orthonormal columns,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: which in general in not the identity matrix. Indeed it corresponds instead to
    the projection matrix onto the column space of \(A\), since the columns of \(U\)
    form an orthonormal basis of that linear subspace. As a result
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(A A^+\) is not the identity matrix, but it does map the columns of \(A\)
    to themselves. Put differently, it is the identity map “when restricted to \(\mathrm{col}(A)\)”.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
  prefs: []
  type: TYPE_NORMAL
- en: is the projection matrix onto the row space of \(A\), and
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A^+ A) A^+ = A^+. \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 2:* If \(A\) has full column rank \(m \leq n\), then \(r = m\).
    In that case, the columns of \(V\) form an orthonormal basis of all of \(\mathbb{R}^m\),
    i.e., \(V\) is orthogonal. Hence,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ A = V V^T = I_{m \times m}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if \(A\) has full row rank \(n \leq m\), then \(A A^+ = I_{n \times
    n}\).
  prefs: []
  type: TYPE_NORMAL
- en: If both cases hold, then \(n = m\), i.e., \(A\) is square, and \(\mathrm{rk}(A)
    = n\), i.e., \(A\) is invertible. We then get
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^+ = A^+ A = I_{n\times n}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That implies that \(A^+ = A^{-1}\) by the *Existence of an Inverse Lemma* (which
    includes uniqueness of the matrix inverse).
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 3:* Recall that, when \(A\) is nonsingular, the system \(A \mathbf{x}
    = \mathbf{b}\) admits the unique solution \(\mathbf{x} = A^{-1} \mathbf{b}\).
    In the overdetermined case, the pseudoinverse provides a solution to the linear
    least squares problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Least Squares)** Let \(A \in \mathbb{R}^{n \times
    m}\) with \(m \leq n\). A solution to the linear least squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  prefs: []
  type: TYPE_NORMAL
- en: is given by \(\mathbf{x}^* = A^+ \mathbf{b}\). Further, if \(A\) has full column
    rank \(m\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = (A^T A)^{-1} A^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* For the first part, we use that the solution to the least squares
    problem is the orthogonal projection. For the second part, we use the SVD definition
    and check that the two sides are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(A = U \Sigma V^T\) be a compact SVD of \(A\). For the first
    claim, note that the choice of \(\mathbf{x}^*\) in the statement gives'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(A \mathbf{x}^*\) is the orthogonal projection of \(\mathbf{b}\) onto
    the column space of \(A\) - which we proved previously is the solution to the
    linear least squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: Now onto the second claim. Recall that, when \(A\) is of full rank, the matrix
    \(A^T A\) is nonsingular. We then note that, using the notation \(\Sigma^{-2}
    = (\Sigma^{-1})^2\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. Here, we used that \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudoinverse also provides a solution in the case of an underdetermined
    system. Here, however, there are in general infinitely many solutions. The one
    chosen by the pseudoinverse has a special property as we see now: it is the least
    norm solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Underdetermined Systems)** Let \(A \in \mathbb{R}^{n
    \times m}\) with \(m > n\) and \(\mathbf{b} \in \mathbb{R}^n\). Further assume
    that \(A\) has full row rank \(n\). Then \(\mathbf{x}^* = A^+ \mathbf{b}\) is
    a solution to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in that case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = A^T (A A^T)^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We first prove the formula for \(A^+\). As we did in the overdetermined
    case, it can be checked by substituting a compact SVD \(A = U \Sigma V^T\). Recall
    that, when \(A^T\) is of full rank, the matrix \(A A^T\) is nonsingular. We then
    note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. Here, we used that \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Because \(A\) has full row rank, \(\mathbf{b} \in \mathrm{col}(A)\) and there
    is at least one \(\mathbf{x}\) such that \(A \mathbf{x} = \mathbf{b}\). One such
    solution is provided by the pseudoinverse. Indeed, from a previous observation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the columns of \(U\) form an orthonormal basis of \(\mathbb{R}^n\)
    by the rank assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{x}\) be any other solution to the system. Then \(A(\mathbf{x}
    - \mathbf{x}^*) = \mathbf{b} - \mathbf{b} = \mathbf{0}\). That implies
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, \(\mathbf{x} - \mathbf{x}^*\) and \(\mathbf{x}^*\) are orthogonal.
    By *Pythagoras*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves that \(\mathbf{x}^*\) has the smallest norm among all solutions
    to the system \(A \mathbf{x} = \mathbf{b}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Continuing a previous example, let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We compute the pseudoinverse. By the formula, in the rank one case, it is simply
    (Check this!)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be a square nonsingular
    matrix. Let \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be a compact
    SVD of \(A\), where we used the fact that the rank of \(A\) is \(n\) so it has
    \(n\) strictly positive singular values. We seek to compute \(\|A^{-1}\|_2\) in
    terms of the singular values.'
  prefs: []
  type: TYPE_NORMAL
- en: Because \(A\) is invertible, \(A^+ = A^{-1}\). So we compute the pseudoinverse
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: The sum on the right-hand side is not quite a compact SVD of \(A^{-1}\) because
    the coefficients \(\sigma_j^{-1}\) are non-decreasing in \(j\).
  prefs: []
  type: TYPE_NORMAL
- en: But writing the sum in reverse order
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
  prefs: []
  type: TYPE_NORMAL
- en: does give a compact SVD of \(A^{-1}\), since \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1}
    > 0\) and \(\{\mathbf{v}_j\}_{j=1}^n\) and \(\{\mathbf{u}_j\}_{j=1}^n\) are orthonormal
    lists. Hence, the \(2\)-norm is given by the largest singular value, that is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the pseudoinverse of a matrix can be computed
    using the function [`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try our previous example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2.3\. Condition numbers[#](#condition-numbers "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we introduce condition numbers, a measure of perturbation sensitivity
    for numerical problems. We look in particular at the conditioning of the least-squares
    problem. We begin with the concept of pseudoinverse, which is important in its
    own right.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditioning of matrix-vector multiplication** We define the condition number
    of a matrix and show that it captures some information about the sensitivity to
    perturbations of matrix-vector multiplications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Condition number of a matrix)** The condition number (in
    the induced \(2\)-norm) of a square, nonsingular matrix \(A \in \mathbb{R}^{n
    \times n}\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this can be computed as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the example above. In words, \(\kappa_2(A)\) is the ratio of the
    largest to the smallest stretching under \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Conditioning of Matrix-Vector Multiplication)** Let \(M \in
    \mathbb{R}^{n \times n}\) be nonsingular. Then, for any \(\mathbf{z} \in \mathbb{R}^n\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
  prefs: []
  type: TYPE_NORMAL
- en: and the inequality is tight in the sense that there is an \(\mathbf{x}\) and
    a \(\mathbf{d}\) that achieves it.
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: The ratio above measures the worst rate of relative change in \(M \mathbf{z}\)
    under infinitesimal perturbations of \(\mathbf{z}\). The theorem says that when
    \(\kappa_2(M)\) is large, a case referred to as ill-conditioning, large relative
    changes in \(M \mathbf{z}\) can be obtained from relatively small perturbations
    to \(\mathbf{z}\). In words, a matrix-vector product is potentially sensitive
    to perturbations when the matrix is ill-conditioned.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Write'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\),
    which was shown in a previous example.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we see that the ratio can achieve its maximum by taking \(\mathbf{d}\)
    and \(\mathbf{z}\) to be the right singular vectors corresponding to \(\sigma_1\)
    and \(\sigma_n\) respectively. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: If we apply the theorem to the inverse instead, we get that the relative conditioning
    of the nonsingular linear system \(A \mathbf{x} = \mathbf{b}\) to perturbations
    in \(\mathbf{b}\) is \(\kappa_2(A)\). The latter can be large in particular when
    the columns of \(A\) are close to linearly dependent. This is detailed in the
    next example.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be nonsingular. Then, for
    any \(\mathbf{b} \in \mathbb{R}^n\), there exists a unique solution to \(A \mathbf{x}
    = \mathbf{b}\), namely,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we solve the perturbed system
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
  prefs: []
  type: TYPE_NORMAL
- en: for some vector \(\delta\mathbf{b}\). We use the *Conditioning of Matrix-Vector
    Multiplication Theorem* to bound the norm of \(\delta\mathbf{x}\).
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, set
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So we get that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
  prefs: []
  type: TYPE_NORMAL
- en: Note also that, because \((A^{-1})^{-1} = A\), we have \(\kappa_2(A^{-1}) =
    \kappa_2(A)\). Rearranging, we finally get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the larger the condition number is, the larger the potential relative
    effect on the solution of the linear system is for a given relative perturbation
    size. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the condition number of a matrix can be computed
    using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, orthogonal matrices have condition number \(1\), the lowest possible
    value for it (Why?). That indicates that orthogonal matrices have good numerical
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In contrast, matrices with nearly linearly dependent columns have large condition
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the SVD of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We compute the solution to \(A \mathbf{x} = \mathbf{b}\) when \(\mathbf{b}\)
    is the left singular vector of \(A\) corresponding to the largest singular value.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case bound is achieved when \(\mathbf{z} =
    \mathbf{b}\) is right singular vector of \(M= A^{-1}\) corresponding to the lowest
    singular value. In a previous example, given a matrix \(A = \sum_{j=1}^n \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) in compact SVD form, we derived a compact SVD for
    the inverse as
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, compared to the SVD of \(A\), the order of the singular values is reversed
    and the roles of the left and right singular vectors are exchanged. So we take
    \(\mathbf{b}\) to be the top left singular vector of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We make a small perturbation in the direction of the second right singular vector.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case is achieved when \(\mathbf{d} = \delta\mathbf{b}\)
    is top right singular vector of \(M = A^{-1}\). By the argument above, that is
    the left singular vector of \(A\) corresponding to the lowest singular value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The relative change in solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is exactly the condition number of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to the least-squares problem** We return to the least-squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We showed that the solution satisfies the normal equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(A\) may not be square and invertible. We define a more general notion
    of condition number.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Condition number of a matrix: general case)** The condition
    number (in the induced \(2\)-norm) of a matrix \(A \in \mathbb{R}^{n \times m}\)
    is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: As we show next, the condition number of \(A^T A\) can be much larger than that
    of \(A\) itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Condition number of \(A^T A\))** Let \(A \in \mathbb{R}^{n \times
    m}\) have full column rank. The'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the SVD.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(A = U \Sigma V^T\) be an SVD of \(A\) with singular values \(\sigma_1
    \geq \cdots \geq \sigma_m > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular the latter expression is an SVD of \(A^T A\), and hence the condition
    number of \(A^T A\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We give a quick example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This observation – and the resulting increased numerical instability – is one
    of the reasons we previously developed an alternative approach to the least-squares
    problem. Quoting [Sol, Section 5.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, a primary reason that \(\mathrm{cond}(A^T A)\) can be large is
    that columns of \(A\) might look “similar” […] If two columns \(\mathbf{a}_i\)
    and \(\mathbf{a}_j\) satisfy \(\mathbf{a}_i \approx \mathbf{a}_j\), then the least-squares
    residual length \(\|\mathbf{b} - A \mathbf{x}\|_2\) will not suffer much if we
    replace multiples of \(\mathbf{a}_i\) with multiples of \(\mathbf{a}_j\) or vice
    versa. This wide range of nearly—but not completely—equivalent solutions yields
    poor conditioning. […] To solve such poorly conditioned problems, we will employ
    an alternative technique with closer attention to the column space of \(A\) rather
    than employing row operations as in Gaussian elimination. This strategy identifies
    and deals with such near-dependencies explicitly, bringing about greater numerical
    stability.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We quote without proof a theorem from [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Theorem 4.2.7] which sheds further light on this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Accuracy of Least-squares Solutions)** Let \(\mathbf{x}^*\)
    be the solution of the least-squares problem \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\). Let \(\mathbf{x}_{\mathrm{NE}}\) be the solution
    obtained by forming and solving the normal equations in [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    with rounding unit \(\epsilon_M\). Then \(\mathbf{x}_{\mathrm{NE}}\) satisfies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{x}_{\mathrm{QR}}\) be the solution obtained from a QR factorization
    in the same arithmetic. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) is the residual vector.
    The constants \(\gamma\) are slowly growing functions of the dimensions of the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain, let’s quote [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Section 4.2.3] again:'
  prefs: []
  type: TYPE_NORMAL
- en: The perturbation theory for the normal equations shows that \(\kappa_2^2(A)\)
    controls the size of the errors we can expect. The bound for the solution computed
    from the QR equation also has a term multiplied by \(\kappa_2^2(A)\), but this
    term is also multiplied by the scaled residual, which can diminish its effect.
    However, in many applications the vector \(\mathbf{b}\) is contaminated with error,
    and the residual can, in general, be no smaller than the size of that error.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Here is a numerical example taken from [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    Lecture 19]. We will approximate the following function with a polynomial.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We use a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix),
    which can be constructed using [`numpy.vander`](https://numpy.org/doc/stable/reference/generated/numpy.vander.html),
    to perform polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The condition numbers of \(A\) and \(A^T A\) are both high in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We first use the normal equations and plot the residual vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We then use `numpy.linalg.qr` to compute the QR solution instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Proof of Greedy Finds Best Subspace** In this section, we prove the full
    version of the *Greedy Finds Best Subspace Theorem*. In particular, we do not
    use the *Spectral Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We proceed by induction. For an arbitrary orthonormal list \(\mathbf{w}_1,\ldots,\mathbf{w}_k\),
    we find an orthonormal basis of their span containing an element orthogonal to
    \(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1}\). Then we use the defintion of \(\mathbf{v}_k\)
    to conclude.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We reformulate the problem as a maximization'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max \left\{ \sum_{j=1}^k \|A \mathbf{w}_j\|^2\ :\ \text{$\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}$
    is an orthonormal list} \right\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we also replace the \(k\)-dimensional linear subspace \(\mathcal{Z}\)
    with an arbitrary orthonormal basis \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\).
  prefs: []
  type: TYPE_NORMAL
- en: We then proceed by induction. For \(k=1\), we define \(\mathbf{v}_1\) as a solution
    of the above maximization problem. Assume that, for any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_\ell\}\)
    with \(\ell < k\), we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^\ell \|A \mathbf{w}_j\|^2 \leq \sum_{j=1}^\ell \|A \mathbf{v}_j\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Now consider any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\) and
    let its span be \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1,\ldots,\mathbf{w}_k)\).
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 1:*** For \(j=1,\ldots,k-1\), let \(\mathbf{v}_j''\) be the orthogonal
    projection of \(\mathbf{v}_j\) onto \(\mathcal{W}\) and let \(\mathcal{V}'' =
    \mathrm{span}(\mathbf{v}''_1,\ldots,\mathbf{v}''_{k-1})\). Because \(\mathcal{V}''
    \subseteq \mathcal{W}\) has dimension at most \(k-1\) while \(\mathcal{W}\) itself
    has dimension \(k\), we can find an orthonormal basis \(\mathbf{w}''_1,\ldots,\mathbf{w}''_{k}\)
    of \(\mathcal{W}\) such that \(\mathbf{w}''_k\) is orthogonal to \(\mathcal{V}''\)
    (Why?). Then, for any \(j=1,\ldots,k-1\), we have the decomposition \(\mathbf{v}_j
    = \mathbf{v}''_j + (\mathbf{v}_j - \mathbf{v}''_j)\) where \(\mathbf{v}''_j \in
    \mathcal{V}''\) is orthogonal to \(\mathbf{w}''_k\) and \(\mathbf{v}_j - \mathbf{v}''_j\)
    is also orthogonal to \(\mathbf{w}''_k \in \mathcal{W}\) by the properties of
    the orthogonal projection onto \(\mathcal{W}\). Hence'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left\langle \sum_{j=1}^{k-1}\beta_j \mathbf{v}_j, \mathbf{w}'_k
    \right\rangle &= \left\langle \sum_{j=1}^{k-1}\beta_j [\mathbf{v}'_j + (\mathbf{v}_j
    - \mathbf{v}'_j)], \mathbf{w}'_k \right\rangle\\ &= \left\langle \sum_{j=1}^{k-1}\beta_j
    \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \left\langle \sum_{j=1}^{k-1}\beta_j
    (\mathbf{v}_j - \mathbf{v}'_j), \mathbf{w}'_k \right\rangle\\ &= \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}_j - \mathbf{v}'_j, \mathbf{w}'_k \right\rangle\\ &= 0
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\beta_j\)’s. That is, \(\mathbf{w}'_k\) is orthogonal to \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\).
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 2:*** By the induction hypothesis, we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*) \qquad \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 \leq \sum_{j=1}^{k-1} \|A
    \mathbf{v}_j\|^2\. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, recalling that the \(\boldsymbol{\alpha}_i^T\)’s are the rows of \(A\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ (**) \qquad \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{i=1}^n \|\mathrm{proj}_{\mathcal{W}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{j=1}^k \|A \mathbf{w}_j'\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: since the \(\mathbf{w}_j\)’s and \(\mathbf{w}'_j\)’s form an orthonormal basis
    of the same subspace \(\mathcal{W}\). Since \(\mathbf{w}'_k\) is orthogonal to
    \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\) by the conclusion of Step
    1, by the definition of \(\mathbf{v}_k\) as a solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}_k\in \arg\max \{\|A \mathbf{v}\|:\|\mathbf{v}\| = 1, \ \langle
    \mathbf{v}, \mathbf{v}_j \rangle = 0, \forall j \leq k-1\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*\!*\!*) \qquad \|A \mathbf{w}'_k\|^2 \leq \|A \mathbf{v}_k\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 3:*** Putting everything together'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sum_{j=1}^k \|A \mathbf{w}_j\|^2 &= \sum_{j=1}^k \|A \mathbf{w}_j'\|^2
    &\text{by $(**)$}\\ &= \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 + \|A \mathbf{w}'_k\|^2\\
    &\leq \sum_{j=1}^{k-1} \|A \mathbf{v}_j\|^2 + \|A \mathbf{v}_k\|^2 &\text{by $(*)$
    and $(*\!*\!*)$}\\ &= \sum_{j=1}^{k} \|A \mathbf{v}_j\|^2\\ \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: which proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof of Existence of the SVD** We return to the proof of the *Existence
    of SVD Theorem*. We give an alternative proof that does not rely on the *Spectral
    Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We have already done most of the work. The proof works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Compute the greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) from the
    *Greedy Finds Best Subspace Theorem* until the largest \(r\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{v}_r\|^2 > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: or, otherwise, until \(r=m\). The \(\mathbf{v}_j\)’s are orthonormal by construction.
  prefs: []
  type: TYPE_NORMAL
- en: (2) For \(j=1,\ldots,r\), let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_j = \|A \mathbf{v}_j\| \quad\text{and}\quad \mathbf{u}_j = \frac{1}{\sigma_j}
    A \mathbf{v}_j. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe that, by our choice of \(r\), the \(\sigma_j\)’s are \(> 0\). They
    are also non-increasing: by definition of the greedy sequence,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_i^2 = \max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \ \langle
    \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the set of orthogonality constraints gets larger as \(i\) increases. Hence,
    the \(\mathbf{u}_j\)’s have unit norm by definition.
  prefs: []
  type: TYPE_NORMAL
- en: We show below that they are also orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Let \(\mathbf{z} \in \mathbb{R}^m\) be any vector. To show that our construction
    is correct, we prove that \(A \mathbf{z} = \left(U \Sigma V^T\right)\mathbf{z}\).
    Let \(\mathcal{V} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\) and decompose
    \(\mathbf{z}\) into orthogonal components
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} = \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) + (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}))
    = \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle \,\mathbf{v}_j + (\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \]
  prefs: []
  type: TYPE_NORMAL
- en: Applying \(A\) and using linearity, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, A\mathbf{v}_j + A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We claim that \(A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})) = \mathbf{0}\).
    If \(\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) = \mathbf{0}\), that
    is certainly the case. If not, let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w} = \frac{\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})}{\|\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: By definition of \(r\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \max \{\|A \mathbf{w}_{r+1}\|^2 :\|\mathbf{w}_{r+1}\| = 1, \ \langle
    \mathbf{w}_{r+1}, \mathbf{v}_j \rangle = 0, \forall j \leq r\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, \(\|A \mathbf{w}_{r+1}\|^2 = 0\) (i.e. \(A \mathbf{w}_{r+1}
    = \mathbf{0}\)), for any unit vector \(\mathbf{w}_{r+1}\) orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_r\).
    That applies in particular to \(\mathbf{w}_{r+1} = \mathbf{w}\) by the *Orthogonal
    Projection Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, using the definition of \(\mathbf{u}_j\) and \(\sigma_j\), we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, \sigma_j \mathbf{u}_j\\ &= \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
    \mathbf{z}\\ &=\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{z}\\
    &= \left(U \Sigma V^T\right)\mathbf{z}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the existence of the SVD.
  prefs: []
  type: TYPE_NORMAL
- en: All that is left to prove is the orthogonality of the \(\mathbf{u}_j\)’s.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Left Singular Vectors are Orthogonal)** For all \(1 \leq i \neq
    j \leq r\), \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Quoting [BHK, Section 3.6]:'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively if \(\mathbf{u}_i\) and \(\mathbf{u}_j\), \(i < j\), were not orthogonal,
    one would suspect that the right singular vector \(\mathbf{v}_j\) had a component
    of \(\mathbf{v}_i\) which would contradict that \(\mathbf{v}_i\) and \(\mathbf{v}_j\)
    were orthogonal. Let \(i\) be the smallest integer such that \(\mathbf{u}_i\)
    is not orthogonal to all other \(\mathbf{u}_j\). Then to prove that \(\mathbf{u}_i\)
    and \(\mathbf{u}_j\) are orthogonal, we add a small component of \(\mathbf{v}_j\)
    to \(\mathbf{v}_i\), normalize the result to be a unit vector \(\mathbf{v}'_i
    \propto \mathbf{v}_i + \varepsilon \mathbf{v}_j\) and show that \(\|A \mathbf{v}'_i\|
    > \|A \mathbf{v}_i\|\), a contradiction.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Proof:* We argue by contradiction. Let \(i\) be the smallest index such that
    there is a \(j > i\) with \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = \delta
    \neq 0\). Assume \(\delta > 0\) (otherwise use \(-\mathbf{u}_i\)). For \(\varepsilon
    \in (0,1)\), because the \(\mathbf{v}_k\)’s are orthonormal, \(\|\mathbf{v}_i
    + \varepsilon \mathbf{v}_j\|^2 = 1+\varepsilon^2\). Consider the vectors'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}'_i = \frac{\mathbf{v}_i + \varepsilon \mathbf{v}_j}{\sqrt{1+\varepsilon^2}}
    \quad\text{and}\quad A \mathbf{v}'_i = \frac{\sigma_i \mathbf{u}_i + \varepsilon
    \sigma_j \mathbf{u}_j}{\sqrt{1+\varepsilon^2}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Observe that \(\mathbf{v}'_i\) is orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{i-1}\),
    so that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{v}'_i\| \leq \|A \mathbf{v}_i\| =\sigma_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, by the *Orthogonal Decomposition Lemma*, we can write \(A
    \mathbf{v}_i'\) as a sum of its orthogonal projection on the unit vector \(\mathbf{u}_i\)
    and \(A \mathbf{v}_i' - \mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\), which
    is orthogonal to \(\mathbf{u}_i\). In particular, by *Pythagoras*, \(\|A \mathbf{v}_i'\|
    \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|\). But that implies, for
    \(\varepsilon \in (0,1)\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|
    = \left\langle \mathbf{u}_i, A \mathbf{v}_i'\right\rangle = \frac{\sigma_i + \varepsilon
    \sigma_j \delta}{\sqrt{1+\varepsilon^2}} \geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the second inequality follows from a [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series)
    or the observation
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1+\varepsilon^2)\,(1-\varepsilon^2/2)^2 = (1+\varepsilon^2)\,(1-\varepsilon^2
    + \varepsilon^4/4) = 1 - 3/4 \varepsilon^4 + \varepsilon^6/4 \leq 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Now note that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|A \mathbf{v}_i'\| &\geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2)\\ &= \sigma_i + \varepsilon \sigma_j \delta - \varepsilon^2\sigma_i/2
    - \varepsilon^3 \sigma_i \sigma_j \delta/2\\ &= \sigma_i + \varepsilon \left(
    \sigma_j \delta - \varepsilon\sigma_i/2 - \varepsilon^2 \sigma_i \sigma_j \delta/2\right)\\
    &> \sigma_i \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\varepsilon\) small enough, contradicting the inequality above. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**SVD and Approximating Subspace** In constructing the SVD of \(A\), we used
    the greedy sequence for the best approximating subspace. Vice versa, given an
    SVD of \(A\), we can read off the solution to the approximating subspace problem.
    In other words, there was nothing special about the specific construction we used
    to prove existence of the SVD. While a matrix may have many SVDs, they all give
    a solution to the approximating subspace problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Further, this perspective gives what is known as a variational characterization
    of the singular values. We will have more to say about variational characterizations
    and their uses in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '*SVD and greedy sequence* Indeed, let'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  prefs: []
  type: TYPE_NORMAL
- en: be an SVD of \(A\) with
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: We show that the \(\mathbf{v}_j\)’s form a greedy sequence for the approximating
    subspace problem. Complete \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) into an orthonormal
    basis of \(\mathbb{R}^m\) by adding appropriate vectors \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\).
    By construction, \(A\mathbf{v}_{i} = \mathbf{0}\) for all \(i=j+1,\ldots,m\).
  prefs: []
  type: TYPE_NORMAL
- en: We start with the case \(j=1\). For any unit vector \(\mathbf{w} \in \mathbb{R}^m\),
    we expand it as \(\mathbf{w} = \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle
    \,\mathbf{v}_i\). By the *Properties of Orthonormal Lists*,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| A \left( \sum_{i=1}^m \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i \right) \right\|^2\\ &= \left\|
    \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\
    &= \left\| \sum_{i=1}^r \langle \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i
    \right\|^2\\ &= \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2,
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the orthonormality of the \(\mathbf{u}_i\)’s and the fact that
    \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\) are in the null space of \(A\). Because
    the \(\sigma_i\)’s are non-increasing, this last sum is maximized by taking \(\mathbf{w}
    = \mathbf{v}_1\). So we have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  prefs: []
  type: TYPE_NORMAL
- en: By the *Properties of Orthonormal Lists*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = \left\|\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle \mathbf{v}_i \right\|^2 = \|\mathbf{w}\|^2
    = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, because the \(\sigma_i\)’s are non-increasing, the sum
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{w}\|^2 = \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    \leq \sigma_1^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: This upper bound is achieved by taking \(\mathbf{w} = \mathbf{v}_1\). So we
    have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  prefs: []
  type: TYPE_NORMAL
- en: More generally, for any unit vector \(\mathbf{w} \in \mathbb{R}^m\) that is
    orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{j-1}\), we expand it as \(\mathbf{w}
    = \sum_{i=j}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i\). Then,
    as long as \(j\leq r\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| \sum_{i=j}^m \langle \mathbf{w},
    \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\ &= \left\| \sum_{i=j}^r \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i \right\|^2\\ &= \sum_{i=j}^r
    \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2\\ &\leq \sigma_j^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where again we used that the \(\sigma_i\)’s are non-increasing and \(\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = 1\). This last bound is achieved by
    taking \(\mathbf{w} = \mathbf{v}_j\).
  prefs: []
  type: TYPE_NORMAL
- en: So we have shown the following.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Variational Characterization of Singular Values)** Let \(A =
    \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be an SVD of \(A\) with \(\sigma_1
    \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_j^2 = \max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle \mathbf{w},
    \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}_j\in \arg\max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle
    \mathbf{w}, \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.1** The answer is \(\mathrm{rk}(A) = 2\). To see this, observe that
    the third column is the sum of the first two columns, so the column space is spanned
    by the first two columns. These two columns are linearly independent, so the dimension
    of the column space (i.e., the rank) is 2.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.3** The eigenvalues are \(\lambda_1 = 4\) and \(\lambda_2 = 2\). For
    \(\lambda_1 = 4\), solving \((A - 4I)\mathbf{x} = \mathbf{0}\) gives the eigenvector
    \(\mathbf{v}_1 = (1, 1)\). For \(\lambda_2 = 2\), solving \((A - 2I)\mathbf{x}
    = \mathbf{0}\) gives the eigenvector \(\mathbf{v}_2 = (1, -1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.5** Normalizing the eigenvectors to get an orthonormal basis: \(\mathbf{q}_1
    = \frac{1}{\sqrt{2}}(1, 1)\) and \(\mathbf{q}_2 = \frac{1}{\sqrt{2}}(1, -1)\).
    Then \(A = Q \Lambda Q^T\) where \(Q = (\mathbf{q}_1, \mathbf{q}_2)\) and \(\Lambda
    = \mathrm{diag}(4, 2)\). Explicitly,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.7**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.9** The rank of \(A\) is 1\. The second column is a multiple of the
    first column, so the column space is one-dimensional.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.11** The characteristic polynomial of \(A\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the eigenvalues are \(\lambda_1 = 1\) and \(\lambda_2 = 3\). For \(\lambda_1
    = 1\), we solve \((A - I)\mathbf{v} = \mathbf{0}\) to get \(\mathbf{v}_1 = \begin{pmatrix}
    1 \\ -1 \end{pmatrix}\). For \(\lambda_2 = 3\), we solve \((A - 3I)\mathbf{v}
    = 0\) to get \(\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.13** The columns of \(A\) are linearly dependent, since the second column
    is twice the first column. Hence, a basis for the column space of \(A\) is given
    by \(\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix} \right\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.15** The eigenvalues of \(A\) are \(\lambda_1 = 3\) and \(\lambda_2
    = -1\). Since \(A\) has a negative eigenvalue, it is not positive semidefinite.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.17** The Hessian of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues of the Hessian are \(\lambda_1 = 2\) and \(\lambda_2 = -2\).
    Since one eigenvalue is negative, the Hessian is not positive semidefinite, and
    \(f\) is not convex.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.19** The Hessian of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues of the Hessian are \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) and
    \(\lambda_2 = 0\), which are both nonnegative for all \(x, y\). Therefore, the
    Hessian is positive semidefinite, and \(f\) is convex.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.1** We have \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\). By
    the method described in the text, \(w_1\) is a unit eigenvector of \(A^TA = \begin{pmatrix}
    5 & 0 \\ 0 & 5 \end{pmatrix}\) corresponding to the largest eigenvalue. Thus,
    we can take \(\mathbf{w}_1 = (1, 0)\) or \(\mathbf{w}_1 = (0, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.3** We can take \(U = I_2\), \(\Sigma = A\), and \(V = I_2\). This is
    an SVD of \(A\) because \(U\) and \(V\) are orthogonal and \(\Sigma\) is diagonal.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.5** We have \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\).
    The characteristic polynomial of \(A^TA\) is \(\lambda^2 - 25\lambda = \lambda(\lambda
    - 25)\), so the eigenvalues are \(\lambda_1 = 25\) and \(\lambda_2 = 0\). An eigenvector
    corresponding to \(\lambda_1\) is \((1, 2)\), and an eigenvector corresponding
    to \(\lambda_2\) is \((-2, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.7** We have that the singular values of \(A\) are \(\sigma_1 = \sqrt{25}
    = 5\) and \(\sigma_2 = 0\). We can take \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1 = (1/\sqrt{5}, 2/\sqrt{5})\). Since
    the rank of \(A\) is 1, this gives a compact SVD of \(A\): \(A = U \Sigma V^T\)
    with \(U = \mathbf{u}_1\), \(\Sigma = (5)\), and \(V = \mathbf{v}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.9** From the full SVD of \(A\), we have that: An orthonormal basis for
    \(\mathrm{col}(A)\) is given by the first column of \(U\): \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\).
    An orthonormal basis for \(\mathrm{row}(A)\) is given by the first column of \(V\):
    \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\). An orthonormal basis for \(\mathrm{null}(A)\)
    is given by the second column of \(V\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\). An
    orthonormal basis for \(\mathrm{null}(A^T)\) is given by the second column of
    \(U\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.11** From its diagonal form, we see that \(\lambda_1 = 15\), \(\mathbf{q}_1
    = (1, 0)\); \(\lambda_2 = 7\), \(\mathbf{q}_2 = (0, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.13** By direct computation, \(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix}
    1 & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.15** By direct computation,'
  prefs: []
  type: TYPE_NORMAL
- en: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
  prefs: []
  type: TYPE_NORMAL
- en: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.17** The best approximating subspace of dimension \(k=1\) is the line
    spanned by the vector \(\mathbf{v}_1 = (1, 0)\). The sum of squared distances
    to this subspace is \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 =
    5 + 5 + 2 + 10 - 15 = 7.\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal entries are being raised to increasing powers, while the off-diagonal
    entries remain zero.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.7** We have \(A = Q \Lambda Q^T\), where \(Q\) is the matrix of normalized
    eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues. Then,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly, \(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.9** We have \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\).
    Let’s find the eigenvalues of the matrix \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 &
    5 \end{pmatrix}\). We solve the characteristic equation \(\det(A^TA - \lambda
    I) = 0\). First, let’s calculate the characteristic polynomial: \(\det(A^TA -
    \lambda I) = \det(\begin{pmatrix} 1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})=
    \lambda^2 - 6\lambda + 1\), so \((\lambda - 3)^2 - 8 = 0\) or \((\lambda - 3)^2
    = 8\) or \(\lambda = 3 \pm 2\sqrt{2}\). Therefore, the eigenvalues of \(A^TA\)
    are: \(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) and \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\). Hence, this matrix is positive semidefinite because its eigenvalues
    are 0 and 6, both of which are non-negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). The unit
    norm constraint requires that \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\).
    The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\). Uncorrelatedness requires that \(\frac{1}{n-1}\sum_{i=1}^n
    t_{i1}t_{i2} = 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.7** First, compute the mean of each column:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean-centered data matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\), \(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\). The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.11** The second principal component must be uncorrelated with the first,
    so its loading vector must be orthogonal to \(\varphi_1\). One such vector is
    \(\varphi_2 = (-0.6, 0.8)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.1** The Frobenius norm is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.7** The induced 2-norm of the difference between a matrix and its rank-1
    truncated SVD is equal to the second singular value. Therefore, using the SVD
    from before, we have \(\|A - A_1\|_2 = \sigma_2 = 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.9** The matrix \(A\) is singular (its determinant is zero), so it doesn’t
    have an inverse. Therefore, \(\|A^{-1}\|_2\) is undefined. Note that the induced
    2-norm of the pseudoinverse \(A^+\) is not the same as the induced 2-norm of the
    inverse (when it exists).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define the column space, row space, and rank of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the row rank equals the column rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply properties of matrix rank, such as \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\)
    and \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the Rank-Nullity Theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define eigenvalues and eigenvectors of a square matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that a symmetric matrix has at most d distinct eigenvalues, where d is
    the matrix size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Spectral Theorem for symmetric matrices and explain its implications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write the spectral decomposition of a symmetric matrix using outer products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine whether a symmetric matrix is positive semidefinite or positive definite
    based on its eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute eigenvalues and eigenvectors of symmetric matrices using programming
    tools like NumPy’s `linalg.eig`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the objective of the best approximating subspace problem and formulate
    it mathematically as a minimization of the sum of squared distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the best approximating subspace problem can be solved greedily by
    finding the best one-dimensional subspace, then the best one-dimensional subspace
    orthogonal to the first, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the singular value decomposition (SVD) of a matrix and describe the properties
    of the matrices involved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the existence of the SVD for any real matrix using the Spectral Theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the connection between the SVD of a matrix \(A\) and the spectral decompositions
    of \(A^T A\) and \(A A^T\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the SVD of simple matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the SVD of the data matrix to find the best k-dimensional approximating
    subspace to a set of data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the singular values as capturing the contributions of the right singular
    vectors to the fit of the approximating subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain low-dimensional representations of data points using the truncated SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinguish between full and compact forms of the SVD and convert between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the key lemma for power iteration in the positive semidefinite case and
    explain why it implies convergence to the top eigenvector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the power iteration lemma to the general case of singular value decomposition
    (SVD) and justify why repeated multiplication of A^T A with a random vector converges
    to the top right singular vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the corresponding top singular value and left singular vector given
    the converged top right singular vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the orthogonal iteration method for finding additional singular vectors
    beyond the top one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the power iteration method and orthogonal iteration to compute the SVD
    of a given matrix and use it to find the best low-dimensional subspace approximation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the power iteration method and orthogonal iteration in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define principal components and loadings in the context of PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express the objective of PCA as a constrained optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish the connection between PCA and singular value decomposition (SVD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement PCA using an SVD algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the results of PCA in the context of dimensionality reduction and
    data visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Frobenius norm and the induced 2-norm of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express the Frobenius norm and the induced 2-norm of a matrix in terms of its
    singular values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Eckart-Young theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the pseudoinverse of a matrix using the SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the pseudoinverse to solve least squares problems when the matrix has
    full column rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the pseudoinverse to find the least norm solution for underdetermined
    systems with full row rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the ridge regression problem as a regularized optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how ridge regression works by analyzing its solution in terms of the
    SVD of the design matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 4.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.1** The answer is \(\mathrm{rk}(A) = 2\). To see this, observe that
    the third column is the sum of the first two columns, so the column space is spanned
    by the first two columns. These two columns are linearly independent, so the dimension
    of the column space (i.e., the rank) is 2.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.3** The eigenvalues are \(\lambda_1 = 4\) and \(\lambda_2 = 2\). For
    \(\lambda_1 = 4\), solving \((A - 4I)\mathbf{x} = \mathbf{0}\) gives the eigenvector
    \(\mathbf{v}_1 = (1, 1)\). For \(\lambda_2 = 2\), solving \((A - 2I)\mathbf{x}
    = \mathbf{0}\) gives the eigenvector \(\mathbf{v}_2 = (1, -1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.5** Normalizing the eigenvectors to get an orthonormal basis: \(\mathbf{q}_1
    = \frac{1}{\sqrt{2}}(1, 1)\) and \(\mathbf{q}_2 = \frac{1}{\sqrt{2}}(1, -1)\).
    Then \(A = Q \Lambda Q^T\) where \(Q = (\mathbf{q}_1, \mathbf{q}_2)\) and \(\Lambda
    = \mathrm{diag}(4, 2)\). Explicitly,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.7**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.9** The rank of \(A\) is 1\. The second column is a multiple of the
    first column, so the column space is one-dimensional.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.11** The characteristic polynomial of \(A\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the eigenvalues are \(\lambda_1 = 1\) and \(\lambda_2 = 3\). For \(\lambda_1
    = 1\), we solve \((A - I)\mathbf{v} = \mathbf{0}\) to get \(\mathbf{v}_1 = \begin{pmatrix}
    1 \\ -1 \end{pmatrix}\). For \(\lambda_2 = 3\), we solve \((A - 3I)\mathbf{v}
    = 0\) to get \(\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.13** The columns of \(A\) are linearly dependent, since the second column
    is twice the first column. Hence, a basis for the column space of \(A\) is given
    by \(\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix} \right\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.15** The eigenvalues of \(A\) are \(\lambda_1 = 3\) and \(\lambda_2
    = -1\). Since \(A\) has a negative eigenvalue, it is not positive semidefinite.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.17** The Hessian of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues of the Hessian are \(\lambda_1 = 2\) and \(\lambda_2 = -2\).
    Since one eigenvalue is negative, the Hessian is not positive semidefinite, and
    \(f\) is not convex.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.2.19** The Hessian of \(f\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues of the Hessian are \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) and
    \(\lambda_2 = 0\), which are both nonnegative for all \(x, y\). Therefore, the
    Hessian is positive semidefinite, and \(f\) is convex.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.1** We have \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\). By
    the method described in the text, \(w_1\) is a unit eigenvector of \(A^TA = \begin{pmatrix}
    5 & 0 \\ 0 & 5 \end{pmatrix}\) corresponding to the largest eigenvalue. Thus,
    we can take \(\mathbf{w}_1 = (1, 0)\) or \(\mathbf{w}_1 = (0, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.3** We can take \(U = I_2\), \(\Sigma = A\), and \(V = I_2\). This is
    an SVD of \(A\) because \(U\) and \(V\) are orthogonal and \(\Sigma\) is diagonal.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.5** We have \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\).
    The characteristic polynomial of \(A^TA\) is \(\lambda^2 - 25\lambda = \lambda(\lambda
    - 25)\), so the eigenvalues are \(\lambda_1 = 25\) and \(\lambda_2 = 0\). An eigenvector
    corresponding to \(\lambda_1\) is \((1, 2)\), and an eigenvector corresponding
    to \(\lambda_2\) is \((-2, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.7** We have that the singular values of \(A\) are \(\sigma_1 = \sqrt{25}
    = 5\) and \(\sigma_2 = 0\). We can take \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1 = (1/\sqrt{5}, 2/\sqrt{5})\). Since
    the rank of \(A\) is 1, this gives a compact SVD of \(A\): \(A = U \Sigma V^T\)
    with \(U = \mathbf{u}_1\), \(\Sigma = (5)\), and \(V = \mathbf{v}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.9** From the full SVD of \(A\), we have that: An orthonormal basis for
    \(\mathrm{col}(A)\) is given by the first column of \(U\): \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\).
    An orthonormal basis for \(\mathrm{row}(A)\) is given by the first column of \(V\):
    \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\). An orthonormal basis for \(\mathrm{null}(A)\)
    is given by the second column of \(V\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\). An
    orthonormal basis for \(\mathrm{null}(A^T)\) is given by the second column of
    \(U\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.11** From its diagonal form, we see that \(\lambda_1 = 15\), \(\mathbf{q}_1
    = (1, 0)\); \(\lambda_2 = 7\), \(\mathbf{q}_2 = (0, 1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.13** By direct computation, \(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix}
    1 & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.15** By direct computation,'
  prefs: []
  type: TYPE_NORMAL
- en: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
  prefs: []
  type: TYPE_NORMAL
- en: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.3.17** The best approximating subspace of dimension \(k=1\) is the line
    spanned by the vector \(\mathbf{v}_1 = (1, 0)\). The sum of squared distances
    to this subspace is \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 =
    5 + 5 + 2 + 10 - 15 = 7.\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal entries are being raised to increasing powers, while the off-diagonal
    entries remain zero.
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.7** We have \(A = Q \Lambda Q^T\), where \(Q\) is the matrix of normalized
    eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues. Then,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly, \(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.4.9** We have \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\).
    Let’s find the eigenvalues of the matrix \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 &
    5 \end{pmatrix}\). We solve the characteristic equation \(\det(A^TA - \lambda
    I) = 0\). First, let’s calculate the characteristic polynomial: \(\det(A^TA -
    \lambda I) = \det(\begin{pmatrix} 1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})=
    \lambda^2 - 6\lambda + 1\), so \((\lambda - 3)^2 - 8 = 0\) or \((\lambda - 3)^2
    = 8\) or \(\lambda = 3 \pm 2\sqrt{2}\). Therefore, the eigenvalues of \(A^TA\)
    are: \(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) and \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\). Hence, this matrix is positive semidefinite because its eigenvalues
    are 0 and 6, both of which are non-negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). The unit
    norm constraint requires that \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\).
    The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\). Uncorrelatedness requires that \(\frac{1}{n-1}\sum_{i=1}^n
    t_{i1}t_{i2} = 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.7** First, compute the mean of each column:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean-centered data matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\), \(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\). The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.5.11** The second principal component must be uncorrelated with the first,
    so its loading vector must be orthogonal to \(\varphi_1\). One such vector is
    \(\varphi_2 = (-0.6, 0.8)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.1** The Frobenius norm is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.7** The induced 2-norm of the difference between a matrix and its rank-1
    truncated SVD is equal to the second singular value. Therefore, using the SVD
    from before, we have \(\|A - A_1\|_2 = \sigma_2 = 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E4.6.9** The matrix \(A\) is singular (its determinant is zero), so it doesn’t
    have an inverse. Therefore, \(\|A^{-1}\|_2\) is undefined. Note that the induced
    2-norm of the pseudoinverse \(A^+\) is not the same as the induced 2-norm of the
    inverse (when it exists).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define the column space, row space, and rank of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the row rank equals the column rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply properties of matrix rank, such as \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\)
    and \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and prove the Rank-Nullity Theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define eigenvalues and eigenvectors of a square matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that a symmetric matrix has at most d distinct eigenvalues, where d is
    the matrix size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Spectral Theorem for symmetric matrices and explain its implications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write the spectral decomposition of a symmetric matrix using outer products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine whether a symmetric matrix is positive semidefinite or positive definite
    based on its eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute eigenvalues and eigenvectors of symmetric matrices using programming
    tools like NumPy’s `linalg.eig`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the objective of the best approximating subspace problem and formulate
    it mathematically as a minimization of the sum of squared distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the best approximating subspace problem can be solved greedily by
    finding the best one-dimensional subspace, then the best one-dimensional subspace
    orthogonal to the first, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the singular value decomposition (SVD) of a matrix and describe the properties
    of the matrices involved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the existence of the SVD for any real matrix using the Spectral Theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the connection between the SVD of a matrix \(A\) and the spectral decompositions
    of \(A^T A\) and \(A A^T\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the SVD of simple matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the SVD of the data matrix to find the best k-dimensional approximating
    subspace to a set of data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the singular values as capturing the contributions of the right singular
    vectors to the fit of the approximating subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain low-dimensional representations of data points using the truncated SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinguish between full and compact forms of the SVD and convert between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the key lemma for power iteration in the positive semidefinite case and
    explain why it implies convergence to the top eigenvector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend the power iteration lemma to the general case of singular value decomposition
    (SVD) and justify why repeated multiplication of A^T A with a random vector converges
    to the top right singular vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the corresponding top singular value and left singular vector given
    the converged top right singular vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the orthogonal iteration method for finding additional singular vectors
    beyond the top one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the power iteration method and orthogonal iteration to compute the SVD
    of a given matrix and use it to find the best low-dimensional subspace approximation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the power iteration method and orthogonal iteration in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define principal components and loadings in the context of PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express the objective of PCA as a constrained optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish the connection between PCA and singular value decomposition (SVD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement PCA using an SVD algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the results of PCA in the context of dimensionality reduction and
    data visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Frobenius norm and the induced 2-norm of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express the Frobenius norm and the induced 2-norm of a matrix in terms of its
    singular values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Eckart-Young theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the pseudoinverse of a matrix using the SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the pseudoinverse to solve least squares problems when the matrix has
    full column rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the pseudoinverse to find the least norm solution for underdetermined
    systems with full row rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the ridge regression problem as a regularized optimization problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how ridge regression works by analyzing its solution in terms of the
    SVD of the design matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.8.2.1\. Computing more singular vectors[#](#computing-more-singular-vectors
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have shown how to compute the first singular vector. How do we compute more
    singular vectors? One approach is to first compute \(\mathbf{v}_1\) (or \(-\mathbf{v}_1\)),
    then find a vector \(\mathbf{y}\) orthogonal to it, and proceed as above. And
    then we repeat until we have all \(m\) right singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are often interested only in the top, say \(\ell < m\), singular vectors.
    An alternative approach in that case is to start with \(\ell\) random vectors
    and, first, find an orthonormal basis for the space they span. Then to quote [BHK,
    Section 3.7.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Then compute \(B\) times each of the basis vectors, and find an orthonormal
    basis for the space spanned by the resulting vectors. Intuitively, one has applied
    \(B\) to a subspace rather than a single vector. One repeatedly applies \(B\)
    to the subspace, calculating an orthonormal basis after each application to prevent
    the subspace collapsing to the one dimensional subspace spanned by the first singular
    vector.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will not prove here that this approach, known as orthogonal iteration, works.
    The proof is similar to that of the *Power Iteration Lemma*.
  prefs: []
  type: TYPE_NORMAL
- en: We implement this last algorithm. We will need our previous implementation of
    *Gram-Schimdt*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Note that above we avoided forming the matrix \(A^T A\). With a small number
    of iterations, that approach potentially requires fewer arithmetic operations
    overall and it allows to take advantage of the possible sparsity of \(A\) (i.e.
    the fact that it may have many zeros).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We apply it again to our two-cluster example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try again, but after projecting on the top two singular vectors. Recall
    that this corresponds to finding the best two-dimensional approximating subspace.
    The projection can be computed using the truncated SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\). We can interpret the rows of \(U_{(2)} \Sigma_{(2)}\) as the coefficients
    of each data point in the basis \(\mathbf{v}_1,\mathbf{v}_2\). We will work in
    that basis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, looking at the first two right singular vectors, we see that the first
    one does align quite well with the first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2.2\. Pseudoinverse[#](#pseudoinverse "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SVD leads to a natural generalization of the matrix inverse. First an observation.
    Recall that, to take the product of two square diagonal matrices, we simply multiply
    the corresponding diagonal entries. Let \(\Sigma \in \mathbb{R}^{r \times r}\)
    be a square diagonal matrix with diagonal entries \(\sigma_1,\ldots,\sigma_r\).
    If all diagonal entries are non-zero, then the matrix is invertible (since its
    columns then form a basis of the full space). The inverse of \(\Sigma\) in that
    case is simply the diagonal matrix \(\Sigma^{-1}\) with diagonal entries \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\).
    This can be confirmed by checking the definition of the inverse
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We are ready for our main definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Pseudoinverse)** Let \(A \in \mathbb{R}^{n \times m}\) be
    a matrix with compact SVD \(A = U \Sigma V^T\) and singular values \(\sigma_1
    \geq \cdots \geq \sigma_r > 0\). A pseudoinverse \(A^+ \in \mathbb{R}^{m \times
    n}\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = V \Sigma^{-1} U^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: While it is not obvious from the definition (why?), the pseudoinverse is in
    fact unique. To see that it is indeed a generalization of an inverse, we make
    a series of observations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 1:* Note that, using that \(U\) has orthonormal columns,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: which in general in not the identity matrix. Indeed it corresponds instead to
    the projection matrix onto the column space of \(A\), since the columns of \(U\)
    form an orthonormal basis of that linear subspace. As a result
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(A A^+\) is not the identity matrix, but it does map the columns of \(A\)
    to themselves. Put differently, it is the identity map “when restricted to \(\mathrm{col}(A)\)”.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
  prefs: []
  type: TYPE_NORMAL
- en: is the projection matrix onto the row space of \(A\), and
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A^+ A) A^+ = A^+. \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 2:* If \(A\) has full column rank \(m \leq n\), then \(r = m\).
    In that case, the columns of \(V\) form an orthonormal basis of all of \(\mathbb{R}^m\),
    i.e., \(V\) is orthogonal. Hence,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ A = V V^T = I_{m \times m}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if \(A\) has full row rank \(n \leq m\), then \(A A^+ = I_{n \times
    n}\).
  prefs: []
  type: TYPE_NORMAL
- en: If both cases hold, then \(n = m\), i.e., \(A\) is square, and \(\mathrm{rk}(A)
    = n\), i.e., \(A\) is invertible. We then get
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^+ = A^+ A = I_{n\times n}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That implies that \(A^+ = A^{-1}\) by the *Existence of an Inverse Lemma* (which
    includes uniqueness of the matrix inverse).
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 3:* Recall that, when \(A\) is nonsingular, the system \(A \mathbf{x}
    = \mathbf{b}\) admits the unique solution \(\mathbf{x} = A^{-1} \mathbf{b}\).
    In the overdetermined case, the pseudoinverse provides a solution to the linear
    least squares problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Least Squares)** Let \(A \in \mathbb{R}^{n \times
    m}\) with \(m \leq n\). A solution to the linear least squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  prefs: []
  type: TYPE_NORMAL
- en: is given by \(\mathbf{x}^* = A^+ \mathbf{b}\). Further, if \(A\) has full column
    rank \(m\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = (A^T A)^{-1} A^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* For the first part, we use that the solution to the least squares
    problem is the orthogonal projection. For the second part, we use the SVD definition
    and check that the two sides are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(A = U \Sigma V^T\) be a compact SVD of \(A\). For the first
    claim, note that the choice of \(\mathbf{x}^*\) in the statement gives'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(A \mathbf{x}^*\) is the orthogonal projection of \(\mathbf{b}\) onto
    the column space of \(A\) - which we proved previously is the solution to the
    linear least squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: Now onto the second claim. Recall that, when \(A\) is of full rank, the matrix
    \(A^T A\) is nonsingular. We then note that, using the notation \(\Sigma^{-2}
    = (\Sigma^{-1})^2\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. Here, we used that \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudoinverse also provides a solution in the case of an underdetermined
    system. Here, however, there are in general infinitely many solutions. The one
    chosen by the pseudoinverse has a special property as we see now: it is the least
    norm solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Underdetermined Systems)** Let \(A \in \mathbb{R}^{n
    \times m}\) with \(m > n\) and \(\mathbf{b} \in \mathbb{R}^n\). Further assume
    that \(A\) has full row rank \(n\). Then \(\mathbf{x}^* = A^+ \mathbf{b}\) is
    a solution to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in that case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = A^T (A A^T)^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We first prove the formula for \(A^+\). As we did in the overdetermined
    case, it can be checked by substituting a compact SVD \(A = U \Sigma V^T\). Recall
    that, when \(A^T\) is of full rank, the matrix \(A A^T\) is nonsingular. We then
    note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. Here, we used that \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Because \(A\) has full row rank, \(\mathbf{b} \in \mathrm{col}(A)\) and there
    is at least one \(\mathbf{x}\) such that \(A \mathbf{x} = \mathbf{b}\). One such
    solution is provided by the pseudoinverse. Indeed, from a previous observation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the columns of \(U\) form an orthonormal basis of \(\mathbb{R}^n\)
    by the rank assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{x}\) be any other solution to the system. Then \(A(\mathbf{x}
    - \mathbf{x}^*) = \mathbf{b} - \mathbf{b} = \mathbf{0}\). That implies
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, \(\mathbf{x} - \mathbf{x}^*\) and \(\mathbf{x}^*\) are orthogonal.
    By *Pythagoras*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves that \(\mathbf{x}^*\) has the smallest norm among all solutions
    to the system \(A \mathbf{x} = \mathbf{b}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Continuing a previous example, let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We compute the pseudoinverse. By the formula, in the rank one case, it is simply
    (Check this!)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be a square nonsingular
    matrix. Let \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be a compact
    SVD of \(A\), where we used the fact that the rank of \(A\) is \(n\) so it has
    \(n\) strictly positive singular values. We seek to compute \(\|A^{-1}\|_2\) in
    terms of the singular values.'
  prefs: []
  type: TYPE_NORMAL
- en: Because \(A\) is invertible, \(A^+ = A^{-1}\). So we compute the pseudoinverse
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: The sum on the right-hand side is not quite a compact SVD of \(A^{-1}\) because
    the coefficients \(\sigma_j^{-1}\) are non-decreasing in \(j\).
  prefs: []
  type: TYPE_NORMAL
- en: But writing the sum in reverse order
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
  prefs: []
  type: TYPE_NORMAL
- en: does give a compact SVD of \(A^{-1}\), since \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1}
    > 0\) and \(\{\mathbf{v}_j\}_{j=1}^n\) and \(\{\mathbf{u}_j\}_{j=1}^n\) are orthonormal
    lists. Hence, the \(2\)-norm is given by the largest singular value, that is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the pseudoinverse of a matrix can be computed
    using the function [`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try our previous example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2.3\. Condition numbers[#](#condition-numbers "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we introduce condition numbers, a measure of perturbation sensitivity
    for numerical problems. We look in particular at the conditioning of the least-squares
    problem. We begin with the concept of pseudoinverse, which is important in its
    own right.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditioning of matrix-vector multiplication** We define the condition number
    of a matrix and show that it captures some information about the sensitivity to
    perturbations of matrix-vector multiplications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Condition number of a matrix)** The condition number (in
    the induced \(2\)-norm) of a square, nonsingular matrix \(A \in \mathbb{R}^{n
    \times n}\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this can be computed as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the example above. In words, \(\kappa_2(A)\) is the ratio of the
    largest to the smallest stretching under \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Conditioning of Matrix-Vector Multiplication)** Let \(M \in
    \mathbb{R}^{n \times n}\) be nonsingular. Then, for any \(\mathbf{z} \in \mathbb{R}^n\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
  prefs: []
  type: TYPE_NORMAL
- en: and the inequality is tight in the sense that there is an \(\mathbf{x}\) and
    a \(\mathbf{d}\) that achieves it.
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: The ratio above measures the worst rate of relative change in \(M \mathbf{z}\)
    under infinitesimal perturbations of \(\mathbf{z}\). The theorem says that when
    \(\kappa_2(M)\) is large, a case referred to as ill-conditioning, large relative
    changes in \(M \mathbf{z}\) can be obtained from relatively small perturbations
    to \(\mathbf{z}\). In words, a matrix-vector product is potentially sensitive
    to perturbations when the matrix is ill-conditioned.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Write'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\),
    which was shown in a previous example.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we see that the ratio can achieve its maximum by taking \(\mathbf{d}\)
    and \(\mathbf{z}\) to be the right singular vectors corresponding to \(\sigma_1\)
    and \(\sigma_n\) respectively. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: If we apply the theorem to the inverse instead, we get that the relative conditioning
    of the nonsingular linear system \(A \mathbf{x} = \mathbf{b}\) to perturbations
    in \(\mathbf{b}\) is \(\kappa_2(A)\). The latter can be large in particular when
    the columns of \(A\) are close to linearly dependent. This is detailed in the
    next example.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be nonsingular. Then, for
    any \(\mathbf{b} \in \mathbb{R}^n\), there exists a unique solution to \(A \mathbf{x}
    = \mathbf{b}\), namely,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we solve the perturbed system
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
  prefs: []
  type: TYPE_NORMAL
- en: for some vector \(\delta\mathbf{b}\). We use the *Conditioning of Matrix-Vector
    Multiplication Theorem* to bound the norm of \(\delta\mathbf{x}\).
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, set
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So we get that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
  prefs: []
  type: TYPE_NORMAL
- en: Note also that, because \((A^{-1})^{-1} = A\), we have \(\kappa_2(A^{-1}) =
    \kappa_2(A)\). Rearranging, we finally get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the larger the condition number is, the larger the potential relative
    effect on the solution of the linear system is for a given relative perturbation
    size. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the condition number of a matrix can be computed
    using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, orthogonal matrices have condition number \(1\), the lowest possible
    value for it (Why?). That indicates that orthogonal matrices have good numerical
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: In contrast, matrices with nearly linearly dependent columns have large condition
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the SVD of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: We compute the solution to \(A \mathbf{x} = \mathbf{b}\) when \(\mathbf{b}\)
    is the left singular vector of \(A\) corresponding to the largest singular value.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case bound is achieved when \(\mathbf{z} =
    \mathbf{b}\) is right singular vector of \(M= A^{-1}\) corresponding to the lowest
    singular value. In a previous example, given a matrix \(A = \sum_{j=1}^n \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) in compact SVD form, we derived a compact SVD for
    the inverse as
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, compared to the SVD of \(A\), the order of the singular values is reversed
    and the roles of the left and right singular vectors are exchanged. So we take
    \(\mathbf{b}\) to be the top left singular vector of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: We make a small perturbation in the direction of the second right singular vector.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case is achieved when \(\mathbf{d} = \delta\mathbf{b}\)
    is top right singular vector of \(M = A^{-1}\). By the argument above, that is
    the left singular vector of \(A\) corresponding to the lowest singular value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The relative change in solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is exactly the condition number of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to the least-squares problem** We return to the least-squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We showed that the solution satisfies the normal equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(A\) may not be square and invertible. We define a more general notion
    of condition number.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Condition number of a matrix: general case)** The condition
    number (in the induced \(2\)-norm) of a matrix \(A \in \mathbb{R}^{n \times m}\)
    is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: As we show next, the condition number of \(A^T A\) can be much larger than that
    of \(A\) itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Condition number of \(A^T A\))** Let \(A \in \mathbb{R}^{n \times
    m}\) have full column rank. The'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the SVD.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(A = U \Sigma V^T\) be an SVD of \(A\) with singular values \(\sigma_1
    \geq \cdots \geq \sigma_m > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular the latter expression is an SVD of \(A^T A\), and hence the condition
    number of \(A^T A\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We give a quick example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'This observation – and the resulting increased numerical instability – is one
    of the reasons we previously developed an alternative approach to the least-squares
    problem. Quoting [Sol, Section 5.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, a primary reason that \(\mathrm{cond}(A^T A)\) can be large is
    that columns of \(A\) might look “similar” […] If two columns \(\mathbf{a}_i\)
    and \(\mathbf{a}_j\) satisfy \(\mathbf{a}_i \approx \mathbf{a}_j\), then the least-squares
    residual length \(\|\mathbf{b} - A \mathbf{x}\|_2\) will not suffer much if we
    replace multiples of \(\mathbf{a}_i\) with multiples of \(\mathbf{a}_j\) or vice
    versa. This wide range of nearly—but not completely—equivalent solutions yields
    poor conditioning. […] To solve such poorly conditioned problems, we will employ
    an alternative technique with closer attention to the column space of \(A\) rather
    than employing row operations as in Gaussian elimination. This strategy identifies
    and deals with such near-dependencies explicitly, bringing about greater numerical
    stability.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We quote without proof a theorem from [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Theorem 4.2.7] which sheds further light on this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Accuracy of Least-squares Solutions)** Let \(\mathbf{x}^*\)
    be the solution of the least-squares problem \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\). Let \(\mathbf{x}_{\mathrm{NE}}\) be the solution
    obtained by forming and solving the normal equations in [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    with rounding unit \(\epsilon_M\). Then \(\mathbf{x}_{\mathrm{NE}}\) satisfies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{x}_{\mathrm{QR}}\) be the solution obtained from a QR factorization
    in the same arithmetic. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) is the residual vector.
    The constants \(\gamma\) are slowly growing functions of the dimensions of the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain, let’s quote [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Section 4.2.3] again:'
  prefs: []
  type: TYPE_NORMAL
- en: The perturbation theory for the normal equations shows that \(\kappa_2^2(A)\)
    controls the size of the errors we can expect. The bound for the solution computed
    from the QR equation also has a term multiplied by \(\kappa_2^2(A)\), but this
    term is also multiplied by the scaled residual, which can diminish its effect.
    However, in many applications the vector \(\mathbf{b}\) is contaminated with error,
    and the residual can, in general, be no smaller than the size of that error.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Here is a numerical example taken from [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    Lecture 19]. We will approximate the following function with a polynomial.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We use a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix),
    which can be constructed using [`numpy.vander`](https://numpy.org/doc/stable/reference/generated/numpy.vander.html),
    to perform polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: The condition numbers of \(A\) and \(A^T A\) are both high in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: We first use the normal equations and plot the residual vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We then use `numpy.linalg.qr` to compute the QR solution instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2.1\. Computing more singular vectors[#](#computing-more-singular-vectors
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have shown how to compute the first singular vector. How do we compute more
    singular vectors? One approach is to first compute \(\mathbf{v}_1\) (or \(-\mathbf{v}_1\)),
    then find a vector \(\mathbf{y}\) orthogonal to it, and proceed as above. And
    then we repeat until we have all \(m\) right singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are often interested only in the top, say \(\ell < m\), singular vectors.
    An alternative approach in that case is to start with \(\ell\) random vectors
    and, first, find an orthonormal basis for the space they span. Then to quote [BHK,
    Section 3.7.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Then compute \(B\) times each of the basis vectors, and find an orthonormal
    basis for the space spanned by the resulting vectors. Intuitively, one has applied
    \(B\) to a subspace rather than a single vector. One repeatedly applies \(B\)
    to the subspace, calculating an orthonormal basis after each application to prevent
    the subspace collapsing to the one dimensional subspace spanned by the first singular
    vector.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will not prove here that this approach, known as orthogonal iteration, works.
    The proof is similar to that of the *Power Iteration Lemma*.
  prefs: []
  type: TYPE_NORMAL
- en: We implement this last algorithm. We will need our previous implementation of
    *Gram-Schimdt*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Note that above we avoided forming the matrix \(A^T A\). With a small number
    of iterations, that approach potentially requires fewer arithmetic operations
    overall and it allows to take advantage of the possible sparsity of \(A\) (i.e.
    the fact that it may have many zeros).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We apply it again to our two-cluster example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try again, but after projecting on the top two singular vectors. Recall
    that this corresponds to finding the best two-dimensional approximating subspace.
    The projection can be computed using the truncated SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\). We can interpret the rows of \(U_{(2)} \Sigma_{(2)}\) as the coefficients
    of each data point in the basis \(\mathbf{v}_1,\mathbf{v}_2\). We will work in
    that basis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, looking at the first two right singular vectors, we see that the first
    one does align quite well with the first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2.2\. Pseudoinverse[#](#pseudoinverse "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SVD leads to a natural generalization of the matrix inverse. First an observation.
    Recall that, to take the product of two square diagonal matrices, we simply multiply
    the corresponding diagonal entries. Let \(\Sigma \in \mathbb{R}^{r \times r}\)
    be a square diagonal matrix with diagonal entries \(\sigma_1,\ldots,\sigma_r\).
    If all diagonal entries are non-zero, then the matrix is invertible (since its
    columns then form a basis of the full space). The inverse of \(\Sigma\) in that
    case is simply the diagonal matrix \(\Sigma^{-1}\) with diagonal entries \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\).
    This can be confirmed by checking the definition of the inverse
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We are ready for our main definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Pseudoinverse)** Let \(A \in \mathbb{R}^{n \times m}\) be
    a matrix with compact SVD \(A = U \Sigma V^T\) and singular values \(\sigma_1
    \geq \cdots \geq \sigma_r > 0\). A pseudoinverse \(A^+ \in \mathbb{R}^{m \times
    n}\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = V \Sigma^{-1} U^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: While it is not obvious from the definition (why?), the pseudoinverse is in
    fact unique. To see that it is indeed a generalization of an inverse, we make
    a series of observations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 1:* Note that, using that \(U\) has orthonormal columns,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: which in general in not the identity matrix. Indeed it corresponds instead to
    the projection matrix onto the column space of \(A\), since the columns of \(U\)
    form an orthonormal basis of that linear subspace. As a result
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(A A^+\) is not the identity matrix, but it does map the columns of \(A\)
    to themselves. Put differently, it is the identity map “when restricted to \(\mathrm{col}(A)\)”.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
  prefs: []
  type: TYPE_NORMAL
- en: is the projection matrix onto the row space of \(A\), and
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A^+ A) A^+ = A^+. \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 2:* If \(A\) has full column rank \(m \leq n\), then \(r = m\).
    In that case, the columns of \(V\) form an orthonormal basis of all of \(\mathbb{R}^m\),
    i.e., \(V\) is orthogonal. Hence,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ A = V V^T = I_{m \times m}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if \(A\) has full row rank \(n \leq m\), then \(A A^+ = I_{n \times
    n}\).
  prefs: []
  type: TYPE_NORMAL
- en: If both cases hold, then \(n = m\), i.e., \(A\) is square, and \(\mathrm{rk}(A)
    = n\), i.e., \(A\) is invertible. We then get
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^+ = A^+ A = I_{n\times n}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That implies that \(A^+ = A^{-1}\) by the *Existence of an Inverse Lemma* (which
    includes uniqueness of the matrix inverse).
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 3:* Recall that, when \(A\) is nonsingular, the system \(A \mathbf{x}
    = \mathbf{b}\) admits the unique solution \(\mathbf{x} = A^{-1} \mathbf{b}\).
    In the overdetermined case, the pseudoinverse provides a solution to the linear
    least squares problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Least Squares)** Let \(A \in \mathbb{R}^{n \times
    m}\) with \(m \leq n\). A solution to the linear least squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  prefs: []
  type: TYPE_NORMAL
- en: is given by \(\mathbf{x}^* = A^+ \mathbf{b}\). Further, if \(A\) has full column
    rank \(m\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = (A^T A)^{-1} A^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* For the first part, we use that the solution to the least squares
    problem is the orthogonal projection. For the second part, we use the SVD definition
    and check that the two sides are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(A = U \Sigma V^T\) be a compact SVD of \(A\). For the first
    claim, note that the choice of \(\mathbf{x}^*\) in the statement gives'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(A \mathbf{x}^*\) is the orthogonal projection of \(\mathbf{b}\) onto
    the column space of \(A\) - which we proved previously is the solution to the
    linear least squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: Now onto the second claim. Recall that, when \(A\) is of full rank, the matrix
    \(A^T A\) is nonsingular. We then note that, using the notation \(\Sigma^{-2}
    = (\Sigma^{-1})^2\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. Here, we used that \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudoinverse also provides a solution in the case of an underdetermined
    system. Here, however, there are in general infinitely many solutions. The one
    chosen by the pseudoinverse has a special property as we see now: it is the least
    norm solution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Underdetermined Systems)** Let \(A \in \mathbb{R}^{n
    \times m}\) with \(m > n\) and \(\mathbf{b} \in \mathbb{R}^n\). Further assume
    that \(A\) has full row rank \(n\). Then \(\mathbf{x}^* = A^+ \mathbf{b}\) is
    a solution to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in that case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = A^T (A A^T)^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We first prove the formula for \(A^+\). As we did in the overdetermined
    case, it can be checked by substituting a compact SVD \(A = U \Sigma V^T\). Recall
    that, when \(A^T\) is of full rank, the matrix \(A A^T\) is nonsingular. We then
    note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. Here, we used that \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Because \(A\) has full row rank, \(\mathbf{b} \in \mathrm{col}(A)\) and there
    is at least one \(\mathbf{x}\) such that \(A \mathbf{x} = \mathbf{b}\). One such
    solution is provided by the pseudoinverse. Indeed, from a previous observation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that the columns of \(U\) form an orthonormal basis of \(\mathbb{R}^n\)
    by the rank assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{x}\) be any other solution to the system. Then \(A(\mathbf{x}
    - \mathbf{x}^*) = \mathbf{b} - \mathbf{b} = \mathbf{0}\). That implies
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, \(\mathbf{x} - \mathbf{x}^*\) and \(\mathbf{x}^*\) are orthogonal.
    By *Pythagoras*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves that \(\mathbf{x}^*\) has the smallest norm among all solutions
    to the system \(A \mathbf{x} = \mathbf{b}\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Continuing a previous example, let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We compute the pseudoinverse. By the formula, in the rank one case, it is simply
    (Check this!)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be a square nonsingular
    matrix. Let \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be a compact
    SVD of \(A\), where we used the fact that the rank of \(A\) is \(n\) so it has
    \(n\) strictly positive singular values. We seek to compute \(\|A^{-1}\|_2\) in
    terms of the singular values.'
  prefs: []
  type: TYPE_NORMAL
- en: Because \(A\) is invertible, \(A^+ = A^{-1}\). So we compute the pseudoinverse
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: The sum on the right-hand side is not quite a compact SVD of \(A^{-1}\) because
    the coefficients \(\sigma_j^{-1}\) are non-decreasing in \(j\).
  prefs: []
  type: TYPE_NORMAL
- en: But writing the sum in reverse order
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
  prefs: []
  type: TYPE_NORMAL
- en: does give a compact SVD of \(A^{-1}\), since \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1}
    > 0\) and \(\{\mathbf{v}_j\}_{j=1}^n\) and \(\{\mathbf{u}_j\}_{j=1}^n\) are orthonormal
    lists. Hence, the \(2\)-norm is given by the largest singular value, that is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the pseudoinverse of a matrix can be computed
    using the function [`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try our previous example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2.3\. Condition numbers[#](#condition-numbers "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we introduce condition numbers, a measure of perturbation sensitivity
    for numerical problems. We look in particular at the conditioning of the least-squares
    problem. We begin with the concept of pseudoinverse, which is important in its
    own right.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditioning of matrix-vector multiplication** We define the condition number
    of a matrix and show that it captures some information about the sensitivity to
    perturbations of matrix-vector multiplications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Condition number of a matrix)** The condition number (in
    the induced \(2\)-norm) of a square, nonsingular matrix \(A \in \mathbb{R}^{n
    \times n}\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this can be computed as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the example above. In words, \(\kappa_2(A)\) is the ratio of the
    largest to the smallest stretching under \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Conditioning of Matrix-Vector Multiplication)** Let \(M \in
    \mathbb{R}^{n \times n}\) be nonsingular. Then, for any \(\mathbf{z} \in \mathbb{R}^n\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
  prefs: []
  type: TYPE_NORMAL
- en: and the inequality is tight in the sense that there is an \(\mathbf{x}\) and
    a \(\mathbf{d}\) that achieves it.
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: The ratio above measures the worst rate of relative change in \(M \mathbf{z}\)
    under infinitesimal perturbations of \(\mathbf{z}\). The theorem says that when
    \(\kappa_2(M)\) is large, a case referred to as ill-conditioning, large relative
    changes in \(M \mathbf{z}\) can be obtained from relatively small perturbations
    to \(\mathbf{z}\). In words, a matrix-vector product is potentially sensitive
    to perturbations when the matrix is ill-conditioned.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Write'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\),
    which was shown in a previous example.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we see that the ratio can achieve its maximum by taking \(\mathbf{d}\)
    and \(\mathbf{z}\) to be the right singular vectors corresponding to \(\sigma_1\)
    and \(\sigma_n\) respectively. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: If we apply the theorem to the inverse instead, we get that the relative conditioning
    of the nonsingular linear system \(A \mathbf{x} = \mathbf{b}\) to perturbations
    in \(\mathbf{b}\) is \(\kappa_2(A)\). The latter can be large in particular when
    the columns of \(A\) are close to linearly dependent. This is detailed in the
    next example.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be nonsingular. Then, for
    any \(\mathbf{b} \in \mathbb{R}^n\), there exists a unique solution to \(A \mathbf{x}
    = \mathbf{b}\), namely,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we solve the perturbed system
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
  prefs: []
  type: TYPE_NORMAL
- en: for some vector \(\delta\mathbf{b}\). We use the *Conditioning of Matrix-Vector
    Multiplication Theorem* to bound the norm of \(\delta\mathbf{x}\).
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, set
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So we get that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
  prefs: []
  type: TYPE_NORMAL
- en: Note also that, because \((A^{-1})^{-1} = A\), we have \(\kappa_2(A^{-1}) =
    \kappa_2(A)\). Rearranging, we finally get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence the larger the condition number is, the larger the potential relative
    effect on the solution of the linear system is for a given relative perturbation
    size. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the condition number of a matrix can be computed
    using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, orthogonal matrices have condition number \(1\), the lowest possible
    value for it (Why?). That indicates that orthogonal matrices have good numerical
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: In contrast, matrices with nearly linearly dependent columns have large condition
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the SVD of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: We compute the solution to \(A \mathbf{x} = \mathbf{b}\) when \(\mathbf{b}\)
    is the left singular vector of \(A\) corresponding to the largest singular value.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case bound is achieved when \(\mathbf{z} =
    \mathbf{b}\) is right singular vector of \(M= A^{-1}\) corresponding to the lowest
    singular value. In a previous example, given a matrix \(A = \sum_{j=1}^n \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) in compact SVD form, we derived a compact SVD for
    the inverse as
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, compared to the SVD of \(A\), the order of the singular values is reversed
    and the roles of the left and right singular vectors are exchanged. So we take
    \(\mathbf{b}\) to be the top left singular vector of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: We make a small perturbation in the direction of the second right singular vector.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case is achieved when \(\mathbf{d} = \delta\mathbf{b}\)
    is top right singular vector of \(M = A^{-1}\). By the argument above, that is
    the left singular vector of \(A\) corresponding to the lowest singular value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'The relative change in solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is exactly the condition number of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to the least-squares problem** We return to the least-squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We showed that the solution satisfies the normal equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(A\) may not be square and invertible. We define a more general notion
    of condition number.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Condition number of a matrix: general case)** The condition
    number (in the induced \(2\)-norm) of a matrix \(A \in \mathbb{R}^{n \times m}\)
    is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: As we show next, the condition number of \(A^T A\) can be much larger than that
    of \(A\) itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Condition number of \(A^T A\))** Let \(A \in \mathbb{R}^{n \times
    m}\) have full column rank. The'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the SVD.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(A = U \Sigma V^T\) be an SVD of \(A\) with singular values \(\sigma_1
    \geq \cdots \geq \sigma_m > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: In particular the latter expression is an SVD of \(A^T A\), and hence the condition
    number of \(A^T A\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We give a quick example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'This observation – and the resulting increased numerical instability – is one
    of the reasons we previously developed an alternative approach to the least-squares
    problem. Quoting [Sol, Section 5.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, a primary reason that \(\mathrm{cond}(A^T A)\) can be large is
    that columns of \(A\) might look “similar” […] If two columns \(\mathbf{a}_i\)
    and \(\mathbf{a}_j\) satisfy \(\mathbf{a}_i \approx \mathbf{a}_j\), then the least-squares
    residual length \(\|\mathbf{b} - A \mathbf{x}\|_2\) will not suffer much if we
    replace multiples of \(\mathbf{a}_i\) with multiples of \(\mathbf{a}_j\) or vice
    versa. This wide range of nearly—but not completely—equivalent solutions yields
    poor conditioning. […] To solve such poorly conditioned problems, we will employ
    an alternative technique with closer attention to the column space of \(A\) rather
    than employing row operations as in Gaussian elimination. This strategy identifies
    and deals with such near-dependencies explicitly, bringing about greater numerical
    stability.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We quote without proof a theorem from [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Theorem 4.2.7] which sheds further light on this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Accuracy of Least-squares Solutions)** Let \(\mathbf{x}^*\)
    be the solution of the least-squares problem \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\). Let \(\mathbf{x}_{\mathrm{NE}}\) be the solution
    obtained by forming and solving the normal equations in [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    with rounding unit \(\epsilon_M\). Then \(\mathbf{x}_{\mathrm{NE}}\) satisfies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{x}_{\mathrm{QR}}\) be the solution obtained from a QR factorization
    in the same arithmetic. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) is the residual vector.
    The constants \(\gamma\) are slowly growing functions of the dimensions of the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain, let’s quote [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Section 4.2.3] again:'
  prefs: []
  type: TYPE_NORMAL
- en: The perturbation theory for the normal equations shows that \(\kappa_2^2(A)\)
    controls the size of the errors we can expect. The bound for the solution computed
    from the QR equation also has a term multiplied by \(\kappa_2^2(A)\), but this
    term is also multiplied by the scaled residual, which can diminish its effect.
    However, in many applications the vector \(\mathbf{b}\) is contaminated with error,
    and the residual can, in general, be no smaller than the size of that error.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Here is a numerical example taken from [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    Lecture 19]. We will approximate the following function with a polynomial.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We use a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix),
    which can be constructed using [`numpy.vander`](https://numpy.org/doc/stable/reference/generated/numpy.vander.html),
    to perform polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: The condition numbers of \(A\) and \(A^T A\) are both high in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: We first use the normal equations and plot the residual vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We then use `numpy.linalg.qr` to compute the QR solution instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Proof of Greedy Finds Best Subspace** In this section, we prove the full
    version of the *Greedy Finds Best Subspace Theorem*. In particular, we do not
    use the *Spectral Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We proceed by induction. For an arbitrary orthonormal list \(\mathbf{w}_1,\ldots,\mathbf{w}_k\),
    we find an orthonormal basis of their span containing an element orthogonal to
    \(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1}\). Then we use the defintion of \(\mathbf{v}_k\)
    to conclude.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We reformulate the problem as a maximization'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \max \left\{ \sum_{j=1}^k \|A \mathbf{w}_j\|^2\ :\ \text{$\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}$
    is an orthonormal list} \right\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we also replace the \(k\)-dimensional linear subspace \(\mathcal{Z}\)
    with an arbitrary orthonormal basis \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\).
  prefs: []
  type: TYPE_NORMAL
- en: We then proceed by induction. For \(k=1\), we define \(\mathbf{v}_1\) as a solution
    of the above maximization problem. Assume that, for any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_\ell\}\)
    with \(\ell < k\), we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^\ell \|A \mathbf{w}_j\|^2 \leq \sum_{j=1}^\ell \|A \mathbf{v}_j\|^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Now consider any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\) and
    let its span be \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1,\ldots,\mathbf{w}_k)\).
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 1:*** For \(j=1,\ldots,k-1\), let \(\mathbf{v}_j''\) be the orthogonal
    projection of \(\mathbf{v}_j\) onto \(\mathcal{W}\) and let \(\mathcal{V}'' =
    \mathrm{span}(\mathbf{v}''_1,\ldots,\mathbf{v}''_{k-1})\). Because \(\mathcal{V}''
    \subseteq \mathcal{W}\) has dimension at most \(k-1\) while \(\mathcal{W}\) itself
    has dimension \(k\), we can find an orthonormal basis \(\mathbf{w}''_1,\ldots,\mathbf{w}''_{k}\)
    of \(\mathcal{W}\) such that \(\mathbf{w}''_k\) is orthogonal to \(\mathcal{V}''\)
    (Why?). Then, for any \(j=1,\ldots,k-1\), we have the decomposition \(\mathbf{v}_j
    = \mathbf{v}''_j + (\mathbf{v}_j - \mathbf{v}''_j)\) where \(\mathbf{v}''_j \in
    \mathcal{V}''\) is orthogonal to \(\mathbf{w}''_k\) and \(\mathbf{v}_j - \mathbf{v}''_j\)
    is also orthogonal to \(\mathbf{w}''_k \in \mathcal{W}\) by the properties of
    the orthogonal projection onto \(\mathcal{W}\). Hence'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left\langle \sum_{j=1}^{k-1}\beta_j \mathbf{v}_j, \mathbf{w}'_k
    \right\rangle &= \left\langle \sum_{j=1}^{k-1}\beta_j [\mathbf{v}'_j + (\mathbf{v}_j
    - \mathbf{v}'_j)], \mathbf{w}'_k \right\rangle\\ &= \left\langle \sum_{j=1}^{k-1}\beta_j
    \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \left\langle \sum_{j=1}^{k-1}\beta_j
    (\mathbf{v}_j - \mathbf{v}'_j), \mathbf{w}'_k \right\rangle\\ &= \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}_j - \mathbf{v}'_j, \mathbf{w}'_k \right\rangle\\ &= 0
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\beta_j\)’s. That is, \(\mathbf{w}'_k\) is orthogonal to \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\).
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 2:*** By the induction hypothesis, we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*) \qquad \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 \leq \sum_{j=1}^{k-1} \|A
    \mathbf{v}_j\|^2\. \]
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, recalling that the \(\boldsymbol{\alpha}_i^T\)’s are the rows of \(A\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ (**) \qquad \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{i=1}^n \|\mathrm{proj}_{\mathcal{W}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{j=1}^k \|A \mathbf{w}_j'\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: since the \(\mathbf{w}_j\)’s and \(\mathbf{w}'_j\)’s form an orthonormal basis
    of the same subspace \(\mathcal{W}\). Since \(\mathbf{w}'_k\) is orthogonal to
    \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\) by the conclusion of Step
    1, by the definition of \(\mathbf{v}_k\) as a solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}_k\in \arg\max \{\|A \mathbf{v}\|:\|\mathbf{v}\| = 1, \ \langle
    \mathbf{v}, \mathbf{v}_j \rangle = 0, \forall j \leq k-1\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*\!*\!*) \qquad \|A \mathbf{w}'_k\|^2 \leq \|A \mathbf{v}_k\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 3:*** Putting everything together'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \sum_{j=1}^k \|A \mathbf{w}_j\|^2 &= \sum_{j=1}^k \|A \mathbf{w}_j'\|^2
    &\text{by $(**)$}\\ &= \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 + \|A \mathbf{w}'_k\|^2\\
    &\leq \sum_{j=1}^{k-1} \|A \mathbf{v}_j\|^2 + \|A \mathbf{v}_k\|^2 &\text{by $(*)$
    and $(*\!*\!*)$}\\ &= \sum_{j=1}^{k} \|A \mathbf{v}_j\|^2\\ \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: which proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof of Existence of the SVD** We return to the proof of the *Existence
    of SVD Theorem*. We give an alternative proof that does not rely on the *Spectral
    Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We have already done most of the work. The proof works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Compute the greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) from the
    *Greedy Finds Best Subspace Theorem* until the largest \(r\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{v}_r\|^2 > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: or, otherwise, until \(r=m\). The \(\mathbf{v}_j\)’s are orthonormal by construction.
  prefs: []
  type: TYPE_NORMAL
- en: (2) For \(j=1,\ldots,r\), let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_j = \|A \mathbf{v}_j\| \quad\text{and}\quad \mathbf{u}_j = \frac{1}{\sigma_j}
    A \mathbf{v}_j. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe that, by our choice of \(r\), the \(\sigma_j\)’s are \(> 0\). They
    are also non-increasing: by definition of the greedy sequence,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_i^2 = \max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \ \langle
    \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the set of orthogonality constraints gets larger as \(i\) increases. Hence,
    the \(\mathbf{u}_j\)’s have unit norm by definition.
  prefs: []
  type: TYPE_NORMAL
- en: We show below that they are also orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Let \(\mathbf{z} \in \mathbb{R}^m\) be any vector. To show that our construction
    is correct, we prove that \(A \mathbf{z} = \left(U \Sigma V^T\right)\mathbf{z}\).
    Let \(\mathcal{V} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\) and decompose
    \(\mathbf{z}\) into orthogonal components
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} = \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) + (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}))
    = \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle \,\mathbf{v}_j + (\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \]
  prefs: []
  type: TYPE_NORMAL
- en: Applying \(A\) and using linearity, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, A\mathbf{v}_j + A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We claim that \(A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})) = \mathbf{0}\).
    If \(\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) = \mathbf{0}\), that
    is certainly the case. If not, let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w} = \frac{\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})}{\|\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: By definition of \(r\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \max \{\|A \mathbf{w}_{r+1}\|^2 :\|\mathbf{w}_{r+1}\| = 1, \ \langle
    \mathbf{w}_{r+1}, \mathbf{v}_j \rangle = 0, \forall j \leq r\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, \(\|A \mathbf{w}_{r+1}\|^2 = 0\) (i.e. \(A \mathbf{w}_{r+1}
    = \mathbf{0}\)), for any unit vector \(\mathbf{w}_{r+1}\) orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_r\).
    That applies in particular to \(\mathbf{w}_{r+1} = \mathbf{w}\) by the *Orthogonal
    Projection Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, using the definition of \(\mathbf{u}_j\) and \(\sigma_j\), we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, \sigma_j \mathbf{u}_j\\ &= \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
    \mathbf{z}\\ &=\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{z}\\
    &= \left(U \Sigma V^T\right)\mathbf{z}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the existence of the SVD.
  prefs: []
  type: TYPE_NORMAL
- en: All that is left to prove is the orthogonality of the \(\mathbf{u}_j\)’s.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Left Singular Vectors are Orthogonal)** For all \(1 \leq i \neq
    j \leq r\), \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Quoting [BHK, Section 3.6]:'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively if \(\mathbf{u}_i\) and \(\mathbf{u}_j\), \(i < j\), were not orthogonal,
    one would suspect that the right singular vector \(\mathbf{v}_j\) had a component
    of \(\mathbf{v}_i\) which would contradict that \(\mathbf{v}_i\) and \(\mathbf{v}_j\)
    were orthogonal. Let \(i\) be the smallest integer such that \(\mathbf{u}_i\)
    is not orthogonal to all other \(\mathbf{u}_j\). Then to prove that \(\mathbf{u}_i\)
    and \(\mathbf{u}_j\) are orthogonal, we add a small component of \(\mathbf{v}_j\)
    to \(\mathbf{v}_i\), normalize the result to be a unit vector \(\mathbf{v}'_i
    \propto \mathbf{v}_i + \varepsilon \mathbf{v}_j\) and show that \(\|A \mathbf{v}'_i\|
    > \|A \mathbf{v}_i\|\), a contradiction.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Proof:* We argue by contradiction. Let \(i\) be the smallest index such that
    there is a \(j > i\) with \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = \delta
    \neq 0\). Assume \(\delta > 0\) (otherwise use \(-\mathbf{u}_i\)). For \(\varepsilon
    \in (0,1)\), because the \(\mathbf{v}_k\)’s are orthonormal, \(\|\mathbf{v}_i
    + \varepsilon \mathbf{v}_j\|^2 = 1+\varepsilon^2\). Consider the vectors'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}'_i = \frac{\mathbf{v}_i + \varepsilon \mathbf{v}_j}{\sqrt{1+\varepsilon^2}}
    \quad\text{and}\quad A \mathbf{v}'_i = \frac{\sigma_i \mathbf{u}_i + \varepsilon
    \sigma_j \mathbf{u}_j}{\sqrt{1+\varepsilon^2}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Observe that \(\mathbf{v}'_i\) is orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{i-1}\),
    so that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{v}'_i\| \leq \|A \mathbf{v}_i\| =\sigma_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, by the *Orthogonal Decomposition Lemma*, we can write \(A
    \mathbf{v}_i'\) as a sum of its orthogonal projection on the unit vector \(\mathbf{u}_i\)
    and \(A \mathbf{v}_i' - \mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\), which
    is orthogonal to \(\mathbf{u}_i\). In particular, by *Pythagoras*, \(\|A \mathbf{v}_i'\|
    \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|\). But that implies, for
    \(\varepsilon \in (0,1)\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|
    = \left\langle \mathbf{u}_i, A \mathbf{v}_i'\right\rangle = \frac{\sigma_i + \varepsilon
    \sigma_j \delta}{\sqrt{1+\varepsilon^2}} \geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2) \]
  prefs: []
  type: TYPE_NORMAL
- en: where the second inequality follows from a [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series)
    or the observation
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1+\varepsilon^2)\,(1-\varepsilon^2/2)^2 = (1+\varepsilon^2)\,(1-\varepsilon^2
    + \varepsilon^4/4) = 1 - 3/4 \varepsilon^4 + \varepsilon^6/4 \leq 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Now note that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|A \mathbf{v}_i'\| &\geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2)\\ &= \sigma_i + \varepsilon \sigma_j \delta - \varepsilon^2\sigma_i/2
    - \varepsilon^3 \sigma_i \sigma_j \delta/2\\ &= \sigma_i + \varepsilon \left(
    \sigma_j \delta - \varepsilon\sigma_i/2 - \varepsilon^2 \sigma_i \sigma_j \delta/2\right)\\
    &> \sigma_i \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: for \(\varepsilon\) small enough, contradicting the inequality above. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**SVD and Approximating Subspace** In constructing the SVD of \(A\), we used
    the greedy sequence for the best approximating subspace. Vice versa, given an
    SVD of \(A\), we can read off the solution to the approximating subspace problem.
    In other words, there was nothing special about the specific construction we used
    to prove existence of the SVD. While a matrix may have many SVDs, they all give
    a solution to the approximating subspace problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Further, this perspective gives what is known as a variational characterization
    of the singular values. We will have more to say about variational characterizations
    and their uses in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '*SVD and greedy sequence* Indeed, let'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  prefs: []
  type: TYPE_NORMAL
- en: be an SVD of \(A\) with
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: We show that the \(\mathbf{v}_j\)’s form a greedy sequence for the approximating
    subspace problem. Complete \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) into an orthonormal
    basis of \(\mathbb{R}^m\) by adding appropriate vectors \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\).
    By construction, \(A\mathbf{v}_{i} = \mathbf{0}\) for all \(i=j+1,\ldots,m\).
  prefs: []
  type: TYPE_NORMAL
- en: We start with the case \(j=1\). For any unit vector \(\mathbf{w} \in \mathbb{R}^m\),
    we expand it as \(\mathbf{w} = \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle
    \,\mathbf{v}_i\). By the *Properties of Orthonormal Lists*,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| A \left( \sum_{i=1}^m \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i \right) \right\|^2\\ &= \left\|
    \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\
    &= \left\| \sum_{i=1}^r \langle \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i
    \right\|^2\\ &= \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2,
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the orthonormality of the \(\mathbf{u}_i\)’s and the fact that
    \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\) are in the null space of \(A\). Because
    the \(\sigma_i\)’s are non-increasing, this last sum is maximized by taking \(\mathbf{w}
    = \mathbf{v}_1\). So we have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  prefs: []
  type: TYPE_NORMAL
- en: By the *Properties of Orthonormal Lists*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = \left\|\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle \mathbf{v}_i \right\|^2 = \|\mathbf{w}\|^2
    = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, because the \(\sigma_i\)’s are non-increasing, the sum
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A \mathbf{w}\|^2 = \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    \leq \sigma_1^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: This upper bound is achieved by taking \(\mathbf{w} = \mathbf{v}_1\). So we
    have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  prefs: []
  type: TYPE_NORMAL
- en: More generally, for any unit vector \(\mathbf{w} \in \mathbb{R}^m\) that is
    orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{j-1}\), we expand it as \(\mathbf{w}
    = \sum_{i=j}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i\). Then,
    as long as \(j\leq r\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| \sum_{i=j}^m \langle \mathbf{w},
    \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\ &= \left\| \sum_{i=j}^r \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i \right\|^2\\ &= \sum_{i=j}^r
    \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2\\ &\leq \sigma_j^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where again we used that the \(\sigma_i\)’s are non-increasing and \(\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = 1\). This last bound is achieved by
    taking \(\mathbf{w} = \mathbf{v}_j\).
  prefs: []
  type: TYPE_NORMAL
- en: So we have shown the following.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Variational Characterization of Singular Values)** Let \(A =
    \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be an SVD of \(A\) with \(\sigma_1
    \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_j^2 = \max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle \mathbf{w},
    \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}_j\in \arg\max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle
    \mathbf{w}, \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
