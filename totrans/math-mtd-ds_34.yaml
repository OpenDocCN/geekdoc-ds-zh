- en: 4.8\. Online supplementary materials#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.8\. 在线补充材料#
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/supp/roch-mmids-svd-supp.html](https://mmids-textbook.github.io/chap04_svd/supp/roch-mmids-svd-supp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap04_svd/supp/roch-mmids-svd-supp.html](https://mmids-textbook.github.io/chap04_svd/supp/roch-mmids-svd-supp.html)
- en: 4.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8.1\. 测验、解答、代码等.[#](#quizzes-solutions-code-etc "链接到本标题")
- en: 4.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.1\. 仅代码[#](#just-the-code "链接到本标题")
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个交互式Jupyter笔记本，其中包含本章的代码（推荐使用Google Colab）。鼓励您对其进行实验。一些建议的计算练习散布在其中。笔记本也可作为幻灯片使用。
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[幻灯片](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
- en: 4.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.2\. 自我评估测验[#](#self-assessment-quizzes "链接到本标题")
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下链接可以获取更广泛的自我评估测验的网络版本。
- en: '[Section 4.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.2节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
- en: '[Section 4.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.3节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
- en: '[Section 4.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.4节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
- en: '[Section 4.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.5节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
- en: '[Section 4.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.6节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
- en: 4.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.3\. 自动测验[#](#auto-quizzes "链接到本标题")
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在此处访问本章的自动生成的测验（推荐使用Google Colab）。
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动测验](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
- en: 4.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.4\. 奇数编号预热练习的解答[#](#solutions-to-odd-numbered-warm-up-exercises "链接到本标题")
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*(在Claude、Gemini和ChatGPT的帮助下)*'
- en: '**E4.2.1** The answer is \(\mathrm{rk}(A) = 2\). To see this, observe that
    the third column is the sum of the first two columns, so the column space is spanned
    by the first two columns. These two columns are linearly independent, so the dimension
    of the column space (i.e., the rank) is 2.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.1** 答案是 \(\mathrm{rk}(A) = 2\)。要看到这一点，观察第三列是前两列的和，因此列空间由前两列张成。这两列是线性无关的，所以列空间的维度（即秩）是2。'
- en: '**E4.2.3** The eigenvalues are \(\lambda_1 = 4\) and \(\lambda_2 = 2\). For
    \(\lambda_1 = 4\), solving \((A - 4I)\mathbf{x} = \mathbf{0}\) gives the eigenvector
    \(\mathbf{v}_1 = (1, 1)\). For \(\lambda_2 = 2\), solving \((A - 2I)\mathbf{x}
    = \mathbf{0}\) gives the eigenvector \(\mathbf{v}_2 = (1, -1)\).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.3** 特征值为 \(\lambda_1 = 4\) 和 \(\lambda_2 = 2\)。对于 \(\lambda_1 = 4\)，解
    \((A - 4I)\mathbf{x} = \mathbf{0}\) 得到特征向量 \(\mathbf{v}_1 = (1, 1)\)。对于 \(\lambda_2
    = 2\)，解 \((A - 2I)\mathbf{x} = \mathbf{0}\) 得到特征向量 \(\mathbf{v}_2 = (1, -1)\)。'
- en: '**E4.2.5** Normalizing the eigenvectors to get an orthonormal basis: \(\mathbf{q}_1
    = \frac{1}{\sqrt{2}}(1, 1)\) and \(\mathbf{q}_2 = \frac{1}{\sqrt{2}}(1, -1)\).
    Then \(A = Q \Lambda Q^T\) where \(Q = (\mathbf{q}_1, \mathbf{q}_2)\) and \(\Lambda
    = \mathrm{diag}(4, 2)\). Explicitly,'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.5** 将特征向量归一化以获得正交基：\(\mathbf{q}_1 = \frac{1}{\sqrt{2}}(1, 1)\) 和 \(\mathbf{q}_2
    = \frac{1}{\sqrt{2}}(1, -1)\)。然后 \(A = Q \Lambda Q^T\) 其中 \(Q = (\mathbf{q}_1,
    \mathbf{q}_2)\) 和 \(\Lambda = \mathrm{diag}(4, 2)\)。具体地，'
- en: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
- en: '**E4.2.7**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.7**'
- en: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
- en: '**E4.2.9** The rank of \(A\) is 1\. The second column is a multiple of the
    first column, so the column space is one-dimensional.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.9** \(A\) 的秩为 1。第二列是第一列的倍数，所以列空间是一维的。'
- en: '**E4.2.11** The characteristic polynomial of \(A\) is'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.11** \(A\) 的特征多项式为'
- en: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
- en: Hence, the eigenvalues are \(\lambda_1 = 1\) and \(\lambda_2 = 3\). For \(\lambda_1
    = 1\), we solve \((A - I)\mathbf{v} = \mathbf{0}\) to get \(\mathbf{v}_1 = \begin{pmatrix}
    1 \\ -1 \end{pmatrix}\). For \(\lambda_2 = 3\), we solve \((A - 3I)\mathbf{v}
    = 0\) to get \(\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，特征值为 \(\lambda_1 = 1\) 和 \(\lambda_2 = 3\)。对于 \(\lambda_1 = 1\)，我们解 \((A
    - I)\mathbf{v} = \mathbf{0}\) 得到 \(\mathbf{v}_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\)。对于
    \(\lambda_2 = 3\)，我们解 \((A - 3I)\mathbf{v} = 0\) 得到 \(\mathbf{v}_2 = \begin{pmatrix}
    1 \\ 1 \end{pmatrix}\)。
- en: '**E4.2.13** The columns of \(A\) are linearly dependent, since the second column
    is twice the first column. Hence, a basis for the column space of \(A\) is given
    by \(\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix} \right\}\).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.13** \(A\) 的列向量线性相关，因为第二列是第一列的两倍。因此，\(A\) 的列空间的一个基由 \(\left\{ \begin{pmatrix}
    1 \\ 2 \end{pmatrix} \right\}\) 给出。'
- en: '**E4.2.15** The eigenvalues of \(A\) are \(\lambda_1 = 3\) and \(\lambda_2
    = -1\). Since \(A\) has a negative eigenvalue, it is not positive semidefinite.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.15** \(A\) 的特征值为 \(\lambda_1 = 3\) 和 \(\lambda_2 = -1\)。由于 \(A\) 有一个负特征值，它不是正半定的。'
- en: '**E4.2.17** The Hessian of \(f\) is'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.17** \(f\) 的海森矩阵为'
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
- en: The eigenvalues of the Hessian are \(\lambda_1 = 2\) and \(\lambda_2 = -2\).
    Since one eigenvalue is negative, the Hessian is not positive semidefinite, and
    \(f\) is not convex.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 海森矩阵的特征值为 \(\lambda_1 = 2\) 和 \(\lambda_2 = -2\)。由于有一个特征值是负的，海森矩阵不是正半定的，且 \(f\)
    不是凸函数。
- en: '**E4.2.19** The Hessian of \(f\) is'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.19** \(f\) 的海森矩阵为'
- en: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
- en: The eigenvalues of the Hessian are \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) and
    \(\lambda_2 = 0\), which are both nonnegative for all \(x, y\). Therefore, the
    Hessian is positive semidefinite, and \(f\) is convex.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 海森矩阵的特征值为 \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) 和 \(\lambda_2 = 0\)，对于所有 \(x,
    y\) 都是非负的。因此，海森矩阵是正半定的，且 \(f\) 是凸函数。
- en: '**E4.3.1** We have \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\). By
    the method described in the text, \(w_1\) is a unit eigenvector of \(A^TA = \begin{pmatrix}
    5 & 0 \\ 0 & 5 \end{pmatrix}\) corresponding to the largest eigenvalue. Thus,
    we can take \(\mathbf{w}_1 = (1, 0)\) or \(\mathbf{w}_1 = (0, 1)\).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.1** 我们有 \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\)。按照文中描述的方法，\(w_1\)
    是 \(A^TA = \begin{pmatrix} 5 & 0 \\ 0 & 5 \end{pmatrix}\) 对应于最大特征值的单位特征向量。因此，我们可以取
    \(\mathbf{w}_1 = (1, 0)\) 或 \(\mathbf{w}_1 = (0, 1)\)。'
- en: '**E4.3.3** We can take \(U = I_2\), \(\Sigma = A\), and \(V = I_2\). This is
    an SVD of \(A\) because \(U\) and \(V\) are orthogonal and \(\Sigma\) is diagonal.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.3** 我们可以取 \(U = I_2\), \(\Sigma = A\), 和 \(V = I_2\)。这是一个 \(A\) 的奇异值分解，因为
    \(U\) 和 \(V\) 是正交的，且 \(\Sigma\) 是对角线。'
- en: '**E4.3.5** We have \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\).
    The characteristic polynomial of \(A^TA\) is \(\lambda^2 - 25\lambda = \lambda(\lambda
    - 25)\), so the eigenvalues are \(\lambda_1 = 25\) and \(\lambda_2 = 0\). An eigenvector
    corresponding to \(\lambda_1\) is \((1, 2)\), and an eigenvector corresponding
    to \(\lambda_2\) is \((-2, 1)\).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.5** 我们有 \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\)。\(A^TA\)
    的特征多项式是 \(\lambda^2 - 25\lambda = \lambda(\lambda - 25)\)，所以特征值是 \(\lambda_1 =
    25\) 和 \(\lambda_2 = 0\)。对应于 \(\lambda_1\) 的一个特征向量是 \((1, 2)\)，对应于 \(\lambda_2\)
    的一个特征向量是 \((-2, 1)\)。'
- en: '**E4.3.7** We have that the singular values of \(A\) are \(\sigma_1 = \sqrt{25}
    = 5\) and \(\sigma_2 = 0\). We can take \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1 = (1/\sqrt{5}, 2/\sqrt{5})\). Since
    the rank of \(A\) is 1, this gives a compact SVD of \(A\): \(A = U \Sigma V^T\)
    with \(U = \mathbf{u}_1\), \(\Sigma = (5)\), and \(V = \mathbf{v}_1\).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.7** 我们有 \(A\) 的奇异值是 \(\sigma_1 = \sqrt{25} = 5\) 和 \(\sigma_2 = 0\)。我们可以取
    \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\) 和 \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1
    = (1/\sqrt{5}, 2/\sqrt{5})\)。由于 \(A\) 的秩为 1，这给出了 \(A\) 的紧凑奇异值分解：\(A = U \Sigma
    V^T\)，其中 \(U = \mathbf{u}_1\)，\(\Sigma = (5)\)，和 \(V = \mathbf{v}_1\)。'
- en: '**E4.3.9** From the full SVD of \(A\), we have that: An orthonormal basis for
    \(\mathrm{col}(A)\) is given by the first column of \(U\): \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\).
    An orthonormal basis for \(\mathrm{row}(A)\) is given by the first column of \(V\):
    \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\). An orthonormal basis for \(\mathrm{null}(A)\)
    is given by the second column of \(V\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\). An
    orthonormal basis for \(\mathrm{null}(A^T)\) is given by the second column of
    \(U\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.9** 从 \(A\) 的完全奇异值分解中，我们有：\(\mathrm{col}(A)\) 的一个正交基由 \(U\) 的第一列给出：\(\{(1/\sqrt{5},
    2/\sqrt{5})\}\)。\(\mathrm{row}(A)\) 的一个正交基由 \(V\) 的第一列给出：\(\{(1/\sqrt{5}, 2/\sqrt{5})\}\)。\(\mathrm{null}(A)\)
    的一个正交基由 \(V\) 的第二列给出：\(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\)。\(\mathrm{null}(A^T)\)
    的一个正交基由 \(U\) 的第二列给出：\(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\)。'
- en: '**E4.3.11** From its diagonal form, we see that \(\lambda_1 = 15\), \(\mathbf{q}_1
    = (1, 0)\); \(\lambda_2 = 7\), \(\mathbf{q}_2 = (0, 1)\).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.11** 从其对角形式中，我们看到 \(\lambda_1 = 15\), \(\mathbf{q}_1 = (1, 0)\)；\(\lambda_2
    = 7\), \(\mathbf{q}_2 = (0, 1)\)。'
- en: '**E4.3.13** By direct computation, \(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix}
    1 & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.13** 通过直接计算，\(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix} 1
    & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
- en: '**E4.3.15** By direct computation,'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.15** 通过直接计算，'
- en: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
- en: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
- en: '**E4.3.17** The best approximating subspace of dimension \(k=1\) is the line
    spanned by the vector \(\mathbf{v}_1 = (1, 0)\). The sum of squared distances
    to this subspace is \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 =
    5 + 5 + 2 + 10 - 15 = 7.\)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.17** 维度为 \(k=1\) 的最佳逼近子空间是由向量 \(\mathbf{v}_1 = (1, 0)\) 张成的直线。到这个子空间的平方距离之和是
    \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 = 5 + 5 + 2 + 10 - 15
    = 7.\)'
- en: '**E4.4.1**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.1**'
- en: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
- en: The diagonal entries are being raised to increasing powers, while the off-diagonal
    entries remain zero.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线元素正在被提高到递增的幂次，而非对角线元素保持为零。
- en: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
- en: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
- en: '**E4.4.7** We have \(A = Q \Lambda Q^T\), where \(Q\) is the matrix of normalized
    eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues. Then,'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.7** 我们有 \(A = Q \Lambda Q^T\)，其中 \(Q\) 是归一化特征向量的矩阵，\(\Lambda\) 是特征值的对角矩阵。然后，'
- en: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
- en: and similarly, \(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，\(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\)。
- en: '**E4.4.9** We have \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\).
    Let’s find the eigenvalues of the matrix \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 &
    5 \end{pmatrix}\). We solve the characteristic equation \(\det(A^TA - \lambda
    I) = 0\). First, let’s calculate the characteristic polynomial: \(\det(A^TA -
    \lambda I) = \det(\begin{pmatrix} 1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})=
    \lambda^2 - 6\lambda + 1\), so \((\lambda - 3)^2 - 8 = 0\) or \((\lambda - 3)^2
    = 8\) or \(\lambda = 3 \pm 2\sqrt{2}\). Therefore, the eigenvalues of \(A^TA\)
    are: \(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) and \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\). Hence, this matrix is positive semidefinite because its eigenvalues
    are 0 and 6, both of which are non-negative.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.9** 我们有 \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\)。让我们找到矩阵
    \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\) 的特征值。我们解特征方程 \(\det(A^TA
    - \lambda I) = 0\)。首先，让我们计算特征多项式：\(\det(A^TA - \lambda I) = \det(\begin{pmatrix}
    1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})= \lambda^2 - 6\lambda + 1\)，所以 \((\lambda
    - 3)^2 - 8 = 0\) 或 \((\lambda - 3)^2 = 8\) 或 \(\lambda = 3 \pm 2\sqrt{2}\)。因此，\(A^TA\)
    的特征值为：\(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) 和 \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\)。因此，这个矩阵是正半定的，因为它的特征值是 0 和 6，两者都是非负的。'
- en: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). The unit
    norm constraint requires that \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\)。单位范数约束要求 \(\sum_{j=1}^p
    \phi_{j1}^2 = 1\)。'
- en: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\).
    The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\)，\(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\)。分数计算为
    \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\)。'
- en: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\). Uncorrelatedness requires that \(\frac{1}{n-1}\sum_{i=1}^n
    t_{i1}t_{i2} = 0\).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\)。不相关性要求 \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = 0\)。'
- en: '**E4.5.7** First, compute the mean of each column:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.7** 首先，计算每一列的平均值：'
- en: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
- en: 'Mean-centered data matrix:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 均值中心化的数据矩阵：
- en: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
- en: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\), \(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\). The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\)，\(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\)，\(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\)。分数计算为 \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\)。'
- en: '**E4.5.11** The second principal component must be uncorrelated with the first,
    so its loading vector must be orthogonal to \(\varphi_1\). One such vector is
    \(\varphi_2 = (-0.6, 0.8)\).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.11** 第二个主成分必须与第一个主成分不相关，因此其载荷向量必须与 \(\varphi_1\) 正交。一个这样的向量是 \(\varphi_2
    = (-0.6, 0.8)\)。'
- en: '**E4.6.1** The Frobenius norm is given by:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.1** 弗罗贝尼乌斯范数由以下给出：'
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
- en: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\)。'
- en: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\)。'
- en: '**E4.6.7** The induced 2-norm of the difference between a matrix and its rank-1
    truncated SVD is equal to the second singular value. Therefore, using the SVD
    from before, we have \(\|A - A_1\|_2 = \sigma_2 = 0\).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.7** 矩阵与其秩为1的截断奇异值分解之差的诱导2-范数等于第二个奇异值。因此，使用之前的SVD，我们有 \(\|A - A_1\|_2
    = \sigma_2 = 0\)。'
- en: '**E4.6.9** The matrix \(A\) is singular (its determinant is zero), so it doesn’t
    have an inverse. Therefore, \(\|A^{-1}\|_2\) is undefined. Note that the induced
    2-norm of the pseudoinverse \(A^+\) is not the same as the induced 2-norm of the
    inverse (when it exists).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.9** 矩阵 \(A\) 是奇异的（其行列式为零），因此它没有逆。因此，\(\|A^{-1}\|_2\) 是未定义的。注意，伪逆 \(A^+\)
    的诱导2-范数与逆的诱导2-范数（当存在时）不同。'
- en: 4.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.5\. 学习成果[#](#learning-outcomes "链接到这个标题")
- en: Define the column space, row space, and rank of a matrix.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的列空间、行空间和秩。
- en: Prove that the row rank equals the column rank.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明行秩等于列秩。
- en: Apply properties of matrix rank, such as \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\)
    and \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用矩阵秩的性质，例如 \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\) 和 \(\mathrm{rk}(A+B) \leq
    \mathrm{rk}(A) + \mathrm{rk}(B)\)。
- en: State and prove the Rank-Nullity Theorem.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述并证明秩-零度定理。
- en: Define eigenvalues and eigenvectors of a square matrix.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义方阵的特征值和特征向量。
- en: Prove that a symmetric matrix has at most d distinct eigenvalues, where d is
    the matrix size.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明对称矩阵最多有d个不同的特征值，其中d是矩阵的大小。
- en: State the Spectral Theorem for symmetric matrices and explain its implications.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述对称矩阵的谱定理及其含义。
- en: Write the spectral decomposition of a symmetric matrix using outer products.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外积写出对称矩阵的谱分解。
- en: Determine whether a symmetric matrix is positive semidefinite or positive definite
    based on its eigenvalues.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据其特征值确定一个对称矩阵是正半定还是正定。
- en: Compute eigenvalues and eigenvectors of symmetric matrices using programming
    tools like NumPy’s `linalg.eig`.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编程工具（如NumPy的`linalg.eig`）计算对称矩阵的特征值和特征向量。
- en: State the objective of the best approximating subspace problem and formulate
    it mathematically as a minimization of the sum of squared distances.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述最佳逼近子空间问题的目标，并将其数学上表述为平方距离之和的最小化。
- en: Prove that the best approximating subspace problem can be solved greedily by
    finding the best one-dimensional subspace, then the best one-dimensional subspace
    orthogonal to the first, and so on.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明最佳逼近子空间问题可以通过贪婪地找到最佳一维子空间，然后是第一个子空间正交的一维子空间，以此类推，来贪婪地解决。
- en: Define the singular value decomposition (SVD) of a matrix and describe the properties
    of the matrices involved.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的奇异值分解（SVD）及其涉及矩阵的性质。
- en: Prove the existence of the SVD for any real matrix using the Spectral Theorem.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用谱定理证明任何实矩阵的奇异值分解（SVD）的存在性。
- en: Explain the connection between the SVD of a matrix \(A\) and the spectral decompositions
    of \(A^T A\) and \(A A^T\).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释矩阵\(A\)的SVD与\(A^T A\)和\(A A^T\)的谱分解之间的联系。
- en: Compute the SVD of simple matrices.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算简单矩阵的SVD。
- en: Use the SVD of the data matrix to find the best k-dimensional approximating
    subspace to a set of data points.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据矩阵的SVD找到数据点集的最佳k维逼近子空间。
- en: Interpret the singular values as capturing the contributions of the right singular
    vectors to the fit of the approximating subspace.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将奇异值解释为捕获右奇异向量对逼近子空间拟合的贡献。
- en: Obtain low-dimensional representations of data points using the truncated SVD.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用截断的SVD获得数据点的低维表示。
- en: Distinguish between full and compact forms of the SVD and convert between them.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分SVD的满秩和紧凑形式，并在它们之间进行转换。
- en: State the key lemma for power iteration in the positive semidefinite case and
    explain why it implies convergence to the top eigenvector.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在正半定情况下，陈述幂迭代的关键引理，并解释为什么它意味着收敛到最大的特征向量。
- en: Extend the power iteration lemma to the general case of singular value decomposition
    (SVD) and justify why repeated multiplication of A^T A with a random vector converges
    to the top right singular vector.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将幂迭代引理扩展到奇异值分解（SVD）的通用情况，并证明为什么重复乘以\(A^T A\)与一个随机向量收敛到最大的右奇异向量。
- en: Compute the corresponding top singular value and left singular vector given
    the converged top right singular vector.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定收敛的最大右奇异向量，计算相应的最大奇异值和左奇异向量。
- en: Describe the orthogonal iteration method for finding additional singular vectors
    beyond the top one.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述寻找除最大奇异向量之外的其他奇异向量的正交迭代方法。
- en: Apply the power iteration method and orthogonal iteration to compute the SVD
    of a given matrix and use it to find the best low-dimensional subspace approximation.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用幂迭代方法和正交迭代来计算给定矩阵的SVD，并使用它来找到最佳低维子空间逼近。
- en: Implement the power iteration method and orthogonal iteration in Python.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中实现幂迭代方法和正交迭代。
- en: Define principal components and loadings in the context of PCA.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PCA的背景下定义主成分和载荷。
- en: Express the objective of PCA as a constrained optimization problem.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将主成分分析（PCA）的目标表达为约束优化问题。
- en: Establish the connection between PCA and singular value decomposition (SVD).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立PCA与奇异值分解（SVD）之间的联系。
- en: Implement PCA using an SVD algorithm.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVD算法实现PCA。
- en: Interpret the results of PCA in the context of dimensionality reduction and
    data visualization.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在降维和数据可视化的背景下解释PCA的结果。
- en: Define the Frobenius norm and the induced 2-norm of a matrix.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的Frobenius范数和诱导的2-范数。
- en: Express the Frobenius norm and the induced 2-norm of a matrix in terms of its
    singular values.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用矩阵的奇异值来表示矩阵的Frobenius范数和诱导的2-范数。
- en: State the Eckart-Young theorem.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述Eckart-Young定理。
- en: Define the pseudoinverse of a matrix using the SVD.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVD定义矩阵的伪逆。
- en: Apply the pseudoinverse to solve least squares problems when the matrix has
    full column rank.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当矩阵具有满列秩时，应用伪逆来解决最小二乘问题。
- en: Apply the pseudoinverse to find the least norm solution for underdetermined
    systems with full row rank.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用伪逆来找到具有满行秩的欠定系统的最小范数解。
- en: Formulate the ridge regression problem as a regularized optimization problem.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将岭回归问题表述为正则化优化问题。
- en: Explain how ridge regression works by analyzing its solution in terms of the
    SVD of the design matrix.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分析设计矩阵的奇异值分解（SVD）来解释岭回归的工作原理。
- en: \(\aleph\)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: \(\aleph\)
- en: 4.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8.2\. 其他部分[#](#additional-sections "链接到这个标题")
- en: 4.8.2.1\. Computing more singular vectors[#](#computing-more-singular-vectors
    "Link to this heading")
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.1\. 计算更多的奇异向量[#](#computing-more-singular-vectors "链接到这个标题")
- en: We have shown how to compute the first singular vector. How do we compute more
    singular vectors? One approach is to first compute \(\mathbf{v}_1\) (or \(-\mathbf{v}_1\)),
    then find a vector \(\mathbf{y}\) orthogonal to it, and proceed as above. And
    then we repeat until we have all \(m\) right singular vectors.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何计算第一个奇异向量。我们如何计算更多的奇异向量？一种方法是首先计算 \(\mathbf{v}_1\)（或 \(-\mathbf{v}_1\)），然后找到一个与它正交的向量
    \(\mathbf{y}\)，然后像上面那样继续。然后我们重复这个过程，直到我们得到所有 \(m\) 个右奇异向量。
- en: 'We are often interested only in the top, say \(\ell < m\), singular vectors.
    An alternative approach in that case is to start with \(\ell\) random vectors
    and, first, find an orthonormal basis for the space they span. Then to quote [BHK,
    Section 3.7.1]:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常只对前几个，比如说 \(\ell < m\)，奇异向量感兴趣。在这种情况下，另一种方法是先从 \(\ell\) 个随机向量开始，首先找到它们所张成的空间的正交基。然后，引用
    [BHK, 第 3.7.1 节]：
- en: Then compute \(B\) times each of the basis vectors, and find an orthonormal
    basis for the space spanned by the resulting vectors. Intuitively, one has applied
    \(B\) to a subspace rather than a single vector. One repeatedly applies \(B\)
    to the subspace, calculating an orthonormal basis after each application to prevent
    the subspace collapsing to the one dimensional subspace spanned by the first singular
    vector.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后计算 \(B\) 乘以每个基向量，并找到由结果向量所张成的空间的正交基。直观上，我们是对一个子空间而不是单个向量应用 \(B\)。我们反复将 \(B\)
    应用到子空间上，每次应用后计算一个正交基，以防止子空间塌缩到由第一个奇异向量张成的单维子空间。
- en: We will not prove here that this approach, known as orthogonal iteration, works.
    The proof is similar to that of the *Power Iteration Lemma*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不证明这种称为正交迭代的方法是有效的。证明与 *幂迭代引理* 的证明类似。
- en: We implement this last algorithm. We will need our previous implementation of
    *Gram-Schimdt*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了这个最后的算法。我们将需要我们之前实现的 *Gram-Schimdt*。
- en: '[PRE0]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that above we avoided forming the matrix \(A^T A\). With a small number
    of iterations, that approach potentially requires fewer arithmetic operations
    overall and it allows to take advantage of the possible sparsity of \(A\) (i.e.
    the fact that it may have many zeros).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上面我们避免了形成矩阵 \(A^T A\)。使用少量迭代，这种方法可能需要更少的算术运算，并且可以利用 \(A\) 的可能稀疏性（即它可能有很多零）。
- en: '**NUMERICAL CORNER:** We apply it again to our two-cluster example.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们再次将其应用于我们的两个簇示例。'
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s try again, but after projecting on the top two singular vectors. Recall
    that this corresponds to finding the best two-dimensional approximating subspace.
    The projection can be computed using the truncated SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\). We can interpret the rows of \(U_{(2)} \Sigma_{(2)}\) as the coefficients
    of each data point in the basis \(\mathbf{v}_1,\mathbf{v}_2\). We will work in
    that basis.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试，但这次在投影到前两个奇异向量之后。回想一下，这相当于找到最佳二维逼近子空间。投影可以通过截断奇异值分解 \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\) 来计算。我们可以将 \(U_{(2)} \Sigma_{(2)}\) 的行解释为每个数据点在基 \(\mathbf{v}_1,\mathbf{v}_2\)
    中的系数。我们将在这个基上工作。
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="切换隐藏内容">显示代码单元格源代码 隐藏代码单元格源代码</summary>
- en: '[PRE4]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
- en: Finally, looking at the first two right singular vectors, we see that the first
    one does align quite well with the first dimension.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，观察前两个右奇异向量，我们看到第一个与第一个维度相当吻合。
- en: '[PRE5]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: \(\unlhd\)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.2.2\. Pseudoinverse[#](#pseudoinverse "Link to this heading")
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.2\. 伪逆[#](#pseudoinverse "链接到这个标题")
- en: The SVD leads to a natural generalization of the matrix inverse. First an observation.
    Recall that, to take the product of two square diagonal matrices, we simply multiply
    the corresponding diagonal entries. Let \(\Sigma \in \mathbb{R}^{r \times r}\)
    be a square diagonal matrix with diagonal entries \(\sigma_1,\ldots,\sigma_r\).
    If all diagonal entries are non-zero, then the matrix is invertible (since its
    columns then form a basis of the full space). The inverse of \(\Sigma\) in that
    case is simply the diagonal matrix \(\Sigma^{-1}\) with diagonal entries \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\).
    This can be confirmed by checking the definition of the inverse
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 导致了矩阵逆的自然推广。首先是一个观察。回忆一下，当我们乘以两个平方对角矩阵时，我们只需将相应的对角元素相乘。设 \(\Sigma \in \mathbb{R}^{r
    \times r}\) 是一个对角矩阵，其对角元素为 \(\sigma_1,\ldots,\sigma_r\)。如果所有对角元素都不为零，那么该矩阵是可逆的（因为其列构成了整个空间的基）。在这种情况下，\(\Sigma\)
    的逆是一个对角矩阵 \(\Sigma^{-1}\)，其对角元素为 \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\)。这可以通过检查逆的定义来确认。
- en: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
- en: We are ready for our main definition.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好给出主要定义。
- en: '**DEFINITION** **(Pseudoinverse)** Let \(A \in \mathbb{R}^{n \times m}\) be
    a matrix with compact SVD \(A = U \Sigma V^T\) and singular values \(\sigma_1
    \geq \cdots \geq \sigma_r > 0\). A pseudoinverse \(A^+ \in \mathbb{R}^{m \times
    n}\) is defined as'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（伪逆）** 设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有紧凑 SVD \(A = U \Sigma
    V^T\) 和奇异值 \(\sigma_1 \geq \cdots \geq \sigma_r > 0\) 的矩阵。伪逆 \(A^+ \in \mathbb{R}^{m
    \times n}\) 定义为'
- en: \[ A^+ = V \Sigma^{-1} U^T. \]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = V \Sigma^{-1} U^T. \]
- en: \(\natural\)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: While it is not obvious from the definition (why?), the pseudoinverse is in
    fact unique. To see that it is indeed a generalization of an inverse, we make
    a series of observations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从定义上看并不明显（为什么？），但伪逆实际上是唯一的。为了看到它确实是逆的一个推广，我们进行了一系列观察。
- en: '*Observation 1:* Note that, using that \(U\) has orthonormal columns,'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*观察 1:* 注意到，使用 \(U\) 的正交列，'
- en: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
- en: which in general in not the identity matrix. Indeed it corresponds instead to
    the projection matrix onto the column space of \(A\), since the columns of \(U\)
    form an orthonormal basis of that linear subspace. As a result
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这在一般情况下不是单位矩阵。实际上，它对应于 \(A\) 的列空间的投影矩阵，因为 \(U\) 的列构成了该线性子空间的正交基。因此
- en: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
- en: So \(A A^+\) is not the identity matrix, but it does map the columns of \(A\)
    to themselves. Put differently, it is the identity map “when restricted to \(\mathrm{col}(A)\)”.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 \(A A^+\) 不是单位矩阵，但它确实将 \(A\) 的列映射到自身。换句话说，当限制在 \(\mathrm{col}(A)\) 上时，它是单位映射。
- en: Similarly,
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，
- en: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
- en: is the projection matrix onto the row space of \(A\), and
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 是 \(A\) 的行空间的投影矩阵，并且
- en: \[ (A^+ A) A^+ = A^+. \]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A^+ A) A^+ = A^+. \]
- en: '*Observation 2:* If \(A\) has full column rank \(m \leq n\), then \(r = m\).
    In that case, the columns of \(V\) form an orthonormal basis of all of \(\mathbb{R}^m\),
    i.e., \(V\) is orthogonal. Hence,'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*观察 2:* 如果 \(A\) 的满秩列 \(m \leq n\)，那么 \(r = m\)。在这种情况下，\(V\) 的列构成了 \(\mathbb{R}^m\)
    的正交基，即 \(V\) 是正交的。因此，'
- en: \[ A^+ A = V V^T = I_{m \times m}. \]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ A = V V^T = I_{m \times m}. \]
- en: Similarly, if \(A\) has full row rank \(n \leq m\), then \(A A^+ = I_{n \times
    n}\).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果 \(A\) 的满秩行 \(n \leq m\)，那么 \(A A^+ = I_{n \times n}\)。
- en: If both cases hold, then \(n = m\), i.e., \(A\) is square, and \(\mathrm{rk}(A)
    = n\), i.e., \(A\) is invertible. We then get
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这两种情况都成立，那么 \(n = m\)，即 \(A\) 是方阵，且 \(\mathrm{rk}(A) = n\)，即 \(A\) 是可逆的。然后我们得到
- en: \[ A A^+ = A^+ A = I_{n\times n}. \]
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A A^+ = A^+ A = I_{n\times n}. \]
- en: That implies that \(A^+ = A^{-1}\) by the *Existence of an Inverse Lemma* (which
    includes uniqueness of the matrix inverse).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着根据 *逆的存在性引理*（它包括矩阵逆的唯一性），\(A^+ = A^{-1}\)。
- en: '*Observation 3:* Recall that, when \(A\) is nonsingular, the system \(A \mathbf{x}
    = \mathbf{b}\) admits the unique solution \(\mathbf{x} = A^{-1} \mathbf{b}\).
    In the overdetermined case, the pseudoinverse provides a solution to the linear
    least squares problem.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*观察 3:* 回忆一下，当 \(A\) 是非奇异的时，系统 \(A \mathbf{x} = \mathbf{b}\) 有唯一的解 \(\mathbf{x}
    = A^{-1} \mathbf{b}\)。在超定情况下，伪逆提供了线性最小二乘问题的解。'
- en: '**LEMMA** **(Pseudoinverse and Least Squares)** Let \(A \in \mathbb{R}^{n \times
    m}\) with \(m \leq n\). A solution to the linear least squares problem'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（伪逆和最小二乘）** 设 \(A \in \mathbb{R}^{n \times m}\) 且 \(m \leq n\)。线性最小二乘问题的解'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
- en: is given by \(\mathbf{x}^* = A^+ \mathbf{b}\). Further, if \(A\) has full column
    rank \(m\), then
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 是由 \(\mathbf{x}^* = A^+ \mathbf{b}\) 给出的。进一步，如果 \(A\) 具有满秩列 \(m\)，那么
- en: \[ A^+ = (A^T A)^{-1} A^T. \]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = (A^T A)^{-1} A^T. \]
- en: \(\flat\)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* For the first part, we use that the solution to the least squares
    problem is the orthogonal projection. For the second part, we use the SVD definition
    and check that the two sides are the same.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* 对于第一部分，我们使用最小二乘问题的解是正交投影。对于第二部分，我们使用奇异值分解的定义并检查两边是否相同。'
- en: '*Proof:* Let \(A = U \Sigma V^T\) be a compact SVD of \(A\). For the first
    claim, note that the choice of \(\mathbf{x}^*\) in the statement gives'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 设 \(A = U \Sigma V^T\) 为 \(A\) 的紧凑奇异值分解。对于第一个断言，注意陈述中 \(\mathbf{x}^*\)
    的选择给出'
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
- en: Hence, \(A \mathbf{x}^*\) is the orthogonal projection of \(\mathbf{b}\) onto
    the column space of \(A\) - which we proved previously is the solution to the
    linear least squares problem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(A \mathbf{x}^*\) 是 \(\mathbf{b}\) 在 \(A\) 的列空间上的正交投影——这是我们之前证明的线性最小二乘问题的解。
- en: Now onto the second claim. Recall that, when \(A\) is of full rank, the matrix
    \(A^T A\) is nonsingular. We then note that, using the notation \(\Sigma^{-2}
    = (\Sigma^{-1})^2\),
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是第二个断言。回忆一下，当 \(A\) 是满秩时，矩阵 \(A^T A\) 是非奇异的。然后我们注意到，使用符号 \(\Sigma^{-2} =
    (\Sigma^{-1})^2\)，
- en: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
- en: as claimed. Here, we used that \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix. \(\square\)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如所述。在这里，我们使用了 \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\)，这可以通过检查逆矩阵的定义和我们对对角矩阵逆的先前观察来证实。
    \(\square\)
- en: 'The pseudoinverse also provides a solution in the case of an underdetermined
    system. Here, however, there are in general infinitely many solutions. The one
    chosen by the pseudoinverse has a special property as we see now: it is the least
    norm solution.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 伪逆在欠定系统的情况下也提供了一个解。然而，在这种情况下，通常有无穷多个解。伪逆选择的解具有一个特殊性质，我们现在可以看到：它是最小范数解。
- en: '**LEMMA** **(Pseudoinverse and Underdetermined Systems)** Let \(A \in \mathbb{R}^{n
    \times m}\) with \(m > n\) and \(\mathbf{b} \in \mathbb{R}^n\). Further assume
    that \(A\) has full row rank \(n\). Then \(\mathbf{x}^* = A^+ \mathbf{b}\) is
    a solution to'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（伪逆和欠定系统）** 设 \(A \in \mathbb{R}^{n \times m}\) 且 \(m > n\)，\(\mathbf{b}
    \in \mathbb{R}^n\)。进一步假设 \(A\) 具有满秩行 \(n\)。那么 \(\mathbf{x}^* = A^+ \mathbf{b}\)
    是'
- en: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
- en: Moreover, in that case,
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这种情况下，
- en: \[ A^+ = A^T (A A^T)^{-1}. \]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = A^T (A A^T)^{-1}. \]
- en: \(\flat\)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* We first prove the formula for \(A^+\). As we did in the overdetermined
    case, it can be checked by substituting a compact SVD \(A = U \Sigma V^T\). Recall
    that, when \(A^T\) is of full rank, the matrix \(A A^T\) is nonsingular. We then
    note that'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 我们首先证明 \(A^+\) 的公式。正如我们在超定情况下所做的那样，可以通过代入紧凑的奇异值分解 \(A = U \Sigma V^T\)
    来验证。回忆一下，当 \(A^T\) 是满秩时，矩阵 \(A A^T\) 是非奇异的。然后我们注意到'
- en: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
- en: as claimed. Here, we used that \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如所述。在这里，我们使用了 \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\)，这可以通过检查逆矩阵的定义和我们对对角矩阵逆的先前观察来证实。
- en: Because \(A\) has full row rank, \(\mathbf{b} \in \mathrm{col}(A)\) and there
    is at least one \(\mathbf{x}\) such that \(A \mathbf{x} = \mathbf{b}\). One such
    solution is provided by the pseudoinverse. Indeed, from a previous observation,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(A\) 具有满秩行，\(\mathbf{b} \in \mathrm{col}(A)\) 并且至少存在一个 \(\mathbf{x}\) 使得
    \(A \mathbf{x} = \mathbf{b}\)。这样一个解由伪逆提供。确实，从先前的观察中，
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
- en: where we used that the columns of \(U\) form an orthonormal basis of \(\mathbb{R}^n\)
    by the rank assumption.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(U\) 的列由秩的假设形成 \(\mathbb{R}^n\) 的正交基。
- en: Let \(\mathbf{x}\) be any other solution to the system. Then \(A(\mathbf{x}
    - \mathbf{x}^*) = \mathbf{b} - \mathbf{b} = \mathbf{0}\). That implies
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\mathbf{x}\) 为系统的一个其他解。那么 \(A(\mathbf{x} - \mathbf{x}^*) = \mathbf{b} -
    \mathbf{b} = \mathbf{0}\)。这表明
- en: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
- en: In words, \(\mathbf{x} - \mathbf{x}^*\) and \(\mathbf{x}^*\) are orthogonal.
    By *Pythagoras*,
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，\(\mathbf{x} - \mathbf{x}^*\) 和 \(\mathbf{x}^*\) 是正交的。根据 *毕达哥拉斯*，
- en: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
- en: That proves that \(\mathbf{x}^*\) has the smallest norm among all solutions
    to the system \(A \mathbf{x} = \mathbf{b}\). \(\square\)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了 \(\mathbf{x}^*\) 在系统 \(A \mathbf{x} = \mathbf{b}\) 的所有解中具有最小的范数。 \(\square\)
- en: '**EXAMPLE:** Continuing a previous example, let'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** 继续之前的例子，设'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: Recall that
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
- en: where
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
- en: We compute the pseudoinverse. By the formula, in the rank one case, it is simply
    (Check this!)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算伪逆。根据公式，在秩为1的情况下，它简单地是（检查这个！）
- en: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
- en: \(\lhd\)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be a square nonsingular
    matrix. Let \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be a compact
    SVD of \(A\), where we used the fact that the rank of \(A\) is \(n\) so it has
    \(n\) strictly positive singular values. We seek to compute \(\|A^{-1}\|_2\) in
    terms of the singular values.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** 设 \(A \in \mathbb{R}^{n \times n}\) 为一个 \(n \times n\) 的非奇异方阵。设
    \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) 为 \(A\) 的紧凑奇异值分解，其中我们使用了
    \(A\) 的秩为 \(n\) 的性质，因此它有 \(n\) 个严格正的奇异值。我们寻求用奇异值来计算 \(\|A^{-1}\|_2\)。'
- en: Because \(A\) is invertible, \(A^+ = A^{-1}\). So we compute the pseudoinverse
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(A\) 是可逆的，所以 \(A^+ = A^{-1}\)。因此我们计算伪逆
- en: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
- en: The sum on the right-hand side is not quite a compact SVD of \(A^{-1}\) because
    the coefficients \(\sigma_j^{-1}\) are non-decreasing in \(j\).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的和并不完全是一个 \(A^{-1}\) 的紧凑奇异值分解，因为系数 \(\sigma_j^{-1}\) 在 \(j\) 上是非递减的。
- en: But writing the sum in reverse order
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 但将和写成相反的顺序
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
- en: does give a compact SVD of \(A^{-1}\), since \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1}
    > 0\) and \(\{\mathbf{v}_j\}_{j=1}^n\) and \(\{\mathbf{u}_j\}_{j=1}^n\) are orthonormal
    lists. Hence, the \(2\)-norm is given by the largest singular value, that is,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然给出了 \(A^{-1}\) 的紧凑奇异值分解，因为 \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1} > 0\)，并且
    \(\{\mathbf{v}_j\}_{j=1}^n\) 和 \(\{\mathbf{u}_j\}_{j=1}^n\) 是正交列表。因此，2-范数由最大的奇异值给出，即
- en: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
- en: \(\lhd\)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**NUMERICAL CORNER:** In Numpy, the pseudoinverse of a matrix can be computed
    using the function [`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**NUMERICAL CORNER:** 在 Numpy 中，可以使用函数 `numpy.linalg.pinv` 计算矩阵的伪逆。[`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html)。'
- en: '[PRE7]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s try our previous example.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试之前的例子。
- en: '[PRE13]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: \(\unlhd\)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.2.3\. Condition numbers[#](#condition-numbers "Link to this heading")
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.3\. 条件数[#](#condition-numbers "链接到这个标题")
- en: In this section we introduce condition numbers, a measure of perturbation sensitivity
    for numerical problems. We look in particular at the conditioning of the least-squares
    problem. We begin with the concept of pseudoinverse, which is important in its
    own right.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍条件数，它是数值问题扰动敏感性的一个度量。我们特别关注最小二乘问题的条件。我们首先介绍伪逆的概念，它本身也很重要。
- en: '**Conditioning of matrix-vector multiplication** We define the condition number
    of a matrix and show that it captures some information about the sensitivity to
    perturbations of matrix-vector multiplications.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵-向量乘法的条件** 我们定义矩阵的条件数，并展示它能够捕捉矩阵-向量乘法对扰动的敏感性的一些信息。'
- en: '**DEFINITION** **(Condition number of a matrix)** The condition number (in
    the induced \(2\)-norm) of a square, nonsingular matrix \(A \in \mathbb{R}^{n
    \times n}\) is defined as'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(矩阵的条件数)** 方阵、非奇异矩阵 \(A \in \mathbb{R}^{n \times n}\) 的条件数（在诱导的 \(2\)-范数下）定义为'
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
- en: \(\natural\)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: In fact, this can be computed as
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这可以计算为
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
- en: where we used the example above. In words, \(\kappa_2(A)\) is the ratio of the
    largest to the smallest stretching under \(A\).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了上面的例子。换句话说，\(\kappa_2(A)\) 是在 \(A\) 下最大拉伸与最小拉伸的比率。
- en: '**THEOREM** **(Conditioning of Matrix-Vector Multiplication)** Let \(M \in
    \mathbb{R}^{n \times n}\) be nonsingular. Then, for any \(\mathbf{z} \in \mathbb{R}^n\),'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(矩阵-向量乘法的条件)** 设 \(M \in \mathbb{R}^{n \times n}\) 为非奇异矩阵。那么，对于任意的
    \(\mathbf{z} \in \mathbb{R}^n\),'
- en: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
- en: and the inequality is tight in the sense that there is an \(\mathbf{x}\) and
    a \(\mathbf{d}\) that achieves it.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 并且不等式在紧致的意义上是说存在一个 \(\mathbf{x}\) 和一个 \(\mathbf{d}\) 可以达到它。
- en: \(\sharp\)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: The ratio above measures the worst rate of relative change in \(M \mathbf{z}\)
    under infinitesimal perturbations of \(\mathbf{z}\). The theorem says that when
    \(\kappa_2(M)\) is large, a case referred to as ill-conditioning, large relative
    changes in \(M \mathbf{z}\) can be obtained from relatively small perturbations
    to \(\mathbf{z}\). In words, a matrix-vector product is potentially sensitive
    to perturbations when the matrix is ill-conditioned.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的比率衡量了在 \(\mathbf{z}\) 的无穷小扰动下 \(M \mathbf{z}\) 的相对变化的最坏速率。定理表明，当 \(\kappa_2(M)\)
    很大时，即所谓的病态情况，从对 \(\mathbf{z}\) 的相对较小的扰动可以得到 \(M \mathbf{z}\) 的较大相对变化。换句话说，当矩阵病态时，矩阵-向量乘积对扰动可能很敏感。
- en: '*Proof:* Write'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明** 写出'
- en: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
- en: where we used \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\),
    which was shown in a previous example.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\)，这在之前的例子中已经证明过。
- en: In particular, we see that the ratio can achieve its maximum by taking \(\mathbf{d}\)
    and \(\mathbf{z}\) to be the right singular vectors corresponding to \(\sigma_1\)
    and \(\sigma_n\) respectively. \(\square\)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们看到这个比率可以通过取 \(\mathbf{d}\) 和 \(\mathbf{z}\) 分别为对应于 \(\sigma_1\) 和 \(\sigma_n\)
    的右奇异向量来达到其最大值。 \(\square\)
- en: If we apply the theorem to the inverse instead, we get that the relative conditioning
    of the nonsingular linear system \(A \mathbf{x} = \mathbf{b}\) to perturbations
    in \(\mathbf{b}\) is \(\kappa_2(A)\). The latter can be large in particular when
    the columns of \(A\) are close to linearly dependent. This is detailed in the
    next example.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将定理应用于逆矩阵，我们得到非奇异线性系统 \(A \mathbf{x} = \mathbf{b}\) 对 \(\mathbf{b}\) 中扰动的相对条件数为
    \(\kappa_2(A)\)。特别是当 \(A\) 的列接近线性相关时，这个值可能很大。这一点将在下一个例子中详细说明。
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be nonsingular. Then, for
    any \(\mathbf{b} \in \mathbb{R}^n\), there exists a unique solution to \(A \mathbf{x}
    = \mathbf{b}\), namely,'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 设 \(A \in \mathbb{R}^{n \times n}\) 是非奇异的。那么，对于任何 \(\mathbf{b} \in
    \mathbb{R}^n\)，存在一个唯一的解 \(A \mathbf{x} = \mathbf{b}\)，即，'
- en: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
- en: Suppose we solve the perturbed system
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们求解扰动系统
- en: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
- en: for some vector \(\delta\mathbf{b}\). We use the *Conditioning of Matrix-Vector
    Multiplication Theorem* to bound the norm of \(\delta\mathbf{x}\).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个向量 \(\delta\mathbf{b}\)。我们使用 *矩阵-向量乘法的条件定理* 来界定 \(\delta\mathbf{x}\) 的范数。
- en: Specifically, set
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，设
- en: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
- en: Then
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
- en: and
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
- en: So we get that
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们得到
- en: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
- en: Note also that, because \((A^{-1})^{-1} = A\), we have \(\kappa_2(A^{-1}) =
    \kappa_2(A)\). Rearranging, we finally get
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，因为 \((A^{-1})^{-1} = A\)，所以 \(\kappa_2(A^{-1}) = \kappa_2(A)\)。重新排列后，我们最终得到
- en: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
- en: Hence the larger the condition number is, the larger the potential relative
    effect on the solution of the linear system is for a given relative perturbation
    size. \(\lhd\)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，条件数越大，对于给定的相对扰动大小，线性系统解的潜在相对影响也越大。 \(\lhd\)
- en: '**NUMERICAL CORNER:** In Numpy, the condition number of a matrix can be computed
    using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 在 Numpy 中，可以使用函数 `numpy.linalg.cond` 计算矩阵的条件数（[链接](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html)）。'
- en: For example, orthogonal matrices have condition number \(1\), the lowest possible
    value for it (Why?). That indicates that orthogonal matrices have good numerical
    properties.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正交矩阵的条件数是 \(1\)，这是它的可能最小值（为什么？）。这表明正交矩阵具有良好的数值特性。
- en: '[PRE17]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In contrast, matrices with nearly linearly dependent columns have large condition
    numbers.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，列几乎线性相关的矩阵具有较大的条件数。
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s look at the SVD of \(A\).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 \(A\) 的奇异值分解（SVD）。
- en: '[PRE25]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We compute the solution to \(A \mathbf{x} = \mathbf{b}\) when \(\mathbf{b}\)
    is the left singular vector of \(A\) corresponding to the largest singular value.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case bound is achieved when \(\mathbf{z} =
    \mathbf{b}\) is right singular vector of \(M= A^{-1}\) corresponding to the lowest
    singular value. In a previous example, given a matrix \(A = \sum_{j=1}^n \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) in compact SVD form, we derived a compact SVD for
    the inverse as
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\mathbf{b}\) 是对应于最大奇异值的 \(A\) 的左奇异向量时，我们计算 \(A \mathbf{x} = \mathbf{b}\)
    的解。回忆一下，在 *矩阵-向量乘法的条件定理* 的证明中，我们展示了当 \(\mathbf{z} = \mathbf{b}\) 是 \(M = A^{-1}\)
    的对应于最低奇异值的右奇异向量时，最坏情况下的界限被达到。在一个先前的例子中，给定一个矩阵 \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) 以紧凑的 SVD 形式，我们推导了逆的紧凑 SVD 为
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
- en: Here, compared to the SVD of \(A\), the order of the singular values is reversed
    and the roles of the left and right singular vectors are exchanged. So we take
    \(\mathbf{b}\) to be the top left singular vector of \(A\).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与 \(A\) 的 SVD 相比，奇异值的顺序被颠倒，左奇异向量和右奇异向量的角色也发生了交换。因此，我们取 \(\mathbf{b}\) 为
    \(A\) 的左上奇异向量。
- en: '[PRE27]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We make a small perturbation in the direction of the second right singular vector.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case is achieved when \(\mathbf{d} = \delta\mathbf{b}\)
    is top right singular vector of \(M = A^{-1}\). By the argument above, that is
    the left singular vector of \(A\) corresponding to the lowest singular value.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二个右奇异向量的方向上进行微小扰动。回忆在矩阵-向量乘法条件数定理的证明中，我们展示了当 \(\mathbf{d} = \delta\mathbf{b}\)
    是 \(M = A^{-1}\) 的右上奇异向量时，最坏情况发生。根据上述论证，这是 \(A\) 对应于最小奇异值的左奇异向量。
- en: '[PRE31]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The relative change in solution is:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 解的相对变化是：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note that this is exactly the condition number of \(A\).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这正好是 \(A\) 的条件数。
- en: \(\unlhd\)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Back to the least-squares problem** We return to the least-squares problem'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**回到最小二乘问题** 我们回到最小二乘问题'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
- en: where
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{和} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
- en: We showed that the solution satisfies the normal equations
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明了该解满足正则方程
- en: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
- en: Here \(A\) may not be square and invertible. We define a more general notion
    of condition number.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，\(A\) 可能不是方阵且不可逆。我们定义一个更一般的条件数概念。
- en: '**DEFINITION** **(Condition number of a matrix: general case)** The condition
    number (in the induced \(2\)-norm) of a matrix \(A \in \mathbb{R}^{n \times m}\)
    is defined as'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(矩阵的条件数：一般情况**) 矩阵 \(A \in \mathbb{R}^{n \times m}\) 的条件数（在诱导的 \(2\)-范数下）定义为'
- en: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
- en: \(\natural\)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: As we show next, the condition number of \(A^T A\) can be much larger than that
    of \(A\) itself.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们接下来要展示的，\(A^T A\) 的条件数可能远大于 \(A\) 本身。
- en: '**LEMMA** **(Condition number of \(A^T A\))** Let \(A \in \mathbb{R}^{n \times
    m}\) have full column rank. The'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(矩阵 \(A^T A\) 的条件数**) 设 \(A \in \mathbb{R}^{n \times m}\) 具有满列秩。我们有'
- en: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
- en: \(\flat\)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* We use the SVD.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们使用奇异值分解。'
- en: '*Proof:* Let \(A = U \Sigma V^T\) be an SVD of \(A\) with singular values \(\sigma_1
    \geq \cdots \geq \sigma_m > 0\). Then'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 设 \(A = U \Sigma V^T\) 为 \(A\) 的奇异值分解，其中奇异值 \(\sigma_1 \geq \cdots \geq
    \sigma_m > 0\)。那么'
- en: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
- en: In particular the latter expression is an SVD of \(A^T A\), and hence the condition
    number of \(A^T A\) is
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，后一种表达式是 \(A^T A\) 的奇异值分解，因此 \(A^T A\) 的条件数是
- en: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
- en: \(\square\)
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: '**NUMERICAL CORNER:** We give a quick example.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们给出一个快速示例。'
- en: '[PRE37]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This observation – and the resulting increased numerical instability – is one
    of the reasons we previously developed an alternative approach to the least-squares
    problem. Quoting [Sol, Section 5.1]:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '这个观察结果——以及由此产生的数值不稳定性增加——是我们之前开发最小二乘问题替代方法的原因之一。引用 [Sol, 第5.1节]:'
- en: Intuitively, a primary reason that \(\mathrm{cond}(A^T A)\) can be large is
    that columns of \(A\) might look “similar” […] If two columns \(\mathbf{a}_i\)
    and \(\mathbf{a}_j\) satisfy \(\mathbf{a}_i \approx \mathbf{a}_j\), then the least-squares
    residual length \(\|\mathbf{b} - A \mathbf{x}\|_2\) will not suffer much if we
    replace multiples of \(\mathbf{a}_i\) with multiples of \(\mathbf{a}_j\) or vice
    versa. This wide range of nearly—but not completely—equivalent solutions yields
    poor conditioning. […] To solve such poorly conditioned problems, we will employ
    an alternative technique with closer attention to the column space of \(A\) rather
    than employing row operations as in Gaussian elimination. This strategy identifies
    and deals with such near-dependencies explicitly, bringing about greater numerical
    stability.
  id: totrans-311
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直观地说，\(\mathrm{cond}(A^T A)\) 可以很大的一个主要原因是 \(A\) 的列可能看起来“相似” [...] 如果两个列 \(\mathbf{a}_i\)
    和 \(\mathbf{a}_j\) 满足 \(\mathbf{a}_i \approx \mathbf{a}_j\)，那么如果我们用 \(\mathbf{a}_j\)
    的倍数替换 \(\mathbf{a}_i\) 的倍数，或者反之亦然，则最小二乘残差长度 \(\|\mathbf{b} - A \mathbf{x}\|_2\)
    不会受到太大影响。 [...] 为了解决这种病态问题，我们将采用一种替代技术，这种技术更关注 \(A\) 的列空间，而不是像高斯消元法那样使用行操作。这种策略明确识别和处理这种近依赖关系，从而提高了数值稳定性。
- en: \(\unlhd\)
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: We quote without proof a theorem from [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Theorem 4.2.7] which sheds further light on this issue.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引用了 [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408), 定理4.2.7]]
    的定理，该定理进一步阐明了这个问题。
- en: '**THEOREM** **(Accuracy of Least-squares Solutions)** Let \(\mathbf{x}^*\)
    be the solution of the least-squares problem \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\). Let \(\mathbf{x}_{\mathrm{NE}}\) be the solution
    obtained by forming and solving the normal equations in [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    with rounding unit \(\epsilon_M\). Then \(\mathbf{x}_{\mathrm{NE}}\) satisfies'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（最小二乘解的精度）** 设 \(\mathbf{x}^*\) 是最小二乘问题 \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\) 的解。设 \(\mathbf{x}_{\mathrm{NE}}\) 是通过在 [浮点算术](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    中形成和求解正则方程得到的解，其中舍入单位为 \(\epsilon_M\)。那么 \(\mathbf{x}_{\mathrm{NE}}\) 满足'
- en: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
- en: Let \(\mathbf{x}_{\mathrm{QR}}\) be the solution obtained from a QR factorization
    in the same arithmetic. Then
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\mathbf{x}_{\mathrm{QR}}\) 是在同一算术中从 QR 分解得到的解。那么
- en: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
- en: where \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) is the residual vector.
    The constants \(\gamma\) are slowly growing functions of the dimensions of the
    problem.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) 是残差向量。常数 \(\gamma\) 是问题维度的缓慢增长函数。
- en: \(\sharp\)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: 'To explain, let’s quote [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Section 4.2.3] again:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释，让我们再次引用 [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    第4.2.3节]]：
- en: The perturbation theory for the normal equations shows that \(\kappa_2^2(A)\)
    controls the size of the errors we can expect. The bound for the solution computed
    from the QR equation also has a term multiplied by \(\kappa_2^2(A)\), but this
    term is also multiplied by the scaled residual, which can diminish its effect.
    However, in many applications the vector \(\mathbf{b}\) is contaminated with error,
    and the residual can, in general, be no smaller than the size of that error.
  id: totrans-321
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正则方程的扰动理论表明 \(\kappa_2^2(A)\) 控制着我们期望的误差大小。从 QR 方程计算出的解的界限也有一个乘以 \(\kappa_2^2(A)\)
    的项，但这个项还乘以缩放残差，这可能会减弱其效果。然而，在许多应用中，向量 \(\mathbf{b}\) 被误差污染，残差在一般情况下不能小于该误差的大小。
- en: '**NUMERICAL CORNER:** Here is a numerical example taken from [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    Lecture 19]. We will approximate the following function with a polynomial.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落**：这里是一个来自 [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    第19讲]] 的数值示例。我们将用多项式来逼近以下函数。'
- en: '[PRE43]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE44]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE44]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
- en: We use a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix),
    which can be constructed using [`numpy.vander`](https://numpy.org/doc/stable/reference/generated/numpy.vander.html),
    to perform polynomial regression.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 [Vandermonde 矩阵](https://en.wikipedia.org/wiki/Vandermonde_matrix)，它可以通过
    `numpy.vander` 构建，来进行多项式回归。
- en: '[PRE45]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The condition numbers of \(A\) and \(A^T A\) are both high in this case.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，\(A\) 和 \(A^T A\) 的条件数都很高。
- en: '[PRE46]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We first use the normal equations and plot the residual vector.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用正则方程并绘制残差向量。
- en: '[PRE50]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE52]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE52]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
- en: We then use `numpy.linalg.qr` to compute the QR solution instead.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用 `numpy.linalg.qr` 来计算 QR 解。
- en: '[PRE53]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE55]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE55]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
- en: \(\unlhd\)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8.3\. 额外证明[#](#additional-proofs "链接到本标题")
- en: '**Proof of Greedy Finds Best Subspace** In this section, we prove the full
    version of the *Greedy Finds Best Subspace Theorem*. In particular, we do not
    use the *Spectral Theorem*.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪算法找到最佳子空间证明** 在本节中，我们证明 *贪婪算法找到最佳子空间定理* 的完整版本。特别是，我们不使用 *谱定理*。'
- en: '*Proof idea:* We proceed by induction. For an arbitrary orthonormal list \(\mathbf{w}_1,\ldots,\mathbf{w}_k\),
    we find an orthonormal basis of their span containing an element orthogonal to
    \(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1}\). Then we use the defintion of \(\mathbf{v}_k\)
    to conclude.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们通过归纳法进行。对于任意正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\)，我们找到一个包含与
    \(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1}\) 正交的元素的它们的张成的正交基。然后我们使用 \(\mathbf{v}_k\)
    的定义来得出结论。'
- en: '*Proof:* We reformulate the problem as a maximization'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们将问题重新表述为最大化问题'
- en: \[ \max \left\{ \sum_{j=1}^k \|A \mathbf{w}_j\|^2\ :\ \text{$\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}$
    is an orthonormal list} \right\}, \]
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max \left\{ \sum_{j=1}^k \|A \mathbf{w}_j\|^2\ :\ \text{$\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}$
    是一个正交归一列表} \right\}, \]
- en: where we also replace the \(k\)-dimensional linear subspace \(\mathcal{Z}\)
    with an arbitrary orthonormal basis \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，我们也用任意正交归一基 \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\) 替换了 \(k\) 维线性子空间 \(\mathcal{Z}\)。
- en: We then proceed by induction. For \(k=1\), we define \(\mathbf{v}_1\) as a solution
    of the above maximization problem. Assume that, for any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_\ell\}\)
    with \(\ell < k\), we have
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后通过归纳法进行。对于 \(k=1\)，我们将 \(\mathbf{v}_1\) 定义为上述最大化问题的解。假设对于任何正交归一列表 \(\{\mathbf{w}_1,\ldots,\mathbf{w}_\ell\}\)
    且 \(\ell < k\)，我们有
- en: \[ \sum_{j=1}^\ell \|A \mathbf{w}_j\|^2 \leq \sum_{j=1}^\ell \|A \mathbf{v}_j\|^2.
    \]
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^\ell \|A \mathbf{w}_j\|^2 \leq \sum_{j=1}^\ell \|A \mathbf{v}_j\|^2.
    \]
- en: Now consider any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\) and
    let its span be \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1,\ldots,\mathbf{w}_k)\).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑任意正交归一列表 \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\) 以及其张成 \(\mathcal{W} =
    \mathrm{span}(\mathbf{w}_1,\ldots,\mathbf{w}_k)\)。
- en: '***Step 1:*** For \(j=1,\ldots,k-1\), let \(\mathbf{v}_j''\) be the orthogonal
    projection of \(\mathbf{v}_j\) onto \(\mathcal{W}\) and let \(\mathcal{V}'' =
    \mathrm{span}(\mathbf{v}''_1,\ldots,\mathbf{v}''_{k-1})\). Because \(\mathcal{V}''
    \subseteq \mathcal{W}\) has dimension at most \(k-1\) while \(\mathcal{W}\) itself
    has dimension \(k\), we can find an orthonormal basis \(\mathbf{w}''_1,\ldots,\mathbf{w}''_{k}\)
    of \(\mathcal{W}\) such that \(\mathbf{w}''_k\) is orthogonal to \(\mathcal{V}''\)
    (Why?). Then, for any \(j=1,\ldots,k-1\), we have the decomposition \(\mathbf{v}_j
    = \mathbf{v}''_j + (\mathbf{v}_j - \mathbf{v}''_j)\) where \(\mathbf{v}''_j \in
    \mathcal{V}''\) is orthogonal to \(\mathbf{w}''_k\) and \(\mathbf{v}_j - \mathbf{v}''_j\)
    is also orthogonal to \(\mathbf{w}''_k \in \mathcal{W}\) by the properties of
    the orthogonal projection onto \(\mathcal{W}\). Hence'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 1:*** 对于 \(j=1,\ldots,k-1\)，令 \(\mathbf{v}_j''\) 为 \(\mathbf{v}_j\) 在
    \(\mathcal{W}\) 上的正交投影，并令 \(\mathcal{V}'' = \mathrm{span}(\mathbf{v}''_1,\ldots,\mathbf{v}''_{k-1})\)。因为
    \(\mathcal{V}'' \subseteq \mathcal{W}\) 的维度最多为 \(k-1\)，而 \(\mathcal{W}\) 本身的维度为
    \(k\)，我们可以找到一个 \(\mathcal{W}\) 的正交基 \(\mathbf{w}''_1,\ldots,\mathbf{w}''_{k}\)，使得
    \(\mathbf{w}''_k\) 与 \(\mathcal{V}''\) 正交（为什么？）。然后，对于任意的 \(j=1,\ldots,k-1\)，我们有分解
    \(\mathbf{v}_j = \mathbf{v}''_j + (\mathbf{v}_j - \mathbf{v}''_j)\)，其中 \(\mathbf{v}''_j
    \in \mathcal{V}''\) 与 \(\mathbf{w}''_k\) 正交，且 \(\mathbf{v}_j - \mathbf{v}''_j\)
    也与 \(\mathbf{w}''_k \in \mathcal{W}\) 正交，这是由于正交投影到 \(\mathcal{W}\) 的性质。因此'
- en: \[\begin{align*} \left\langle \sum_{j=1}^{k-1}\beta_j \mathbf{v}_j, \mathbf{w}'_k
    \right\rangle &= \left\langle \sum_{j=1}^{k-1}\beta_j [\mathbf{v}'_j + (\mathbf{v}_j
    - \mathbf{v}'_j)], \mathbf{w}'_k \right\rangle\\ &= \left\langle \sum_{j=1}^{k-1}\beta_j
    \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \left\langle \sum_{j=1}^{k-1}\beta_j
    (\mathbf{v}_j - \mathbf{v}'_j), \mathbf{w}'_k \right\rangle\\ &= \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}_j - \mathbf{v}'_j, \mathbf{w}'_k \right\rangle\\ &= 0
    \end{align*}\]
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \left\langle \sum_{j=1}^{k-1}\beta_j \mathbf{v}_j, \mathbf{w}'_k
    \right\rangle &= \left\langle \sum_{j=1}^{k-1}\beta_j [\mathbf{v}'_j + (\mathbf{v}_j
    - \mathbf{v}'_j)], \mathbf{w}'_k \right\rangle\\ &= \left\langle \sum_{j=1}^{k-1}\beta_j
    \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \left\langle \sum_{j=1}^{k-1}\beta_j
    (\mathbf{v}_j - \mathbf{v}'_j), \mathbf{w}'_k \right\rangle\\ &= \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}_j - \mathbf{v}'_j, \mathbf{w}'_k \right\rangle\\ &= 0
    \end{align*}\]
- en: for any \(\beta_j\)’s. That is, \(\mathbf{w}'_k\) is orthogonal to \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意的 \(\beta_j\)。也就是说，\(\mathbf{w}'_k\) 与 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\)
    正交。
- en: '***Step 2:*** By the induction hypothesis, we have'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 2:*** 根据归纳假设，我们有'
- en: \[ (*) \qquad \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 \leq \sum_{j=1}^{k-1} \|A
    \mathbf{v}_j\|^2\. \]
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (*) \qquad \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 \leq \sum_{j=1}^{k-1} \|A
    \mathbf{v}_j\|^2\. \]
- en: Moreover, recalling that the \(\boldsymbol{\alpha}_i^T\)’s are the rows of \(A\),
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，回顾一下，\(\boldsymbol{\alpha}_i^T\) 是 \(A\) 的行，
- en: \[ (**) \qquad \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{i=1}^n \|\mathrm{proj}_{\mathcal{W}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{j=1}^k \|A \mathbf{w}_j'\|^2 \]
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (**) \qquad \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{i=1}^n \|\mathrm{proj}_{\mathcal{W}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{j=1}^k \|A \mathbf{w}_j'\|^2 \]
- en: since the \(\mathbf{w}_j\)’s and \(\mathbf{w}'_j\)’s form an orthonormal basis
    of the same subspace \(\mathcal{W}\). Since \(\mathbf{w}'_k\) is orthogonal to
    \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\) by the conclusion of Step
    1, by the definition of \(\mathbf{v}_k\) as a solution to
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(\mathbf{w}_j\) 和 \(\mathbf{w}'_j\) 构成了相同子空间 \(\mathcal{W}\) 的正交基。由于 \(\mathbf{w}'_k\)
    与 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\) 正交，根据步骤 1 的结论，根据 \(\mathbf{v}_k\)
    作为解的定义
- en: \[ \mathbf{v}_k\in \arg\max \{\|A \mathbf{v}\|:\|\mathbf{v}\| = 1, \ \langle
    \mathbf{v}, \mathbf{v}_j \rangle = 0, \forall j \leq k-1\}. \]
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_k\in \arg\max \{\|A \mathbf{v}\|:\|\mathbf{v}\| = 1, \ \langle
    \mathbf{v}, \mathbf{v}_j \rangle = 0, \forall j \leq k-1\}. \]
- en: we have
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: \[ (*\!*\!*) \qquad \|A \mathbf{w}'_k\|^2 \leq \|A \mathbf{v}_k\|^2. \]
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (*\!*\!*) \qquad \|A \mathbf{w}'_k\|^2 \leq \|A \mathbf{v}_k\|^2. \]
- en: '***Step 3:*** Putting everything together'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 3:*** 将所有内容综合起来'
- en: \[\begin{align*} \sum_{j=1}^k \|A \mathbf{w}_j\|^2 &= \sum_{j=1}^k \|A \mathbf{w}_j'\|^2
    &\text{by $(**)$}\\ &= \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 + \|A \mathbf{w}'_k\|^2\\
    &\leq \sum_{j=1}^{k-1} \|A \mathbf{v}_j\|^2 + \|A \mathbf{v}_k\|^2 &\text{by $(*)$
    and $(*\!*\!*)$}\\ &= \sum_{j=1}^{k} \|A \mathbf{v}_j\|^2\\ \end{align*}\]
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{j=1}^k \|A \mathbf{w}_j\|^2 &= \sum_{j=1}^k \|A \mathbf{w}_j'\|^2
    &\text{根据 $(**)$}\\ &= \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 + \|A \mathbf{w}'_k\|^2\\
    &\leq \sum_{j=1}^{k-1} \|A \mathbf{v}_j\|^2 + \|A \mathbf{v}_k\|^2 &\text{根据 $(*)$
    和 $(*\!*\!*)$}\\ &= \sum_{j=1}^{k} \|A \mathbf{v}_j\|^2\\ \end{align*}\]
- en: which proves the claim. \(\square\)
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 证明了该命题。 \(\square\)
- en: '**Proof of Existence of the SVD** We return to the proof of the *Existence
    of SVD Theorem*. We give an alternative proof that does not rely on the *Spectral
    Theorem*.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解存在的证明** 我们回到**奇异值分解存在定理**的证明。我们给出一个不依赖于**谱定理**的替代证明。'
- en: '*Proof:* We have already done most of the work. The proof works as follows:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：** 我们已经做了大部分工作。证明如下：'
- en: (1) Compute the greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) from the
    *Greedy Finds Best Subspace Theorem* until the largest \(r\) such that
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 从**贪婪寻找最佳子空间定理**计算贪婪序列 \(\mathbf{v}_1,\ldots,\mathbf{v}_r\)，直到最大的 \(r\)，使得
- en: \[ \|A \mathbf{v}_r\|^2 > 0 \]
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{v}_r\|^2 > 0 \]
- en: or, otherwise, until \(r=m\). The \(\mathbf{v}_j\)’s are orthonormal by construction.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，否则，直到 \(r=m\)。根据构造，\(\mathbf{v}_j\) 是正交归一的。
- en: (2) For \(j=1,\ldots,r\), let
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 对于 \(j=1,\ldots,r\)，设
- en: \[ \sigma_j = \|A \mathbf{v}_j\| \quad\text{and}\quad \mathbf{u}_j = \frac{1}{\sigma_j}
    A \mathbf{v}_j. \]
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_j = \|A \mathbf{v}_j\| \quad\text{and}\quad \mathbf{u}_j = \frac{1}{\sigma_j}
    A \mathbf{v}_j. \]
- en: 'Observe that, by our choice of \(r\), the \(\sigma_j\)’s are \(> 0\). They
    are also non-increasing: by definition of the greedy sequence,'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于我们选择 \(r\) 的方式，\(\sigma_j\) 都是 \(> 0\)。它们也是非递增的：根据贪婪序列的定义，
- en: \[ \sigma_i^2 = \max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \ \langle
    \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}, \]
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_i^2 = \max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \ \langle
    \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}, \]
- en: where the set of orthogonality constraints gets larger as \(i\) increases. Hence,
    the \(\mathbf{u}_j\)’s have unit norm by definition.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，正交约束的集合随着 \(i\) 的增加而增大。因此，根据定义，\(\mathbf{u}_j\) 的范数为单位。
- en: We show below that they are also orthogonal.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面将证明它们也是正交的。
- en: (3) Let \(\mathbf{z} \in \mathbb{R}^m\) be any vector. To show that our construction
    is correct, we prove that \(A \mathbf{z} = \left(U \Sigma V^T\right)\mathbf{z}\).
    Let \(\mathcal{V} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\) and decompose
    \(\mathbf{z}\) into orthogonal components
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 设 \(\mathbf{z} \in \mathbb{R}^m\) 为任意向量。为了证明我们的构造是正确的，我们证明 \(A \mathbf{z}
    = \left(U \Sigma V^T\right)\mathbf{z}\)。设 \(\mathcal{V} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\)
    并将 \(\mathbf{z}\) 分解为正交分量
- en: \[ \mathbf{z} = \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) + (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}))
    = \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle \,\mathbf{v}_j + (\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \]
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z} = \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) + (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}))
    = \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle \,\mathbf{v}_j + (\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \]
- en: Applying \(A\) and using linearity, we get
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 \(A\) 并使用线性，我们得到
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, A\mathbf{v}_j + A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \end{align*}\]
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, A\mathbf{v}_j + A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \end{align*}\]
- en: We claim that \(A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})) = \mathbf{0}\).
    If \(\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) = \mathbf{0}\), that
    is certainly the case. If not, let
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们断言 \(A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})) = \mathbf{0}\)。如果
    \(\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) = \mathbf{0}\)，那么这当然成立。如果不成立，那么让
- en: \[ \mathbf{w} = \frac{\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})}{\|\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})\|}. \]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w} = \frac{\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})}{\|\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})\|}. \]
- en: By definition of \(r\),
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 \(r\) 的定义，
- en: \[ 0 = \max \{\|A \mathbf{w}_{r+1}\|^2 :\|\mathbf{w}_{r+1}\| = 1, \ \langle
    \mathbf{w}_{r+1}, \mathbf{v}_j \rangle = 0, \forall j \leq r\}. \]
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: \[ 0 = \max \{\|A \mathbf{w}_{r+1}\|^2 :\|\mathbf{w}_{r+1}\| = 1, \ \langle
    \mathbf{w}_{r+1}, \mathbf{v}_j \rangle = 0, \forall j \leq r\}. \]
- en: Put differently, \(\|A \mathbf{w}_{r+1}\|^2 = 0\) (i.e. \(A \mathbf{w}_{r+1}
    = \mathbf{0}\)), for any unit vector \(\mathbf{w}_{r+1}\) orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_r\).
    That applies in particular to \(\mathbf{w}_{r+1} = \mathbf{w}\) by the *Orthogonal
    Projection Theorem*.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，\(\|A \mathbf{w}_{r+1}\|^2 = 0\)（即 \(A \mathbf{w}_{r+1} = \mathbf{0}\)），对于任何与
    \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) 正交的单位向量 \(\mathbf{w}_{r+1}\)。这特别适用于 \(\mathbf{w}_{r+1}
    = \mathbf{w}\) 根据**正交投影定理**。
- en: Hence, using the definition of \(\mathbf{u}_j\) and \(\sigma_j\), we get
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用 \(\mathbf{u}_j\) 和 \(\sigma_j\) 的定义，我们得到
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, \sigma_j \mathbf{u}_j\\ &= \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
    \mathbf{z}\\ &=\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{z}\\
    &= \left(U \Sigma V^T\right)\mathbf{z}. \end{align*}\]
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, \sigma_j \mathbf{u}_j\\ &= \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
    \mathbf{z}\\ &=\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{z}\\
    &= \left(U \Sigma V^T\right)\mathbf{z}. \end{align*}\]
- en: That proves the existence of the SVD.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了奇异值分解的存在性。
- en: All that is left to prove is the orthogonality of the \(\mathbf{u}_j\)’s.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下要证明的只是 \(\mathbf{u}_j\) 的正交性。
- en: '**LEMMA** **(Left Singular Vectors are Orthogonal)** For all \(1 \leq i \neq
    j \leq r\), \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0\). \(\flat\)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(左奇异向量是正交的)** 对于所有 \(1 \leq i \neq j \leq r\)，\(\langle \mathbf{u}_i,
    \mathbf{u}_j \rangle = 0\)。\(\flat\)'
- en: '*Proof idea:* Quoting [BHK, Section 3.6]:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明思路**：引用 [BHK, 第 3.6 节]：'
- en: Intuitively if \(\mathbf{u}_i\) and \(\mathbf{u}_j\), \(i < j\), were not orthogonal,
    one would suspect that the right singular vector \(\mathbf{v}_j\) had a component
    of \(\mathbf{v}_i\) which would contradict that \(\mathbf{v}_i\) and \(\mathbf{v}_j\)
    were orthogonal. Let \(i\) be the smallest integer such that \(\mathbf{u}_i\)
    is not orthogonal to all other \(\mathbf{u}_j\). Then to prove that \(\mathbf{u}_i\)
    and \(\mathbf{u}_j\) are orthogonal, we add a small component of \(\mathbf{v}_j\)
    to \(\mathbf{v}_i\), normalize the result to be a unit vector \(\mathbf{v}'_i
    \propto \mathbf{v}_i + \varepsilon \mathbf{v}_j\) and show that \(\|A \mathbf{v}'_i\|
    > \|A \mathbf{v}_i\|\), a contradiction.
  id: totrans-393
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直观上，如果 \(\mathbf{u}_i\) 和 \(\mathbf{u}_j\)，\(i < j\)，不是正交的，人们会怀疑右奇异向量 \(\mathbf{v}_j\)
    有 \(\mathbf{v}_i\) 的一个分量，这将与 \(\mathbf{v}_i\) 和 \(\mathbf{v}_j\) 正交相矛盾。设 \(i\)
    为最小的整数，使得 \(\mathbf{u}_i\) 与所有其他 \(\mathbf{u}_j\) 不正交。然后为了证明 \(\mathbf{u}_i\)
    和 \(\mathbf{u}_j\) 是正交的，我们向 \(\mathbf{v}_i\) 添加一个小的 \(\mathbf{v}_j\) 分量，将结果归一化为一个单位向量
    \(\mathbf{v}'_i \propto \mathbf{v}_i + \varepsilon \mathbf{v}_j\)，并证明 \(\|A \mathbf{v}'_i\|
    > \|A \mathbf{v}_i\|\)，这是矛盾的。
- en: '*Proof:* We argue by contradiction. Let \(i\) be the smallest index such that
    there is a \(j > i\) with \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = \delta
    \neq 0\). Assume \(\delta > 0\) (otherwise use \(-\mathbf{u}_i\)). For \(\varepsilon
    \in (0,1)\), because the \(\mathbf{v}_k\)’s are orthonormal, \(\|\mathbf{v}_i
    + \varepsilon \mathbf{v}_j\|^2 = 1+\varepsilon^2\). Consider the vectors'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：我们通过反证法来论证。设 \(i\) 为最小的索引，使得存在 \(j > i\)，使得 \(\langle \mathbf{u}_i,
    \mathbf{u}_j \rangle = \delta \neq 0\)。假设 \(\delta > 0\)（否则使用 \(-\mathbf{u}_i\)）。对于
    \(\varepsilon \in (0,1)\)，因为 \(\mathbf{v}_k\) 是正交归一的，\(\|\mathbf{v}_i + \varepsilon
    \mathbf{v}_j\|^2 = 1+\varepsilon^2\)。考虑以下向量'
- en: \[ \mathbf{v}'_i = \frac{\mathbf{v}_i + \varepsilon \mathbf{v}_j}{\sqrt{1+\varepsilon^2}}
    \quad\text{and}\quad A \mathbf{v}'_i = \frac{\sigma_i \mathbf{u}_i + \varepsilon
    \sigma_j \mathbf{u}_j}{\sqrt{1+\varepsilon^2}}. \]
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}'_i = \frac{\mathbf{v}_i + \varepsilon \mathbf{v}_j}{\sqrt{1+\varepsilon^2}}
    \quad\text{和}\quad A \mathbf{v}'_i = \frac{\sigma_i \mathbf{u}_i + \varepsilon
    \sigma_j \mathbf{u}_j}{\sqrt{1+\varepsilon^2}}. \]
- en: Observe that \(\mathbf{v}'_i\) is orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{i-1}\),
    so that
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mathbf{v}'_i\) 与 \(\mathbf{v}_1,\ldots,\mathbf{v}_{i-1}\) 正交，因此
- en: \[ \|A \mathbf{v}'_i\| \leq \|A \mathbf{v}_i\| =\sigma_i. \]
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{v}'_i\| \leq \|A \mathbf{v}_i\| =\sigma_i. \]
- en: On the other hand, by the *Orthogonal Decomposition Lemma*, we can write \(A
    \mathbf{v}_i'\) as a sum of its orthogonal projection on the unit vector \(\mathbf{u}_i\)
    and \(A \mathbf{v}_i' - \mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\), which
    is orthogonal to \(\mathbf{u}_i\). In particular, by *Pythagoras*, \(\|A \mathbf{v}_i'\|
    \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|\). But that implies, for
    \(\varepsilon \in (0,1)\),
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，根据**正交分解引理**，我们可以将 \(A \mathbf{v}_i'\) 写成其在单位向量 \(\mathbf{u}_i\) 上的正交投影和
    \(A \mathbf{v}_i' - \mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\) 的和，后者与 \(\mathbf{u}_i\)
    正交。特别是，根据**毕达哥拉斯定理**，\(\|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A
    \mathbf{v}_i')\|\)。但这意味着，对于 \(\varepsilon \in (0,1)\)，
- en: \[ \|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|
    = \left\langle \mathbf{u}_i, A \mathbf{v}_i'\right\rangle = \frac{\sigma_i + \varepsilon
    \sigma_j \delta}{\sqrt{1+\varepsilon^2}} \geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2) \]
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|
    = \left\langle \mathbf{u}_i, A \mathbf{v}_i'\right\rangle = \frac{\sigma_i + \varepsilon
    \sigma_j \delta}{\sqrt{1+\varepsilon^2}} \geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2) \]
- en: where the second inequality follows from a [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series)
    or the observation
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第二个不等式来自于泰勒展开 [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series)
    或观察
- en: \[ (1+\varepsilon^2)\,(1-\varepsilon^2/2)^2 = (1+\varepsilon^2)\,(1-\varepsilon^2
    + \varepsilon^4/4) = 1 - 3/4 \varepsilon^4 + \varepsilon^6/4 \leq 1. \]
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (1+\varepsilon^2)\,(1-\varepsilon^2/2)^2 = (1+\varepsilon^2)\,(1-\varepsilon^2
    + \varepsilon^4/4) = 1 - 3/4 \varepsilon^4 + \varepsilon^6/4 \leq 1. \]
- en: Now note that
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请注意，
- en: \[\begin{align*} \|A \mathbf{v}_i'\| &\geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2)\\ &= \sigma_i + \varepsilon \sigma_j \delta - \varepsilon^2\sigma_i/2
    - \varepsilon^3 \sigma_i \sigma_j \delta/2\\ &= \sigma_i + \varepsilon \left(
    \sigma_j \delta - \varepsilon\sigma_i/2 - \varepsilon^2 \sigma_i \sigma_j \delta/2\right)\\
    &> \sigma_i \end{align*}\]
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|A \mathbf{v}_i'\| &\geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2)\\ &= \sigma_i + \varepsilon \sigma_j \delta - \varepsilon^2\sigma_i/2
    - \varepsilon^3 \sigma_i \sigma_j \delta/2\\ &= \sigma_i + \varepsilon \left(
    \sigma_j \delta - \varepsilon\sigma_i/2 - \varepsilon^2 \sigma_i \sigma_j \delta/2\right)\\
    &> \sigma_i \end{align*}\]
- en: for \(\varepsilon\) small enough, contradicting the inequality above. \(\square\)
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 对于足够小的 \(\varepsilon\)，与上述不等式矛盾。 \(\square\)
- en: '**SVD and Approximating Subspace** In constructing the SVD of \(A\), we used
    the greedy sequence for the best approximating subspace. Vice versa, given an
    SVD of \(A\), we can read off the solution to the approximating subspace problem.
    In other words, there was nothing special about the specific construction we used
    to prove existence of the SVD. While a matrix may have many SVDs, they all give
    a solution to the approximating subspace problems.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解（SVD）和逼近子空间** 在构造 \(A\) 的 SVD 时，我们使用了最佳逼近子空间的贪婪序列。反之，给定 \(A\) 的 SVD，我们可以读出逼近子空间问题的解。换句话说，我们用来证明
    SVD 存在的具体构造并没有什么特殊之处。虽然一个矩阵可能有多个 SVD，但它们都给出了逼近子空间问题的解。'
- en: Further, this perspective gives what is known as a variational characterization
    of the singular values. We will have more to say about variational characterizations
    and their uses in the next chapter.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种观点给出了奇异值的一个变分特征。我们将在下一章中更多地讨论变分特征及其应用。
- en: '*SVD and greedy sequence* Indeed, let'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVD 和贪婪序列** 事实上，设'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: be an SVD of \(A\) with
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(A\) 的 SVD 为
- en: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0. \]
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0. \]
- en: We show that the \(\mathbf{v}_j\)’s form a greedy sequence for the approximating
    subspace problem. Complete \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) into an orthonormal
    basis of \(\mathbb{R}^m\) by adding appropriate vectors \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\).
    By construction, \(A\mathbf{v}_{i} = \mathbf{0}\) for all \(i=j+1,\ldots,m\).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们证明 \(\mathbf{v}_j\) 形成了逼近子空间问题的贪婪序列。通过添加适当的向量 \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\)，将
    \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) 补充为 \(\mathbb{R}^m\) 的一个正交归一基。根据构造，对于所有 \(i=j+1,\ldots,m\)，有
    \(A\mathbf{v}_{i} = \mathbf{0}\)。
- en: We start with the case \(j=1\). For any unit vector \(\mathbf{w} \in \mathbb{R}^m\),
    we expand it as \(\mathbf{w} = \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle
    \,\mathbf{v}_i\). By the *Properties of Orthonormal Lists*,
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 \(j=1\) 的情况开始。对于任意单位向量 \(\mathbf{w} \in \mathbb{R}^m\)，我们将其展开为 \(\mathbf{w}
    = \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i\)。根据**正交归一列表的性质**，
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| A \left( \sum_{i=1}^m \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i \right) \right\|^2\\ &= \left\|
    \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\
    &= \left\| \sum_{i=1}^r \langle \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i
    \right\|^2\\ &= \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2,
    \end{align*}\]
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| A \left( \sum_{i=1}^m \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i \right) \right\|^2\\ &= \left\|
    \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\
    &= \left\| \sum_{i=1}^r \langle \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i
    \right\|^2\\ &= \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2,
    \end{align*}\]
- en: where we used the orthonormality of the \(\mathbf{u}_i\)’s and the fact that
    \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\) are in the null space of \(A\). Because
    the \(\sigma_i\)’s are non-increasing, this last sum is maximized by taking \(\mathbf{w}
    = \mathbf{v}_1\). So we have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\mathbf{u}_i\) 的正交归一性以及 \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\) 在
    \(A\) 的零空间中的事实。因为 \(\sigma_i\) 是非递增的，所以这个和通过取 \(\mathbf{w} = \mathbf{v}_1\) 来最大化。因此，我们已经证明了
    \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1\}\)。
- en: By the *Properties of Orthonormal Lists*,
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 根据正交归一列表的性质，
- en: \[ \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = \left\|\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle \mathbf{v}_i \right\|^2 = \|\mathbf{w}\|^2
    = 1. \]
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = \left\|\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle \mathbf{v}_i \right\|^2 = \|\mathbf{w}\|^2
    = 1. \]
- en: Hence, because the \(\sigma_i\)’s are non-increasing, the sum
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于 \(\sigma_i\) 是非递增的，和
- en: \[ \|A \mathbf{w}\|^2 = \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    \leq \sigma_1^2. \]
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}\|^2 = \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    \leq \sigma_1^2. \]
- en: This upper bound is achieved by taking \(\mathbf{w} = \mathbf{v}_1\). So we
    have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这个上界是通过取 \(\mathbf{w} = \mathbf{v}_1\) 来实现的。因此，我们已经证明了 \(\mathbf{v}_1 \in \arg\max\{\|A
    \mathbf{w}\|^2:\|\mathbf{w}\| = 1\}\)。
- en: More generally, for any unit vector \(\mathbf{w} \in \mathbb{R}^m\) that is
    orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{j-1}\), we expand it as \(\mathbf{w}
    = \sum_{i=j}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i\). Then,
    as long as \(j\leq r\),
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，对于任何与 \(\mathbf{v}_1,\ldots,\mathbf{v}_{j-1}\) 正交的单位向量 \(\mathbf{w} \in
    \mathbb{R}^m\)，我们将其展开为 \(\mathbf{w} = \sum_{i=j}^m \langle \mathbf{w}, \mathbf{v}_i\rangle
    \,\mathbf{v}_i\)。然后，只要 \(j\leq r\)，
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| \sum_{i=j}^m \langle \mathbf{w},
    \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\ &= \left\| \sum_{i=j}^r \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i \right\|^2\\ &= \sum_{i=j}^r
    \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2\\ &\leq \sigma_j^2. \end{align*}\]
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| \sum_{i=j}^m \langle \mathbf{w},
    \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\ &= \left\| \sum_{i=j}^r \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i \right\|^2\\ &= \sum_{i=j}^r
    \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2\\ &\leq \sigma_j^2. \end{align*}\]
- en: where again we used that the \(\sigma_i\)’s are non-increasing and \(\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = 1\). This last bound is achieved by
    taking \(\mathbf{w} = \mathbf{v}_j\).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 其中再次使用了 \(\sigma_i\) 是非递增的，并且 \(\sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    = 1\)。这个最后的界限是通过取 \(\mathbf{w} = \mathbf{v}_j\) 来实现的。
- en: So we have shown the following.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经证明了以下内容。
- en: '**THEOREM** **(Variational Characterization of Singular Values)** Let \(A =
    \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be an SVD of \(A\) with \(\sigma_1
    \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\). Then'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(奇异值的变分特征)** 设 \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)
    为 \(A\) 的奇异值分解，其中 \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\)。那么'
- en: \[ \sigma_j^2 = \max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle \mathbf{w},
    \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}, \]
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_j^2 = \max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle \mathbf{w},
    \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}, \]
- en: and
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbf{v}_j\in \arg\max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle
    \mathbf{w}, \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}. \]
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_j\in \arg\max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle
    \mathbf{w}, \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}. \]
- en: \(\sharp\)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: 4.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8.1\. 测验、解答、代码等.[#](#quizzes-solutions-code-etc "链接到本标题")
- en: 4.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.1\. 仅代码[#](#just-the-code "链接到本标题")
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 下面可以访问本章代码的交互式 Jupyter 笔记本（推荐使用 Google Colab）。鼓励您对其进行尝试。一些建议的计算练习散布在其中。笔记本也可以作为幻灯片查看。
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[幻灯片](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
- en: 4.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.2\. 自我评估测验[#](#self-assessment-quizzes "链接到本标题")
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下链接可以获取更全面的自我评估测验的网页版本。
- en: '[Section 4.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 4.2 节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
- en: '[Section 4.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 4.3 节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
- en: '[Section 4.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 4.4 节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
- en: '[Section 4.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 4.5 节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
- en: '[Section 4.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第 4.6 节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
- en: 4.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.3\. 自动测验[#](#auto-quizzes "链接到这个标题")
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在此处访问本章的自动生成的测验（推荐使用 Google Colab）。
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动测验](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
- en: 4.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.4\. 奇数练习题的解答[#](#solutions-to-odd-numbered-warm-up-exercises "链接到这个标题")
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '*(在 Claude、Gemini 和 ChatGPT 的帮助下)*'
- en: '**E4.2.1** The answer is \(\mathrm{rk}(A) = 2\). To see this, observe that
    the third column is the sum of the first two columns, so the column space is spanned
    by the first two columns. These two columns are linearly independent, so the dimension
    of the column space (i.e., the rank) is 2.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.1** 答案是 \(\mathrm{rk}(A) = 2\)。要看到这一点，观察第三列是前两列的和，所以列空间由前两列张成。这两列是线性无关的，所以列空间的维度（即秩）是
    2。'
- en: '**E4.2.3** The eigenvalues are \(\lambda_1 = 4\) and \(\lambda_2 = 2\). For
    \(\lambda_1 = 4\), solving \((A - 4I)\mathbf{x} = \mathbf{0}\) gives the eigenvector
    \(\mathbf{v}_1 = (1, 1)\). For \(\lambda_2 = 2\), solving \((A - 2I)\mathbf{x}
    = \mathbf{0}\) gives the eigenvector \(\mathbf{v}_2 = (1, -1)\).'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.3** 特征值是 \(\lambda_1 = 4\) 和 \(\lambda_2 = 2\)。对于 \(\lambda_1 = 4\)，解
    \((A - 4I)\mathbf{x} = \mathbf{0}\) 得到特征向量 \(\mathbf{v}_1 = (1, 1)\)。对于 \(\lambda_2
    = 2\)，解 \((A - 2I)\mathbf{x} = \mathbf{0}\) 得到特征向量 \(\mathbf{v}_2 = (1, -1)\)。'
- en: '**E4.2.5** Normalizing the eigenvectors to get an orthonormal basis: \(\mathbf{q}_1
    = \frac{1}{\sqrt{2}}(1, 1)\) and \(\mathbf{q}_2 = \frac{1}{\sqrt{2}}(1, -1)\).
    Then \(A = Q \Lambda Q^T\) where \(Q = (\mathbf{q}_1, \mathbf{q}_2)\) and \(\Lambda
    = \mathrm{diag}(4, 2)\). Explicitly,'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.5** 将特征向量归一化以获得正交基：\(\mathbf{q}_1 = \frac{1}{\sqrt{2}}(1, 1)\) 和 \(\mathbf{q}_2
    = \frac{1}{\sqrt{2}}(1, -1)\)。然后 \(A = Q \Lambda Q^T\) 其中 \(Q = (\mathbf{q}_1,
    \mathbf{q}_2)\) 和 \(\Lambda = \mathrm{diag}(4, 2)\)。具体来说，'
- en: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
- en: '**E4.2.7**'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.7**'
- en: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
- en: '**E4.2.9** The rank of \(A\) is 1\. The second column is a multiple of the
    first column, so the column space is one-dimensional.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.9** \(A\) 的秩是 1。第二列是第一列的倍数，所以列空间是一维的。'
- en: '**E4.2.11** The characteristic polynomial of \(A\) is'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.11** \(A\) 的特征多项式是'
- en: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
- en: Hence, the eigenvalues are \(\lambda_1 = 1\) and \(\lambda_2 = 3\). For \(\lambda_1
    = 1\), we solve \((A - I)\mathbf{v} = \mathbf{0}\) to get \(\mathbf{v}_1 = \begin{pmatrix}
    1 \\ -1 \end{pmatrix}\). For \(\lambda_2 = 3\), we solve \((A - 3I)\mathbf{v}
    = 0\) to get \(\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，特征值为 \(\lambda_1 = 1\) 和 \(\lambda_2 = 3\)。对于 \(\lambda_1 = 1\)，我们解 \((A
    - I)\mathbf{v} = \mathbf{0}\) 得到 \(\mathbf{v}_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\)。对于
    \(\lambda_2 = 3\)，我们解 \((A - 3I)\mathbf{v} = 0\) 得到 \(\mathbf{v}_2 = \begin{pmatrix}
    1 \\ 1 \end{pmatrix}\)。
- en: '**E4.2.13** The columns of \(A\) are linearly dependent, since the second column
    is twice the first column. Hence, a basis for the column space of \(A\) is given
    by \(\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix} \right\}\).'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.13** \(A\) 的列是线性相关的，因为第二列是第一列的两倍。因此，\(A\) 的列空间的基由 \(\left\{ \begin{pmatrix}
    1 \\ 2 \end{pmatrix} \right\}\) 给出。'
- en: '**E4.2.15** The eigenvalues of \(A\) are \(\lambda_1 = 3\) and \(\lambda_2
    = -1\). Since \(A\) has a negative eigenvalue, it is not positive semidefinite.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.15** \(A\) 的特征值为 \(\lambda_1 = 3\) 和 \(\lambda_2 = -1\)。由于 \(A\) 有一个负特征值，它不是正半定。'
- en: '**E4.2.17** The Hessian of \(f\) is'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.17** \(f\) 的Hessian矩阵是'
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
- en: The eigenvalues of the Hessian are \(\lambda_1 = 2\) and \(\lambda_2 = -2\).
    Since one eigenvalue is negative, the Hessian is not positive semidefinite, and
    \(f\) is not convex.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵的特征值为 \(\lambda_1 = 2\) 和 \(\lambda_2 = -2\)。由于有一个特征值是负的，Hessian矩阵不是正半定的，且
    \(f\) 不是凸函数。
- en: '**E4.2.19** The Hessian of \(f\) is'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.19** \(f\) 的Hessian矩阵是'
- en: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
- en: The eigenvalues of the Hessian are \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) and
    \(\lambda_2 = 0\), which are both nonnegative for all \(x, y\). Therefore, the
    Hessian is positive semidefinite, and \(f\) is convex.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵的特征值为 \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) 和 \(\lambda_2 = 0\)，对于所有的
    \(x, y\) 都是非负的。因此，Hessian矩阵是正半定的，且 \(f\) 是凸函数。
- en: '**E4.3.1** We have \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\). By
    the method described in the text, \(w_1\) is a unit eigenvector of \(A^TA = \begin{pmatrix}
    5 & 0 \\ 0 & 5 \end{pmatrix}\) corresponding to the largest eigenvalue. Thus,
    we can take \(\mathbf{w}_1 = (1, 0)\) or \(\mathbf{w}_1 = (0, 1)\).'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.1** 我们有 \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\)。按照文中描述的方法，\(w_1\)
    是 \(A^TA = \begin{pmatrix} 5 & 0 \\ 0 & 5 \end{pmatrix}\) 对应于最大特征值的单位特征向量。因此，我们可以取
    \(\mathbf{w}_1 = (1, 0)\) 或 \(\mathbf{w}_1 = (0, 1)\)。'
- en: '**E4.3.3** We can take \(U = I_2\), \(\Sigma = A\), and \(V = I_2\). This is
    an SVD of \(A\) because \(U\) and \(V\) are orthogonal and \(\Sigma\) is diagonal.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.3** 我们可以取 \(U = I_2\)，\(\Sigma = A\)，和 \(V = I_2\)。这是一个 \(A\) 的奇异值分解，因为
    \(U\) 和 \(V\) 是正交的，且 \(\Sigma\) 是对角矩阵。'
- en: '**E4.3.5** We have \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\).
    The characteristic polynomial of \(A^TA\) is \(\lambda^2 - 25\lambda = \lambda(\lambda
    - 25)\), so the eigenvalues are \(\lambda_1 = 25\) and \(\lambda_2 = 0\). An eigenvector
    corresponding to \(\lambda_1\) is \((1, 2)\), and an eigenvector corresponding
    to \(\lambda_2\) is \((-2, 1)\).'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.5** 我们有 \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\)。\(A^TA\)
    的特征多项式是 \(\lambda^2 - 25\lambda = \lambda(\lambda - 25)\)，所以特征值是 \(\lambda_1 =
    25\) 和 \(\lambda_2 = 0\)。对应于 \(\lambda_1\) 的一个特征向量是 \((1, 2)\)，对应于 \(\lambda_2\)
    的一个特征向量是 \((-2, 1)\)。'
- en: '**E4.3.7** We have that the singular values of \(A\) are \(\sigma_1 = \sqrt{25}
    = 5\) and \(\sigma_2 = 0\). We can take \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1 = (1/\sqrt{5}, 2/\sqrt{5})\). Since
    the rank of \(A\) is 1, this gives a compact SVD of \(A\): \(A = U \Sigma V^T\)
    with \(U = \mathbf{u}_1\), \(\Sigma = (5)\), and \(V = \mathbf{v}_1\).'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.7** 我们有 \(A\) 的奇异值为 \(\sigma_1 = \sqrt{25} = 5\) 和 \(\sigma_2 = 0\)。我们可以取
    \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\) 和 \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1
    = (1/\sqrt{5}, 2/\sqrt{5})\)。由于 \(A\) 的秩为 1，这给出了 \(A\) 的紧凑奇异值分解：\(A = U \Sigma
    V^T\)，其中 \(U = \mathbf{u}_1\)，\(\Sigma = (5)\)，和 \(V = \mathbf{v}_1\)。'
- en: '**E4.3.9** From the full SVD of \(A\), we have that: An orthonormal basis for
    \(\mathrm{col}(A)\) is given by the first column of \(U\): \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\).
    An orthonormal basis for \(\mathrm{row}(A)\) is given by the first column of \(V\):
    \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\). An orthonormal basis for \(\mathrm{null}(A)\)
    is given by the second column of \(V\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\). An
    orthonormal basis for \(\mathrm{null}(A^T)\) is given by the second column of
    \(U\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\).'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.9** 从 \(A\) 的完全奇异值分解中，我们有：\(\mathrm{col}(A)\) 的一个正交基由 \(U\) 的第一列给出：\(\{(1/\sqrt{5},
    2/\sqrt{5})\}\)。\(\mathrm{row}(A)\) 的一个正交基由 \(V\) 的第一列给出：\(\{(1/\sqrt{5}, 2/\sqrt{5})\}\)。\(\mathrm{null}(A)\)
    的一个正交基由 \(V\) 的第二列给出：\(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\)。\(\mathrm{null}(A^T)\)
    的一个正交基由 \(U\) 的第二列给出：\(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\)。'
- en: '**E4.3.11** From its diagonal form, we see that \(\lambda_1 = 15\), \(\mathbf{q}_1
    = (1, 0)\); \(\lambda_2 = 7\), \(\mathbf{q}_2 = (0, 1)\).'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.11** 从其对角形式中，我们可以看到 \(\lambda_1 = 15\)，\(\mathbf{q}_1 = (1, 0)\)；\(\lambda_2
    = 7\)，\(\mathbf{q}_2 = (0, 1)\)。'
- en: '**E4.3.13** By direct computation, \(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix}
    1 & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.13** 通过直接计算，\(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix} 1
    & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
- en: '**E4.3.15** By direct computation,'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.15** 通过直接计算，'
- en: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
- en: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
- en: '**E4.3.17** The best approximating subspace of dimension \(k=1\) is the line
    spanned by the vector \(\mathbf{v}_1 = (1, 0)\). The sum of squared distances
    to this subspace is \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 =
    5 + 5 + 2 + 10 - 15 = 7.\)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.17** 维度为 \(k=1\) 的最佳逼近子空间是由向量 \(\mathbf{v}_1 = (1, 0)\) 张成的直线。到该子空间的平方距离之和为
    \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 = 5 + 5 + 2 + 10 - 15
    = 7.\)'
- en: '**E4.4.1**'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.1**'
- en: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
- en: The diagonal entries are being raised to increasing powers, while the off-diagonal
    entries remain zero.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线元素正在被提高到递增的幂次，而非对角线元素保持为零。
- en: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
- en: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
- en: '**E4.4.7** We have \(A = Q \Lambda Q^T\), where \(Q\) is the matrix of normalized
    eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues. Then,'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.7** 我们有 \(A = Q \Lambda Q^T\)，其中 \(Q\) 是归一化特征向量的矩阵，\(\Lambda\) 是特征值的对角矩阵。然后，'
- en: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
- en: and similarly, \(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，\(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\)。
- en: '**E4.4.9** We have \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\).
    Let’s find the eigenvalues of the matrix \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 &
    5 \end{pmatrix}\). We solve the characteristic equation \(\det(A^TA - \lambda
    I) = 0\). First, let’s calculate the characteristic polynomial: \(\det(A^TA -
    \lambda I) = \det(\begin{pmatrix} 1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})=
    \lambda^2 - 6\lambda + 1\), so \((\lambda - 3)^2 - 8 = 0\) or \((\lambda - 3)^2
    = 8\) or \(\lambda = 3 \pm 2\sqrt{2}\). Therefore, the eigenvalues of \(A^TA\)
    are: \(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) and \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\). Hence, this matrix is positive semidefinite because its eigenvalues
    are 0 and 6, both of which are non-negative.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.9** 我们有 \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\). 让我们找到矩阵
    \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\) 的特征值。我们解特征方程 \(\det(A^TA
    - \lambda I) = 0\)。首先，让我们计算特征多项式：\(\det(A^TA - \lambda I) = \det(\begin{pmatrix}
    1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})= \lambda^2 - 6\lambda + 1\)，所以 \((\lambda
    - 3)^2 - 8 = 0\) 或 \((\lambda - 3)^2 = 8\) 或 \(\lambda = 3 \pm 2\sqrt{2}\)。因此，\(A^TA\)
    的特征值为：\(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) 和 \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\)。因此，这个矩阵是正半定的，因为它的特征值是 0 和 6，都是非负的。'
- en: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). The unit
    norm constraint requires that \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). 单位范数约束要求
    \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
- en: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\).
    The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\)，\(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\)。得分计算为
    \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\)。'
- en: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\). Uncorrelatedness requires that \(\frac{1}{n-1}\sum_{i=1}^n
    t_{i1}t_{i2} = 0\).'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\)。不相关性要求 \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = 0\)。'
- en: '**E4.5.7** First, compute the mean of each column:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.7** 首先，计算每一列的平均值：'
- en: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
- en: 'Mean-centered data matrix:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 均值中心化的数据矩阵：
- en: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
- en: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\), \(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\). The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\)，\(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\)，\(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\)。得分计算为 \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\)。'
- en: '**E4.5.11** The second principal component must be uncorrelated with the first,
    so its loading vector must be orthogonal to \(\varphi_1\). One such vector is
    \(\varphi_2 = (-0.6, 0.8)\).'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.11** 第二个主成分必须与第一个不相关，因此其载荷向量必须与 \(\varphi_1\) 正交。这样一个向量是 \(\varphi_2
    = (-0.6, 0.8)\)。'
- en: '**E4.6.1** The Frobenius norm is given by:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.1** 弗罗贝尼乌斯范数由以下公式给出：'
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
- en: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\).'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\)。'
- en: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\).'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\)。'
- en: '**E4.6.7** The induced 2-norm of the difference between a matrix and its rank-1
    truncated SVD is equal to the second singular value. Therefore, using the SVD
    from before, we have \(\|A - A_1\|_2 = \sigma_2 = 0\).'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.7** 矩阵与其秩-1截断SVD之间的诱导2-范数等于第二个奇异值。因此，使用之前的SVD，我们有 \(\|A - A_1\|_2 =
    \sigma_2 = 0\)。'
- en: '**E4.6.9** The matrix \(A\) is singular (its determinant is zero), so it doesn’t
    have an inverse. Therefore, \(\|A^{-1}\|_2\) is undefined. Note that the induced
    2-norm of the pseudoinverse \(A^+\) is not the same as the induced 2-norm of the
    inverse (when it exists).'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.9** 矩阵 \(A\) 是奇异的（其行列式为零），因此它没有逆。因此，\(\|A^{-1}\|_2\) 是未定义的。请注意，伪逆 \(A^+\)
    的诱导2-范数与逆的诱导2-范数（当它存在时）不同。'
- en: 4.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  id: totrans-499
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.5\. 学习成果[#](#learning-outcomes "链接到这个标题")
- en: Define the column space, row space, and rank of a matrix.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的列空间、行空间和秩。
- en: Prove that the row rank equals the column rank.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明行秩等于列秩。
- en: Apply properties of matrix rank, such as \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\)
    and \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用矩阵秩的性质，例如 \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\) 和 \(\mathrm{rk}(A+B) \leq
    \mathrm{rk}(A) + \mathrm{rk}(B)\)。
- en: State and prove the Rank-Nullity Theorem.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述并证明秩-零度定理。
- en: Define eigenvalues and eigenvectors of a square matrix.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义方阵的特征值和特征向量。
- en: Prove that a symmetric matrix has at most d distinct eigenvalues, where d is
    the matrix size.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明对称矩阵最多有d个不同的特征值，其中d是矩阵的大小。
- en: State the Spectral Theorem for symmetric matrices and explain its implications.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述对称矩阵的谱定理及其含义。
- en: Write the spectral decomposition of a symmetric matrix using outer products.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外积写出对称矩阵的谱分解。
- en: Determine whether a symmetric matrix is positive semidefinite or positive definite
    based on its eigenvalues.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据其特征值确定对称矩阵是否为正半定或正定。
- en: Compute eigenvalues and eigenvectors of symmetric matrices using programming
    tools like NumPy’s `linalg.eig`.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编程工具（如NumPy的 `linalg.eig`）计算对称矩阵的特征值和特征向量。
- en: State the objective of the best approximating subspace problem and formulate
    it mathematically as a minimization of the sum of squared distances.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述最佳逼近子空间问题的目标，并将其数学上表述为平方距离之和的最小化。
- en: Prove that the best approximating subspace problem can be solved greedily by
    finding the best one-dimensional subspace, then the best one-dimensional subspace
    orthogonal to the first, and so on.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明最佳逼近子空间问题可以通过贪婪地找到最佳一维子空间，然后是第一个子空间正交的最佳一维子空间，以此类推，来贪婪地解决。
- en: Define the singular value decomposition (SVD) of a matrix and describe the properties
    of the matrices involved.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的奇异值分解（SVD）并描述涉及矩阵的性质。
- en: Prove the existence of the SVD for any real matrix using the Spectral Theorem.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用谱定理证明任何实矩阵SVD的存在性。
- en: Explain the connection between the SVD of a matrix \(A\) and the spectral decompositions
    of \(A^T A\) and \(A A^T\).
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释矩阵 \(A\) 的SVD与 \(A^T A\) 和 \(A A^T\) 的谱分解之间的联系。
- en: Compute the SVD of simple matrices.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算简单矩阵的SVD。
- en: Use the SVD of the data matrix to find the best k-dimensional approximating
    subspace to a set of data points.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据矩阵的SVD来找到一组数据点的最佳k维逼近子空间。
- en: Interpret the singular values as capturing the contributions of the right singular
    vectors to the fit of the approximating subspace.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释奇异值作为捕捉右奇异向量对逼近子空间拟合贡献的表示。
- en: Obtain low-dimensional representations of data points using the truncated SVD.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用截断奇异值分解（SVD）来获得数据点的低维表示。
- en: Distinguish between full and compact forms of the SVD and convert between them.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分SVD的完整形式和紧致形式，并在它们之间进行转换。
- en: State the key lemma for power iteration in the positive semidefinite case and
    explain why it implies convergence to the top eigenvector.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述正半定情况下幂迭代的键引理，并解释为什么它意味着收敛到最大特征向量。
- en: Extend the power iteration lemma to the general case of singular value decomposition
    (SVD) and justify why repeated multiplication of A^T A with a random vector converges
    to the top right singular vector.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将幂迭代引理扩展到奇异值分解（SVD）的一般情况，并证明为什么重复乘以随机向量的 \(A^T A\) 会收敛到右上奇异向量。
- en: Compute the corresponding top singular value and left singular vector given
    the converged top right singular vector.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据收敛的右上奇异向量计算相应的最大奇异值和左奇异向量。
- en: Describe the orthogonal iteration method for finding additional singular vectors
    beyond the top one.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述用于找到除第一个奇异向量之外的其他奇异向量的正交迭代法。
- en: Apply the power iteration method and orthogonal iteration to compute the SVD
    of a given matrix and use it to find the best low-dimensional subspace approximation.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用幂迭代法和正交迭代法来计算给定矩阵的SVD，并使用它来找到最佳低维子空间逼近。
- en: Implement the power iteration method and orthogonal iteration in Python.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中实现幂迭代法和正交迭代法。
- en: Define principal components and loadings in the context of PCA.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PCA的背景下定义主成分和载荷。
- en: Express the objective of PCA as a constrained optimization problem.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将PCA的目标表达为约束优化问题。
- en: Establish the connection between PCA and singular value decomposition (SVD).
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立主成分分析（PCA）与奇异值分解（SVD）之间的联系。
- en: Implement PCA using an SVD algorithm.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVD算法实现PCA。
- en: Interpret the results of PCA in the context of dimensionality reduction and
    data visualization.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在降维和数据可视化的背景下解释PCA的结果。
- en: Define the Frobenius norm and the induced 2-norm of a matrix.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的Frobenius范数和诱导2-范数。
- en: Express the Frobenius norm and the induced 2-norm of a matrix in terms of its
    singular values.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用矩阵的奇异值表示矩阵的Frobenius范数和诱导2-范数。
- en: State the Eckart-Young theorem.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述Eckart-Young定理。
- en: Define the pseudoinverse of a matrix using the SVD.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用奇异值分解定义矩阵的伪逆。
- en: Apply the pseudoinverse to solve least squares problems when the matrix has
    full column rank.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当矩阵具有满列秩时，应用伪逆来解决最小二乘问题。
- en: Apply the pseudoinverse to find the least norm solution for underdetermined
    systems with full row rank.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当矩阵具有满行秩时，应用伪逆来找到欠定系统的最小范数解。
- en: Formulate the ridge regression problem as a regularized optimization problem.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将岭回归问题表述为正则化优化问题。
- en: Explain how ridge regression works by analyzing its solution in terms of the
    SVD of the design matrix.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分析设计矩阵的SVD来解释岭回归的工作原理。
- en: \(\aleph\)
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: \(\aleph\)
- en: 4.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.1\. 仅代码[#](#just-the-code "链接到本标题")
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以访问的交互式Jupyter笔记本，其中包含本章的代码（推荐使用Google Colab）。鼓励您对其进行实验。一些建议的计算练习散布在其中。笔记本也可以作为幻灯片查看。
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb)
    ([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_svd_notebook.ipynb))'
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[幻灯片](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_svd_notebook_slides.slides.html)'
- en: 4.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.2\. 自我评估测验[#](#self-assessment-quizzes "链接到本标题")
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下链接可以获得更全面的自我评估测验的网页版本。
- en: '[Section 4.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.2节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_2.html)'
- en: '[Section 4.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.3节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_3.html)'
- en: '[Section 4.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.4节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_4.html)'
- en: '[Section 4.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.5节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_5.html)'
- en: '[Section 4.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4.6节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_4_6.html)'
- en: 4.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  id: totrans-551
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.3\. 自动测验[#](#auto-quizzes "链接到本标题")
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的自动生成的测验可以在此处访问（推荐使用 Google Colab）。
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动测验](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb)
    ([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-svd-autoquiz.ipynb))'
- en: 4.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  id: totrans-554
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.4\. 奇数编号的预热练习的解答[#](#solutions-to-odd-numbered-warm-up-exercises "链接到这个标题")
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '*(在 Claude、Gemini 和 ChatGPT 的帮助下)*'
- en: '**E4.2.1** The answer is \(\mathrm{rk}(A) = 2\). To see this, observe that
    the third column is the sum of the first two columns, so the column space is spanned
    by the first two columns. These two columns are linearly independent, so the dimension
    of the column space (i.e., the rank) is 2.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.1** 答案是 \(\mathrm{rk}(A) = 2\)。为了看到这一点，观察第三列是前两列的和，所以列空间由前两列张成。这两列是线性无关的，所以列空间的维度（即秩）是
    2。'
- en: '**E4.2.3** The eigenvalues are \(\lambda_1 = 4\) and \(\lambda_2 = 2\). For
    \(\lambda_1 = 4\), solving \((A - 4I)\mathbf{x} = \mathbf{0}\) gives the eigenvector
    \(\mathbf{v}_1 = (1, 1)\). For \(\lambda_2 = 2\), solving \((A - 2I)\mathbf{x}
    = \mathbf{0}\) gives the eigenvector \(\mathbf{v}_2 = (1, -1)\).'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.3** 特征值为 \(\lambda_1 = 4\) 和 \(\lambda_2 = 2\)。对于 \(\lambda_1 = 4\)，解
    \((A - 4I)\mathbf{x} = \mathbf{0}\) 得到特征向量 \(\mathbf{v}_1 = (1, 1)\)。对于 \(\lambda_2
    = 2\)，解 \((A - 2I)\mathbf{x} = \mathbf{0}\) 得到特征向量 \(\mathbf{v}_2 = (1, -1)\)。'
- en: '**E4.2.5** Normalizing the eigenvectors to get an orthonormal basis: \(\mathbf{q}_1
    = \frac{1}{\sqrt{2}}(1, 1)\) and \(\mathbf{q}_2 = \frac{1}{\sqrt{2}}(1, -1)\).
    Then \(A = Q \Lambda Q^T\) where \(Q = (\mathbf{q}_1, \mathbf{q}_2)\) and \(\Lambda
    = \mathrm{diag}(4, 2)\). Explicitly,'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.5** 将特征向量正交化以得到一个正交基：\(\mathbf{q}_1 = \frac{1}{\sqrt{2}}(1, 1)\) 和 \(\mathbf{q}_2
    = \frac{1}{\sqrt{2}}(1, -1)\)。然后 \(A = Q \Lambda Q^T\) 其中 \(Q = (\mathbf{q}_1,
    \mathbf{q}_2)\) 和 \(\Lambda = \mathrm{diag}(4, 2)\)。具体地，'
- en: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix} \begin{pmatrix} 4 & 0\\
    0 & 2 \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}. \end{split}\]
- en: '**E4.2.7**'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.7**'
- en: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}\mathbf{v}^T = \begin{pmatrix} 1\\ 2\\ 3 \end{pmatrix}
    (4, 5) = \begin{pmatrix} 4 & 5\\ 8 & 10\\ 12 & 15 \end{pmatrix}. \end{split}\]
- en: '**E4.2.9** The rank of \(A\) is 1\. The second column is a multiple of the
    first column, so the column space is one-dimensional.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.9** \(A\) 的秩为 1。第二列是第一列的倍数，所以列空间是一维的。'
- en: '**E4.2.11** The characteristic polynomial of \(A\) is'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.11** \(A\) 的特征多项式是'
- en: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \det(A - \lambda I) = (2 - \lambda)^2 - 1 = \lambda^2 - 4 \lambda + 3 = (\lambda
    - 1)(\lambda - 3). \]
- en: Hence, the eigenvalues are \(\lambda_1 = 1\) and \(\lambda_2 = 3\). For \(\lambda_1
    = 1\), we solve \((A - I)\mathbf{v} = \mathbf{0}\) to get \(\mathbf{v}_1 = \begin{pmatrix}
    1 \\ -1 \end{pmatrix}\). For \(\lambda_2 = 3\), we solve \((A - 3I)\mathbf{v}
    = 0\) to get \(\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\).
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，特征值为 \(\lambda_1 = 1\) 和 \(\lambda_2 = 3\)。对于 \(\lambda_1 = 1\)，我们解 \((A
    - I)\mathbf{v} = \mathbf{0}\) 得到 \(\mathbf{v}_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\)。对于
    \(\lambda_2 = 3\)，我们解 \((A - 3I)\mathbf{v} = 0\) 得到 \(\mathbf{v}_2 = \begin{pmatrix}
    1 \\ 1 \end{pmatrix}\)。
- en: '**E4.2.13** The columns of \(A\) are linearly dependent, since the second column
    is twice the first column. Hence, a basis for the column space of \(A\) is given
    by \(\left\{ \begin{pmatrix} 1 \\ 2 \end{pmatrix} \right\}\).'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.13** \(A\) 的列向量线性相关，因为第二列是第一列的两倍。因此，\(A\) 的列空间的一个基是 \(\left\{ \begin{pmatrix}
    1 \\ 2 \end{pmatrix} \right\}\)。'
- en: '**E4.2.15** The eigenvalues of \(A\) are \(\lambda_1 = 3\) and \(\lambda_2
    = -1\). Since \(A\) has a negative eigenvalue, it is not positive semidefinite.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.15** \(A\) 的特征值为 \(\lambda_1 = 3\) 和 \(\lambda_2 = -1\)。由于 \(A\) 有一个负特征值，它不是正半定的。'
- en: '**E4.2.17** The Hessian of \(f\) is'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.17** 函数 \(f\) 的 Hessian 矩阵是'
- en: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla^2 f(x, y) = \begin{pmatrix} 2 & 0\\ 0 & -2 \end{pmatrix}.
    \end{split}\]
- en: The eigenvalues of the Hessian are \(\lambda_1 = 2\) and \(\lambda_2 = -2\).
    Since one eigenvalue is negative, the Hessian is not positive semidefinite, and
    \(f\) is not convex.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵的特征值为 \(\lambda_1 = 2\) 和 \(\lambda_2 = -2\)。由于有一个特征值是负的，Hessian矩阵不是正半定的，且
    \(f\) 不是凸函数。
- en: '**E4.2.19** The Hessian of \(f\) is'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.2.19** \(f\) 的Hessian矩阵是'
- en: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla^2 f(x, y) = \frac{1}{(x^2 + y^2 + 1)^2} \begin{pmatrix}
    2y^2 - 2x^2 + 2 & -4xy\\ -4xy & 2x^2 - 2y^2 + 2 \end{pmatrix}. \end{split}\]
- en: The eigenvalues of the Hessian are \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) and
    \(\lambda_2 = 0\), which are both nonnegative for all \(x, y\). Therefore, the
    Hessian is positive semidefinite, and \(f\) is convex.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵的特征值为 \(\lambda_1 = \frac{4}{x^2 + y^2 + 1}\) 和 \(\lambda_2 = 0\)，对于所有
    \(x, y\) 都是正的。因此，Hessian矩阵是正半定的，且 \(f\) 是凸函数。
- en: '**E4.3.1** We have \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\). By
    the method described in the text, \(w_1\) is a unit eigenvector of \(A^TA = \begin{pmatrix}
    5 & 0 \\ 0 & 5 \end{pmatrix}\) corresponding to the largest eigenvalue. Thus,
    we can take \(\mathbf{w}_1 = (1, 0)\) or \(\mathbf{w}_1 = (0, 1)\).'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.1** 我们有 \(A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix}\)。按照文中描述的方法，\(w_1\)
    是 \(A^TA = \begin{pmatrix} 5 & 0 \\ 0 & 5 \end{pmatrix}\) 对应于最大特征值的单位特征向量。因此，我们可以取
    \(\mathbf{w}_1 = (1, 0)\) 或 \(\mathbf{w}_1 = (0, 1)\)。'
- en: '**E4.3.3** We can take \(U = I_2\), \(\Sigma = A\), and \(V = I_2\). This is
    an SVD of \(A\) because \(U\) and \(V\) are orthogonal and \(\Sigma\) is diagonal.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.3** 我们可以取 \(U = I_2\), \(\Sigma = A\), 和 \(V = I_2\)。这是一个 \(A\) 的奇异值分解，因为
    \(U\) 和 \(V\) 是正交的，且 \(\Sigma\) 是对角的。'
- en: '**E4.3.5** We have \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\).
    The characteristic polynomial of \(A^TA\) is \(\lambda^2 - 25\lambda = \lambda(\lambda
    - 25)\), so the eigenvalues are \(\lambda_1 = 25\) and \(\lambda_2 = 0\). An eigenvector
    corresponding to \(\lambda_1\) is \((1, 2)\), and an eigenvector corresponding
    to \(\lambda_2\) is \((-2, 1)\).'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.5** 我们有 \(A^TA = \begin{pmatrix} 5 & 10 \\ 10 & 20 \end{pmatrix}\)。\(A^TA\)
    的特征多项式是 \(\lambda^2 - 25\lambda = \lambda(\lambda - 25)\)，所以特征值是 \(\lambda_1 =
    25\) 和 \(\lambda_2 = 0\)。对应于 \(\lambda_1\) 的一个特征向量是 \((1, 2)\)，对应于 \(\lambda_2\)
    的一个特征向量是 \((-2, 1)\)。'
- en: '**E4.3.7** We have that the singular values of \(A\) are \(\sigma_1 = \sqrt{25}
    = 5\) and \(\sigma_2 = 0\). We can take \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1 = (1/\sqrt{5}, 2/\sqrt{5})\). Since
    the rank of \(A\) is 1, this gives a compact SVD of \(A\): \(A = U \Sigma V^T\)
    with \(U = \mathbf{u}_1\), \(\Sigma = (5)\), and \(V = \mathbf{v}_1\).'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.7** 我们有 \(A\) 的奇异值为 \(\sigma_1 = \sqrt{25} = 5\) 和 \(\sigma_2 = 0\)。我们可以取
    \(\mathbf{v}_1 = (1/\sqrt{5}, 2/\sqrt{5})\) 和 \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1
    = (1/\sqrt{5}, 2/\sqrt{5})\)。由于 \(A\) 的秩为 1，这给出了 \(A\) 的紧凑奇异值分解：\(A = U \Sigma
    V^T\)，其中 \(U = \mathbf{u}_1\)，\(\Sigma = (5)\)，和 \(V = \mathbf{v}_1\)。'
- en: '**E4.3.9** From the full SVD of \(A\), we have that: An orthonormal basis for
    \(\mathrm{col}(A)\) is given by the first column of \(U\): \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\).
    An orthonormal basis for \(\mathrm{row}(A)\) is given by the first column of \(V\):
    \(\{(1/\sqrt{5}, 2/\sqrt{5})\}\). An orthonormal basis for \(\mathrm{null}(A)\)
    is given by the second column of \(V\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\). An
    orthonormal basis for \(\mathrm{null}(A^T)\) is given by the second column of
    \(U\): \(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\).'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.9** 从 \(A\) 的完全奇异值分解中，我们得到：\(\mathrm{col}(A)\) 的一个正交基由 \(U\) 的第一列给出：\(\{(1/\sqrt{5},
    2/\sqrt{5})\}\)。\(\mathrm{row}(A)\) 的一个正交基由 \(V\) 的第一列给出：\(\{(1/\sqrt{5}, 2/\sqrt{5})\}\)。\(\mathrm{null}(A)\)
    的一个正交基由 \(V\) 的第二列给出：\(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\)。\(\mathrm{null}(A^T)\)
    的一个正交基由 \(U\) 的第二列给出：\(\{(-2/\sqrt{5}, 1/\sqrt{5})\}\)。'
- en: '**E4.3.11** From its diagonal form, we see that \(\lambda_1 = 15\), \(\mathbf{q}_1
    = (1, 0)\); \(\lambda_2 = 7\), \(\mathbf{q}_2 = (0, 1)\).'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.11** 从其对角形式中，我们看到 \(\lambda_1 = 15\), \(\mathbf{q}_1 = (1, 0)\)；\(\lambda_2
    = 7\), \(\mathbf{q}_2 = (0, 1)\)。'
- en: '**E4.3.13** By direct computation, \(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix}
    1 & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.13** 通过直接计算，\(U_1 \Sigma_1 V_1^T = U \Sigma V^T = \begin{pmatrix} 1
    & 2 \\ 2 & 1 \\ -1 & 1 \\ 3 & -1 \end{pmatrix} = A\).'
- en: '**E4.3.15** By direct computation,'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.15** 通过直接计算，'
- en: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: \(A^TA \mathbf{v}_1 = 15 \mathbf{v}_1\), \(A^TA \mathbf{v}_2 = 7 \mathbf{v}_2\),
- en: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: \(AA^T\mathbf{u}_1 = 15 \mathbf{u}_1\), \(AA^T \mathbf{u}_2 = 7 \mathbf{u}_2\).
- en: '**E4.3.17** The best approximating subspace of dimension \(k=1\) is the line
    spanned by the vector \(\mathbf{v}_1 = (1, 0)\). The sum of squared distances
    to this subspace is \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 =
    5 + 5 + 2 + 10 - 15 = 7.\)'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.3.17** 维度为 \(k=1\) 的最佳逼近子空间是由向量 \(\mathbf{v}_1 = (1, 0)\) 张成的直线。到这个子空间的平方距离之和是
    \(\sum_{i=1}^4 \|\boldsymbol{\alpha}_i\|^2 - \sigma_1^2 = 5 + 5 + 2 + 10 - 15
    = 7.\)'
- en: '**E4.4.1**'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.1**'
- en: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^2 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^3
    = \begin{pmatrix} 27 & 0 \\ 0 & 1 \end{pmatrix}, \quad A^4 = \begin{pmatrix} 81
    & 0 \\ 0 & 1 \end{pmatrix}. \end{split}\]
- en: The diagonal entries are being raised to increasing powers, while the off-diagonal
    entries remain zero.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线元素正在被提高到更高的幂次，而非对角线元素保持为零。
- en: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\),
    \(\frac{A^1 \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix}
    2 \\ 1 \end{pmatrix}\) \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\),
    \(\frac{A^2 \mathbf{x}}{\|A^2 \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix}
    5 \\ 4 \end{pmatrix}\) \(A^3 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 5 \\ 4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\),
    \(\frac{A^3 \mathbf{x}}{\|A^3 \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix}
    14 \\ 13 \end{pmatrix}\)'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.3** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)，\(\frac{A^1
    \mathbf{x}}{\|A^1 \mathbf{x}\|} = \frac{1}{\sqrt{5}} \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 5 \\ 4 \end{pmatrix}\)，\(\frac{A^2 \mathbf{x}}{\|A^2
    \mathbf{x}\|} = \frac{1}{\sqrt{41}} \begin{pmatrix} 5 \\ 4 \end{pmatrix}\) \(A^3
    \mathbf{x} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} 5 \\
    4 \end{pmatrix} = \begin{pmatrix} 14 \\ 13 \end{pmatrix}\)，\(\frac{A^3 \mathbf{x}}{\|A^3
    \mathbf{x}\|} = \frac{1}{\sqrt{365}} \begin{pmatrix} 14 \\ 13 \end{pmatrix}\)'
- en: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.5** \(A^1 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)
    \(A^2 \mathbf{x} = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}
    2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \end{pmatrix}\) \(A^3 \mathbf{x}
    = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 4 \\ 1 \end{pmatrix}
    = \begin{pmatrix} 8 \\ 1 \end{pmatrix}\)'
- en: '**E4.4.7** We have \(A = Q \Lambda Q^T\), where \(Q\) is the matrix of normalized
    eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues. Then,'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.7** 我们有 \(A = Q \Lambda Q^T\)，其中 \(Q\) 是归一化特征向量的矩阵，\(\Lambda\) 是特征值的对角矩阵。然后，'
- en: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^2 = Q \Lambda^2 Q^T = \frac{1}{2}\begin{pmatrix} 1 & 1 \\
    1 & -1 \end{pmatrix} \begin{pmatrix} 25 & 0 \\ 0 & 9 \end{pmatrix} \begin{pmatrix}
    1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix},
    \end{split}\]
- en: and similarly, \(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\).
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，\(A^3 = \begin{pmatrix} 76 & 49 \\ 49 & 76 \end{pmatrix}\).
- en: '**E4.4.9** We have \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\).
    Let’s find the eigenvalues of the matrix \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 &
    5 \end{pmatrix}\). We solve the characteristic equation \(\det(A^TA - \lambda
    I) = 0\). First, let’s calculate the characteristic polynomial: \(\det(A^TA -
    \lambda I) = \det(\begin{pmatrix} 1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})=
    \lambda^2 - 6\lambda + 1\), so \((\lambda - 3)^2 - 8 = 0\) or \((\lambda - 3)^2
    = 8\) or \(\lambda = 3 \pm 2\sqrt{2}\). Therefore, the eigenvalues of \(A^TA\)
    are: \(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) and \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\). Hence, this matrix is positive semidefinite because its eigenvalues
    are 0 and 6, both of which are non-negative.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.4.9** We have \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}\).
    Let’s find the eigenvalues of the matrix \(A^TA = \begin{pmatrix} 1 & 2 \\ 2 &
    5 \end{pmatrix}\). We solve the characteristic equation \(\det(A^TA - \lambda
    I) = 0\). First, let’s calculate the characteristic polynomial: \(\det(A^TA -
    \lambda I) = \det(\begin{pmatrix} 1-\lambda & 2 \\ 2 & 5-\lambda \end{pmatrix})=
    \lambda^2 - 6\lambda + 1\), so \((\lambda - 3)^2 - 8 = 0\) or \((\lambda - 3)^2
    = 8\) or \(\lambda = 3 \pm 2\sqrt{2}\). Therefore, the eigenvalues of \(A^TA\)
    are: \(\lambda_1 = 3 + 2\sqrt{2} \approx 5.83\) and \(\lambda_2 = 3 - 2\sqrt{2}
    \approx 0.17\). Hence, this matrix is positive semidefinite because its eigenvalues
    are 0 and 6, both of which are non-negative.'
- en: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). The unit
    norm constraint requires that \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.1** \(\sum_{j=1}^p \phi_{j1}^2 = \left(\frac{1}{\sqrt{2}}\right)^2 +
    \left(\frac{1}{\sqrt{2}}\right)^2 = \frac{1}{2} + \frac{1}{2} = 1\). The unit
    norm constraint requires that \(\sum_{j=1}^p \phi_{j1}^2 = 1\).'
- en: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\).
    The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.3** \(t_{11} = -1 \cdot \frac{1}{\sqrt{2}} + 1 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{21} = 1 \cdot \frac{1}{\sqrt{2}} - 1 \cdot \frac{1}{\sqrt{2}} = 0\).
    The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
- en: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\). Uncorrelatedness requires that \(\frac{1}{n-1}\sum_{i=1}^n
    t_{i1}t_{i2} = 0\).'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.5** \(\frac{1}{n-1}\sum_{i=1}^n t_{i1}t_{i2} = \frac{1}{1}(0 \cdot (-\sqrt{2})
    + 0 \cdot 0) = 0\). Uncorrelatedness requires that \(\frac{1}{n-1}\sum_{i=1}^n
    t_{i1}t_{i2} = 0\).'
- en: '**E4.5.7** First, compute the mean of each column:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.7** First, compute the mean of each column:'
- en: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bar{x}_1 = \frac{1+3+5}{3} = 3, \quad \bar{x}_2 = \frac{2+4+6}{3} = 4. \]
- en: 'Mean-centered data matrix:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 均值中心化的数据矩阵：
- en: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} X' = X - \begin{pmatrix} 3 & 4 \\ 3 & 4 \\ 3 & 4 \end{pmatrix}
    = \begin{pmatrix} 1-3 & 2-4 \\ 3-3 & 4-4 \\ 5-3 & 6-4 \end{pmatrix} = \begin{pmatrix}
    -2 & -2 \\ 0 & 0 \\ 2 & 2 \end{pmatrix}. \end{split}\]
- en: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\), \(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\). The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.9** \(t_{11} = -2 \cdot \left(\frac{1}{\sqrt{2}}\right) - 2 \cdot \frac{1}{\sqrt{2}}
    = -2\sqrt{2}\), \(t_{21} = 0 \cdot \left(\frac{1}{\sqrt{2}}\right) + 0 \cdot \frac{1}{\sqrt{2}}
    = 0\), \(t_{31} = 2 \cdot \left(\frac{1}{\sqrt{2}}\right) + 2 \cdot \frac{1}{\sqrt{2}}
    = 2\sqrt{2}\). The scores are computed as \(t_{i1} = \sum_{j=1}^p \phi_{j1} \tilde{x}_{ij}\).'
- en: '**E4.5.11** The second principal component must be uncorrelated with the first,
    so its loading vector must be orthogonal to \(\varphi_1\). One such vector is
    \(\varphi_2 = (-0.6, 0.8)\).'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.5.11** The second principal component must be uncorrelated with the first,
    so its loading vector must be orthogonal to \(\varphi_1\). One such vector is
    \(\varphi_2 = (-0.6, 0.8)\).'
- en: '**E4.6.1** The Frobenius norm is given by:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.1** The Frobenius norm is given by:'
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_F = \sqrt{\sum_{i=1}^{3} \sum_{j=1}^{2} a_{ij}^2} = \sqrt{1^2 + 2^2
    + 3^2 + 4^2 + 5^2 + 6^2} = \sqrt{91}. \]
- en: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\).'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.3** \(\|A\|_F = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\).'
- en: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\).'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.5** \(\|A - B\|_F = \sqrt{(1-1)^2 + (2-1)^2 + (2-1)^2 + (4-1)^2} = \sqrt{1
    + 1 + 9} = \sqrt{11}\).'
- en: '**E4.6.7** The induced 2-norm of the difference between a matrix and its rank-1
    truncated SVD is equal to the second singular value. Therefore, using the SVD
    from before, we have \(\|A - A_1\|_2 = \sigma_2 = 0\).'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.7** The induced 2-norm of the difference between a matrix and its rank-1
    truncated SVD is equal to the second singular value. Therefore, using the SVD
    from before, we have \(\|A - A_1\|_2 = \sigma_2 = 0\).'
- en: '**E4.6.9** The matrix \(A\) is singular (its determinant is zero), so it doesn’t
    have an inverse. Therefore, \(\|A^{-1}\|_2\) is undefined. Note that the induced
    2-norm of the pseudoinverse \(A^+\) is not the same as the induced 2-norm of the
    inverse (when it exists).'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '**E4.6.9** 矩阵 \(A\) 是奇异的（其行列式为零），因此它没有逆。因此，\(\|A^{-1}\|_2\) 是未定义的。注意，伪逆 \(A^+\)
    的诱导 2-范数与逆的诱导 2-范数（当存在时）不同。'
- en: 4.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  id: totrans-609
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1.5\. 学习成果[#](#learning-outcomes "链接到这个标题")
- en: Define the column space, row space, and rank of a matrix.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的列空间、行空间和秩。
- en: Prove that the row rank equals the column rank.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明行秩等于列秩。
- en: Apply properties of matrix rank, such as \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\)
    and \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用矩阵秩的性质，例如 \(\mathrm{rk}(AB)\leq \mathrm{rk}(A)\) 和 \(\mathrm{rk}(A+B) \leq
    \mathrm{rk}(A) + \mathrm{rk}(B)\)。
- en: State and prove the Rank-Nullity Theorem.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述并证明秩-零度定理。
- en: Define eigenvalues and eigenvectors of a square matrix.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义方阵的特征值和特征向量。
- en: Prove that a symmetric matrix has at most d distinct eigenvalues, where d is
    the matrix size.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明对称矩阵最多有 d 个不同的特征值，其中 d 是矩阵的大小。
- en: State the Spectral Theorem for symmetric matrices and explain its implications.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述对称矩阵的谱定理及其含义。
- en: Write the spectral decomposition of a symmetric matrix using outer products.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外积写出对称矩阵的谱分解。
- en: Determine whether a symmetric matrix is positive semidefinite or positive definite
    based on its eigenvalues.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据特征值确定对称矩阵是正半定还是正定。
- en: Compute eigenvalues and eigenvectors of symmetric matrices using programming
    tools like NumPy’s `linalg.eig`.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编程工具（如 NumPy 的 `linalg.eig`）计算对称矩阵的特征值和特征向量。
- en: State the objective of the best approximating subspace problem and formulate
    it mathematically as a minimization of the sum of squared distances.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述最佳逼近子空间问题的目标，并将其数学上表述为平方距离之和的最小化。
- en: Prove that the best approximating subspace problem can be solved greedily by
    finding the best one-dimensional subspace, then the best one-dimensional subspace
    orthogonal to the first, and so on.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明最佳逼近子空间问题可以通过找到最佳一维子空间，然后是第一个子空间正交的一维子空间，依此类推，贪婪地解决。
- en: Define the singular value decomposition (SVD) of a matrix and describe the properties
    of the matrices involved.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的奇异值分解（SVD）并描述涉及矩阵的性质。
- en: Prove the existence of the SVD for any real matrix using the Spectral Theorem.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用谱定理证明任何实矩阵的奇异值分解的存在性。
- en: Explain the connection between the SVD of a matrix \(A\) and the spectral decompositions
    of \(A^T A\) and \(A A^T\).
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释矩阵 \(A\) 的 SVD 与 \(A^T A\) 和 \(A A^T\) 的谱分解之间的联系。
- en: Compute the SVD of simple matrices.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算简单矩阵的 SVD。
- en: Use the SVD of the data matrix to find the best k-dimensional approximating
    subspace to a set of data points.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据矩阵的 SVD 找到一组数据点的最佳 k 维逼近子空间。
- en: Interpret the singular values as capturing the contributions of the right singular
    vectors to the fit of the approximating subspace.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释奇异值如何捕捉右奇异向量对逼近子空间拟合的贡献。
- en: Obtain low-dimensional representations of data points using the truncated SVD.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用截断 SVD 获取数据点的低维表示。
- en: Distinguish between full and compact forms of the SVD and convert between them.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分 SVD 的完整形式和紧凑形式，并在它们之间进行转换。
- en: State the key lemma for power iteration in the positive semidefinite case and
    explain why it implies convergence to the top eigenvector.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述正半定情况下幂迭代的键引理，并解释为什么它意味着收敛到最大特征向量。
- en: Extend the power iteration lemma to the general case of singular value decomposition
    (SVD) and justify why repeated multiplication of A^T A with a random vector converges
    to the top right singular vector.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将幂迭代引理扩展到奇异值分解（SVD）的一般情况，并证明为什么重复乘以 \(A^T A\) 与随机向量的乘积收敛到右上奇异向量。
- en: Compute the corresponding top singular value and left singular vector given
    the converged top right singular vector.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定收敛的右上奇异向量，计算相应的最大奇异值和左奇异向量。
- en: Describe the orthogonal iteration method for finding additional singular vectors
    beyond the top one.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述用于找到除第一个奇异向量之外的其他奇异向量的正交迭代法。
- en: Apply the power iteration method and orthogonal iteration to compute the SVD
    of a given matrix and use it to find the best low-dimensional subspace approximation.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用幂迭代法和正交迭代法计算给定矩阵的奇异值分解（SVD），并使用它来找到最佳低维子空间逼近。
- en: Implement the power iteration method and orthogonal iteration in Python.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中实现幂迭代法和正交迭代法。
- en: Define principal components and loadings in the context of PCA.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PCA的背景下定义主成分和载荷。
- en: Express the objective of PCA as a constrained optimization problem.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将PCA的目标表述为约束优化问题。
- en: Establish the connection between PCA and singular value decomposition (SVD).
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立PCA与奇异值分解（SVD）之间的联系。
- en: Implement PCA using an SVD algorithm.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVD算法实现PCA。
- en: Interpret the results of PCA in the context of dimensionality reduction and
    data visualization.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在降维和数据可视化的背景下解释PCA的结果。
- en: Define the Frobenius norm and the induced 2-norm of a matrix.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义矩阵的Frobenius范数和诱导的2-范数。
- en: Express the Frobenius norm and the induced 2-norm of a matrix in terms of its
    singular values.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用矩阵的奇异值表示Frobenius范数和诱导的2-范数。
- en: State the Eckart-Young theorem.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述Eckart-Young定理。
- en: Define the pseudoinverse of a matrix using the SVD.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVD定义矩阵的伪逆。
- en: Apply the pseudoinverse to solve least squares problems when the matrix has
    full column rank.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当矩阵具有满列秩时，使用伪逆求解最小二乘问题。
- en: Apply the pseudoinverse to find the least norm solution for underdetermined
    systems with full row rank.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将伪逆应用于求解具有满秩行的欠定系统的最小范数解。
- en: Formulate the ridge regression problem as a regularized optimization problem.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将岭回归问题表述为正则化优化问题。
- en: Explain how ridge regression works by analyzing its solution in terms of the
    SVD of the design matrix.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分析设计矩阵的SVD来解释岭回归的工作原理。
- en: \(\aleph\)
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: \(\aleph\)
- en: 4.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  id: totrans-650
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8.2\. 其他部分[#](#additional-sections "链接到本标题")
- en: 4.8.2.1\. Computing more singular vectors[#](#computing-more-singular-vectors
    "Link to this heading")
  id: totrans-651
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.1\. 计算更多的奇异向量[#](#computing-more-singular-vectors "链接到本标题")
- en: We have shown how to compute the first singular vector. How do we compute more
    singular vectors? One approach is to first compute \(\mathbf{v}_1\) (or \(-\mathbf{v}_1\)),
    then find a vector \(\mathbf{y}\) orthogonal to it, and proceed as above. And
    then we repeat until we have all \(m\) right singular vectors.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何计算第一个奇异向量。我们如何计算更多的奇异向量？一种方法是首先计算 \(\mathbf{v}_1\)（或 \(-\mathbf{v}_1\)），然后找到一个与它正交的向量
    \(\mathbf{y}\)，然后像上面那样进行。然后我们重复，直到我们有了所有的 \(m\) 个右奇异向量。
- en: 'We are often interested only in the top, say \(\ell < m\), singular vectors.
    An alternative approach in that case is to start with \(\ell\) random vectors
    and, first, find an orthonormal basis for the space they span. Then to quote [BHK,
    Section 3.7.1]:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常只对顶部，比如说 \(\ell < m\)，的奇异向量感兴趣。在这种情况下，一种替代方法是首先使用 \(\ell\) 个随机向量，首先找到它们所张成的空间的正交基。然后，引用[BHK，第3.7.1节]：
- en: Then compute \(B\) times each of the basis vectors, and find an orthonormal
    basis for the space spanned by the resulting vectors. Intuitively, one has applied
    \(B\) to a subspace rather than a single vector. One repeatedly applies \(B\)
    to the subspace, calculating an orthonormal basis after each application to prevent
    the subspace collapsing to the one dimensional subspace spanned by the first singular
    vector.
  id: totrans-654
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后计算基向量的每个向量乘以 \(B\)，并找到由结果向量张成的空间的正交基。直观上，我们是对子空间而不是单个向量应用了 \(B\)。我们反复将 \(B\)
    应用到子空间，每次应用后计算正交基，以防止子空间塌缩到由第一个奇异向量张成的单维子空间。
- en: We will not prove here that this approach, known as orthogonal iteration, works.
    The proof is similar to that of the *Power Iteration Lemma*.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会证明这种称为正交迭代的方法是有效的。证明与 *幂迭代引理* 的证明类似。
- en: We implement this last algorithm. We will need our previous implementation of
    *Gram-Schimdt*.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现最后一个算法。我们将需要我们之前实现的 *Gram-Schimdt*。
- en: '[PRE56]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note that above we avoided forming the matrix \(A^T A\). With a small number
    of iterations, that approach potentially requires fewer arithmetic operations
    overall and it allows to take advantage of the possible sparsity of \(A\) (i.e.
    the fact that it may have many zeros).
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上面我们避免了形成矩阵 \(A^T A\)。使用少量迭代，这种方法可能需要更少的算术运算，并且可以利用 \(A\) 的可能稀疏性（即它可能有很多零）。
- en: '**NUMERICAL CORNER:** We apply it again to our two-cluster example.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们再次将其应用于我们的两个簇示例。'
- en: '[PRE57]'
  id: totrans-660
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Let’s try again, but after projecting on the top two singular vectors. Recall
    that this corresponds to finding the best two-dimensional approximating subspace.
    The projection can be computed using the truncated SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\). We can interpret the rows of \(U_{(2)} \Sigma_{(2)}\) as the coefficients
    of each data point in the basis \(\mathbf{v}_1,\mathbf{v}_2\). We will work in
    that basis.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试，但这次在投影到前两个奇异向量之后。回想一下，这对应于找到最佳二维逼近子空间。投影可以通过截断SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\) 来计算。我们可以将 \(U_{(2)} \Sigma_{(2)}\) 的行解释为每个数据点在基 \(\mathbf{v}_1,\mathbf{v}_2\)
    中的系数。我们将在这个基上工作。
- en: '[PRE58]'
  id: totrans-662
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="切换隐藏内容">显示代码单元格源 隐藏代码单元格源</summary>
- en: '[PRE60]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE60]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
- en: Finally, looking at the first two right singular vectors, we see that the first
    one does align quite well with the first dimension.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，观察前两个右奇异向量，我们看到第一个与第一个维度相当吻合。
- en: '[PRE61]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: \(\unlhd\)
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.2.2\. Pseudoinverse[#](#pseudoinverse "Link to this heading")
  id: totrans-670
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.2\. 伪逆[#](#pseudoinverse "链接到这个标题")
- en: The SVD leads to a natural generalization of the matrix inverse. First an observation.
    Recall that, to take the product of two square diagonal matrices, we simply multiply
    the corresponding diagonal entries. Let \(\Sigma \in \mathbb{R}^{r \times r}\)
    be a square diagonal matrix with diagonal entries \(\sigma_1,\ldots,\sigma_r\).
    If all diagonal entries are non-zero, then the matrix is invertible (since its
    columns then form a basis of the full space). The inverse of \(\Sigma\) in that
    case is simply the diagonal matrix \(\Sigma^{-1}\) with diagonal entries \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\).
    This can be confirmed by checking the definition of the inverse
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 导致了矩阵逆的自然推广。首先是一个观察。回想一下，要计算两个平方对角矩阵的乘积，我们只需乘以相应的对角元素。设 \(\Sigma \in \mathbb{R}^{r
    \times r}\) 是一个对角矩阵，其对角元素为 \(\sigma_1,\ldots,\sigma_r\)。如果所有对角元素都不为零，则该矩阵是可逆的（因为其列构成了整个空间的一个基）。在这种情况下，\(\Sigma\)
    的逆是一个对角矩阵 \(\Sigma^{-1}\)，其对角元素为 \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\)。这可以通过检查逆的定义来证实
- en: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
- en: We are ready for our main definition.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好我们的主要定义。
- en: '**DEFINITION** **(Pseudoinverse)** Let \(A \in \mathbb{R}^{n \times m}\) be
    a matrix with compact SVD \(A = U \Sigma V^T\) and singular values \(\sigma_1
    \geq \cdots \geq \sigma_r > 0\). A pseudoinverse \(A^+ \in \mathbb{R}^{m \times
    n}\) is defined as'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（伪逆）** 设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有紧凑SVD \(A = U \Sigma
    V^T\) 和奇异值 \(\sigma_1 \geq \cdots \geq \sigma_r > 0\) 的矩阵。伪逆 \(A^+ \in \mathbb{R}^{m
    \times n}\) 定义为'
- en: \[ A^+ = V \Sigma^{-1} U^T. \]
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = V \Sigma^{-1} U^T. \]
- en: \(\natural\)
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: While it is not obvious from the definition (why?), the pseudoinverse is in
    fact unique. To see that it is indeed a generalization of an inverse, we make
    a series of observations.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从定义中不明显（为什么？），但伪逆实际上是唯一的。为了看到它确实是逆的一个推广，我们进行了一系列观察。
- en: '*Observation 1:* Note that, using that \(U\) has orthonormal columns,'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '*观察1:* 注意到，使用 \(U\) 的正交列，'
- en: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
- en: which in general in not the identity matrix. Indeed it corresponds instead to
    the projection matrix onto the column space of \(A\), since the columns of \(U\)
    form an orthonormal basis of that linear subspace. As a result
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下，它不是单位矩阵。实际上，它对应于投影到 \(A\) 的列空间的投影矩阵，因为 \(U\) 的列构成了该线性子空间的正交基。因此
- en: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
- en: So \(A A^+\) is not the identity matrix, but it does map the columns of \(A\)
    to themselves. Put differently, it is the identity map “when restricted to \(\mathrm{col}(A)\)”.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 \(A A^+\) 不是单位矩阵，但它确实将 \(A\) 的列映射到自身。换句话说，当限制在 \(\mathrm{col}(A)\) 上时，它是单位映射。
- en: Similarly,
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，
- en: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
- en: is the projection matrix onto the row space of \(A\), and
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 是投影矩阵，投影到 \(A\) 的行空间，并且
- en: \[ (A^+ A) A^+ = A^+. \]
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A^+ A) A^+ = A^+. \]
- en: '*Observation 2:* If \(A\) has full column rank \(m \leq n\), then \(r = m\).
    In that case, the columns of \(V\) form an orthonormal basis of all of \(\mathbb{R}^m\),
    i.e., \(V\) is orthogonal. Hence,'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ A = V V^T = I_{m \times m}. \]
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if \(A\) has full row rank \(n \leq m\), then \(A A^+ = I_{n \times
    n}\).
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: If both cases hold, then \(n = m\), i.e., \(A\) is square, and \(\mathrm{rk}(A)
    = n\), i.e., \(A\) is invertible. We then get
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^+ = A^+ A = I_{n\times n}. \]
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: That implies that \(A^+ = A^{-1}\) by the *Existence of an Inverse Lemma* (which
    includes uniqueness of the matrix inverse).
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation 3:* Recall that, when \(A\) is nonsingular, the system \(A \mathbf{x}
    = \mathbf{b}\) admits the unique solution \(\mathbf{x} = A^{-1} \mathbf{b}\).
    In the overdetermined case, the pseudoinverse provides a solution to the linear
    least squares problem.'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Least Squares)** Let \(A \in \mathbb{R}^{n \times
    m}\) with \(m \leq n\). A solution to the linear least squares problem'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
- en: is given by \(\mathbf{x}^* = A^+ \mathbf{b}\). Further, if \(A\) has full column
    rank \(m\), then
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = (A^T A)^{-1} A^T. \]
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* For the first part, we use that the solution to the least squares
    problem is the orthogonal projection. For the second part, we use the SVD definition
    and check that the two sides are the same.'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(A = U \Sigma V^T\) be a compact SVD of \(A\). For the first
    claim, note that the choice of \(\mathbf{x}^*\) in the statement gives'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: Hence, \(A \mathbf{x}^*\) is the orthogonal projection of \(\mathbf{b}\) onto
    the column space of \(A\) - which we proved previously is the solution to the
    linear least squares problem.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: Now onto the second claim. Recall that, when \(A\) is of full rank, the matrix
    \(A^T A\) is nonsingular. We then note that, using the notation \(\Sigma^{-2}
    = (\Sigma^{-1})^2\),
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. Here, we used that \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix. \(\square\)
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudoinverse also provides a solution in the case of an underdetermined
    system. Here, however, there are in general infinitely many solutions. The one
    chosen by the pseudoinverse has a special property as we see now: it is the least
    norm solution.'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Pseudoinverse and Underdetermined Systems)** Let \(A \in \mathbb{R}^{n
    \times m}\) with \(m > n\) and \(\mathbf{b} \in \mathbb{R}^n\). Further assume
    that \(A\) has full row rank \(n\). Then \(\mathbf{x}^* = A^+ \mathbf{b}\) is
    a solution to'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in that case,
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = A^T (A A^T)^{-1}. \]
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = A^T (A A^T)^{-1}. \]
- en: \(\flat\)
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* We first prove the formula for \(A^+\). As we did in the overdetermined
    case, it can be checked by substituting a compact SVD \(A = U \Sigma V^T\). Recall
    that, when \(A^T\) is of full rank, the matrix \(A A^T\) is nonsingular. We then
    note that'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：** 我们首先证明 \(A^+\) 的公式。正如我们在超定情况中所做的那样，可以通过代入紧凑奇异值分解 \(A = U \Sigma V^T\)
    来检查。回想一下，当 \(A^T\) 是满秩时，矩阵 \(A A^T\) 是非奇异的。我们接着注意到'
- en: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
- en: as claimed. Here, we used that \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。在这里，我们使用了\((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\)，这可以通过检查逆的定义和我们对对角矩阵逆的先前观察来证实。
- en: Because \(A\) has full row rank, \(\mathbf{b} \in \mathrm{col}(A)\) and there
    is at least one \(\mathbf{x}\) such that \(A \mathbf{x} = \mathbf{b}\). One such
    solution is provided by the pseudoinverse. Indeed, from a previous observation,
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(A\) 有满行秩，\(\mathbf{b} \in \mathrm{col}(A)\) 并且至少存在一个 \(\mathbf{x}\) 使得
    \(A \mathbf{x} = \mathbf{b}\)。一个这样的解由伪逆提供。确实，从先前的观察中，
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
- en: where we used that the columns of \(U\) form an orthonormal basis of \(\mathbb{R}^n\)
    by the rank assumption.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了根据秩的假设，\(U\)的列构成了\(\mathbb{R}^n\)的正交基。
- en: Let \(\mathbf{x}\) be any other solution to the system. Then \(A(\mathbf{x}
    - \mathbf{x}^*) = \mathbf{b} - \mathbf{b} = \mathbf{0}\). That implies
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\mathbf{x}\) 是该系统的另一个解。那么 \(A(\mathbf{x} - \mathbf{x}^*) = \mathbf{b} -
    \mathbf{b} = \mathbf{0}\)。这表明
- en: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
- en: In words, \(\mathbf{x} - \mathbf{x}^*\) and \(\mathbf{x}^*\) are orthogonal.
    By *Pythagoras*,
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，\(\mathbf{x} - \mathbf{x}^*\) 和 \(\mathbf{x}^*\) 是正交的。根据*毕达哥拉斯定理*，
- en: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
- en: That proves that \(\mathbf{x}^*\) has the smallest norm among all solutions
    to the system \(A \mathbf{x} = \mathbf{b}\). \(\square\)
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了 \(\mathbf{x}^*\) 在系统 \(A \mathbf{x} = \mathbf{b}\) 的所有解中具有最小的范数。 \(\square\)
- en: '**EXAMPLE:** Continuing a previous example, let'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 继续先前的例子，设'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: Recall that
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
- en: where
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
- en: We compute the pseudoinverse. By the formula, in the rank one case, it is simply
    (Check this!)
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算伪逆。根据公式，在秩一的情况下，它简单地是（检查这个！）
- en: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
- en: \(\lhd\)
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be a square nonsingular
    matrix. Let \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be a compact
    SVD of \(A\), where we used the fact that the rank of \(A\) is \(n\) so it has
    \(n\) strictly positive singular values. We seek to compute \(\|A^{-1}\|_2\) in
    terms of the singular values.'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 设 \(A \in \mathbb{R}^{n \times n}\) 为一个方阵且非奇异。设 \(A = \sum_{j=1}^n
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) 为 \(A\) 的紧凑奇异值分解，其中我们使用了 \(A\) 的秩为 \(n\)
    因此它有 \(n\) 个严格正的奇异值的事实。我们寻求用奇异值来计算 \(\|A^{-1}\|_2\)。'
- en: Because \(A\) is invertible, \(A^+ = A^{-1}\). So we compute the pseudoinverse
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(A\) 是可逆的，\(A^+ = A^{-1}\)。因此我们计算伪逆
- en: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
- en: The sum on the right-hand side is not quite a compact SVD of \(A^{-1}\) because
    the coefficients \(\sigma_j^{-1}\) are non-decreasing in \(j\).
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的求和并不是 \(A^{-1}\) 的紧凑奇异值分解，因为系数 \(\sigma_j^{-1}\) 在 \(j\) 中是非递减的。
- en: But writing the sum in reverse order
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 但将求和顺序反过来写
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
- en: does give a compact SVD of \(A^{-1}\), since \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1}
    > 0\) and \(\{\mathbf{v}_j\}_{j=1}^n\) and \(\{\mathbf{u}_j\}_{j=1}^n\) are orthonormal
    lists. Hence, the \(2\)-norm is given by the largest singular value, that is,
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 确实给出了 \(A^{-1}\) 的紧凑奇异值分解，因为 \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1} > 0\)，并且
    \(\{\mathbf{v}_j\}_{j=1}^n\) 和 \(\{\mathbf{u}_j\}_{j=1}^n\) 是正交归一列表。因此，\(2\)-范数由最大的奇异值给出，即，
- en: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
- en: \(\lhd\)
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**NUMERICAL CORNER:** In Numpy, the pseudoinverse of a matrix can be computed
    using the function [`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html).'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 在 Numpy 中，可以使用函数 `numpy.linalg.pinv` 计算矩阵的伪逆。[`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html)。'
- en: '[PRE63]'
  id: totrans-742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-744
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-746
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Let’s try our previous example.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试之前的例子。
- en: '[PRE69]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-750
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-752
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: \(\unlhd\)
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.2.3\. Condition numbers[#](#condition-numbers "Link to this heading")
  id: totrans-754
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.3\. 条件数[#](#condition-numbers "链接到本标题")
- en: In this section we introduce condition numbers, a measure of perturbation sensitivity
    for numerical problems. We look in particular at the conditioning of the least-squares
    problem. We begin with the concept of pseudoinverse, which is important in its
    own right.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了条件数，它是数值问题中扰动敏感度的度量。我们特别关注最小二乘问题的条件性。我们首先介绍伪逆的概念，它本身具有重要意义。
- en: '**Conditioning of matrix-vector multiplication** We define the condition number
    of a matrix and show that it captures some information about the sensitivity to
    perturbations of matrix-vector multiplications.'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵-向量乘法的条件性** 我们定义矩阵的条件数，并展示它捕捉了关于矩阵-向量乘法对扰动敏感性的某些信息。'
- en: '**DEFINITION** **(Condition number of a matrix)** The condition number (in
    the induced \(2\)-norm) of a square, nonsingular matrix \(A \in \mathbb{R}^{n
    \times n}\) is defined as'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（矩阵的条件数）** 矩阵 \(A \in \mathbb{R}^{n \times n}\) 的条件数（在诱导的 \(2\)-范数下）定义为'
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
- en: \(\natural\)
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: In fact, this can be computed as
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这可以计算为
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
- en: where we used the example above. In words, \(\kappa_2(A)\) is the ratio of the
    largest to the smallest stretching under \(A\).
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了上面的例子。换句话说，\(\kappa_2(A)\) 是在 \(A\) 下最大拉伸与最小拉伸的比值。
- en: '**THEOREM** **(Conditioning of Matrix-Vector Multiplication)** Let \(M \in
    \mathbb{R}^{n \times n}\) be nonsingular. Then, for any \(\mathbf{z} \in \mathbb{R}^n\),'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（矩阵-向量乘法的条件性）** 设 \(M \in \mathbb{R}^{n \times n}\) 是非奇异的。那么，对于任意的
    \(\mathbf{z} \in \mathbb{R}^n\)，'
- en: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
- en: and the inequality is tight in the sense that there is an \(\mathbf{x}\) and
    a \(\mathbf{d}\) that achieves it.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 并且不等式是紧的，因为存在一个 \(\mathbf{x}\) 和一个 \(\mathbf{d}\) 可以达到它。
- en: \(\sharp\)
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: The ratio above measures the worst rate of relative change in \(M \mathbf{z}\)
    under infinitesimal perturbations of \(\mathbf{z}\). The theorem says that when
    \(\kappa_2(M)\) is large, a case referred to as ill-conditioning, large relative
    changes in \(M \mathbf{z}\) can be obtained from relatively small perturbations
    to \(\mathbf{z}\). In words, a matrix-vector product is potentially sensitive
    to perturbations when the matrix is ill-conditioned.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的比值衡量了在 \(\mathbf{z}\) 的无穷小扰动下 \(M \mathbf{z}\) 的相对变化的最坏速率。定理表明，当 \(\kappa_2(M)\)
    很大时，即所谓的病态条件，从对 \(\mathbf{z}\) 的相对较小的扰动可以得到 \(M \mathbf{z}\) 的较大相对变化。换句话说，当矩阵是病态的时，矩阵-向量乘积可能对扰动敏感。
- en: '*Proof:* Write'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 写成'
- en: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
- en: where we used \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\),
    which was shown in a previous example.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\)，这在之前的例子中已经证明。
- en: In particular, we see that the ratio can achieve its maximum by taking \(\mathbf{d}\)
    and \(\mathbf{z}\) to be the right singular vectors corresponding to \(\sigma_1\)
    and \(\sigma_n\) respectively. \(\square\)
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们可以看到，通过将 \(\mathbf{d}\) 和 \(\mathbf{z}\) 分别取为对应于 \(\sigma_1\) 和 \(\sigma_n\)
    的右奇异向量，该比率可以达到其最大值。\(\square\)
- en: If we apply the theorem to the inverse instead, we get that the relative conditioning
    of the nonsingular linear system \(A \mathbf{x} = \mathbf{b}\) to perturbations
    in \(\mathbf{b}\) is \(\kappa_2(A)\). The latter can be large in particular when
    the columns of \(A\) are close to linearly dependent. This is detailed in the
    next example.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将定理应用于逆矩阵，我们得到非奇异线性系统 \(A \mathbf{x} = \mathbf{b}\) 对 \(\mathbf{b}\) 中扰动的相对条件为
    \(\kappa_2(A)\)。特别是当 \(A\) 的列接近线性相关时，后者可能很大。这将在下一个例子中详细说明。
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be nonsingular. Then, for
    any \(\mathbf{b} \in \mathbb{R}^n\), there exists a unique solution to \(A \mathbf{x}
    = \mathbf{b}\), namely,'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 设 \(A \in \mathbb{R}^{n \times n}\) 为非奇异矩阵。那么，对于任何 \(\mathbf{b} \in
    \mathbb{R}^n\)，存在 \(A \mathbf{x} = \mathbf{b}\) 的唯一解，即，'
- en: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
- en: Suppose we solve the perturbed system
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们求解扰动系统
- en: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
- en: for some vector \(\delta\mathbf{b}\). We use the *Conditioning of Matrix-Vector
    Multiplication Theorem* to bound the norm of \(\delta\mathbf{x}\).
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个向量 \(\delta\mathbf{b}\)。我们使用 *矩阵-向量乘法的条件定理* 来界定 \(\delta\mathbf{x}\) 的范数。
- en: Specifically, set
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，设
- en: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
- en: Then
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
- en: and
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
- en: So we get that
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们得到，
- en: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
- en: Note also that, because \((A^{-1})^{-1} = A\), we have \(\kappa_2(A^{-1}) =
    \kappa_2(A)\). Rearranging, we finally get
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于 \((A^{-1})^{-1} = A\)，我们有 \(\kappa_2(A^{-1}) = \kappa_2(A)\)。重新排列后，我们最终得到
- en: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
- en: Hence the larger the condition number is, the larger the potential relative
    effect on the solution of the linear system is for a given relative perturbation
    size. \(\lhd\)
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，条件数越大，对于给定的相对扰动大小，线性系统解的潜在相对影响也越大。\(\lhd\)
- en: '**NUMERICAL CORNER:** In Numpy, the condition number of a matrix can be computed
    using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html).'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 在 Numpy 中，可以使用函数 `numpy.linalg.cond` 计算矩阵的条件数。[`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html)。'
- en: For example, orthogonal matrices have condition number \(1\), the lowest possible
    value for it (Why?). That indicates that orthogonal matrices have good numerical
    properties.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正交矩阵的条件数为 \(1\)，这是它的可能最低值（为什么？）。这表明正交矩阵具有良好的数值特性。
- en: '[PRE73]'
  id: totrans-791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-793
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: In contrast, matrices with nearly linearly dependent columns have large condition
    numbers.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，列几乎线性相关的矩阵具有较大的条件数。
- en: '[PRE77]'
  id: totrans-796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-798
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Let’s look at the SVD of \(A\).
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 \(A\) 的奇异值分解。
- en: '[PRE81]'
  id: totrans-801
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-802
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: We compute the solution to \(A \mathbf{x} = \mathbf{b}\) when \(\mathbf{b}\)
    is the left singular vector of \(A\) corresponding to the largest singular value.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case bound is achieved when \(\mathbf{z} =
    \mathbf{b}\) is right singular vector of \(M= A^{-1}\) corresponding to the lowest
    singular value. In a previous example, given a matrix \(A = \sum_{j=1}^n \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) in compact SVD form, we derived a compact SVD for
    the inverse as
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\mathbf{b}\) 是对应于最大奇异值的 \(A\) 的左奇异向量时，我们计算 \(A \mathbf{x} = \mathbf{b}\)
    的解。回顾在矩阵-向量乘法条件数定理的证明中，我们展示了当 \(\mathbf{z} = \mathbf{b}\) 是 \(M= A^{-1}\) 对应于最小奇异值的右奇异向量时，最坏情况界限被达到。在先前的例子中，给定一个矩阵
    \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) 在紧凑奇异值分解形式中，我们推导出逆的紧凑奇异值分解为
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
- en: Here, compared to the SVD of \(A\), the order of the singular values is reversed
    and the roles of the left and right singular vectors are exchanged. So we take
    \(\mathbf{b}\) to be the top left singular vector of \(A\).
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与 \(A\) 的奇异值分解相比，奇异值的顺序被颠倒，左奇异向量和右奇异向量的角色也交换了。因此，我们将 \(\mathbf{b}\) 取为 \(A\)
    的顶部左奇异向量。
- en: '[PRE83]'
  id: totrans-806
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-808
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: We make a small perturbation in the direction of the second right singular vector.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case is achieved when \(\mathbf{d} = \delta\mathbf{b}\)
    is top right singular vector of \(M = A^{-1}\). By the argument above, that is
    the left singular vector of \(A\) corresponding to the lowest singular value.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二个右奇异向量的方向上进行小的扰动。回顾在矩阵-向量乘法条件数定理的证明中，我们展示了当 \(\mathbf{d} = \delta\mathbf{b}\)
    是 \(M = A^{-1}\) 的右上奇异向量时，最坏情况发生。根据上述论证，这是 \(A\) 对应于最小奇异值的左奇异向量。
- en: '[PRE87]'
  id: totrans-811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The relative change in solution is:'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 解的相对变化为：
- en: '[PRE89]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-816
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-817
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Note that this is exactly the condition number of \(A\).
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这正是 \(A\) 的条件数。
- en: \(\unlhd\)
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Back to the least-squares problem** We return to the least-squares problem'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '**回到最小二乘问题** 我们回到最小二乘问题'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
- en: where
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{和} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
- en: We showed that the solution satisfies the normal equations
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明了该解满足正则方程
- en: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
- en: Here \(A\) may not be square and invertible. We define a more general notion
    of condition number.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 \(A\) 可能不是方阵且不可逆。我们定义一个更一般的条件数概念。
- en: '**DEFINITION** **(Condition number of a matrix: general case)** The condition
    number (in the induced \(2\)-norm) of a matrix \(A \in \mathbb{R}^{n \times m}\)
    is defined as'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（矩阵的条件数：一般情况）** 矩阵 \(A \in \mathbb{R}^{n \times m}\) 的条件数（在诱导的 \(2\)-范数下）定义为'
- en: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
- en: \(\natural\)
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: As we show next, the condition number of \(A^T A\) can be much larger than that
    of \(A\) itself.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们接下来要展示的，\(A^T A\) 的条件数可以远大于 \(A\) 本身。
- en: '**LEMMA** **(Condition number of \(A^T A\))** Let \(A \in \mathbb{R}^{n \times
    m}\) have full column rank. The'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（\(A^T A\) 的条件数）** 设 \(A \in \mathbb{R}^{n \times m}\) 具有满列秩。当'
- en: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
- en: \(\flat\)
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* We use the SVD.'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们使用奇异值分解（SVD）。'
- en: '*Proof:* Let \(A = U \Sigma V^T\) be an SVD of \(A\) with singular values \(\sigma_1
    \geq \cdots \geq \sigma_m > 0\). Then'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 设 \(A = U \Sigma V^T\) 是 \(A\) 的奇异值分解，具有奇异值 \(\sigma_1 \geq \cdots \geq
    \sigma_m > 0\)。那么'
- en: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
- en: In particular the latter expression is an SVD of \(A^T A\), and hence the condition
    number of \(A^T A\) is
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，后一种表达式是 \(A^T A\) 的奇异值分解，因此 \(A^T A\) 的条件数为
- en: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
- en: \(\square\)
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: '**NUMERICAL CORNER:** We give a quick example.'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角** 我们给出一个快速示例。'
- en: '[PRE93]'
  id: totrans-841
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-843
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-846
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'This observation – and the resulting increased numerical instability – is one
    of the reasons we previously developed an alternative approach to the least-squares
    problem. Quoting [Sol, Section 5.1]:'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: '这一观察结果——以及由此产生的数值不稳定性增加——是我们之前开发替代最小二乘问题方法的原因之一。引用 [Sol, 第5.1节]:'
- en: Intuitively, a primary reason that \(\mathrm{cond}(A^T A)\) can be large is
    that columns of \(A\) might look “similar” […] If two columns \(\mathbf{a}_i\)
    and \(\mathbf{a}_j\) satisfy \(\mathbf{a}_i \approx \mathbf{a}_j\), then the least-squares
    residual length \(\|\mathbf{b} - A \mathbf{x}\|_2\) will not suffer much if we
    replace multiples of \(\mathbf{a}_i\) with multiples of \(\mathbf{a}_j\) or vice
    versa. This wide range of nearly—but not completely—equivalent solutions yields
    poor conditioning. […] To solve such poorly conditioned problems, we will employ
    an alternative technique with closer attention to the column space of \(A\) rather
    than employing row operations as in Gaussian elimination. This strategy identifies
    and deals with such near-dependencies explicitly, bringing about greater numerical
    stability.
  id: totrans-848
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直观地说，\(\mathrm{cond}(A^T A)\) 可以很大的一个主要原因是 \(A\) 的列可能看起来“相似” [...] 如果两个列 \(\mathbf{a}_i\)
    和 \(\mathbf{a}_j\) 满足 \(\mathbf{a}_i \approx \mathbf{a}_j\)，那么如果我们用 \(\mathbf{a}_j\)
    的倍数替换 \(\mathbf{a}_i\) 的倍数，或者反之亦然，则最小二乘残差长度 \(\|\mathbf{b} - A \mathbf{x}\|_2\)
    不会受到很大影响。这种广泛但并非完全等效的解的范围会导致条件数差。 [...] 为了解决这种条件数差的问题，我们将采用一种替代技术，该技术更关注 \(A\)
    的列空间，而不是像高斯消元法那样使用行操作。这种策略明确识别和处理这种近依赖关系，从而提高了数值稳定性。
- en: \(\unlhd\)
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: We quote without proof a theorem from [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Theorem 4.2.7] which sheds further light on this issue.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引用了 [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408), 定理4.2.7]
    的定理，该定理进一步阐明了这个问题。
- en: '**THEOREM** **(Accuracy of Least-squares Solutions)** Let \(\mathbf{x}^*\)
    be the solution of the least-squares problem \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\). Let \(\mathbf{x}_{\mathrm{NE}}\) be the solution
    obtained by forming and solving the normal equations in [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    with rounding unit \(\epsilon_M\). Then \(\mathbf{x}_{\mathrm{NE}}\) satisfies'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（最小二乘解的精度）** 设 \(\mathbf{x}^*\) 为最小二乘问题 \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\) 的解。设 \(\mathbf{x}_{\mathrm{NE}}\) 为通过在 [浮点算术](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    中形成和求解正则方程得到的解，其中舍入单位为 \(\epsilon_M\)。那么 \(\mathbf{x}_{\mathrm{NE}}\) 满足'
- en: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
- en: Let \(\mathbf{x}_{\mathrm{QR}}\) be the solution obtained from a QR factorization
    in the same arithmetic. Then
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\mathbf{x}_{\mathrm{QR}}\) 为在同一算术中通过 QR 分解得到的解。然后
- en: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
- en: where \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) is the residual vector.
    The constants \(\gamma\) are slowly growing functions of the dimensions of the
    problem.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) 是残差向量。常数 \(\gamma\) 是问题维度的缓慢增长函数。
- en: \(\sharp\)
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: 'To explain, let’s quote [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Section 4.2.3] again:'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解释，让我们再次引用 [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    第4.2.3节]]:'
- en: The perturbation theory for the normal equations shows that \(\kappa_2^2(A)\)
    controls the size of the errors we can expect. The bound for the solution computed
    from the QR equation also has a term multiplied by \(\kappa_2^2(A)\), but this
    term is also multiplied by the scaled residual, which can diminish its effect.
    However, in many applications the vector \(\mathbf{b}\) is contaminated with error,
    and the residual can, in general, be no smaller than the size of that error.
  id: totrans-858
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正则方程的扰动理论表明 \(\kappa_2^2(A)\) 控制着我们可预期的误差大小。从 QR 方程计算出的解的界限也包含一个乘以 \(\kappa_2^2(A)\)
    的项，但这个项还乘以了缩放残差，这可能会减弱其效果。然而，在许多应用中，向量 \(\mathbf{b}\) 被误差污染，残差在一般情况下不能小于该误差的大小。
- en: '**NUMERICAL CORNER:** Here is a numerical example taken from [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    Lecture 19]. We will approximate the following function with a polynomial.'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落**：以下是一个从 [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC)，第
    19 节] 中取出的数值示例。我们将使用多项式来近似以下函数。'
- en: '[PRE99]'
  id: totrans-860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="切换隐藏内容">显示代码单元格源代码 隐藏代码单元格源代码</summary>
- en: '[PRE100]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE100]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
- en: We use a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix),
    which can be constructed using [`numpy.vander`](https://numpy.org/doc/stable/reference/generated/numpy.vander.html),
    to perform polynomial regression.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个 [范德蒙德矩阵](https://en.wikipedia.org/wiki/Vandermonde_matrix)，可以使用 `numpy.vander`
    来构造，来进行多项式回归。
- en: '[PRE101]'
  id: totrans-864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: The condition numbers of \(A\) and \(A^T A\) are both high in this case.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，\(A\) 和 \(A^T A\) 的条件数都很高。
- en: '[PRE102]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-867
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-868
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: We first use the normal equations and plot the residual vector.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用正则方程并绘制残差向量。
- en: '[PRE106]'
  id: totrans-871
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-872
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="切换隐藏内容">显示代码单元格源代码 隐藏代码单元格源代码</summary>
- en: '[PRE108]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE108]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
- en: We then use `numpy.linalg.qr` to compute the QR solution instead.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用 `numpy.linalg.qr` 来计算 QR 解。
- en: '[PRE109]'
  id: totrans-876
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-877
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="切换隐藏内容">显示代码单元格源代码 隐藏代码单元格源代码</summary>
- en: '[PRE111]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE111]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
- en: \(\unlhd\)
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.2.1\. Computing more singular vectors[#](#computing-more-singular-vectors
    "Link to this heading")
  id: totrans-881
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.1\. 计算更多奇异向量[#](#computing-more-singular-vectors "链接到本标题")
- en: We have shown how to compute the first singular vector. How do we compute more
    singular vectors? One approach is to first compute \(\mathbf{v}_1\) (or \(-\mathbf{v}_1\)),
    then find a vector \(\mathbf{y}\) orthogonal to it, and proceed as above. And
    then we repeat until we have all \(m\) right singular vectors.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何计算第一个奇异向量。我们如何计算更多的奇异向量？一种方法是首先计算 \(\mathbf{v}_1\)（或 \(-\mathbf{v}_1\)），然后找到一个与它正交的向量
    \(\mathbf{y}\)，然后像上面那样进行。然后我们重复这个过程，直到我们得到所有 \(m\) 个正确的奇异向量。
- en: 'We are often interested only in the top, say \(\ell < m\), singular vectors.
    An alternative approach in that case is to start with \(\ell\) random vectors
    and, first, find an orthonormal basis for the space they span. Then to quote [BHK,
    Section 3.7.1]:'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常只对前几个奇异向量感兴趣，比如说 \(\ell < m\)。在这种情况下，一个替代的方法是从 \(\ell\) 个随机向量开始，首先找到它们张成的空间的正交基。然后，引用
    [BHK, 第 3.7.1 节]：
- en: Then compute \(B\) times each of the basis vectors, and find an orthonormal
    basis for the space spanned by the resulting vectors. Intuitively, one has applied
    \(B\) to a subspace rather than a single vector. One repeatedly applies \(B\)
    to the subspace, calculating an orthonormal basis after each application to prevent
    the subspace collapsing to the one dimensional subspace spanned by the first singular
    vector.
  id: totrans-884
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后计算每个基向量乘以 \(B\)，并找到由这些结果向量张成的空间的正交基。直观上，我们是对一个子空间而不是单个向量应用 \(B\)。我们反复对子空间应用
    \(B\)，每次应用后计算一个正交基，以防止子空间塌缩到由第一个奇异向量张成的一维子空间。
- en: We will not prove here that this approach, known as orthogonal iteration, works.
    The proof is similar to that of the *Power Iteration Lemma*.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会证明这种称为正交迭代的方法是有效的。证明与 *幂迭代引理* 的证明类似。
- en: We implement this last algorithm. We will need our previous implementation of
    *Gram-Schimdt*.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现这个最后的算法。我们需要我们之前实现的 *Gram-Schimdt* 算法。
- en: '[PRE112]'
  id: totrans-887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Note that above we avoided forming the matrix \(A^T A\). With a small number
    of iterations, that approach potentially requires fewer arithmetic operations
    overall and it allows to take advantage of the possible sparsity of \(A\) (i.e.
    the fact that it may have many zeros).
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上面我们避免了形成矩阵 \(A^T A\)。使用少量迭代，这种方法可能需要更少的算术运算，并且可以利用 \(A\) 的可能稀疏性（即它可能有很多零）。
- en: '**NUMERICAL CORNER:** We apply it again to our two-cluster example.'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们再次将其应用于我们的两个簇示例。'
- en: '[PRE113]'
  id: totrans-890
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Let’s try again, but after projecting on the top two singular vectors. Recall
    that this corresponds to finding the best two-dimensional approximating subspace.
    The projection can be computed using the truncated SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\). We can interpret the rows of \(U_{(2)} \Sigma_{(2)}\) as the coefficients
    of each data point in the basis \(\mathbf{v}_1,\mathbf{v}_2\). We will work in
    that basis.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试，但这次在投影到前两个奇异向量之后。回忆一下，这对应于找到最佳二维逼近子空间。投影可以使用截断SVD \(Z= U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\) 来计算。我们可以将 \(U_{(2)} \Sigma_{(2)}\) 的行解释为每个数据点在基 \(\mathbf{v}_1,\mathbf{v}_2\)
    中的系数。我们将在这个基上工作。
- en: '[PRE114]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="切换隐藏内容">显示代码单元格源代码 隐藏代码单元格源代码</summary>
- en: '[PRE116]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE116]</details> ![../../_images/96a49f4906e0a0eef77ba149ea2af81c86bceb7368a32bb8e0a3c3e493a16e38.png](../Images/228cdfdbd198d0c26e06064852d3adbe.png)'
- en: Finally, looking at the first two right singular vectors, we see that the first
    one does align quite well with the first dimension.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，观察前两个右奇异向量，我们看到第一个与第一个维度相当吻合。
- en: '[PRE117]'
  id: totrans-897
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: \(\unlhd\)
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.2.2\. Pseudoinverse[#](#pseudoinverse "Link to this heading")
  id: totrans-900
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.2\. 伪逆[#](#pseudoinverse "链接到这个标题")
- en: The SVD leads to a natural generalization of the matrix inverse. First an observation.
    Recall that, to take the product of two square diagonal matrices, we simply multiply
    the corresponding diagonal entries. Let \(\Sigma \in \mathbb{R}^{r \times r}\)
    be a square diagonal matrix with diagonal entries \(\sigma_1,\ldots,\sigma_r\).
    If all diagonal entries are non-zero, then the matrix is invertible (since its
    columns then form a basis of the full space). The inverse of \(\Sigma\) in that
    case is simply the diagonal matrix \(\Sigma^{-1}\) with diagonal entries \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\).
    This can be confirmed by checking the definition of the inverse
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: SVD导致矩阵逆的自然推广。首先是一个观察。回忆一下，为了计算两个平方对角矩阵的乘积，我们只需乘以相应的对角元素。设 \(\Sigma \in \mathbb{R}^{r
    \times r}\) 是一个对角矩阵，其对角元素为 \(\sigma_1,\ldots,\sigma_r\)。如果所有对角元素都不为零，则该矩阵是可逆的（因为其列构成了整个空间的基）。在这种情况下，\(\Sigma\)
    的逆是一个对角矩阵 \(\Sigma^{-1}\)，其对角元素为 \(\sigma_1^{-1},\ldots,\sigma_r^{-1}\)。这可以通过检查逆的定义来确认
- en: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \Sigma \Sigma^{-1} = \Sigma^{-1} \Sigma = I_{r\times r}. \]
- en: We are ready for our main definition.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好我们的主要定义。
- en: '**DEFINITION** **(Pseudoinverse)** Let \(A \in \mathbb{R}^{n \times m}\) be
    a matrix with compact SVD \(A = U \Sigma V^T\) and singular values \(\sigma_1
    \geq \cdots \geq \sigma_r > 0\). A pseudoinverse \(A^+ \in \mathbb{R}^{m \times
    n}\) is defined as'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(伪逆)** 设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有紧凑SVD \(A = U \Sigma
    V^T\) 和奇异值 \(\sigma_1 \geq \cdots \geq \sigma_r > 0\) 的矩阵。伪逆 \(A^+ \in \mathbb{R}^{m
    \times n}\) 定义为'
- en: \[ A^+ = V \Sigma^{-1} U^T. \]
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = V \Sigma^{-1} U^T. \]
- en: \(\natural\)
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: While it is not obvious from the definition (why?), the pseudoinverse is in
    fact unique. To see that it is indeed a generalization of an inverse, we make
    a series of observations.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从定义中（为什么？）并不明显，但伪逆实际上是唯一的。为了看到它确实是逆的推广，我们进行了一系列观察。
- en: '*Observation 1:* Note that, using that \(U\) has orthonormal columns,'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '*观察1:* 注意，使用 \(U\) 的正交归一列，'
- en: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A A^+ = U \Sigma V^T V \Sigma^{-1} U^T = U U^T, \]
- en: which in general in not the identity matrix. Indeed it corresponds instead to
    the projection matrix onto the column space of \(A\), since the columns of \(U\)
    form an orthonormal basis of that linear subspace. As a result
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 这在一般情况下不是单位矩阵。实际上，它对应于投影矩阵到 \(A\) 的列空间，因为 \(U\) 的列构成了该线性子空间的正交归一基。因此
- en: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A A^+) A = U U^T U \Sigma V^T = U \Sigma V^T = A. \]
- en: So \(A A^+\) is not the identity matrix, but it does map the columns of \(A\)
    to themselves. Put differently, it is the identity map “when restricted to \(\mathrm{col}(A)\)”.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(A A^+\) 不是单位矩阵，但它确实将 \(A\) 的列映射到自身。换句话说，当限制在 \(\mathrm{col}(A)\) 上时，它是单位映射。
- en: Similarly,
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，
- en: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ A = V \Sigma^{-1} U^T U \Sigma V^T = V V^T \]
- en: is the projection matrix onto the row space of \(A\), and
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 是 \(A\) 的行空间的投影矩阵，并且
- en: \[ (A^+ A) A^+ = A^+. \]
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A^+ A) A^+ = A^+. \]
- en: '*Observation 2:* If \(A\) has full column rank \(m \leq n\), then \(r = m\).
    In that case, the columns of \(V\) form an orthonormal basis of all of \(\mathbb{R}^m\),
    i.e., \(V\) is orthogonal. Hence,'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: '*观察2:* 如果 \(A\) 有满列秩 \(m \leq n\)，那么 \(r = m\)。在这种情况下，\(V\) 的列构成 \(\mathbb{R}^m\)
    的正交基，即 \(V\) 是正交的。因此，'
- en: \[ A^+ A = V V^T = I_{m \times m}. \]
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ A = V V^T = I_{m \times m}. \]
- en: Similarly, if \(A\) has full row rank \(n \leq m\), then \(A A^+ = I_{n \times
    n}\).
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果 \(A\) 有满行秩 \(n \leq m\)，那么 \(A A^+ = I_{n \times n}\)。
- en: If both cases hold, then \(n = m\), i.e., \(A\) is square, and \(\mathrm{rk}(A)
    = n\), i.e., \(A\) is invertible. We then get
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这两种情况都成立，那么 \(n = m\)，即 \(A\) 是方阵，且 \(\mathrm{rk}(A) = n\)，即 \(A\) 是可逆的。然后我们得到
- en: \[ A A^+ = A^+ A = I_{n\times n}. \]
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A A^+ = A^+ A = I_{n\times n}. \]
- en: That implies that \(A^+ = A^{-1}\) by the *Existence of an Inverse Lemma* (which
    includes uniqueness of the matrix inverse).
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着根据 *逆的存在性引理*（它包括矩阵逆的唯一性），\(A^+ = A^{-1}\)。
- en: '*Observation 3:* Recall that, when \(A\) is nonsingular, the system \(A \mathbf{x}
    = \mathbf{b}\) admits the unique solution \(\mathbf{x} = A^{-1} \mathbf{b}\).
    In the overdetermined case, the pseudoinverse provides a solution to the linear
    least squares problem.'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '*观察3:* 回想一下，当 \(A\) 是非奇异的，系统 \(A \mathbf{x} = \mathbf{b}\) 承认唯一的解 \(\mathbf{x}
    = A^{-1} \mathbf{b}\)。在过定情况下，伪逆为线性最小二乘问题提供了一个解。'
- en: '**LEMMA** **(Pseudoinverse and Least Squares)** Let \(A \in \mathbb{R}^{n \times
    m}\) with \(m \leq n\). A solution to the linear least squares problem'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（伪逆和最小二乘）** 设 \(A \in \mathbb{R}^{n \times m}\) 且 \(m \leq n\)。线性最小二乘问题的解'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
- en: is given by \(\mathbf{x}^* = A^+ \mathbf{b}\). Further, if \(A\) has full column
    rank \(m\), then
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 由 \(\mathbf{x}^* = A^+ \mathbf{b}\) 给出。此外，如果 \(A\) 有满列秩 \(m\)，那么
- en: \[ A^+ = (A^T A)^{-1} A^T. \]
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = (A^T A)^{-1} A^T. \]
- en: \(\flat\)
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* For the first part, we use that the solution to the least squares
    problem is the orthogonal projection. For the second part, we use the SVD definition
    and check that the two sides are the same.'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 对于第一部分，我们使用最小二乘问题的解是正交投影的事实。对于第二部分，我们使用奇异值分解的定义并检查两边是否相同。'
- en: '*Proof:* Let \(A = U \Sigma V^T\) be a compact SVD of \(A\). For the first
    claim, note that the choice of \(\mathbf{x}^*\) in the statement gives'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 设 \(A = U \Sigma V^T\) 为 \(A\) 的紧凑奇异值分解。对于第一个断言，注意陈述中 \(\mathbf{x}^*\)
    的选择给出'
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b}. \]
- en: Hence, \(A \mathbf{x}^*\) is the orthogonal projection of \(\mathbf{b}\) onto
    the column space of \(A\) - which we proved previously is the solution to the
    linear least squares problem.
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(A \mathbf{x}^*\) 是 \(\mathbf{b}\) 在 \(A\) 的列空间上的正交投影——这是我们之前证明的线性最小二乘问题的解。
- en: Now onto the second claim. Recall that, when \(A\) is of full rank, the matrix
    \(A^T A\) is nonsingular. We then note that, using the notation \(\Sigma^{-2}
    = (\Sigma^{-1})^2\),
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是第二个断言。回想一下，当 \(A\) 是满秩时，矩阵 \(A^T A\) 是非奇异的。我们注意到，使用符号 \(\Sigma^{-2} = (\Sigma^{-1})^2\)，
- en: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (A^T A)^{-1} A^T = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T = V \Sigma^{-2}
    V^T V \Sigma U^T = A^+ \]
- en: as claimed. Here, we used that \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix. \(\square\)
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 如所断言。在这里，我们使用了 \((V \Sigma^2 V^T)^{-1} = V \Sigma^{-2} V^T\)，这可以通过检查逆的定义和我们对对角矩阵逆的先前观察来证实。
    \(\square\)
- en: 'The pseudoinverse also provides a solution in the case of an underdetermined
    system. Here, however, there are in general infinitely many solutions. The one
    chosen by the pseudoinverse has a special property as we see now: it is the least
    norm solution.'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 伪逆在欠定系统的情况下也提供了一个解。然而，这里通常有无穷多个解。伪逆选择的解具有一个特殊性质，我们现在可以看到：它是最小范数解。
- en: '**LEMMA** **(Pseudoinverse and Underdetermined Systems)** Let \(A \in \mathbb{R}^{n
    \times m}\) with \(m > n\) and \(\mathbf{b} \in \mathbb{R}^n\). Further assume
    that \(A\) has full row rank \(n\). Then \(\mathbf{x}^* = A^+ \mathbf{b}\) is
    a solution to'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（伪逆与欠定系统）** 设 \(A \in \mathbb{R}^{n \times m}\) 且 \(m > n\)，\(\mathbf{b}
    \in \mathbb{R}^n\)。进一步假设 \(A\) 具有满秩 \(n\)。那么 \(\mathbf{x}^* = A^+ \mathbf{b}\)
    是以下方程的解'
- en: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min \left\{ \|\mathbf{x}\|\,:\, \mathbf{x} \in \mathbb{R}^m\ \text{s.t.}\
    A\mathbf{x} = \mathbf{b} \right\}. \]
- en: Moreover, in that case,
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这种情况下，
- en: \[ A^+ = A^T (A A^T)^{-1}. \]
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^+ = A^T (A A^T)^{-1}. \]
- en: \(\flat\)
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* We first prove the formula for \(A^+\). As we did in the overdetermined
    case, it can be checked by substituting a compact SVD \(A = U \Sigma V^T\). Recall
    that, when \(A^T\) is of full rank, the matrix \(A A^T\) is nonsingular. We then
    note that'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们首先证明 \(A^+\) 的公式。正如我们在超定情况中所做的那样，可以通过代入紧凑的奇异值分解 \(A = U \Sigma V^T\)
    来验证。回想一下，当 \(A^T\) 具有满秩时，矩阵 \(A A^T\) 是非奇异的。我们接着注意到'
- en: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T (A A^T)^{-1} = V \Sigma U^T (U \Sigma V^T V \Sigma U^T)^{-1} = V \Sigma
    U^T U \Sigma^{-2} U^T = A^+ \]
- en: as claimed. Here, we used that \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\),
    which can be confirmed by checking the definition of an inverse and our previous
    observation about the inverse of a diagonal matrix.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。在这里，我们使用了 \((U \Sigma^2 U^T)^{-1} = U \Sigma^{-2} U^T\)，这可以通过检查逆的定义和我们的关于对角矩阵逆的先前观察来证实。
- en: Because \(A\) has full row rank, \(\mathbf{b} \in \mathrm{col}(A)\) and there
    is at least one \(\mathbf{x}\) such that \(A \mathbf{x} = \mathbf{b}\). One such
    solution is provided by the pseudoinverse. Indeed, from a previous observation,
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(A\) 具有满秩，\(\mathbf{b} \in \mathrm{col}(A)\)，并且至少存在一个 \(\mathbf{x}\) 使得
    \(A \mathbf{x} = \mathbf{b}\)。伪逆提供了一个这样的解。实际上，从之前的观察中，
- en: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{x}^* = A A^+ \mathbf{b} = U U^T \mathbf{b} = I_{n \times n} \mathbf{b}
    = \mathbf{b}, \]
- en: where we used that the columns of \(U\) form an orthonormal basis of \(\mathbb{R}^n\)
    by the rank assumption.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(U\) 的列向量构成 \(\mathbb{R}^n\) 的正交基这一秩假设。
- en: Let \(\mathbf{x}\) be any other solution to the system. Then \(A(\mathbf{x}
    - \mathbf{x}^*) = \mathbf{b} - \mathbf{b} = \mathbf{0}\). That implies
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\mathbf{x}\) 是该系统的一个其他解。那么 \(A(\mathbf{x} - \mathbf{x}^*) = \mathbf{b} -
    \mathbf{b} = \mathbf{0}\)。这表明
- en: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{x} - \mathbf{x}^*)^T \mathbf{x}^* = (\mathbf{x} - \mathbf{x}^*)^T
    A^T (A A^T)^{-1} \mathbf{b} = [A(\mathbf{x} - \mathbf{x}^*)]^T (A A^T)^{-1} \mathbf{b}
    = \mathbf{0}. \]
- en: In words, \(\mathbf{x} - \mathbf{x}^*\) and \(\mathbf{x}^*\) are orthogonal.
    By *Pythagoras*,
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 从字面上讲，\(\mathbf{x} - \mathbf{x}^*\) 和 \(\mathbf{x}^*\) 是正交的。根据 *毕达哥拉斯* 定理，
- en: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{x}\|^2 = \|(\mathbf{x} - \mathbf{x}^*) + \mathbf{x}^*\|^2 = \|\mathbf{x}
    - \mathbf{x}^*\|^2 + \|\mathbf{x}^*\|^2 \geq \|\mathbf{x}^*\|^2. \]
- en: That proves that \(\mathbf{x}^*\) has the smallest norm among all solutions
    to the system \(A \mathbf{x} = \mathbf{b}\). \(\square\)
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了 \(\mathbf{x}^*\) 在所有满足 \(A \mathbf{x} = \mathbf{b}\) 的解中具有最小的范数。 \(\square\)
- en: '**EXAMPLE:** Continuing a previous example, let'
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: '**例:** 继续之前的例子，设'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: Recall that
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下
- en: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T \]
- en: where
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{u}_1 = \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix},
    \quad \quad \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}, \quad \text{and}
    \quad \sigma_1 = \sqrt{2}. \end{split}\]
- en: We compute the pseudoinverse. By the formula, in the rank one case, it is simply
    (Check this!)
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算伪逆。根据公式，在秩为 1 的情况下，它简单地是（检查这个！）
- en: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^+ = \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T = \frac{1}{\sqrt{2}}
    \begin{pmatrix} 1\\ 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2}\\ -1/\sqrt{2} \end{pmatrix}^T
    = \begin{pmatrix} 1/2 & -1/2\\ 0 & 0 \end{pmatrix} \end{split}\]
- en: \(\lhd\)
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be a square nonsingular
    matrix. Let \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be a compact
    SVD of \(A\), where we used the fact that the rank of \(A\) is \(n\) so it has
    \(n\) strictly positive singular values. We seek to compute \(\|A^{-1}\|_2\) in
    terms of the singular values.'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: '**例子：** 设 \(A \in \mathbb{R}^{n \times n}\) 为一个方阵非奇异矩阵。设 \(A = \sum_{j=1}^n
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) 为 \(A\) 的紧凑奇异值分解，其中我们使用了 \(A\) 的秩为 \(n\)
    因此它有 \(n\) 个严格正的奇异值的事实。我们寻求用奇异值来计算 \(\|A^{-1}\|_2\)。'
- en: Because \(A\) is invertible, \(A^+ = A^{-1}\). So we compute the pseudoinverse
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(A\) 是可逆的，\(A^+ = A^{-1}\)。所以我们计算伪逆
- en: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = A^+ = \sum_{j=1}^n \sigma_j^{-1} \mathbf{v}_j \mathbf{u}_j^T. \]
- en: The sum on the right-hand side is not quite a compact SVD of \(A^{-1}\) because
    the coefficients \(\sigma_j^{-1}\) are non-decreasing in \(j\).
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的求和并不完全是 \(A^{-1}\) 的紧凑奇异值分解，因为系数 \(\sigma_j^{-1}\) 在 \(j\) 上是非递减的。
- en: But writing the sum in reverse order
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 但是将求和顺序反过来
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T \]
- en: does give a compact SVD of \(A^{-1}\), since \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1}
    > 0\) and \(\{\mathbf{v}_j\}_{j=1}^n\) and \(\{\mathbf{u}_j\}_{j=1}^n\) are orthonormal
    lists. Hence, the \(2\)-norm is given by the largest singular value, that is,
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 确实给出了 \(A^{-1}\) 的紧凑奇异值分解，因为 \(\sigma_n^{-1} \geq \cdots \sigma_1^{-1} > 0\)，并且
    \(\{\mathbf{v}_j\}_{j=1}^n\) 和 \(\{\mathbf{u}_j\}_{j=1}^n\) 是正交归一列表。因此，\(2\)-范数由最大的奇异值给出，即，
- en: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A^{-1}\|_2 = \sigma_n^{-1}. \]
- en: \(\lhd\)
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**NUMERICAL CORNER:** In Numpy, the pseudoinverse of a matrix can be computed
    using the function [`numpy.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html).'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 在 Numpy 中，可以使用函数 `numpy.linalg.pinv` 计算矩阵的伪逆[链接](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html)。'
- en: '[PRE119]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-975
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-977
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Let’s try our previous example.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试之前的例子。
- en: '[PRE125]'
  id: totrans-979
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-980
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-982
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: \(\unlhd\)
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.2.3\. Condition numbers[#](#condition-numbers "Link to this heading")
  id: totrans-984
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2.3\. 条件数[#](#condition-numbers "链接到本标题")
- en: In this section we introduce condition numbers, a measure of perturbation sensitivity
    for numerical problems. We look in particular at the conditioning of the least-squares
    problem. We begin with the concept of pseudoinverse, which is important in its
    own right.
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍条件数，它是数值问题的扰动敏感度的一个度量。我们特别关注最小二乘问题的条件数。我们首先从伪逆的概念开始，这个概念本身很重要。
- en: '**Conditioning of matrix-vector multiplication** We define the condition number
    of a matrix and show that it captures some information about the sensitivity to
    perturbations of matrix-vector multiplications.'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵-向量乘法的条件数** 我们定义矩阵的条件数并展示它捕捉了矩阵-向量乘法对扰动的敏感性的一些信息。'
- en: '**DEFINITION** **(Condition number of a matrix)** The condition number (in
    the induced \(2\)-norm) of a square, nonsingular matrix \(A \in \mathbb{R}^{n
    \times n}\) is defined as'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（矩阵的条件数）** 方阵、非奇异矩阵 \(A \in \mathbb{R}^{n \times n}\) 的条件数（在诱导的 \(2\)-范数下）定义为'
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2. \]
- en: \(\natural\)
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: In fact, this can be computed as
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这可以计算为
- en: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_1}{\sigma_n} \]
- en: where we used the example above. In words, \(\kappa_2(A)\) is the ratio of the
    largest to the smallest stretching under \(A\).
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了上面的例子。用语言来说，\(\kappa_2(A)\) 是在 \(A\) 下最大拉伸与最小拉伸的比率。
- en: '**THEOREM** **(Conditioning of Matrix-Vector Multiplication)** Let \(M \in
    \mathbb{R}^{n \times n}\) be nonsingular. Then, for any \(\mathbf{z} \in \mathbb{R}^n\),'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（矩阵-向量乘法的条件数）** 设 \(M \in \mathbb{R}^{n \times n}\) 为非奇异。那么，对于任何 \(\mathbf{z}
    \in \mathbb{R}^n\)，'
- en: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max_{\mathbf{d} \neq \mathbf{0}} \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|}
    {\|\mathbf{d}\|/\|\mathbf{z}\|} \leq \kappa_2(M) \]
- en: and the inequality is tight in the sense that there is an \(\mathbf{x}\) and
    a \(\mathbf{d}\) that achieves it.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 并且不等式在紧致的意义上是成立的，即存在一个 \(\mathbf{x}\) 和一个 \(\mathbf{d}\) 可以达到它。
- en: \(\sharp\)
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: The ratio above measures the worst rate of relative change in \(M \mathbf{z}\)
    under infinitesimal perturbations of \(\mathbf{z}\). The theorem says that when
    \(\kappa_2(M)\) is large, a case referred to as ill-conditioning, large relative
    changes in \(M \mathbf{z}\) can be obtained from relatively small perturbations
    to \(\mathbf{z}\). In words, a matrix-vector product is potentially sensitive
    to perturbations when the matrix is ill-conditioned.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 上述比率衡量了在 \(\mathbf{z}\) 的无穷小扰动下 \(M \mathbf{z}\) 的相对变化的最坏速率。定理表明，当 \(\kappa_2(M)\)
    很大时，即所谓的病态情况，从对 \(\mathbf{z}\) 的相对较小的扰动可以得到 \(M \mathbf{z}\) 的较大相对变化。换句话说，当矩阵病态时，矩阵-向量乘积对扰动是敏感的。
- en: '*Proof:* Write'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 写出'
- en: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    = \frac{\|M \mathbf{d}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|} = \frac{\|M
    (\mathbf{d}/\|\mathbf{d}\|)\|}{\|M(\mathbf{z}/\|\mathbf{z}\|)\|} \leq \frac{\sigma_1}{\sigma_n}
    \]
- en: where we used \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\),
    which was shown in a previous example.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\min_{\mathbf{u} \in \mathbb{S}^{n-1}} \|A \mathbf{u}\| = \sigma_n\)，这在之前的例子中已经证明过。
- en: In particular, we see that the ratio can achieve its maximum by taking \(\mathbf{d}\)
    and \(\mathbf{z}\) to be the right singular vectors corresponding to \(\sigma_1\)
    and \(\sigma_n\) respectively. \(\square\)
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们看到，通过将 \(\mathbf{d}\) 和 \(\mathbf{z}\) 分别取为对应于 \(\sigma_1\) 和 \(\sigma_n\)
    的右奇异向量，比率可以达到其最大值。 \(\square\)
- en: If we apply the theorem to the inverse instead, we get that the relative conditioning
    of the nonsingular linear system \(A \mathbf{x} = \mathbf{b}\) to perturbations
    in \(\mathbf{b}\) is \(\kappa_2(A)\). The latter can be large in particular when
    the columns of \(A\) are close to linearly dependent. This is detailed in the
    next example.
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将定理应用于逆矩阵，我们得到非奇异线性系统 \(A \mathbf{x} = \mathbf{b}\) 对 \(\mathbf{b}\) 的扰动的相对条件数为
    \(\kappa_2(A)\)。特别是，当 \(A\) 的列接近线性相关时，后者可能很大。这将在下一个例子中详细说明。
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times n}\) be nonsingular. Then, for
    any \(\mathbf{b} \in \mathbb{R}^n\), there exists a unique solution to \(A \mathbf{x}
    = \mathbf{b}\), namely,'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 设 \(A \in \mathbb{R}^{n \times n}\) 是非奇异的。那么，对于任何 \(\mathbf{b} \in
    \mathbb{R}^n\)，存在一个唯一的解 \(A \mathbf{x} = \mathbf{b}\)，即，'
- en: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x} = A^{-1} \mathbf{b}. \]
- en: Suppose we solve the perturbed system
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们求解扰动系统
- en: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x} + \delta\mathbf{x} = A^{-1} (\mathbf{b} + \delta\mathbf{b}), \]
- en: for some vector \(\delta\mathbf{b}\). We use the *Conditioning of Matrix-Vector
    Multiplication Theorem* to bound the norm of \(\delta\mathbf{x}\).
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个向量 \(\delta\mathbf{b}\)。我们使用 *矩阵-向量乘法的条件定理* 来界定 \(\delta\mathbf{x}\) 的范数。
- en: Specifically, set
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，设
- en: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M = A^{-1}, \qquad \mathbf{z} = \mathbf{b}, \qquad \mathbf{d} = \delta\mathbf{b}.
    \]
- en: Then
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M \mathbf{z} = A^{-1}\mathbf{b} = \mathbf{x}, \]
- en: and
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: \[ M(\mathbf{z}+\mathbf{d}) - M \mathbf{z} = A^{-1}(\mathbf{b} + \delta\mathbf{b})
    - A^{-1}\mathbf{b} = \mathbf{x} + \delta\mathbf{x} - \mathbf{x} = \delta\mathbf{x}.
    \]
- en: So we get that
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们得到
- en: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\delta\mathbf{x}\|/\|\mathbf{x}\|}{\|\delta\mathbf{b}\|/\|\mathbf{b}\|}
    =\frac{\|M(\mathbf{z}+\mathbf{d}) - M \mathbf{z}\|/\|M\mathbf{z}\|} {\|\mathbf{d}\|/\|\mathbf{z}\|}
    \leq \kappa_2(M) = \kappa_2(A^{-1}). \]
- en: Note also that, because \((A^{-1})^{-1} = A\), we have \(\kappa_2(A^{-1}) =
    \kappa_2(A)\). Rearranging, we finally get
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，因为 \((A^{-1})^{-1} = A\)，所以我们有 \(\kappa_2(A^{-1}) = \kappa_2(A)\)。重新排列，我们最终得到
- en: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa_2(A) \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}.
    \]
- en: Hence the larger the condition number is, the larger the potential relative
    effect on the solution of the linear system is for a given relative perturbation
    size. \(\lhd\)
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，条件数越大，对于给定的相对扰动大小，线性系统解的潜在相对影响就越大。 \(\lhd\)
- en: '**NUMERICAL CORNER:** In Numpy, the condition number of a matrix can be computed
    using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html).'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落** 在 Numpy 中，可以使用函数 `numpy.linalg.cond` 计算矩阵的条件数（[链接](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html)）。'
- en: For example, orthogonal matrices have condition number \(1\), the lowest possible
    value for it (Why?). That indicates that orthogonal matrices have good numerical
    properties.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正交矩阵的条件数为 \(1\)，这是它的最低可能值（为什么？）。这表明正交矩阵具有良好的数值特性。
- en: '[PRE129]'
  id: totrans-1021
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-1023
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-1024
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: In contrast, matrices with nearly linearly dependent columns have large condition
    numbers.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，列几乎线性相关的矩阵具有很大的条件数。
- en: '[PRE133]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-1027
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-1029
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Let’s look at the SVD of \(A\).
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 \(A\) 的奇异值分解。
- en: '[PRE137]'
  id: totrans-1031
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-1032
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: We compute the solution to \(A \mathbf{x} = \mathbf{b}\) when \(\mathbf{b}\)
    is the left singular vector of \(A\) corresponding to the largest singular value.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case bound is achieved when \(\mathbf{z} =
    \mathbf{b}\) is right singular vector of \(M= A^{-1}\) corresponding to the lowest
    singular value. In a previous example, given a matrix \(A = \sum_{j=1}^n \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) in compact SVD form, we derived a compact SVD for
    the inverse as
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\mathbf{b}\) 是 \(A\) 对应于最大奇异值的左奇异向量时，我们计算 \(A \mathbf{x} = \mathbf{b}\)
    的解。回顾在矩阵-向量乘法条件定理的证明中，我们展示了当 \(\mathbf{z} = \mathbf{b}\) 是 \(M= A^{-1}\) 对应于最小奇异值的右奇异向量时，最坏情况界限得到实现。在一个先前的例子中，给定一个矩阵
    \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) 在紧凑奇异值分解形式中，我们推导出逆的紧凑奇异值分解为
- en: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = \sigma_n^{-1} \mathbf{v}_n \mathbf{u}_n^T + \sigma_{n-1}^{-1} \mathbf{v}_{n-1}
    \mathbf{u}_{n-1}^T + \cdots + \sigma_1^{-1} \mathbf{v}_1 \mathbf{u}_1^T. \]
- en: Here, compared to the SVD of \(A\), the order of the singular values is reversed
    and the roles of the left and right singular vectors are exchanged. So we take
    \(\mathbf{b}\) to be the top left singular vector of \(A\).
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与 \(A\) 的奇异值分解相比，奇异值的顺序被颠倒，左和右奇异向量的角色也交换了。因此，我们取 \(\mathbf{b}\) 为 \(A\)
    的顶部左奇异向量。
- en: '[PRE139]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-1037
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-1038
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-1039
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: We make a small perturbation in the direction of the second right singular vector.
    Recall that in the proof of the *Conditioning of Matrix-Vector Multiplication
    Theorem*, we showed that the worst case is achieved when \(\mathbf{d} = \delta\mathbf{b}\)
    is top right singular vector of \(M = A^{-1}\). By the argument above, that is
    the left singular vector of \(A\) corresponding to the lowest singular value.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二个右奇异向量的方向上进行小的扰动。回顾在矩阵-向量乘法条件定理的证明中，我们展示了当 \(\mathbf{d} = \delta\mathbf{b}\)
    是 \(M = A^{-1}\) 的右上奇异向量时，最坏情况发生。根据上述论证，这是 \(A\) 对应于最小奇异值的左奇异向量。
- en: '[PRE143]'
  id: totrans-1041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-1042
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'The relative change in solution is:'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 解的相对变化为：
- en: '[PRE145]'
  id: totrans-1044
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-1045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-1046
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-1047
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Note that this is exactly the condition number of \(A\).
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这正是 \(A\) 的条件数。
- en: \(\unlhd\)
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Back to the least-squares problem** We return to the least-squares problem'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '**回到最小二乘问题** 我们回到最小二乘问题'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\| \]
- en: where
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{和} \quad \mathbf{b} = \begin{pmatrix} b_1
    \\ \vdots \\ b_n \end{pmatrix}. \end{split}\]
- en: We showed that the solution satisfies the normal equations
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明了解满足正则方程
- en: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{x} = A^T \mathbf{b}. \]
- en: Here \(A\) may not be square and invertible. We define a more general notion
    of condition number.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 \(A\) 可能不是方阵且不可逆。我们定义一个更一般的概念——条件数。
- en: '**DEFINITION** **(Condition number of a matrix: general case)** The condition
    number (in the induced \(2\)-norm) of a matrix \(A \in \mathbb{R}^{n \times m}\)
    is defined as'
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(矩阵的条件数：一般情况**) 矩阵 \(A \in \mathbb{R}^{n \times m}\) 的条件数（在诱导的 \(2\)-范数下）定义为'
- en: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A) = \|A\|_2 \|A^+\|_2. \]
- en: \(\natural\)
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: As we show next, the condition number of \(A^T A\) can be much larger than that
    of \(A\) itself.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们接下来要展示的，\(A^T A\) 的条件数可能远大于 \(A\) 本身。
- en: '**LEMMA** **(Condition number of \(A^T A\))** Let \(A \in \mathbb{R}^{n \times
    m}\) have full column rank. The'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(矩阵 \(A^T A\) 的条件数**) 设 \(A \in \mathbb{R}^{n \times m}\) 具有满列秩。'
- en: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A^T A) = \kappa_2(A)^2. \]
- en: \(\flat\)
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* We use the SVD.'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们使用奇异值分解（SVD）。'
- en: '*Proof:* Let \(A = U \Sigma V^T\) be an SVD of \(A\) with singular values \(\sigma_1
    \geq \cdots \geq \sigma_m > 0\). Then'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 设 \(A = U \Sigma V^T\) 是 \(A\) 的奇异值分解，具有奇异值 \(\sigma_1 \geq \cdots \geq
    \sigma_m > 0\)。那么'
- en: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A = V \Sigma U^T U \Sigma V^T = V \Sigma^2 V^T. \]
- en: In particular the latter expression is an SVD of \(A^T A\), and hence the condition
    number of \(A^T A\) is
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是后一种表达式是 \(A^T A\) 的奇异值分解（SVD），因此 \(A^T A\) 的条件数是
- en: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \kappa_2(A^T A) = \frac{\sigma_1^2}{\sigma_m^2} = \kappa_2(A)^2. \]
- en: \(\square\)
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: '**NUMERICAL CORNER:** We give a quick example.'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们给出一个快速示例。'
- en: '[PRE149]'
  id: totrans-1071
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-1072
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-1073
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-1074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-1075
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-1076
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'This observation – and the resulting increased numerical instability – is one
    of the reasons we previously developed an alternative approach to the least-squares
    problem. Quoting [Sol, Section 5.1]:'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观察结果——以及由此产生的数值不稳定性增加——是我们之前开发替代最小二乘问题方法的原因之一。引用 [Sol, 第 5.1 节]：
- en: Intuitively, a primary reason that \(\mathrm{cond}(A^T A)\) can be large is
    that columns of \(A\) might look “similar” […] If two columns \(\mathbf{a}_i\)
    and \(\mathbf{a}_j\) satisfy \(\mathbf{a}_i \approx \mathbf{a}_j\), then the least-squares
    residual length \(\|\mathbf{b} - A \mathbf{x}\|_2\) will not suffer much if we
    replace multiples of \(\mathbf{a}_i\) with multiples of \(\mathbf{a}_j\) or vice
    versa. This wide range of nearly—but not completely—equivalent solutions yields
    poor conditioning. […] To solve such poorly conditioned problems, we will employ
    an alternative technique with closer attention to the column space of \(A\) rather
    than employing row operations as in Gaussian elimination. This strategy identifies
    and deals with such near-dependencies explicitly, bringing about greater numerical
    stability.
  id: totrans-1078
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直观地说，\(\mathrm{cond}(A^T A)\) 可以很大的一个主要原因是 \(A\) 的列可能看起来“相似” [...] 如果两个列 \(\mathbf{a}_i\)
    和 \(\mathbf{a}_j\) 满足 \(\mathbf{a}_i \approx \mathbf{a}_j\)，那么如果我们用 \(\mathbf{a}_j\)
    的倍数替换 \(\mathbf{a}_i\) 的倍数，或者反之亦然，则最小二乘残差长度 \(\|\mathbf{b} - A \mathbf{x}\|_2\)
    不会受到很大影响。这种广泛但并非完全等价解的范围导致条件数差。 [...] 为了解决这种条件数差的问题，我们将采用一种替代技术，更关注 \(A\) 的列空间，而不是像高斯消元法那样使用行操作。这种策略明确识别并处理这种近相关性，从而提高了数值稳定性。
- en: \(\unlhd\)
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: We quote without proof a theorem from [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Theorem 4.2.7] which sheds further light on this issue.
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引用了 [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408), 定理 4.2.7]
    的定理，该定理进一步阐明了这个问题。
- en: '**THEOREM** **(Accuracy of Least-squares Solutions)** Let \(\mathbf{x}^*\)
    be the solution of the least-squares problem \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\). Let \(\mathbf{x}_{\mathrm{NE}}\) be the solution
    obtained by forming and solving the normal equations in [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    with rounding unit \(\epsilon_M\). Then \(\mathbf{x}_{\mathrm{NE}}\) satisfies'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（最小二乘解的精度）** 设 \(\mathbf{x}^*\) 是最小二乘问题 \(\min_{\mathbf{x} \in \mathbb{R}^m}
    \|A \mathbf{x} - \mathbf{b}\|\) 的解。设 \(\mathbf{x}_{\mathrm{NE}}\) 是通过在 [浮点算术](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
    中形成和求解正规方程得到的解。那么 \(\mathbf{x}_{\mathrm{NE}}\) 满足'
- en: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\mathbf{x}_{\mathrm{NE}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    \gamma_{\mathrm{NE}} \kappa_2^2(A) \left( 1 + \frac{\|\mathbf{b}\|}{\|A\|_2 \|\mathbf{x}^*\|}
    \right) \epsilon_M. \]
- en: Let \(\mathbf{x}_{\mathrm{QR}}\) be the solution obtained from a QR factorization
    in the same arithmetic. Then
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\mathbf{x}_{\mathrm{QR}}\) 是在同一算术中从 QR 分解得到的解。然后
- en: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\|\mathbf{x}_{\mathrm{QR}} - \mathbf{x}^*\|}{\|\mathbf{x}^*\|} \leq
    2 \gamma_{\mathrm{QR}} \kappa_2(A) \epsilon_M + \gamma_{\mathrm{NE}} \kappa_2^2(A)
    \frac{\|\mathbf{r}^*\|}{\|A\|_2 \|\mathbf{x}^*\|} \epsilon_M \]
- en: where \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) is the residual vector.
    The constants \(\gamma\) are slowly growing functions of the dimensions of the
    problem.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{r}^* = \mathbf{b} - A \mathbf{x}^*\) 是残差向量。常数 \(\gamma\) 是问题维度的缓慢增长函数。
- en: \(\sharp\)
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: 'To explain, let’s quote [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    Section 4.2.3] again:'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释，让我们再次引用 [[Ste](https://epubs.siam.org/doi/book/10.1137/1.9781611971408),
    第 4.2.3 节]：
- en: The perturbation theory for the normal equations shows that \(\kappa_2^2(A)\)
    controls the size of the errors we can expect. The bound for the solution computed
    from the QR equation also has a term multiplied by \(\kappa_2^2(A)\), but this
    term is also multiplied by the scaled residual, which can diminish its effect.
    However, in many applications the vector \(\mathbf{b}\) is contaminated with error,
    and the residual can, in general, be no smaller than the size of that error.
  id: totrans-1088
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正则方程的扰动理论表明 \(\kappa_2^2(A)\) 控制着我们期望的误差的大小。从 QR 方程计算出的解的界限也有一个乘以 \(\kappa_2^2(A)\)
    的项，但这个项也乘以了缩放残差，这可能会减弱其效果。然而，在许多应用中，向量 \(\mathbf{b}\) 被误差污染，残差通常不能小于该误差的大小。
- en: '**NUMERICAL CORNER:** Here is a numerical example taken from [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    Lecture 19]. We will approximate the following function with a polynomial.'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 这里是一个从 [[TB](https://books.google.com/books/about/Numerical_Linear_Algebra.html?id=JaPtxOytY7kC),
    第19讲] 中取出的数值示例。我们将使用多项式来近似以下函数。'
- en: '[PRE155]'
  id: totrans-1090
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE156]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE156]</details> ![../../_images/a5591c9fb33c2a3c6b622a58c056c60cb6b83b95a2267fb326f4f5dfb41ae698.png](../Images/52b7594aff73e374e33a057acf067173.png)'
- en: We use a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix),
    which can be constructed using [`numpy.vander`](https://numpy.org/doc/stable/reference/generated/numpy.vander.html),
    to perform polynomial regression.
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 [Vandermonde 矩阵](https://en.wikipedia.org/wiki/Vandermonde_matrix)，它可以通过
    `numpy.vander` 构建，来进行多项式回归。
- en: '[PRE157]'
  id: totrans-1094
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: The condition numbers of \(A\) and \(A^T A\) are both high in this case.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，\(A\) 和 \(A^T A\) 的条件数都很高。
- en: '[PRE158]'
  id: totrans-1096
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-1097
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-1098
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-1099
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: We first use the normal equations and plot the residual vector.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用正则方程并绘制残差向量。
- en: '[PRE162]'
  id: totrans-1101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-1102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE164]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE164]</details> ![../../_images/b8cda4aba98857ca56535ad321d81001c4c2a202a087718443bd516cc440b8c1.png](../Images/72048f928bbd56712403f6fcd40485f5.png)'
- en: We then use `numpy.linalg.qr` to compute the QR solution instead.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 `numpy.linalg.qr` 来计算 QR 解。
- en: '[PRE165]'
  id: totrans-1106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-1107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE167]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE167]</details> ![../../_images/58f4ebdf12a8e7a258bcbab8c2db97da72285d0359386607072fcffb1a215368.png](../Images/bbd4e97cc1a1f337b74aadd8f86710aa.png)'
- en: \(\unlhd\)
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 4.8.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  id: totrans-1111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8.3\. 额外证明[#](#additional-proofs "链接到这个标题")
- en: '**Proof of Greedy Finds Best Subspace** In this section, we prove the full
    version of the *Greedy Finds Best Subspace Theorem*. In particular, we do not
    use the *Spectral Theorem*.'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪寻找最佳子空间证明**: 在本节中，我们证明 *贪婪寻找最佳子空间定理* 的完整版本。特别是，我们不使用 *谱定理*。'
- en: '*Proof idea:* We proceed by induction. For an arbitrary orthonormal list \(\mathbf{w}_1,\ldots,\mathbf{w}_k\),
    we find an orthonormal basis of their span containing an element orthogonal to
    \(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1}\). Then we use the defintion of \(\mathbf{v}_k\)
    to conclude.'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路*: 我们通过归纳法进行。对于任意正交归一列表 \(\mathbf{w}_1,\ldots,\mathbf{w}_k\)，我们找到包含与 \(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1}\)
    正交的元素的它们张成的正交基。然后我们使用 \(\mathbf{v}_k\) 的定义来得出结论。'
- en: '*Proof:* We reformulate the problem as a maximization'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 我们将问题重新表述为最大化问题'
- en: \[ \max \left\{ \sum_{j=1}^k \|A \mathbf{w}_j\|^2\ :\ \text{$\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}$
    is an orthonormal list} \right\}, \]
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \max \left\{ \sum_{j=1}^k \|A \mathbf{w}_j\|^2\ :\ \text{$\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}$
    是一个正交归一列表} \right\}, \]
- en: where we also replace the \(k\)-dimensional linear subspace \(\mathcal{Z}\)
    with an arbitrary orthonormal basis \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\).
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，我们还用任意正交归一基 \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\) 替换了 \(k\) 维线性子空间 \(\mathcal{Z}\)。
- en: We then proceed by induction. For \(k=1\), we define \(\mathbf{v}_1\) as a solution
    of the above maximization problem. Assume that, for any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_\ell\}\)
    with \(\ell < k\), we have
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后通过归纳法进行。对于 \(k=1\)，我们定义 \(\mathbf{v}_1\) 为上述最大化问题的解。假设对于任何正交列表 \(\{\mathbf{w}_1,\ldots,\mathbf{w}_\ell\}\)
    且 \(\ell < k\)，我们有
- en: \[ \sum_{j=1}^\ell \|A \mathbf{w}_j\|^2 \leq \sum_{j=1}^\ell \|A \mathbf{v}_j\|^2.
    \]
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^\ell \|A \mathbf{w}_j\|^2 \leq \sum_{j=1}^\ell \|A \mathbf{v}_j\|^2.
    \]
- en: Now consider any orthonormal list \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\) and
    let its span be \(\mathcal{W} = \mathrm{span}(\mathbf{w}_1,\ldots,\mathbf{w}_k)\).
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑任何正交列表 \(\{\mathbf{w}_1,\ldots,\mathbf{w}_k\}\)，并令其生成集为 \(\mathcal{W} =
    \mathrm{span}(\mathbf{w}_1,\ldots,\mathbf{w}_k)\)。
- en: '***Step 1:*** For \(j=1,\ldots,k-1\), let \(\mathbf{v}_j''\) be the orthogonal
    projection of \(\mathbf{v}_j\) onto \(\mathcal{W}\) and let \(\mathcal{V}'' =
    \mathrm{span}(\mathbf{v}''_1,\ldots,\mathbf{v}''_{k-1})\). Because \(\mathcal{V}''
    \subseteq \mathcal{W}\) has dimension at most \(k-1\) while \(\mathcal{W}\) itself
    has dimension \(k\), we can find an orthonormal basis \(\mathbf{w}''_1,\ldots,\mathbf{w}''_{k}\)
    of \(\mathcal{W}\) such that \(\mathbf{w}''_k\) is orthogonal to \(\mathcal{V}''\)
    (Why?). Then, for any \(j=1,\ldots,k-1\), we have the decomposition \(\mathbf{v}_j
    = \mathbf{v}''_j + (\mathbf{v}_j - \mathbf{v}''_j)\) where \(\mathbf{v}''_j \in
    \mathcal{V}''\) is orthogonal to \(\mathbf{w}''_k\) and \(\mathbf{v}_j - \mathbf{v}''_j\)
    is also orthogonal to \(\mathbf{w}''_k \in \mathcal{W}\) by the properties of
    the orthogonal projection onto \(\mathcal{W}\). Hence'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 1:*** 对于 \(j=1,\ldots,k-1\)，令 \(\mathbf{v}_j''\) 为 \(\mathbf{v}_j\) 在
    \(\mathcal{W}\) 上的正交投影，并令 \(\mathcal{V}'' = \mathrm{span}(\mathbf{v}''_1,\ldots,\mathbf{v}''_{k-1})\)。因为
    \(\mathcal{V}'' \subseteq \mathcal{W}\) 的维度最多为 \(k-1\)，而 \(\mathcal{W}\) 本身的维度为
    \(k\)，我们可以找到一个 \(\mathcal{W}\) 的正交基 \(\mathbf{w}''_1,\ldots,\mathbf{w}''_{k}\)，使得
    \(\mathbf{w}''_k\) 与 \(\mathcal{V}''\) 正交（为什么？）。然后，对于任意的 \(j=1,\ldots,k-1\)，我们有分解
    \(\mathbf{v}_j = \mathbf{v}''_j + (\mathbf{v}_j - \mathbf{v}''_j)\)，其中 \(\mathbf{v}''_j
    \in \mathcal{V}''\) 与 \(\mathbf{w}''_k\) 正交，且 \(\mathbf{v}_j - \mathbf{v}''_j\)
    也与 \(\mathbf{w}''_k \in \mathcal{W}\) 正交，这是由于正交投影到 \(\mathcal{W}\) 的性质。因此'
- en: \[\begin{align*} \left\langle \sum_{j=1}^{k-1}\beta_j \mathbf{v}_j, \mathbf{w}'_k
    \right\rangle &= \left\langle \sum_{j=1}^{k-1}\beta_j [\mathbf{v}'_j + (\mathbf{v}_j
    - \mathbf{v}'_j)], \mathbf{w}'_k \right\rangle\\ &= \left\langle \sum_{j=1}^{k-1}\beta_j
    \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \left\langle \sum_{j=1}^{k-1}\beta_j
    (\mathbf{v}_j - \mathbf{v}'_j), \mathbf{w}'_k \right\rangle\\ &= \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}_j - \mathbf{v}'_j, \mathbf{w}'_k \right\rangle\\ &= 0
    \end{align*}\]
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \left\langle \sum_{j=1}^{k-1}\beta_j \mathbf{v}_j, \mathbf{w}'_k
    \right\rangle &= \left\langle \sum_{j=1}^{k-1}\beta_j [\mathbf{v}'_j + (\mathbf{v}_j
    - \mathbf{v}'_j)], \mathbf{w}'_k \right\rangle\\ &= \left\langle \sum_{j=1}^{k-1}\beta_j
    \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \left\langle \sum_{j=1}^{k-1}\beta_j
    (\mathbf{v}_j - \mathbf{v}'_j), \mathbf{w}'_k \right\rangle\\ &= \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}'_j, \mathbf{w}'_k \right\rangle + \sum_{j=1}^{k-1}\beta_j
    \left\langle \mathbf{v}_j - \mathbf{v}'_j, \mathbf{w}'_k \right\rangle\\ &= 0
    \end{align*}\]
- en: for any \(\beta_j\)’s. That is, \(\mathbf{w}'_k\) is orthogonal to \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\).
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意的 \(\beta_j\)。也就是说，\(\mathbf{w}'_k\) 与 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\)
    正交。
- en: '***Step 2:*** By the induction hypothesis, we have'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 2:*** 根据归纳假设，我们有'
- en: \[ (*) \qquad \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 \leq \sum_{j=1}^{k-1} \|A
    \mathbf{v}_j\|^2\. \]
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (*) \qquad \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 \leq \sum_{j=1}^{k-1} \|A
    \mathbf{v}_j\|^2\. \]
- en: Moreover, recalling that the \(\boldsymbol{\alpha}_i^T\)’s are the rows of \(A\),
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，回忆一下，\(\boldsymbol{\alpha}_i^T\) 是矩阵 \(A\) 的行，
- en: \[ (**) \qquad \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{i=1}^n \|\mathrm{proj}_{\mathcal{W}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{j=1}^k \|A \mathbf{w}_j'\|^2 \]
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (**) \qquad \sum_{j=1}^k \|A \mathbf{w}_j\|^2 = \sum_{i=1}^n \|\mathrm{proj}_{\mathcal{W}}(\boldsymbol{\alpha}_i)\|^2
    = \sum_{j=1}^k \|A \mathbf{w}_j'\|^2 \]
- en: since the \(\mathbf{w}_j\)’s and \(\mathbf{w}'_j\)’s form an orthonormal basis
    of the same subspace \(\mathcal{W}\). Since \(\mathbf{w}'_k\) is orthogonal to
    \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\) by the conclusion of Step
    1, by the definition of \(\mathbf{v}_k\) as a solution to
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(\mathbf{w}_j\) 和 \(\mathbf{w}'_j\) 形成了相同子空间 \(\mathcal{W}\) 的正交基。由于 \(\mathbf{w}'_k\)
    根据步骤 1 的结论与 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k-1})\) 正交，根据 \(\mathbf{v}_k\)
    作为解的定义
- en: \[ \mathbf{v}_k\in \arg\max \{\|A \mathbf{v}\|:\|\mathbf{v}\| = 1, \ \langle
    \mathbf{v}, \mathbf{v}_j \rangle = 0, \forall j \leq k-1\}. \]
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_k\in \arg\max \{\|A \mathbf{v}\|:\|\mathbf{v}\| = 1, \ \langle
    \mathbf{v}, \mathbf{v}_j \rangle = 0, \forall j \leq k-1\}. \]
- en: we have
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: \[ (*\!*\!*) \qquad \|A \mathbf{w}'_k\|^2 \leq \|A \mathbf{v}_k\|^2. \]
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (*\!*\!*) \qquad \|A \mathbf{w}'_k\|^2 \leq \|A \mathbf{v}_k\|^2. \]
- en: '***Step 3:*** Putting everything together'
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: '***步骤 3：*** 将所有内容组合起来'
- en: \[\begin{align*} \sum_{j=1}^k \|A \mathbf{w}_j\|^2 &= \sum_{j=1}^k \|A \mathbf{w}_j'\|^2
    &\text{by $(**)$}\\ &= \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 + \|A \mathbf{w}'_k\|^2\\
    &\leq \sum_{j=1}^{k-1} \|A \mathbf{v}_j\|^2 + \|A \mathbf{v}_k\|^2 &\text{by $(*)$
    and $(*\!*\!*)$}\\ &= \sum_{j=1}^{k} \|A \mathbf{v}_j\|^2\\ \end{align*}\]
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{j=1}^k \|A \mathbf{w}_j\|^2 &= \sum_{j=1}^k \|A \mathbf{w}_j'\|^2
    &\text{根据 $(**)$}\\ &= \sum_{j=1}^{k-1} \|A \mathbf{w}'_j\|^2 + \|A \mathbf{w}'_k\|^2\\
    &\leq \sum_{j=1}^{k-1} \|A \mathbf{v}_j\|^2 + \|A \mathbf{v}_k\|^2 &\text{根据 $(*)$
    和 $(*\!*\!*)$}\\ &= \sum_{j=1}^{k} \|A \mathbf{v}_j\|^2\\ \end{align*}\]
- en: which proves the claim. \(\square\)
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了所声称的。 \(\square\)
- en: '**Proof of Existence of the SVD** We return to the proof of the *Existence
    of SVD Theorem*. We give an alternative proof that does not rely on the *Spectral
    Theorem*.'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVD 存在性的证明** 我们回到 SVD 存在定理的证明。我们给出一个不依赖于 **谱定理** 的替代证明。'
- en: '*Proof:* We have already done most of the work. The proof works as follows:'
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：** 我们已经完成了大部分工作。证明如下：'
- en: (1) Compute the greedy sequence \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) from the
    *Greedy Finds Best Subspace Theorem* until the largest \(r\) such that
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 根据 **贪婪寻找最佳子空间定理**，从 \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) 计算贪婪序列，直到最大的 \(r\)，使得
- en: \[ \|A \mathbf{v}_r\|^2 > 0 \]
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{v}_r\|^2 > 0 \]
- en: or, otherwise, until \(r=m\). The \(\mathbf{v}_j\)’s are orthonormal by construction.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，否则，直到 \(r=m\)。由于 \(\mathbf{v}_j\) 是通过构造正交的。
- en: (2) For \(j=1,\ldots,r\), let
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 对于 \(j=1,\ldots,r\)，设
- en: \[ \sigma_j = \|A \mathbf{v}_j\| \quad\text{and}\quad \mathbf{u}_j = \frac{1}{\sigma_j}
    A \mathbf{v}_j. \]
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_j = \|A \mathbf{v}_j\| \quad\text{和}\quad \mathbf{u}_j = \frac{1}{\sigma_j}
    A \mathbf{v}_j. \]
- en: 'Observe that, by our choice of \(r\), the \(\sigma_j\)’s are \(> 0\). They
    are also non-increasing: by definition of the greedy sequence,'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，根据我们选择的 \(r\)，\(\sigma_j\) 是 \(> 0\) 的。它们也是非递增的：根据贪婪序列的定义，
- en: \[ \sigma_i^2 = \max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \ \langle
    \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}, \]
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_i^2 = \max \{\|A \mathbf{w}_i\|^2 :\|\mathbf{w}_i\| = 1, \ \langle
    \mathbf{w}_i, \mathbf{v}_j \rangle = 0, \forall j \leq i-1\}, \]
- en: where the set of orthogonality constraints gets larger as \(i\) increases. Hence,
    the \(\mathbf{u}_j\)’s have unit norm by definition.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，随着 \(i\) 的增加，正交约束的集合变得更大。因此，根据定义，\(\mathbf{u}_j\) 的范数是单位范数。
- en: We show below that they are also orthogonal.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下面将证明它们也是正交的。
- en: (3) Let \(\mathbf{z} \in \mathbb{R}^m\) be any vector. To show that our construction
    is correct, we prove that \(A \mathbf{z} = \left(U \Sigma V^T\right)\mathbf{z}\).
    Let \(\mathcal{V} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\) and decompose
    \(\mathbf{z}\) into orthogonal components
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 设 \(\mathbf{z} \in \mathbb{R}^m\) 为任意向量。为了证明我们的构造是正确的，我们证明 \(A \mathbf{z}
    = \left(U \Sigma V^T\right)\mathbf{z}\)。设 \(\mathcal{V} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_r)\)
    并将 \(\mathbf{z}\) 分解为正交分量
- en: \[ \mathbf{z} = \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) + (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}))
    = \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle \,\mathbf{v}_j + (\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \]
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z} = \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) + (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}))
    = \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle \,\mathbf{v}_j + (\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \]
- en: Applying \(A\) and using linearity, we get
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 \(A\) 并使用线性性质，我们得到
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, A\mathbf{v}_j + A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \end{align*}\]
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, A\mathbf{v}_j + A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})). \end{align*}\]
- en: We claim that \(A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})) = \mathbf{0}\).
    If \(\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) = \mathbf{0}\), that
    is certainly the case. If not, let
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声称 \(A (\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})) = \mathbf{0}\)。如果
    \(\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z}) = \mathbf{0}\)，那么当然如此。如果不是，那么
- en: \[ \mathbf{w} = \frac{\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})}{\|\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})\|}. \]
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w} = \frac{\mathbf{z} - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})}{\|\mathbf{z}
    - \mathrm{proj}_{\mathcal{V}}(\mathbf{z})\|}. \]
- en: By definition of \(r\),
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义 \(r\)，
- en: \[ 0 = \max \{\|A \mathbf{w}_{r+1}\|^2 :\|\mathbf{w}_{r+1}\| = 1, \ \langle
    \mathbf{w}_{r+1}, \mathbf{v}_j \rangle = 0, \forall j \leq r\}. \]
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: \[ 0 = \max \{\|A \mathbf{w}_{r+1}\|^2 :\|\mathbf{w}_{r+1}\| = 1, \ \langle
    \mathbf{w}_{r+1}, \mathbf{v}_j \rangle = 0, \forall j \leq r\}. \]
- en: Put differently, \(\|A \mathbf{w}_{r+1}\|^2 = 0\) (i.e. \(A \mathbf{w}_{r+1}
    = \mathbf{0}\)), for any unit vector \(\mathbf{w}_{r+1}\) orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_r\).
    That applies in particular to \(\mathbf{w}_{r+1} = \mathbf{w}\) by the *Orthogonal
    Projection Theorem*.
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，\(\|A \mathbf{w}_{r+1}\|^2 = 0\)（即 \(A \mathbf{w}_{r+1} = \mathbf{0}\)），对于任何与
    \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) 正交的单位向量 \(\mathbf{w}_{r+1}\)。这一点特别适用于 \(\mathbf{w}_{r+1}
    = \mathbf{w}\)，根据**正交投影定理**。
- en: Hence, using the definition of \(\mathbf{u}_j\) and \(\sigma_j\), we get
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用 \(\mathbf{u}_j\) 和 \(\sigma_j\) 的定义，我们得到
- en: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, \sigma_j \mathbf{u}_j\\ &= \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
    \mathbf{z}\\ &=\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{z}\\
    &= \left(U \Sigma V^T\right)\mathbf{z}. \end{align*}\]
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{z} &= \sum_{j=1}^r \langle \mathbf{z}, \mathbf{v}_j\rangle
    \, \sigma_j \mathbf{u}_j\\ &= \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
    \mathbf{z}\\ &=\left(\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right)\mathbf{z}\\
    &= \left(U \Sigma V^T\right)\mathbf{z}. \end{align*}\]
- en: That proves the existence of the SVD.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了奇异值分解的存在性。
- en: All that is left to prove is the orthogonality of the \(\mathbf{u}_j\)’s.
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下要证明的是 \(\mathbf{u}_j\) 的正交性。
- en: '**LEMMA** **(Left Singular Vectors are Orthogonal)** For all \(1 \leq i \neq
    j \leq r\), \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0\). \(\flat\)'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(左奇异向量是正交的)** 对于所有 \(1 \leq i \neq j \leq r\)，\(\langle \mathbf{u}_i,
    \mathbf{u}_j \rangle = 0\)。 \(\flat\)'
- en: '*Proof idea:* Quoting [BHK, Section 3.6]:'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 引用[BHK, 第3.6节]:'
- en: Intuitively if \(\mathbf{u}_i\) and \(\mathbf{u}_j\), \(i < j\), were not orthogonal,
    one would suspect that the right singular vector \(\mathbf{v}_j\) had a component
    of \(\mathbf{v}_i\) which would contradict that \(\mathbf{v}_i\) and \(\mathbf{v}_j\)
    were orthogonal. Let \(i\) be the smallest integer such that \(\mathbf{u}_i\)
    is not orthogonal to all other \(\mathbf{u}_j\). Then to prove that \(\mathbf{u}_i\)
    and \(\mathbf{u}_j\) are orthogonal, we add a small component of \(\mathbf{v}_j\)
    to \(\mathbf{v}_i\), normalize the result to be a unit vector \(\mathbf{v}'_i
    \propto \mathbf{v}_i + \varepsilon \mathbf{v}_j\) and show that \(\|A \mathbf{v}'_i\|
    > \|A \mathbf{v}_i\|\), a contradiction.
  id: totrans-1160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直观上，如果 \(\mathbf{u}_i\) 和 \(\mathbf{u}_j\)（\(i < j\)）不正交，人们会怀疑右奇异向量 \(\mathbf{v}_j\)
    有 \(\mathbf{v}_i\) 的一个分量，这将与 \(\mathbf{v}_i\) 和 \(\mathbf{v}_j\) 正交相矛盾。设 \(i\)
    是最小的整数，使得 \(\mathbf{u}_i\) 与所有其他 \(\mathbf{u}_j\) 不正交。然后为了证明 \(\mathbf{u}_i\)
    和 \(\mathbf{u}_j\) 正交，我们向 \(\mathbf{v}_i\) 添加一个小的 \(\mathbf{v}_j\) 分量，将结果归一化以成为一个单位向量
    \(\mathbf{v}'_i \propto \mathbf{v}_i + \varepsilon \mathbf{v}_j\)，并证明 \(\|A \mathbf{v}'_i\|
    > \|A \mathbf{v}_i\|\)，这是矛盾的。
- en: '*Proof:* We argue by contradiction. Let \(i\) be the smallest index such that
    there is a \(j > i\) with \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = \delta
    \neq 0\). Assume \(\delta > 0\) (otherwise use \(-\mathbf{u}_i\)). For \(\varepsilon
    \in (0,1)\), because the \(\mathbf{v}_k\)’s are orthonormal, \(\|\mathbf{v}_i
    + \varepsilon \mathbf{v}_j\|^2 = 1+\varepsilon^2\). Consider the vectors'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们通过反证法来论证。设 \(i\) 是最小的索引，使得存在 \(j > i\)，使得 \(\langle \mathbf{u}_i, \mathbf{u}_j
    \rangle = \delta \neq 0\)。假设 \(\delta > 0\)（否则使用 \(-\mathbf{u}_i\)）。对于 \(\varepsilon
    \in (0,1)\)，因为 \(\mathbf{v}_k\) 是正交归一的，\(\|\mathbf{v}_i + \varepsilon \mathbf{v}_j\|^2
    = 1+\varepsilon^2\)。考虑以下向量'
- en: \[ \mathbf{v}'_i = \frac{\mathbf{v}_i + \varepsilon \mathbf{v}_j}{\sqrt{1+\varepsilon^2}}
    \quad\text{and}\quad A \mathbf{v}'_i = \frac{\sigma_i \mathbf{u}_i + \varepsilon
    \sigma_j \mathbf{u}_j}{\sqrt{1+\varepsilon^2}}. \]
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}'_i = \frac{\mathbf{v}_i + \varepsilon \mathbf{v}_j}{\sqrt{1+\varepsilon^2}}
    \quad\text{and}\quad A \mathbf{v}'_i = \frac{\sigma_i \mathbf{u}_i + \varepsilon
    \sigma_j \mathbf{u}_j}{\sqrt{1+\varepsilon^2}}. \]
- en: Observe that \(\mathbf{v}'_i\) is orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{i-1}\),
    so that
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mathbf{v}'_i\) 与 \(\mathbf{v}_1,\ldots,\mathbf{v}_{i-1}\) 正交，因此
- en: \[ \|A \mathbf{v}'_i\| \leq \|A \mathbf{v}_i\| =\sigma_i. \]
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{v}'_i\| \leq \|A \mathbf{v}_i\| =\sigma_i. \]
- en: On the other hand, by the *Orthogonal Decomposition Lemma*, we can write \(A
    \mathbf{v}_i'\) as a sum of its orthogonal projection on the unit vector \(\mathbf{u}_i\)
    and \(A \mathbf{v}_i' - \mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\), which
    is orthogonal to \(\mathbf{u}_i\). In particular, by *Pythagoras*, \(\|A \mathbf{v}_i'\|
    \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|\). But that implies, for
    \(\varepsilon \in (0,1)\),
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，根据**正交分解引理**，我们可以将 \(A \mathbf{v}_i'\) 写成其在单位向量 \(\mathbf{u}_i\) 上的正交投影和
    \(A \mathbf{v}_i' - \mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\) 的和，后者与 \(\mathbf{u}_i\)
    正交。特别是，根据**毕达哥拉斯定理**，\(\|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A
    \mathbf{v}_i')\|\)。但这意味着，对于 \(\varepsilon \in (0,1)\)，
- en: \[ \|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|
    = \left\langle \mathbf{u}_i, A \mathbf{v}_i'\right\rangle = \frac{\sigma_i + \varepsilon
    \sigma_j \delta}{\sqrt{1+\varepsilon^2}} \geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2) \]
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{v}_i'\| \geq \|\mathrm{proj}_{\mathbf{u}_i}(A \mathbf{v}_i')\|
    = \left\langle \mathbf{u}_i, A \mathbf{v}_i'\right\rangle = \frac{\sigma_i + \varepsilon
    \sigma_j \delta}{\sqrt{1+\varepsilon^2}} \geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2) \]
- en: where the second inequality follows from a [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series)
    or the observation
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个不等式来源于 [泰勒展开](https://en.wikipedia.org/wiki/Taylor_series) 或观察
- en: \[ (1+\varepsilon^2)\,(1-\varepsilon^2/2)^2 = (1+\varepsilon^2)\,(1-\varepsilon^2
    + \varepsilon^4/4) = 1 - 3/4 \varepsilon^4 + \varepsilon^6/4 \leq 1. \]
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (1+\varepsilon^2)\,(1-\varepsilon^2/2)^2 = (1+\varepsilon^2)\,(1-\varepsilon^2
    + \varepsilon^4/4) = 1 - 3/4 \varepsilon^4 + \varepsilon^6/4 \leq 1. \]
- en: Now note that
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意
- en: \[\begin{align*} \|A \mathbf{v}_i'\| &\geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2)\\ &= \sigma_i + \varepsilon \sigma_j \delta - \varepsilon^2\sigma_i/2
    - \varepsilon^3 \sigma_i \sigma_j \delta/2\\ &= \sigma_i + \varepsilon \left(
    \sigma_j \delta - \varepsilon\sigma_i/2 - \varepsilon^2 \sigma_i \sigma_j \delta/2\right)\\
    &> \sigma_i \end{align*}\]
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|A \mathbf{v}_i'\| &\geq (\sigma_i + \varepsilon \sigma_j
    \delta) \,(1-\varepsilon^2/2)\\ &= \sigma_i + \varepsilon \sigma_j \delta - \varepsilon^2\sigma_i/2
    - \varepsilon^3 \sigma_i \sigma_j \delta/2\\ &= \sigma_i + \varepsilon \left(
    \sigma_j \delta - \varepsilon\sigma_i/2 - \varepsilon^2 \sigma_i \sigma_j \delta/2\right)\\
    &> \sigma_i \end{align*}\]
- en: for \(\varepsilon\) small enough, contradicting the inequality above. \(\square\)
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于足够小的 \(\varepsilon\)，与上述不等式矛盾。 \(\square\)
- en: '**SVD and Approximating Subspace** In constructing the SVD of \(A\), we used
    the greedy sequence for the best approximating subspace. Vice versa, given an
    SVD of \(A\), we can read off the solution to the approximating subspace problem.
    In other words, there was nothing special about the specific construction we used
    to prove existence of the SVD. While a matrix may have many SVDs, they all give
    a solution to the approximating subspace problems.'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解与逼近子空间** 在构造 \(A\) 的奇异值分解时，我们使用了最佳逼近子空间的贪婪序列。反之，给定 \(A\) 的奇异值分解，我们可以直接读出逼近子空间问题的解。换句话说，我们用来证明奇异值分解存在性的具体构造并没有什么特别之处。虽然一个矩阵可能有多个奇异值分解，但它们都给出了逼近子空间问题的解。'
- en: Further, this perspective gives what is known as a variational characterization
    of the singular values. We will have more to say about variational characterizations
    and their uses in the next chapter.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种观点给出了奇异值已知的变分特征。我们将在下一章中更多地讨论变分特征及其应用。
- en: '*SVD and greedy sequence* Indeed, let'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解与贪婪序列** 实际上，设'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: be an SVD of \(A\) with
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 是 \(A\) 的一个奇异值分解，其中
- en: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0. \]
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0. \]
- en: We show that the \(\mathbf{v}_j\)’s form a greedy sequence for the approximating
    subspace problem. Complete \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) into an orthonormal
    basis of \(\mathbb{R}^m\) by adding appropriate vectors \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\).
    By construction, \(A\mathbf{v}_{i} = \mathbf{0}\) for all \(i=j+1,\ldots,m\).
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们证明 \(\mathbf{v}_j\) 形成了逼近子空间问题的贪婪序列。通过添加适当的向量 \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\)，将
    \(\mathbf{v}_1,\ldots,\mathbf{v}_r\) 完整地扩展为 \(\mathbb{R}^m\) 的一个正交归一基。根据构造，对于所有
    \(i=j+1,\ldots,m\)，有 \(A\mathbf{v}_{i} = \mathbf{0}\)。
- en: We start with the case \(j=1\). For any unit vector \(\mathbf{w} \in \mathbb{R}^m\),
    we expand it as \(\mathbf{w} = \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle
    \,\mathbf{v}_i\). By the *Properties of Orthonormal Lists*,
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 \(j=1\) 的情况开始。对于任意单位向量 \(\mathbf{w} \in \mathbb{R}^m\)，我们可以将其展开为 \(\mathbf{w}
    = \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i\)。根据 **正交归一列表的性质**，
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| A \left( \sum_{i=1}^m \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i \right) \right\|^2\\ &= \left\|
    \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\
    &= \left\| \sum_{i=1}^r \langle \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i
    \right\|^2\\ &= \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2,
    \end{align*}\]
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| A \left( \sum_{i=1}^m \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i \right) \right\|^2\\ &= \left\|
    \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\
    &= \left\| \sum_{i=1}^r \langle \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i
    \right\|^2\\ &= \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2,
    \end{align*}\]
- en: where we used the orthonormality of the \(\mathbf{u}_i\)’s and the fact that
    \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\) are in the null space of \(A\). Because
    the \(\sigma_i\)’s are non-increasing, this last sum is maximized by taking \(\mathbf{w}
    = \mathbf{v}_1\). So we have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\mathbf{u}_i\) 的正交性以及 \(\mathbf{v}_{j+1},\ldots,\mathbf{v}_m\) 在 \(A\)
    的零空间中的事实。因为 \(\sigma_i\) 是非递增的，所以这个最后的和通过取 \(\mathbf{w} = \mathbf{v}_1\) 来最大化。因此，我们已经证明了
    \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1\}\)。
- en: By the *Properties of Orthonormal Lists*,
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 *正交列表的性质*，
- en: \[ \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = \left\|\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle \mathbf{v}_i \right\|^2 = \|\mathbf{w}\|^2
    = 1. \]
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = \left\|\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle \mathbf{v}_i \right\|^2 = \|\mathbf{w}\|^2
    = 1. \]
- en: Hence, because the \(\sigma_i\)’s are non-increasing, the sum
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，因为 \(\sigma_i\) 是非递增的，所以这个和
- en: \[ \|A \mathbf{w}\|^2 = \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    \leq \sigma_1^2. \]
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A \mathbf{w}\|^2 = \sum_{i=1}^r \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    \leq \sigma_1^2. \]
- en: This upper bound is achieved by taking \(\mathbf{w} = \mathbf{v}_1\). So we
    have shown that \(\mathbf{v}_1 \in \arg\max\{\|A \mathbf{w}\|^2:\|\mathbf{w}\|
    = 1\}\).
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个上界是通过取 \(\mathbf{w} = \mathbf{v}_1\) 来实现的。因此，我们已经证明了 \(\mathbf{v}_1 \in \arg\max\{\|A
    \mathbf{w}\|^2:\|\mathbf{w}\| = 1\}\)。
- en: More generally, for any unit vector \(\mathbf{w} \in \mathbb{R}^m\) that is
    orthogonal to \(\mathbf{v}_1,\ldots,\mathbf{v}_{j-1}\), we expand it as \(\mathbf{w}
    = \sum_{i=j}^m \langle \mathbf{w}, \mathbf{v}_i\rangle \,\mathbf{v}_i\). Then,
    as long as \(j\leq r\),
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，对于任何与 \(\mathbf{v}_1,\ldots,\mathbf{v}_{j-1}\) 正交的单位向量 \(\mathbf{w} \in
    \mathbb{R}^m\)，我们将其展开为 \(\mathbf{w} = \sum_{i=j}^m \langle \mathbf{w}, \mathbf{v}_i\rangle
    \,\mathbf{v}_i\). 然后，只要 \(j\leq r\),
- en: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| \sum_{i=j}^m \langle \mathbf{w},
    \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\ &= \left\| \sum_{i=j}^r \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i \right\|^2\\ &= \sum_{i=j}^r
    \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2\\ &\leq \sigma_j^2. \end{align*}\]
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|A \mathbf{w}\|^2 &= \left\| \sum_{i=j}^m \langle \mathbf{w},
    \mathbf{v}_i\rangle \,A \mathbf{v}_i \right\|^2\\ &= \left\| \sum_{i=j}^r \langle
    \mathbf{w}, \mathbf{v}_i\rangle \,\sigma_i \mathbf{u}_i \right\|^2\\ &= \sum_{i=j}^r
    \sigma_i^2 \langle \mathbf{w}, \mathbf{v}_i\rangle^2\\ &\leq \sigma_j^2. \end{align*}\]
- en: where again we used that the \(\sigma_i\)’s are non-increasing and \(\sum_{i=1}^m
    \langle \mathbf{w}, \mathbf{v}_i\rangle^2 = 1\). This last bound is achieved by
    taking \(\mathbf{w} = \mathbf{v}_j\).
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们再次使用了 \(\sigma_i\) 是非递增的，并且 \(\sum_{i=1}^m \langle \mathbf{w}, \mathbf{v}_i\rangle^2
    = 1\). 这个最后的界限是通过取 \(\mathbf{w} = \mathbf{v}_j\) 来实现的。
- en: So we have shown the following.
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经证明了以下内容。
- en: '**THEOREM** **(Variational Characterization of Singular Values)** Let \(A =
    \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) be an SVD of \(A\) with \(\sigma_1
    \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\). Then'
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(奇异值的变分特征)** 设 \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)
    是 \(A\) 的奇异值分解，其中 \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\)。那么'
- en: \[ \sigma_j^2 = \max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle \mathbf{w},
    \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}, \]
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_j^2 = \max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle \mathbf{w},
    \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}, \]
- en: and
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbf{v}_j\in \arg\max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle
    \mathbf{w}, \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}. \]
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_j\in \arg\max \{\|A \mathbf{w}\|^2:\|\mathbf{w}\| = 1, \ \langle
    \mathbf{w}, \mathbf{v}_i \rangle = 0, \forall i \leq j-1\}. \]
- en: \(\sharp\)
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
