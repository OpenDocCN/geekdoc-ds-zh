- en: Memory Latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://en.algorithmica.org/hpc/cpu-cache/latency/](https://en.algorithmica.org/hpc/cpu-cache/latency/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Despite that [bandwidth](../bandwidth) is a more complicated concept, it is
    much easier to observe and measure than latency: you can simply execute a long
    series of independent read or write queries, and the scheduler, having access
    to them in advance, reorders and overlaps them, hiding their latency and maximizing
    the total throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure *latency*, we need to design an experiment where the CPU can’t cheat
    by knowing the memory locations we will request in advance. One way to ensure
    this is to generate a random permutation of size $N$ that corresponds to a cycle
    and then repeatedly follow the permutation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Compared to linear iteration, it is *much* slower — by multiple orders of magnitude
    — to visit all elements of an array this way. Not only does it make [SIMD](/hpc/simd)
    impossible, but it also [stalls the pipeline](/hpc/pipelining), creating a large
    traffic jam of instructions, all waiting for a single piece of data to be fetched
    from the memory.
  prefs: []
  type: TYPE_NORMAL
- en: This performance anti-pattern is known as *pointer chasing*, and it is very
    frequent in data structures, especially those written high-level languages that
    use lots of heap-allocated objects and pointers to them necessary for dynamic
    typing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/679a67a9cc2e67d6e8233f2aef54db53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When talking about latency, it makes more sense to use cycles or nanoseconds
    rather than throughput units, so we replace this graph with its reciprocal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37ff0412d79dea6330b801c37cc61f52.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the cliffs on both graphs aren’t as distinctive as they were for the
    bandwidth. This is because we still have some chance of hitting the previous layer
    of cache even if the array can’t fit into it entirely.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/latency/#theoretical-latency)Theoretical
    Latency'
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, if there are $k$ levels in the cache hierarchy with sizes $s_i$
    and latencies $l_i$, then, instead of being equal to the slowest access, their
    expected latency will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '$$ E[L] = \frac{ s_1 \cdot l_1 + (s_2 - s_1) \cdot l_2 % + (s_3 - s_2) \cdot
    l_3 + \ldots + (N - s_k) \cdot l_{RAM} }{N} $$ If we abstract away from all that
    happens before the slowest cache layer, we can reduce the formula to just this:
    $$ E[L] = \frac{N \cdot l_{last} - C}{N} = l_{last} - \frac{C}{N} $$ As $N$ increases,
    the expected latency slowly approaches $l_{last}$, and if you squint hard enough,
    the graph of the throughput (reciprocal latency) should roughly look like if it
    is composed of a few transposed and scaled hyperbolas: $$ \begin{aligned} E[L]^{-1}
    &= \frac{1}{l_{last} - \frac{C}{N}} \\ &= \frac{N}{N \cdot l_{last} - C} \\ &=
    \frac{1}{l_{last}} \cdot \frac{N + \frac{C}{l_{last}} - \frac{C}{l_{last}}}{N
    - \frac{C}{l_{last}}} \\ &= \frac{1}{l_{last}} \cdot \left(\frac{1}{N \cdot \frac{l_{last}}{C}
    - 1} + 1\right) \\ &= \frac{1}{k \cdot (x - x_0)} + y_0 \end{aligned} $$'
  prefs: []
  type: TYPE_NORMAL
- en: To get the actual latency numbers, we can iteratively apply the first formula
    to deduce $l_1$, then $l_2$, and so on. Or just look at the values right before
    the cliff — they should be within 10-15% of the true latency.
  prefs: []
  type: TYPE_NORMAL
- en: There are more direct ways to measure latency, including the use of [non-temporal
    reads](../bandwidth), but this benchmark is more representable of practical access
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/latency/#frequency-scaling)Frequency
    Scaling'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to bandwidth, the latency of all CPU caches proportionally scales with
    its clock frequency, while the RAM does not. We can also observe this difference
    if we change the frequency by turning turbo boost on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52a8679535465aa0773ba3893cbcc533.png)'
  prefs: []
  type: TYPE_IMG
- en: The graph starts making more sense if we plot it as a relative speedup.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3448a82210f51d78cfa7ee4a30cc4db3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You would expect 2x rates for array sizes that fit into CPU cache entirely,
    but then roughly equal for arrays stored in RAM. But this is not quite what is
    happening: there is a small, fixed-latency delay on lower clocked run even for
    RAM accesses. This happens because the CPU first has to check its cache before
    dispatching a read query to the main memory — to save RAM bandwidth for other
    processes that potentially need it.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory latency is also slightly affected by some details of the [virtual memory
    implementation](../paging) and [RAM-specific timings](../mlp), which we will discuss
    later. [← Memory Bandwidth](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/)[Cache
    Lines →](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)
  prefs: []
  type: TYPE_NORMAL
