- en: Memory Latency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存延迟
- en: 原文：[https://en.algorithmica.org/hpc/cpu-cache/latency/](https://en.algorithmica.org/hpc/cpu-cache/latency/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/cpu-cache/latency/](https://en.algorithmica.org/hpc/cpu-cache/latency/)
- en: 'Despite that [bandwidth](../bandwidth) is a more complicated concept, it is
    much easier to observe and measure than latency: you can simply execute a long
    series of independent read or write queries, and the scheduler, having access
    to them in advance, reorders and overlaps them, hiding their latency and maximizing
    the total throughput.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管带宽是一个更复杂的概念，但它比延迟更容易观察和测量：你可以简单地执行一系列独立的读取或写入查询，调度器在事先知道这些查询的情况下，重新排序并重叠它们，隐藏它们的延迟，并最大化总吞吐量。
- en: 'To measure *latency*, we need to design an experiment where the CPU can’t cheat
    by knowing the memory locations we will request in advance. One way to ensure
    this is to generate a random permutation of size $N$ that corresponds to a cycle
    and then repeatedly follow the permutation:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量*延迟*，我们需要设计一个实验，在这个实验中，CPU 不能通过提前知道我们将请求的内存位置来作弊。确保这一点的 一种方法是通过生成一个大小为 $N$
    的随机排列，该排列对应于一个循环，然后反复遵循该排列：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Compared to linear iteration, it is *much* slower — by multiple orders of magnitude
    — to visit all elements of an array this way. Not only does it make [SIMD](/hpc/simd)
    impossible, but it also [stalls the pipeline](/hpc/pipelining), creating a large
    traffic jam of instructions, all waiting for a single piece of data to be fetched
    from the memory.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性迭代相比，以这种方式遍历数组的所有元素要慢得多——慢得多——多个数量级。这不仅使得 [SIMD](/hpc/simd) 成为不可能，而且还 [使流水线停滞](/hpc/pipelining)，创建了一个巨大的指令交通堵塞，所有指令都在等待从内存中获取单个数据。
- en: This performance anti-pattern is known as *pointer chasing*, and it is very
    frequent in data structures, especially those written high-level languages that
    use lots of heap-allocated objects and pointers to them necessary for dynamic
    typing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这种性能反模式被称为*指针追查*，它在数据结构中非常常见，尤其是在使用大量堆分配对象及其指针的高级语言编写的那些数据结构中，这些指针对于动态类型是必要的。
- en: '![](../Images/679a67a9cc2e67d6e8233f2aef54db53.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/679a67a9cc2e67d6e8233f2aef54db53.png)'
- en: 'When talking about latency, it makes more sense to use cycles or nanoseconds
    rather than throughput units, so we replace this graph with its reciprocal:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈论延迟时，使用周期或纳秒而不是吞吐量单位更有意义，因此我们用它的倒数来替换这个图表：
- en: '![](../Images/37ff0412d79dea6330b801c37cc61f52.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37ff0412d79dea6330b801c37cc61f52.png)'
- en: Note that the cliffs on both graphs aren’t as distinctive as they were for the
    bandwidth. This is because we still have some chance of hitting the previous layer
    of cache even if the array can’t fit into it entirely.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这两个图表上的悬崖不像带宽图表上那么明显。这是因为即使数组不能完全放入其中，我们仍然有可能命中缓存的前一层。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/latency/#theoretical-latency)Theoretical
    Latency'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/latency/#theoretical-latency)理论延迟'
- en: 'More formally, if there are $k$ levels in the cache hierarchy with sizes $s_i$
    and latencies $l_i$, then, instead of being equal to the slowest access, their
    expected latency will be:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，如果缓存层次结构中有 $k$ 个级别，大小分别为 $s_i$，延迟分别为 $l_i$，那么，它们的期望延迟将不会等于最慢的访问，而是：
- en: '$$ E[L] = \frac{ s_1 \cdot l_1 + (s_2 - s_1) \cdot l_2 % + (s_3 - s_2) \cdot
    l_3 + \ldots + (N - s_k) \cdot l_{RAM} }{N} $$ If we abstract away from all that
    happens before the slowest cache layer, we can reduce the formula to just this:
    $$ E[L] = \frac{N \cdot l_{last} - C}{N} = l_{last} - \frac{C}{N} $$ As $N$ increases,
    the expected latency slowly approaches $l_{last}$, and if you squint hard enough,
    the graph of the throughput (reciprocal latency) should roughly look like if it
    is composed of a few transposed and scaled hyperbolas: $$ \begin{aligned} E[L]^{-1}
    &= \frac{1}{l_{last} - \frac{C}{N}} \\ &= \frac{N}{N \cdot l_{last} - C} \\ &=
    \frac{1}{l_{last}} \cdot \frac{N + \frac{C}{l_{last}} - \frac{C}{l_{last}}}{N
    - \frac{C}{l_{last}}} \\ &= \frac{1}{l_{last}} \cdot \left(\frac{1}{N \cdot \frac{l_{last}}{C}
    - 1} + 1\right) \\ &= \frac{1}{k \cdot (x - x_0)} + y_0 \end{aligned} $$'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: $$ E[L] = \frac{ s_1 \cdot l_1 + (s_2 - s_1) \cdot l_2 % + (s_3 - s_2) \cdot
    l_3 + \ldots + (N - s_k) \cdot l_{RAM} }{N} $$ 如果我们抽象掉所有发生在最慢缓存层之前的事情，我们可以将公式简化为仅仅这个：$$
    E[L] = \frac{N \cdot l_{last} - C}{N} = l_{last} - \frac{C}{N} $$ 随着 $N$ 的增加，期望延迟逐渐接近
    $l_{last}$，如果你足够用力地眯眼，吞吐量（倒数延迟）的图表应该大致看起来像是由几个转置和缩放的双曲线组成的：$$ \begin{aligned}
    E[L]^{-1} &= \frac{1}{l_{last} - \frac{C}{N}} \\ &= \frac{N}{N \cdot l_{last}
    - C} \\ &= \frac{1}{l_{last}} \cdot \frac{N + \frac{C}{l_{last}} - \frac{C}{l_{last}}}{N
    - \frac{C}{l_{last}}} \\ &= \frac{1}{l_{last}} \cdot \left(\frac{1}{N \cdot \frac{l_{last}}{C}
    - 1} + 1\right) \\ &= \frac{1}{k \cdot (x - x_0)} + y_0 \end{aligned} $$
- en: To get the actual latency numbers, we can iteratively apply the first formula
    to deduce $l_1$, then $l_2$, and so on. Or just look at the values right before
    the cliff — they should be within 10-15% of the true latency.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取实际的延迟数值，我们可以迭代地应用第一个公式来推导$l_1$，然后是$l_2$，依此类推。或者，只需查看悬崖前的值——它们应该在大约10-15%的真延迟范围内。
- en: There are more direct ways to measure latency, including the use of [non-temporal
    reads](../bandwidth), but this benchmark is more representable of practical access
    patterns.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有更直接的方式来测量延迟，包括使用[非临时读取](../bandwidth)，但这个基准更能代表实际的访问模式。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/latency/#frequency-scaling)Frequency
    Scaling'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/latency/#frequency-scaling)频率缩放'
- en: Similar to bandwidth, the latency of all CPU caches proportionally scales with
    its clock frequency, while the RAM does not. We can also observe this difference
    if we change the frequency by turning turbo boost on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与带宽类似，所有CPU缓存的延迟与其时钟频率成比例增加，而RAM则不然。如果我们通过开启超频来改变频率，我们也可以观察到这种差异。
- en: '![](../Images/52a8679535465aa0773ba3893cbcc533.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52a8679535465aa0773ba3893cbcc533.png)'
- en: The graph starts making more sense if we plot it as a relative speedup.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将它作为相对加速率来绘制，图表开始变得更有意义。
- en: '![](../Images/3448a82210f51d78cfa7ee4a30cc4db3.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3448a82210f51d78cfa7ee4a30cc4db3.png)'
- en: 'You would expect 2x rates for array sizes that fit into CPU cache entirely,
    but then roughly equal for arrays stored in RAM. But this is not quite what is
    happening: there is a small, fixed-latency delay on lower clocked run even for
    RAM accesses. This happens because the CPU first has to check its cache before
    dispatching a read query to the main memory — to save RAM bandwidth for other
    processes that potentially need it.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会期望对于完全适合CPU缓存的数组大小有2倍的速度，但对于存储在RAM中的数组则大致相等。但实际情况并非如此：即使是对于RAM访问，也存在一个小的、固定的延迟。这是因为CPU在将读取查询派遣到主内存之前，首先必须检查其缓存——为了节省可能需要的RAM带宽给其他进程。
- en: Memory latency is also slightly affected by some details of the [virtual memory
    implementation](../paging) and [RAM-specific timings](../mlp), which we will discuss
    later. [← Memory Bandwidth](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/)[Cache
    Lines →](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 内存延迟也受到[虚拟内存实现](../paging)和[RAM特定时序](../mlp)的一些细节的影响，这些我们将在后面讨论。[← 内存带宽](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/)[缓存行
    →](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)
