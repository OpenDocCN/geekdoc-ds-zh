- en: Chapter 10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Libraries and SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Writing programs directly in CUDA is not the only option available to people
    wishing to speed up their work by making use of GPUs. There are three broad ways
    of developing applications for CUDA:'
  prefs: []
  type: TYPE_NORMAL
- en: • Using libraries
  prefs: []
  type: TYPE_NORMAL
- en: • Directive-based programming
  prefs: []
  type: TYPE_NORMAL
- en: • Writing CUDA kernels directly
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at each of these in turn and when you should apply them.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Libraries are useful components that can save you weeks or months of development
    effort. It makes perfect sense to use libraries where possible because, generally,
    they are developed by experts in their particular field and thus are both reliable
    and fast. Some of the more common, and free, libraries are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: • Thrust—An implementation of the C++ STL (Standard Template Interface).
  prefs: []
  type: TYPE_NORMAL
- en: • NVPP—NVIDIA performance primitives (similar to Intel’s MKK).
  prefs: []
  type: TYPE_NORMAL
- en: • CuBLAS—GPU version of BLAS (basic linear algebra) library.
  prefs: []
  type: TYPE_NORMAL
- en: • cuFFT—GPU-accelerated fast Fourier transformation library.
  prefs: []
  type: TYPE_NORMAL
- en: • cuSparse—Linear algebra and matrix manipulation for sparse matrix data.
  prefs: []
  type: TYPE_NORMAL
- en: • Magma—LAPACK and BLAS library.
  prefs: []
  type: TYPE_NORMAL
- en: • GPU AI—GPU-based path planning and collision avoidance.
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA Math Lib—C99 standard math support and optimized functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also a number of commercial offerings, including the following, many
    of which offer either a limited functionality free or trial version:'
  prefs: []
  type: TYPE_NORMAL
- en: • Jacket—An alternative GPU-based Matlab engine for M-code.
  prefs: []
  type: TYPE_NORMAL
- en: • ArrayFire—Matrix, signal, and image processing similar to IPP, MKL, and Eigen.
  prefs: []
  type: TYPE_NORMAL
- en: • CULA tools—Linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: • IMSL—Implementation of the Fortran IMSL numerical library.
  prefs: []
  type: TYPE_NORMAL
- en: There are, of course, many others that are not shown here. We maintain a list
    of CUDA libraries at [*www.cudalibs.com*](http://www.cudalibs.com), including
    a number of our own libraries, provided free for personal or academic use, or
    licensed and supported for commercial use.
  prefs: []
  type: TYPE_NORMAL
- en: General library conventions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The NVIDIA-provided libraries, as a general principle, do no memory management
    for the caller. They instead expect the caller to provide pointers to the area
    of allocated memory *on the device*. This allows for a number of functions on
    the device to be run one after another without unnecessary device/host transfer
    operations between calls.
  prefs: []
  type: TYPE_NORMAL
- en: As they perform no memory operations, it is the caller’s responsibility to both
    allocate and free memory after usage. This extends even to providing memory for
    any scratch space or buffer areas used by the library calls.
  prefs: []
  type: TYPE_NORMAL
- en: Although this may seem an overhead to place onto the programmer, it’s actually
    a very good design principle and one you should follow when designing libraries.
    Memory allocation is a costly operation. Resources are limited. Having a library
    continuously allocating and freeing memory in the background is far less efficient
    than you performing these operations once at startup and then again at program
    exit.
  prefs: []
  type: TYPE_NORMAL
- en: NPP (Nvidia Performance Primitives)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The NPP library provides a set of functions for image and general signal processing.
    It supports all CUDA platforms. To include NPP into your project, simply include
    the relevant header files and link to the precompiled library.
  prefs: []
  type: TYPE_NORMAL
- en: For signal processing functions, the library expects one or more source pointers
    (`pSrc1`, `pSrc2`, etc.), one or more destination pointers (`pDst1`, `pDst2`,
    etc.), or one or more mixed pointers for in-place operations (`pSrcDst1`, `pSrcDst2`,
    etc). The library names the functions according to the data type processed. C++
    function name overloading—that is, using a single name for a common function—is
    not supported.
  prefs: []
  type: TYPE_NORMAL
- en: The supported data types for signal processing are `Npp8u`, `Npp8s`, `Npp16u`,
    `Npp16s`, `Npp32u`, `Npp32s`, `Npp64u`, `Npp64s`, `Npp32f`, and `Npp64f`. These
    equate to unsigned and signed versions of 8, 16, 32, and 64 types plus the 32-
    and 64-bit single-precision and double-precision floating-point types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image part of the library follows a similar naming convention, in that
    the function names reflect intended usage and data type. Image data can be organized
    in a number of ways, so there are a few key letters that allow you to see the
    functionality and data type from the name. These are:'
  prefs: []
  type: TYPE_NORMAL
- en: • A—Used where the image contains an alpha channel that should not be processed.
  prefs: []
  type: TYPE_NORMAL
- en: • Cn—Used where the data is laid out in a packed or interleaved format of *n*
    channels, for example {R, G, B}, {R, G, B}, {R, G, B}, etc. would be C3.
  prefs: []
  type: TYPE_NORMAL
- en: • Pn—Used where the color data is split into planes, such as all the data from
    one color is contiguous, so {R, R, R}, {G, G, G}, {B, B,.B}, etc. would be P3.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to how the data is organized, the naming also tells you how the
    function will manipulate the data.
  prefs: []
  type: TYPE_NORMAL
- en: • I—Used where the image data is manipulated in-place. That is, the source image
    data will be overwritten by the operation being performed on it.
  prefs: []
  type: TYPE_NORMAL
- en: • M—Indicates that a nonzero mask will be used to determine which pixels meet
    the criteria. Only those pixels will be processed. Useful, for example, in overlaying
    one image onto another.
  prefs: []
  type: TYPE_NORMAL
- en: • R—Indicates that the function operates on a subsection of the image, via the
    caller specifying an ROI (region of interest).
  prefs: []
  type: TYPE_NORMAL
- en: • Sfs—Indicates the function performs a fixed scaling and saturation on the
    output data as part of the operation.
  prefs: []
  type: TYPE_NORMAL
- en: The use of such short function naming postfixes leads to function names that
    may appear somewhat obscure to the casual reader. However, once you memorize the
    meaning of each attribute of the function name and work with NPP a little bit,
    you’ll quickly recognize what operation a given function is performing.
  prefs: []
  type: TYPE_NORMAL
- en: The image data functions take also an additional parameter of `pSrcStep` or
    `pDstStep`, which are pointers to the size of a given image line/row in bytes,
    including any padding bytes that are added to the line width to ensure alignment.
    Many image processing functions will add padding bytes to the end of a line to
    ensure the following lines starts on a suitable boundary. Thus, an image 460 pixels
    wide may be padded to 512 bytes per line. A line width value that is a multiple
    of 128 is a good choice, as this will allow entire cache lines to be brought in
    from the memory subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a simple example from the signal manipulation library. We’ll take
    two sets of random data and XOR them together. We’ll do this both on the host
    and the device and then compare the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '` const int num_bytes = (1024u ∗ 255u) ∗ sizeof(Npp8u);`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '` CUDA_CALL(cudaMemcpy(host_dst_ptr1, device_dst_ptr2, num_bytes, cudaMemcpyDeviceToHost));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice in the code we have used an `NPP_CALL` macro around the call to the NPP
    library. This is similar to the `CUDA_CALL` macro we’ve used throughout this text.
    It checks that the return value from the caller is always equal to `NPP_SUCESS`
    (zero) and otherwise prints the error code associated with the returned value.
    Negative values are associated with errors and positive values with warnings.
    Unfortunately, there is no function to convert the error code to an error message,
    so you have to look up the error value in the NPP documentation (Section 7.2,
    “NPP Type Definitions and Constants,” as of v4.1 of the library).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each of the NPP calls is invoking a kernel on the device. By default, NPP operates
    in a synchronous mode using the default stream 0\. However, often you will want
    to perform a number of operations one after another. You may then want to do some
    other work on the CPU, so you will come back later to check the progress of the
    GPU task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify that NPP will use a given, already defined stream, use the following
    API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As we saw from some other examples in this text, if you have a number of sequential
    kernel calls, you can achieve much better overall performance by pushing them
    into the nondefault stream. This is largely because this permits asynchronous
    memory transfers and thus overlapping compute and transfer work. However, to achieve
    this, we need to change the program somewhat, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`  CUDA_CALL(cudaMalloc((void ∗∗) &(device_dst_ptr1[i]), num_bytes));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '` }`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The major difference we see with the streamed version is that we now need multiple
    output data blocks on the host as well as multiple copies on the device. Thus,
    all the device arrays are now indexed by `[NUM_STREAMS]`, allowing the streams
    to operate entirely separately and independently of one another.
  prefs: []
  type: TYPE_NORMAL
- en: To use the asynchronous model we need to allocate host memory as pinned, so
    we have to use `cudaHostMalloc` instead of `malloc`, paired with `cudaFreeHost`
    instead of `free`. We also need to wait on the completion of the stream prior
    to processing its data. In this example we wait on all four streams, but in reality
    as one stream completes, it would be provided with more work. See [Chapters 8](CHP008.html)
    and [9](CHP009.html) regarding multi-GPU programming and optimization, respectively,
    to see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at a plot from Parallel Nsight we can actually see this happening
    with our new streamed version of the code ([Figure 10.1](#F0010)). Notice on the
    output two large transfers to the device followed by a small series of kernel
    operations. Notice also that the transfer in stream 3 starts while the kernels
    in stream 2 are still running (Memory and Compute rows). Finally, notice all the
    transfers back to the host come one after another.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000107f10-01-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.1 NPP streamed calls.
  prefs: []
  type: TYPE_NORMAL
- en: In this example the transfers, as can often be the case, dominate the overall
    timeframe. It depends largely on the amount of processing you are doing on the
    GPU and if, in fact, you need to transfer the data all the time. Leaving the data
    on the GPU is a good solution, especially if you later intend to visualize it
    or just simply do not need a host copy.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, because the series of kernels is small in comparison
    to the transfer, the synchronous time was 300 ms whereas the asynchronous time
    was 280 ms. We have a very small kernel/transfer overlap, so we save only this
    time from the overall timeframe. To benefit significantly from parallel independent
    workloads we actually need multiple GPUs where the transfers and kernels can operate
    in parallel across *N* GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the mix of events, memory copies, and kernels, you can achieve
    a significant improvement by using the asynchronous mode. This is because the
    CUDA device can simply get on with the work set, rather than idle while the CPU
    organizes more work for it. By using multiple streams for independent work units
    you can define task level parallelism in addition to the regular data level parallelism.
    This is exploitable on Fermi-class GPUs (compute 2.x), to some extent, in that
    it’s used to fill the GPU via back-to-back and concurrent kernels. As the SM devices
    within the GPUs become larger, as is the case for Kepler, this becomes more and
    more important.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the setup here is for a single DMA transfer engine, as found on consumer
    cards. The Telsa devices have both DMA engines enabled, allowing transfer to and
    from the device to also be overlapped. In the previous timeline this would cause,
    with some program changes, the copy-back-to-host transfers to occur while the
    copy-to-device transfers were occurring. In effect, we’d eliminate the time of
    the copy-back-to-host transfers, a significant savings. Due to both DMA engines
    being enabled on Tesla devices, enabling streams can bring significant benefits
    for this platform.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that for this test we are using a PCI-E 2.0 X8 link. Using a PCI-E
    3.0 X16 link would reduce the transfer time to around a quarter of what is shown
    here, making transfers less of an issue.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your kernel working, and only once you have it working, switch
    to an asynchronous mode of operation. An Asynchronous operation, however, can
    make debugging somewhat more complex so this is best done only once everything
    works correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**SDK Samples:** Grabcut, Histogram Equalization, BoxFilter, Image Segmentation,
    Interoperability with the FreeImage library.'
  prefs: []
  type: TYPE_NORMAL
- en: Thrust
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Those familiar with C++ may have used the C++ STL and specifically the BOOST
    library. For those not familiar with templates, they are actually a very useful
    feature of C++. In traditional C if you have a simple function that you wish to
    perform an operation, sum for example, you have to specify the operand type explicitly.
    Thus, you might have something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If I wish to call the function and pass a float then I need a new function.
    As C does not allow the same name for two functions, I need something like `sum_i32`,
    `sum_u8`, `sum_u32`, `sum_f32`, etc. It’s somewhat tedious to provide and use
    a library that is type specific.
  prefs: []
  type: TYPE_NORMAL
- en: C++ tried to address this in the form of function overloading. This allows the
    same name to be used for many functions. Depending on the type of the parameters
    passed, the appropriate function is called. However, the library provider still
    needs to write one function body to handle `int8`, `int16`, `f32`, etc. even if
    he or she can now use a common name for the function.
  prefs: []
  type: TYPE_NORMAL
- en: The C++ template system addresses this. We have a generic function that changes
    only in terms of the type of its parameters. Thus, why does the programmer have
    to copy and paste the code a dozen times to support all the possible permutations
    he or she might imagine? Templates, as the name suggests, mean supplying a template
    of what you’d like to do in a type agnostic manner. The compiler then generates
    a version of the function at compile time if, and only if, it is actually called.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the STL was born, a type agnostic definition of some very common functions
    in C++. If you happen to use the `int32` version, but not the `f32` version, only
    the `int32` version results in actual code within your application. The downside
    of templates, as opposed to libraries, is that they are compiled during every
    compilation run, so they can increase compile times.
  prefs: []
  type: TYPE_NORMAL
- en: The NVCC compiler is actually a C++ front end, rather than a C front end. The
    standard development package on Windows, Microsoft Visual Studio, also supports
    C++ language, as do the primary Linux and Mac development environments.
  prefs: []
  type: TYPE_NORMAL
- en: The Thrust library supports many of the STL containers for which it makes sense
    to support on a massively parallel processor. The simplest structures are often
    the best and thus arrays (or vectors as the STL calls them) are well supported
    in Thrust. Not all containers make sense, such as lists which have unknown access
    times and are of variable length.
  prefs: []
  type: TYPE_NORMAL
- en: 'One other C++ concept we need to cover before we look at Thrust is C++ namespaces.
    In C if you declare two functions with the same name you get an error at the link
    stage if it is not detected during compilation. In C++ this is perfectly valid
    providing the two functions belong to a different namespace. A namespace is a
    little like specifying a library prefix to differentiate which library the compiler
    should search for the function. The C++ namespace is actually a class selector.
    Classes in C++, at a very high level, are simply a way of grouping related functions
    and data together in one definition. Thus, we can have two calls to the same function,
    providing the namespace used is different. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Because of the namespace prefix the compiler can identify which function is
    intended to be called. The `::` (double colon) is equivalent to the `->` operator
    if you think of the class definition as a structure definition that has a number
    of function pointers as well as data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to use the Thrust library we need one more C++ concept, that of a *functor*.
    A functor can be thought of as a function pointer that can also store state or
    data. It is actually a pointer to an instance of an object that is based on a
    given class definition.
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever used any functions like `qsort` (Quicksort) from the standard
    C libraries, you’ll be familiar with the concept of a user-provided function.
    In the case of `qsort` you provide the comparison function to say whether one
    opaque data object is greater than or less than another opaque data object. You
    may have specific or multiple criteria to rank records by. The provider of the
    `qsort` library cannot hope to cover a significant number of these possibilities,
    so they provide a formal parameter where you must provide your own comparison
    function.
  prefs: []
  type: TYPE_NORMAL
- en: The Thrust library provides you with two vector containers. Vectors in the STL
    are simply resizable arrays that can hold elements of any type. Thrust provides
    both host and device vector classes that, reside in the global memory of the host
    and device, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors can be read or modified using the array subscript notation (the `[`
    and `]` symbols). However, be aware that Thrust is in the background performing
    individual transfers over the PCI-E bus for each such access, if the vector is
    on the device. Therefore, putting such a construct within a loop is a *really*
    bad idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first aspect of using Thrust is simply how to get the data you need into
    and out of thrust vectors. Thus, we first need to include the necessary include
    files. Thrust provides the following broad set of function types:'
  prefs: []
  type: TYPE_NORMAL
- en: • Transformation
  prefs: []
  type: TYPE_NORMAL
- en: • Reduction
  prefs: []
  type: TYPE_NORMAL
- en: • Prefix sum
  prefs: []
  type: TYPE_NORMAL
- en: • Reordering
  prefs: []
  type: TYPE_NORMAL
- en: • Sorting
  prefs: []
  type: TYPE_NORMAL
- en: Thrust is not a library in the traditional sense, as all of its contents are
    contained within the header files you include into your source. Thus, you wish
    to avoid simply including everything and should include only the header files
    you need.
  prefs: []
  type: TYPE_NORMAL
- en: Thrust provides two vector objects, `host_vector` and `device_vector`, which
    are used with C++ terminology to create an instance of these objects. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this code we declare two objects, one that physically resides on the host
    and one that will physically reside on the device.
  prefs: []
  type: TYPE_NORMAL
- en: The `thrust::` part specifies a class namespace that you can largely think of
    as a library specifier in C. The `host_vector` and `device_vector` are functions
    (constructors) provided by the object. The `<int>` and `<float>` specifiers are
    passed to the constructor (the initialization function) for the object. The constructor
    then uses them along with the value passed into the function to allocate 200 elements
    of `sizeof(float)` and 500 elements of `sizeof(int)`, respectively. Internally
    there may be other data structures, but at a high level this is effectively what
    you are doing when using such a C++ constructor.
  prefs: []
  type: TYPE_NORMAL
- en: Objects in C++ also have a destructor, a function that is called when the object
    goes out of scope. This function is responsible for deallocating any resources
    allocated by the constructor or during the runtime of the object. Thus, unlike
    C, it’s not necessary to call `free` or `cudaFree` for Thrust vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Having defined a vector you now need to get data into and out of the vector.
    A Thrust vector is conceptually simply a resizable array. Thus, a vector object
    provides a `size()` function that returns the number of elements in the vector.
    This allows you to use standard loop constructs, for example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example the array `[]` operator is being supplied by the class definition.
    Thus, in every iteration, a function is being called to transform `i` into a physical
    piece of data. Because of this, if the data happens to be on the device, Thrust
    will generate a transfer from the device to the host, in the background, which
    is completely hidden from the programmer. The `size()` function means the number
    of iterations is only known at runtime. As this can change within the loop body,
    it must be called on each loop iteration. This in turn prevents the compiler from
    statically unrolling the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your view of such things, you’ll either love it or hate it. The
    love it camp loves abstractions because they make programming easier and you don’t
    have to care about the hardware. This camp is primarily made up of inexperienced
    programmers and people who simply want to get a calculation done. The hate it
    camp wants to know what is happening and is very keen to maximize performance
    from the hardware it has. They don’t want to have a “simple” array dereference
    initiate a hugely inefficient 4-byte PCI-E transfer. They’d much prefer a far
    more efficient several-megabyte transfer to or from the device/host when they
    schedule it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thrust actually makes both camps happy. Large transfers are simply done by
    initiating a host vector with a device vector or vice versa. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`thrust::host_vector <float> my_host_float_out_vector (my_device_float_results_vector.begin(),
    my_device_float_results_vector.end() );`'
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the first example we are creating a new vector on the host side and initializing
    the host vector with the device vector. Notice we did not specify only one value
    for the constructor as we did previously when creating a host vector, but two.
    In such cases Thrust does a subtraction to work out the number of elements that
    need to be copied and allocates storage on the host accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: In the second example we use the explicit copy method. This method (function)
    takes three parameters, the start and end of the destination region, plus the
    start of the source region. Because Thrust knows what type of vector you are using,
    the copy method works for both host and device vectors. There is no need to specify
    additional parameters such as `cudaMemcpyDeviceToHost` or `cudaMemcpyHostToDevice`,
    or to call different functions depending on the type passed. Thrust is simply
    using C++ templates to overload the namespace to invoke a number of functions
    depending on the parameters passed. As this is done at compile time, you have
    the benefit of strong type checking and no runtime overhead. Templates are one
    of the major benefits of C++ over C.
  prefs: []
  type: TYPE_NORMAL
- en: Using the functions provided by Thrust
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the data is within a Thrust device vector or host vector container, there
    are a number of standard functions Thrust provides. Thrust provides a simple sort
    function that requires only the start and end of the vector. It distributes the
    work over the different blocks and performs any reduction and interblock communications
    for you. This is often code that people new to CUDA get wrong. Having something
    such as a sort function makes using the GPU as easy as using the common C `qsort`
    library routine.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can see this in action with a short program.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '` **thrust::generate(host_array.begin(), host_array.end(), rand);**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Problems with Thrust
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: What is interesting to note here is that the GPU- and CPU-based sorts may or
    may not be performed at the same time, depending on how you arrange the transfers.
    Unfortunately, Thrust always uses the default stream and you cannot change this
    as with NPP library. There is no stream parameter to pass, or function to set
    the currently selected stream.
  prefs: []
  type: TYPE_NORMAL
- en: Using the default stream has some serious implications. The sort operation is
    actually just a series of kernels run in stream 0\. Kernels, like regular kernels,
    launch asynchronously. However, memory transfers, unless explicitly done asynchronously,
    operate synchronously. Thus, any function you call from Thrust that returns a
    value, `reduce` for example, and any copy back to the host causes an implicit
    synchronization. In the example code, placing the sort host array call after the
    copy back from the device code would have serialized the GPU and CPU sorts.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-CPU/GPU considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On the CPU side, Thrust automatically spawns *N* threads where *N* is the number
    of physical processor cores. Thrust is actually using OpenMP for the CPU side,
    and by default OpenMP will use the number of physical cores on the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice if the GPU version did this also, splitting the task over *N*
    GPUs. The host side implements a NUMA (nonuniform memory access)-based memory
    system. This means all memory addresses are accessible to any CPU socket and any
    CPU core. Thus, even on a dual-CPU system, 8, 12, or 16 CPU cores can work in
    parallel on a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple GPUs are more like a cluster of distributed memory machines all attached
    to the PCI-E bus. The GPUs can talk over the bus directly to one another using
    the peer-to-peer (P2P) functionality if you have the correct hardware and operating
    system (OS). To have multiple GPUs work together on a sort is a little more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: However, just like regular multi-GPU programming, Thrust supports the single-thread/multiple-GPU
    and multiple-threads/multiple-GPU model. It does not implicitly make use of multiple
    GPUs. It’s left to the programmer to either spawn multiple threads or use `cudaSetDevice`
    calls where appropriate to select the correct device to work on.
  prefs: []
  type: TYPE_NORMAL
- en: When sorting on multiple processors there are two basic approaches. The first
    is used by merge sort. Here the data is split into equal-size blocks with each
    block independently sorted and a final merge operation applied. The second, used
    by algorithms like the sample sort we looked at earlier, is to partially sort,
    or presort, the blocks. The resultant blocks can then be independently sorted,
    or can also simply be concatenated together to form the sorted output.
  prefs: []
  type: TYPE_NORMAL
- en: As memory access time is much slower than comparison time, algorithms that have
    the fewest passes over the data, both in terms of reading and writing, tend to
    be the fastest. Operations that create contention, such as merging, ultimately
    limit scaling compared with those algorithms that can maintain wide parallelism
    throughout the entire process.
  prefs: []
  type: TYPE_NORMAL
- en: For basic types (`u8`, `u16`, `u32`, `s8`, `s16`, `s32`, `f32`, `f64`) Thrust
    uses a very fast radix sort, something we looked at earlier with sample sort.
    For other types and user-defined types it uses a merge sort. Thrust automatically
    adjusts the number of bits used for the radix sort depending on the type and the
    range of the data. Thus, a 32-bit sort where the maximum range of the data is
    only 256 is significantly faster than one where the entire range is used.
  prefs: []
  type: TYPE_NORMAL
- en: Timing sort
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To see some timings, let’s add some timers to the Thrust example code and see
    what values we get.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '` CUDA_CALL( cudaGetDeviceProperties( &prop, device_num ) );`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`  **thrust::device_vector<int> device_array = host_array;**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: As Thrust uses the default stream for all calls, to time device code we simply
    insert a number of events and then get the delta time between the various events.
    Notice, however, that we need to synchronize the stream after the sort and after
    the final event. The `cudaEventRecord` function, even if the device is not currently
    doing anything, returns immediately, without setting the event. Thus, leaving
    out the synchronize call after the device sort significantly extends the actual
    time reported.
  prefs: []
  type: TYPE_NORMAL
- en: The timings we see across our four devices are shown in [Table 10.1](#T0010)
    and [Figure 10.2](#F0015) for sorting 16 MB of random data. As you can see from
    the table there is a fairly linear decline of speed as we move back the various
    GPU generations. As we hit the compute 1.1 9800 GT device we see a significant
    jump in execution time. The 9800 GT has less than half of the memory bandwidth
    and around two-thirds, at best, of the processing power of the GTX260.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.1 Thrust Sort Timings on Various Devices
  prefs: []
  type: TYPE_NORMAL
- en: '| Device | Time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GTX470 | 67.45 |'
  prefs: []
  type: TYPE_TB
- en: '| GTX460 | 85.18 |'
  prefs: []
  type: TYPE_TB
- en: '| GTX260 | 109.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 9800 GT | 234.83 |'
  prefs: []
  type: TYPE_TB
- en: '![image](../images/F000107f10-02-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.2 Thrust sort time by device (16 MB data).
  prefs: []
  type: TYPE_NORMAL
- en: Host times by comparison are pretty poor on our 2.5 Ghz AMD Phenom II X4\. Sort
    time averages around 2400 ms, some 10× slower than even the 9800 GT. However,
    is this really a fair comparison? It depends on how efficiently Thrust implements
    the sort on the CPU, and on the particular CPU used and the host memory bandwidth.
    Both Parallel Nsight and the task manager indicate Thrust does not load the CPU
    on our test system by more than 25%. This would indicate it’s far from making
    the best use of the CPU resources. Thus, to use it as a comparison is unfair to
    the CPU and artificially inflates the GPU performance figures.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see a single-core `qsort` easily outperforms Thrust sorting on the
    CPU side and uses near 100% of a single core. If we assume a similar speedup for
    a parallel version as we saw on the OpenMP reduce we looked at earlier, a typical
    CPU figure would be half that shown here, let’s say 475 ms. Even so, a GPU-based
    Thrust sort is outperforming the CPU by a factor of almost six times, even accounting
    for transfers to and from the PCI-E bus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thrust also has a number of other useful functions:'
  prefs: []
  type: TYPE_NORMAL
- en: • Binary search
  prefs: []
  type: TYPE_NORMAL
- en: • Reductions
  prefs: []
  type: TYPE_NORMAL
- en: • Merging
  prefs: []
  type: TYPE_NORMAL
- en: • Reordering
  prefs: []
  type: TYPE_NORMAL
- en: • Prefix sum
  prefs: []
  type: TYPE_NORMAL
- en: • Set operations
  prefs: []
  type: TYPE_NORMAL
- en: • Transformations
  prefs: []
  type: TYPE_NORMAL
- en: Documentation on each of these is provided in the Thrust user guide. The usage
    of each is similar to the sort example we’ve used here.
  prefs: []
  type: TYPE_NORMAL
- en: We could obviously write a great deal on Thrust, but this chapter is about libraries
    in general, so we’ll look at just one more example, that of reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`#include <cstdlib>`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`#define NUM_ELEM_END (1024∗1024∗256)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`   reduce_h_t = (get_time() - reduce_h_t);`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '` return 0;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Reduction is an interesting problem in that, as we saw earlier, it’s difficult
    to write a reduction that is faster than an OpenMP version given the transfer
    time of the data to the GPU. We win considerably because there is only one significant
    transfer to the device, but this dominates the overall time. However, if the data
    is already on the GPU, then of course this transfer is not associated with the
    reduction step.
  prefs: []
  type: TYPE_NORMAL
- en: We will time a Thrust-based reduction, both on the host and device, against
    a standard single-core serial reduction, an OpenMP quad-core reduction, and the
    four GPU test devices with a range of data sizes. See [Table 10.2](#T0015) and
    [Figure 10.3](#F0020). The one item missing from the table is the Thrust reduce
    time on the CPU. It was excluded because it was significantly larger than any
    of the other figures. With consistency across all the block sizes, the CPU Thrust
    sort took approximately 10 times the execution time of the single-core serial
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.2 Reduce Timing for Multiple Sizes and GPUs (in ms)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000107tabT0015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000107f10-03-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.3 GPU reduce time (in ms) by size.
  prefs: []
  type: TYPE_NORMAL
- en: Notice in [Figure 10.3](#F0020), and also in [Table 10.2](#T0015), the *y* axis
    is time in milliseconds. Thus, a lower value is better. The GTX260, surprisingly,
    comes out with the best reduction times, marginally outperforming the later-generation
    GTX470.
  prefs: []
  type: TYPE_NORMAL
- en: This is all very well, but how does it compare with the CPU versions? All the
    cards are faster than the single-core serial implementation, by 3.5× to 7×. The
    GTX470 and GTX260 cards fare very well against the OpenMP version, coming in at
    around two-thirds of the time of the parallel CPU version. The GTX460 is about
    the same time as the CPU, with the 9800 GT being slower. However, if we take into
    account the (PCI-E 2.0 x8) transfer time, 278 ms for 512 MB of data, even the
    GTX260 at 106 ms (plus 278 ms) is slower than the OpenMP version at 164 ms.
  prefs: []
  type: TYPE_NORMAL
- en: With a CUDA-implemented reduction, or a Thrust asynchronous version that supported
    streams, we could have subtracted the kernel time as we overlapped successive
    transfers and kernels. This is, of course, assuming we have more than one reduction
    to perform or we broke the single reduction down into a series of reductions.
    Even with this, we’re looking at a best case of 178 ms, which is still slower
    than the OpenMP version. The clear message here is make use of OpenMP and the
    CPU when appropriate. If the data is already on the GPU, then perform the reduction
    on the GPU. Otherwise, use the CPU for some useful purpose. See [Figure 10.4](#F0025).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000107f10-04-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.4 OpenMP and serial reduction timing (in ms).
  prefs: []
  type: TYPE_NORMAL
- en: Using Thrust and CUDA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Thus, we’d like to be able to use these features of the Thrust library with
    regular CUDA code. It may well be that you can write your application entirely
    using the provided Thrust operations. However, libraries never cover everything
    and always end up doing certain things well and others not so well. Therefore,
    we don’t want to be forced into a particular way of thinking just to make use
    of a library.
  prefs: []
  type: TYPE_NORMAL
- en: Thrust does not provide a mechanism to copy data out of its `host_vector` structure
    in an easy manner. It provides only a read single element method. Thus, a copy
    can only be performed one element at a time, which is laboriously slow. However,
    with `device_vectors` we have an alternative method.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to allocate the storage space on the device yourself, thus obtaining
    a pointer to the data and not a Thrust iterator. Then you need to cast the regular
    device pointer to a Thrust device pointer. This is done using the `device_ptr`
    constructor. You may then pass this Thrust device pointer to the various Thrust
    functions. Now Thrust works on the underlying data you have supplied and thus
    it is visible to you, rather than being hidden within the Thrust library.
  prefs: []
  type: TYPE_NORMAL
- en: We can adapt the sort example to make use of this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '` printf("\nSorting %lu data items (%lu MB)", NUM_ELEM, (size_in_bytes/1024/1024));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Notice the constructor `thrust::device_ptr`, which creates the object `thrust_dev_ptr`
    that can then be passed into the `thrust::sort` function. Unlike the conventional
    iteration a Thrust device pointer does not have “begin” and “end” functions, so
    we simply use base plus length to obtain the last element for the sort.
  prefs: []
  type: TYPE_NORMAL
- en: This allows host-initiated Thrust calls to be implemented alongside simple device
    kernels. However, be aware there is (as of 4.2 SDK) no device level interface
    for Thrust, so you cannot call Thrust functions from within a device or global
    function. Functions like `sort`, for example, spawn multiple kernels themselves.
    As the GPU cannot, at least until Kepler K20 is released, spawn additional work
    itself, we’re limited to host-based control.
  prefs: []
  type: TYPE_NORMAL
- en: '**SDK Samples:** Line of Sight, Radix Sort, Particles, Marching Cubes, Smoke
    Particles.'
  prefs: []
  type: TYPE_NORMAL
- en: CuRAND
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CuRAND library provides various types of random number generation on the
    GPU. In C you are probably used to calling the standard library function `rand()`
    on the host. Like many standard library functions `rand()` is not available to
    be called in device code. Thus, your only option is to create a block of random
    numbers on the host and copy this over to the device. This causes a number of
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: • Increased startup time on the host.
  prefs: []
  type: TYPE_NORMAL
- en: • Increased PCI-E bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: • In practice, usually a poorly distributed random number set.
  prefs: []
  type: TYPE_NORMAL
- en: The standard library `rand()` function is not designed for true randomness.
    It works, like many random number generation algorithms, by creating a list of
    pseudo-random numbers and simply selecting the next element from the list. Thus,
    anyone who knows the seed used can use this knowledge to accurately predict the
    next random number in the given sequence.
  prefs: []
  type: TYPE_NORMAL
- en: This has some implications, not least of which is from the security field. Many
    algorithms use randomness, in one way or another, to make it difficult to impersonate
    a peer. Suppose two peers exchange a seed of a random number generator. Peer A
    encodes a random number into the message frame. Peer B using the same seed and
    same random number generator knows what data identifier it should expect from
    peer A. Given a captured sequence of identifiers from peers A and B, it’s possible
    for an attacker, C, to work out the next number and spoof (pretend to be) either
    peer A or B.
  prefs: []
  type: TYPE_NORMAL
- en: This is possible because the random numbers are usually just a small, repeating
    sequence of pseudo-random numbers. If the set of random numbers is small, then
    an attack is easy. The seed is either never set by the programmer, set to some
    “secret” number, or set based on the current time. Startup times are not really
    very random and thus time-based seeds are actually within a very small window.
    Secrets rarely remain secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is password generation. If you have a few hundred users to set
    up on a system, they will usually be issued “random” passwords that are changed
    on first login. These passwords may be long character strings, leading to the
    belief that they are secure. However, if they are actually chosen from a random
    number generator with a small pseudo-random set of numbers, the actual search
    space for a brute-force attack is quite small.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for anything where predictability of the sequence is a problem, we need
    much better random numbers than most standard library implementations of `rand()`
    provide.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the CuRAND library, you need to include the following header file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, you need to ensure you link to the following library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, replace the path with the current version of the CUDA toolkit you
    are using. Let’s therefore look at an example of generating some random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`   printf("\n");`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '` CUDA_CALL(cudaMemcpy(host_ptr, device_ptr, size_in_bytes, cudaMemcpyDeviceToHost));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This program generates `num_elem` number of random numbers on the device and
    the host using the CuRand API. It then prints both sets of random numbers. The
    output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: One important issue to note from the example program is the API calls are the
    same for device and host functions, except for registering the generator. The
    device version must be used with `curandCreateGenerator` and the host version
    with `curandCreateGeneratorHost`. Additionally, note that the `curandGenerateUniform`
    function must be called with the associated host or device-based pointer. Getting
    either of these mixed up will likely result in a CUDA “unknown error” issue or
    the program simply crashing. Unfortunately, as both host-side and device-side
    memory allocations are just regular C pointers, it’s not possible for the library
    to tell if this pointer passed to it, is a host-side pointer, or device-side pointer.
  prefs: []
  type: TYPE_NORMAL
- en: Also be aware that CuRand, as with NPP, supports streams. Thus, a call to `curandSet
    Stream(generator, stream)` will switch the library to an asynchronous operation
    in that stream. By default the library will use stream 0, the default stream.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of generators you can use with the CuRand library, including
    one based on the Mersenne Twister algorithm used for Monte Carlo simulations.
  prefs: []
  type: TYPE_NORMAL
- en: '**SDK Samples:** Monte Carlo, Random Fog, Mersenne Twister, Sobol.'
  prefs: []
  type: TYPE_NORMAL
- en: CuBLAS (CUDA basic linear algebra) library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last library we’ll mention is the CuBLAS. The CuBLAS library aims to replicate
    the functionality of the Fortran BLAS library commonly used in Fortran scientific
    applications. To allow easy porting of existing Fortran BLAS code, the CuBLAS
    library maintains the Fortran column-major layout, the opposite of the standard
    C row-major layout. It also uses the 1..N as opposed to the C standard 0..(N-1)
    notation when accessing array elements.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for porting legacy Fortran code to CUDA, the CuBLAS library is ideal.
    There are many large codebases written over the past decades in Fortran. Allowing
    this existing legacy code to run on modern GPU-accelerated hardware without significant
    code changes is one of the great strengths of this library. However, it’s also
    one of its weaknesses, as it will not appeal to anyone who learned to program
    in a modern computing language.
  prefs: []
  type: TYPE_NORMAL
- en: The CuBLAS library documentation provides some sample macros to convert the
    old-style Fortran array indexes into what most programmers would consider “regular”
    array indexing. However, even if implemented as macros or as an inline function,
    this adds execution time overhead to anyone attempting to work with non-Fortran
    indexing. This makes using the library rather a pain for C programmers. C programmers
    may have preferred to see a separate C style CuBLAS implementation that natively
    supported C array indexing.
  prefs: []
  type: TYPE_NORMAL
- en: As of version four of the library, it deprecated the older API. It now requires
    all callers to first create a handle via a call to the `cublasCreate` function
    call before any other calls are made. The handle is used in subsequent calls and
    allows CuBLAS to be both re-entrant and to support multiple GPUs using multiple
    asynchronous streams for maximum performance. Note although these features are
    provided, it’s the programmers responsibility to handle multiple devices. Like
    so many of the other libraries provided, the CuBLAS library does not automatically
    distribute its load across multi-GPU devices.
  prefs: []
  type: TYPE_NORMAL
- en: The current API can be used by including the `cublas_v2.h` file instead of the
    older `cublas.h` include file. Any current usage of the older API should be replaced
    with the newer API. As with the NPP library, operations are expected to be performed
    on data already present on the GPU and the caller is therefore responsible for
    transferring the data to and from the device. A number of “helper” functions are
    provided for such purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The new CuBLAS interface is entirely asynchronous in nature, meaning even functions
    that return values do it in such a way as the value may not be available unless
    the programmer specifically waits for the asynchronous GPU operation to complete.
    This is part of the move to asynchronous streams that will become important when
    the Kepler K20 is released.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at a simple example here, declaring a matrix on the host side, copying
    it to the device, performing some operations, copying the data back to the host,
    and printing the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '` CUDA_CALL( cudaMallocHost( (void ∗∗) &host_dest_ptr_B, size_in_bytes ));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '` int host_max_idx_A, host_max_idx_B, host_max_idx_dest;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '` CUDA_CALL(cudaFreeHost(host_dest_ptr_B));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The basic steps of the program are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: • Create a CuBLAS handle using the `cublasCreate` function.
  prefs: []
  type: TYPE_NORMAL
- en: • Allocate resources on the device and host.
  prefs: []
  type: TYPE_NORMAL
- en: • Set a matrix on the device directly from a matrix on the host.
  prefs: []
  type: TYPE_NORMAL
- en: • Run Saxpy on the device.
  prefs: []
  type: TYPE_NORMAL
- en: • Run `max` and `sum` functions on the device.
  prefs: []
  type: TYPE_NORMAL
- en: • Copy the resultant matrix back to the host and display it.
  prefs: []
  type: TYPE_NORMAL
- en: • Free any allocated resources.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, real programs will be significantly more complex. We’ve attempted
    to show here the basic template necessary to get some simple CuBLAS functions
    working on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '**SDK Samples:** Matrix Multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Computing SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CUDA SDK is a separate download from regular toolkits and drivers, although
    it is now bundled with the CUDA 5 release candidate for Windows users, so it may
    become a single download in the future. It contains lots of sample code and a
    nice interface to find all the provided CUDA documentation.
  prefs: []
  type: TYPE_NORMAL
- en: There are almost 200 samples, so we’ll select a few sample applications to look
    at in detail. We’ll look at some general-purpose applications since these are
    easier to understand for the wider audience this text is aimed at than some of
    the more domain-specific examples in the toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: The computing samples are incredibly useful for someone starting out with GPU
    programming, as well as more advanced programmers who want examples of how something
    should be done. Unfortunately, a lot of the underlying CUDA API is hidden from
    you. When you are learning a new API the last thing you need is yet another API
    layer on top of the one you wish to learn, so this rather complicates understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the SDK examples use the `cutil` or other packages that are *not* part
    of the standard CUDA release. Thus, when you see the line
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: you might expect it to work in your own code. However, to do this it’s also
    necessary to include the relevant `cutil` source headers from the SDK. NVIDIA
    makes no guarantee about the version-to-version compatibility of these libraries.
    They are not part of the official CUDA release and therefore are not supported.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA APIs always start `cuda…`. Therefore, if you see anything other than
    this, you should realize that you will need to bring in additional code from the
    SDK samples, should you wish to use such calls.
  prefs: []
  type: TYPE_NORMAL
- en: So what does `cutilSafeCall` do? Fairly much the same as the `CUDA_CALL` macro
    we’ve used throughout this text. If there is an error returned from the caller,
    it prints the file and line number and then exits. So why not use the `cutil`
    package directly? Largely because there are many functions in this library and
    you only need a small number of them in reality.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, many useful functions within this package, for example,
    the `gpuGetMaxGflopsDeviceId` function that identifies the fastest GPU device
    in the system. You should browse through the libraries provided with the SDK to
    help you understand some of the samples before jumping into the samples themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Device Query
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Device query is an interesting application in that it’s quite simple and allows
    you to see what your GPU is capable of. It is run from the command line rather
    than the Windows interface and can be found in “C:\ProgramData\NVIDIA Corporation\NVIDIA
    GPU Computing SDK 4.1\C\bin\win64\Release.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously it is the Windows 64-bit version we’re using here from the 4.1 toolkit,
    which may be different on your system. The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '` Device has ECC support enabled:                No`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The program will iterate through all GPUs to find and list the various details
    of each device. For brevity, we have listed only one of the four devices completely
    and extracted the interesting parts from the other devices. For those interested
    in Kepler GK104, the relevant details are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '` CUDA Capability Major/Minor version number:    3.0`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Items reported of note are the current driver and runtime version, which should
    be the same. Compute capability defines what type of device we have for a given
    device number. Also detailed is the number of cores/SMs per device, speed of the
    device, along with memory speed and width. Thus, it’s possible to calculate the
    peak bandwidth on a given device. Talking of bandwidth, this brings us to the
    next useful application in the SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The bandwidth example provided by the SDK provides the following useful statistics
    about your particular device/host setup:'
  prefs: []
  type: TYPE_NORMAL
- en: • Host-to-device bandwidth (paged and pinned memory)
  prefs: []
  type: TYPE_NORMAL
- en: • Device-to-host bandwidth (paged and pinned memory)
  prefs: []
  type: TYPE_NORMAL
- en: • Device-to-device bandwidth
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual output is shown here for a GTX470 on an x8 PCI-E 2.0 link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`  Transfer Size (Bytes) Bandwidth(MB/s)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: One of the things you’ll see when using this example program is just how much
    benefit using pinned memory on your system brings. We looked at this in [Chapter
    9](CHP009.html), but there is nothing like seeing it on your own system to drive
    home the point that pinned memory can be much faster for memory transfers. Even
    a modern Sandybridge-E processor achieves 3 GB/s versus 2.3 GB/s when using pinned
    versus paged memory on a similar PCI-E 2.0 x8 link.
  prefs: []
  type: TYPE_NORMAL
- en: Typical memory on a consumer GPU card is anything from 512 MB (88/9800 series)
    to 2 GB (GTX680). There is really no reason why you should not pin system memory
    to do the transfers to or from the GPU. Even in a 32-bit system with a 4 GB memory
    limit, CUDA will still be using pinned transfers in the background. Therefore,
    you may just as well pin the memory yourself and avoid the implicit pageable to
    pinned memory copy within the driver.
  prefs: []
  type: TYPE_NORMAL
- en: As memory is now very cheap there is no reason why you should not fully load
    the machine. This is especially the case if you have more than one GPU card or
    are using Tesla cards. You can purchase 16 GB of host memory for less than 100
    euros/dollars/pounds.
  prefs: []
  type: TYPE_NORMAL
- en: If we take the memory clock from the GTX470, there is 1674 MHz with a bus width
    of 320 bits. Thus, we take the bus width and divide by 8 to get bytes (40 bytes).
    Next we multiply this by the clock rate (66,960 MB/s). Then we multiply by 2 for
    GDDR5 (133,920 MB/s). Then we divide by 1000 to get listed memory bandwidth (133.9
    GB/s) or 1024 (130.8 GB/s) to get actual bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: So why do we get device-to-device bandwidth of 113,232 MB/s instead of 133,920
    MB/s? Where did the missing 20 MB/s or 15% of the memory bandwidth go? The GPU
    never achieves this theoretical peak bandwidth. This is why it’s useful to run
    the bandwidth test as opposed to calculating the theoretical peak. You then have
    a very good idea of what bandwidth you will get in *your* system, with your PCI-E
    arrangement, your host CPU, your CPU chipset, your host memory, etc. By knowing
    this you know what your application should be able to achieve on a given target
    and can therefore see how much potential you have yet to exploit.
  prefs: []
  type: TYPE_NORMAL
- en: Note with Tesla-based Fermi devices you can gain a significant boost in bandwidth
    by disabling the ECC memory option using the nvidia-smi tool. Error checking and
    correction (ECC) distributes the bit patterns using Hamming codes. This, in effect,
    means you need a larger memory space to store the same data block. This additional
    storage requirement means you trade both space and speed for the extra redundancy
    ECC brings. NVIDA claims to have addressed this issue in Kepler K20 (GK110), where
    the impact of using ECC is claimed to be around one-third of that of Fermi.
  prefs: []
  type: TYPE_NORMAL
- en: SimpleP2P
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SimpleP2P example shows how to use the P2P memory transfer capabilities
    introduced in compute 2.x devices (Fermi). The principle of P2P transfers to avoid
    having to go through host memory (see [Figure 10.5](#F0030)). Host memory may
    be directly accessible to the PCI-E I/O hub (Northbridge), as is often the case
    with Intel’s QPI-based system. It may also be to the other side of the processor,
    as with Intel’s DMI and AMD’s hypertransport-based systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000107f10-05-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.5 P2P transfers.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the number of GPUs in the system, the host memory may represent
    a bottleneck for the transfer in terms of its own speed. The maximum PCI-E transfer
    speed approaches 6 GB/s on a PCI-E 2.0 link. With PCI-E 3.0 (GTX680, GTX690, Tesla
    K10) the actual bandwidth almost doubles to just under 12 GB/s in each direction.
    To maximize bandwidth, you typically define two pinned memory areas and use a
    double buffer scheme to transfer into one block and out of the other. Especially
    with the older processors, you can rapidly consume the entire host memory bandwidth
    simply by performing transfers between GPUs through host memory. This will severely
    hamper any attempt to use the CPU for additional processing capabilities, as it
    will be competing for host memory bandwidth with the GPU transfers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of P2P is to keep the data out of the host memory space and to do
    a transfer directly between the GPUs. While this is an extremely useful feature,
    support for this in the mainstream Windows 7 systems has been noticeably lacking.
    Thus it is not something we’ve looked at yet in this text, so we will cover it
    here as there are a number of uses for this technology. Requirements to use the
    P2P functionality are:'
  prefs: []
  type: TYPE_NORMAL
- en: • A 64-bit OS, and thus UVA (unified virtual addressing) enabled, a requirement
    for P2P to be available.
  prefs: []
  type: TYPE_NORMAL
- en: • Two or more compute 2.x devices that support this feature.
  prefs: []
  type: TYPE_NORMAL
- en: • GPU Devices that are on the same PCI-E I/O hub.
  prefs: []
  type: TYPE_NORMAL
- en: • Appropriate driver level support.
  prefs: []
  type: TYPE_NORMAL
- en: To use this feature under Windows 7 you need the 64-bit OS plus the TCC (Tesla
    compute cluster) drivers active. As the TCC driver will only activate with Tesla
    cards, effectively there is no mainstream consumer support for this in Windows
    7\. Thus, this should be considered as a feature suitable for clusters, high-performance
    computing (HPC), and other compute-centered applications. It’s not something you
    can exploit with, say, a video transcoding application for consumer PCs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To support P2P first you should check UVA is enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: UVA will only be enabled under a 64-bit OS, or Windows 7 using the TCC driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next you need to check if device A can talk to device B. Note, just because
    this test passes, it does not imply that device B can talk to device A. Resources
    are consumed in enabling P2P access in a given direction. This test can be performed
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Once peer access has been enabled, memory can be accessed, in the direction
    of the peer access, either in device kernels or as host-initiated memory copies.
    Thus, device 0 can enable peer access to device 1, as in the previous example.
    You can then call a kernel on device 1, passing it a pointer to the global memory
    space from device 0\. The kernel will then dereference this pointer in the same
    way as it would a zero-copy device pointer to host memory. Every time there is
    an access through that pointer device 1 will initiate a fetch over the PCI-E bus
    from device 0\. Of course, the same caveats apply as with zero-copy memory usage,
    specifically that you should avoid re-reading such memory and try to achieve coalesced
    access patterns for performance reasons.
  prefs: []
  type: TYPE_NORMAL
- en: You can of course use such features for device-initiated memory copies. To do
    this from the device, have a device kernel fetch the data via a device pointer,
    and simply store it to the local device’s global memory. Equally you can push
    as well as pull data from one device to another. However, if you want bidirectional
    access, you will need to remember to enable P2P access in both directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach is an explicit memory copy, something we must initiate
    from the host. There are the two standard forms of this, the synchronous version
    and the asynchronous streamed version:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaMemcpyPeer(dest_device_ptr, dst_device_num, src_device_ptr, src_device_num,num_bytes);`'
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Finally, once we’re done, we need to disable the provisioning of the resources
    for the P2P access by calling
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Performance wise the SimpleP2P application reports 2.4 GB/s, which is quite
    close to the peak 3 GB/s available on this particular (PCI-E 2.0 x8) test system.
  prefs: []
  type: TYPE_NORMAL
- en: The SimpleP2P example program in the SDK provides some simple template code
    as to how to do this in practice. It does a series of GPU transfers between two
    GPUs and then computes the transfer speed. With the background we’ve covered here
    you should be able to read and follow the example code.
  prefs: []
  type: TYPE_NORMAL
- en: asyncAPI and cudaOpenMP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `asyncAPI` SDK sample provides an example of using the asynchronous API,
    but is not actually very simple for someone new to CUDA to understand. We’ve covered
    streams and asynchronous operation already in the text. These are important for
    getting multi-GPU setups to work alongside CPU usage. Therefore, we’ll look at
    this example and see what exactly it does.
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise of the `asyncAPI` example is that it creates an asynchronous
    stream, into which it puts a memory copy to the device, a kernel, and finally
    a memory copy back to the host. During this time it runs some code on the CPU
    that simply counts up while the GPU is running the asynchronous kernel.
  prefs: []
  type: TYPE_NORMAL
- en: The `cudaOpenMP` example shows how to use OpenMP with CUDA. It identifies the
    number of CPU threads, and the number and name of each attached CUDA device. It
    then tries to spawn one thread per GPU device and work-share the different devices.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll provide a similar example here that fuses the two SDK examples, but simplifies
    them somewhat and is potentially more useful as template code for your own work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '` if (idx < num_elem)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`  CUDA_CALL(cudaGetDeviceProperties(&device_prop[device_num], device_num));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '`  int complete = 0;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '`  CUDA_CALL(cudaFreeHost(host_ptr[device_num]));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: There are a few points in the SDK examples that need further discussion. First,
    with the `asyncAPI` example, stream 0, the default stream, is used. Unfortunately,
    there are many instances where the default stream causes implicit synchronization
    between streams. You will almost certainly end up using a double- or triple-buffered
    method and this implicit synchronization will catch you out. When using asynchronous
    operations, always create your own streams.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The second point from the `asyncAPI` stream example that you may not have noticed
    is that it takes the number of elements, *N*, and divides it directly by the number
    of threads to get the number of blocks for the grid. As it happens *N* is a multiple
    of the number of threads, but what if it is not? What happens is, the last elements
    in the array are not processed by the GPU kernel. This may not be at all obvious
    for anyone starting out with CUDA. Always use the following formula for generating
    the number of blocks if you plan on allowing *N* not to be a multiple of the number
    of threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'And in the kernel, add a check for array overrun:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now this creates the overhead of passing `num_elem` to the kernel and checking
    it within the kernel. If you can *guarantee* you will always use a multiple of
    the number of threads, then you can avoid the need for this code and stick with
    the much simpler `num_blocks = num_elem / num_threads` approach. Most of the time
    we can say as programmers this holds true, as we often control the data block
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the `cudaOpenMP` example now, how are multiple CPU threads launched?
    It uses a call to `omp_set_num_threads`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two approaches here: to set one thread per GPU or multiple threads
    per GPU ([Figure 10.6](#F0035)). The later approach is more useful where you have
    many more CPU cores than GPUs. A simpler form of this OpenMP directive that often
    works more reliably is the one we’ve used in the sample program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000107f10-06-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.6 Multiple GPUs with OpenMP.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: With this approach it does not matter how OpenMP may or may not have been configured,
    what environment variables are set or not; it spawns the specified number of threads.
    Note that the current thread is one of the threads used to execute work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: You can see from the program output that, by using different GPUs, the threads
    finish at different times. You can see from [Figure 10.6](#F0035) that there are
    four threads running, including the originating thread. If viewed on screen you
    would see dark green bars along the top showing the threads are mostly running
    (~95%) with occasional stalls that would be shown in light green. Below are the
    four GPU tasks each of which is performing a memset, a kernel launch, and then
    a copy back to host. The bottom row of bars shows the CPU utilization for this
    timeframe. You can see the CPU is busy almost the entire time.
  prefs: []
  type: TYPE_NORMAL
- en: As the four GPUs finish, the CPU threads continue to work until all GPUs in
    the set have completed. We could of course, and you would in practice, allocate
    more GPU work to these GPUs if we really had such different performance characteristics
    with our GPUs. However, most GPU systems will have all of the same GPUs present
    and thus we’d not have to care about reissuing work until they had all completed.
    As they are all the same, given a similar job, they would all complete around
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The next issue we should address with using OpenMP is where to put resource
    allocations and deallocations. Allocation of memory and creation of resources
    on a given device is a time-consuming process. Often there needs to be a common
    understanding of the allocation across threads and thus common data structures.
    To share common data structures across threads requires locking and this in turn
    often causes serialization. We see exactly this when we place the resource allocation/deallocation
    within the OpenMP parallel region. Therefore, allocation/deallocation prior to
    and after the OpenMP parallel region achieves the best CPU utilization within
    that region.
  prefs: []
  type: TYPE_NORMAL
- en: In connection with this is the use of calls into the CUDA API, in particular
    the `cudaEventQuery` call, to check if the device has completed. Such calls should
    in no way be considered as low overhead. If we change the value of `loop_iteration_check`
    constant from one million to just one, we see the CPU count drop from 1,239,000,000
    to just 16,136\. In effect, every thread is then asking, in every loop iteration,
    for the status of the device. Thus, the CPU spends more time in the driver than
    doing anything else. Unfortunately, this is exactly how the `asyncAPI` is coded
    and one of the reasons for highlighting it here. Be sensible about any API call
    you make within a loop. It will take time, so don’t just have the CPU poll the
    device every cycle. Do something useful with the CPU between device queries.
  prefs: []
  type: TYPE_NORMAL
- en: Aligned types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The aligned types example seeks to show the effect of using the _`_align__(n)`
    directive. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Here the `8` part is the number of bytes the start of any element shall be aligned
    to. The example explains, in the associated text, that the `align` directive allows
    the compiler to use larger reads per thread than it would otherwise use. In the
    preceding `LA32` case, the compiler can use a 64-bit read instead of two 32-bit
    reads. As we saw in [Chapter 9](CHP009.html), less memory transactions equate
    to more bandwidth. We used the vector types in the examples there, which also
    used the `align` directive within their definitions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the things we saw in the earlier examples was that to achieve anything
    like peak bandwidth you had to generate a sufficient number of memory transactions
    in flight. Unfortunately, this SDK sample is not written with this in mind. It
    uses 64 blocks of 256 threads, a total of 32 warps. To load a compute 2.x device
    fully we need 48 warps, (64 for Kepler) so the example uses too few blocks. We
    therefore extended this to 1024 blocks and chose a figure of 192 threads, a figure
    that works well across the entire set of compute levels.
  prefs: []
  type: TYPE_NORMAL
- en: We also added the basic type output to the test so we can see baseline figures.
    Additionally each run was compiled specifically generating code for that device
    compute level. Note that this SDK example, even with the changes, only reaches
    about 50% of the peak memory transfer capacity. However, the relative memory bandwidth
    is actually the figure we’re interested in here.
  prefs: []
  type: TYPE_NORMAL
- en: Initially we see the baseline figures shown in [Table 10.3](#T0020) and [Figure
    10.7](#F0040) from the various devices. We can use this baseline performance table
    to assess how well aligned and nonaligned types perform.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.3 Table of Baseline Performance across Devices
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000107tabT0020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000107f10-07-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.7 Graph of baseline performance across the devices (MB/s vs. transfer
    size).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from [Figure 10.7](#F0040), they all hit the maximum coalesced
    memory size at `u32`, or four bytes. This would equate to 32 threads, multiplied
    by 4 bytes, or 128 bytes in total. On Fermi, this is the size of a single cache
    line, so we flatline at this point on compute 2.x devices.
  prefs: []
  type: TYPE_NORMAL
- en: The GTX285 device (compute 1.3) is executing 16-thread coalesced memory reads
    instead of 32 as in compute 2.x devices. Thus, it benefits from back-to-back reads
    and can make use of the 64-bit (8-byte) reads per thread. Additionally, with twice
    the number of SMs than the Fermi generation cards, and a wider memory bus than
    the GTX470, in this particular kernel it’s able to outperform the GTX470.
  prefs: []
  type: TYPE_NORMAL
- en: In the 9800 GT (compute 1.1) we see a similar pattern to the GTX285\. However,
    the major difference here is the physical memory bandwidth is only around half
    of that of the GTX285\. Thus, we see a minor gain between 32- to 64-bit accesses
    per thread, much less than we see with the GTX285\. See [Table 10.4](#T0025).
    We can see from running the example the percentage change in the aligned versus
    the nonaligned access. In [Table 10.5](#T0030), 100% would represent no change.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.4 MB/s Aligned/Nonaligned for Various Devices
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000107tabT0025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 10.5 Percentage Change for Aligned versus Nonaligned Access Patterns
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000107tabT0030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we can see that as we move back through the compute levels, especially
    for the early compute levels, aligned access gains greatly. In the best case we
    see a 31× speed improvement when adding such a directive to the data structure.
    Even moving to the modern GPUs we can see a 2× performance gain. Clearly, adding
    such a directive is very beneficial in all cases except where it causes more memory
    to be moved from main memory to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Note the `RGB32` case. This is actually a 96-bit structure (three `u32`s), effectively
    an `int3` for `float3` type. Adding the `align` directive inserts 4 bytes of padding
    at the end of the structure. Although this allows coalesced accesses, 25% of the
    data being transferred from the memory system is being discarded. In the nonaligned
    case, the overfetch from the previous cache line on Fermi devices saves 33% of
    the subsequent memory fetch.
  prefs: []
  type: TYPE_NORMAL
- en: The conclusion we can draw from this example is that, if you are using structures,
    you need to think about the coalescing impact of this and, at a minimum, use the
    `align` directive. A better solution entirely is to create structures of arrays,
    rather than arrays of structures. For example, have separate red, green, and blue
    (RGB) color planes instead of interleaved RGB values.
  prefs: []
  type: TYPE_NORMAL
- en: Directive-Based Programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book has largely focused on writing CUDA directly. This is good if you
    enjoy writing programs and are maybe from a CS (computer science) background like
    myself. However, very many people who find themselves writing CUDA today are not
    in this category. Many people’s primary concern is their own problem space, not
    CUDA or elegant solutions from a CS perspective.
  prefs: []
  type: TYPE_NORMAL
- en: One of the great successes of OpenMP is that it’s relatively easy to learn and
    pick up. It involves decorating the C source code with directives that tell the
    compiler various things about the parallel nature of the code it’s currently compiling.
    Thus, it requires the programmer to explicitly identify parallelism within the
    code. The compiler takes care of the somewhat harder task of exploiting that parallelism.
    On the whole, it does this reasonably well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the obvious solution to making GPU programming easier is to extend the
    OpenMP model to GPUs. There are, unfortunately, two standards that have/will come
    about for this: the OpenMP4ACC and OpenACC standards. We’ll concentrate here on
    the OpenACC standard, as this is the one NVIDIA is clearly supporting. Generally,
    you find the size of a backer and the take up among programmers will largely dictate
    the success or failure of a given software programming initiative. Most standards,
    regardless of who develops them, largely cover the same space, so in most cases
    learning one makes it much easier to learn another.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in writing GPU code using directives, you will likely
    already have a reasonable understanding of the OpenMP directives for CPUs. The
    major difference we find with standards such as OpenACC is that they, and thus
    the programmer, also have to deal with the location of data. In an OpenMP system
    where there is more than a single physical socket for the CPU we have what is
    called a NUMA (nonuniform memory access) system.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from [Figure 10.8](#F0045), memory in a system with more than
    one CPU is attached directly to a given CPU. Thus, a process that resides on CPU[0]
    takes considerably longer to access memory that resides on CPU[1] than if that
    memory was local to CPU[0]. Let’s assume we have eight processes running over
    two CPU sockets, each CPU with four cores. To perform an exchange of data that
    requires many-to-many communications means we’re limited to the throughput of
    the slowest communication link. This will be the QPI/hypertransport link between
    processors over which the memory traffic to the other processor’s memory bus must
    go. The OpenMP model simply ignores this effect and lacks many of the data concepts
    accelerator-based solutions require.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000107f10-08-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.8 Multi-GPU data pathways.
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenACC is a move toward directive programming and very much follows in the
    footsteps of OpenMP, which has been very successful within standalone, single
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenACC is aimed at:'
  prefs: []
  type: TYPE_NORMAL
- en: • Independent loop-based parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: • Programmers who have not yet been exposed to CUDA or found it too complex.
  prefs: []
  type: TYPE_NORMAL
- en: • Programmers who have no wish to learn CUDA and are happy to abstract the details
    of the particular target architecture to the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: • Programmers who would like rapidly to prototype an existing serial application
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC, as with OpenMP, tries to abstract the hardware and let the programmer
    write standard serial code that the compiler then transforms into code that runs
    on the accelerator. As with OpenMP it involves adding a series of pragma statements
    around loops to instruct the compiler to run particular loops in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: • Looks similar to OpenMP so it is easy to learn for anyone who has used OpenMP.
  prefs: []
  type: TYPE_NORMAL
- en: • Existing serial source code remains unchanged and is simply decorated with
    directives.
  prefs: []
  type: TYPE_NORMAL
- en: • Single set of source for both CPU and GPU accelerated versions.
  prefs: []
  type: TYPE_NORMAL
- en: • Accelerator vendor agnostic. The potential, as with OpenCL, to target multiple
    hardware platforms including CPU-based AVX acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: • Takes care of many of the “details”, such as moving data to and from shared
    memory for data the user specifies shall be cached.
  prefs: []
  type: TYPE_NORMAL
- en: • Vendor-cited studies show easy learning curve for non-CUDA programmers.
  prefs: []
  type: TYPE_NORMAL
- en: • Supports Fortran in addition to C. Allows many existing Fortran programs to
    benefit from acceleration without a massive rewrite.
  prefs: []
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: • Not currently supported under Visual Studio, so is effectively a Linux-only
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: • Commercial product currently supported by PGI, CAPS, and Cray, so it is not
    part of the free CUDA SDK product suite.
  prefs: []
  type: TYPE_NORMAL
- en: • To achieve a comparable or better level of performance to OpenMP with nontrivial
    programs, the user must additionally specify various simple data clauses to minimize
    PCI-E-based transfers.
  prefs: []
  type: TYPE_NORMAL
- en: • Is targeted at single-CPU/single-GPU solutions. Does not autoscale when additional
    GPUs are added. Multiple GPU usage requires the use of multiple CPU threads/processes.
    This may change in the future.
  prefs: []
  type: TYPE_NORMAL
- en: • New features of the CUDA toolkit or the hardware may require explicit support
    from the compiler vendor. Currently OpenACC compiler support can take several
    months to switch over to a CUDA SDK release or to support a new hardware release.
  prefs: []
  type: TYPE_NORMAL
- en: The main issue with regard to OpenACC versus OpenMP is that OpenMP has no concept
    of various levels of memory or various locations of memory because these concepts
    do not exist in the traditional CPU programming models. In OpenMP data is either
    thread private or global (shared).
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast the GPU system is much more complex. You have:'
  prefs: []
  type: TYPE_NORMAL
- en: • Host memory
  prefs: []
  type: TYPE_NORMAL
- en: • GPU global memory
  prefs: []
  type: TYPE_NORMAL
- en: • GPU constant memory
  prefs: []
  type: TYPE_NORMAL
- en: • GPU block private memory (shared memory in CUDA)
  prefs: []
  type: TYPE_NORMAL
- en: • GPU thread private memory (local memory in CUDA)
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC model, for simplicity, works on the basis that the data resides
    on the host and is shipped to the accelerator memory space at the start of the
    parallel region and shipped back at the end of the parallel region. Thus, every
    parallel region is, by default, bounded by these implicit memory copies over the
    PCI-E bus.
  prefs: []
  type: TYPE_NORMAL
- en: Although a simplistic way to think of this, conceptually it’s an easy way to
    ensure correctness at the potential expense of performance. If you had only one
    calculation and would not reuse the data, then this is effectively what you’d
    do in CUDA anyway. If, however, you plan to make a number of transformations on
    the data, then you need to explicitly specify what data is to remain on the device
    by adding data qualifiers to the directives.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s look at a simple program to give some idea of how it might be converted
    to OpenMP/OpenACC. If we take the classic reduction, you typically see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see all we do is replace the OpenMP directive with an OpenACC directive.
    We then compile with the vendor-supplied OpenACC compiler. This may generate anything
    from high-level CUDA code to raw PTX code. It will then usually invoke the NVCC
    compiler to generate the target GPU code. Some vendors support additional targets
    other than simply NVIDIA GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: During the compilation stage most vendors’ compilers provide statistics about
    how they are transforming the serial code to device code. However, this is a little
    like the `-v` option in NCC, in that you need to be able to understand what the
    compiler is telling you. We look here at an example of the PGI compiler output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: To understand this output, you need to understand how the OpenACC terminology
    maps onto CUDA terminology ([Table 10.6](#T0035)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.6 OpenACC and CUDA Terminology
  prefs: []
  type: TYPE_NORMAL
- en: '| OpenACC | CUDA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gangs | Blocks |'
  prefs: []
  type: TYPE_TB
- en: '| Workers | Warps |'
  prefs: []
  type: TYPE_TB
- en: '| Vectors | Threads |'
  prefs: []
  type: TYPE_TB
- en: The first line states that the kernel occupied 60 gangs (blocks in CUDA terminology).
    It then states it generated output for “CC 1.3 and CC 2.0,” compute capacity 1.3
    and 2.0 devices, respectively. It also tells you the number of registers used,
    the number of shared memory per block used, the number of bytes of constant memory
    per block, and any registers spilled to local memory.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it calculates the ideal number of threads (OpenACC calls these vectors)
    to achieve near 100% occupancy as possible based on the number of registers and
    shared memory the kernel is using. It may, however, not always select the best
    values for a given kernel/data pattern. Specifying this allows us to override
    or partially override such choices.
  prefs: []
  type: TYPE_NORMAL
- en: It will look at your data and decide on the best launch parameters (number of
    threads, number of blocks, number of grids, etc.). It will also automatically
    try to allocate data to constant and/or global memory. You are free to override
    these selections if you wish.
  prefs: []
  type: TYPE_NORMAL
- en: 'To override the default behavior of mirroring global data on the host (automatic
    background update commands), you need to specify how the data must be managed.
    This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'where `<directives>` can be one of the following plus some additional more
    complex ones not shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**copy** (data1, data2, …)—Maintain an identical CPU version by copying in
    at the start of the kernel and out at the end (the default behavior).'
  prefs: []
  type: TYPE_NORMAL
- en: '**copyin** (data1, data2, …)—Only copy data to the GPU and do not copy it back,
    that is, discard the GPU data. This is useful for read-only data the GPU will
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: '**copyout** (data1, data2, …)—Only copy data from the GPU back to the CPU.
    Useful for declaring output data on the GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '**create** (data1, data2, …)—Allocates temporary storage on the GPU with no
    copy operation in either direction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**present** (data1, data2, …)—Data is already present on the GPU so does not
    need to be copied or allocated anew.'
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the OpenACC model expects you to use the C99 standard and in particular
    the `__restrict__` keyword in C to specify that any pointers used do not alias
    with one another. Failure to do this will likely result in your code failing to
    vectorize.
  prefs: []
  type: TYPE_NORMAL
- en: You can tell if adding data directives helps (it almost always will) by using
    the `PGI_ACC_TIME=1` (vendor-specific) option. This, in the case of the PGI compiler,
    will enable profiling. It will then tell you how often the kernel was called,
    the block dimensions of the kernel and how long it took, and finally how much
    time was spent transferring data. It’s this later part that is often the most
    critical and where the data clauses help out. You can also use the standard profiling
    tools available in Linux, such as the Visual Profiler, to see into what the OpenACC
    compiler is doing in reality. In doing so you may spot issues you would otherwise
    be unaware of.
  prefs: []
  type: TYPE_NORMAL
- en: 'In being able to see the block size chosen you can also then perform certain
    optimizations to it. For example, you can specify less blocks and threads than
    you have data elements. By default, OpenACC compilers tend to select one thread
    per element, although there is nothing in the standard to say they have to. Thus,
    if you’d like to process four elements per thread, something we have seen tends
    to work well, you can do it by specifying a smaller number of blocks and threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Here we’ve specified the loop should use 64 blocks (gangs) of 128 threads (vectors)
    each. Thus, we have 8192 active threads on the device. Assuming a 16 SM device
    such as the GTX580, this would be four blocks per SM, each of 128 threads. This
    equates to 16 warps per SM, which is too few for ideal occupancy on the GTX580\.
    To solve the issue, we’d need to increase the block (gang) or thread (vector)
    count.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the particular algorithm, you may wish to process more than one
    element per thread, rather than increase the block or thread count. As long as
    the number of elements is known to the compiler, as in the previous example, it
    will process multiple elements per thread, in this case four.
  prefs: []
  type: TYPE_NORMAL
- en: Remember also, as with regular CUDA, threads in reality run as warps, groups
    of 32 threads. Allocating 33 threads allocates 64 threads in the hardware, 31
    of which do nothing but consume space resources on the device. Always allocate
    thread blocks (vectors in OpenACC) in blocks of 32.
  prefs: []
  type: TYPE_NORMAL
- en: Also as with CUDA, if you specify gangs or vectors (blocks or threads), which
    you don’t have to, then the usual kernel launch rules apply. Thus, there is a
    limit on the number of threads a block can support, which will change depending
    on the compute level of the hardware you are targeting. Generally, you’ll find
    64, 128, 192, and 256 vales work well with compute 1.x devices. Values of 128,
    192, 256, 384, and 512 work well with compute 2.x devices. The 256 value is usually
    the best for the compute 3.x platform.
  prefs: []
  type: TYPE_NORMAL
- en: However, when considering adding any specifiers here, consider the likely impact
    of future hardware and how this might limit the use of other accelerator targets.
    By specifying nothing you are letting the compiler select what it thinks is the
    best value. When a new GPU comes out with more threads per block and more blocks
    per SM, once the vendors update the compiler to accommodate it, all works. If
    you do specify these parameters, you should be specifying some multiple of the
    current maximum to allow for your code to run on future devices without running
    out of blocks.
  prefs: []
  type: TYPE_NORMAL
- en: By default the OpenACC model uses synchronous kernel calls. That is, the host
    processor will wait for the GPU to complete and then continue execution once the
    GPU kernel call returns. This is akin to making a function call in C as opposed
    to spawning a worker thread and later converging.
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware by now that this approach, although nice to develop the
    initial application on, should be replaced with an asynchronous model as soon
    as the application is running well. You probably have a reasonable multicore CPU
    in the machine and could make good use of it while the GPU is off calculating
    something. On the top of the list of things to allocate to the CPU should be those
    operations requiring few compute actions compared to loads and stores to and from
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons why we see the reduction operation perform better, or at
    least the at same speed as the GPU, is the amount of work done per memory read/write.
    To calculate data on the GPU we need to either generate it there or send it over
    the PCI-E bus. If you are shipping two data items over the bus just to perform
    a simple operation such as addition, forget it and do it on the CPU instead. The
    cost of the PCI-E transfer greatly outweighs any other consideration in such a
    scenario. The best candidates for the GPU are those computationally intensive
    areas, or where the additional memory bandwidth on the GPU can make a difference.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, OpenACC provides the `async` clause for kernels and data to allow them
    to run asynchronously to the host and perform asynchronous transfers with the
    host.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Asynchronous transfers require the use of pinned memory, that is, memory that
    cannot be swapped to disk. You do not need to explicitly care about this in OpenACC
    as you do with CUDA. Specifying the `async` clause will cause the OpenACC compiler
    to use pinned memory under the hood for transfers. Of course, one thing to remember
    when using an asynchronous operation is that you cannot change the data that is
    being transferred or operated on by the kernel until the asynchronous operation
    has completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once people have mastered asynchronous communication and achieved the best
    performance they are able to on a single-core/GPU pair, the obvious question is:
    Can I speed up my application by using multiple GPUs? The answer is of course
    yes, and very often you’ll see near linear scaling if you can stay within a single
    node.'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC standard supports only a “one CPU thread per GPU” view of multiple
    GPUs on a single node. If you plan on performing some work on the CPU, this makes
    perfect sense, as it allows you to exploit the full potential of a multicore CPU.
    Thus, with OpenMP you simply launch a number of threads using the OpenMP directive
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Assuming you have a quad-core CPU and four GPU cards attached, then you would
    specify to OpenACC that you wish the current thread to use a given GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have only two GPUs in the system then you might be better off specifying
    two threads for OpenMP. If you wished to make use of four threads, but only have
    two for GPU usage, you could do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: We can do the same in MPI by using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: One issue to be careful of here is that the `acc_set_device_num` API call is
    a one-time event only per host thread. This is very much the way the `cudaSetDevice`
    call used to work prior to the CUDA 4.x SDK. You cannot select a context from
    a single host thread and thus control multiple GPUs from that single thread. The
    only model supported is one where there is a single host thread per GPU context.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a dedicated 1:1 ratio of CPU cores to GPUs is the ideal for heavily
    used systems. However, oversubscribing GPUs to CPU cores can be useful, as rarely
    will GPU programs actually saturate the GPU. Thus, there may be points where the
    GPUs are underutilized, typically at synchronization points or between kernel
    invocations. In cases where you have a master/worker arrangement, which is typical
    in MPI, it can be beneficial to dedicate a non-GPU, CPU core to be the master.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect I should touch on here is memory patterns. OpenACC, when implemented
    on an accelerator that does coalescing of global memory, will be just as badly
    affected by a poor memory layout as a CUDA program will. There is no automatic
    transpose. You need to think about your memory layout and create one that is optimal
    for a GPU (data in columns of 32 elements, rather than sequential rows).
  prefs: []
  type: TYPE_NORMAL
- en: Overall OpenACC represents a very interesting development in GPU programming
    and potentially opens up the GPU programming arena to many non-GPU programmers.
    Many of these people will progress to use learn CUDA, as it’s perfectly possible
    to mix OpenACC and CUDA. Thus, you can start with OpenACC, and if you find specific
    areas where you need that extra control, switch over to CUDA, while leaving most
    of the application untouched.
  prefs: []
  type: TYPE_NORMAL
- en: Writing Your Own Kernels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve presented a number of other options in this chapter that range from specifying
    the parallelism at a high level and having the compiler do the heavy lifting,
    to using libraries developed by those far better at exploiting the hardware than
    you are. You will never, and in fact probably should not try to, be the best in
    everything. Tools such as compiler directives and libraries allow you to leverage
    the effort of others to achieve your goals. Your knowledge resides primarily within
    your own field of interest.
  prefs: []
  type: TYPE_NORMAL
- en: As a professional developer, or even as a student, you should be conscious of
    the time you take to develop a solution. It may be technically challenging to
    develop the most efficient parallel quick sort, but probably some bright computer
    science graduate has already written a paper on it. If you are hiring, then the
    obvious thing to do is bring this person on board. Buying in knowledge, in terms
    of people or software, is something that can give you a huge head start on whomever
    your competition may be.
  prefs: []
  type: TYPE_NORMAL
- en: It also makes a lot of sense to select libraries where they cover something
    that is not your area of expertise. If you are developing an image blur algorithm,
    for example, loading/saving the images from the disk is not really what you are
    interested in. There are a number of open-source, or commercial, libraries that
    may cover this aspect of your development.
  prefs: []
  type: TYPE_NORMAL
- en: One common problem you may encounter using libraries is memory allocation. Most
    CPU-based solutions, if they allocate memory, do not allocate pinned memory. Thus,
    an image library that returns a pointer to the loaded image will cause a slowdown
    in your application when you transfer that image data to the GPU. Therefore, look
    for libraries that allow the user to control the memory management, or are GPU
    aware and support pinned memory.
  prefs: []
  type: TYPE_NORMAL
- en: The next issue we hit with the directive and library approach is they are, generally,
    not multi-GPU aware unless written as such. As you can usually get up to four
    GPU cards into a workstation, this approach is a bit like using only one of the
    cores in a standard quad-core CPU. The programming required to support multi-GPU
    configurations is not trivial, but neither is it rocket science. The libraries
    we use internally at CudaDeveloper support multiple GPU setups. It complicates
    the handling of the data and requires a lot more thought, but is certainly doable.
  prefs: []
  type: TYPE_NORMAL
- en: The issue of how much you need to write yourself often is a question of performance.
    In using directives you trade a certain percentage of performance for quicker
    program development. Libraries, by comparison, may bring a significant speedup
    along with a reduction in development effort, but at the potential cost of flexibility
    and license issues. Many are restricted in terms of commercial usage, which simply
    reflects that if you intend to avoid your own development costs by using libraries,
    you should be prepared to pay for that privilege. For academic usage, simply acknowledging
    the contribution is usually sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, there are a number of reasons why you might chose to develop your own
    kernels in CUDA. This text provides good insight to the issues of developing kernels
    using CUDA. The basic principles (coalesced memory access, fully utilizing the
    hardware, avoiding contention of resources, understanding the hardware limitations,
    data locality) apply regardless of whether you write the kernels yourself or abstract
    them to someone else’s problem.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered in this section some of the NVIDIA-provided libraries. If you
    are working in a field that these cover, why would you not chose to use such libraries?
    They are developed by the manufacturer to run well on their hardware. They are
    designed to be used as the basic building blocks of more complex algorithms. NVIDIA’s
    licensing terms are very generous in that they want people to use the libraries
    and to build CUDA applications. This is hardly surprising when you consider wider
    acceptance of CUDA means more GPUs get sold, and of course the more valuable your
    knowledge of CUDA becomes.
  prefs: []
  type: TYPE_NORMAL
- en: The question is really does this bring you sufficient level of performance?
    Most people program in a high-level language because it’s much more productive
    than something like Assembler. The better programmers out there understand both
    C and Assembler in great detail. They know when they should use C for the productivity
    gains and know when a small number of functions need to be hand-coded in Assembler.
    The question of using libraries/directives is largely a similar one. You could
    write everything yourself, but unless you have to, why make your life so hard?
  prefs: []
  type: TYPE_NORMAL
- en: When developing applications for GPUs, a good approach is to first get a prototype
    working on the CPU side. Consider how you’d like to make that CPU version multicore
    aware and if it would benefit the application. What will be the CPU/GPU work balance?
    How will you create threads on the CPU side if you need them? However, at least
    initially, stick with a single CPU thread and a single GPU, but think at the start
    about what you want to achieve in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Now think about the host/device transfers. The transfer-compute-transfer model
    will usually (depending on the ratios) underutilize the GPU. To some extent we
    can overlap transfer/compute depending on the hardware you have to support.
  prefs: []
  type: TYPE_NORMAL
- en: Next think about the memory hierarchy within the GPU. What locality (registers,
    shared, cache, constant, texture) are you going to exploit on the GPU? What data
    layout do you need for these various types of data stores?
  prefs: []
  type: TYPE_NORMAL
- en: Now think about the kernel design. The decomposition into threads and blocks
    has implications in terms of the amount of interthread/interblock communication
    and resource usage. What serialization or contention issues are you likely to
    have?
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a working CPU/GPU application, profile it and get it working as
    efficiently as you can. At this stage keep a very close eye on correctness, preferably
    through some back-to-back automated tests.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us then to the issue of efficiency of the kernel implementation
    and where you need to consider the CUDA/libraries/directives choice. Given the
    plan of how you’d like to use the GPU, how does your choice here affect your ability
    to do that? Is your choice to use our CUDA/libraries/directives positively or
    negatively impacting performance, and by what percentage?
  prefs: []
  type: TYPE_NORMAL
- en: Consider shared memory as an example. OpenACC has a `cache` qualifier that instructs
    the compiler to place and hold this data in shared memory, a resource it may otherwise
    ignore or use depending on the compiler vendor. Libraries rarely expose shared
    memory, but often use it very efficiently internally and will usually document
    this fact. Newer hardware may have different implementations. For example, Kepler
    can configure shared memory as 32- or 64-bit wide, meaning many financial and
    other applications could benefit significantly from this optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Can you make use of such significant optimizations? If you are reliant on the
    directive vendor or library developer to do this, what level of support will they
    provide and how long might this take? If the library was written by a student
    as part of his or her thesis work, unless you or someone else is willing to maintain
    it or you pay someone to do so, it won’t get updated. If you require a feature
    the directive vendor doesn’t think there is a widespread need for, it’s unlikely
    they will develop it just for your application.
  prefs: []
  type: TYPE_NORMAL
- en: When you have an efficient single-CPU/single-GPU implementation, move it to
    a multicore/multi-GPU solution as appropriate for your workload. For GPU-dominated
    workflow where the CPU is underutilized, the simple single-CPU core controls and
    all-GPU asynchronous model works fine. Where the CPU core is also loaded, how
    might using multiple threads and thus one GPU per thread help? With the underutilized
    CPU load case, is there not something useful the multicore CPU can be doing? Optimal
    design is about using the resources you have most effectively to solve a given
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Moving to a multithread/multi-GPU approach may be a painless or very painful
    experience. Your GPU global memory data is now split over multiple GPUs’ memory
    spaces. What inter-GPU communication is now needed? The P2P model, if supported,
    is usually the best method for such communication. Alternatively, the coordination
    or transfers need to be done by the host. Having a single CPU coordinate *N* GPUs
    may be simpler that having multiple CPU threads coordinate those same GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: How well do your directives or libraries support a multi-GPU approach? Are they
    thread safe, or do they maintain an internal state assuming there will only be
    one instance or CPU thread? What support is there for exchanging data and concurrent
    operations? Are you forced to serially send or receive data to or from each GPU
    in turn, or can you perform *N* simultaneous transfers?
  prefs: []
  type: TYPE_NORMAL
- en: When selecting tools or libraries, consider how mature they are and for what
    purpose they were written. How do you debug the code when it goes wrong, as it
    inevitably will? Are you left on your own to figure out the issue or is there
    support provided for bug fixes, feature requests, etc.? When were they written
    and for which GPU do they work best? Are they optimized for, or aware of, different
    GPU generations?
  prefs: []
  type: TYPE_NORMAL
- en: By thinking about your design in advance and realizing where you’d like to end
    up, you can decide what sort of software/tools you will need at the outset. You
    may be able to prototype a solution with one approach, but may ultimately have
    to use CUDA to get the performance and efficiency you’d like. There is no mystical
    “silver bullet” in software development. You have to think about the design, plan
    how you will achieve it, and understand how far certain approaches can take you.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have looked at a number of approaches to development of code for the GPU
    in this chapter. What appeals to you will largely depend on your background and
    how comfortable and experienced you currently are with CUDA. I specifically encourage
    you to look at the NVIDA-provided libraries as they provide very large coverage
    of many common problems.
  prefs: []
  type: TYPE_NORMAL
- en: We have looked at a number of the nondomain-specific examples in the SDK, specifically
    because everyone can follow and benefit from looking at these. There are many
    domain-specific examples in the SDK. I encourage you to explore these as, with
    now a good understanding of CUDA, you will be able to get a lot more out of looking
    at these examples.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have seen from this chapter that writing everything yourself in CUDA
    is not the *only* option. Significant productivity gains can be made by the use
    of libraries. Directives also allow a much higher level of programming that many
    people may prefer to the more low-level CUDA approach. People make different choices
    for various reasons. Understand what the key criteria are for you, and select
    accordingly.
  prefs: []
  type: TYPE_NORMAL
