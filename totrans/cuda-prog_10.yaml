- en: Chapter 10
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章
- en: Libraries and SDK
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库和 SDK
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Writing programs directly in CUDA is not the only option available to people
    wishing to speed up their work by making use of GPUs. There are three broad ways
    of developing applications for CUDA:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 直接用 CUDA 编写程序并不是唯一能够通过利用 GPU 加速工作的方法。开发 CUDA 应用程序有三种广泛的方式：
- en: • Using libraries
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用库
- en: • Directive-based programming
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: • 基于指令的编程
- en: • Writing CUDA kernels directly
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: • 直接编写 CUDA 内核
- en: We’ll look at each of these in turn and when you should apply them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次查看这些库，并讨论何时应用它们。
- en: Libraries
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库
- en: 'Libraries are useful components that can save you weeks or months of development
    effort. It makes perfect sense to use libraries where possible because, generally,
    they are developed by experts in their particular field and thus are both reliable
    and fast. Some of the more common, and free, libraries are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 库是有用的组件，能够节省你数周或数月的开发时间。在可能的情况下使用库是非常合理的，因为通常它们是由该领域的专家开发的，因此既可靠又快速。一些更常见且免费的库如下所示：
- en: • Thrust—An implementation of the C++ STL (Standard Template Interface).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: • Thrust—C++ STL（标准模板库）的实现。
- en: • NVPP—NVIDIA performance primitives (similar to Intel’s MKK).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: • NVPP—NVIDIA 性能原语（类似于英特尔的 MKK）。
- en: • CuBLAS—GPU version of BLAS (basic linear algebra) library.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: • CuBLAS—BLAS（基本线性代数）库的 GPU 版本。
- en: • cuFFT—GPU-accelerated fast Fourier transformation library.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: • cuFFT—GPU 加速的快速傅里叶变换库。
- en: • cuSparse—Linear algebra and matrix manipulation for sparse matrix data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: • cuSparse—稀疏矩阵数据的线性代数和矩阵操作。
- en: • Magma—LAPACK and BLAS library.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: • Magma—LAPACK 和 BLAS 库。
- en: • GPU AI—GPU-based path planning and collision avoidance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: • GPU AI—基于 GPU 的路径规划和避障。
- en: • CUDA Math Lib—C99 standard math support and optimized functions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: • CUDA 数学库—C99 标准数学支持和优化函数。
- en: 'There are also a number of commercial offerings, including the following, many
    of which offer either a limited functionality free or trial version:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些商业产品，包括以下内容，其中许多提供有限功能的免费或试用版本：
- en: • Jacket—An alternative GPU-based Matlab engine for M-code.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: • Jacket—基于 GPU 的 Matlab M-code 引擎替代方案。
- en: • ArrayFire—Matrix, signal, and image processing similar to IPP, MKL, and Eigen.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: • ArrayFire—类似于 IPP、MKL 和 Eigen 的矩阵、信号和图像处理。
- en: • CULA tools—Linear algebra.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: • CULA 工具—线性代数。
- en: • IMSL—Implementation of the Fortran IMSL numerical library.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: • IMSL—Fortran IMSL 数值库的实现。
- en: There are, of course, many others that are not shown here. We maintain a list
    of CUDA libraries at [*www.cudalibs.com*](http://www.cudalibs.com), including
    a number of our own libraries, provided free for personal or academic use, or
    licensed and supported for commercial use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有许多其他库没有在此列出。我们在 [*www.cudalibs.com*](http://www.cudalibs.com) 上维护着一份 CUDA
    库的列表，其中包括一些我们自己的库，免费提供给个人或学术使用，或为商业用途提供许可和支持。
- en: General library conventions
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一般库约定
- en: The NVIDIA-provided libraries, as a general principle, do no memory management
    for the caller. They instead expect the caller to provide pointers to the area
    of allocated memory *on the device*. This allows for a number of functions on
    the device to be run one after another without unnecessary device/host transfer
    operations between calls.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA提供的库通常不为调用者执行内存管理。它们期望调用者提供指向*设备上*已分配内存区域的指针。这使得设备上的多个函数可以一个接一个地运行，而无需在调用之间进行不必要的设备/主机传输操作。
- en: As they perform no memory operations, it is the caller’s responsibility to both
    allocate and free memory after usage. This extends even to providing memory for
    any scratch space or buffer areas used by the library calls.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们不执行任何内存操作，因此调用者有责任在使用后分配和释放内存。这甚至包括为库调用使用的任何临时空间或缓冲区区域提供内存。
- en: Although this may seem an overhead to place onto the programmer, it’s actually
    a very good design principle and one you should follow when designing libraries.
    Memory allocation is a costly operation. Resources are limited. Having a library
    continuously allocating and freeing memory in the background is far less efficient
    than you performing these operations once at startup and then again at program
    exit.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将这一负担加到程序员身上似乎是一个额外的开销，但它实际上是一个非常好的设计原则，也是你在设计库时应该遵循的原则。内存分配是一项代价高昂的操作，资源是有限的。让库在后台不断进行内存分配和释放，远不如你在启动时进行一次内存分配，在程序退出时再进行一次释放更为高效。
- en: NPP (Nvidia Performance Primitives)
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NPP（Nvidia性能原语）
- en: The NPP library provides a set of functions for image and general signal processing.
    It supports all CUDA platforms. To include NPP into your project, simply include
    the relevant header files and link to the precompiled library.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NPP库提供了一套用于图像和通用信号处理的函数。它支持所有CUDA平台。要将NPP包含到项目中，只需包含相关的头文件并链接到预编译库。
- en: For signal processing functions, the library expects one or more source pointers
    (`pSrc1`, `pSrc2`, etc.), one or more destination pointers (`pDst1`, `pDst2`,
    etc.), or one or more mixed pointers for in-place operations (`pSrcDst1`, `pSrcDst2`,
    etc). The library names the functions according to the data type processed. C++
    function name overloading—that is, using a single name for a common function—is
    not supported.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于信号处理函数，库期望一个或多个源指针（`pSrc1`、`pSrc2`等）、一个或多个目标指针（`pDst1`、`pDst2`等），或一个或多个混合指针用于就地操作（`pSrcDst1`、`pSrcDst2`等）。库根据处理的数据类型命名函数。C++函数名重载—即使用一个名称表示通用函数—是不支持的。
- en: The supported data types for signal processing are `Npp8u`, `Npp8s`, `Npp16u`,
    `Npp16s`, `Npp32u`, `Npp32s`, `Npp64u`, `Npp64s`, `Npp32f`, and `Npp64f`. These
    equate to unsigned and signed versions of 8, 16, 32, and 64 types plus the 32-
    and 64-bit single-precision and double-precision floating-point types.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的信号处理数据类型有`Npp8u`、`Npp8s`、`Npp16u`、`Npp16s`、`Npp32u`、`Npp32s`、`Npp64u`、`Npp64s`、`Npp32f`和`Npp64f`。这些数据类型包括8、16、32和64位无符号和有符号版本，以及32位和64位的单精度和双精度浮点类型。
- en: 'The image part of the library follows a similar naming convention, in that
    the function names reflect intended usage and data type. Image data can be organized
    in a number of ways, so there are a few key letters that allow you to see the
    functionality and data type from the name. These are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 库中的图像部分遵循类似的命名约定，即函数名反映了预期的用途和数据类型。图像数据可以以多种方式组织，因此有一些关键字母可以让你从函数名中看出功能和数据类型。这些包括：
- en: • A—Used where the image contains an alpha channel that should not be processed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: • A—用于图像包含一个不应处理的Alpha通道的情况。
- en: • Cn—Used where the data is laid out in a packed or interleaved format of *n*
    channels, for example {R, G, B}, {R, G, B}, {R, G, B}, etc. would be C3.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: • Cn—用于数据以*n*通道的打包或交错格式布局的情况，例如{R, G, B}、{R, G, B}、{R, G, B}等将是C3。
- en: • Pn—Used where the color data is split into planes, such as all the data from
    one color is contiguous, so {R, R, R}, {G, G, G}, {B, B,.B}, etc. would be P3.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: • Pn—用于颜色数据分为不同平面的情况，即每种颜色的数据是连续的，比如{R, R, R}、{G, G, G}、{B, B, B}等将是P3。
- en: In addition to how the data is organized, the naming also tells you how the
    function will manipulate the data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据的组织方式外，命名还告诉你该函数将如何处理数据。
- en: • I—Used where the image data is manipulated in-place. That is, the source image
    data will be overwritten by the operation being performed on it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: • I—用于图像数据就地操作的情况，即源图像数据将被执行的操作覆盖。
- en: • M—Indicates that a nonzero mask will be used to determine which pixels meet
    the criteria. Only those pixels will be processed. Useful, for example, in overlaying
    one image onto another.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: • M—表示将使用非零掩码来确定哪些像素符合条件，只有这些像素会被处理。例如，这在将一幅图像覆盖到另一幅图像时非常有用。
- en: • R—Indicates that the function operates on a subsection of the image, via the
    caller specifying an ROI (region of interest).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: • R—表示该函数操作的是图像的一个子区域，通过调用者指定感兴趣区域（ROI，Region of Interest）来实现。
- en: • Sfs—Indicates the function performs a fixed scaling and saturation on the
    output data as part of the operation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: • Sfs—表示该函数在输出数据上执行固定的缩放和饱和度调整，作为操作的一部分。
- en: The use of such short function naming postfixes leads to function names that
    may appear somewhat obscure to the casual reader. However, once you memorize the
    meaning of each attribute of the function name and work with NPP a little bit,
    you’ll quickly recognize what operation a given function is performing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种简短的函数命名后缀会导致函数名对普通读者来说可能有些晦涩。然而，一旦你记住了每个函数名属性的含义，并且稍微使用过NPP，你就能很快识别出给定函数执行的操作。
- en: The image data functions take also an additional parameter of `pSrcStep` or
    `pDstStep`, which are pointers to the size of a given image line/row in bytes,
    including any padding bytes that are added to the line width to ensure alignment.
    Many image processing functions will add padding bytes to the end of a line to
    ensure the following lines starts on a suitable boundary. Thus, an image 460 pixels
    wide may be padded to 512 bytes per line. A line width value that is a multiple
    of 128 is a good choice, as this will allow entire cache lines to be brought in
    from the memory subsystem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据函数还会额外接受一个参数 `pSrcStep` 或 `pDstStep`，它们是指向给定图像行/列大小（以字节为单位）的指针，包括为确保对齐而在行宽中添加的任何填充字节。许多图像处理函数会在行末尾添加填充字节，以确保接下来的行在合适的边界处开始。因此，一张宽度为
    460 像素的图像可能会将每行填充至 512 字节。选择一个行宽值为 128 的倍数是一个不错的选择，因为这将允许整个缓存行从内存子系统中加载。
- en: Let’s look at a simple example from the signal manipulation library. We’ll take
    two sets of random data and XOR them together. We’ll do this both on the host
    and the device and then compare the result.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个来自信号处理库的简单例子。我们将取两组随机数据并对它们进行异或操作。我们将在主机和设备上都进行此操作，然后比较结果。
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '` const int num_bytes = (1024u ∗ 255u) ∗ sizeof(Npp8u);`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`const int num_bytes = (1024u ∗ 255u) ∗ sizeof(Npp8u);`'
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '` CUDA_CALL(cudaMemcpy(host_dst_ptr1, device_dst_ptr2, num_bytes, cudaMemcpyDeviceToHost));`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA_CALL(cudaMemcpy(host_dst_ptr1, device_dst_ptr2, num_bytes, cudaMemcpyDeviceToHost));`'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice in the code we have used an `NPP_CALL` macro around the call to the NPP
    library. This is similar to the `CUDA_CALL` macro we’ve used throughout this text.
    It checks that the return value from the caller is always equal to `NPP_SUCESS`
    (zero) and otherwise prints the error code associated with the returned value.
    Negative values are associated with errors and positive values with warnings.
    Unfortunately, there is no function to convert the error code to an error message,
    so you have to look up the error value in the NPP documentation (Section 7.2,
    “NPP Type Definitions and Constants,” as of v4.1 of the library).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在代码中我们在调用 NPP 库时使用了 `NPP_CALL` 宏。这与我们在本文本中多次使用的 `CUDA_CALL` 宏类似。它检查调用者的返回值是否始终等于
    `NPP_SUCESS`（零），如果不等则打印与返回值相关联的错误代码。负值表示错误，正值表示警告。不幸的是，没有函数将错误代码转换为错误信息，因此你必须在
    NPP 文档中查找错误值（根据 v4.1 版本的库，参见第 7.2 节“ NPP 类型定义和常量”）。
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each of the NPP calls is invoking a kernel on the device. By default, NPP operates
    in a synchronous mode using the default stream 0\. However, often you will want
    to perform a number of operations one after another. You may then want to do some
    other work on the CPU, so you will come back later to check the progress of the
    GPU task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 NPP 调用都在设备上调用一个内核。默认情况下，NPP 在同步模式下运行，使用默认流 0。然而，通常你会希望连续执行多个操作。此时，你可能希望在
    CPU 上做一些其他工作，稍后再回来检查 GPU 任务的进度。
- en: 'To specify that NPP will use a given, already defined stream, use the following
    API call:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要指定 NPP 使用一个已定义的流，可以使用以下 API 调用：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As we saw from some other examples in this text, if you have a number of sequential
    kernel calls, you can achieve much better overall performance by pushing them
    into the nondefault stream. This is largely because this permits asynchronous
    memory transfers and thus overlapping compute and transfer work. However, to achieve
    this, we need to change the program somewhat, as follows.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本文中的其他示例所看到的，如果有多个顺序的内核调用，通过将它们推送到非默认流中，可以实现更好的整体性能。这主要是因为这允许异步内存传输，从而使计算和传输工作重叠。然而，为了实现这一点，我们需要对程序做一些修改，如下所示。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`  CUDA_CALL(cudaMalloc((void ∗∗) &(device_dst_ptr1[i]), num_bytes));`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`  CUDA_CALL(cudaMalloc((void ∗∗) &(device_dst_ptr1[i]), num_bytes));`'
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '` }`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '` }`'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The major difference we see with the streamed version is that we now need multiple
    output data blocks on the host as well as multiple copies on the device. Thus,
    all the device arrays are now indexed by `[NUM_STREAMS]`, allowing the streams
    to operate entirely separately and independently of one another.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在流式版本中看到的主要区别是，现在我们需要在主机上有多个输出数据块，并且在设备上有多个副本。因此，所有设备数组现在都由`[NUM_STREAMS]`进行索引，允许流完全独立地操作，彼此之间互不干扰。
- en: To use the asynchronous model we need to allocate host memory as pinned, so
    we have to use `cudaHostMalloc` instead of `malloc`, paired with `cudaFreeHost`
    instead of `free`. We also need to wait on the completion of the stream prior
    to processing its data. In this example we wait on all four streams, but in reality
    as one stream completes, it would be provided with more work. See [Chapters 8](CHP008.html)
    and [9](CHP009.html) regarding multi-GPU programming and optimization, respectively,
    to see how this works.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用异步模型，我们需要将主机内存分配为固定内存，因此我们必须使用`cudaHostMalloc`而不是`malloc`，并配合使用`cudaFreeHost`而不是`free`。我们还需要在处理流数据之前等待该流完成。在这个示例中，我们等待所有四个流，但实际上，随着一个流完成，它将被分配更多的工作。有关多GPU编程和优化的详细信息，请参见[第8章](CHP008.html)和[第9章](CHP009.html)。
- en: If we look at a plot from Parallel Nsight we can actually see this happening
    with our new streamed version of the code ([Figure 10.1](#F0010)). Notice on the
    output two large transfers to the device followed by a small series of kernel
    operations. Notice also that the transfer in stream 3 starts while the kernels
    in stream 2 are still running (Memory and Compute rows). Finally, notice all the
    transfers back to the host come one after another.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看来自Parallel Nsight的图表，实际上可以看到我们的新流式版本代码中的这些操作（[图10.1](#F0010)）。注意，在输出中，先有两个大的传输到设备，然后是一小系列的内核操作。还要注意，流3中的传输在流2中的内核仍在运行时就开始了（内存和计算行）。最后，注意所有传输回主机的操作是一个接一个的。
- en: '![image](../images/F000107f10-01-9780124159334.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000107f10-01-9780124159334.jpg)'
- en: FIGURE 10.1 NPP streamed calls.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 NPP流式调用。
- en: In this example the transfers, as can often be the case, dominate the overall
    timeframe. It depends largely on the amount of processing you are doing on the
    GPU and if, in fact, you need to transfer the data all the time. Leaving the data
    on the GPU is a good solution, especially if you later intend to visualize it
    or just simply do not need a host copy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，传输通常占据了整个时间框架的大部分。这在很大程度上取决于你在GPU上执行的处理量，以及你是否确实需要持续进行数据传输。将数据保留在GPU上是一个不错的解决方案，尤其是如果你打算稍后进行可视化，或者根本不需要主机上的拷贝。
- en: In this particular example, because the series of kernels is small in comparison
    to the transfer, the synchronous time was 300 ms whereas the asynchronous time
    was 280 ms. We have a very small kernel/transfer overlap, so we save only this
    time from the overall timeframe. To benefit significantly from parallel independent
    workloads we actually need multiple GPUs where the transfers and kernels can operate
    in parallel across *N* GPUs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，由于内核的序列相对于传输较小，同步时间为300毫秒，而异步时间为280毫秒。我们有一个非常小的内核/传输重叠，因此我们只节省了这一部分时间。从整体时间框架中来看，要从并行的独立工作负载中获得显著收益，我们实际上需要多个GPU，在这些GPU上，传输和内核可以在*N*个GPU上并行操作。
- en: Depending on the mix of events, memory copies, and kernels, you can achieve
    a significant improvement by using the asynchronous mode. This is because the
    CUDA device can simply get on with the work set, rather than idle while the CPU
    organizes more work for it. By using multiple streams for independent work units
    you can define task level parallelism in addition to the regular data level parallelism.
    This is exploitable on Fermi-class GPUs (compute 2.x), to some extent, in that
    it’s used to fill the GPU via back-to-back and concurrent kernels. As the SM devices
    within the GPUs become larger, as is the case for Kepler, this becomes more and
    more important.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据事件、内存拷贝和内核的组合，你可以通过使用异步模式实现显著的性能提升。这是因为CUDA设备可以直接开始工作，而不是在CPU为其组织更多任务时处于空闲状态。通过使用多个流来处理独立的工作单元，你可以定义任务级别的并行性，除了常规的数据级并行性之外。这在Fermi架构的GPU（计算能力2.x）上是可以利用的，某种程度上，它用于通过连续的内核和并行的内核填充GPU。随着GPU中SM设备的增大，比如Kepler架构，这一点变得越来越重要。
- en: Note that the setup here is for a single DMA transfer engine, as found on consumer
    cards. The Telsa devices have both DMA engines enabled, allowing transfer to and
    from the device to also be overlapped. In the previous timeline this would cause,
    with some program changes, the copy-back-to-host transfers to occur while the
    copy-to-device transfers were occurring. In effect, we’d eliminate the time of
    the copy-back-to-host transfers, a significant savings. Due to both DMA engines
    being enabled on Tesla devices, enabling streams can bring significant benefits
    for this platform.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的设置是针对单个 DMA 传输引擎的，这是消费级显卡上的标准配置。Tesla 设备启用了两个 DMA 引擎，允许设备之间的传输和往返的传输同时进行。在之前的时间线中，通过一些程序修改，这会使得在数据传输到设备时，数据传回主机的操作能够重叠进行。实际上，我们将消除数据传回主机的时间，这是一项显著的节省。由于
    Tesla 设备启用了两个 DMA 引擎，启用流式传输可以为该平台带来显著的好处。
- en: Note also that for this test we are using a PCI-E 2.0 X8 link. Using a PCI-E
    3.0 X16 link would reduce the transfer time to around a quarter of what is shown
    here, making transfers less of an issue.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另请注意，测试中我们使用的是 PCI-E 2.0 X8 链接。如果使用 PCI-E 3.0 X16 链接，传输时间将减少到此处所示的四分之一左右，从而减少传输问题的影响。
- en: Once you have your kernel working, and only once you have it working, switch
    to an asynchronous mode of operation. An Asynchronous operation, however, can
    make debugging somewhat more complex so this is best done only once everything
    works correctly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的内核正常工作，且仅在它正常工作之后，才切换到异步操作模式。然而，异步操作可能会使调试变得更加复杂，因此最好在一切正常后再进行此切换。
- en: '**SDK Samples:** Grabcut, Histogram Equalization, BoxFilter, Image Segmentation,
    Interoperability with the FreeImage library.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**SDK 示例：** Grabcut、直方图均衡化、BoxFilter、图像分割、与 FreeImage 库的互操作性。'
- en: Thrust
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Thrust
- en: 'Those familiar with C++ may have used the C++ STL and specifically the BOOST
    library. For those not familiar with templates, they are actually a very useful
    feature of C++. In traditional C if you have a simple function that you wish to
    perform an operation, sum for example, you have to specify the operand type explicitly.
    Thus, you might have something like the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对 C++ 熟悉的人可能使用过 C++ STL，特别是 BOOST 库。对于不熟悉模板的人来说，模板实际上是 C++ 中非常有用的功能。在传统的 C 语言中，如果你有一个简单的函数，假设是进行加法运算的函数，你必须显式地指定操作数的类型。因此，你可能会有如下所示的代码：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If I wish to call the function and pass a float then I need a new function.
    As C does not allow the same name for two functions, I need something like `sum_i32`,
    `sum_u8`, `sum_u32`, `sum_f32`, etc. It’s somewhat tedious to provide and use
    a library that is type specific.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我希望调用这个函数并传递一个浮动值，那么我需要一个新的函数。由于 C 语言不允许两个函数使用相同的名称，我需要像 `sum_i32`、`sum_u8`、`sum_u32`、`sum_f32`
    等这样的命名方式。提供和使用一个类型特定的库有些繁琐。
- en: C++ tried to address this in the form of function overloading. This allows the
    same name to be used for many functions. Depending on the type of the parameters
    passed, the appropriate function is called. However, the library provider still
    needs to write one function body to handle `int8`, `int16`, `f32`, etc. even if
    he or she can now use a common name for the function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: C++尝试通过函数重载来解决这个问题。这允许同一个名称用于多个函数。根据传递的参数类型，调用适当的函数。然而，库提供者仍然需要编写一个函数体来处理`int8`、`int16`、`f32`等，即使现在可以使用统一的名称来调用该函数。
- en: The C++ template system addresses this. We have a generic function that changes
    only in terms of the type of its parameters. Thus, why does the programmer have
    to copy and paste the code a dozen times to support all the possible permutations
    he or she might imagine? Templates, as the name suggests, mean supplying a template
    of what you’d like to do in a type agnostic manner. The compiler then generates
    a version of the function at compile time if, and only if, it is actually called.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: C++的模板系统解决了这个问题。我们有一个通用函数，它仅在其参数类型上发生变化。因此，为什么程序员需要将代码复制粘贴十几次，以支持他或她可能想象的所有组合呢？模板，顾名思义，意味着以类型无关的方式提供你想做的事情的模板。然后，编译器会在编译时生成函数的一个版本，前提是该版本确实被调用。
- en: Thus, the STL was born, a type agnostic definition of some very common functions
    in C++. If you happen to use the `int32` version, but not the `f32` version, only
    the `int32` version results in actual code within your application. The downside
    of templates, as opposed to libraries, is that they are compiled during every
    compilation run, so they can increase compile times.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，STL应运而生，它为C++中一些非常常见的函数提供了类型无关的定义。如果你使用的是`int32`版本，而不是`f32`版本，那么只有`int32`版本的代码会在你的应用程序中生成。模板的缺点，与库不同，是它们在每次编译时都会被编译，因此可能会增加编译时间。
- en: The NVCC compiler is actually a C++ front end, rather than a C front end. The
    standard development package on Windows, Microsoft Visual Studio, also supports
    C++ language, as do the primary Linux and Mac development environments.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: NVCC编译器实际上是C++前端，而不是C前端。Windows上的标准开发包——Microsoft Visual Studio——也支持C++语言，主要的Linux和Mac开发环境也同样支持。
- en: The Thrust library supports many of the STL containers for which it makes sense
    to support on a massively parallel processor. The simplest structures are often
    the best and thus arrays (or vectors as the STL calls them) are well supported
    in Thrust. Not all containers make sense, such as lists which have unknown access
    times and are of variable length.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Thrust库支持许多适合在大规模并行处理器上使用的STL容器。最简单的结构通常是最好的，因此数组（或者STL称之为向量）在Thrust中得到了很好的支持。并不是所有容器都适合使用，比如列表，因为列表的访问时间未知且长度可变。
- en: 'One other C++ concept we need to cover before we look at Thrust is C++ namespaces.
    In C if you declare two functions with the same name you get an error at the link
    stage if it is not detected during compilation. In C++ this is perfectly valid
    providing the two functions belong to a different namespace. A namespace is a
    little like specifying a library prefix to differentiate which library the compiler
    should search for the function. The C++ namespace is actually a class selector.
    Classes in C++, at a very high level, are simply a way of grouping related functions
    and data together in one definition. Thus, we can have two calls to the same function,
    providing the namespace used is different. For example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论 Thrust 之前，还需要了解一个 C++ 概念，那就是 C++ 命名空间。在 C 中，如果你声明两个同名的函数，且在编译时未被发现，就会在链接阶段出现错误。而在
    C++ 中，只要这两个函数属于不同的命名空间，这是完全有效的。命名空间有点像指定库前缀，用来区分编译器应该搜索哪个库中的函数。C++ 命名空间实际上是一个类选择器。C++
    中的类，从非常高的层次上讲，实际上就是一种将相关函数和数据组合在一个定义中的方式。因此，只要命名空间不同，我们可以对同一个函数进行两次调用。例如：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Because of the namespace prefix the compiler can identify which function is
    intended to be called. The `::` (double colon) is equivalent to the `->` operator
    if you think of the class definition as a structure definition that has a number
    of function pointers as well as data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于命名空间前缀，编译器可以识别出要调用的是哪个函数。`::`（双冒号）等同于`->`运算符，如果你把类定义看作是一个包含多个函数指针以及数据的结构定义。
- en: Finally, to use the Thrust library we need one more C++ concept, that of a *functor*.
    A functor can be thought of as a function pointer that can also store state or
    data. It is actually a pointer to an instance of an object that is based on a
    given class definition.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了使用 Thrust 库，我们还需要了解一个 C++ 概念——*函数对象（functor）*。函数对象可以被看作是一个函数指针，但它还能存储状态或数据。实际上，它是指向一个基于给定类定义的对象实例的指针。
- en: If you have ever used any functions like `qsort` (Quicksort) from the standard
    C libraries, you’ll be familiar with the concept of a user-provided function.
    In the case of `qsort` you provide the comparison function to say whether one
    opaque data object is greater than or less than another opaque data object. You
    may have specific or multiple criteria to rank records by. The provider of the
    `qsort` library cannot hope to cover a significant number of these possibilities,
    so they provide a formal parameter where you must provide your own comparison
    function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过标准 C 库中的任何函数，例如`qsort`（快速排序），你就会熟悉用户提供函数的概念。在`qsort`的情况下，你需要提供比较函数，用来判断一个不透明数据对象是否大于或小于另一个不透明数据对象。你可能有特定的或多个标准来排序记录。`qsort`库的提供者无法覆盖所有这些可能性，因此他们提供了一个正式的参数，你必须提供自己的比较函数。
- en: The Thrust library provides you with two vector containers. Vectors in the STL
    are simply resizable arrays that can hold elements of any type. Thrust provides
    both host and device vector classes that, reside in the global memory of the host
    and device, respectively.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Thrust 库为你提供了两个向量容器。STL 中的向量只是可调整大小的数组，可以容纳任何类型的元素。Thrust 提供了主机和设备向量类，分别驻留在主机和设备的全局内存中。
- en: Vectors can be read or modified using the array subscript notation (the `[`
    and `]` symbols). However, be aware that Thrust is in the background performing
    individual transfers over the PCI-E bus for each such access, if the vector is
    on the device. Therefore, putting such a construct within a loop is a *really*
    bad idea.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 向量可以使用数组下标符号（`[` 和 `]`）来读取或修改。然而，请注意，如果向量位于设备上，Thrust 会在后台通过 PCI-E 总线对每次访问进行单独的传输。因此，在循环中使用这样的结构是一个*非常*糟糕的主意。
- en: 'The first aspect of using Thrust is simply how to get the data you need into
    and out of thrust vectors. Thus, we first need to include the necessary include
    files. Thrust provides the following broad set of function types:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Thrust 的第一个方面就是如何将你需要的数据输入到 Thrust 向量中以及如何将数据从中输出。因此，我们首先需要包含必要的头文件。Thrust
    提供了以下几种广泛的函数类型：
- en: • Transformation
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: • 转换
- en: • Reduction
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: • 归约
- en: • Prefix sum
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: • 前缀和
- en: • Reordering
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: • 重排序
- en: • Sorting
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: • 排序
- en: Thrust is not a library in the traditional sense, as all of its contents are
    contained within the header files you include into your source. Thus, you wish
    to avoid simply including everything and should include only the header files
    you need.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Thrust 不是传统意义上的库，因为它的所有内容都包含在你包含到源代码中的头文件内。因此，你应该避免仅仅包含所有内容，而应该只包含你需要的头文件。
- en: Thrust provides two vector objects, `host_vector` and `device_vector`, which
    are used with C++ terminology to create an instance of these objects. For example,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Thrust 提供了两个向量对象，`host_vector` 和 `device_vector`，它们使用 C++ 术语来创建这些对象的实例。例如，
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this code we declare two objects, one that physically resides on the host
    and one that will physically reside on the device.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们声明了两个对象，一个物理上位于主机上，另一个物理上位于设备上。
- en: The `thrust::` part specifies a class namespace that you can largely think of
    as a library specifier in C. The `host_vector` and `device_vector` are functions
    (constructors) provided by the object. The `<int>` and `<float>` specifiers are
    passed to the constructor (the initialization function) for the object. The constructor
    then uses them along with the value passed into the function to allocate 200 elements
    of `sizeof(float)` and 500 elements of `sizeof(int)`, respectively. Internally
    there may be other data structures, but at a high level this is effectively what
    you are doing when using such a C++ constructor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`thrust::`部分指定了一个类命名空间，可以将其大致理解为C语言中的库说明符。`host_vector`和`device_vector`是由对象提供的函数（构造函数）。`<int>`和`<float>`说明符被传递给构造函数（初始化函数）。构造函数然后利用这些说明符以及传递给函数的值来分别分配200个`sizeof(float)`元素和500个`sizeof(int)`元素。在内部可能还有其他数据结构，但从高层次来看，这就是在使用此类C++构造函数时实际做的事情。'
- en: Objects in C++ also have a destructor, a function that is called when the object
    goes out of scope. This function is responsible for deallocating any resources
    allocated by the constructor or during the runtime of the object. Thus, unlike
    C, it’s not necessary to call `free` or `cudaFree` for Thrust vectors.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: C++中的对象也有一个析构函数，它在对象超出作用域时被调用。这个函数负责释放构造函数或对象运行期间分配的任何资源。因此，与C语言不同，Thrust向量不需要调用`free`或`cudaFree`。
- en: Having defined a vector you now need to get data into and out of the vector.
    A Thrust vector is conceptually simply a resizable array. Thus, a vector object
    provides a `size()` function that returns the number of elements in the vector.
    This allows you to use standard loop constructs, for example,
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个向量后，你现在需要将数据输入和输出到该向量中。Thrust向量在概念上仅仅是一个可调整大小的数组。因此，一个向量对象提供了一个`size()`函数，用于返回向量中的元素数量。这使得你可以使用标准的循环结构，例如：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example the array `[]` operator is being supplied by the class definition.
    Thus, in every iteration, a function is being called to transform `i` into a physical
    piece of data. Because of this, if the data happens to be on the device, Thrust
    will generate a transfer from the device to the host, in the background, which
    is completely hidden from the programmer. The `size()` function means the number
    of iterations is only known at runtime. As this can change within the loop body,
    it must be called on each loop iteration. This in turn prevents the compiler from
    statically unrolling the loop.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，数组`[]`操作符由类定义提供。因此，在每次迭代中，都会调用一个函数将`i`转化为一个实际的数据单元。因为这个原因，如果数据恰好位于设备上，Thrust会在后台生成一个从设备到主机的传输，而这一过程对程序员完全透明。`size()`函数意味着迭代次数只有在运行时才能知道。由于这在循环体内可能发生变化，因此必须在每次循环迭代中调用。这反过来又阻止了编译器静态展开该循环。
- en: Depending on your view of such things, you’ll either love it or hate it. The
    love it camp loves abstractions because they make programming easier and you don’t
    have to care about the hardware. This camp is primarily made up of inexperienced
    programmers and people who simply want to get a calculation done. The hate it
    camp wants to know what is happening and is very keen to maximize performance
    from the hardware it has. They don’t want to have a “simple” array dereference
    initiate a hugely inefficient 4-byte PCI-E transfer. They’d much prefer a far
    more efficient several-megabyte transfer to or from the device/host when they
    schedule it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你对这些事情的看法，你要么会喜欢它，要么会讨厌它。喜欢它的人喜欢抽象，因为它们让编程变得更简单，你不需要关心硬件。这一阵营主要由没有经验的程序员和那些只想完成计算的人组成。讨厌它的人则想知道发生了什么，并且非常渴望最大化利用他们所拥有的硬件性能。他们不想让一个“简单”的数组解引用引发一个极其低效的
    4 字节 PCI-E 传输。他们更愿意在安排时进行一次更高效的几兆字节传输，不管是从设备到主机，还是从主机到设备。
- en: 'Thrust actually makes both camps happy. Large transfers are simply done by
    initiating a host vector with a device vector or vice versa. For example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Thrust 实际上让两个阵营都满意。大规模传输只需通过启动主机向量与设备向量，或者反之来完成。例如：
- en: '`thrust::host_vector <float> my_host_float_out_vector (my_device_float_results_vector.begin(),
    my_device_float_results_vector.end() );`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`thrust::host_vector <float> my_host_float_out_vector (my_device_float_results_vector.begin(),
    my_device_float_results_vector.end() );`'
- en: or
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the first example we are creating a new vector on the host side and initializing
    the host vector with the device vector. Notice we did not specify only one value
    for the constructor as we did previously when creating a host vector, but two.
    In such cases Thrust does a subtraction to work out the number of elements that
    need to be copied and allocates storage on the host accordingly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，我们在主机端创建了一个新的向量，并用设备向量初始化了主机向量。注意，我们没有像之前创建主机向量时那样只指定一个构造函数参数，而是指定了两个。在这种情况下，Thrust
    会通过减法计算出需要复制的元素数量，并相应地在主机上分配存储空间。
- en: In the second example we use the explicit copy method. This method (function)
    takes three parameters, the start and end of the destination region, plus the
    start of the source region. Because Thrust knows what type of vector you are using,
    the copy method works for both host and device vectors. There is no need to specify
    additional parameters such as `cudaMemcpyDeviceToHost` or `cudaMemcpyHostToDevice`,
    or to call different functions depending on the type passed. Thrust is simply
    using C++ templates to overload the namespace to invoke a number of functions
    depending on the parameters passed. As this is done at compile time, you have
    the benefit of strong type checking and no runtime overhead. Templates are one
    of the major benefits of C++ over C.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，我们使用了显式的复制方法。这个方法（函数）接受三个参数：目标区域的起始和结束位置，以及源区域的起始位置。由于 Thrust 知道你正在使用的向量类型，因此复制方法适用于主机和设备向量。无需指定额外的参数，如
    `cudaMemcpyDeviceToHost` 或 `cudaMemcpyHostToDevice`，也无需根据传递的类型调用不同的函数。Thrust 通过使用
    C++ 模板重载命名空间来根据传递的参数调用不同的函数。因为这一过程发生在编译时，所以你可以享受到强类型检查和零运行时开销的好处。模板是 C++ 相较于 C
    的一个主要优势。
- en: Using the functions provided by Thrust
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Thrust 提供的函数
- en: Once the data is within a Thrust device vector or host vector container, there
    are a number of standard functions Thrust provides. Thrust provides a simple sort
    function that requires only the start and end of the vector. It distributes the
    work over the different blocks and performs any reduction and interblock communications
    for you. This is often code that people new to CUDA get wrong. Having something
    such as a sort function makes using the GPU as easy as using the common C `qsort`
    library routine.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据进入 Thrust 的设备向量或主机向量容器，Thrust 提供了许多标准函数。Thrust 提供了一个简单的排序函数，只需要向量的起始和结束位置。它会将工作分配到不同的块，并为你执行任何归约和块间通信。这通常是
    CUDA 新手容易出错的代码。像排序函数这样的功能使得使用 GPU 像使用常见的 C `qsort` 库函数一样简单。
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see this in action with a short program.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个简短的程序看到这一点。
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '` **thrust::generate(host_array.begin(), host_array.end(), rand);**`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '` **thrust::generate(host_array.begin(), host_array.end(), rand);**`'
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Problems with Thrust
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Thrust 的问题
- en: What is interesting to note here is that the GPU- and CPU-based sorts may or
    may not be performed at the same time, depending on how you arrange the transfers.
    Unfortunately, Thrust always uses the default stream and you cannot change this
    as with NPP library. There is no stream parameter to pass, or function to set
    the currently selected stream.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一个有趣点是，GPU 和 CPU 基于的排序可能会同时执行，也可能不会，取决于你如何安排数据传输。不幸的是，Thrust 总是使用默认流，并且无法像
    NPP 库一样更改流。没有流参数可以传递，也没有函数可以设置当前选择的流。
- en: Using the default stream has some serious implications. The sort operation is
    actually just a series of kernels run in stream 0\. Kernels, like regular kernels,
    launch asynchronously. However, memory transfers, unless explicitly done asynchronously,
    operate synchronously. Thus, any function you call from Thrust that returns a
    value, `reduce` for example, and any copy back to the host causes an implicit
    synchronization. In the example code, placing the sort host array call after the
    copy back from the device code would have serialized the GPU and CPU sorts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认流会带来一些严重的影响。排序操作实际上只是一个在流0中运行的内核序列。内核像普通内核一样是异步启动的。然而，内存传输除非明确以异步方式进行，否则是同步进行的。因此，从Thrust中调用的任何返回值的函数，例如`reduce`，以及任何回传主机的拷贝，都会导致隐式同步。在示例代码中，如果将排序主机数组调用放在从设备代码回传后的位置，则会使GPU和CPU的排序操作串行化。
- en: Multi-CPU/GPU considerations
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多CPU/GPU考量
- en: On the CPU side, Thrust automatically spawns *N* threads where *N* is the number
    of physical processor cores. Thrust is actually using OpenMP for the CPU side,
    and by default OpenMP will use the number of physical cores on the CPU.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU端，Thrust会自动生成*N*个线程，其中*N*是物理处理器核心的数量。Thrust实际上使用OpenMP来处理CPU端，并且默认情况下OpenMP会使用CPU上的物理核心数量。
- en: It would be nice if the GPU version did this also, splitting the task over *N*
    GPUs. The host side implements a NUMA (nonuniform memory access)-based memory
    system. This means all memory addresses are accessible to any CPU socket and any
    CPU core. Thus, even on a dual-CPU system, 8, 12, or 16 CPU cores can work in
    parallel on a problem.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果GPU版本也能像这样将任务分配到*N*个GPU上，那就太好了。主机端实现了一个基于NUMA（非统一内存访问）的内存系统。这意味着所有的内存地址对任何CPU插槽和任何CPU核心都是可访问的。因此，即使在双CPU系统上，8、12或16个CPU核心也可以并行地处理一个问题。
- en: Multiple GPUs are more like a cluster of distributed memory machines all attached
    to the PCI-E bus. The GPUs can talk over the bus directly to one another using
    the peer-to-peer (P2P) functionality if you have the correct hardware and operating
    system (OS). To have multiple GPUs work together on a sort is a little more complicated.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 多个GPU更像是一组分布式内存机器，所有这些机器都连接到PCI-E总线上。如果你有合适的硬件和操作系统（OS），这些GPU可以通过点对点（P2P）功能直接通过总线进行通信。让多个GPU共同工作进行排序稍微复杂一些。
- en: However, just like regular multi-GPU programming, Thrust supports the single-thread/multiple-GPU
    and multiple-threads/multiple-GPU model. It does not implicitly make use of multiple
    GPUs. It’s left to the programmer to either spawn multiple threads or use `cudaSetDevice`
    calls where appropriate to select the correct device to work on.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，和常规的多GPU编程一样，Thrust支持单线程/多GPU和多线程/多GPU模型。它不会隐式地使用多个GPU。程序员需要自己选择启动多个线程，或者在适当的位置使用`cudaSetDevice`调用来选择正确的设备进行操作。
- en: When sorting on multiple processors there are two basic approaches. The first
    is used by merge sort. Here the data is split into equal-size blocks with each
    block independently sorted and a final merge operation applied. The second, used
    by algorithms like the sample sort we looked at earlier, is to partially sort,
    or presort, the blocks. The resultant blocks can then be independently sorted,
    or can also simply be concatenated together to form the sorted output.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 排序多处理器时有两种基本方法。第一种方法由归并排序使用。在这种方法中，数据被分成相等大小的块，每个块独立排序，然后进行最终的归并操作。第二种方法，由像我们之前看到的样本排序算法使用，是部分排序或预排序这些块。结果块可以独立排序，或者也可以直接连接在一起形成排序后的输出。
- en: As memory access time is much slower than comparison time, algorithms that have
    the fewest passes over the data, both in terms of reading and writing, tend to
    be the fastest. Operations that create contention, such as merging, ultimately
    limit scaling compared with those algorithms that can maintain wide parallelism
    throughout the entire process.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存访问时间远比比较时间慢，因此那些对数据进行最少读取和写入操作的算法，往往是最快的。那些会引起竞争的操作，比如归并，最终会限制算法的扩展性，而相比之下，能够在整个过程中保持广泛并行性的算法则更具扩展性。
- en: For basic types (`u8`, `u16`, `u32`, `s8`, `s16`, `s32`, `f32`, `f64`) Thrust
    uses a very fast radix sort, something we looked at earlier with sample sort.
    For other types and user-defined types it uses a merge sort. Thrust automatically
    adjusts the number of bits used for the radix sort depending on the type and the
    range of the data. Thus, a 32-bit sort where the maximum range of the data is
    only 256 is significantly faster than one where the entire range is used.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基本类型（`u8`、`u16`、`u32`、`s8`、`s16`、`s32`、`f32`、`f64`），Thrust 使用非常快速的基数排序，这是我们之前在样本排序中看到的。对于其他类型和用户定义的类型，它使用归并排序。Thrust
    会根据类型和数据的范围自动调整基数排序中使用的位数。因此，32 位排序，其中数据的最大范围只有 256，比使用整个范围的排序要快得多。
- en: Timing sort
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 排序计时
- en: To see some timings, let’s add some timers to the Thrust example code and see
    what values we get.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看一些计时结果，让我们在 Thrust 示例代码中添加一些计时器，看看得到的值。
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '` CUDA_CALL( cudaGetDeviceProperties( &prop, device_num ) );`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '` CUDA_CALL( cudaGetDeviceProperties( &prop, device_num ) );`'
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`  **thrust::device_vector<int> device_array = host_array;**`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '` **thrust::device_vector<int> device_array = host_array;**`'
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`}`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: As Thrust uses the default stream for all calls, to time device code we simply
    insert a number of events and then get the delta time between the various events.
    Notice, however, that we need to synchronize the stream after the sort and after
    the final event. The `cudaEventRecord` function, even if the device is not currently
    doing anything, returns immediately, without setting the event. Thus, leaving
    out the synchronize call after the device sort significantly extends the actual
    time reported.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Thrust 对所有调用都使用默认流，为了计时设备代码，我们只需插入多个事件，然后计算这些事件之间的时间差。然而，请注意，在排序后和最终事件后，我们需要同步流。`cudaEventRecord`
    函数即使设备当前没有执行任何操作，也会立即返回，而不会设置事件。因此，如果在设备排序后遗漏同步调用，会显著延长实际报告的时间。
- en: The timings we see across our four devices are shown in [Table 10.1](#T0010)
    and [Figure 10.2](#F0015) for sorting 16 MB of random data. As you can see from
    the table there is a fairly linear decline of speed as we move back the various
    GPU generations. As we hit the compute 1.1 9800 GT device we see a significant
    jump in execution time. The 9800 GT has less than half of the memory bandwidth
    and around two-thirds, at best, of the processing power of the GTX260.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在四个设备上看到的排序时间如 [表 10.1](#T0010) 和 [图 10.2](#F0015) 所示，数据为 16 MB 的随机数据。从表中可以看到，随着我们回退到不同的
    GPU 代际，速度呈现出相当线性的下降趋势。当我们使用计算能力 1.1 的 9800 GT 设备时，执行时间显著增加。9800 GT 的内存带宽不到 GTX260
    的一半，且其处理能力最多也只有 GTX260 的三分之二。
- en: Table 10.1 Thrust Sort Timings on Various Devices
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1 不同设备上的 Thrust 排序时间
- en: '| Device | Time |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 设备 | 时间 |'
- en: '| --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GTX470 | 67.45 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| GTX470 | 67.45 |'
- en: '| GTX460 | 85.18 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| GTX460 | 85.18 |'
- en: '| GTX260 | 109.02 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| GTX260 | 109.02 |'
- en: '| 9800 GT | 234.83 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 9800 GT | 234.83 |'
- en: '![image](../images/F000107f10-02-9780124159334.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000107f10-02-9780124159334.jpg)'
- en: FIGURE 10.2 Thrust sort time by device (16 MB data).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 按设备划分的 Thrust 排序时间（16 MB 数据）。
- en: Host times by comparison are pretty poor on our 2.5 Ghz AMD Phenom II X4\. Sort
    time averages around 2400 ms, some 10× slower than even the 9800 GT. However,
    is this really a fair comparison? It depends on how efficiently Thrust implements
    the sort on the CPU, and on the particular CPU used and the host memory bandwidth.
    Both Parallel Nsight and the task manager indicate Thrust does not load the CPU
    on our test system by more than 25%. This would indicate it’s far from making
    the best use of the CPU resources. Thus, to use it as a comparison is unfair to
    the CPU and artificially inflates the GPU performance figures.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 相较之下，我们的 2.5 GHz AMD Phenom II X4 主机的排序时间相当差，平均约为 2400 毫秒，甚至比 9800 GT 慢 10 倍。然而，这真的是一个公平的比较吗？这取决于
    Thrust 在 CPU 上如何高效实现排序，所使用的具体 CPU，以及主机的内存带宽。Parallel Nsight 和任务管理器都表明，Thrust 在我们的测试系统上并未充分利用
    CPU，CPU 的负载没有超过 25%。这表明它远未充分利用 CPU 资源。因此，将其作为比较是不公平的，这人为地抬高了 GPU 的性能数据。
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can see a single-core `qsort` easily outperforms Thrust sorting on the
    CPU side and uses near 100% of a single core. If we assume a similar speedup for
    a parallel version as we saw on the OpenMP reduce we looked at earlier, a typical
    CPU figure would be half that shown here, let’s say 475 ms. Even so, a GPU-based
    Thrust sort is outperforming the CPU by a factor of almost six times, even accounting
    for transfers to and from the PCI-E bus.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，单核的 `qsort` 在 CPU 端轻松超过了 Thrust 排序，并且几乎完全利用了一个核心的计算资源。如果我们假设并行版本的加速效果与我们之前看到的
    OpenMP 降维相似，那么典型的 CPU 计算时间大约是此处所示的一半，大约是 475 毫秒。即便如此，基于 GPU 的 Thrust 排序仍然比 CPU
    快了接近六倍，即使考虑到从 PCI-E 总线的传输时间。
- en: 'Thrust also has a number of other useful functions:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Thrust 还具有许多其他有用的功能：
- en: • Binary search
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: • 二分查找
- en: • Reductions
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: • 降维
- en: • Merging
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: • 合并
- en: • Reordering
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: • 重新排序
- en: • Prefix sum
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: • 前缀和
- en: • Set operations
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: • 集合操作
- en: • Transformations
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: • 变换
- en: Documentation on each of these is provided in the Thrust user guide. The usage
    of each is similar to the sort example we’ve used here.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容的文档可以在 Thrust 用户手册中找到。每个操作的用法与我们在这里使用的排序示例相似。
- en: We could obviously write a great deal on Thrust, but this chapter is about libraries
    in general, so we’ll look at just one more example, that of reduction.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显然可以写很多关于 Thrust 的内容，但本章是关于库的，因此我们只看一个例子，即降维。
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`#include <cstdlib>`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`#include <cstdlib>`'
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`#define NUM_ELEM_END (1024∗1024∗256)`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`#define NUM_ELEM_END (1024∗1024∗256)`'
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`   reduce_h_t = (get_time() - reduce_h_t);`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`   reduce_h_t = (get_time() - reduce_h_t);`'
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '` return 0;`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '` return 0;`'
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Reduction is an interesting problem in that, as we saw earlier, it’s difficult
    to write a reduction that is faster than an OpenMP version given the transfer
    time of the data to the GPU. We win considerably because there is only one significant
    transfer to the device, but this dominates the overall time. However, if the data
    is already on the GPU, then of course this transfer is not associated with the
    reduction step.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是一个有趣的问题，正如我们之前看到的，由于数据传输到 GPU 的时间，写出比 OpenMP 版本更快的降维算法是困难的。我们获得了显著的优势，因为只有一次重要的设备传输，但这占据了总时间的主导地位。然而，如果数据已经在
    GPU 上，那么当然这一传输就与降维步骤无关了。
- en: We will time a Thrust-based reduction, both on the host and device, against
    a standard single-core serial reduction, an OpenMP quad-core reduction, and the
    four GPU test devices with a range of data sizes. See [Table 10.2](#T0015) and
    [Figure 10.3](#F0020). The one item missing from the table is the Thrust reduce
    time on the CPU. It was excluded because it was significantly larger than any
    of the other figures. With consistency across all the block sizes, the CPU Thrust
    sort took approximately 10 times the execution time of the single-core serial
    version.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对基于 Thrust 的降维进行计时，分别在主机和设备上与标准的单核串行降维、OpenMP 四核降维以及四个 GPU 测试设备进行对比，数据规模不同。请参见
    [表 10.2](#T0015) 和 [图 10.3](#F0020)。表中唯一缺失的项是 CPU 上的 Thrust 降维时间。由于它显著大于其他任何数值，因此被排除在外。在所有块大小一致的情况下，CPU
    上的 Thrust 排序耗时大约是单核串行版本的 10 倍。
- en: Table 10.2 Reduce Timing for Multiple Sizes and GPUs (in ms)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2 多种大小和 GPU 的归约时间（单位：毫秒）
- en: '![Image](../images/T000107tabT0015.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000107tabT0015.jpg)'
- en: '![image](../images/F000107f10-03-9780124159334.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000107f10-03-9780124159334.jpg)'
- en: FIGURE 10.3 GPU reduce time (in ms) by size.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 GPU 归约时间（单位：毫秒），按大小分类。
- en: Notice in [Figure 10.3](#F0020), and also in [Table 10.2](#T0015), the *y* axis
    is time in milliseconds. Thus, a lower value is better. The GTX260, surprisingly,
    comes out with the best reduction times, marginally outperforming the later-generation
    GTX470.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在[图 10.3](#F0020)中，以及在[表 10.2](#T0015)中，*y* 轴表示时间，单位是毫秒。因此，数值越低越好。令人惊讶的是，GTX260
    的归约时间最好，略微超过了后代的 GTX470。
- en: This is all very well, but how does it compare with the CPU versions? All the
    cards are faster than the single-core serial implementation, by 3.5× to 7×. The
    GTX470 and GTX260 cards fare very well against the OpenMP version, coming in at
    around two-thirds of the time of the parallel CPU version. The GTX460 is about
    the same time as the CPU, with the 9800 GT being slower. However, if we take into
    account the (PCI-E 2.0 x8) transfer time, 278 ms for 512 MB of data, even the
    GTX260 at 106 ms (plus 278 ms) is slower than the OpenMP version at 164 ms.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都很好，但与 CPU 版本相比如何呢？所有显卡的速度都比单核串行实现快，快了 3.5 倍到 7 倍。GTX470 和 GTX260 显卡与 OpenMP
    版本的对比表现非常好，归约时间大约是并行 CPU 版本的三分之二。GTX460 的时间与 CPU 相差不大，而 9800 GT 则较慢。然而，如果考虑到（PCI-E
    2.0 x8）传输时间，对于 512 MB 的数据为 278 毫秒，即便是 GTX260 的 106 毫秒（加上 278 毫秒）也比 OpenMP 版本的
    164 毫秒慢。
- en: With a CUDA-implemented reduction, or a Thrust asynchronous version that supported
    streams, we could have subtracted the kernel time as we overlapped successive
    transfers and kernels. This is, of course, assuming we have more than one reduction
    to perform or we broke the single reduction down into a series of reductions.
    Even with this, we’re looking at a best case of 178 ms, which is still slower
    than the OpenMP version. The clear message here is make use of OpenMP and the
    CPU when appropriate. If the data is already on the GPU, then perform the reduction
    on the GPU. Otherwise, use the CPU for some useful purpose. See [Figure 10.4](#F0025).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CUDA 实现的归约，或支持流的 Thrust 异步版本，我们可以通过重叠连续的传输和内核来减去内核时间。当然，这是在假设我们有多个归约操作要执行，或者将单个归约操作分解成一系列归约操作的前提下。即使如此，我们仍然只能得到最好的情况——178
    毫秒，这仍然比 OpenMP 版本慢。这里的明确信息是，适当时应利用 OpenMP 和 CPU。如果数据已经在 GPU 上，则在 GPU 上执行归约。否则，使用
    CPU 来完成一些有用的任务。请参见[图 10.4](#F0025)。
- en: '![image](../images/F000107f10-04-9780124159334.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000107f10-04-9780124159334.jpg)'
- en: FIGURE 10.4 OpenMP and serial reduction timing (in ms).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 OpenMP 和串行归约的时间（单位：毫秒）。
- en: Using Thrust and CUDA
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Thrust 和 CUDA
- en: Thus, we’d like to be able to use these features of the Thrust library with
    regular CUDA code. It may well be that you can write your application entirely
    using the provided Thrust operations. However, libraries never cover everything
    and always end up doing certain things well and others not so well. Therefore,
    we don’t want to be forced into a particular way of thinking just to make use
    of a library.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望能够将 Thrust 库的这些特性与常规 CUDA 代码结合使用。也许你可以完全使用提供的 Thrust 操作来编写应用程序。然而，库永远无法覆盖所有内容，总会有做得好的部分和做得不那么好的部分。因此，我们不想为了使用某个库而被迫采用某种特定的思维方式。
- en: Thrust does not provide a mechanism to copy data out of its `host_vector` structure
    in an easy manner. It provides only a read single element method. Thus, a copy
    can only be performed one element at a time, which is laboriously slow. However,
    with `device_vectors` we have an alternative method.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Thrust 不提供一种简便的方法来将数据从其`host_vector`结构中复制出来。它只提供读取单个元素的方法。因此，复制操作只能一次复制一个元素，这样非常慢。然而，通过`device_vectors`，我们有了另一种方法。
- en: First, you need to allocate the storage space on the device yourself, thus obtaining
    a pointer to the data and not a Thrust iterator. Then you need to cast the regular
    device pointer to a Thrust device pointer. This is done using the `device_ptr`
    constructor. You may then pass this Thrust device pointer to the various Thrust
    functions. Now Thrust works on the underlying data you have supplied and thus
    it is visible to you, rather than being hidden within the Thrust library.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要自己分配设备上的存储空间，从而获得数据的指针，而不是 Thrust 迭代器。然后，你需要将常规设备指针转换为 Thrust 设备指针。这个过程是通过使用`device_ptr`构造函数来完成的。接下来，你可以将这个
    Thrust 设备指针传递给各种 Thrust 函数。现在，Thrust 会在你提供的底层数据上工作，因此它对你是可见的，而不是隐藏在 Thrust 库内部。
- en: We can adapt the sort example to make use of this.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整排序示例来利用这一点。
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '` printf("\nSorting %lu data items (%lu MB)", NUM_ELEM, (size_in_bytes/1024/1024));`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '` printf("\nSorting %lu data items (%lu MB)", NUM_ELEM, (size_in_bytes/1024/1024));`'
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Notice the constructor `thrust::device_ptr`, which creates the object `thrust_dev_ptr`
    that can then be passed into the `thrust::sort` function. Unlike the conventional
    iteration a Thrust device pointer does not have “begin” and “end” functions, so
    we simply use base plus length to obtain the last element for the sort.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意构造函数`thrust::device_ptr`，它创建了对象`thrust_dev_ptr`，然后可以将其传递给`thrust::sort`函数。与常规迭代不同，Thrust
    设备指针没有“begin”和“end”函数，因此我们只需使用基地址加上长度来获取排序的最后一个元素。
- en: This allows host-initiated Thrust calls to be implemented alongside simple device
    kernels. However, be aware there is (as of 4.2 SDK) no device level interface
    for Thrust, so you cannot call Thrust functions from within a device or global
    function. Functions like `sort`, for example, spawn multiple kernels themselves.
    As the GPU cannot, at least until Kepler K20 is released, spawn additional work
    itself, we’re limited to host-based control.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得主机启动的 Thrust 调用可以与简单的设备内核一起实现。然而，请注意（截至 4.2 SDK），Thrust 没有设备级接口，因此你不能在设备或全局函数中调用
    Thrust 函数。例如，像 `sort` 这样的函数会自我生成多个内核。由于 GPU 至少在 Kepler K20 发布之前无法自行生成额外的工作任务，因此我们只能依赖主机控制。
- en: '**SDK Samples:** Line of Sight, Radix Sort, Particles, Marching Cubes, Smoke
    Particles.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**SDK 示例：** 视距、基数排序、粒子、行进立方体、烟雾粒子。'
- en: CuRAND
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CuRAND
- en: 'The CuRAND library provides various types of random number generation on the
    GPU. In C you are probably used to calling the standard library function `rand()`
    on the host. Like many standard library functions `rand()` is not available to
    be called in device code. Thus, your only option is to create a block of random
    numbers on the host and copy this over to the device. This causes a number of
    problems:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: CuRAND 库提供了在 GPU 上生成各种类型的随机数。在 C 语言中，你可能习惯于在主机上调用标准库函数 `rand()`。像许多标准库函数一样，`rand()`
    不能在设备代码中调用。因此，你唯一的选择是先在主机上创建一块随机数数据，然后将其复制到设备上。这会引发一些问题：
- en: • Increased startup time on the host.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: • 提高了主机启动时间。
- en: • Increased PCI-E bandwidth.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: • 增加了 PCI-E 带宽。
- en: • In practice, usually a poorly distributed random number set.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: • 实际应用中，通常随机数分布不均匀。
- en: The standard library `rand()` function is not designed for true randomness.
    It works, like many random number generation algorithms, by creating a list of
    pseudo-random numbers and simply selecting the next element from the list. Thus,
    anyone who knows the seed used can use this knowledge to accurately predict the
    next random number in the given sequence.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 标准库的 `rand()` 函数并非设计用来生成真正的随机数。它的工作原理类似于许多随机数生成算法，通过创建一个伪随机数列表并简单地选择下一个元素。因此，知道使用的种子的人可以利用这一知识准确预测给定序列中的下一个随机数。
- en: This has some implications, not least of which is from the security field. Many
    algorithms use randomness, in one way or another, to make it difficult to impersonate
    a peer. Suppose two peers exchange a seed of a random number generator. Peer A
    encodes a random number into the message frame. Peer B using the same seed and
    same random number generator knows what data identifier it should expect from
    peer A. Given a captured sequence of identifiers from peers A and B, it’s possible
    for an attacker, C, to work out the next number and spoof (pretend to be) either
    peer A or B.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这有一些影响，尤其是在安全领域。许多算法以某种方式使用随机性，目的是让伪装成对等体变得困难。假设两个对等体交换了一个随机数生成器的种子。对等体A将一个随机数编码到消息框架中。对等体B使用相同的种子和相同的随机数生成器，知道应该从对等体A接收到什么数据标识符。给定从对等体A和B捕获的标识符序列，攻击者C可以推算出下一个数字，并伪装成（假冒）对等体A或B。
- en: This is possible because the random numbers are usually just a small, repeating
    sequence of pseudo-random numbers. If the set of random numbers is small, then
    an attack is easy. The seed is either never set by the programmer, set to some
    “secret” number, or set based on the current time. Startup times are not really
    very random and thus time-based seeds are actually within a very small window.
    Secrets rarely remain secrets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可能的，因为随机数通常只是一个小的、重复的伪随机数序列。如果随机数集合很小，那么攻击就很容易。种子要么从未由程序员设置，要么设置为某个“秘密”数字，或者基于当前时间进行设置。启动时间实际上并不是非常随机，因此基于时间的种子实际上是在一个非常小的窗口内。秘密很少能保持秘密。
- en: Another example is password generation. If you have a few hundred users to set
    up on a system, they will usually be issued “random” passwords that are changed
    on first login. These passwords may be long character strings, leading to the
    belief that they are secure. However, if they are actually chosen from a random
    number generator with a small pseudo-random set of numbers, the actual search
    space for a brute-force attack is quite small.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个示例是密码生成。如果你需要在一个系统上设置几百个用户，他们通常会被分配“随机”密码，并在第一次登录时更改。这些密码可能是较长的字符字符串，从而让人认为它们是安全的。然而，如果它们实际上是从一个具有小伪随机数集的随机数生成器中选出的，那么暴力破解攻击的实际搜索空间就相当小。
- en: Thus, for anything where predictability of the sequence is a problem, we need
    much better random numbers than most standard library implementations of `rand()`
    provide.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于任何预测序列可能存在问题的情况，我们需要比大多数标准库实现的`rand()`提供的随机数更好的随机数。
- en: 'To use the CuRAND library, you need to include the following header file:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用CuRAND库，你需要包含以下头文件：
- en: '[PRE29]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Additionally, you need to ensure you link to the following library:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要确保链接到以下库：
- en: '[PRE30]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Obviously, replace the path with the current version of the CUDA toolkit you
    are using. Let’s therefore look at an example of generating some random numbers:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，替换路径为你当前使用的CUDA工具包版本。因此，让我们看一个生成随机数的示例：
- en: '[PRE31]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`   printf("\n");`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`   printf("\n");`'
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '` CUDA_CALL(cudaMemcpy(host_ptr, device_ptr, size_in_bytes, cudaMemcpyDeviceToHost));`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA_CALL(cudaMemcpy(host_ptr, device_ptr, size_in_bytes, cudaMemcpyDeviceToHost));`'
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This program generates `num_elem` number of random numbers on the device and
    the host using the CuRand API. It then prints both sets of random numbers. The
    output is shown here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序使用CuRand API在设备和主机上生成`num_elem`个随机数，然后打印两组随机数。输出如下所示：
- en: '[PRE34]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: One important issue to note from the example program is the API calls are the
    same for device and host functions, except for registering the generator. The
    device version must be used with `curandCreateGenerator` and the host version
    with `curandCreateGeneratorHost`. Additionally, note that the `curandGenerateUniform`
    function must be called with the associated host or device-based pointer. Getting
    either of these mixed up will likely result in a CUDA “unknown error” issue or
    the program simply crashing. Unfortunately, as both host-side and device-side
    memory allocations are just regular C pointers, it’s not possible for the library
    to tell if this pointer passed to it, is a host-side pointer, or device-side pointer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从示例程序中需要注意的一个重要问题是，API调用在设备和主机功能之间是相同的，除了注册生成器。设备版本必须使用`curandCreateGenerator`，而主机版本则使用`curandCreateGeneratorHost`。另外，请注意，`curandGenerateUniform`函数必须与相关的主机或设备指针一起调用。混淆这两者可能会导致CUDA的“未知错误”或程序崩溃。不幸的是，由于主机端和设备端的内存分配只是普通的C指针，因此库无法判断传递给它的指针是主机指针还是设备指针。
- en: Also be aware that CuRand, as with NPP, supports streams. Thus, a call to `curandSet
    Stream(generator, stream)` will switch the library to an asynchronous operation
    in that stream. By default the library will use stream 0, the default stream.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，CuRand与NPP一样，支持流。因此，调用`curandSetStream(generator, stream)`会将库切换到该流中的异步操作。默认情况下，库将使用流0，即默认流。
- en: There are many types of generators you can use with the CuRand library, including
    one based on the Mersenne Twister algorithm used for Monte Carlo simulations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用CuRand库的多种生成器类型，包括基于梅森旋转算法的生成器，该算法用于蒙特卡洛模拟。
- en: '**SDK Samples:** Monte Carlo, Random Fog, Mersenne Twister, Sobol.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**SDK 示例：** 蒙特卡洛，随机雾霾，梅森旋转算法，Sobol。'
- en: CuBLAS (CUDA basic linear algebra) library
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CuBLAS（CUDA基本线性代数）库
- en: The last library we’ll mention is the CuBLAS. The CuBLAS library aims to replicate
    the functionality of the Fortran BLAS library commonly used in Fortran scientific
    applications. To allow easy porting of existing Fortran BLAS code, the CuBLAS
    library maintains the Fortran column-major layout, the opposite of the standard
    C row-major layout. It also uses the 1..N as opposed to the C standard 0..(N-1)
    notation when accessing array elements.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要提到的最后一个库是CuBLAS。CuBLAS库旨在复制Fortran BLAS库的功能，这个库通常用于Fortran科学应用程序。为了方便现有Fortran
    BLAS代码的移植，CuBLAS库保持了Fortran的列主序布局，这与标准C的行主序布局相反。它还使用1..N，而不是C标准的0..(N-1)表示法来访问数组元素。
- en: Thus, for porting legacy Fortran code to CUDA, the CuBLAS library is ideal.
    There are many large codebases written over the past decades in Fortran. Allowing
    this existing legacy code to run on modern GPU-accelerated hardware without significant
    code changes is one of the great strengths of this library. However, it’s also
    one of its weaknesses, as it will not appeal to anyone who learned to program
    in a modern computing language.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于将旧版Fortran代码迁移到CUDA，CuBLAS库是理想的选择。在过去几十年里，有许多大型代码库是用Fortran编写的。允许这些现有的遗留代码在现代GPU加速硬件上运行，而无需进行重大代码更改，是该库的一个重要优势。然而，这也是其弱点之一，因为它不会吸引那些在现代计算语言中学习编程的人。
- en: The CuBLAS library documentation provides some sample macros to convert the
    old-style Fortran array indexes into what most programmers would consider “regular”
    array indexing. However, even if implemented as macros or as an inline function,
    this adds execution time overhead to anyone attempting to work with non-Fortran
    indexing. This makes using the library rather a pain for C programmers. C programmers
    may have preferred to see a separate C style CuBLAS implementation that natively
    supported C array indexing.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: CuBLAS库文档提供了一些示例宏，用于将旧式Fortran数组索引转换为大多数程序员认为的“常规”数组索引。然而，即使是作为宏或内联函数实现，这也会为任何尝试使用非Fortran索引的人员增加执行时间的开销。这使得C语言程序员使用该库时感到相当麻烦。C语言程序员可能更希望看到一个单独的C语言风格的CuBLAS实现，原生支持C语言数组索引。
- en: As of version four of the library, it deprecated the older API. It now requires
    all callers to first create a handle via a call to the `cublasCreate` function
    call before any other calls are made. The handle is used in subsequent calls and
    allows CuBLAS to be both re-entrant and to support multiple GPUs using multiple
    asynchronous streams for maximum performance. Note although these features are
    provided, it’s the programmers responsibility to handle multiple devices. Like
    so many of the other libraries provided, the CuBLAS library does not automatically
    distribute its load across multi-GPU devices.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从库的版本四开始，弃用了旧版API。现在要求所有调用者首先通过调用`cublasCreate`函数创建一个句柄，然后才能进行其他任何调用。该句柄将在后续调用中使用，并允许CuBLAS支持重入，并使用多个异步流来支持多GPU以获得最大性能。请注意，虽然提供了这些功能，但管理多设备是程序员的责任。像许多其他提供的库一样，CuBLAS库不会自动将负载分配到多GPU设备上。
- en: The current API can be used by including the `cublas_v2.h` file instead of the
    older `cublas.h` include file. Any current usage of the older API should be replaced
    with the newer API. As with the NPP library, operations are expected to be performed
    on data already present on the GPU and the caller is therefore responsible for
    transferring the data to and from the device. A number of “helper” functions are
    provided for such purposes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的API可以通过包含`cublas_v2.h`文件来使用，而不是旧的`cublas.h`包含文件。任何当前使用旧版API的代码应替换为新版API。与NPP库一样，操作预计将在已经存在于GPU上的数据上执行，因此调用者有责任将数据从设备传输到设备。为此提供了许多“助手”函数。
- en: The new CuBLAS interface is entirely asynchronous in nature, meaning even functions
    that return values do it in such a way as the value may not be available unless
    the programmer specifically waits for the asynchronous GPU operation to complete.
    This is part of the move to asynchronous streams that will become important when
    the Kepler K20 is released.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 新的CuBLAS接口完全是异步的，这意味着即使是返回值的函数，也以一种方式返回值，除非程序员明确等待异步GPU操作完成，否则该值可能不可用。这是向异步流的过渡的一部分，这在Kepler
    K20发布时将变得更加重要。
- en: We’ll look at a simple example here, declaring a matrix on the host side, copying
    it to the device, performing some operations, copying the data back to the host,
    and printing the matrix.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里查看一个简单的示例，声明一个主机端的矩阵，将其复制到设备上，执行一些操作，将数据复制回主机并打印矩阵。
- en: '[PRE35]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '` CUDA_CALL( cudaMallocHost( (void ∗∗) &host_dest_ptr_B, size_in_bytes ));`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '` CUDA_CALL( cudaMallocHost( (void ∗∗) &host_dest_ptr_B, size_in_bytes ));`'
- en: '[PRE36]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '` int host_max_idx_A, host_max_idx_B, host_max_idx_dest;`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '` int host_max_idx_A, host_max_idx_B, host_max_idx_dest;`'
- en: '[PRE37]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '` CUDA_CALL(cudaFreeHost(host_dest_ptr_B));`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '` CUDA_CALL(cudaFreeHost(host_dest_ptr_B));`'
- en: '[PRE38]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The basic steps of the program are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的基本步骤如下：
- en: • Create a CuBLAS handle using the `cublasCreate` function.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用`cublasCreate`函数创建一个CuBLAS句柄。
- en: • Allocate resources on the device and host.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: • 在设备和主机上分配资源。
- en: • Set a matrix on the device directly from a matrix on the host.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: • 直接从主机上的矩阵设置设备上的矩阵。
- en: • Run Saxpy on the device.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: • 在设备上运行Saxpy。
- en: • Run `max` and `sum` functions on the device.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: • 在设备上运行`max`和`sum`函数。
- en: • Copy the resultant matrix back to the host and display it.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: • 将结果矩阵复制回主机并显示。
- en: • Free any allocated resources.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: • 释放任何已分配的资源。
- en: In practice, real programs will be significantly more complex. We’ve attempted
    to show here the basic template necessary to get some simple CuBLAS functions
    working on the GPU.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，真正的程序将复杂得多。我们在这里尝试展示的是使一些简单的CuBLAS函数在GPU上运行所需的基本模板。
- en: '**SDK Samples:** Matrix Multiplication.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**SDK示例：** 矩阵乘法。'
- en: CUDA Computing SDK
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA计算SDK
- en: The CUDA SDK is a separate download from regular toolkits and drivers, although
    it is now bundled with the CUDA 5 release candidate for Windows users, so it may
    become a single download in the future. It contains lots of sample code and a
    nice interface to find all the provided CUDA documentation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA SDK是一个单独的下载项，和常规工具包及驱动程序不同，虽然它现在已与Windows用户的CUDA 5版本候选包捆绑在一起，因此未来可能会成为一个单一的下载项。它包含大量示例代码，并提供一个很好的界面来查找所有提供的CUDA文档。
- en: There are almost 200 samples, so we’ll select a few sample applications to look
    at in detail. We’ll look at some general-purpose applications since these are
    easier to understand for the wider audience this text is aimed at than some of
    the more domain-specific examples in the toolkit.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 有将近200个示例，所以我们将选择一些示例应用程序进行详细查看。我们将查看一些通用应用程序，因为这些对于本文本目标受众更易于理解，而不像工具包中的一些更专业的示例。
- en: The computing samples are incredibly useful for someone starting out with GPU
    programming, as well as more advanced programmers who want examples of how something
    should be done. Unfortunately, a lot of the underlying CUDA API is hidden from
    you. When you are learning a new API the last thing you need is yet another API
    layer on top of the one you wish to learn, so this rather complicates understanding.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 计算示例对刚开始学习 GPU 编程的人来说非常有用，也对那些想要了解如何实现某些功能的高级程序员有帮助。不幸的是，很多底层的 CUDA API 对你来说是隐藏的。当你学习一个新的
    API 时，你最不需要的就是在你希望学习的 API 之上再增加一层 API，这样反而会让理解变得更加复杂。
- en: Many of the SDK examples use the `cutil` or other packages that are *not* part
    of the standard CUDA release. Thus, when you see the line
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 SDK 示例使用了 `cutil` 或其他不属于标准 CUDA 发布版的包。因此，当你看到这一行时：
- en: '[PRE39]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: you might expect it to work in your own code. However, to do this it’s also
    necessary to include the relevant `cutil` source headers from the SDK. NVIDIA
    makes no guarantee about the version-to-version compatibility of these libraries.
    They are not part of the official CUDA release and therefore are not supported.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能期望它在你自己的代码中正常工作。然而，为了实现这一点，还需要从 SDK 中包含相关的 `cutil` 源代码头文件。NVIDIA 并不保证这些库在不同版本之间的兼容性。它们不是官方
    CUDA 发布的一部分，因此不受支持。
- en: The CUDA APIs always start `cuda…`. Therefore, if you see anything other than
    this, you should realize that you will need to bring in additional code from the
    SDK samples, should you wish to use such calls.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA API 总是以 `cuda…` 开头。因此，如果你看到任何其他的内容，你应该意识到，如果你希望使用这些调用，你将需要从 SDK 示例中引入额外的代码。
- en: So what does `cutilSafeCall` do? Fairly much the same as the `CUDA_CALL` macro
    we’ve used throughout this text. If there is an error returned from the caller,
    it prints the file and line number and then exits. So why not use the `cutil`
    package directly? Largely because there are many functions in this library and
    you only need a small number of them in reality.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 `cutilSafeCall` 是做什么的呢？它的作用与我们在本文中使用的 `CUDA_CALL` 宏基本相同。如果调用者返回错误，它会打印文件名和行号，然后退出。那么为什么不直接使用
    `cutil` 包呢？主要是因为这个库中有很多函数，而实际上你只需要其中的一小部分。
- en: There are, however, many useful functions within this package, for example,
    the `gpuGetMaxGflopsDeviceId` function that identifies the fastest GPU device
    in the system. You should browse through the libraries provided with the SDK to
    help you understand some of the samples before jumping into the samples themselves.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这个包中有许多有用的函数，例如，`gpuGetMaxGflopsDeviceId` 函数，它可以识别系统中最快的 GPU 设备。你应该浏览 SDK
    中提供的库，帮助你在深入了解示例之前理解一些示例。
- en: Device Query
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设备查询
- en: Device query is an interesting application in that it’s quite simple and allows
    you to see what your GPU is capable of. It is run from the command line rather
    than the Windows interface and can be found in “C:\ProgramData\NVIDIA Corporation\NVIDIA
    GPU Computing SDK 4.1\C\bin\win64\Release.”
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 设备查询是一个有趣的应用程序，它非常简单，并且可以让你查看GPU的能力。它是通过命令行运行，而不是通过Windows界面，位于“C:\ProgramData\NVIDIA
    Corporation\NVIDIA GPU Computing SDK 4.1\C\bin\win64\Release”目录下。
- en: 'Obviously it is the Windows 64-bit version we’re using here from the 4.1 toolkit,
    which may be different on your system. The output is shown here:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们这里使用的是Windows 64位版本，来自4.1工具包，这可能与你的系统有所不同。输出如下所示：
- en: '[PRE40]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '` Device has ECC support enabled:                No`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '` 设备是否启用了ECC支持： No`'
- en: '[PRE41]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The program will iterate through all GPUs to find and list the various details
    of each device. For brevity, we have listed only one of the four devices completely
    and extracted the interesting parts from the other devices. For those interested
    in Kepler GK104, the relevant details are as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 程序将遍历所有GPU，查找并列出每个设备的各种详细信息。为了简洁起见，我们只完全列出了四个设备中的一个，并从其他设备中提取了有趣的部分。对于对Kepler
    GK104感兴趣的用户，相关详细信息如下：
- en: '[PRE42]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '` CUDA Capability Major/Minor version number:    3.0`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '` CUDA能力主/次版本号： 3.0`'
- en: '[PRE43]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Items reported of note are the current driver and runtime version, which should
    be the same. Compute capability defines what type of device we have for a given
    device number. Also detailed is the number of cores/SMs per device, speed of the
    device, along with memory speed and width. Thus, it’s possible to calculate the
    peak bandwidth on a given device. Talking of bandwidth, this brings us to the
    next useful application in the SDK.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的重点项目是当前的驱动程序和运行时版本，它们应该是相同的。计算能力定义了我们在给定设备编号下的设备类型。还详细列出了每个设备的核心数/SM数、设备速度、内存速度和宽度。因此，可以计算出给定设备的峰值带宽。说到带宽，这引出了SDK中的下一个有用应用。
- en: Bandwidth test
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带宽测试
- en: 'The bandwidth example provided by the SDK provides the following useful statistics
    about your particular device/host setup:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: SDK提供的带宽示例提供了有关你特定设备/主机设置的以下有用统计信息：
- en: • Host-to-device bandwidth (paged and pinned memory)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: • 主机到设备带宽（分页和固定内存）
- en: • Device-to-host bandwidth (paged and pinned memory)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: • 设备到主机带宽（分页和固定内存）
- en: • Device-to-device bandwidth
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: • 设备到设备带宽
- en: 'The actual output is shown here for a GTX470 on an x8 PCI-E 2.0 link:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 实际输出显示了在x8 PCI-E 2.0连接上的GTX470：
- en: '[PRE44]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`  Transfer Size (Bytes) Bandwidth(MB/s)`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '` 传输大小（字节） 带宽（MB/s）`'
- en: '[PRE45]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: One of the things you’ll see when using this example program is just how much
    benefit using pinned memory on your system brings. We looked at this in [Chapter
    9](CHP009.html), but there is nothing like seeing it on your own system to drive
    home the point that pinned memory can be much faster for memory transfers. Even
    a modern Sandybridge-E processor achieves 3 GB/s versus 2.3 GB/s when using pinned
    versus paged memory on a similar PCI-E 2.0 x8 link.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个示例程序时，你会看到使用锁定内存带来的好处有多大。我们在[第9章](CHP009.html)中已经讨论过这一点，但亲自看到自己系统上的表现，能够更加深刻地理解锁定内存在内存传输中的优势。即使是现代的Sandybridge-E处理器，在类似的PCI-E
    2.0 x8连接下，使用锁定内存时的速度为3 GB/s，而使用分页内存时仅为2.3 GB/s。
- en: Typical memory on a consumer GPU card is anything from 512 MB (88/9800 series)
    to 2 GB (GTX680). There is really no reason why you should not pin system memory
    to do the transfers to or from the GPU. Even in a 32-bit system with a 4 GB memory
    limit, CUDA will still be using pinned transfers in the background. Therefore,
    you may just as well pin the memory yourself and avoid the implicit pageable to
    pinned memory copy within the driver.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 消费级GPU显卡的典型内存范围从512MB（88/9800系列）到2GB（GTX680）。实际上，没理由不将系统内存锁定，用于与GPU之间的数据传输。即使是在4GB内存限制的32位系统中，CUDA仍然会在后台使用锁定传输。因此，你完全可以自己锁定内存，避免驱动程序内隐式地将可分页内存复制到锁定内存。
- en: As memory is now very cheap there is no reason why you should not fully load
    the machine. This is especially the case if you have more than one GPU card or
    are using Tesla cards. You can purchase 16 GB of host memory for less than 100
    euros/dollars/pounds.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存现在非常便宜，因此完全没有理由不将机器的内存加载满。这尤其适用于拥有多个GPU显卡或使用Tesla显卡的情况。你可以以不到100欧元/美元/英镑的价格购买16GB的主机内存。
- en: If we take the memory clock from the GTX470, there is 1674 MHz with a bus width
    of 320 bits. Thus, we take the bus width and divide by 8 to get bytes (40 bytes).
    Next we multiply this by the clock rate (66,960 MB/s). Then we multiply by 2 for
    GDDR5 (133,920 MB/s). Then we divide by 1000 to get listed memory bandwidth (133.9
    GB/s) or 1024 (130.8 GB/s) to get actual bandwidth.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以GTX470的内存时钟为例，它的时钟频率为1674 MHz，数据总线宽度为320位。因此，我们将总线宽度除以8得到字节数（40字节）。接着我们将其乘以时钟频率（66,960
    MB/s）。然后再乘以2来适应GDDR5（133,920 MB/s）。最后，我们除以1000得到标注的内存带宽（133.9 GB/s），或除以1024得到实际带宽（130.8
    GB/s）。
- en: So why do we get device-to-device bandwidth of 113,232 MB/s instead of 133,920
    MB/s? Where did the missing 20 MB/s or 15% of the memory bandwidth go? The GPU
    never achieves this theoretical peak bandwidth. This is why it’s useful to run
    the bandwidth test as opposed to calculating the theoretical peak. You then have
    a very good idea of what bandwidth you will get in *your* system, with your PCI-E
    arrangement, your host CPU, your CPU chipset, your host memory, etc. By knowing
    this you know what your application should be able to achieve on a given target
    and can therefore see how much potential you have yet to exploit.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们得到的设备间带宽是113,232 MB/s，而不是133,920 MB/s呢？丢失的20 MB/s或15%的内存带宽去了哪里？GPU从未达到这个理论的峰值带宽。这就是为什么运行带宽测试比计算理论峰值更有用的原因。这样，你就能非常清楚地了解在*你的*系统中，使用你的PCI-E配置、主机CPU、CPU芯片组、主机内存等，你将得到什么样的带宽。通过了解这些，你就能知道你的应用在给定目标上应该能够达到的水平，从而可以看出你还有多少潜力尚未开发。
- en: Note with Tesla-based Fermi devices you can gain a significant boost in bandwidth
    by disabling the ECC memory option using the nvidia-smi tool. Error checking and
    correction (ECC) distributes the bit patterns using Hamming codes. This, in effect,
    means you need a larger memory space to store the same data block. This additional
    storage requirement means you trade both space and speed for the extra redundancy
    ECC brings. NVIDA claims to have addressed this issue in Kepler K20 (GK110), where
    the impact of using ECC is claimed to be around one-third of that of Fermi.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在基于Tesla的Fermi设备中，你可以通过使用nvidia-smi工具禁用ECC内存选项来显著提高带宽。错误检查和纠正（ECC）使用哈明码分配比特模式。实际上，这意味着你需要更大的内存空间来存储相同的数据块。这一额外的存储需求意味着你为了ECC带来的冗余而牺牲了空间和速度。NVIDIA声称在Kepler
    K20（GK110）中已经解决了这个问题，据称使用ECC的影响大约是Fermi的三分之一。
- en: SimpleP2P
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SimpleP2P
- en: The SimpleP2P example shows how to use the P2P memory transfer capabilities
    introduced in compute 2.x devices (Fermi). The principle of P2P transfers to avoid
    having to go through host memory (see [Figure 10.5](#F0030)). Host memory may
    be directly accessible to the PCI-E I/O hub (Northbridge), as is often the case
    with Intel’s QPI-based system. It may also be to the other side of the processor,
    as with Intel’s DMI and AMD’s hypertransport-based systems.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: SimpleP2P示例展示了如何使用计算2.x设备（Fermi）中引入的P2P内存传输功能。P2P传输的原理是避免必须通过主机内存（见[图 10.5](#F0030)）。主机内存可能直接可通过PCI-E
    I/O集线器（Northbridge）访问，这在基于Intel QPI的系统中常见。它也可能位于处理器的另一侧，例如Intel的DMI和AMD的超传输（HyperTransport）系统。
- en: '![image](../images/F000107f10-05-9780124159334.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000107f10-05-9780124159334.jpg)'
- en: FIGURE 10.5 P2P transfers.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 P2P传输。
- en: Depending on the number of GPUs in the system, the host memory may represent
    a bottleneck for the transfer in terms of its own speed. The maximum PCI-E transfer
    speed approaches 6 GB/s on a PCI-E 2.0 link. With PCI-E 3.0 (GTX680, GTX690, Tesla
    K10) the actual bandwidth almost doubles to just under 12 GB/s in each direction.
    To maximize bandwidth, you typically define two pinned memory areas and use a
    double buffer scheme to transfer into one block and out of the other. Especially
    with the older processors, you can rapidly consume the entire host memory bandwidth
    simply by performing transfers between GPUs through host memory. This will severely
    hamper any attempt to use the CPU for additional processing capabilities, as it
    will be competing for host memory bandwidth with the GPU transfers.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 根据系统中 GPU 的数量，主机内存可能会成为传输的瓶颈，影响其自身的速度。在 PCI-E 2.0 链接中，最大传输速度接近 6 GB/s。使用 PCI-E
    3.0（如 GTX680、GTX690、Tesla K10）时，实际带宽几乎翻倍，达到每个方向接近 12 GB/s。为了最大化带宽，通常会定义两个固定内存区域，并使用双缓冲方案，一个区域传输数据，另一个区域接收数据。特别是在较旧的处理器上，仅通过主机内存在
    GPU 之间进行数据传输，就能迅速耗尽整个主机内存带宽。这将严重影响任何尝试使用 CPU 进行额外处理的操作，因为它将与 GPU 传输竞争主机内存带宽。
- en: 'The idea of P2P is to keep the data out of the host memory space and to do
    a transfer directly between the GPUs. While this is an extremely useful feature,
    support for this in the mainstream Windows 7 systems has been noticeably lacking.
    Thus it is not something we’ve looked at yet in this text, so we will cover it
    here as there are a number of uses for this technology. Requirements to use the
    P2P functionality are:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: P2P 的概念是将数据保持在主机内存空间之外，直接在 GPU 之间进行传输。虽然这是一个非常有用的功能，但在主流的 Windows 7 系统中，P2P
    的支持明显不足。因此，在本文本中我们尚未涉及此功能，这里将重点介绍，因为这项技术有很多应用。使用 P2P 功能的要求包括：
- en: • A 64-bit OS, and thus UVA (unified virtual addressing) enabled, a requirement
    for P2P to be available.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: • 64 位操作系统，从而启用 UVA（统一虚拟地址），这是 P2P 可用的前提。
- en: • Two or more compute 2.x devices that support this feature.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: • 支持此功能的两个或更多计算 2.x 设备。
- en: • GPU Devices that are on the same PCI-E I/O hub.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: • 在同一 PCI-E I/O 集线器上的 GPU 设备。
- en: • Appropriate driver level support.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: • 适当的驱动程序级别支持。
- en: To use this feature under Windows 7 you need the 64-bit OS plus the TCC (Tesla
    compute cluster) drivers active. As the TCC driver will only activate with Tesla
    cards, effectively there is no mainstream consumer support for this in Windows
    7\. Thus, this should be considered as a feature suitable for clusters, high-performance
    computing (HPC), and other compute-centered applications. It’s not something you
    can exploit with, say, a video transcoding application for consumer PCs.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 7 下使用此功能需要 64 位操作系统并启用 TCC（Tesla 计算集群）驱动程序。由于 TCC 驱动程序仅在 Tesla 卡上激活，因此
    Windows 7 上实际上并不支持主流消费级设备。因此，这应视为适用于集群、高性能计算（HPC）和其他计算密集型应用程序的功能。这不是你可以在消费级 PC
    上使用的视频转码应用程序中利用的功能。
- en: 'To support P2P first you should check UVA is enabled:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持 P2P，首先应检查是否启用了 UVA：
- en: '[PRE46]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: UVA will only be enabled under a 64-bit OS, or Windows 7 using the TCC driver.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: UVA 只会在 64 位操作系统或使用 TCC 驱动程序的 Windows 7 上启用。
- en: 'Next you need to check if device A can talk to device B. Note, just because
    this test passes, it does not imply that device B can talk to device A. Resources
    are consumed in enabling P2P access in a given direction. This test can be performed
    with the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要检查设备 A 是否可以与设备 B 通信。请注意，仅仅因为这个测试通过，并不意味着设备 B 可以与设备 A 通信。在启用 P2P 访问的特定方向时会消耗资源。可以使用以下代码来执行此测试：
- en: '[PRE47]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Once peer access has been enabled, memory can be accessed, in the direction
    of the peer access, either in device kernels or as host-initiated memory copies.
    Thus, device 0 can enable peer access to device 1, as in the previous example.
    You can then call a kernel on device 1, passing it a pointer to the global memory
    space from device 0\. The kernel will then dereference this pointer in the same
    way as it would a zero-copy device pointer to host memory. Every time there is
    an access through that pointer device 1 will initiate a fetch over the PCI-E bus
    from device 0\. Of course, the same caveats apply as with zero-copy memory usage,
    specifically that you should avoid re-reading such memory and try to achieve coalesced
    access patterns for performance reasons.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用了对等访问，内存可以按照对等访问的方向进行访问，无论是在设备内核中还是作为主机发起的内存复制。因此，设备 0 可以启用对设备 1 的对等访问，如前面的示例所示。然后，您可以在设备
    1 上调用一个内核，传递一个指向设备 0 全局内存空间的指针。内核将像访问零拷贝设备指针访问主机内存一样解引用这个指针。每次通过该指针进行访问时，设备 1
    将通过 PCI-E 总线从设备 0 发起数据获取。当然，与零拷贝内存使用一样，适用相同的注意事项，特别是您应该避免重新读取这些内存，并尽量实现合并访问模式以提高性能。
- en: You can of course use such features for device-initiated memory copies. To do
    this from the device, have a device kernel fetch the data via a device pointer,
    and simply store it to the local device’s global memory. Equally you can push
    as well as pull data from one device to another. However, if you want bidirectional
    access, you will need to remember to enable P2P access in both directions.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以使用这些功能进行设备发起的内存复制。要从设备进行此操作，可以让设备内核通过设备指针获取数据，并将其简单地存储到本地设备的全局内存中。同样，您可以将数据推送或拉取到另一个设备。然而，如果您需要双向访问，您需要记得在两个方向上启用
    P2P 访问。
- en: 'The second approach is an explicit memory copy, something we must initiate
    from the host. There are the two standard forms of this, the synchronous version
    and the asynchronous streamed version:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是显式的内存复制，这是我们必须从主机发起的。它有两种标准形式，同步版本和异步流式版本：
- en: '`cudaMemcpyPeer(dest_device_ptr, dst_device_num, src_device_ptr, src_device_num,num_bytes);`'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaMemcpyPeer(dest_device_ptr, dst_device_num, src_device_ptr, src_device_num,num_bytes);`'
- en: and
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '[PRE48]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Finally, once we’re done, we need to disable the provisioning of the resources
    for the P2P access by calling
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦我们完成操作，我们需要通过调用禁用 P2P 访问资源的配置。
- en: '[PRE49]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Performance wise the SimpleP2P application reports 2.4 GB/s, which is quite
    close to the peak 3 GB/s available on this particular (PCI-E 2.0 x8) test system.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 性能方面，SimpleP2P应用报告了2.4 GB/s的传输速度，这非常接近此特定（PCI-E 2.0 x8）测试系统的峰值3 GB/s。
- en: The SimpleP2P example program in the SDK provides some simple template code
    as to how to do this in practice. It does a series of GPU transfers between two
    GPUs and then computes the transfer speed. With the background we’ve covered here
    you should be able to read and follow the example code.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: SDK中的SimpleP2P示例程序提供了一些简单的模板代码，演示如何在实践中实现这一点。它在两个GPU之间进行一系列GPU传输，然后计算传输速度。通过我们在此处介绍的背景，您应该能够阅读并跟随示例代码。
- en: asyncAPI and cudaOpenMP
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: asyncAPI 和 cudaOpenMP
- en: The `asyncAPI` SDK sample provides an example of using the asynchronous API,
    but is not actually very simple for someone new to CUDA to understand. We’ve covered
    streams and asynchronous operation already in the text. These are important for
    getting multi-GPU setups to work alongside CPU usage. Therefore, we’ll look at
    this example and see what exactly it does.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncAPI` SDK示例提供了使用异步API的示例，但对于CUDA新手来说，其实并不简单易懂。我们已经在文本中介绍了流和异步操作。它们对于让多GPU设置与CPU一起工作非常重要。因此，我们将查看这个示例，了解它到底是如何实现的。'
- en: The basic premise of the `asyncAPI` example is that it creates an asynchronous
    stream, into which it puts a memory copy to the device, a kernel, and finally
    a memory copy back to the host. During this time it runs some code on the CPU
    that simply counts up while the GPU is running the asynchronous kernel.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncAPI`示例的基本前提是，它创建一个异步流，将内存复制到设备、一个内核，并最终将内存复制回主机。在此期间，它在CPU上运行一些代码，该代码简单地计数，而GPU正在运行异步内核。'
- en: The `cudaOpenMP` example shows how to use OpenMP with CUDA. It identifies the
    number of CPU threads, and the number and name of each attached CUDA device. It
    then tries to spawn one thread per GPU device and work-share the different devices.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaOpenMP` 示例展示了如何将OpenMP与CUDA一起使用。它识别CPU线程的数量以及每个附加CUDA设备的数量和名称。然后，它尝试为每个GPU设备生成一个线程，并对不同设备进行工作共享。'
- en: We’ll provide a similar example here that fuses the two SDK examples, but simplifies
    them somewhat and is potentially more useful as template code for your own work.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里提供一个类似的示例，融合这两个SDK示例，但将其简化一些，并且可能作为您自己工作的模板代码更加实用。
- en: '[PRE50]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '` if (idx < num_elem)`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`  if (idx < num_elem)`'
- en: '[PRE51]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`  CUDA_CALL(cudaGetDeviceProperties(&device_prop[device_num], device_num));`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`  CUDA_CALL(cudaGetDeviceProperties(&device_prop[device_num], device_num));`'
- en: '[PRE52]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`  int complete = 0;`'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`  int complete = 0;`'
- en: '[PRE53]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`  CUDA_CALL(cudaFreeHost(host_ptr[device_num]));`'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`  CUDA_CALL(cudaFreeHost(host_ptr[device_num]));`'
- en: '[PRE54]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: There are a few points in the SDK examples that need further discussion. First,
    with the `asyncAPI` example, stream 0, the default stream, is used. Unfortunately,
    there are many instances where the default stream causes implicit synchronization
    between streams. You will almost certainly end up using a double- or triple-buffered
    method and this implicit synchronization will catch you out. When using asynchronous
    operations, always create your own streams.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The second point from the `asyncAPI` stream example that you may not have noticed
    is that it takes the number of elements, *N*, and divides it directly by the number
    of threads to get the number of blocks for the grid. As it happens *N* is a multiple
    of the number of threads, but what if it is not? What happens is, the last elements
    in the array are not processed by the GPU kernel. This may not be at all obvious
    for anyone starting out with CUDA. Always use the following formula for generating
    the number of blocks if you plan on allowing *N* not to be a multiple of the number
    of threads:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'And in the kernel, add a check for array overrun:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Now this creates the overhead of passing `num_elem` to the kernel and checking
    it within the kernel. If you can *guarantee* you will always use a multiple of
    the number of threads, then you can avoid the need for this code and stick with
    the much simpler `num_blocks = num_elem / num_threads` approach. Most of the time
    we can say as programmers this holds true, as we often control the data block
    sizes.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the `cudaOpenMP` example now, how are multiple CPU threads launched?
    It uses a call to `omp_set_num_threads`:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'There are two approaches here: to set one thread per GPU or multiple threads
    per GPU ([Figure 10.6](#F0035)). The later approach is more useful where you have
    many more CPU cores than GPUs. A simpler form of this OpenMP directive that often
    works more reliably is the one we’ve used in the sample program:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000107f10-06-9780124159334.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.6 Multiple GPUs with OpenMP.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 多GPU与OpenMP。
- en: '[PRE59]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: With this approach it does not matter how OpenMP may or may not have been configured,
    what environment variables are set or not; it spawns the specified number of threads.
    Note that the current thread is one of the threads used to execute work.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，OpenMP的配置情况、设置了哪些环境变量或没有设置都不重要，它会生成指定数量的线程。请注意，当前线程是用于执行工作的线程之一。
- en: '[PRE60]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: You can see from the program output that, by using different GPUs, the threads
    finish at different times. You can see from [Figure 10.6](#F0035) that there are
    four threads running, including the originating thread. If viewed on screen you
    would see dark green bars along the top showing the threads are mostly running
    (~95%) with occasional stalls that would be shown in light green. Below are the
    four GPU tasks each of which is performing a memset, a kernel launch, and then
    a copy back to host. The bottom row of bars shows the CPU utilization for this
    timeframe. You can see the CPU is busy almost the entire time.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序输出中可以看出，通过使用不同的GPU，线程完成的时间不同。从[图10.6](#F0035)中可以看到有四个线程在运行，包括原始线程。如果在屏幕上查看，您会看到顶部有深绿色的条形，表示线程大部分时间在运行（约95%），偶尔会有停顿，这些停顿会以浅绿色显示。下面是四个GPU任务，每个任务执行memset、启动内核，然后将数据复制回主机。底部的条形显示了此时间段内CPU的使用情况。您可以看到CPU几乎整个时间都在忙碌。
- en: As the four GPUs finish, the CPU threads continue to work until all GPUs in
    the set have completed. We could of course, and you would in practice, allocate
    more GPU work to these GPUs if we really had such different performance characteristics
    with our GPUs. However, most GPU systems will have all of the same GPUs present
    and thus we’d not have to care about reissuing work until they had all completed.
    As they are all the same, given a similar job, they would all complete around
    the same time.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 随着四个GPU的完成，CPU线程继续工作，直到所有GPU任务完成。当然，如果我们的GPU性能差异非常大，我们可以（并且在实际操作中会）为这些GPU分配更多的工作。然而，大多数GPU系统都会有相同的GPU，因此我们不必担心重新分配工作，直到所有GPU完成。由于它们相同，在类似的任务下，它们会在大约相同的时间完成。
- en: The next issue we should address with using OpenMP is where to put resource
    allocations and deallocations. Allocation of memory and creation of resources
    on a given device is a time-consuming process. Often there needs to be a common
    understanding of the allocation across threads and thus common data structures.
    To share common data structures across threads requires locking and this in turn
    often causes serialization. We see exactly this when we place the resource allocation/deallocation
    within the OpenMP parallel region. Therefore, allocation/deallocation prior to
    and after the OpenMP parallel region achieves the best CPU utilization within
    that region.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OpenMP 时，我们需要解决的下一个问题是如何处理资源的分配和释放。给定设备上的内存分配和资源创建是一个耗时的过程。通常，线程之间需要对分配有共同的理解，因此需要共享数据结构。为了在线程之间共享公共数据结构，需要加锁，这反过来通常会导致串行化。当我们将资源分配/释放放置在
    OpenMP 并行区域内时，正好会看到这种情况。因此，在 OpenMP 并行区域之前和之后进行分配/释放，可以实现该区域内最佳的 CPU 利用率。
- en: In connection with this is the use of calls into the CUDA API, in particular
    the `cudaEventQuery` call, to check if the device has completed. Such calls should
    in no way be considered as low overhead. If we change the value of `loop_iteration_check`
    constant from one million to just one, we see the CPU count drop from 1,239,000,000
    to just 16,136\. In effect, every thread is then asking, in every loop iteration,
    for the status of the device. Thus, the CPU spends more time in the driver than
    doing anything else. Unfortunately, this is exactly how the `asyncAPI` is coded
    and one of the reasons for highlighting it here. Be sensible about any API call
    you make within a loop. It will take time, so don’t just have the CPU poll the
    device every cycle. Do something useful with the CPU between device queries.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相关的是调用 CUDA API，特别是 `cudaEventQuery` 调用，用于检查设备是否已完成。这样的调用绝不能被认为是低开销的。如果我们将
    `loop_iteration_check` 常量的值从一百万更改为仅为 1，我们会看到 CPU 计数从 1,239,000,000 降低到仅 16,136。在这种情况下，每个线程每次循环迭代都会询问设备的状态。因此，CPU
    花更多时间在驱动程序中，而不是做其他任何事情。不幸的是，这正是 `asyncAPI` 编码的方式，也是我们在此强调它的原因之一。在循环中调用任何 API 时要谨慎。它会消耗时间，所以不要让
    CPU 每个周期都轮询设备。在设备查询之间，做一些有用的事情。
- en: Aligned types
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对齐类型
- en: 'The aligned types example seeks to show the effect of using the _`_align__(n)`
    directive. For example:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐类型示例旨在展示使用 _`_align__(n)`_ 指令的效果。例如：
- en: '[PRE61]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Here the `8` part is the number of bytes the start of any element shall be aligned
    to. The example explains, in the associated text, that the `align` directive allows
    the compiler to use larger reads per thread than it would otherwise use. In the
    preceding `LA32` case, the compiler can use a 64-bit read instead of two 32-bit
    reads. As we saw in [Chapter 9](CHP009.html), less memory transactions equate
    to more bandwidth. We used the vector types in the examples there, which also
    used the `align` directive within their definitions.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: One of the things we saw in the earlier examples was that to achieve anything
    like peak bandwidth you had to generate a sufficient number of memory transactions
    in flight. Unfortunately, this SDK sample is not written with this in mind. It
    uses 64 blocks of 256 threads, a total of 32 warps. To load a compute 2.x device
    fully we need 48 warps, (64 for Kepler) so the example uses too few blocks. We
    therefore extended this to 1024 blocks and chose a figure of 192 threads, a figure
    that works well across the entire set of compute levels.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: We also added the basic type output to the test so we can see baseline figures.
    Additionally each run was compiled specifically generating code for that device
    compute level. Note that this SDK example, even with the changes, only reaches
    about 50% of the peak memory transfer capacity. However, the relative memory bandwidth
    is actually the figure we’re interested in here.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Initially we see the baseline figures shown in [Table 10.3](#T0020) and [Figure
    10.7](#F0040) from the various devices. We can use this baseline performance table
    to assess how well aligned and nonaligned types perform.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.3 Table of Baseline Performance across Devices
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000107tabT0020.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000107f10-07-9780124159334.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.7 Graph of baseline performance across the devices (MB/s vs. transfer
    size).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from [Figure 10.7](#F0040), they all hit the maximum coalesced
    memory size at `u32`, or four bytes. This would equate to 32 threads, multiplied
    by 4 bytes, or 128 bytes in total. On Fermi, this is the size of a single cache
    line, so we flatline at this point on compute 2.x devices.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: The GTX285 device (compute 1.3) is executing 16-thread coalesced memory reads
    instead of 32 as in compute 2.x devices. Thus, it benefits from back-to-back reads
    and can make use of the 64-bit (8-byte) reads per thread. Additionally, with twice
    the number of SMs than the Fermi generation cards, and a wider memory bus than
    the GTX470, in this particular kernel it’s able to outperform the GTX470.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: In the 9800 GT (compute 1.1) we see a similar pattern to the GTX285\. However,
    the major difference here is the physical memory bandwidth is only around half
    of that of the GTX285\. Thus, we see a minor gain between 32- to 64-bit accesses
    per thread, much less than we see with the GTX285\. See [Table 10.4](#T0025).
    We can see from running the example the percentage change in the aligned versus
    the nonaligned access. In [Table 10.5](#T0030), 100% would represent no change.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.4 MB/s Aligned/Nonaligned for Various Devices
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000107tabT0025.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: Table 10.5 Percentage Change for Aligned versus Nonaligned Access Patterns
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000107tabT0030.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: Thus, we can see that as we move back through the compute levels, especially
    for the early compute levels, aligned access gains greatly. In the best case we
    see a 31× speed improvement when adding such a directive to the data structure.
    Even moving to the modern GPUs we can see a 2× performance gain. Clearly, adding
    such a directive is very beneficial in all cases except where it causes more memory
    to be moved from main memory to the GPU.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Note the `RGB32` case. This is actually a 96-bit structure (three `u32`s), effectively
    an `int3` for `float3` type. Adding the `align` directive inserts 4 bytes of padding
    at the end of the structure. Although this allows coalesced accesses, 25% of the
    data being transferred from the memory system is being discarded. In the nonaligned
    case, the overfetch from the previous cache line on Fermi devices saves 33% of
    the subsequent memory fetch.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: The conclusion we can draw from this example is that, if you are using structures,
    you need to think about the coalescing impact of this and, at a minimum, use the
    `align` directive. A better solution entirely is to create structures of arrays,
    rather than arrays of structures. For example, have separate red, green, and blue
    (RGB) color planes instead of interleaved RGB values.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Directive-Based Programming
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book has largely focused on writing CUDA directly. This is good if you
    enjoy writing programs and are maybe from a CS (computer science) background like
    myself. However, very many people who find themselves writing CUDA today are not
    in this category. Many people’s primary concern is their own problem space, not
    CUDA or elegant solutions from a CS perspective.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: One of the great successes of OpenMP is that it’s relatively easy to learn and
    pick up. It involves decorating the C source code with directives that tell the
    compiler various things about the parallel nature of the code it’s currently compiling.
    Thus, it requires the programmer to explicitly identify parallelism within the
    code. The compiler takes care of the somewhat harder task of exploiting that parallelism.
    On the whole, it does this reasonably well.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the obvious solution to making GPU programming easier is to extend the
    OpenMP model to GPUs. There are, unfortunately, two standards that have/will come
    about for this: the OpenMP4ACC and OpenACC standards. We’ll concentrate here on
    the OpenACC standard, as this is the one NVIDIA is clearly supporting. Generally,
    you find the size of a backer and the take up among programmers will largely dictate
    the success or failure of a given software programming initiative. Most standards,
    regardless of who develops them, largely cover the same space, so in most cases
    learning one makes it much easier to learn another.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in writing GPU code using directives, you will likely
    already have a reasonable understanding of the OpenMP directives for CPUs. The
    major difference we find with standards such as OpenACC is that they, and thus
    the programmer, also have to deal with the location of data. In an OpenMP system
    where there is more than a single physical socket for the CPU we have what is
    called a NUMA (nonuniform memory access) system.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from [Figure 10.8](#F0045), memory in a system with more than
    one CPU is attached directly to a given CPU. Thus, a process that resides on CPU[0]
    takes considerably longer to access memory that resides on CPU[1] than if that
    memory was local to CPU[0]. Let’s assume we have eight processes running over
    two CPU sockets, each CPU with four cores. To perform an exchange of data that
    requires many-to-many communications means we’re limited to the throughput of
    the slowest communication link. This will be the QPI/hypertransport link between
    processors over which the memory traffic to the other processor’s memory bus must
    go. The OpenMP model simply ignores this effect and lacks many of the data concepts
    accelerator-based solutions require.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000107f10-08-9780124159334.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: FIGURE 10.8 Multi-GPU data pathways.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenACC is a move toward directive programming and very much follows in the
    footsteps of OpenMP, which has been very successful within standalone, single
    machines.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenACC is aimed at:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: • Independent loop-based parallelism.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: • Programmers who have not yet been exposed to CUDA or found it too complex.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: • Programmers who have no wish to learn CUDA and are happy to abstract the details
    of the particular target architecture to the compiler.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: • Programmers who would like rapidly to prototype an existing serial application
    on the GPU.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC, as with OpenMP, tries to abstract the hardware and let the programmer
    write standard serial code that the compiler then transforms into code that runs
    on the accelerator. As with OpenMP it involves adding a series of pragma statements
    around loops to instruct the compiler to run particular loops in parallel.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: • Looks similar to OpenMP so it is easy to learn for anyone who has used OpenMP.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: • Existing serial source code remains unchanged and is simply decorated with
    directives.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: • Single set of source for both CPU and GPU accelerated versions.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: • Accelerator vendor agnostic. The potential, as with OpenCL, to target multiple
    hardware platforms including CPU-based AVX acceleration.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: • Takes care of many of the “details”, such as moving data to and from shared
    memory for data the user specifies shall be cached.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: • Vendor-cited studies show easy learning curve for non-CUDA programmers.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: • Supports Fortran in addition to C. Allows many existing Fortran programs to
    benefit from acceleration without a massive rewrite.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: • Not currently supported under Visual Studio, so is effectively a Linux-only
    solution.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: • Commercial product currently supported by PGI, CAPS, and Cray, so it is not
    part of the free CUDA SDK product suite.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: • To achieve a comparable or better level of performance to OpenMP with nontrivial
    programs, the user must additionally specify various simple data clauses to minimize
    PCI-E-based transfers.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: • Is targeted at single-CPU/single-GPU solutions. Does not autoscale when additional
    GPUs are added. Multiple GPU usage requires the use of multiple CPU threads/processes.
    This may change in the future.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: • New features of the CUDA toolkit or the hardware may require explicit support
    from the compiler vendor. Currently OpenACC compiler support can take several
    months to switch over to a CUDA SDK release or to support a new hardware release.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: The main issue with regard to OpenACC versus OpenMP is that OpenMP has no concept
    of various levels of memory or various locations of memory because these concepts
    do not exist in the traditional CPU programming models. In OpenMP data is either
    thread private or global (shared).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast the GPU system is much more complex. You have:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: • Host memory
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: • GPU global memory
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: • GPU constant memory
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: • GPU block private memory (shared memory in CUDA)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: • GPU thread private memory (local memory in CUDA)
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC model, for simplicity, works on the basis that the data resides
    on the host and is shipped to the accelerator memory space at the start of the
    parallel region and shipped back at the end of the parallel region. Thus, every
    parallel region is, by default, bounded by these implicit memory copies over the
    PCI-E bus.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Although a simplistic way to think of this, conceptually it’s an easy way to
    ensure correctness at the potential expense of performance. If you had only one
    calculation and would not reuse the data, then this is effectively what you’d
    do in CUDA anyway. If, however, you plan to make a number of transformations on
    the data, then you need to explicitly specify what data is to remain on the device
    by adding data qualifiers to the directives.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s look at a simple program to give some idea of how it might be converted
    to OpenMP/OpenACC. If we take the classic reduction, you typically see the following:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '`}`'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: As you can see all we do is replace the OpenMP directive with an OpenACC directive.
    We then compile with the vendor-supplied OpenACC compiler. This may generate anything
    from high-level CUDA code to raw PTX code. It will then usually invoke the NVCC
    compiler to generate the target GPU code. Some vendors support additional targets
    other than simply NVIDIA GPUs.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: During the compilation stage most vendors’ compilers provide statistics about
    how they are transforming the serial code to device code. However, this is a little
    like the `-v` option in NCC, in that you need to be able to understand what the
    compiler is telling you. We look here at an example of the PGI compiler output.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: To understand this output, you need to understand how the OpenACC terminology
    maps onto CUDA terminology ([Table 10.6](#T0035)).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.6 OpenACC and CUDA Terminology
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '| OpenACC | CUDA |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| Gangs | Blocks |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| Workers | Warps |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| Vectors | Threads |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: The first line states that the kernel occupied 60 gangs (blocks in CUDA terminology).
    It then states it generated output for “CC 1.3 and CC 2.0,” compute capacity 1.3
    and 2.0 devices, respectively. It also tells you the number of registers used,
    the number of shared memory per block used, the number of bytes of constant memory
    per block, and any registers spilled to local memory.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it calculates the ideal number of threads (OpenACC calls these vectors)
    to achieve near 100% occupancy as possible based on the number of registers and
    shared memory the kernel is using. It may, however, not always select the best
    values for a given kernel/data pattern. Specifying this allows us to override
    or partially override such choices.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: It will look at your data and decide on the best launch parameters (number of
    threads, number of blocks, number of grids, etc.). It will also automatically
    try to allocate data to constant and/or global memory. You are free to override
    these selections if you wish.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'To override the default behavior of mirroring global data on the host (automatic
    background update commands), you need to specify how the data must be managed.
    This can be done as follows:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'where `<directives>` can be one of the following plus some additional more
    complex ones not shown here:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '**copy** (data1, data2, …)—Maintain an identical CPU version by copying in
    at the start of the kernel and out at the end (the default behavior).'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '**copyin** (data1, data2, …)—Only copy data to the GPU and do not copy it back,
    that is, discard the GPU data. This is useful for read-only data the GPU will
    process.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '**copyout** (data1, data2, …)—Only copy data from the GPU back to the CPU.
    Useful for declaring output data on the GPU.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '**create** (data1, data2, …)—Allocates temporary storage on the GPU with no
    copy operation in either direction.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '**present** (data1, data2, …)—Data is already present on the GPU so does not
    need to be copied or allocated anew.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the OpenACC model expects you to use the C99 standard and in particular
    the `__restrict__` keyword in C to specify that any pointers used do not alias
    with one another. Failure to do this will likely result in your code failing to
    vectorize.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: You can tell if adding data directives helps (it almost always will) by using
    the `PGI_ACC_TIME=1` (vendor-specific) option. This, in the case of the PGI compiler,
    will enable profiling. It will then tell you how often the kernel was called,
    the block dimensions of the kernel and how long it took, and finally how much
    time was spent transferring data. It’s this later part that is often the most
    critical and where the data clauses help out. You can also use the standard profiling
    tools available in Linux, such as the Visual Profiler, to see into what the OpenACC
    compiler is doing in reality. In doing so you may spot issues you would otherwise
    be unaware of.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'In being able to see the block size chosen you can also then perform certain
    optimizations to it. For example, you can specify less blocks and threads than
    you have data elements. By default, OpenACC compilers tend to select one thread
    per element, although there is nothing in the standard to say they have to. Thus,
    if you’d like to process four elements per thread, something we have seen tends
    to work well, you can do it by specifying a smaller number of blocks and threads:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Here we’ve specified the loop should use 64 blocks (gangs) of 128 threads (vectors)
    each. Thus, we have 8192 active threads on the device. Assuming a 16 SM device
    such as the GTX580, this would be four blocks per SM, each of 128 threads. This
    equates to 16 warps per SM, which is too few for ideal occupancy on the GTX580\.
    To solve the issue, we’d need to increase the block (gang) or thread (vector)
    count.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the particular algorithm, you may wish to process more than one
    element per thread, rather than increase the block or thread count. As long as
    the number of elements is known to the compiler, as in the previous example, it
    will process multiple elements per thread, in this case four.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Remember also, as with regular CUDA, threads in reality run as warps, groups
    of 32 threads. Allocating 33 threads allocates 64 threads in the hardware, 31
    of which do nothing but consume space resources on the device. Always allocate
    thread blocks (vectors in OpenACC) in blocks of 32.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Also as with CUDA, if you specify gangs or vectors (blocks or threads), which
    you don’t have to, then the usual kernel launch rules apply. Thus, there is a
    limit on the number of threads a block can support, which will change depending
    on the compute level of the hardware you are targeting. Generally, you’ll find
    64, 128, 192, and 256 vales work well with compute 1.x devices. Values of 128,
    192, 256, 384, and 512 work well with compute 2.x devices. The 256 value is usually
    the best for the compute 3.x platform.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: However, when considering adding any specifiers here, consider the likely impact
    of future hardware and how this might limit the use of other accelerator targets.
    By specifying nothing you are letting the compiler select what it thinks is the
    best value. When a new GPU comes out with more threads per block and more blocks
    per SM, once the vendors update the compiler to accommodate it, all works. If
    you do specify these parameters, you should be specifying some multiple of the
    current maximum to allow for your code to run on future devices without running
    out of blocks.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: By default the OpenACC model uses synchronous kernel calls. That is, the host
    processor will wait for the GPU to complete and then continue execution once the
    GPU kernel call returns. This is akin to making a function call in C as opposed
    to spawning a worker thread and later converging.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware by now that this approach, although nice to develop the
    initial application on, should be replaced with an asynchronous model as soon
    as the application is running well. You probably have a reasonable multicore CPU
    in the machine and could make good use of it while the GPU is off calculating
    something. On the top of the list of things to allocate to the CPU should be those
    operations requiring few compute actions compared to loads and stores to and from
    memory.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons why we see the reduction operation perform better, or at
    least the at same speed as the GPU, is the amount of work done per memory read/write.
    To calculate data on the GPU we need to either generate it there or send it over
    the PCI-E bus. If you are shipping two data items over the bus just to perform
    a simple operation such as addition, forget it and do it on the CPU instead. The
    cost of the PCI-E transfer greatly outweighs any other consideration in such a
    scenario. The best candidates for the GPU are those computationally intensive
    areas, or where the additional memory bandwidth on the GPU can make a difference.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Thus, OpenACC provides the `async` clause for kernels and data to allow them
    to run asynchronously to the host and perform asynchronous transfers with the
    host.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Asynchronous transfers require the use of pinned memory, that is, memory that
    cannot be swapped to disk. You do not need to explicitly care about this in OpenACC
    as you do with CUDA. Specifying the `async` clause will cause the OpenACC compiler
    to use pinned memory under the hood for transfers. Of course, one thing to remember
    when using an asynchronous operation is that you cannot change the data that is
    being transferred or operated on by the kernel until the asynchronous operation
    has completed.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'Once people have mastered asynchronous communication and achieved the best
    performance they are able to on a single-core/GPU pair, the obvious question is:
    Can I speed up my application by using multiple GPUs? The answer is of course
    yes, and very often you’ll see near linear scaling if you can stay within a single
    node.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC standard supports only a “one CPU thread per GPU” view of multiple
    GPUs on a single node. If you plan on performing some work on the CPU, this makes
    perfect sense, as it allows you to exploit the full potential of a multicore CPU.
    Thus, with OpenMP you simply launch a number of threads using the OpenMP directive
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Assuming you have a quad-core CPU and four GPU cards attached, then you would
    specify to OpenACC that you wish the current thread to use a given GPU.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'If you have only two GPUs in the system then you might be better off specifying
    two threads for OpenMP. If you wished to make use of four threads, but only have
    two for GPU usage, you could do the following:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We can do the same in MPI by using
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '`}`'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: One issue to be careful of here is that the `acc_set_device_num` API call is
    a one-time event only per host thread. This is very much the way the `cudaSetDevice`
    call used to work prior to the CUDA 4.x SDK. You cannot select a context from
    a single host thread and thus control multiple GPUs from that single thread. The
    only model supported is one where there is a single host thread per GPU context.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Note that a dedicated 1:1 ratio of CPU cores to GPUs is the ideal for heavily
    used systems. However, oversubscribing GPUs to CPU cores can be useful, as rarely
    will GPU programs actually saturate the GPU. Thus, there may be points where the
    GPUs are underutilized, typically at synchronization points or between kernel
    invocations. In cases where you have a master/worker arrangement, which is typical
    in MPI, it can be beneficial to dedicate a non-GPU, CPU core to be the master.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: One aspect I should touch on here is memory patterns. OpenACC, when implemented
    on an accelerator that does coalescing of global memory, will be just as badly
    affected by a poor memory layout as a CUDA program will. There is no automatic
    transpose. You need to think about your memory layout and create one that is optimal
    for a GPU (data in columns of 32 elements, rather than sequential rows).
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Overall OpenACC represents a very interesting development in GPU programming
    and potentially opens up the GPU programming arena to many non-GPU programmers.
    Many of these people will progress to use learn CUDA, as it’s perfectly possible
    to mix OpenACC and CUDA. Thus, you can start with OpenACC, and if you find specific
    areas where you need that extra control, switch over to CUDA, while leaving most
    of the application untouched.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Writing Your Own Kernels
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve presented a number of other options in this chapter that range from specifying
    the parallelism at a high level and having the compiler do the heavy lifting,
    to using libraries developed by those far better at exploiting the hardware than
    you are. You will never, and in fact probably should not try to, be the best in
    everything. Tools such as compiler directives and libraries allow you to leverage
    the effort of others to achieve your goals. Your knowledge resides primarily within
    your own field of interest.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: As a professional developer, or even as a student, you should be conscious of
    the time you take to develop a solution. It may be technically challenging to
    develop the most efficient parallel quick sort, but probably some bright computer
    science graduate has already written a paper on it. If you are hiring, then the
    obvious thing to do is bring this person on board. Buying in knowledge, in terms
    of people or software, is something that can give you a huge head start on whomever
    your competition may be.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: It also makes a lot of sense to select libraries where they cover something
    that is not your area of expertise. If you are developing an image blur algorithm,
    for example, loading/saving the images from the disk is not really what you are
    interested in. There are a number of open-source, or commercial, libraries that
    may cover this aspect of your development.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: One common problem you may encounter using libraries is memory allocation. Most
    CPU-based solutions, if they allocate memory, do not allocate pinned memory. Thus,
    an image library that returns a pointer to the loaded image will cause a slowdown
    in your application when you transfer that image data to the GPU. Therefore, look
    for libraries that allow the user to control the memory management, or are GPU
    aware and support pinned memory.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: The next issue we hit with the directive and library approach is they are, generally,
    not multi-GPU aware unless written as such. As you can usually get up to four
    GPU cards into a workstation, this approach is a bit like using only one of the
    cores in a standard quad-core CPU. The programming required to support multi-GPU
    configurations is not trivial, but neither is it rocket science. The libraries
    we use internally at CudaDeveloper support multiple GPU setups. It complicates
    the handling of the data and requires a lot more thought, but is certainly doable.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: The issue of how much you need to write yourself often is a question of performance.
    In using directives you trade a certain percentage of performance for quicker
    program development. Libraries, by comparison, may bring a significant speedup
    along with a reduction in development effort, but at the potential cost of flexibility
    and license issues. Many are restricted in terms of commercial usage, which simply
    reflects that if you intend to avoid your own development costs by using libraries,
    you should be prepared to pay for that privilege. For academic usage, simply acknowledging
    the contribution is usually sufficient.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Thus, there are a number of reasons why you might chose to develop your own
    kernels in CUDA. This text provides good insight to the issues of developing kernels
    using CUDA. The basic principles (coalesced memory access, fully utilizing the
    hardware, avoiding contention of resources, understanding the hardware limitations,
    data locality) apply regardless of whether you write the kernels yourself or abstract
    them to someone else’s problem.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered in this section some of the NVIDIA-provided libraries. If you
    are working in a field that these cover, why would you not chose to use such libraries?
    They are developed by the manufacturer to run well on their hardware. They are
    designed to be used as the basic building blocks of more complex algorithms. NVIDIA’s
    licensing terms are very generous in that they want people to use the libraries
    and to build CUDA applications. This is hardly surprising when you consider wider
    acceptance of CUDA means more GPUs get sold, and of course the more valuable your
    knowledge of CUDA becomes.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: The question is really does this bring you sufficient level of performance?
    Most people program in a high-level language because it’s much more productive
    than something like Assembler. The better programmers out there understand both
    C and Assembler in great detail. They know when they should use C for the productivity
    gains and know when a small number of functions need to be hand-coded in Assembler.
    The question of using libraries/directives is largely a similar one. You could
    write everything yourself, but unless you have to, why make your life so hard?
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: When developing applications for GPUs, a good approach is to first get a prototype
    working on the CPU side. Consider how you’d like to make that CPU version multicore
    aware and if it would benefit the application. What will be the CPU/GPU work balance?
    How will you create threads on the CPU side if you need them? However, at least
    initially, stick with a single CPU thread and a single GPU, but think at the start
    about what you want to achieve in the end.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Now think about the host/device transfers. The transfer-compute-transfer model
    will usually (depending on the ratios) underutilize the GPU. To some extent we
    can overlap transfer/compute depending on the hardware you have to support.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Next think about the memory hierarchy within the GPU. What locality (registers,
    shared, cache, constant, texture) are you going to exploit on the GPU? What data
    layout do you need for these various types of data stores?
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Now think about the kernel design. The decomposition into threads and blocks
    has implications in terms of the amount of interthread/interblock communication
    and resource usage. What serialization or contention issues are you likely to
    have?
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a working CPU/GPU application, profile it and get it working as
    efficiently as you can. At this stage keep a very close eye on correctness, preferably
    through some back-to-back automated tests.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: This brings us then to the issue of efficiency of the kernel implementation
    and where you need to consider the CUDA/libraries/directives choice. Given the
    plan of how you’d like to use the GPU, how does your choice here affect your ability
    to do that? Is your choice to use our CUDA/libraries/directives positively or
    negatively impacting performance, and by what percentage?
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Consider shared memory as an example. OpenACC has a `cache` qualifier that instructs
    the compiler to place and hold this data in shared memory, a resource it may otherwise
    ignore or use depending on the compiler vendor. Libraries rarely expose shared
    memory, but often use it very efficiently internally and will usually document
    this fact. Newer hardware may have different implementations. For example, Kepler
    can configure shared memory as 32- or 64-bit wide, meaning many financial and
    other applications could benefit significantly from this optimization.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Can you make use of such significant optimizations? If you are reliant on the
    directive vendor or library developer to do this, what level of support will they
    provide and how long might this take? If the library was written by a student
    as part of his or her thesis work, unless you or someone else is willing to maintain
    it or you pay someone to do so, it won’t get updated. If you require a feature
    the directive vendor doesn’t think there is a widespread need for, it’s unlikely
    they will develop it just for your application.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: When you have an efficient single-CPU/single-GPU implementation, move it to
    a multicore/multi-GPU solution as appropriate for your workload. For GPU-dominated
    workflow where the CPU is underutilized, the simple single-CPU core controls and
    all-GPU asynchronous model works fine. Where the CPU core is also loaded, how
    might using multiple threads and thus one GPU per thread help? With the underutilized
    CPU load case, is there not something useful the multicore CPU can be doing? Optimal
    design is about using the resources you have most effectively to solve a given
    problem.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Moving to a multithread/multi-GPU approach may be a painless or very painful
    experience. Your GPU global memory data is now split over multiple GPUs’ memory
    spaces. What inter-GPU communication is now needed? The P2P model, if supported,
    is usually the best method for such communication. Alternatively, the coordination
    or transfers need to be done by the host. Having a single CPU coordinate *N* GPUs
    may be simpler that having multiple CPU threads coordinate those same GPUs.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: How well do your directives or libraries support a multi-GPU approach? Are they
    thread safe, or do they maintain an internal state assuming there will only be
    one instance or CPU thread? What support is there for exchanging data and concurrent
    operations? Are you forced to serially send or receive data to or from each GPU
    in turn, or can you perform *N* simultaneous transfers?
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: When selecting tools or libraries, consider how mature they are and for what
    purpose they were written. How do you debug the code when it goes wrong, as it
    inevitably will? Are you left on your own to figure out the issue or is there
    support provided for bug fixes, feature requests, etc.? When were they written
    and for which GPU do they work best? Are they optimized for, or aware of, different
    GPU generations?
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: By thinking about your design in advance and realizing where you’d like to end
    up, you can decide what sort of software/tools you will need at the outset. You
    may be able to prototype a solution with one approach, but may ultimately have
    to use CUDA to get the performance and efficiency you’d like. There is no mystical
    “silver bullet” in software development. You have to think about the design, plan
    how you will achieve it, and understand how far certain approaches can take you.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have looked at a number of approaches to development of code for the GPU
    in this chapter. What appeals to you will largely depend on your background and
    how comfortable and experienced you currently are with CUDA. I specifically encourage
    you to look at the NVIDA-provided libraries as they provide very large coverage
    of many common problems.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: We have looked at a number of the nondomain-specific examples in the SDK, specifically
    because everyone can follow and benefit from looking at these. There are many
    domain-specific examples in the SDK. I encourage you to explore these as, with
    now a good understanding of CUDA, you will be able to get a lot more out of looking
    at these examples.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have seen from this chapter that writing everything yourself in CUDA
    is not the *only* option. Significant productivity gains can be made by the use
    of libraries. Directives also allow a much higher level of programming that many
    people may prefer to the more low-level CUDA approach. People make different choices
    for various reasons. Understand what the key criteria are for you, and select
    accordingly.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
