<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introduction to GPU programming models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Introduction to GPU programming models</h1>
<blockquote>原文：<a href="https://enccs.github.io/gpu-programming/5-intro-to-gpu-prog-models/">https://enccs.github.io/gpu-programming/5-intro-to-gpu-prog-models/</a></blockquote><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"/>
          <a href="../">GPU programming: why, when and how?</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"/></li>
      <li class="breadcrumb-item active">Introduction to GPU programming models</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/gpu-programming/blob/main/content/5-intro-to-gpu-prog-models.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction-to-gpu-programming-models">
<span id="intro-to-gpu-prog-models"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What are the key differences between different GPU programming approaches?</p></li>
<li><p>How should I choose which framework to use for my project?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand the  basic ideas in different GPU programming frameworks</p></li>
<li><p>Perform a quick cost-benefit analysis in the context of own code projects</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>20 min teaching</p></li>
<li><p>10 min discussion</p></li>
</ul>
</div>
<p>There are different ways to use GPUs for computations. In the best case, when the code has already been written, one only needs to set the parameters and initial configuration in order to get started. In some other cases the problem is posed in such a way that a third-party library can be used to solve the most intensive part of the code (for example, this is increasingly the case with machine-learning workflows in Python).
However, these cases are stil quite limited; in general, some additional programming might be needed. There are many GPU programming software environments and APIs available, which can be broadly grouped into <strong>directive-based models</strong>, <strong>non-portable kernel-based models</strong>, and <strong>portable kernel-based models</strong>, as well as high-level frameworks and libraries (including attempts at language-level support).</p>
<section id="standard-c-fortran">
<h2>Standard C++/Fortran</h2>
<p>Programs written in standard C++ and Fortran languages can now take advantage of NVIDIA GPUs without depending on any external library. This is possible thanks to the <a class="reference external" href="https://developer.nvidia.com/hpc-sdk">NVIDIA SDK</a> suite of compilers that translates and optimizes the code for running on GPUs.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/">Here</a> is the series of articles on acceleration with standard language parallelism.</p></li>
<li><p>Guidelines for writing C++ code can be found <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/">here</a>,</p></li>
<li><p>while those for Fortran code can be found <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/">here</a>.</p></li>
</ul>
<p>The performance of these two approaches is promising, as can be seen in the examples provided in those guidelines.</p>
</section>
<section id="directive-based-programming">
<h2>Directive-based programming</h2>
<p>A fast and cheap way is to use <strong>directive based</strong> approaches. In this case the existing <em>serial</em> code is annotated with <em>hints</em> which indicate to the compiler which loops and regions should be executed on the GPU. In the absence of the API the directives are treated as comments and the code will just be executed as a usual serial code. This approach is focused on productivity and easy usage (but to the possible detriment of performance), and allows employing accelerators with minimal programming effort by adding parallelism to existing code without the need to write accelerator-specific code. There are two common ways to program using directives, namely <strong>OpenACC</strong> and <strong>OpenMP</strong>.</p>
<section id="openacc">
<h3>OpenACC</h3>
<p><a class="reference external" href="https://www.openacc.org/">OpenACC</a> is developed by a consortium formed in 2010 with the goal of developing a standard, portable, and scalable programming model for accelerators, including GPUs. Members of the OpenACC consortium include GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers, universities, and software companies. Until recently it was supporting only NVIDIA GPUs, but now there is effort to support more devices and architectures.</p>
</section>
<section id="openmp">
<h3>OpenMP</h3>
<p><a class="reference external" href="https://www.openmp.org/">OpenMP</a> started as a multi-platform, shared-memory parallel programming API for multi-core CPUs and relatively recently has added support for GPU offloading. OpenMP aims to support various types of GPUs, which is done through the parent compiler.</p>
<p>The directive based approaches work with C/C++ and FORTRAN codes, while some third party extensions are available for other languages.</p>
</section>
</section>
<section id="non-portable-kernel-based-models-native-programming-models">
<h2>Non-portable kernel-based models (native programming models)</h2>
<p>When doing direct GPU programming the developer has a large level of control by writing low-level code that directly communicates with the GPU and its hardware. Theoretically direct GPU programming methods provide the ability to write low-level, GPU-accelerated code that can provide significant performance improvements over CPU-only code. However, they also require a deeper understanding of the GPU architecture and its capabilities, as well as the specific programming method being used.</p>
<section id="cuda">
<h3>CUDA</h3>
<p><a class="reference external" href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> is a parallel computing platform and API developed by NVIDIA. It is historically the first mainstream GPU programming framework. It allows developers to write C-like code that is executed on the GPU. CUDA provides a set of libraries and tools for low-level GPU programming and provides a performance boost for demanding computationally-intensive applications. While there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.</p>
</section>
<section id="hip">
<h3>HIP</h3>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html">HIP</a> (Heterogeneous Interface for Portability) is an API developed by AMD that provides a low-level interface for GPU programming. HIP is designed to provide a single source code that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming model and provides an almost identical programming interface to CUDA.</p>
<p>Multiple examples of CUDA/HIP code are available in the <a class="reference external" href="https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip">content/examples/cuda-hip</a> directory of this repository.</p>
</section>
</section>
<section id="portable-kernel-based-models-cross-platform-portability-ecosystems">
<h2>Portable kernel-based models (cross-platform portability ecosystems)</h2>
<p>Cross-platform portability ecosystems typically provide a higher-level abstraction layer which enables a convenient and portable programming model for GPU programming. They can help reduce the time and effort required to maintain and deploy GPU-accelerated applications. The goal of these ecosystems is to achieve performance portability with a single-source application. In C++, the most notable cross-platform portability ecosystems are <a class="reference external" href="https://www.khronos.org/sycl/">SYCL</a>, <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> (C and C++ APIs), and <a class="reference external" href="https://github.com/kokkos/kokkos">Kokkos</a>; others include <a class="reference external" href="https://alpaka.readthedocs.io/">alpaka</a> and <a class="reference external" href="https://github.com/LLNL/RAJA">RAJA</a>.</p>
<section id="id5">
<h3>OpenCL</h3>
<p><a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> (Open Computing Language) is a cross-platform, open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs. It supports a wide range of hardware from multiple vendors. OpenCL provides a low-level programming interface for GPU programming and enables developers to write programs that can be executed on a variety of platforms. Unlike programming models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model. Recent versions of the OpenCL standard added C++ support for both API and the kernel code, but the C-based interface is still more widely used.
The OpenCL Working Group doesn’t provide any frameworks of its own. Instead, vendors who produce OpenCL-compliant devices release frameworks as part of their software development kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD. In both cases, the development kits are free and contain the libraries and tools that make it possible to build OpenCL applications.</p>
</section>
<section id="id7">
<h3>Kokkos</h3>
<p><a class="reference external" href="https://github.com/kokkos/kokkos">Kokkos</a> is an open-source performance portable programming model for heterogeneous parallel computing that has been mainly developed at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming model for developing efficient and scalable parallel applications that run on many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists of several components, such as the Kokkos core library, which provides parallel execution and memory abstraction, the Kokkos kernel library, which provides math kernels for linear algebra and graph algorithms, and the Kokkos tools library, which provides profiling and debugging tools. Kokkos components integrate well with other software libraries and technologies, such as MPI and OpenMP. Furthermore, the project collaborates with other projects, in order to provide interoperability and standardization for portable C++ programming.</p>
</section>
<section id="id9">
<h3>alpaka</h3>
<p><a class="reference external" href="https://alpaka.readthedocs.io/">alpaka</a> (Abstraction Library for Parallel Kernel Acceleration) is an open-source C++ header-only library that aims to provide performance portability across heterogeneous accelerator architectures by abstracting the underlying levels of parallelism. The library is platform-independent and supports the concurrent and cooperative use of multiple devices, including host CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).</p>
<p>A key advantage of alpaka is that it requires only a single implementation of a user kernel, expressed as a function object with a standardized interface. This eliminates the need to write specialized code for different backends. The library provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP, and serial execution, that can be selected based on the target device. Moreover, multiple accelerator backends can even be combined to target different vendor hardware within a single application.</p>
</section>
<section id="id11">
<h3>SYCL</h3>
<p><a class="reference external" href="https://www.khronos.org/sycl/">SYCL</a> is a royalty-free, open-standard C++ programming model for multi-device programming. It provides a high-level, single-source programming model for heterogeneous systems, including GPUs. Originally SYCL was developed on top of OpenCL; however, it is no more limited to just that. It can be implemented on top of other low-level heterogeneous computing APIs, such as CUDA or HIP, enabling developers to write programs that can be executed on a variety of platforms. Note that while SYCL is relatively high-level model, the developers are still required to write GPU kernels explicitly.</p>
<p>While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only a standard, for which several implementations exist. For GPU programming, <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html">Intel oneAPI DPC++</a> (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with <a class="reference external" href="https://codeplay.com/solutions/oneapi/">Codeplay oneAPI plugins</a>) and <a class="reference external" href="https://github.com/AdaptiveCpp/AdaptiveCpp/">AdaptiveCpp</a> (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs, with experimental Intel GPU support available in combination with Intel oneAPI DPC++) are the most widely used. Other implementations of note are <a class="reference external" href="https://github.com/triSYCL/triSYCL">triSYCL</a> and <a class="reference external" href="https://developer.codeplay.com/products/computecpp/ce/home/">ComputeCPP</a>.</p>
</section>
</section>
<section id="high-level-language-support">
<h2>High-level language support</h2>
<section id="python">
<h3>Python</h3>
<p>Python offers support for GPU programming through multiple abstraction levels.</p>
<p><strong>CUDA Python, HIP Python and PyCUDA</strong></p>
<p>These projects are, respectively, <a class="reference external" href="https://developer.nvidia.com/cuda-python">NVIDIA-</a>, <a class="reference external" href="https://rocm.docs.amd.com/projects/hip-python/en/latest/">AMD-</a>
and <a class="reference external" href="https://documen.tician.de/pycuda/">community-supported</a> wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use these approaches directly, in most cases knowledge of CUDA or HIP programming is needed.</p>
<p>CUDA Python also aims to support higher-level toolkits and libraries, such as <strong>CuPy</strong> and <strong>Numba</strong>.</p>
<p><strong>CuPy</strong></p>
<p><a class="reference external" href="https://cupy.dev/">CuPy</a> is a GPU-based data array library compatible with NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it easy for developers to transition to GPU computing. Code written with NumPy can often be adapted to use CuPy with minimal modifications; in most straightforward cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’ in their Python code.</p>
<p><strong>Numba</strong></p>
<p><a class="reference external" href="https://numba.pydata.org/">Numba</a> is an open-source JIT compiler that translates a subset of Python and NumPy code into optimized machine code. Numba supports CUDA-capable GPUs and is able to generate code for them using several different syntax variants.
In 2021, upstream support for <a class="reference external" href="https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021">AMD (ROCm) support</a> was discontinued.
However, as of 2025, AMD has added downstream support for the Numba API through the
<a class="reference external" href="https://github.com/ROCm/numba-hip">Numba HIP package</a>.</p>
</section>
<section id="julia">
<h3>Julia</h3>
<p>Julia has first-class support for GPU programming through the following packages that target GPUs from all three major vendors:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cuda.juliagpu.org/stable/">CUDA.jl</a> for NVIDIA GPUs</p></li>
<li><p><a class="reference external" href="https://amdgpu.juliagpu.org/stable/">AMDGPU.jl</a> for AMD GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/oneAPI.jl">oneAPI.jl</a> for Intel GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/Metal.jl">Metal.jl</a> for Apple M-series GPUs</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">CUDA.jl</span></code> is the most mature, <code class="docutils literal notranslate"><span class="pre">AMDGPU.jl</span></code> is somewhat behind but still ready for general use, while <code class="docutils literal notranslate"><span class="pre">oneAPI.jl</span></code> and <code class="docutils literal notranslate"><span class="pre">Metal.jl</span></code> are functional but might contain bugs, miss some features and provide suboptimal performance. Their respective APIs are however completely analogous and translation between libraries is straightforward.</p>
<p>All packages offer both high-level abstractions that require very little programming effort and a lower level approach for writing kernels for fine-grained control.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p><strong>Directive-based programming:</strong></p>
<ul>
<li><p>Existing serial code is annotated with directives to indicate which parts should be executed on the GPU.</p></li>
<li><p>OpenACC and OpenMP are common directive-based programming models.</p></li>
<li><p>Productivity and easy usage are prioritized over performance.</p></li>
<li><p>Minimum programming effort is required to add parallelism to existing code.</p></li>
</ul>
</li>
<li><p><strong>Non-portable kernel-based models:</strong></p>
<ul>
<li><p>Low-level code is written to directly communicate with the GPU.</p></li>
<li><p>CUDA is NVIDIA’s parallel computing platform and API for GPU programming.</p></li>
<li><p>HIP is an API developed by AMD that provides a similar programming interface to CUDA for both NVIDIA and AMD GPUs.</p></li>
<li><p>Deeper understanding of GPU architecture and programming methods is needed.</p></li>
</ul>
</li>
<li><p><strong>Portable kernel-based models:</strong></p>
<ul>
<li><p>Higher-level abstractions for GPU programming that provide portability.</p></li>
<li><p>Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.</p></li>
<li><p>Aim to achieve performance portability with a single-source application.</p></li>
<li><p>Can run on various GPUs and platforms, reducing the effort required to maintain and deploy GPU-accelerated applications.</p></li>
</ul>
</li>
<li><p><strong>High-level language support:</strong></p>
<ul>
<li><p>C++ and Fortran feature initiatives to support GPUs through language-standard parallelism.</p></li>
<li><p>Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.</p></li>
<li><p>Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU programming.</p></li>
<li><p>These approaches provide high-level abstraction and interfaces for GPU programming in the respective languages.</p></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="summary">
<h2>Summary</h2>
<p>Each of these GPU programming environments has its own strengths and weaknesses, and the best choice for a given project will depend on a range of factors, including:</p>
<ul class="simple">
<li><p>the hardware platforms being targeted,</p></li>
<li><p>the type of computation being performed, and</p></li>
<li><p>the developer’s experience and preferences.</p></li>
</ul>
<p><strong>High-level and productivity-focused APIs</strong> provide a simplified programming model and maximize code portability, while <strong>low-level and performance-focused APIs</strong> provide a high level of control over the GPU’s hardware but also require more coding effort and expertise.</p>
</section>
<section id="exercises">
<h2>Exercises</h2>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>Which GPU programming frameworks have you already used previously, if any?</p></li>
<li><p>What did you find most challenging? What was most useful?</p></li>
</ul>
<p>Let us know in the main room or via comments in HackMD document.</p>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPU programming approaches can be split into 1) directive-based, 2) non-portable kernel-based, 3) portable kernel-based, and 4) high-level language support.</p></li>
<li><p>There are multiple frameworks/languages available for each approach, each with pros and cons.</p></li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../4-gpu-concepts/" class="btn btn-neutral float-left" title="GPU programming concepts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"/> Previous</a>
        <a href="../6-directive-based-models/" class="btn btn-neutral float-right" title="Directive-based models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"/></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>© Copyright 2023-2024, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    &#13;

<span id="intro-to-gpu-prog-models"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What are the key differences between different GPU programming approaches?</p></li>
<li><p>How should I choose which framework to use for my project?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand the  basic ideas in different GPU programming frameworks</p></li>
<li><p>Perform a quick cost-benefit analysis in the context of own code projects</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>20 min teaching</p></li>
<li><p>10 min discussion</p></li>
</ul>
</div>
<p>There are different ways to use GPUs for computations. In the best case, when the code has already been written, one only needs to set the parameters and initial configuration in order to get started. In some other cases the problem is posed in such a way that a third-party library can be used to solve the most intensive part of the code (for example, this is increasingly the case with machine-learning workflows in Python).
However, these cases are stil quite limited; in general, some additional programming might be needed. There are many GPU programming software environments and APIs available, which can be broadly grouped into <strong>directive-based models</strong>, <strong>non-portable kernel-based models</strong>, and <strong>portable kernel-based models</strong>, as well as high-level frameworks and libraries (including attempts at language-level support).</p>
<section id="standard-c-fortran">
<h2>Standard C++/Fortran</h2>
<p>Programs written in standard C++ and Fortran languages can now take advantage of NVIDIA GPUs without depending on any external library. This is possible thanks to the <a class="reference external" href="https://developer.nvidia.com/hpc-sdk">NVIDIA SDK</a> suite of compilers that translates and optimizes the code for running on GPUs.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/">Here</a> is the series of articles on acceleration with standard language parallelism.</p></li>
<li><p>Guidelines for writing C++ code can be found <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/">here</a>,</p></li>
<li><p>while those for Fortran code can be found <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/">here</a>.</p></li>
</ul>
<p>The performance of these two approaches is promising, as can be seen in the examples provided in those guidelines.</p>
</section>
<section id="directive-based-programming">
<h2>Directive-based programming</h2>
<p>A fast and cheap way is to use <strong>directive based</strong> approaches. In this case the existing <em>serial</em> code is annotated with <em>hints</em> which indicate to the compiler which loops and regions should be executed on the GPU. In the absence of the API the directives are treated as comments and the code will just be executed as a usual serial code. This approach is focused on productivity and easy usage (but to the possible detriment of performance), and allows employing accelerators with minimal programming effort by adding parallelism to existing code without the need to write accelerator-specific code. There are two common ways to program using directives, namely <strong>OpenACC</strong> and <strong>OpenMP</strong>.</p>
<section id="openacc">
<h3>OpenACC</h3>
<p><a class="reference external" href="https://www.openacc.org/">OpenACC</a> is developed by a consortium formed in 2010 with the goal of developing a standard, portable, and scalable programming model for accelerators, including GPUs. Members of the OpenACC consortium include GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers, universities, and software companies. Until recently it was supporting only NVIDIA GPUs, but now there is effort to support more devices and architectures.</p>
</section>
<section id="openmp">
<h3>OpenMP</h3>
<p><a class="reference external" href="https://www.openmp.org/">OpenMP</a> started as a multi-platform, shared-memory parallel programming API for multi-core CPUs and relatively recently has added support for GPU offloading. OpenMP aims to support various types of GPUs, which is done through the parent compiler.</p>
<p>The directive based approaches work with C/C++ and FORTRAN codes, while some third party extensions are available for other languages.</p>
</section>
</section>
<section id="non-portable-kernel-based-models-native-programming-models">
<h2>Non-portable kernel-based models (native programming models)</h2>
<p>When doing direct GPU programming the developer has a large level of control by writing low-level code that directly communicates with the GPU and its hardware. Theoretically direct GPU programming methods provide the ability to write low-level, GPU-accelerated code that can provide significant performance improvements over CPU-only code. However, they also require a deeper understanding of the GPU architecture and its capabilities, as well as the specific programming method being used.</p>
<section id="cuda">
<h3>CUDA</h3>
<p><a class="reference external" href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> is a parallel computing platform and API developed by NVIDIA. It is historically the first mainstream GPU programming framework. It allows developers to write C-like code that is executed on the GPU. CUDA provides a set of libraries and tools for low-level GPU programming and provides a performance boost for demanding computationally-intensive applications. While there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.</p>
</section>
<section id="hip">
<h3>HIP</h3>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html">HIP</a> (Heterogeneous Interface for Portability) is an API developed by AMD that provides a low-level interface for GPU programming. HIP is designed to provide a single source code that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming model and provides an almost identical programming interface to CUDA.</p>
<p>Multiple examples of CUDA/HIP code are available in the <a class="reference external" href="https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip">content/examples/cuda-hip</a> directory of this repository.</p>
</section>
</section>
<section id="portable-kernel-based-models-cross-platform-portability-ecosystems">
<h2>Portable kernel-based models (cross-platform portability ecosystems)</h2>
<p>Cross-platform portability ecosystems typically provide a higher-level abstraction layer which enables a convenient and portable programming model for GPU programming. They can help reduce the time and effort required to maintain and deploy GPU-accelerated applications. The goal of these ecosystems is to achieve performance portability with a single-source application. In C++, the most notable cross-platform portability ecosystems are <a class="reference external" href="https://www.khronos.org/sycl/">SYCL</a>, <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> (C and C++ APIs), and <a class="reference external" href="https://github.com/kokkos/kokkos">Kokkos</a>; others include <a class="reference external" href="https://alpaka.readthedocs.io/">alpaka</a> and <a class="reference external" href="https://github.com/LLNL/RAJA">RAJA</a>.</p>
<section id="id5">
<h3>OpenCL</h3>
<p><a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> (Open Computing Language) is a cross-platform, open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs. It supports a wide range of hardware from multiple vendors. OpenCL provides a low-level programming interface for GPU programming and enables developers to write programs that can be executed on a variety of platforms. Unlike programming models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model. Recent versions of the OpenCL standard added C++ support for both API and the kernel code, but the C-based interface is still more widely used.
The OpenCL Working Group doesn’t provide any frameworks of its own. Instead, vendors who produce OpenCL-compliant devices release frameworks as part of their software development kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD. In both cases, the development kits are free and contain the libraries and tools that make it possible to build OpenCL applications.</p>
</section>
<section id="id7">
<h3>Kokkos</h3>
<p><a class="reference external" href="https://github.com/kokkos/kokkos">Kokkos</a> is an open-source performance portable programming model for heterogeneous parallel computing that has been mainly developed at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming model for developing efficient and scalable parallel applications that run on many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists of several components, such as the Kokkos core library, which provides parallel execution and memory abstraction, the Kokkos kernel library, which provides math kernels for linear algebra and graph algorithms, and the Kokkos tools library, which provides profiling and debugging tools. Kokkos components integrate well with other software libraries and technologies, such as MPI and OpenMP. Furthermore, the project collaborates with other projects, in order to provide interoperability and standardization for portable C++ programming.</p>
</section>
<section id="id9">
<h3>alpaka</h3>
<p><a class="reference external" href="https://alpaka.readthedocs.io/">alpaka</a> (Abstraction Library for Parallel Kernel Acceleration) is an open-source C++ header-only library that aims to provide performance portability across heterogeneous accelerator architectures by abstracting the underlying levels of parallelism. The library is platform-independent and supports the concurrent and cooperative use of multiple devices, including host CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).</p>
<p>A key advantage of alpaka is that it requires only a single implementation of a user kernel, expressed as a function object with a standardized interface. This eliminates the need to write specialized code for different backends. The library provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP, and serial execution, that can be selected based on the target device. Moreover, multiple accelerator backends can even be combined to target different vendor hardware within a single application.</p>
</section>
<section id="id11">
<h3>SYCL</h3>
<p><a class="reference external" href="https://www.khronos.org/sycl/">SYCL</a> is a royalty-free, open-standard C++ programming model for multi-device programming. It provides a high-level, single-source programming model for heterogeneous systems, including GPUs. Originally SYCL was developed on top of OpenCL; however, it is no more limited to just that. It can be implemented on top of other low-level heterogeneous computing APIs, such as CUDA or HIP, enabling developers to write programs that can be executed on a variety of platforms. Note that while SYCL is relatively high-level model, the developers are still required to write GPU kernels explicitly.</p>
<p>While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only a standard, for which several implementations exist. For GPU programming, <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html">Intel oneAPI DPC++</a> (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with <a class="reference external" href="https://codeplay.com/solutions/oneapi/">Codeplay oneAPI plugins</a>) and <a class="reference external" href="https://github.com/AdaptiveCpp/AdaptiveCpp/">AdaptiveCpp</a> (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs, with experimental Intel GPU support available in combination with Intel oneAPI DPC++) are the most widely used. Other implementations of note are <a class="reference external" href="https://github.com/triSYCL/triSYCL">triSYCL</a> and <a class="reference external" href="https://developer.codeplay.com/products/computecpp/ce/home/">ComputeCPP</a>.</p>
</section>
</section>
<section id="high-level-language-support">
<h2>High-level language support</h2>
<section id="python">
<h3>Python</h3>
<p>Python offers support for GPU programming through multiple abstraction levels.</p>
<p><strong>CUDA Python, HIP Python and PyCUDA</strong></p>
<p>These projects are, respectively, <a class="reference external" href="https://developer.nvidia.com/cuda-python">NVIDIA-</a>, <a class="reference external" href="https://rocm.docs.amd.com/projects/hip-python/en/latest/">AMD-</a>
and <a class="reference external" href="https://documen.tician.de/pycuda/">community-supported</a> wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use these approaches directly, in most cases knowledge of CUDA or HIP programming is needed.</p>
<p>CUDA Python also aims to support higher-level toolkits and libraries, such as <strong>CuPy</strong> and <strong>Numba</strong>.</p>
<p><strong>CuPy</strong></p>
<p><a class="reference external" href="https://cupy.dev/">CuPy</a> is a GPU-based data array library compatible with NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it easy for developers to transition to GPU computing. Code written with NumPy can often be adapted to use CuPy with minimal modifications; in most straightforward cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’ in their Python code.</p>
<p><strong>Numba</strong></p>
<p><a class="reference external" href="https://numba.pydata.org/">Numba</a> is an open-source JIT compiler that translates a subset of Python and NumPy code into optimized machine code. Numba supports CUDA-capable GPUs and is able to generate code for them using several different syntax variants.
In 2021, upstream support for <a class="reference external" href="https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021">AMD (ROCm) support</a> was discontinued.
However, as of 2025, AMD has added downstream support for the Numba API through the
<a class="reference external" href="https://github.com/ROCm/numba-hip">Numba HIP package</a>.</p>
</section>
<section id="julia">
<h3>Julia</h3>
<p>Julia has first-class support for GPU programming through the following packages that target GPUs from all three major vendors:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cuda.juliagpu.org/stable/">CUDA.jl</a> for NVIDIA GPUs</p></li>
<li><p><a class="reference external" href="https://amdgpu.juliagpu.org/stable/">AMDGPU.jl</a> for AMD GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/oneAPI.jl">oneAPI.jl</a> for Intel GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/Metal.jl">Metal.jl</a> for Apple M-series GPUs</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">CUDA.jl</span></code> is the most mature, <code class="docutils literal notranslate"><span class="pre">AMDGPU.jl</span></code> is somewhat behind but still ready for general use, while <code class="docutils literal notranslate"><span class="pre">oneAPI.jl</span></code> and <code class="docutils literal notranslate"><span class="pre">Metal.jl</span></code> are functional but might contain bugs, miss some features and provide suboptimal performance. Their respective APIs are however completely analogous and translation between libraries is straightforward.</p>
<p>All packages offer both high-level abstractions that require very little programming effort and a lower level approach for writing kernels for fine-grained control.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p><strong>Directive-based programming:</strong></p>
<ul>
<li><p>Existing serial code is annotated with directives to indicate which parts should be executed on the GPU.</p></li>
<li><p>OpenACC and OpenMP are common directive-based programming models.</p></li>
<li><p>Productivity and easy usage are prioritized over performance.</p></li>
<li><p>Minimum programming effort is required to add parallelism to existing code.</p></li>
</ul>
</li>
<li><p><strong>Non-portable kernel-based models:</strong></p>
<ul>
<li><p>Low-level code is written to directly communicate with the GPU.</p></li>
<li><p>CUDA is NVIDIA’s parallel computing platform and API for GPU programming.</p></li>
<li><p>HIP is an API developed by AMD that provides a similar programming interface to CUDA for both NVIDIA and AMD GPUs.</p></li>
<li><p>Deeper understanding of GPU architecture and programming methods is needed.</p></li>
</ul>
</li>
<li><p><strong>Portable kernel-based models:</strong></p>
<ul>
<li><p>Higher-level abstractions for GPU programming that provide portability.</p></li>
<li><p>Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.</p></li>
<li><p>Aim to achieve performance portability with a single-source application.</p></li>
<li><p>Can run on various GPUs and platforms, reducing the effort required to maintain and deploy GPU-accelerated applications.</p></li>
</ul>
</li>
<li><p><strong>High-level language support:</strong></p>
<ul>
<li><p>C++ and Fortran feature initiatives to support GPUs through language-standard parallelism.</p></li>
<li><p>Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.</p></li>
<li><p>Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU programming.</p></li>
<li><p>These approaches provide high-level abstraction and interfaces for GPU programming in the respective languages.</p></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="summary">
<h2>Summary</h2>
<p>Each of these GPU programming environments has its own strengths and weaknesses, and the best choice for a given project will depend on a range of factors, including:</p>
<ul class="simple">
<li><p>the hardware platforms being targeted,</p></li>
<li><p>the type of computation being performed, and</p></li>
<li><p>the developer’s experience and preferences.</p></li>
</ul>
<p><strong>High-level and productivity-focused APIs</strong> provide a simplified programming model and maximize code portability, while <strong>low-level and performance-focused APIs</strong> provide a high level of control over the GPU’s hardware but also require more coding effort and expertise.</p>
</section>
<section id="exercises">
<h2>Exercises</h2>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>Which GPU programming frameworks have you already used previously, if any?</p></li>
<li><p>What did you find most challenging? What was most useful?</p></li>
</ul>
<p>Let us know in the main room or via comments in HackMD document.</p>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPU programming approaches can be split into 1) directive-based, 2) non-portable kernel-based, 3) portable kernel-based, and 4) high-level language support.</p></li>
<li><p>There are multiple frameworks/languages available for each approach, each with pros and cons.</p></li>
</ul>
</div>
</section>
&#13;

<h2>Standard C++/Fortran</h2>
<p>Programs written in standard C++ and Fortran languages can now take advantage of NVIDIA GPUs without depending on any external library. This is possible thanks to the <a class="reference external" href="https://developer.nvidia.com/hpc-sdk">NVIDIA SDK</a> suite of compilers that translates and optimizes the code for running on GPUs.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/">Here</a> is the series of articles on acceleration with standard language parallelism.</p></li>
<li><p>Guidelines for writing C++ code can be found <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/">here</a>,</p></li>
<li><p>while those for Fortran code can be found <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/">here</a>.</p></li>
</ul>
<p>The performance of these two approaches is promising, as can be seen in the examples provided in those guidelines.</p>
&#13;

<h2>Directive-based programming</h2>
<p>A fast and cheap way is to use <strong>directive based</strong> approaches. In this case the existing <em>serial</em> code is annotated with <em>hints</em> which indicate to the compiler which loops and regions should be executed on the GPU. In the absence of the API the directives are treated as comments and the code will just be executed as a usual serial code. This approach is focused on productivity and easy usage (but to the possible detriment of performance), and allows employing accelerators with minimal programming effort by adding parallelism to existing code without the need to write accelerator-specific code. There are two common ways to program using directives, namely <strong>OpenACC</strong> and <strong>OpenMP</strong>.</p>
<section id="openacc">
<h3>OpenACC</h3>
<p><a class="reference external" href="https://www.openacc.org/">OpenACC</a> is developed by a consortium formed in 2010 with the goal of developing a standard, portable, and scalable programming model for accelerators, including GPUs. Members of the OpenACC consortium include GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers, universities, and software companies. Until recently it was supporting only NVIDIA GPUs, but now there is effort to support more devices and architectures.</p>
</section>
<section id="openmp">
<h3>OpenMP</h3>
<p><a class="reference external" href="https://www.openmp.org/">OpenMP</a> started as a multi-platform, shared-memory parallel programming API for multi-core CPUs and relatively recently has added support for GPU offloading. OpenMP aims to support various types of GPUs, which is done through the parent compiler.</p>
<p>The directive based approaches work with C/C++ and FORTRAN codes, while some third party extensions are available for other languages.</p>
</section>
&#13;

<h3>OpenACC</h3>
<p><a class="reference external" href="https://www.openacc.org/">OpenACC</a> is developed by a consortium formed in 2010 with the goal of developing a standard, portable, and scalable programming model for accelerators, including GPUs. Members of the OpenACC consortium include GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers, universities, and software companies. Until recently it was supporting only NVIDIA GPUs, but now there is effort to support more devices and architectures.</p>
&#13;

<h3>OpenMP</h3>
<p><a class="reference external" href="https://www.openmp.org/">OpenMP</a> started as a multi-platform, shared-memory parallel programming API for multi-core CPUs and relatively recently has added support for GPU offloading. OpenMP aims to support various types of GPUs, which is done through the parent compiler.</p>
<p>The directive based approaches work with C/C++ and FORTRAN codes, while some third party extensions are available for other languages.</p>
&#13;

<h2>Non-portable kernel-based models (native programming models)</h2>
<p>When doing direct GPU programming the developer has a large level of control by writing low-level code that directly communicates with the GPU and its hardware. Theoretically direct GPU programming methods provide the ability to write low-level, GPU-accelerated code that can provide significant performance improvements over CPU-only code. However, they also require a deeper understanding of the GPU architecture and its capabilities, as well as the specific programming method being used.</p>
<section id="cuda">
<h3>CUDA</h3>
<p><a class="reference external" href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> is a parallel computing platform and API developed by NVIDIA. It is historically the first mainstream GPU programming framework. It allows developers to write C-like code that is executed on the GPU. CUDA provides a set of libraries and tools for low-level GPU programming and provides a performance boost for demanding computationally-intensive applications. While there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.</p>
</section>
<section id="hip">
<h3>HIP</h3>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html">HIP</a> (Heterogeneous Interface for Portability) is an API developed by AMD that provides a low-level interface for GPU programming. HIP is designed to provide a single source code that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming model and provides an almost identical programming interface to CUDA.</p>
<p>Multiple examples of CUDA/HIP code are available in the <a class="reference external" href="https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip">content/examples/cuda-hip</a> directory of this repository.</p>
</section>
&#13;

<h3>CUDA</h3>
<p><a class="reference external" href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> is a parallel computing platform and API developed by NVIDIA. It is historically the first mainstream GPU programming framework. It allows developers to write C-like code that is executed on the GPU. CUDA provides a set of libraries and tools for low-level GPU programming and provides a performance boost for demanding computationally-intensive applications. While there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.</p>
&#13;

<h3>HIP</h3>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html">HIP</a> (Heterogeneous Interface for Portability) is an API developed by AMD that provides a low-level interface for GPU programming. HIP is designed to provide a single source code that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming model and provides an almost identical programming interface to CUDA.</p>
<p>Multiple examples of CUDA/HIP code are available in the <a class="reference external" href="https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip">content/examples/cuda-hip</a> directory of this repository.</p>
&#13;

<h2>Portable kernel-based models (cross-platform portability ecosystems)</h2>
<p>Cross-platform portability ecosystems typically provide a higher-level abstraction layer which enables a convenient and portable programming model for GPU programming. They can help reduce the time and effort required to maintain and deploy GPU-accelerated applications. The goal of these ecosystems is to achieve performance portability with a single-source application. In C++, the most notable cross-platform portability ecosystems are <a class="reference external" href="https://www.khronos.org/sycl/">SYCL</a>, <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> (C and C++ APIs), and <a class="reference external" href="https://github.com/kokkos/kokkos">Kokkos</a>; others include <a class="reference external" href="https://alpaka.readthedocs.io/">alpaka</a> and <a class="reference external" href="https://github.com/LLNL/RAJA">RAJA</a>.</p>
<section id="id5">
<h3>OpenCL</h3>
<p><a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> (Open Computing Language) is a cross-platform, open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs. It supports a wide range of hardware from multiple vendors. OpenCL provides a low-level programming interface for GPU programming and enables developers to write programs that can be executed on a variety of platforms. Unlike programming models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model. Recent versions of the OpenCL standard added C++ support for both API and the kernel code, but the C-based interface is still more widely used.
The OpenCL Working Group doesn’t provide any frameworks of its own. Instead, vendors who produce OpenCL-compliant devices release frameworks as part of their software development kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD. In both cases, the development kits are free and contain the libraries and tools that make it possible to build OpenCL applications.</p>
</section>
<section id="id7">
<h3>Kokkos</h3>
<p><a class="reference external" href="https://github.com/kokkos/kokkos">Kokkos</a> is an open-source performance portable programming model for heterogeneous parallel computing that has been mainly developed at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming model for developing efficient and scalable parallel applications that run on many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists of several components, such as the Kokkos core library, which provides parallel execution and memory abstraction, the Kokkos kernel library, which provides math kernels for linear algebra and graph algorithms, and the Kokkos tools library, which provides profiling and debugging tools. Kokkos components integrate well with other software libraries and technologies, such as MPI and OpenMP. Furthermore, the project collaborates with other projects, in order to provide interoperability and standardization for portable C++ programming.</p>
</section>
<section id="id9">
<h3>alpaka</h3>
<p><a class="reference external" href="https://alpaka.readthedocs.io/">alpaka</a> (Abstraction Library for Parallel Kernel Acceleration) is an open-source C++ header-only library that aims to provide performance portability across heterogeneous accelerator architectures by abstracting the underlying levels of parallelism. The library is platform-independent and supports the concurrent and cooperative use of multiple devices, including host CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).</p>
<p>A key advantage of alpaka is that it requires only a single implementation of a user kernel, expressed as a function object with a standardized interface. This eliminates the need to write specialized code for different backends. The library provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP, and serial execution, that can be selected based on the target device. Moreover, multiple accelerator backends can even be combined to target different vendor hardware within a single application.</p>
</section>
<section id="id11">
<h3>SYCL</h3>
<p><a class="reference external" href="https://www.khronos.org/sycl/">SYCL</a> is a royalty-free, open-standard C++ programming model for multi-device programming. It provides a high-level, single-source programming model for heterogeneous systems, including GPUs. Originally SYCL was developed on top of OpenCL; however, it is no more limited to just that. It can be implemented on top of other low-level heterogeneous computing APIs, such as CUDA or HIP, enabling developers to write programs that can be executed on a variety of platforms. Note that while SYCL is relatively high-level model, the developers are still required to write GPU kernels explicitly.</p>
<p>While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only a standard, for which several implementations exist. For GPU programming, <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html">Intel oneAPI DPC++</a> (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with <a class="reference external" href="https://codeplay.com/solutions/oneapi/">Codeplay oneAPI plugins</a>) and <a class="reference external" href="https://github.com/AdaptiveCpp/AdaptiveCpp/">AdaptiveCpp</a> (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs, with experimental Intel GPU support available in combination with Intel oneAPI DPC++) are the most widely used. Other implementations of note are <a class="reference external" href="https://github.com/triSYCL/triSYCL">triSYCL</a> and <a class="reference external" href="https://developer.codeplay.com/products/computecpp/ce/home/">ComputeCPP</a>.</p>
</section>
&#13;

<h3>OpenCL</h3>
<p><a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> (Open Computing Language) is a cross-platform, open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs. It supports a wide range of hardware from multiple vendors. OpenCL provides a low-level programming interface for GPU programming and enables developers to write programs that can be executed on a variety of platforms. Unlike programming models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model. Recent versions of the OpenCL standard added C++ support for both API and the kernel code, but the C-based interface is still more widely used.
The OpenCL Working Group doesn’t provide any frameworks of its own. Instead, vendors who produce OpenCL-compliant devices release frameworks as part of their software development kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD. In both cases, the development kits are free and contain the libraries and tools that make it possible to build OpenCL applications.</p>
&#13;

<h3>Kokkos</h3>
<p><a class="reference external" href="https://github.com/kokkos/kokkos">Kokkos</a> is an open-source performance portable programming model for heterogeneous parallel computing that has been mainly developed at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming model for developing efficient and scalable parallel applications that run on many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists of several components, such as the Kokkos core library, which provides parallel execution and memory abstraction, the Kokkos kernel library, which provides math kernels for linear algebra and graph algorithms, and the Kokkos tools library, which provides profiling and debugging tools. Kokkos components integrate well with other software libraries and technologies, such as MPI and OpenMP. Furthermore, the project collaborates with other projects, in order to provide interoperability and standardization for portable C++ programming.</p>
&#13;

<h3>alpaka</h3>
<p><a class="reference external" href="https://alpaka.readthedocs.io/">alpaka</a> (Abstraction Library for Parallel Kernel Acceleration) is an open-source C++ header-only library that aims to provide performance portability across heterogeneous accelerator architectures by abstracting the underlying levels of parallelism. The library is platform-independent and supports the concurrent and cooperative use of multiple devices, including host CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).</p>
<p>A key advantage of alpaka is that it requires only a single implementation of a user kernel, expressed as a function object with a standardized interface. This eliminates the need to write specialized code for different backends. The library provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP, and serial execution, that can be selected based on the target device. Moreover, multiple accelerator backends can even be combined to target different vendor hardware within a single application.</p>
&#13;

<h3>SYCL</h3>
<p><a class="reference external" href="https://www.khronos.org/sycl/">SYCL</a> is a royalty-free, open-standard C++ programming model for multi-device programming. It provides a high-level, single-source programming model for heterogeneous systems, including GPUs. Originally SYCL was developed on top of OpenCL; however, it is no more limited to just that. It can be implemented on top of other low-level heterogeneous computing APIs, such as CUDA or HIP, enabling developers to write programs that can be executed on a variety of platforms. Note that while SYCL is relatively high-level model, the developers are still required to write GPU kernels explicitly.</p>
<p>While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only a standard, for which several implementations exist. For GPU programming, <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html">Intel oneAPI DPC++</a> (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with <a class="reference external" href="https://codeplay.com/solutions/oneapi/">Codeplay oneAPI plugins</a>) and <a class="reference external" href="https://github.com/AdaptiveCpp/AdaptiveCpp/">AdaptiveCpp</a> (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs, with experimental Intel GPU support available in combination with Intel oneAPI DPC++) are the most widely used. Other implementations of note are <a class="reference external" href="https://github.com/triSYCL/triSYCL">triSYCL</a> and <a class="reference external" href="https://developer.codeplay.com/products/computecpp/ce/home/">ComputeCPP</a>.</p>
&#13;

<h2>High-level language support</h2>
<section id="python">
<h3>Python</h3>
<p>Python offers support for GPU programming through multiple abstraction levels.</p>
<p><strong>CUDA Python, HIP Python and PyCUDA</strong></p>
<p>These projects are, respectively, <a class="reference external" href="https://developer.nvidia.com/cuda-python">NVIDIA-</a>, <a class="reference external" href="https://rocm.docs.amd.com/projects/hip-python/en/latest/">AMD-</a>
and <a class="reference external" href="https://documen.tician.de/pycuda/">community-supported</a> wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use these approaches directly, in most cases knowledge of CUDA or HIP programming is needed.</p>
<p>CUDA Python also aims to support higher-level toolkits and libraries, such as <strong>CuPy</strong> and <strong>Numba</strong>.</p>
<p><strong>CuPy</strong></p>
<p><a class="reference external" href="https://cupy.dev/">CuPy</a> is a GPU-based data array library compatible with NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it easy for developers to transition to GPU computing. Code written with NumPy can often be adapted to use CuPy with minimal modifications; in most straightforward cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’ in their Python code.</p>
<p><strong>Numba</strong></p>
<p><a class="reference external" href="https://numba.pydata.org/">Numba</a> is an open-source JIT compiler that translates a subset of Python and NumPy code into optimized machine code. Numba supports CUDA-capable GPUs and is able to generate code for them using several different syntax variants.
In 2021, upstream support for <a class="reference external" href="https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021">AMD (ROCm) support</a> was discontinued.
However, as of 2025, AMD has added downstream support for the Numba API through the
<a class="reference external" href="https://github.com/ROCm/numba-hip">Numba HIP package</a>.</p>
</section>
<section id="julia">
<h3>Julia</h3>
<p>Julia has first-class support for GPU programming through the following packages that target GPUs from all three major vendors:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cuda.juliagpu.org/stable/">CUDA.jl</a> for NVIDIA GPUs</p></li>
<li><p><a class="reference external" href="https://amdgpu.juliagpu.org/stable/">AMDGPU.jl</a> for AMD GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/oneAPI.jl">oneAPI.jl</a> for Intel GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/Metal.jl">Metal.jl</a> for Apple M-series GPUs</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">CUDA.jl</span></code> is the most mature, <code class="docutils literal notranslate"><span class="pre">AMDGPU.jl</span></code> is somewhat behind but still ready for general use, while <code class="docutils literal notranslate"><span class="pre">oneAPI.jl</span></code> and <code class="docutils literal notranslate"><span class="pre">Metal.jl</span></code> are functional but might contain bugs, miss some features and provide suboptimal performance. Their respective APIs are however completely analogous and translation between libraries is straightforward.</p>
<p>All packages offer both high-level abstractions that require very little programming effort and a lower level approach for writing kernels for fine-grained control.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p><strong>Directive-based programming:</strong></p>
<ul>
<li><p>Existing serial code is annotated with directives to indicate which parts should be executed on the GPU.</p></li>
<li><p>OpenACC and OpenMP are common directive-based programming models.</p></li>
<li><p>Productivity and easy usage are prioritized over performance.</p></li>
<li><p>Minimum programming effort is required to add parallelism to existing code.</p></li>
</ul>
</li>
<li><p><strong>Non-portable kernel-based models:</strong></p>
<ul>
<li><p>Low-level code is written to directly communicate with the GPU.</p></li>
<li><p>CUDA is NVIDIA’s parallel computing platform and API for GPU programming.</p></li>
<li><p>HIP is an API developed by AMD that provides a similar programming interface to CUDA for both NVIDIA and AMD GPUs.</p></li>
<li><p>Deeper understanding of GPU architecture and programming methods is needed.</p></li>
</ul>
</li>
<li><p><strong>Portable kernel-based models:</strong></p>
<ul>
<li><p>Higher-level abstractions for GPU programming that provide portability.</p></li>
<li><p>Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.</p></li>
<li><p>Aim to achieve performance portability with a single-source application.</p></li>
<li><p>Can run on various GPUs and platforms, reducing the effort required to maintain and deploy GPU-accelerated applications.</p></li>
</ul>
</li>
<li><p><strong>High-level language support:</strong></p>
<ul>
<li><p>C++ and Fortran feature initiatives to support GPUs through language-standard parallelism.</p></li>
<li><p>Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.</p></li>
<li><p>Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU programming.</p></li>
<li><p>These approaches provide high-level abstraction and interfaces for GPU programming in the respective languages.</p></li>
</ul>
</li>
</ul>
</div>
</section>
&#13;

<h3>Python</h3>
<p>Python offers support for GPU programming through multiple abstraction levels.</p>
<p><strong>CUDA Python, HIP Python and PyCUDA</strong></p>
<p>These projects are, respectively, <a class="reference external" href="https://developer.nvidia.com/cuda-python">NVIDIA-</a>, <a class="reference external" href="https://rocm.docs.amd.com/projects/hip-python/en/latest/">AMD-</a>
and <a class="reference external" href="https://documen.tician.de/pycuda/">community-supported</a> wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use these approaches directly, in most cases knowledge of CUDA or HIP programming is needed.</p>
<p>CUDA Python also aims to support higher-level toolkits and libraries, such as <strong>CuPy</strong> and <strong>Numba</strong>.</p>
<p><strong>CuPy</strong></p>
<p><a class="reference external" href="https://cupy.dev/">CuPy</a> is a GPU-based data array library compatible with NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it easy for developers to transition to GPU computing. Code written with NumPy can often be adapted to use CuPy with minimal modifications; in most straightforward cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’ in their Python code.</p>
<p><strong>Numba</strong></p>
<p><a class="reference external" href="https://numba.pydata.org/">Numba</a> is an open-source JIT compiler that translates a subset of Python and NumPy code into optimized machine code. Numba supports CUDA-capable GPUs and is able to generate code for them using several different syntax variants.
In 2021, upstream support for <a class="reference external" href="https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021">AMD (ROCm) support</a> was discontinued.
However, as of 2025, AMD has added downstream support for the Numba API through the
<a class="reference external" href="https://github.com/ROCm/numba-hip">Numba HIP package</a>.</p>
&#13;

<h3>Julia</h3>
<p>Julia has first-class support for GPU programming through the following packages that target GPUs from all three major vendors:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cuda.juliagpu.org/stable/">CUDA.jl</a> for NVIDIA GPUs</p></li>
<li><p><a class="reference external" href="https://amdgpu.juliagpu.org/stable/">AMDGPU.jl</a> for AMD GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/oneAPI.jl">oneAPI.jl</a> for Intel GPUs</p></li>
<li><p><a class="reference external" href="https://github.com/JuliaGPU/Metal.jl">Metal.jl</a> for Apple M-series GPUs</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">CUDA.jl</span></code> is the most mature, <code class="docutils literal notranslate"><span class="pre">AMDGPU.jl</span></code> is somewhat behind but still ready for general use, while <code class="docutils literal notranslate"><span class="pre">oneAPI.jl</span></code> and <code class="docutils literal notranslate"><span class="pre">Metal.jl</span></code> are functional but might contain bugs, miss some features and provide suboptimal performance. Their respective APIs are however completely analogous and translation between libraries is straightforward.</p>
<p>All packages offer both high-level abstractions that require very little programming effort and a lower level approach for writing kernels for fine-grained control.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p><strong>Directive-based programming:</strong></p>
<ul>
<li><p>Existing serial code is annotated with directives to indicate which parts should be executed on the GPU.</p></li>
<li><p>OpenACC and OpenMP are common directive-based programming models.</p></li>
<li><p>Productivity and easy usage are prioritized over performance.</p></li>
<li><p>Minimum programming effort is required to add parallelism to existing code.</p></li>
</ul>
</li>
<li><p><strong>Non-portable kernel-based models:</strong></p>
<ul>
<li><p>Low-level code is written to directly communicate with the GPU.</p></li>
<li><p>CUDA is NVIDIA’s parallel computing platform and API for GPU programming.</p></li>
<li><p>HIP is an API developed by AMD that provides a similar programming interface to CUDA for both NVIDIA and AMD GPUs.</p></li>
<li><p>Deeper understanding of GPU architecture and programming methods is needed.</p></li>
</ul>
</li>
<li><p><strong>Portable kernel-based models:</strong></p>
<ul>
<li><p>Higher-level abstractions for GPU programming that provide portability.</p></li>
<li><p>Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.</p></li>
<li><p>Aim to achieve performance portability with a single-source application.</p></li>
<li><p>Can run on various GPUs and platforms, reducing the effort required to maintain and deploy GPU-accelerated applications.</p></li>
</ul>
</li>
<li><p><strong>High-level language support:</strong></p>
<ul>
<li><p>C++ and Fortran feature initiatives to support GPUs through language-standard parallelism.</p></li>
<li><p>Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.</p></li>
<li><p>Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU programming.</p></li>
<li><p>These approaches provide high-level abstraction and interfaces for GPU programming in the respective languages.</p></li>
</ul>
</li>
</ul>
</div>
&#13;

<h2>Summary</h2>
<p>Each of these GPU programming environments has its own strengths and weaknesses, and the best choice for a given project will depend on a range of factors, including:</p>
<ul class="simple">
<li><p>the hardware platforms being targeted,</p></li>
<li><p>the type of computation being performed, and</p></li>
<li><p>the developer’s experience and preferences.</p></li>
</ul>
<p><strong>High-level and productivity-focused APIs</strong> provide a simplified programming model and maximize code portability, while <strong>low-level and performance-focused APIs</strong> provide a high level of control over the GPU’s hardware but also require more coding effort and expertise.</p>
&#13;

<h2>Exercises</h2>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>Which GPU programming frameworks have you already used previously, if any?</p></li>
<li><p>What did you find most challenging? What was most useful?</p></li>
</ul>
<p>Let us know in the main room or via comments in HackMD document.</p>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPU programming approaches can be split into 1) directive-based, 2) non-portable kernel-based, 3) portable kernel-based, and 4) high-level language support.</p></li>
<li><p>There are multiple frameworks/languages available for each approach, each with pros and cons.</p></li>
</ul>
</div>
    
</body>
</html>