- en: 6  The mechanics of least squares
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6  最小二乘法的原理
- en: 原文：[https://mattblackwell.github.io/gov2002-book/least_squares.html](https://mattblackwell.github.io/gov2002-book/least_squares.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mattblackwell.github.io/gov2002-book/least_squares.html](https://mattblackwell.github.io/gov2002-book/least_squares.html)
- en: '[Regression](./linear_model.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[回归](./linear_model.html)'
- en: '[6  The mechanics of least squares](./least_squares.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6  最小二乘法的原理](./least_squares.html)'
- en: 'This chapter explores the most widely used estimator for population linear
    regressions: **ordinary least squares** (OLS). OLS is a plug-in estimator for
    the best linear projection (or population linear regression) described in the
    last chapter. Its popularity is partly due to its ease of interpretation, computational
    simplicity, and statistical efficiency. Because most people in the quantitative
    social sciences rely extensively on OLS for their own research, the time you spend
    developing deep familiarity with this approach will serve you well.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了用于总体线性回归的最广泛使用的估计量：**普通最小二乘法** (OLS)。OLS是上一章中描述的最佳线性投影（或总体线性回归）的插件估计量。其流行部分归因于其易于解释、计算简单和统计效率。由于大多数从事定量社会科学研究的人大量依赖OLS进行自己的研究，因此花时间深入了解这种方法将对你大有裨益。
- en: In this chapter, we focus on motivating the estimator and the mechanical or
    algebraic properties of the OLS estimator. In the next chapter, we will investigate
    its statistical assumptions. Textbooks often introduce OLS under the assumption
    of a linear model for the conditional expectation, but this is unnecessary if
    we view the inference target as the best linear predictor. We discuss this point
    more fully in the next chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于说明估计量和OLS估计量的机械或代数性质。在下一章中，我们将研究其统计假设。教科书通常在条件期望为线性模型的假设下介绍OLS，但如果我们把推断目标视为最佳线性预测器，则这是不必要的。我们将在下一章更详细地讨论这一点。
- en: '![](../Images/f17e01b7a19b165143302da9b3faaf34.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f17e01b7a19b165143302da9b3faaf34.png)'
- en: 'Figure 6.1: Relationship between political institutions and economic development
    from Acemoglu, Johnson, and Robinson (2001).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：来自Acemoglu、Johnson和Robinson（2001年）的政治制度与经济发展之间的关系。
- en: 6.1 Deriving the OLS estimator
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 推导OLS估计量
- en: The last chapter on the linear model and the best linear projection operated
    purely in the population, not samples. We derived the population regression coefficients
    \(\bfbeta\), representing the coefficients on the line of best fit in the population.
    We now take these as our quantity of interest. We now focus on how to use a sample
    from the population to make inferences about the line of best fit in the population
    and the population coefficients. To do this, we will focus on the OLS estimator
    for these population quantities.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一章关于线性模型和最佳线性投影完全是在总体中进行的，而不是在样本中。我们推导出了总体回归系数 \(\bfbeta\)，代表总体中最佳拟合线的系数。我们现在将这些作为我们感兴趣的量。我们现在关注如何使用来自总体的样本来对总体中最佳拟合线和总体系数进行推断。为此，我们将关注这些总体量的OLS估计量。
- en: '*Assumption* *The variables \(\{(Y_1, \X_1), \ldots, (Y_i,\X_i), \ldots, (Y_n,
    \X_n)\}\) are i.i.d. draws from a common distribution \(F\).*  *Recall the population
    linear coefficients (or best linear predictor coefficients) that we derived in
    the last chapter, \[ \bfbeta = \argmin_{\mb{b} \in \real^k}\; \E\bigl[ \bigl(Y_{i}
    - \mb{X}_{i}''\mb{b} \bigr)^2\bigr] = \left(\E[\X_{i}\X_{i}'']\right)^{-1}\E[\X_{i}Y_{i}]
    \]'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设* *变量 \(\{(Y_1, \X_1), \ldots, (Y_i,\X_i), \ldots, (Y_n, \X_n)\}\) 是来自共同分布
    \(F\) 的独立同分布抽取。* *回想一下我们在上一章中推导出的总体线性系数（或最佳线性预测系数），\[ \bfbeta = \argmin_{\mb{b}
    \in \real^k}\; \E\bigl[ \bigl(Y_{i} - \mb{X}_{i}''\mb{b} \bigr)^2\bigr] = \left(\E[\X_{i}\X_{i}'']\right)^{-1}\E[\X_{i}Y_{i}]
    \]'
- en: We will consider two different ways to derive the OLS estimator for these coefficients,
    both of which are versions of the plug-in principle. The first approach is to
    use the closed-form representation of the coefficients and then to replace any
    expectations with sample means, \[ \bhat = \left(\frac{1}{n} \sum_{i=1}^n \X_i\X_i'
    \right)^{-1} \left(\frac{1}{n} \sum_{i=1}^n \X_{i}Y_{i} \right), \] which exists
    if \(\sum_{i=1}^n \X_i\X_i'\) is **positive definite** and thus invertible. We
    will return to this assumption below.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑两种不同的方法来推导这些系数的OLS估计量，这两种方法都是插入原理的版本。第一种方法是使用系数的闭式表示，然后替换任何期望值以样本均值，\[
    \bhat = \left(\frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\frac{1}{n}
    \sum_{i=1}^n \X_{i}Y_{i} \right), \] 其中如果 \(\sum_{i=1}^n \X_i\X_i'\) 是 **正定的**
    并且因此可逆的话，则存在。我们将在下面回到这个假设。
- en: In a simple bivariate linear projection model \(m(X_{i}) = \beta_0 + \beta_1X_{i}\),
    we saw that the population slope was \(\beta_1= \text{cov}(Y_{i},X_{i})/ \V[X_{i}]\).
    This approach means that the estimator for the slope should be the ratio of the
    sample covariance of \(Y_i\) and \(X_i\) to the sample variance of \(X_i\), or
    \[ \widehat{\beta}_{1} = \frac{\widehat{\sigma}_{Y,X}}{\widehat{\sigma}^{2}_{X}}
    = \frac{ \frac{1}{n-1}\sum_{i=1}^{n} (Y_{i} - \overline{Y})(X_{i} - \overline{X})}{\frac{1}{n-1}
    \sum_{i=1}^{n} (X_{i} - \Xbar)^{2}}. \]
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个简单的二元线性投影模型 \(m(X_{i}) = \beta_0 + \beta_1X_{i}\) 中，我们看到了总体斜率是 \(\beta_1=
    \text{cov}(Y_{i},X_{i})/ \V[X_{i}]\)。这种方法意味着斜率的估计量应该是 \(Y_i\) 和 \(X_i\) 的样本协方差与
    \(X_i\) 的样本方差的比值，或 \[ \widehat{\beta}_{1} = \frac{\widehat{\sigma}_{Y,X}}{\widehat{\sigma}^{2}_{X}}
    = \frac{ \frac{1}{n-1}\sum_{i=1}^{n} (Y_{i} - \overline{Y})(X_{i} - \overline{X})}{\frac{1}{n-1}
    \sum_{i=1}^{n} (X_{i} - \Xbar)^{2}}. \]
- en: This plug-in approach is widely applicable and tends to have excellent properties
    in large samples under iid data. But the simplicity of the plug-in approach also
    hides some features of the estimator that become more apparent when deriving the
    estimator more explicitly using calculus. The second approach applies the plug-in
    principle not to the closed-form expression for the coefficients but to the optimization
    problem itself. We call this the **least squares** estimator because it minimizes
    the empirical (or sample) squared prediction error, \[ \bhat = \argmin_{\mb{b}
    \in \real^k}\; \frac{1}{n} \sum_{i=1}^{n}\bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2
    = \argmin_{\mb{b} \in \real^k}\; SSR(\mb{b}), \] where, \[ SSR(\mb{b}) = \sum_{i=1}^{n}\bigl(Y_{i}
    - \mb{X}_{i}'\mb{b} \bigr)^2 \] is the sum of the squared residuals. To distinguish
    it from other, more complicated least squares estimators, we call this the **ordinary
    least squares** estimator, or OLS.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种插件方法具有广泛的适用性，在独立同分布（iid）数据的大样本下往往具有优异的性质。但插件方法的简单性也隐藏了一些估计量的特征，这些特征在更明确地使用微积分推导估计量时变得更加明显。第二种方法将插件原理应用于系数的闭式表达式，而不是应用于优化问题本身。我们称这种方法为**最小二乘**估计量，因为它最小化了经验（或样本）预测误差的平方，\[
    \bhat = \argmin_{\mb{b} \in \real^k}\; \frac{1}{n} \sum_{i=1}^{n}\bigl(Y_{i} -
    \mb{X}_{i}'\mb{b} \bigr)^2 = \argmin_{\mb{b} \in \real^k}\; SSR(\mb{b}), \] 其中，\[
    SSR(\mb{b}) = \sum_{i=1}^{n}\bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2 \] 是残差的平方和。为了将其与其他更复杂的最小二乘估计量区分开来，我们称这种估计量为**普通最小二乘**估计量，或OLS。
- en: Let’s solve this minimization problem. We write down the first-order conditions
    as \[ 0=\frac{\partial SSR(\bhat)}{\partial \bfbeta} = 2 \left(\sum_{i=1}^{n}
    \X_{i}Y_{i}\right) - 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat. \] We can
    rearrange this system of equations to \[ \left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat
    = \left(\sum_{i=1}^{n} \X_{i}Y_{i}\right). \] To obtain the solution for \(\bhat\),
    notice that \(\sum_{i=1}^{n}\X_{i}\X_{i}'\) is a \((k+1) \times (k+1)\) matrix
    and \(\bhat\) and \(\sum_{i=1}^{n} \X_{i}Y_{i}\) are both \(k+1\) length column
    vectors. If \(\sum_{i=1}^{n}\X_{i}\X_{i}'\) is invertible, then we can multiply
    both sides of this equation by that inverse to arrive at \[ \bhat = \left(\sum_{i=1}^n
    \X_i\X_i' \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right), \] which is the
    same expression as the plug-in estimator (after canceling the \(1/n\) terms).
    To confirm that we have found a minimum, we also need to check the second-order
    condition, \[ \frac{\partial^{2} SSR(\bhat)}{\partial \bfbeta\bfbeta'} = 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)
    > 0. \] What does the matrix being “positive” mean? In matrix algebra, this condition
    means that the matrix \(\sum_{i=1}^{n}\X_{i}\X_{i}'\) is **positive definite**,
    a condition that we discuss in [Section 6.4](#sec-rank).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解决这个最小化问题。我们将一阶条件写成 \[ 0=\frac{\partial SSR(\bhat)}{\partial \bfbeta} = 2
    \left(\sum_{i=1}^{n} \X_{i}Y_{i}\right) - 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat.
    \] 我们可以将这个方程组重新排列为 \[ \left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat = \left(\sum_{i=1}^{n}
    \X_{i}Y_{i}\right). \] 为了获得 \(\bhat\) 的解，请注意 \(\sum_{i=1}^{n}\X_{i}\X_{i}'\) 是一个
    \((k+1) \times (k+1)\) 矩阵，而 \(\bhat\) 和 \(\sum_{i=1}^{n} \X_{i}Y_{i}\) 都是 \(k+1\)
    长度的列向量。如果 \(\sum_{i=1}^{n}\X_{i}\X_{i}'\) 是可逆的，那么我们可以将这个方程的两边乘以该逆矩阵，得到 \[ \bhat
    = \left(\sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right),
    \] 这与插件估计量（在消去 \(1/n\) 项之后）具有相同的表达式。为了确认我们已经找到了一个最小值，我们还需要检查二阶条件，\[ \frac{\partial^{2}
    SSR(\bhat)}{\partial \bfbeta\bfbeta'} = 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)
    > 0. \] 矩阵“正定”意味着什么？在矩阵代数中，这个条件意味着矩阵 \(\sum_{i=1}^{n}\X_{i}\X_{i}'\) 是**正定**的，这是一个我们在[第6.4节](#sec-rank)中讨论的条件。
- en: Both the plug-in or least squares approaches yield the same estimator for the
    best linear predictor/population linear regression coefficients.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 插件或最小二乘方法都给出了最佳线性预测器/总体线性回归系数的相同估计量。
- en: '**Theorem 6.1** If the \(\sum_{i=1}^{n}\X_{i}\X_{i}''\) is positive definite,
    then the ordinary least squares estimator is \[ \bhat = \left(\sum_{i=1}^n \X_i\X_i''
    \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right). \]'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 6.1** 如果 \(\sum_{i=1}^{n}\X_{i}\X_{i}''\) 是正定的，那么普通最小二乘估计量是 \[ \bhat =
    \left(\sum_{i=1}^n \X_i\X_i'' \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right).
    \]'
- en: '*Formula for the OLS slopes* *Almost all regression will contain an intercept
    term, usually represented as a constant 1 in the covariate vector. It is also
    possible to obtain expressions for the OLS estimates of the intercept and variable
    coefficients separately. We can rewrite the best linear predictor decomposition
    as \[ Y_{i} = \alpha + \X_{i}''\bfbeta + \e_{i}. \] Defined this way, we can write
    the OLS estimator for the “slopes” on \(\X_i\) as the OLS estimator with all variables
    demeaned: \[ \bhat = \left(\frac{1}{n} \sum_{i=1}^{n} (\X_{i} - \overline{\X})(\X_{i}
    - \overline{\X})''\right) \left(\frac{1}{n} \sum_{i=1}^{n}(\X_{i} - \overline{\X})(Y_{i}
    - \overline{Y})\right) \] which is the inverse of the sample covariance matrix
    of \(\X_i\) times the sample covariance of \(\X_i\) and \(Y_i\). The intercept
    is \[ \widehat{\alpha} = \overline{Y} - \overline{\X}''\bhat. \]*  *When dealing
    with actual data and not the population, we refer to the prediction errors \(\widehat{e}_{i}
    = Y_i - \X_i''\bhat\) as the **residuals**. The predicted value itself, \(\widehat{Y}_i
    = \X_{i}''\bhat\), is also called the **fitted value**. With the population linear
    regression, we saw that the projection errors, \(e_i = Y_i - \X_i''\bfbeta\),
    were mean zero and uncorrelated with the covariates \(\E[\X_{i}e_{i}] = 0\). The
    residuals have a similar property with respect to the covariates in the sample:
    \[ \sum_{i=1}^n \X_i\widehat{e}_i = 0. \] The residuals are *exactly* uncorrelated
    with the covariates (when the covariates include a constant/intercept term), which
    is a mechanical artifact of the OLS estimator.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*OLS斜率公式* 几乎所有的回归都会包含一个截距项，通常在协变量向量中表示为常数 1。也可以分别获得截距和变量系数的OLS估计表达式。我们可以将最佳线性预测分解重写为
    \[ Y_{i} = \alpha + \X_{i}''\bfbeta + \e_{i}. \] 以这种方式定义，我们可以将 \(\X_i\) 上的“斜率”的OLS估计量写为所有变量去均值后的OLS估计量：
    \[ \bhat = \left(\frac{1}{n} \sum_{i=1}^{n} (\X_{i} - \overline{\X})(\X_{i} -
    \overline{\X})''\right) \left(\frac{1}{n} \sum_{i=1}^{n}(\X_{i} - \overline{\X})(Y_{i}
    - \overline{Y})\right) \] 这就是 \(\X_i\) 的样本协方差矩阵的逆乘以 \(\X_i\) 和 \(Y_i\) 的样本协方差。截距是
    \[ \widehat{\alpha} = \overline{Y} - \overline{\X}''\bhat. \]*  *当处理实际数据而不是总体时，我们将预测误差
    \(\widehat{e}_{i} = Y_i - \X_i''\bhat\) 称为**残差**。预测值本身，\(\widehat{Y}_i = \X_{i}''\bhat\)，也称为**拟合值**。在总体线性回归中，我们看到了投影误差
    \(e_i = Y_i - \X_i''\bfbeta\) 的均值为零且与协变量不相关 \(\E[\X_{i}e_{i}] = 0\)。残差在样本协变量方面具有类似的性质：
    \[ \sum_{i=1}^n \X_i\widehat{e}_i = 0. \] 残差与协变量（当协变量包括常数/截距项时）完全不相关，这是OLS估计量的机械属性。'
- en: '[Figure 6.2](#fig-ssr-comp) shows how OLS works in the bivariate case. It displays
    three possible regression lines as well as the sum of the squared residuals for
    each line. OLS aims to find the line that minimizes the function on the right.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.2](#fig-ssr-comp) 展示了在双变量情况下OLS是如何工作的。它显示了三条可能的回归线以及每条线的残差平方和。OLS的目标是找到右侧函数最小化的线。'
- en: '![](../Images/e4f48f2833e1f11662843925d67cf6c8.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4f48f2833e1f11662843925d67cf6c8.png)'
- en: 'Figure 6.2: Different possible lines and their corresponding sum of squared
    residuals.**  **## 6.2 Model fit'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：不同可能的线和它们对应的残差平方和。**  **## 6.2 模型拟合
- en: 'We have learned how to use OLS to obtain an estimate of the best linear predictor,
    but an open question is whether that prediction is any good. Does using \(\X_i\)
    help us predict \(Y_i\)? To investigate this, we consider two different prediction
    errors: (1) those using covariates and (2) those that do not.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何使用OLS来获得最佳线性预测的估计，但一个悬而未决的问题是这种预测是否有效。使用 \(\X_i\) 是否有助于我们预测 \(Y_i\)？为了调查这个问题，我们考虑了两种不同的预测误差：（1）使用协变量的那些；（2）不使用的那些。
- en: We have already seen the prediction error when using the covariates; it is just
    the **sum of the squared residuals**, \[ SSR = \sum_{i=1}^n (Y_i - \X_{i}'\bhat)^2.
    \] Recall that the best predictor for \(Y_i\) without any covariates is simply
    its sample mean \(\overline{Y}\). The prediction error without covariates is what
    we call the **total sum of squares**, \[ TSS = \sum_{i=1}^n (Y_i - \overline{Y})^2.
    \] [Figure 6.3](#fig-ssr-vs-tss) shows the difference between these two types
    of prediction errors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了使用协变量时的预测误差；它仅仅是**残差平方和**，\[ SSR = \sum_{i=1}^n (Y_i - \X_{i}'\bhat)^2.
    \] 回想一下，没有协变量时\(Y_i\)的最佳预测因子仅仅是其样本均值\(\overline{Y}\)。没有协变量的预测误差就是我们所说的**总平方和**，\[
    TSS = \sum_{i=1}^n (Y_i - \overline{Y})^2. \] [图6.3](#fig-ssr-vs-tss)显示了这两种预测误差之间的差异。
- en: '![](../Images/ab8599aca3c52599119edacf9f2d3737.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ab8599aca3c52599119edacf9f2d3737.png)'
- en: 'Figure 6.3: Total sum of squares vs. the sum of squared residuals.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：总平方和与残差平方和的比较。
- en: We can use the **proportion reduction in prediction error** from adding those
    covariates to measure how much those covariates improve the regression’s predictive
    ability. This value, called the **coefficient of determination** or \(R^2\), is
    simply \[ R^2 = \frac{TSS - SSR}{TSS} = 1-\frac{SSR}{TSS}. \] The numerator, \(TSS
    - SSR\), is the reduction in prediction error moving from \(\overline{Y}\) to
    \(\X_i'\bhat\) as the predictor. The denominator is the prediction error using
    \(\overline{Y}\). Thus, the \(R^2\) value is the fraction of the total prediction
    error eliminated by using \(\X_i\) to predict \(Y_i\). Another way to think about
    this value is that it measures how much less noisy the residuals are relative
    to the overall variation in \(Y\). One thing to note is that OLS with covariates
    will *always* improve in-sample fit so that \(TSS \geq SSR\) even if \(\X_i\)
    is unrelated to \(Y_i\). This phantom improvement occurs because the point of
    OLS is to minimize the SSR, and it will do that even if it is just chasing noise.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加这些协变量带来的**预测误差比例减少**来衡量这些协变量提高了回归预测能力多少。这个值，称为**确定系数**或\(R^2\)，可以简单地表示为\[
    R^2 = \frac{TSS - SSR}{TSS} = 1-\frac{SSR}{TSS}. \] 分子，\(TSS - SSR\)，是从\(\overline{Y}\)到\(\X_i'\bhat\)作为预测因子时预测误差的减少。分母是使用\(\overline{Y}\)的预测误差。因此，\(R^2\)值是使用\(\X_i\)预测\(Y_i\)所消除的总预测误差的比例。另一种思考这个值的方法是，它衡量了残差相对于\(Y\)的整体变异性减少的程度。需要注意的是，带有协变量的OLS将**始终**提高样本内拟合度，使得\(TSS
    \geq SSR\)，即使\(\X_i\)与\(Y_i\)无关。这种虚幻的改进发生是因为OLS的目标是最小化SSR，即使它只是在追逐噪声。
- en: Since regression always improves in-sample fit, \(R^2\) will fall between 0
    and 1\. A value 0 zero would indicate exactly 0 estimated coefficients on all
    covariates (except the intercept) so that \(Y_i\) and \(\X_i\) are perfectly orthogonal
    in the data. (This is very unlikely to occur because there will likely be some
    minimal but nonzero relationship by random chance.) A value of 1 indicates a perfect
    linear fit, which occurs when all data points are perfectly predicted by the model
    with zero residuals.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于回归总是提高样本内拟合度，\(R^2\)将介于0和1之间。0的值将表示所有协变量（除了截距）上的估计系数恰好为0，因此\(Y_i\)和\(\X_i\)在数据中完全正交。（这种情况很少发生，因为很可能会由于随机机会存在一些最小但非零的关系。）1的值表示完美的线性拟合，这发生在所有数据点都由模型完美预测且没有残差的情况下。
- en: 6.3 Matrix form of OLS
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 OLS的矩阵形式
- en: 'We derived the OLS estimator above using simple algebra and calculus, but a
    more common representation of the estimator relies on vectors and matrices. We
    usually write the linear model for a generic unit, \(Y_i = \X_i''\bfbeta + e_i\),
    but obviously, there are \(n\) of these equations, \[ \begin{aligned} Y_1 &= \X_1''\bfbeta
    + e_1 \\ Y_2 &= \X_2''\bfbeta + e_2 \\ &\vdots \\ Y_n &= \X_n''\bfbeta + e_n \\
    \end{aligned} \] We can write this system of equations more compactly using matrix
    algebra. Combining the variables here into random vectors/matrices gives us: \[
    \mb{Y} = \begin{pmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix}, \quad \mathbb{X}
    = \begin{pmatrix} \X''_1 \\ \X''_2 \\ \vdots \\ \X''_n \end{pmatrix} = \begin{pmatrix}
    1 & X_{11} & X_{12} & \cdots & X_{1k} \\ 1 & X_{21} & X_{22} & \cdots & X_{2k}
    \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 1 & X_{n1} & X_{n2} & \cdots
    & X_{nk} \\ \end{pmatrix}, \quad \mb{e} = \begin{pmatrix} e_1 \\ e_2 \\ \vdots
    \\ e_n \end{pmatrix} \] We can write the above system of equations as \[ \mb{Y}
    = \mathbb{X}\bfbeta + \mb{e}, \] Note that \(\mathbb{X}\) is an \(n \times (k+1)\)
    matrix and \(\bfbeta\) is a \(k+1\) length column vector.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面使用简单的代数和微积分推导了 OLS 估计量，但更常见的估计量表示依赖于向量和矩阵。我们通常为通用单位写出线性模型，\(Y_i = \X_i'\bfbeta
    + e_i\)，但显然，有 \(n\) 个这样的方程，\[ \begin{aligned} Y_1 &= \X_1'\bfbeta + e_1 \\ Y_2
    &= \X_2'\bfbeta + e_2 \\ &\vdots \\ Y_n &= \X_n'\bfbeta + e_n \\ \end{aligned}
    \] 我们可以使用矩阵代数更紧凑地写出这个方程组。将这里的变量组合成随机向量/矩阵，我们得到：\[ \mb{Y} = \begin{pmatrix} Y_1
    \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix}, \quad \mathbb{X} = \begin{pmatrix} \X'_1
    \\ \X'_2 \\ \vdots \\ \X'_n \end{pmatrix} = \begin{pmatrix} 1 & X_{11} & X_{12}
    & \cdots & X_{1k} \\ 1 & X_{21} & X_{22} & \cdots & X_{2k} \\ \vdots & \vdots
    & \vdots & \vdots & \vdots \\ 1 & X_{n1} & X_{n2} & \cdots & X_{nk} \\ \end{pmatrix},
    \quad \mb{e} = \begin{pmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{pmatrix} \] 我们可以将上述方程组写成
    \[ \mb{Y} = \mathbb{X}\bfbeta + \mb{e}, \] 注意到 \(\mathbb{X}\) 是一个 \(n \times (k+1)\)
    的矩阵，而 \(\bfbeta\) 是一个 \(k+1\) 长度的列向量。
- en: Representing sums in matrix form is the critical link between the definition
    of OLS and matrix notation. In particular, we have \[ \begin{aligned} \sum_{i=1}^n
    \X_i\X_i' &= \Xmat'\Xmat \\ \sum_{i=1}^n \X_iY_i &= \Xmat'\mb{Y}, \end{aligned}
    \] which means we can write the OLS estimator in the more recognizable form as
    \[ \bhat = \left( \mathbb{X}'\mathbb{X} \right)^{-1} \mathbb{X}'\mb{Y}. \]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 用矩阵形式表示求和是 OLS 定义和矩阵表示之间的关键联系。特别是，我们有 \[ \begin{aligned} \sum_{i=1}^n \X_i\X_i'
    &= \Xmat'\Xmat \\ \sum_{i=1}^n \X_iY_i &= \Xmat'\mb{Y}, \end{aligned} \] 这意味着我们可以将
    OLS 估计量写成更易识别的形式，即 \[ \bhat = \left( \mathbb{X}'\mathbb{X} \right)^{-1} \mathbb{X}'\mb{Y}.
    \]
- en: We can of course also define the vector of residuals, \[ \widehat{\mb{e}} =
    \mb{Y} - \mathbb{X}\bhat = \left[ \begin{array}{c} Y_1 \\ Y_2 \\ \vdots \\ Y_n
    \end{array} \right] - \left[ \begin{array}{c} 1\widehat{\beta}_0 + X_{11}\widehat{\beta}_1
    + X_{12}\widehat{\beta}_2 + \dots + X_{1k}\widehat{\beta}_k \\ 1\widehat{\beta}_0
    + X_{21}\widehat{\beta}_1 + X_{22}\widehat{\beta}_2 + \dots + X_{2k}\widehat{\beta}_k
    \\ \vdots \\ 1\widehat{\beta}_0 + X_{n1}\widehat{\beta}_1 + X_{n2}\widehat{\beta}_2
    + \dots + X_{nk}\widehat{\beta}_k \end{array} \right], \] and so the sum of the
    squared residuals in this case becomes \[ SSR(\bfbeta) = \Vert\mb{Y} - \mathbb{X}\bfbeta\Vert^{2}
    = (\mb{Y} - \mathbb{X}\bfbeta)'(\mb{Y} - \mathbb{X}\bfbeta), \] where the double
    vertical lines are the Euclidean norm of the argument, \(\Vert \mb{z} \Vert =
    \sqrt{\sum_{i=1}^n z_i^{2}}\). The OLS minimization problem, then, is \[ \bhat
    = \argmin_{\mb{b} \in \mathbb{R}^{(k+1)}}\; \Vert\mb{Y} - \mathbb{X}\mb{b}\Vert^{2}
    \] Finally, we can write the lack of correlation of the covariates and the residuals
    as \[ \mathbb{X}'\widehat{\mb{e}} = \sum_{i=1}^{n} \X_{i}\widehat{e}_{i} = 0,
    \] which also implies these vectors are **orthogonal**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然也可以定义残差向量，\[ \widehat{\mb{e}} = \mb{Y} - \mathbb{X}\bhat = \left[ \begin{array}{c}
    Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{array} \right] - \left[ \begin{array}{c} 1\widehat{\beta}_0
    + X_{11}\widehat{\beta}_1 + X_{12}\widehat{\beta}_2 + \dots + X_{1k}\widehat{\beta}_k
    \\ 1\widehat{\beta}_0 + X_{21}\widehat{\beta}_1 + X_{22}\widehat{\beta}_2 + \dots
    + X_{2k}\widehat{\beta}_k \\ \vdots \\ 1\widehat{\beta}_0 + X_{n1}\widehat{\beta}_1
    + X_{n2}\widehat{\beta}_2 + \dots + X_{nk}\widehat{\beta}_k \end{array} \right],
    \] 因此在这种情况下，残差平方和变为 \[ SSR(\bfbeta) = \Vert\mb{Y} - \mathbb{X}\bfbeta\Vert^{2}
    = (\mb{Y} - \mathbb{X}\bfbeta)'(\mb{Y} - \mathbb{X}\bfbeta), \] 其中双竖线是参数的欧几里得范数，\(\Vert
    \mb{z} \Vert = \sqrt{\sum_{i=1}^n z_i^{2}}\). 因此，OLS 最小化问题变为 \[ \bhat = \argmin_{\mb{b}
    \in \mathbb{R}^{(k+1)}}\; \Vert\mb{Y} - \mathbb{X}\mb{b}\Vert^{2} \] 最后，我们可以将协变量和残差之间的不相关性表示为
    \[ \mathbb{X}'\widehat{\mb{e}} = \sum_{i=1}^{n} \X_{i}\widehat{e}_{i} = 0, \]
    这也意味着这些向量是 **正交** 的。
- en: 6.4 Rank, linear independence, and multicollinearity
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 排序、线性无关性和多重共线性
- en: We noted that the OLS estimator exists when \(\sum_{i=1}^n \X_i\X_i'\) is positive
    definite or that there is “no multicollinearity.” This assumption is equivalent
    to saying that the matrix \(\mathbb{X}\) is full column rank, meaning that \(\text{rank}(\mathbb{X})
    = (k+1)\), where \(k+1\) is the number of columns of \(\mathbb{X}\). Recall from
    matrix algebra that the column rank is the number of linearly independent columns
    in the matrix, and **linear independence** means that \(\mathbb{X}\mb{b} = 0\)
    if and only if \(\mb{b}\) is a column vector of 0s. In other words, we have \[
    b_{1}\mathbb{X}_{1} + b_{2}\mathbb{X}_{2} + \cdots + b_{k+1}\mathbb{X}_{k+1} =
    0 \quad\iff\quad b_{1} = b_{2} = \cdots = b_{k+1} = 0, \] where \(\mathbb{X}_j\)
    is the \(j\)th column of \(\mathbb{X}\). Thus, full column rank says that all
    the columns are linearly independent or that there is no “multicollinearity.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，当 \(\sum_{i=1}^n \X_i\X_i'\) 是正定的时候，OLS 估计量存在，或者说不存在“多重共线性”。这个假设等同于说矩阵
    \(\mathbb{X}\) 是满列秩，意味着 \(\text{rank}(\mathbb{X}) = (k+1)\)，其中 \(k+1\) 是 \(\mathbb{X}\)
    的列数。从矩阵代数中回忆起，列秩是矩阵中线性无关列的数量，而**线性无关**意味着如果 \(\mathbb{X}\mb{b} = 0\)，那么 \(\mb{b}\)
    必须是一个全零列向量。换句话说，我们有 \[ b_{1}\mathbb{X}_{1} + b_{2}\mathbb{X}_{2} + \cdots + b_{k+1}\mathbb{X}_{k+1}
    = 0 \quad\iff\quad b_{1} = b_{2} = \cdots = b_{k+1} = 0, \] 其中 \(\mathbb{X}_j\)
    是 \(\mathbb{X}\) 的第 \(j\) 列。因此，满列秩意味着所有列都是线性无关的，或者说不存在“多重共线性”。
- en: Could this be violated? Suppose we accidentally included a linear function of
    one variable so that \(\mathbb{X}_2 = 2\mathbb{X}_1\). We then have \[ \begin{aligned}
    \mathbb{X}\mb{b} &= b_{1}\mathbb{X}_{1} + b_{2}2\mathbb{X}_1+ b_{3}\mathbb{X}_{3}+
    \cdots + b_{k+1}\mathbb{X}_{k+1} \\ &= (b_{1} + 2b_{2})\mathbb{X}_{1} + b_{3}\mathbb{X}_{3}
    + \cdots + b_{k+1}\mathbb{X}_{k+1} \end{aligned} \] In this case, this expression
    equals 0 when \(b_3 = b_4 = \cdots = b_{k+1} = 0\) and \(b_1 = -2b_2\). Thus,
    the collection of columns is linearly dependent, so we know that the rank of \(\mathbb{X}\)
    must be less than full column rank (that is, less than \(k+1\)). Hopefully it
    is also clear that if we removed the problematic column \(\mathbb{X}_2\), the
    resulting matrix would have \(k\) linearly independent columns, implying that
    \(\mathbb{X}\) is rank \(k\).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能被违反吗？假设我们不小心包含了一个一元变量的线性函数，使得 \(\mathbb{X}_2 = 2\mathbb{X}_1\)。那么我们有 \[ \begin{aligned}
    \mathbb{X}\mb{b} &= b_{1}\mathbb{X}_{1} + b_{2}2\mathbb{X}_1+ b_{3}\mathbb{X}_{3}+
    \cdots + b_{k+1}\mathbb{X}_{k+1} \\ &= (b_{1} + 2b_{2})\mathbb{X}_{1} + b_{3}\mathbb{X}_{3}
    + \cdots + b_{k+1}\mathbb{X}_{k+1} \end{aligned} \] 在这种情况下，当 \(b_3 = b_4 = \cdots
    = b_{k+1} = 0\) 且 \(b_1 = -2b_2\) 时，这个表达式等于 0。因此，这些列的集合是线性相关的，所以我们知道 \(\mathbb{X}\)
    的秩必须小于满列秩（即小于 \(k+1\)）。希望也很清楚，如果我们移除了有问题的列 \(\mathbb{X}_2\)，得到的矩阵将会有 \(k\) 个线性无关的列，这意味着
    \(\mathbb{X}\) 的秩是 \(k\)。
- en: Why does this rank condition matter for the OLS estimator? In short, linear
    independence of the columns of \(\Xmat\) ensures that the inverse \((\Xmat'\Xmat)^{-1}\)
    exists and so does \(\bhat\). This is because \(\Xmat\) is of full column rank
    if and only if \(\Xmat'\Xmat\) is non-singular and a matrix is invertible if and
    only if it is non-singular. This full rank condition further implies that \(\Xmat'\Xmat
    = \sum_{i=1}^{n}\X_{i}\X_{i}'\) is positive definite, implying that the estimator
    is truly finding the minimal sum of squared residuals.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个秩条件对 OLS 估计量很重要？简而言之，\(\Xmat\) 的列的线性无关性确保了逆 \((\Xmat'\Xmat)^{-1}\) 存在，因此
    \(\bhat\) 也存在。这是因为 \(\Xmat\) 是满列秩当且仅当 \(\Xmat'\Xmat\) 是非奇异的，而一个矩阵是可逆的当且仅当它是非奇异的。这个满秩条件进一步意味着
    \(\Xmat'\Xmat = \sum_{i=1}^{n}\X_{i}\X_{i}'\) 是正定的，这意味着估计量确实在寻找最小平方残差和。
- en: What are common situations that lead to violations of no multicollinearity?
    We have seen one above, with one variable being a linear function of another.
    But this problem can come out in more subtle ways. Suppose we have a set of dummy
    variables corresponding to a single categorical variable, like the region of the
    world. This might mean we have \(X_{i1} = 1\) for units in Asia (0 otherwise),
    \(X_{i2} = 1\) for units in Europe (0 otherwise), \(X_{i3} = 1\) for units in
    Africa (0 otherwise), and \(X_{i4} = 1\) for units in the Americas (0 otherwise),
    and \(X_{i5} = 1\) for countries in Oceania (0 otherwise). Each unit has to be
    in exactly one of these five regions, so there is a linear dependence between
    these variables, \[ X_{i5} = 1 - X_{i1} - X_{i2} - X_{i3} - X_{i4}. \] That is,
    if a unit is not in Asia, Europe, Africa, or the Americas, we know it is in Oceania.
    We would get a linear dependence by including all of these variables in our regression
    with an intercept. (Note the 1 in the relationship between \(X_{i5}\) and the
    other variables, the reason why there will be linear dependence when including
    a constant.) Thus, we usually omit one dummy variable from each categorical variable.
    In that case, the coefficients on the remaining dummies are differences in means
    between that category and the omitted one (perhaps conditional on other variables
    included, if included). So if we omitted \(X_{i5}\) (Oceania), then the coefficient
    on \(X_{i1}\) would be the difference in mean outcomes between units in Asia and
    Oceania.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些常见情况会导致违反无多重共线性？我们上面已经看到了一个例子，其中一个变量是另一个变量的线性函数。但这个问题可能以更微妙的方式出现。假设我们有一组对应于单个分类变量的虚拟变量，比如世界上的地区。这可能意味着对于亚洲的单位，\(X_{i1}
    = 1\)（否则为0），对于欧洲的单位，\(X_{i2} = 1\)（否则为0），对于非洲的单位，\(X_{i3} = 1\)（否则为0），对于美洲的单位，\(X_{i4}
    = 1\)（否则为0），对于大洋洲的国家，\(X_{i5} = 1\)（否则为0）。每个单位必须恰好属于这五个区域之一，因此这些变量之间存在线性依赖性，\[
    X_{i5} = 1 - X_{i1} - X_{i2} - X_{i3} - X_{i4}. \] 这意味着如果一个单位不在亚洲、欧洲、非洲或美洲，我们知道它在大洋洲。如果我们包含所有这些变量并在回归中包含截距项，我们会得到线性依赖性。（注意
    \(X_{i5}\) 与其他变量之间的关系中的1，这是为什么包含常数时会有线性依赖性的原因。）因此，我们通常从每个分类变量中省略一个虚拟变量。在这种情况下，剩余虚拟变量的系数是该类别与省略的类别之间均值差异（如果包含其他变量，则可能是条件均值差异）。所以如果我们省略了
    \(X_{i5}\)（大洋洲），那么 \(X_{i1}\) 的系数将是亚洲单位和大洋洲单位之间平均结果差异。
- en: Collinearity can also occur when including both an intercept term and a variable
    that does not vary. This issue can often happen if we mistakenly subset our data,
    for example in this case if we subsetted the data to only the Asian units but
    still included the Asian dummy variable in the regression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当同时包含截距项和一个不变化的变量时，也可能出现多重共线性。如果我们错误地子集我们的数据，这个问题通常会发生，例如在这个例子中，如果我们只对亚洲单位进行子集，但仍然在回归中包含了亚洲虚拟变量。
- en: Finally, note that most statistical software packages will “solve” the multicollinearity
    by arbitrarily removing as many linearly dependent covariates as is necessary
    to achieve full rank. R will show the estimated coefficients as `NA` in those
    cases.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，大多数统计软件包将通过任意移除尽可能多的线性相关协变量来实现“解决”多重共线性问题，以达到满秩。在这些情况下，R 将显示估计系数为 `NA`。
- en: 6.5 OLS coefficients for binary and categorical regressors
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 二元和分类回归器的OLS系数
- en: 'Suppose that the covariates include just the intercept and a single binary
    variable, \(\X_i = (1\; X_{i})''\), where \(X_i \in \{0,1\}\). In other words,
    the right-hand side contains only one covariate, an indicator variable. In this
    case, the OLS coefficient on \(X_i\), \(\widehat{\beta_{1}}\), is exactly equal
    to the difference in sample means of \(Y_i\) in the \(X_i = 1\) group and the
    \(X_i = 0\) group: \[ \widehat{\beta}_{1} = \frac{\sum_{i=1}^{n} X_{i}Y_{i}}{\sum_{i=1}^{n}
    X_{i}} - \frac{\sum_{i=1}^{n} (1 - X_{i})Y_{i}}{\sum_{i=1}^{n} 1- X_{i}} = \overline{Y}_{X
    =1} - \overline{Y}_{X=0} \] This very useful result is not an approximation: it
    holds exactly for any sample size.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设协变量只包括截距和一个单个二元变量，\(\X_i = (1\; X_{i})'\)，其中 \(X_i \in \{0,1\}\)。换句话说，右侧只包含一个协变量，一个指示变量。在这种情况下，\(X_i\)
    上的OLS系数 \(\widehat{\beta_{1}}\) 与 \(X_i = 1\) 组和 \(X_i = 0\) 组中 \(Y_i\) 的样本均值差异完全相等：\[
    \widehat{\beta}_{1} = \frac{\sum_{i=1}^{n} X_{i}Y_{i}}{\sum_{i=1}^{n} X_{i}} -
    \frac{\sum_{i=1}^{n} (1 - X_{i})Y_{i}}{\sum_{i=1}^{n} 1- X_{i}} = \overline{Y}_{X
    =1} - \overline{Y}_{X=0} \] 这个非常有用的结果不是近似值：它对任何样本大小都成立。
- en: We can generalize this idea to discrete variables more broadly. Suppose we have
    our region variables from the last section and include in our covariates a constant
    and the dummies for Asia, Europe, Africa, and the Americas (with Oceania again
    being the omitted variable/category). Then the coefficient on the West dummy will
    be \[ \widehat{\beta}_{\text{Asia}} = \overline{Y}_{\text{Asia}} - \overline{Y}_{\text{Oceania}},
    \] which is exactly the difference in sample means of \(Y_i\) between Asian units
    and units in Oceania.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法推广到离散变量。假设我们有了上一节中的区域变量，并在协变量中包含一个常数以及亚洲、欧洲、非洲和美洲的虚拟变量（大洋洲再次作为省略的变量/类别）。那么西部的虚拟变量的系数将是
    \[ \widehat{\beta}_{\text{Asia}} = \overline{Y}_{\text{Asia}} - \overline{Y}_{\text{Oceania}},
    \] 这正是 \(Y_i\) 在亚洲单位和在大洋洲单位之间的样本均值之差。
- en: Note that these interpretations only hold when the regression consists solely
    of the binary variable or the set of categorical dummy variables. These exact
    relationships fail when other covariates are added to the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些解释仅在回归仅由二元变量或一组分类虚拟变量组成时成立。当向模型中添加其他协变量时，这些确切的关系将失效。
- en: 6.6 Projection and geometry of least squares
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 最小二乘法的投影和几何
- en: OLS has a very nice geometric interpretation that adds a lot of intuition for
    various aspects of the method. In this geometric approach, we view \(\mb{Y}\)
    as an \(n\)-dimensional vector in \(\mathbb{R}^n\). As we saw above, OLS in matrix
    form is about finding a linear combination of the covariate matrix \(\Xmat\) closest
    to this vector in terms of the Euclidean distance, which is just the sum of squares.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: OLS 有一个非常漂亮的几何解释，这为该方法的各种方面增加了许多直观性。在这种几何方法中，我们将 \(\mb{Y}\) 视为 \(\mathbb{R}^n\)
    中的 \(n\) 维向量。如上所述，矩阵形式的 OLS 是关于找到协变量矩阵 \(\Xmat\) 的线性组合，使其在欧几里得距离上最接近这个向量，这仅仅是平方和。
- en: 'Let \(\mathcal{C}(\Xmat) = \{\Xmat\mb{b} : \mb{b} \in \mathbb{R}^(k+1)\}\)
    be the **column space** of the matrix \(\Xmat\). This set is all linear combinations
    of the columns of \(\Xmat\) or the set of all possible linear predictions we could
    obtain from \(\Xmat\). Note that the OLS fitted values, \(\Xmat\bhat\), are in
    this column space. If, as we assume, \(\Xmat\) has full column rank of \(k+1\),
    then the column space \(\mathcal{C}(\Xmat)\) will be a \(k+1\)-dimensional surface
    inside of the larger \(n\)-dimensional space. If \(\Xmat\) has two columns, the
    column space will be a plane.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '设 \(\mathcal{C}(\Xmat) = \{\Xmat\mb{b} : \mb{b} \in \mathbb{R}^(k+1)\}\) 为矩阵
    \(\Xmat\) 的 **列空间**。这个集合包含了 \(\Xmat\) 列的所有线性组合，或者说是从 \(\Xmat\) 中可以得到的所有可能的线性预测。注意，OLS
    拟合值 \(\Xmat\bhat\) 就在这个列空间中。如果我们假设 \(\Xmat\) 的列满秩为 \(k+1\)，那么列空间 \(\mathcal{C}(\Xmat)\)
    将是更大 \(n\) 维空间中的一个 \(k+1\) 维曲面。如果 \(\Xmat\) 有两列，那么列空间将是一个平面。'
- en: 'Another interpretation of the OLS estimator is that it finds the linear predictor
    as the closest point in the column space of \(\Xmat\) to the outcome vector \(\mb{Y}\).
    This is called the **projection** of \(\mb{Y}\) onto \(\mathcal{C}(\Xmat)\). [Figure 6.4](#fig-projection)
    shows this projection for a case with \(n=3\) and 2 columns in \(\Xmat\). The
    shaded blue region represents the plane of the column space of \(\Xmat\), and
    \(\Xmat\bhat\) is the closest point to \(\mb{Y}\) in that space. This illustrates
    the whole idea of the OLS estimator: find the linear combination of the columns
    of \(\Xmat\) (a point in the column space) that minimizes the Euclidean distance
    between that point and the outcome vector (the sum of squared residuals).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: OLS 估计器的另一种解释是，它找到线性预测器作为 \(\Xmat\) 列空间中到结果向量 \(\mb{Y}\) 最近的点。这被称为 \(\mb{Y}\)
    在 \(\mathcal{C}(\Xmat)\) 上的 **投影**。[图 6.4](#fig-projection) 展示了 \(n=3\) 和 \(\Xmat\)
    中有 2 列的情况下的这个投影。阴影蓝色区域代表 \(\Xmat\) 列空间的平面，\(\Xmat\bhat\) 是在该空间中到 \(\mb{Y}\) 最近的点。这说明了
    OLS 估计器的整个思想：找到 \(\Xmat\) 列的线性组合（列空间中的一个点），使得该点与结果向量（残差平方和）之间的欧几里得距离最小。
- en: '![](../Images/d35d2015fb109b0c81bb2430d5ee3f58.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d35d2015fb109b0c81bb2430d5ee3f58.png)'
- en: 'Figure 6.4: Projection of Y on the column space of the covariates.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：Y 在协变量列空间上的投影。
- en: This figure shows that the residual vector, which is the difference between
    the \(\mb{Y}\) vector and the projection \(\Xmat\bhat\), is perpendicular or orthogonal
    to the column space of \(\Xmat\). This orthogonality is a consequence of the residuals
    being orthogonal to all the columns of \(\Xmat\), \[ \Xmat'\mb{e} = 0, \] as we
    established above. Being orthogonal to all the columns means it will also be orthogonal
    to all linear combinations of the columns.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了残差向量，即 \(\mb{Y}\) 向量与投影 \(\Xmat\bhat\) 之间的差，与 \(\Xmat\) 的列空间正交或垂直。这种正交性是残差与
    \(\Xmat\) 的所有列正交的结果，\[ \Xmat'\mb{e} = 0, \] 正如我们上面所建立的。对所有列的正交性意味着它也将与所有列的线性组合正交。
- en: 6.7 Projection and annihilator matrices
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 投影和湮灭矩阵
- en: 'With the idea of projection to the column space of \(\Xmat\) established, we
    can define a way to project any vector into that space. The \(n\times n\) **projection
    matrix,** \[ \mb{P}_{\Xmat} = \Xmat (\Xmat''\Xmat)^{-1} \Xmat'', \] projects a
    vector into \(\mathcal{C}(\Xmat)\). In particular, we can see that this gives
    us the fitted values for \(\mb{Y}\): \[ \mb{P}_{\Xmat}\mb{Y} = \Xmat (\Xmat''\Xmat)^{-1}
    \Xmat''\mb{Y} = \Xmat\bhat. \] Because we sometimes write the linear predictor
    as \(\widehat{\mb{Y}} = \Xmat\bhat\), the projection matrix is also called the
    **hat matrix**. With either name, multiplying a vector by \(\mb{P}_{\Xmat}\) gives
    the best linear predictor of that vector as a function of \(\Xmat\). Intuitively,
    any vector that is already a linear combination of the columns of \(\Xmat\) (so
    is in \(\mathcal{C}(\Xmat)\)) should be unaffected by this projection: the closest
    point in \(\mathcal{C}(\Xmat)\) to a point already in \(\mathcal{C}(\Xmat)\) is
    itself. We can also see this algebraically for any linear combination \(\Xmat\mb{c}\),
    \[ \mb{P}_{\Xmat}\Xmat\mb{c} = \Xmat (\Xmat''\Xmat)^{-1} \Xmat''\Xmat\mb{c} =
    \Xmat\mb{c}, \] because \((\Xmat''\Xmat)^{-1} \Xmat''\Xmat\) simplifies to the
    identity matrix. In particular, the projection of \(\Xmat\) onto itself is just
    itself: \(\mb{P}_{\Xmat}\Xmat = \Xmat\).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了 \(\Xmat\) 的列空间投影的概念之后，我们可以定义一种将任何向量投影到该空间的方法。\(n\times n\) 的 **投影矩阵**，\[
    \mb{P}_{\Xmat} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat', \] 将一个向量投影到 \(\mathcal{C}(\Xmat)\)。特别是，我们可以看到这给出了
    \(\mb{Y}\) 的拟合值：\[ \mb{P}_{\Xmat}\mb{Y} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\mb{Y}
    = \Xmat\bhat. \] 因为我们有时将线性预测器写作 \(\widehat{\mb{Y}} = \Xmat\bhat\)，所以投影矩阵也称为 **帽矩阵**。无论使用哪个名称，将一个向量乘以
    \(\mb{P}_{\Xmat}\) 都给出了该向量作为 \(\Xmat\) 的函数的最佳线性预测器。直观上，任何已经是 \(\Xmat\) 列的线性组合的向量（因此位于
    \(\mathcal{C}(\Xmat)\) 中）应该不受此投影的影响：\(\mathcal{C}(\Xmat)\) 中到已经位于 \(\mathcal{C}(\Xmat)\)
    的点的最近点是它自己。我们也可以通过代数方式看到任何线性组合 \(\Xmat\mb{c}\)，\[ \mb{P}_{\Xmat}\Xmat\mb{c} =
    \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\Xmat\mb{c} = \Xmat\mb{c}, \] 因为 \((\Xmat'\Xmat)^{-1}
    \Xmat'\Xmat\) 简化为单位矩阵。特别是，\(\Xmat\) 投影到自身的投影就是它自己：\(\mb{P}_{\Xmat}\Xmat = \Xmat\)。
- en: 'The second matrix related to projection is the **annihilator matrix**, \[ \mb{M}_{\Xmat}
    = \mb{I}_{n} - \mb{P}_{\Xmat}, \] which projects any vector into the orthogonal
    complement to the column space of \(\Xmat\), \[ \mathcal{C}^{\perp}(\Xmat) = \{\mb{c}
    \in \mathbb{R}^n\;:\; \Xmat\mb{c} = 0 \}. \] This matrix is called the annihilator
    matrix because applying it to any linear combination of \(\Xmat\), gives us 0:
    \[ \mb{M}_{\Xmat}\Xmat\mb{c} = \Xmat\mb{c} - \mb{P}_{\Xmat}\Xmat\mb{c} = \Xmat\mb{c}
    - \Xmat\mb{c} = 0. \] Note that \(\mb{M}_{\Xmat}\Xmat = 0\). Why should we care
    about this matrix? Perhaps a more evocative name might be the **residual maker**
    since it makes residuals when applied to \(\mb{Y}\), \[ \mb{M}_{\Xmat}\mb{Y} =
    (\mb{I}_{n} - \mb{P}_{\Xmat})\mb{Y} = \mb{Y} - \mb{P}_{\Xmat}\mb{Y} = \mb{Y} -
    \Xmat\bhat = \widehat{\mb{e}}. \]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与投影相关的第二个矩阵是 **湮灭矩阵**，\[ \mb{M}_{\Xmat} = \mb{I}_{n} - \mb{P}_{\Xmat}, \] 它将任何向量投影到
    \(\Xmat\) 的列空间的正交补，\[ \mathcal{C}^{\perp}(\Xmat) = \{\mb{c} \in \mathbb{R}^n\;:\;
    \Xmat\mb{c} = 0 \}. \] 这个矩阵被称为湮灭矩阵，因为将其应用于 \(\Xmat\) 的任何线性组合，都会得到 0：\[ \mb{M}_{\Xmat}\Xmat\mb{c}
    = \Xmat\mb{c} - \mb{P}_{\Xmat}\Xmat\mb{c} = \Xmat\mb{c} - \Xmat\mb{c} = 0. \]
    注意到 \(\mb{M}_{\Xmat}\Xmat = 0\)。我们为什么应该关心这个矩阵呢？也许一个更有启发性的名字可能是 **残差制造者**，因为它在应用于
    \(\mb{Y}\) 时会产生残差，\[ \mb{M}_{\Xmat}\mb{Y} = (\mb{I}_{n} - \mb{P}_{\Xmat})\mb{Y}
    = \mb{Y} - \mb{P}_{\Xmat}\mb{Y} = \mb{Y} - \Xmat\bhat = \widehat{\mb{e}}. \]
- en: 'The projection matrix has several useful properties:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 投影矩阵有几个有用的性质：
- en: '\(\mb{P}_{\Xmat}\) and \(\mb{M}_{\Xmat}\) are **idempotent**, which means that
    when applied to itself, it simply returns itself: \(\mb{P}_{\Xmat}\mb{P}_{\Xmat}
    = \mb{P}_{\Xmat}\) and \(\mb{M}_{\Xmat}\mb{M}_{\Xmat} = \mb{M}_{\Xmat}\).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mb{P}_{\Xmat}\) 和 \(\mb{M}_{\Xmat}\) 是 **幂等的**，这意味着当它们作用于自身时，仅仅返回自身：\(\mb{P}_{\Xmat}\mb{P}_{\Xmat}
    = \mb{P}_{\Xmat}\) 和 \(\mb{M}_{\Xmat}\mb{M}_{\Xmat} = \mb{M}_{\Xmat}\)。
- en: \(\mb{P}_{\Xmat}\) and \(\mb{M}_{\Xmat}\) are symmetric \(n \times n\) matrices
    so that \(\mb{P}_{\Xmat}' = \mb{P}_{\Xmat}\) and \(\mb{M}_{\Xmat}' = \mb{M}_{\Xmat}\).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mb{P}_{\Xmat}\) 和 \(\mb{M}_{\Xmat}\) 是对称的 \(n \times n\) 矩阵，因此 \(\mb{P}_{\Xmat}'
    = \mb{P}_{\Xmat}\) 和 \(\mb{M}_{\Xmat}' = \mb{M}_{\Xmat}\)。
- en: The rank of \(\mb{P}_{\Xmat}\) is \(k+1\) (the number of columns of \(\Xmat\))
    and the rank of \(\mb{M}_{\Xmat}\) is \(n - k - 1\).
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mb{P}_{\Xmat}\) 的秩是 \(k+1\)（\(\Xmat\) 的列数），而 \(\mb{M}_{\Xmat}\) 的秩是 \(n -
    k - 1\)。
- en: 'We can use the projection and annihilator matrices to arrive at an orthogonal
    decomposition of the outcome vector: \[ \mb{Y} = \Xmat\bhat + \widehat{\mb{e}}
    = \mb{P}_{\Xmat}\mb{Y} + \mb{M}_{\Xmat}\mb{Y}. \]'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用投影矩阵和湮灭矩阵来得到结果向量的正交分解：\[ \mb{Y} = \Xmat\bhat + \widehat{\mb{e}} = \mb{P}_{\Xmat}\mb{Y}
    + \mb{M}_{\Xmat}\mb{Y}. \]
- en: 6.8 Residual regression
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.8 残差回归
- en: There are many situations where we can partition the covariates into two groups,
    and we might wonder if it is possible to express or calculate the OLS coefficients
    for just one set of covariates. In particular, let the columns of \(\Xmat\) be
    partitioned into \([\Xmat_{1} \Xmat_{2}]\), so that the linear prediction we are
    estimating is \[ \mb{Y} = \Xmat_{1}\bfbeta_{1} + \Xmat_{2}\bfbeta_{2} + \mb{e},
    \] with estimated coefficients and residuals \[ \mb{Y} = \Xmat_{1}\bhat_{1} +
    \Xmat_{2}\bhat_{2} + \widehat{\mb{e}}. \]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们可以将协变量分为两组，我们可能会想知道是否可以仅对一组协变量表达或计算 OLS 系数。特别是，让 \(\Xmat\) 的列分为 \([\Xmat_{1}
    \Xmat_{2}]\)，这样我们估计的线性预测是 \[ \mb{Y} = \Xmat_{1}\bfbeta_{1} + \Xmat_{2}\bfbeta_{2}
    + \mb{e}, \] 其中估计系数和残差为 \[ \mb{Y} = \Xmat_{1}\bhat_{1} + \Xmat_{2}\bhat_{2} +
    \widehat{\mb{e}}. \]
- en: We now document another way to obtain the estimator \(\bhat_1\) from this regression
    using a technique called **residual regression**, **partitioned regression**,
    or the **Frisch-Waugh-Lovell theorem**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们记录另一种从回归中获取估计量 \(\bhat_1\) 的方法，这种方法称为**残差回归**、**分割回归**或**弗里施-沃格-洛夫定理**。
- en: '*Residual regression approach* *The residual regression approach is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*残差回归方法* 残差回归方法是：'
- en: Use OLS to regress \(\mb{Y}\) on \(\Xmat_2\) and obtain residuals \(\widetilde{\mb{e}}_2\).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 OLS 对 \(\mb{Y}\) 进行回归，以 \(\Xmat_2\) 为因变量，并得到残差 \(\widetilde{\mb{e}}_2\)。
- en: Use OLS to regress each column of \(\Xmat_1\) on \(\Xmat_2\) and obtain residuals
    \(\widetilde{\Xmat}_1\).
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 OLS 对 \(\Xmat_1\) 的每一列进行回归，以 \(\Xmat_2\) 为因变量，并得到残差 \(\widetilde{\Xmat}_1\)。
- en: Use OLS to regress \(\widetilde{\mb{e}}_{2}\) on \(\widetilde{\Xmat}_1\).*  ***Theorem
    6.2 (Frisch-Waugh-Lovell)** The OLS coefficients from a regression of \(\widetilde{\mb{e}}_{2}\)
    on \(\widetilde{\Xmat}_1\) are equivalent to the coefficients on \(\Xmat_{1}\)
    from the regression of \(\mb{Y}\) on both \(\Xmat_{1}\) and \(\Xmat_2\).
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 OLS 对 \(\widetilde{\mb{e}}_{2}\) 进行回归，以 \(\widetilde{\Xmat}_1\) 为因变量。***定理
    6.2（弗里施-沃格-洛夫）*** 从对 \(\widetilde{\mb{e}}_{2}\) 进行 \(\widetilde{\Xmat}_1\) 回归得到的
    OLS 系数与从对 \(\mb{Y}\) 进行 \(\Xmat_{1}\) 和 \(\Xmat_2\) 回归得到的 \(\Xmat_{1}\) 上的系数等价。
- en: An implication of this theorem is that the regression coefficient for a given
    variable captures the relationship between the residual variation in the outcome
    and that variable after accounting for the other covariates. In particular, this
    coefficient focuses on the variation orthogonal to those other covariates.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该定理的一个含义是，给定变量的回归系数捕捉了在考虑其他协变量后，结果残差变异与该变量之间的关系。特别是，这个系数专注于与其他协变量正交的变异。
- en: While perhaps unexpected, this result may not appear particularly useful. We
    can just run the long regression, right? But this trick can be very handy when
    \(\Xmat_2\) consists of dummy variables (or “fixed effects”) for a categorical
    variable with many categories. For example, suppose \(\Xmat_2\) consists of indicators
    for the county of residence for a respondent. In that case, that will have over
    3,000 columns, meaning that direct calculation of the \(\bhat = (\bhat_{1}, \bhat_{2})\)
    will require inverting a matrix that is bigger than \(3,000 \times 3,000\). Computationally,
    this process will be very slow. But above, we saw that predictions of an outcome
    on a categorical variable are just the sample mean within each level of the variable.
    Thus, in this case, the residuals \(\widetilde{\mb{e}}_2\) and \(\Xmat_1\) can
    be computed by demeaning the outcome and \(\Xmat_1\) within levels of the dummies
    in \(\Xmat_2\), which can be considerably faster computationally.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能令人意外，但这个结果可能看起来并不特别有用。我们不是可以直接运行长回归吗？但这个技巧在 \(\Xmat_2\) 由许多类别的分类变量的虚拟变量（或“固定效应”）组成时非常有用。例如，假设
    \(\Xmat_2\) 包含了受访者居住县的指标。在这种情况下，它将超过 3,000 列，这意味着直接计算 \(\bhat = (\bhat_{1}, \bhat_{2})\)
    将需要求逆一个大于 \(3,000 \times 3,000\) 的矩阵。从计算的角度来看，这个过程将会非常慢。但上面我们看到了，在分类变量上的结果预测只是该变量每个水平内的样本均值。因此，在这种情况下，残差
    \(\widetilde{\mb{e}}_2\) 和 \(\Xmat_1\) 可以通过在 \(\Xmat_2\) 的虚拟变量水平内对结果和 \(\Xmat_1\)
    进行去均值计算，这在计算上可以快得多。
- en: Finally, using residual regression allows researchers to visualize the conditional
    relationships between the outcome and a single independent variable after adjusting
    for other covariates. In particular, one can check the relationship using this
    approach with a scatterplot of \(\widetilde{\mb{e}}_2\) on \(\Xmat_1\) (when it
    is a single column). This residualized scatterplot allows researchers to check
    if this conditional relationship appears linear or should be modeled in another
    way.*  *## 6.9 Outliers, leverage points, and influential observations
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用残差回归允许研究人员在调整其他协变量后，可视化结果变量与单个自变量之间的条件关系。特别是，可以通过这种方法使用 \(\widetilde{\mb{e}}_2\)
    在 \(\Xmat_1\) 上的散点图（当它是一列时）来检查这种关系。这种残差散点图允许研究人员检查这种条件关系是否呈现线性关系，或者应该以其他方式建模。*  *##
    6.9 异常值、杠杆点和有影响力的观测值
- en: Given that OLS finds the coefficients that minimize the sum of the squared residuals,
    asking how much impact each residual has on that solution is very helpful. Let
    \(\bhat_{(-i)}\) be the OLS estimates if we omit unit \(i\). Intuitively, **influential
    observations** should significantly impact the estimated coefficients so that
    \(\bhat_{(-i)} - \bhat\) is large in absolute value.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 OLS 寻找最小化平方残差总和的系数，询问每个残差对解决方案的影响有多大是非常有帮助的。设 \(\bhat_{(-i)}\) 为省略单位 \(i\)
    的 OLS 估计。直观上，**有影响力的观测值**应该对估计系数有显著影响，使得 \(\bhat_{(-i)} - \bhat\) 的绝对值很大。
- en: 'Under what conditions do we have influential observations? OLS tries to minimize
    the sum of **squared** residuals, so it will move more in order to shrink larger
    residuals versus smaller ones. Where are large residuals likely to occur? Well,
    notice that any OLS regression line with a constant will exactly pass through
    the means of the outcome and the covariates: \(\overline{Y} = \overline{\X}\bhat\).
    Thus, by definition, this means that, when an observation is close to the average
    of the covariates, \(\overline{\X}\), it cannot have that much influence because
    OLS forces the regression line to go through \(\overline{Y}\). Thus, influential
    points will have two properties:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在什么条件下我们会有有影响力的观测值？OLS 尝试最小化平方残差的总和，因此它会在较大残差和较小残差之间移动更多。大残差可能出现在哪里？注意，任何具有常数的
    OLS 回归线都会恰好通过结果和协变量的均值：\(\overline{Y} = \overline{\X}\bhat\)。因此，根据定义，这意味着当一个观测值接近协变量的平均值
    \(\overline{\X}\) 时，它不可能有太大的影响，因为 OLS 会迫使回归线通过 \(\overline{Y}\)。因此，有影响力的点将有两个特性：
- en: Have high **leverage**, where leverage roughly measures how far \(\X_i\) is
    from \(\overline{\X}\), and
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有高**杠杆**，其中杠杆大致衡量 \(\X_i\) 与 \(\overline{\X}\) 的距离，并且
- en: Be an **outlier** in the sense of having a large residual (if left out of the
    regression).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“残差大”（如果从回归中排除）的意义上，成为一个**异常值**。
- en: We’ll take each of these in turn.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一讨论这些内容。
- en: 6.9.1 Leverage points
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9.1 杠杆点
- en: We can define the **leverage** of an observation by \[ h_{ii} = \X_{i}'\left(\Xmat'\Xmat\right)^{-1}\X_{i},
    \] which is the \(i\)th diagonal entry of the projection matrix, \(\mb{P}_{\Xmat}\).
    Notice that \[ \widehat{\mb{Y}} = \mb{P}_{\Xmat}\mb{Y} \qquad \implies \qquad
    \widehat{Y}_i = \sum_{j=1}^n h_{ij}Y_j, \] so that \(h_{ij}\) is the importance
    of observation \(j\) for the fitted value for observation \(i\). The leverage,
    then, is the importance of the observation for its own fitted value. We can also
    interpret these values in terms of the distribution of \(\X_{i}\). Roughly speaking,
    these values are the weighted distance between \(\X_i\) and \(\overline{\X}\),
    where the weights normalize to the empirical variance/covariance structure of
    the covariates (so that the scale of each covariate is roughly the same). We can
    see this most clearly when we fit a simple linear regression (with one covariate
    and an intercept) with OLS when the leverage is \[ h_{ii} = \frac{1}{n} + \frac{(X_i
    - \overline{X})^2}{\sum_{j=1}^n (X_j - \overline{X})^2} \]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下公式定义观察值的**杠杆作用**：\[ h_{ii} = \X_{i}'\left(\Xmat'\Xmat\right)^{-1}\X_{i},
    \] 这是对角投影矩阵 \(\mb{P}_{\Xmat}\) 的第 \(i\) 个对角元素。请注意，\[ \widehat{\mb{Y}} = \mb{P}_{\Xmat}\mb{Y}
    \qquad \implies \qquad \widehat{Y}_i = \sum_{j=1}^n h_{ij}Y_j, \] 因此 \(h_{ij}\)
    表示观察 \(j\) 对观察 \(i\) 的拟合值的重要性。杠杆作用，因此，是观察对其自身拟合值的重要性。我们还可以用 \(\X_{i}\) 的分布来解释这些值。粗略地说，这些值是
    \(\X_i\) 和 \(\overline{\X}\) 之间的加权距离，其中权重归一化到协变量的经验方差/协方差结构（因此每个协变量的尺度大致相同）。当我们用
    OLS（带有单个协变量和截距）拟合简单线性回归时，我们可以最清楚地看到这一点，此时杠杆作用为 \[ h_{ii} = \frac{1}{n} + \frac{(X_i
    - \overline{X})^2}{\sum_{j=1}^n (X_j - \overline{X})^2} \]
- en: 'Leverage values have three key properties:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 杠杆值有三个关键特性：
- en: \(0 \leq h_{ii} \leq 1\)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(0 \leq h_{ii} \leq 1\)
- en: \(h_{ii} \geq 1/n\) if the model contains an intercept
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型包含截距，则 \(h_{ii} \geq 1/n\)
- en: \(\sum_{i=1}^{n} h_{ii} = k + 1\)
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\sum_{i=1}^{n} h_{ii} = k + 1\)
- en: 6.9.2 Outliers and leave-one-out regression
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9.2 异常值和留一法回归
- en: In the context of OLS, an **outlier** is an observation with a large prediction
    error for a particular OLS specification. [Figure 6.5](#fig-outlier) shows an
    example of an outlier.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OLS 的背景下，**异常值**是指对于特定的 OLS 拟合，预测误差较大的观察值。[图 6.5](#fig-outlier) 展示了异常值的示例。
- en: '![](../Images/e08bf8a216cac143b9649600deb911a8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/e08bf8a216cac143b9649600deb911a8.png)'
- en: 'Figure 6.5: An example of an outlier.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：异常值的示例。
- en: Intuitively, it seems as though we could use the residual \(\widehat{e}_i\)
    to assess the prediction error for a given unit. But the residuals are not valid
    predictions because the OLS estimator is designed to make those as small as possible
    (in machine learning parlance, these were in the training set). In particular,
    if an outlier is influential, we already noted that it might “pull” the regression
    line toward it, and the resulting residual might be pretty small.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上看，我们似乎可以使用残差 \(\widehat{e}_i\) 来评估给定单位的预测误差。但残差并不是有效的预测，因为 OLS 估计器旨在使这些值尽可能小（在机器学习的术语中，这些值在训练集中）。特别是，如果异常值有影响力，我们之前已经指出它可能会“拉”回归线向其靠近，并且产生的残差可能相当小。
- en: 'To assess prediction errors more cleanly, we can use **leave-one-out regression**
    (LOO), which regresses \(\mb{Y}_{(-i)}\) on \(\Xmat_{(-i)}\), where these omit
    unit \(i\): \[ \bhat_{(-i)} = \left(\Xmat''_{(-i)}\Xmat_{(-i)}\right)^{-1}\Xmat_{(-i)}\mb{Y}_{(-i)}.
    \] We can then calculate LOO prediction errors as \[ \widetilde{e}_{i} = Y_{i}
    - \X_{i}''\bhat_{(-i)}. \] Calculating these LOO prediction errors for each unit
    appears to be computationally costly because it seems as though we have to fit
    OLS \(n\) times. Fortunately, there is a closed-form expression for the LOO coefficients
    and prediction errors in terms of the original regression, \[ \bhat_{(-i)} = \bhat
    - \left( \Xmat''\Xmat\right)^{-1}\X_i\widetilde{e}_i \qquad \widetilde{e}_i =
    \frac{\widehat{e}_i}{1 - h_{ii}}. \tag{6.1}\] This shows that the LOO prediction
    errors will differ from the residuals when the leverage of a unit is high. This
    makes sense! We said earlier that observations with low leverage would be close
    to \(\overline{\X}\), where the outcome values have relatively little impact on
    the OLS fit (because the regression line must go through \(\overline{Y}\)).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地评估预测误差，我们可以使用**留一法回归**（LOO），它将 \(\mb{Y}_{(-i)}\) 回归到 \(\Xmat_{(-i)}\)
    上，其中这些省略了单位 \(i\)：\[ \bhat_{(-i)} = \left(\Xmat'_{(-i)}\Xmat_{(-i)}\right)^{-1}\Xmat_{(-i)}\mb{Y}_{(-i)}.
    \] 然后，我们可以计算 LOO 预测误差为 \[ \widetilde{e}_{i} = Y_{i} - \X_{i}'\bhat_{(-i)}. \]
    对于每个单位计算这些 LOO 预测误差似乎在计算上很昂贵，因为这似乎意味着我们必须拟合 OLS \(n\) 次。幸运的是，对于 LOO 系数和预测误差，存在一个关于原始回归的闭式表达式，\[
    \bhat_{(-i)} = \bhat - \left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i \qquad
    \widetilde{e}_i = \frac{\widehat{e}_i}{1 - h_{ii}}. \tag{6.1}\] 这表明，当单位的杠杆作用高时，LOO
    预测误差将与残差不同。这是有道理的！我们之前说过，杠杆作用低的观测值将接近 \(\overline{\X}\)，其中结果值对 OLS 拟合的影响相对较小（因为回归线必须通过
    \(\overline{Y}\)）。
- en: 6.9.3 Influential observations
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9.3 有影响力的观测
- en: An influential observation (also sometimes called an influential point) is a
    unit that has the power to change the coefficients and fitted values for a particular
    OLS specification. [Figure 6.6](#fig-influence) shows an example of such an influence
    point.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有影响力的观察（有时也称为有影响力的点）是一个能够改变特定 OLS 模型系数和拟合值的单位。[图 6.6](#fig-influence) 展示了这样一个影响力点的例子。
- en: '![](../Images/fcabb6ae8553589d6ba01eae403b266f.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcabb6ae8553589d6ba01eae403b266f.png)'
- en: 'Figure 6.6: An example of an influence point.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：一个影响力点的例子。
- en: One measure of influence, called DFBETA\(_i\), measures how much \(i\) changes
    the estimated coefficient vector \[ \bhat - \bhat_{(-i)} = \left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i,
    \] so there is one value for each observation-covariate pair. When divided by
    the standard error of the estimated coefficients, this is called DFBETA**S** (where
    the “S” is for standardized). These are helpful if we focus on a particular coefficient.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 影响度的一个度量，称为 DFBETA\(_i\)，衡量 \(i\) 对估计系数向量 \[ \bhat - \bhat_{(-i)} = \left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i,
    \] 的影响程度，因此对于每个观测-协变量对有一个值。当除以估计系数的标准误时，这被称为 DFBETA**S**（其中“S”表示标准化）。如果我们关注特定的系数，这些很有帮助。
- en: 'When we want to summarize how much an observation matters for the fit, we can
    use a compact measure of the influence of an observation by comparing the fitted
    value from the entire sample to the fitted value from the leave-one-out regression.
    Using the DFBETA above, we have \[ \widehat{Y}_i - \X_{i}\bhat_{(-1)} = \X_{i}''(\bhat
    -\bhat_{(-1)}) = \X_{i}''\left( \Xmat''\Xmat\right)^{-1}\X_i\widetilde{e}_i =
    h_{ii}\widetilde{e}_i, \] so the influence of an observation is its leverage multiplied
    by how much of an outlier it is. This value is sometimes called DFFIT (difference
    in fit). One transformation of this quantity, **Cook’s distance**, standardizes
    this by the sum of the squared residuals: \[ D_i = \frac{n-k-1}{k+1}\frac{h_{ii}\widetilde{e}_{i}^{2}}{\widehat{\mb{e}}''\widehat{\mb{e}}}.
    \] Different cutoffs exist for identifying “influential” observations, but they
    tend to be ad hoc. In any case, the more important question is “how much does
    this observation matter for my substantive interpretation” rather than the narrow
    question of a particular threshold.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要总结一个观察结果对拟合的重要性时，我们可以通过比较整个样本的拟合值与留一法回归的拟合值来使用一个紧凑的观察结果影响力的度量。使用上述DFBETA，我们有\[
    \widehat{Y}_i - \X_{i}\bhat_{(-1)} = \X_{i}'(\bhat -\bhat_{(-1)}) = \X_{i}'\left(
    \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i = h_{ii}\widetilde{e}_i, \] 因此，一个观察结果的影响力是其杠杆作用乘以它是异常值的程度。这个值有时被称为DFFIT（拟合差异）。这个量的一个变换，**库克距离**，通过平方残差的和进行标准化：\[
    D_i = \frac{n-k-1}{k+1}\frac{h_{ii}\widetilde{e}_{i}^{2}}{\widehat{\mb{e}}'\widehat{\mb{e}}}.
    \] 识别“有影响力的”观察结果存在不同的截止点，但它们往往是临时的。无论如何，更重要的问题是“这个观察结果对我的实质性解释有多重要”，而不是特定阈值的问题。
- en: It’s all well and good to find influential observations, but what should be
    done about them? The first thing to check is that the data is not corrupted somehow.
    Influence points sometimes occur because of a coding or data entry error. We may
    consider removing the observation if the error appears in the data acquired from
    another source but exercise transparency if this appears to be the case. Another
    approach is to consider a transformation of the dependent or independent variables,
    like taking the natural logarithm, that might dampen the effects of outliers.
    Finally, consider using methods that are robust to outliers such as least absolute
    deviations or least trimmed squares.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 找到有影响力的观察结果固然很好，但我们应该如何处理它们呢？首先需要检查数据是否以某种方式被损坏。影响点有时是由于编码或数据输入错误而产生的。如果错误出现在从另一个来源获取的数据中，我们可以考虑删除该观察结果，但如果这种情况出现，则应保持透明度。另一种方法是考虑对因变量或自变量进行变换，例如取自然对数，这可能有助于减弱异常值的影响。最后，考虑使用对异常值具有鲁棒性的方法，如最小绝对偏差或最小截尾平方和。
- en: 6.10 Summary
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.10 概述
- en: 'In this chapter, we introduced the **ordinary least squares** estimator, which
    finds the linear function of the \(\X_i\) that minimizes the sum of the squared
    residuals and is the sample version of the best linear predictor in the last chapter.
    The \(R^2\) statistic assesses the in-sample **model fit** of OLS by comparing
    how much better it predicts the outcome compared to a simple baseline predictor
    of the sample mean of the outcome. OLS can also be written in a very compact manner
    using matrix algebra, which allows us to understand the geometry of OLS as a **projection**
    of the outcome into space of linear functions of the independent variables. The
    **Frisch-Waugh-Lovell theorem** describes a residual regression approach to obtaining
    OLS estimates for subsets of coefficients, which can be helpful for computational
    efficiency or data visualization. Lastly, influential observations are those that
    alter the estimated coefficients when they are omitted from the OLS estimation,
    and there are several metrics that help to assess this. In the next chapter, we
    move from the mechanical properties to the statistical properties of OLS: unbiasedness,
    consistency, and asymptotic normality.***'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了**普通最小二乘法**估计量，它找到使平方残差和最小的\(\X_i\)的线性函数，并且是上一章中最佳线性预测器的样本版本。\(R^2\)统计量通过比较与简单基于样本均值的结果预测器相比预测结果的好坏来评估OLS的样本模型拟合。OLS也可以使用矩阵代数以非常紧凑的方式表示，这使我们能够理解OLS作为独立变量线性函数空间的**投影**的几何形状。**弗里施-沃格-洛夫定理**描述了一种残差回归方法，用于获得系数子集的OLS估计，这可能有助于计算效率或数据可视化。最后，有影响力的观察结果是那些在从OLS估计中省略时改变估计系数的观察结果，并且有一些度量可以帮助评估这一点。在下一章中，我们将从OLS的机械属性转向其统计属性：无偏性、一致性和渐近正态性。***
