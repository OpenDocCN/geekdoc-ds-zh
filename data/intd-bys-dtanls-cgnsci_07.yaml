- en: Chapter 2 Introduction to Bayesian data analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章 贝叶斯数据分析简介
- en: 原文：[https://bruno.nicenboim.me/bayescogsci/ch-introBDA.html](https://bruno.nicenboim.me/bayescogsci/ch-introBDA.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://bruno.nicenboim.me/bayescogsci/ch-introBDA.html](https://bruno.nicenboim.me/bayescogsci/ch-introBDA.html)
- en: 'Before we can start analyzing realistic data sets using Bayes’ rule, it is
    important to understand the application of Bayes’ rule in one of the simplest
    of cases, data involving the binomial likelihood. This simple case is important
    to understand because it encapsulates the essence of the Bayesian approach to
    data analysis, and because it allows us to analytically work out the posterior
    distribution of the parameter of interest, using just pen and paper. This simple
    case also helps us to appreciate a crucial point: The posterior distribution of
    a parameter is a compromise between the prior and the likelihood. This important
    insight will play a central role in the realistic data analysis situations we
    will cover in the remainder of this book.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用贝叶斯定理分析实际数据集之前，理解贝叶斯定理在涉及二项似然的最简单情况下的应用非常重要。这个简单情况之所以重要，是因为它包含了贝叶斯数据分析方法的本质，并且它允许我们仅使用笔和纸就分析出感兴趣参数的后验分布。这个简单情况还帮助我们理解一个关键点：参数的后验分布是先验和似然之间的折衷。这个重要的洞察将在本书剩余部分讨论的现实数据分析场景中发挥核心作用。
- en: 2.1 Bayes’ rule
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 贝叶斯定理
- en: 'Recall Bayes’ rule: When \(A\) and \(B\) are observable discrete events (such
    as “it has been raining” or “the streets are wet”), we can state the rule as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾贝叶斯定理：当 \(A\) 和 \(B\) 是可观察的离散事件（例如“下雨了”或“街道湿了”）时，我们可以将规则表述如下：
- en: \[\begin{equation} P(A\mid B) = \frac{P(B\mid A) P(A)}{P(B)} \tag{2.1} \end{equation}\]
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} P(A\mid B) = \frac{P(B\mid A) P(A)}{P(B)} \tag{2.1} \end{equation}\]
- en: Given a vector of data \(\boldsymbol{y}\), Bayes’ rule allows us to work out
    the posterior distributions of the parameters of interest, which we can represent
    as the vector of parameters \(\boldsymbol{\Theta}\). This computation is achieved
    by rewriting Equation [(2.1)](ch-introBDA.html#eq:bayes-P) as [(2.2)](ch-introBDA.html#eq:bayes).
    What is different here is that Bayes’ rule is written in terms of probability
    distributions. Here, \(p(\cdot)\) is a probability density function (continuous
    case) or a probability mass function (discrete case).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据向量 \(\boldsymbol{y}\)，贝叶斯定理允许我们计算出感兴趣参数的后验分布，我们可以将其表示为参数向量 \(\boldsymbol{\Theta}\)。这种计算是通过将方程
    [(2.1)](ch-introBDA.html#eq:bayes-P) 重新表述为 [(2.2)](ch-introBDA.html#eq:bayes)
    来实现的。这里的不同之处在于贝叶斯定理是以概率分布的形式书写的。在这里，\(p(\cdot)\) 是概率密度函数（连续情况）或概率质量函数（离散情况）。
- en: \[\begin{equation} p(\boldsymbol{\Theta}|\boldsymbol{y}) = \cfrac{ p(\boldsymbol{y}|\boldsymbol{\Theta})
    \times p(\boldsymbol{\Theta}) }{p(\boldsymbol{y})} \tag{2.2} \end{equation}\]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\boldsymbol{\Theta}|\boldsymbol{y}) = \cfrac{ p(\boldsymbol{y}|\boldsymbol{\Theta})
    \times p(\boldsymbol{\Theta}) }{p(\boldsymbol{y})} \tag{2.2} \end{equation}\]
- en: 'The above statement can be rewritten in words as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上述陈述可以用以下文字表述：
- en: \[\begin{equation} \hbox{Posterior} = \frac{\hbox{Likelihood} \times \hbox{Prior}}{\hbox{Marginal
    Likelihood}} \end{equation}\]
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \hbox{后验} = \frac{\hbox{似然} \times \hbox{先验}}{\hbox{边缘似然}}
    \end{equation}\]
- en: The terms here have the following meaning. We elaborate on each point with an
    example below.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的术语具有以下含义。以下我们将通过示例详细说明每个要点。
- en: The *Posterior*, \(p(\boldsymbol{\Theta}|\boldsymbol{y})\), is the probability
    distribution of the parameters conditional on the data.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后验*，\(p(\boldsymbol{\Theta}|\boldsymbol{y})\)，是在给定数据的情况下参数的概率分布。'
- en: 'The *Likelihood*, \(p(\boldsymbol{y}|\boldsymbol{\Theta}\)) is as described
    in chapter [1](ch-intro.html#ch-intro): it is the PMF (discrete case) or the PDF
    (continuous case) expressed as a function of \(\boldsymbol{\Theta}\).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*似然*，\(p(\boldsymbol{y}|\boldsymbol{\Theta})\)，在第 [1](ch-intro.html#ch-intro)
    章中有描述：它是 \(\boldsymbol{\Theta}\) 的函数，表示为 PMF（离散情况）或 PDF（连续情况）。'
- en: The *Prior*, \(p(\boldsymbol{\Theta})\), is the initial probability distribution
    of the parameter(s), before seeing the data.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*先验*，\(p(\boldsymbol{\Theta})\)，是在看到数据之前参数的初始概率分布。'
- en: The *Marginal Likelihood*, \(p(\boldsymbol{y})\), was introduced in chapter
    [1](ch-intro.html#ch-intro) and standardizes the posterior distribution to ensure
    that the area under the curve of the distribution sums to 1, that is, it ensures
    that the posterior is a valid probability distribution.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 [1](ch-intro.html#ch-intro) 章中介绍了 *边缘似然*，\(p(\boldsymbol{y})\)，它将后验分布标准化，以确保分布曲线下的面积总和为
    1，也就是说，它确保后验是一个有效的概率分布。
- en: An example will clarify all these terms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子可以阐明所有这些术语。
- en: '2.2 Deriving the posterior using Bayes’ rule: An analytical example'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用贝叶斯定理推导后验：一个分析例子
- en: Recall our cloze probability example earlier. Subjects are shown sentences like
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们之前的完形填空概率例子。受试者被展示类似以下句子：
- en: “It’s raining. I’m going to take the …”
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: “下雨了。我要拿……”
- en: 'Suppose that 100 subjects are asked to complete the sentence. If \(80\) out
    of \(100\) subjects complete the sentence with “umbrella,” the estimated cloze
    probability or predictability (given the preceding context) would be \(\frac{80}{100}=0.8\).
    This is the maximum likelihood estimate of the probability of producing this word;
    we will designate the estimate with a “hat” on the parameter name: \(\hat \theta=0.8\).
    In the frequentist paradigm, \(\hat \theta=0.8\) is an estimate of an unknown
    point value \(\theta\) “out there in nature.”'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有100个受试者被要求完成这个句子。如果100个受试者中有80个用“雨伞”来完成这个句子，那么根据前文给出的上下文，估计的完形填空概率或可预测性（即\(\frac{80}{100}=0.8\)）。这是产生这个单词概率的最大似然估计；我们将用参数名称上的“帽子”来表示这个估计：\(\hat
    \theta=0.8\)。在频率主义范式下，\(\hat \theta=0.8\)是对自然界中未知点值\(\theta\)的估计。
- en: 'A crucial point to notice here is that the proportion \(0.8\) that we estimated
    above from the data can vary from one data set to another, and the variability
    in the estimate will be influenced by the sample size (the number of trials).
    For example, assuming that the true value of the \(\theta\) parameter is in fact
    \(0.8\), if we repeatedly carry out the above experiment with say \(10\) trials,
    we will get some variability in the estimated proportion. Let’s check this by
    carrying out \(100\) simulated experiments and computing the variability of the
    estimated means under repeated sampling:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个关键点是，我们之前从数据中估计出的比例0.8可能从一个数据集变化到另一个数据集，估计的变异性将受样本大小（试验次数）的影响。例如，假设\(\theta\)参数的真实值实际上是0.8，如果我们反复进行上述实验，每次10次试验，我们将在估计的比例中得到一些变异性。让我们通过进行100次模拟实验并计算重复抽样的估计均值变异性来检查这一点：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The repeated runs of the (simulated) experiment are the sole underlying cause
    for the variability (shown by the output of the `sd(estimated)` command above)
    in the estimated proportion; the parameter \(\theta=0.80\) itself is invariant
    here (we are repeatedly estimating this point value).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: （模拟）实验的重复运行是估计比例（如上`sd(estimated)`命令所示）变异性（唯一）的根本原因；参数\(\theta=0.80\)本身在这里是不变的（我们反复估计这个点值）。
- en: 'However, consider now an alternative, radical idea: what if we treat \(\theta\)
    as a random variable? That is, suppose now that \(\theta\) has a PDF associated
    with it. This PDF would now represent our belief about possible values of \(\theta\),
    even before we have seen any data. For example, if at the outset of the experiment,
    we believe that all possible values between \(0\) and \(1\) are equally likely,
    we could represent that belief by stating that \(\theta \sim \mathit{Uniform}(0,1)\).
    The radical new idea here is that we now have a way to represent our prior belief
    or knowledge about plausible values of the parameter.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在考虑一个不同的、激进的想法：如果我们把\(\theta\)看作一个随机变量呢？也就是说，现在假设\(\theta\)有一个与之相关的概率密度函数（PDF）。这个PDF将现在代表我们对\(\theta\)可能值的信念，即使在我们看到任何数据之前。例如，如果在实验开始时，我们相信0到1之间所有可能的值都是等可能的，我们可以通过声明\(\theta
    \sim \mathit{Uniform}(0,1)\)来表示这种信念。这里激进的新想法是我们现在有一种方法来表示我们对参数可能值的先验信念或知识。
- en: 'Now, if we were to run our simulated experiments again and again, there would
    be *two* sources of variability in the estimate of the parameter: the data as
    well as the uncertainty associated with \(\theta\).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们反复运行我们的模拟实验，参数估计的变异性将有两个来源：数据和与\(\theta\)相关的不确定性。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The higher standard deviation is now coming from the uncertainty associated
    with the \(\theta\) parameter. To see this, assume a “tighter” PDF for \(\theta\),
    say \(\theta \sim \mathit{Uniform}(0.3,0.8)\), then the variability in the estimated
    means would again be smaller, but not as small as when we assumed that \(\theta\)
    was a point value:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在更高的标准差来自于与\(\theta\)参数相关的不确定性。为了看到这一点，假设一个“更紧”的PDF，比如\(\theta \sim \mathit{Uniform}(0.3,0.8)\)，那么估计的均值的变化性会再次减小，但不会像当我们假设\(\theta\)是一个点值时那样小：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In other words, the greater the uncertainty associated with the parameter \(\theta\),
    the greater the variability in the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，与参数\(\theta\)相关的确定性越大，数据中的变异性就越大。
- en: The Bayesian approach to parameter estimation makes a radical departure from
    the standard frequentist assumption, which assumes that the true, unknown value
    of \(\theta\) is some point value. In the Bayesian approach, \(\theta\) is a random
    variable with a probability density/mass function associated with it. This PDF
    is called a prior distribution, and represents our prior belief or prior knowledge
    about possible values of this parameter. Once we obtain data, these data serve
    to modify our prior belief about this distribution; this updated probability density
    function of the parameter is called the posterior distribution. These ideas are
    unpacked in the sections below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数估计的贝叶斯方法与标准频率主义假设有根本的不同，频率主义假设认为 \(\theta\) 的真实、未知的值是某个点值。在贝叶斯方法中，\(\theta\)
    是一个随机变量，与它相关联的概率密度/质量函数。这个 PDF 被称为先验分布，代表我们对该参数可能值的先验信念或先验知识。一旦我们获得数据，这些数据就会用来修改我们对该分布的先验信念；这个参数更新的概率密度函数被称为后验分布。以下各节将详细阐述这些概念。
- en: 2.2.1 Choosing a likelihood
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 选择似然函数
- en: Under the assumptions we have set up above, the responses follow a binomial
    distribution, and so the PMF can be written as follows.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面设定的假设下，响应遵循二项分布，因此 PMF 可以写成以下形式。
- en: \[\begin{equation} p(k|n,\theta) = \binom{n} {k} \theta^k (1-\theta)^{n-k} \tag{2.3}
    \end{equation}\]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(k|n,\theta) = \binom{n} {k} \theta^k (1-\theta)^{n-k} \tag{2.3}
    \end{equation}\]
- en: where \(k\) indicates the number of times “umbrella” is given as an answer,
    and \(n\) is the number of trials. In our running example, \(k\) can therefore
    be any whole number going from \(0\) to \(100\).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(k\) 表示“雨伞”被给出作为答案的次数，\(n\) 是试验次数。在我们的例子中，\(k\) 可以是 \(0\) 到 \(100\) 之间的任何整数。
- en: 'In a particular experiment that we carry out, if we collect \(100\) data points
    (\(n=100\)) and it turns out that \(k = 80\), these data are now a fixed quantity.
    The only variable now in the PMF above is \(\theta\):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行的特定实验中，如果我们收集了 \(100\) 个数据点 (\(n=100\))，并且结果 \(k = 80\)，那么这些数据现在是一个固定量。在上述
    PMF 中现在唯一的变量是 \(\theta\)：
- en: \[\begin{equation} p(k=80 | n= 100, \theta) = \binom{100}{80} \theta^{80} (1-\theta)^{20}
    \end{equation}\]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(k=80 | n= 100, \theta) = \binom{100}{80} \theta^{80} (1-\theta)^{20}
    \end{equation}\]
- en: The above function is a now a continuous function of the value \(\theta\), which
    has possible values ranging from \(0\) to \(1\). Compare this to the PMF of the
    binomial, which treats \(\theta\) as a fixed value and defines a discrete distribution
    over the \(n+1\) possible discrete values \(k\) that we can observe (the possible
    number of successes).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数现在是一个关于 \(\theta\) 值的连续函数，其可能值范围从 \(0\) 到 \(1\)。将其与二项分布的 PMF 进行比较，二项分布将
    \(\theta\) 视为固定值，并在我们可能观察到的 \(n+1\) 个可能的离散值 \(k\) 上定义一个离散分布（可能的成功次数）。
- en: Recall that the PMF and the likelihood are the same function seen from different
    points of view. The only difference between the two is what is considered to be
    fixed and what is varying. The PMF treats data as varying from experiment to experiment
    and \(\theta\) as fixed, whereas the likelihood function treats the data that
    have been collected as fixed and the parameter \(\theta\) as varying.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，概率质量函数（PMF）和似然函数是从不同角度看待的同一函数。这两个函数之间的唯一区别在于，我们考虑什么是不变的，什么是在变化的。PMF 将数据视为随实验而变化，而
    \(\theta\) 是固定的，而似然函数将收集到的数据视为固定的，而参数 \(\theta\) 是变化的。
- en: 'We now turn our attention back to our main goal, which is to find out, using
    Bayes’ rule, the posterior distribution of \(\theta\) given our data: \(p(\theta|n,k)\).
    In order to use Bayes’ rule to calculate this posterior distribution, we need
    to define a prior distribution over the parameter \(\theta\). In doing so, we
    are explicitly expressing our prior uncertainty about plausible values of \(\theta\).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将注意力转回到我们的主要目标上，即使用贝叶斯定理，找出给定我们的数据 \(\theta\) 的后验分布：\(p(\theta|n,k)\)。为了使用贝叶斯定理来计算这个后验分布，我们需要在参数
    \(\theta\) 上定义一个先验分布。这样做时，我们明确地表达了对 \(\theta\) 可能值的先验不确定性。
- en: 2.2.2 Choosing a prior for \(\theta\)
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 为 \(\theta\) 选择先验
- en: For the choice of prior for \(\theta\) in the binomial distribution, we need
    to assume that the parameter \(\theta\) is a random variable that has a PDF whose
    range lies within [0,1], the range over which \(\theta\) can vary (this is because
    \(\theta\) represents a probability). The beta distribution, which is a PDF for
    a continuous random variable, is commonly used as prior for parameters representing
    probabilities. One reason for this choice is that its PDF ranges over the interval
    \([0,1]\). The other reason for this choice is that it makes the Bayes’ rule calculation
    remarkably easy.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二项分布中 \(\theta\) 的先验选择，我们需要假设参数 \(\theta\) 是一个随机变量，其概率密度函数（PDF）的范围位于 [0,1]
    内，这是 \(\theta\) 可以变化的范围（这是因为 \(\theta\) 代表一个概率）。贝塔分布，作为一个连续随机变量的 PDF，通常被用作表示概率的参数的先验。选择这个分布的一个原因是它的
    PDF 范围在 \([0,1]\) 区间内。另一个原因是这使得贝叶斯规则的计算变得非常简单。
- en: The beta distribution has the following PDF.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 贝塔分布具有以下 PDF。
- en: \[\begin{equation} p(\theta|a,b)= \frac{1}{B(a,b)} \theta^{a - 1} (1-\theta)^{b-1}
    \tag{2.4} \end{equation}\]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta|a,b)= \frac{1}{B(a,b)} \theta^{a - 1} (1-\theta)^{b-1}
    \tag{2.4} \end{equation}\]
- en: 'The term \(B(a,b)\) expands to \(\int_0^1 \theta^{a-1}(1-\theta)^{b-1}\, \mathrm{d}\theta\),
    and is a normalizing constant that ensures that the area under the curve sums
    to one. In some textbooks, you may see the PDF of the beta distribution with the
    normalizing constant \(\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\) (the expression
    \(\Gamma(n)\) is defined as (n-1)!): \[p(\theta|a,b)= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}
    \theta^{a - 1} (1-\theta)^{b-1}\] These two statements for the beta distribution
    are identical because \(B(a,b)\) can be shown to be equal to \(\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\)
    (Ross [2002](#ref-RossProb)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 项 \(B(a,b)\) 展开为 \(\int_0^1 \theta^{a-1}(1-\theta)^{b-1}\, \mathrm{d}\theta\)，是一个归一化常数，确保曲线下方的面积总和为
    1。在某些教科书中，你可能会看到带有归一化常数 \(\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\) 的贝塔分布 PDF（表达式
    \(\Gamma(n)\) 定义为 (n-1)!）：\[p(\theta|a,b)= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}
    \theta^{a - 1} (1-\theta)^{b-1}\] 这两个关于贝塔分布的陈述是相同的，因为 \(B(a,b)\) 可以证明等于 \(\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\)（参见
    Ross [2002](#ref-RossProb)）。
- en: The beta distribution’s parameters \(a\) and \(b\) can be interpreted as expressing
    our prior beliefs about the probability of success; \(a\) represents the number
    of “successes”, in our case, answers that are “umbrella” and \(b\) the number
    of failures, the answers that are not “umbrella.” Figure [2.1](ch-introBDA.html#fig:betas2)
    shows the different beta distribution shapes given different values of \(a\) and
    \(b\).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 贝塔分布的参数 \(a\) 和 \(b\) 可以解释为我们对成功概率的先验信念的表达；\(a\) 代表“成功”的数量，在我们的例子中，是“雨伞”答案的数量，而
    \(b\) 代表失败的数量，即不是“雨伞”的答案。图 [2.1](ch-introBDA.html#fig:betas2) 展示了不同 \(a\) 和 \(b\)
    值下的不同贝塔分布形状。
- en: '![Examples of beta distributions with different parameters.](../Images/ba978b1d5f101a9c1ca2fa12dc31fa33.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![不同参数下的贝塔分布示例](../Images/ba978b1d5f101a9c1ca2fa12dc31fa33.png)'
- en: 'FIGURE 2.1: Examples of beta distributions with different parameters.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：不同参数下的贝塔分布示例。
- en: 'As in the binomial and normal distributions that we saw in chapter 1, one can
    analytically derive the formulas for the expectation and variance of the beta
    distribution. These are:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 1 章中看到的二项分布和正态分布一样，可以解析地推导出贝塔分布的期望和方差公式。这些是：
- en: \[\begin{equation} \operatorname{E}[X] = \frac{a}{a+b} \quad \operatorname{Var}(X)=\frac
    {a \times b }{(a + b )^{2}(a + b +1)} \tag{2.5} \end{equation}\]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \operatorname{E}[X] = \frac{a}{a+b} \quad \operatorname{Var}(X)=\frac
    {a \times b }{(a + b )^{2}(a + b +1)} \tag{2.5} \end{equation}\]
- en: As an example, choosing \(a=4\) and \(b=4\) would mean that the answer “umbrella”
    is as likely as a different answer, but we are relatively unsure about this. We
    could express our uncertainty by computing the region over which we are 95% certain
    that the value of the parameter lies; this is the *95% credible interval*. For
    this, we would use the `qbeta()` function in R; the parameters \(a\) and \(b\)
    are called `shape1` and `shape2` in R.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，选择 \(a=4\) 和 \(b=4\) 意味着“雨伞”这个答案与其他答案一样可能，但我们对此相对不确定。我们可以通过计算我们 95% 确定参数值所在的区域来表达我们的不确定性；这就是
    *95% 置信区间*。为此，我们将使用 R 中的 `qbeta()` 函数；在 R 中，参数 \(a\) 和 \(b\) 分别称为 `shape1` 和 `shape2`。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The credible interval chosen above is an equal-tailed interval: the area below
    the lower bound and above the upper bound is the same (\(0.025\) in the above
    case). One could define alternative intervals; for example, in a distribution
    with only one mode (one peak; a unimodal distribution), one could choose to use
    the narrowest interval that contains the mode. This is called the highest posterior
    density interval (HDI). In skewed posterior distributions, the equal-tailed credible
    interval and the HDI will not be identical, because the HDI will have unequal
    tail probabilities. Some authors, such as Kruschke ([2014](#ref-kruschke2014doing)),
    prefer to report the HDI. We will use the equal-tailed interval in this book,
    simply because this is the standard output in Stan and `brms`.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上文选择的可信区间是一个等尾区间：低于下限和高于上限的面积相同（在上述情况下为 \(0.025\)）。可以定义其他区间；例如，在只有一个峰（一个峰值；单峰分布）的分布中，可以选择使用包含峰值的狭窄区间。这被称为最高后验密度区间（HDI）。在偏斜的后验分布中，等尾可信区间和
    HDI 不会相同，因为 HDI 将有不等的尾概率。一些作者，如 Kruschke ([2014](#ref-kruschke2014doing))，更喜欢报告
    HDI。我们将在这本书中使用等尾区间，因为这仅仅是 Stan 和 `brms` 的标准输出。
- en: If we were to choose \(a=10\) and \(b=10\), we would still be assuming that
    a priori the answer “umbrella” is just as likely as some other answer, but now
    our prior uncertainty about this mean is lower, as the 95% credible interval computed
    below shows.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择 \(a=10\) 和 \(b=10\)，我们仍然假设先验地，答案“雨伞”与其他答案一样可能，但现在我们对这个均值的先验不确定性更低，如下面的
    95% 可信区间所示。
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In Figure [2.1](ch-introBDA.html#fig:betas2), we can see also the difference
    in uncertainty in these two examples graphically.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2.1](ch-introBDA.html#fig:betas2) 中，我们也可以从图形上看到这两个例子中的不确定性差异。
- en: Which prior should we choose? In a real data analysis problem, the choice of
    prior would depend on what prior knowledge we want to bring into the analysis
    (see the online chapter [E](ch-priors.html#ch-priors)). If we don’t have much
    prior information, we could use \(a=b=1\); this gives us a uniform prior (i.e.,
    \(\mathit{Uniform}(0,1)\)). This kind of prior goes by various names, such as
    *flat, non-informative prior*, or *uninformative prior*. By contrast, if we have
    a lot of prior knowledge and/or a strong belief (e.g., based on a particular theory’s
    predictions, or prior data) that \(\theta\) has a particular range of plausible
    values, we can use a different set of \(a, b\) values to reflect our belief about
    the parameter. Generally speaking, the larger the values of the parameters \(a\)
    and \(b\), the narrower the spread of the distribution; i.e., the lower our uncertainty
    about the mean value of the parameter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择哪个先验？在真实的数据分析问题中，先验的选择将取决于我们希望带入分析中的先验知识（参见在线章节 [E](ch-priors.html#ch-priors)）。如果我们没有太多先验信息，我们可以使用
    \(a=b=1\)；这给我们一个均匀先验（即，\(\mathit{Uniform}(0,1)\)）。这种先验有各种名称，例如 *平坦、非信息先验* 或 *无信息先验*。相比之下，如果我们有很多先验知识以及/或强烈的信念（例如，基于特定理论的预测或先验数据），认为
    \(\theta\) 有一个特定的合理值范围，我们可以使用一组不同的 \(a, b\) 值来反映我们对参数的信念。一般来说，参数 \(a\) 和 \(b\)
    的值越大，分布的分布范围就越窄；即，我们对参数均值的不确定性就越低。
- en: 'We will discuss prior specification in detail in the online chapter [E](ch-priors.html#ch-priors).
    For the moment, just for illustration, we choose the values \(a=4\) and \(b=4\)
    for the beta prior. Then, our prior for \(\theta\) is the following beta PDF:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在在线章节 [E](ch-priors.html#ch-priors) 中详细讨论先验指定。目前，只是为了说明，我们选择 \(a=4\) 和 \(b=4\)
    作为贝塔先验的值。然后，我们的 \(\theta\) 先验是以下贝塔概率密度函数：
- en: \[\begin{equation} p(\theta) = \frac{1}{B(4,4)} \theta^{3} (1-\theta)^{3} \end{equation}\]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta) = \frac{1}{B(4,4)} \theta^{3} (1-\theta)^{3} \end{equation}\]
- en: Having chosen a likelihood, and having defined a prior on \(\theta\), we are
    ready to carry out our first Bayesian analysis to derive a posterior distribution
    for \(\theta\).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择了一个似然函数，并在 \(\theta\) 上定义了一个先验之后，我们准备进行我们的第一次贝叶斯分析，以推导出 \(\theta\) 的后验分布。
- en: 2.2.3 Using Bayes’ rule to compute the posterior \(p(\theta|n,k)\)
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 使用贝叶斯定理计算后验 \(p(\theta|n,k)\)
- en: 'Having specified the likelihood and the prior, we will now use Bayes’ rule
    to calculate \(p(\theta|n,k)\). Using Bayes’ rule simply involves replacing the
    likelihood and the prior we defined above into the equation we saw earlier:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定了似然函数和先验之后，我们现在将使用贝叶斯定理来计算 \(p(\theta|n,k)\)。使用贝叶斯定理简单地将我们上面定义的似然函数和先验代入到我们之前看到的方程中：
- en: \[\begin{equation} \hbox{Posterior} = \frac{\hbox{Likelihood} \times \hbox{Prior}}{\hbox{Marginal
    Likelihood}} \end{equation}\]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \hbox{Posterior} = \frac{\hbox{Likelihood} \times \hbox{Prior}}{\hbox{Marginal
    Likelihood}} \end{equation}\]
- en: 'Replace the terms for likelihood and prior into this equation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将似然和先验的项代入此方程：
- en: \[\begin{equation} p(\theta|n=100,k=80) = \frac{\left[\binom{100}{80} \theta^{80}
    \times (1-\theta)^{20}\right] \times \left[\frac{1}{B(4,4)} \times \theta^{3}
    (1-\theta)^{3}\right]}{p(k=80)} \tag{2.6} \end{equation}\]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta|n=100,k=80) = \frac{\left[\binom{100}{80} \theta^{80}
    \times (1-\theta)^{20}\right] \times \left[\frac{1}{B(4,4)} \times \theta^{3}
    (1-\theta)^{3}\right]}{p(k=80)} \tag{2.6} \end{equation}\]
- en: 'where \(p(k=80)\) is \(\int_{0}^1 p(k=80|n=100,\theta) p(\theta)\, \mathrm{d}\theta\).
    This term will be a constant once the number of successes \(k\) is known; this
    is the marginal likelihood we encountered in chapter 1\. In fact, once \(k\) is
    known, there are several constant values in the above equation; they are constants
    because none of them depend on the parameter of interest, \(\theta\). We can collect
    all of these together:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(p(k=80)\) 是 \(\int_{0}^1 p(k=80|n=100,\theta) p(\theta)\, \mathrm{d}\theta\)。这个项一旦成功次数
    \(k\) 已知，将是一个常数；这是我们在第1章中遇到的边缘似然。事实上，一旦 \(k\) 已知，上述方程中还有几个常数；它们是常数，因为它们都不依赖于感兴趣的参数，\(\theta\)。我们可以将它们全部收集在一起：
- en: \[\begin{equation} p(\theta|n=100,k=80) = \left[ \frac{\binom{100}{80}}{B(4,4)\times
    p(k=80)} \right] [\theta^{80} (1-\theta)^{20} \times \theta^{3} (1-\theta)^{3}]
    \tag{2.7} \end{equation}\]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta|n=100,k=80) = \left[ \frac{\binom{100}{80}}{B(4,4)\times
    p(k=80)} \right] [\theta^{80} (1-\theta)^{20} \times \theta^{3} (1-\theta)^{3}]
    \tag{2.7} \end{equation}\]
- en: The first term that is in square brackets, \(\frac{\binom{100}{80}}{B(4,4)\times
    p(k=80)}\), is all the constants collected together, and is the normalizing constant
    we have seen before; it makes the posterior distribution \(p(\theta|n=100,k=80)\)
    sum to one. Since it is a constant, we can ignore it for now and focus on the
    two other terms in the equation. Because we are ignoring the constant, we will
    now say that the posterior is proportional to the right-hand side.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中第一个括号内的项，\(\frac{\binom{100}{80}}{B(4,4)\times p(k=80)}\)，是所有常数收集在一起，是我们之前看到的归一化常数；它使得后验分布
    \(p(\theta|n=100,k=80)\) 的总和为1。由于它是一个常数，我们现在可以忽略它，并专注于方程中的其他两个项。因为我们忽略了常数，所以现在我们将说后验是右侧的比例。
- en: \[\begin{equation} p(\theta|n=100,k=80) \propto [\theta^{80} (1-\theta)^{20}
    \times \theta^{3} (1-\theta)^{3} ] \tag{2.8} \end{equation}\]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta|n=100,k=80) \propto [\theta^{80} (1-\theta)^{20}
    \times \theta^{3} (1-\theta)^{3} ] \tag{2.8} \end{equation}\]
- en: 'A common way of writing the above equation is:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 写出上述方程的一种常见方式是：
- en: \[\begin{equation} \hbox{Posterior} \propto \hbox{Likelihood} \times \hbox{Prior}
    \end{equation}\]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \hbox{Posterior} \propto \hbox{Likelihood} \times \hbox{Prior}
    \end{equation}\]
- en: Resolving the right-hand side now simply involves adding up the exponents! In
    this example, computing the posterior really does boil down to this simple addition
    operation on the exponents.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在解决右侧的问题只是简单地相加指数！在这个例子中，计算后验实际上归结为对指数进行简单的加法运算。
- en: \[\begin{equation} p(\theta|n=100,k=80) \propto [\theta^{80+3} (1-\theta)^{20+3}]
    = \theta^{83} (1-\theta)^{23} \tag{2.9} \end{equation}\]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta|n=100,k=80) \propto [\theta^{80+3} (1-\theta)^{20+3}]
    = \theta^{83} (1-\theta)^{23} \tag{2.9} \end{equation}\]
- en: The expression on the right-hand side corresponds to a beta distribution with
    parameters \(a=84\), and \(b=24\). This becomes evident if we rewrite the right-hand
    side such that it represents the core part of a beta PDF (see equation [(2.4)](ch-introBDA.html#eq:betach2)).
    All that is missing is a normalizing constant which would make the area under
    the curve sum to one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的表达式对应于参数 \(a=84\) 和 \(b=24\) 的贝塔分布。如果我们重新写右侧的表达式，使其代表贝塔概率密度函数（PDF）的核心部分（参见方程
    [(2.4)](ch-introBDA.html#eq:betach2)），这一点就会变得明显。唯一缺少的是归一化常数，它会使曲线下的面积总和为1。
- en: \[\begin{equation} \theta^{83} (1-\theta)^{23} = \theta^{84-1} (1-\theta)^{24-1}
    \end{equation}\]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \theta^{83} (1-\theta)^{23} = \theta^{84-1} (1-\theta)^{24-1}
    \end{equation}\]
- en: 'This core part of any PDF or PMF is called the kernel of that distribution.
    Without a normalizing constant, the area under the curve will not sum to one.
    Let’s check this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 任何概率密度函数（PDF）或概率质量函数（PMF）的核心部分被称为该分布的核心。没有归一化常数，曲线下的面积将不会总和为1。让我们来验证这一点：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So the area under the curve (AUC) is not \(1\)—the posterior that we computed
    above is not a proper probability distribution. What we have just done above is
    to compute the following integral:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，曲线下的面积（AUC）不是 \(1\) ——我们上面计算的后验分布不是一个合适的概率分布。我们上面所做的是计算以下积分：
- en: \[\begin{equation} \int_{0}^{1} \theta^{84} (1-\theta)^{24} \end{equation}\]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \int_{0}^{1} \theta^{84} (1-\theta)^{24} \end{equation}\]
- en: 'We can use this integral to figure out what the normalizing constant is. Basically,
    we want to know what the constant k is such that the area under the curve sums
    to \(1\):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个积分来确定归一化常数。基本上，我们想知道常数 k 是什么，使得曲线下的面积总和为 1：
- en: \[\begin{equation} k \int_{0}^{1} \theta^{84} (1-\theta)^{24} = 1 \end{equation}\]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} k \int_{0}^{1} \theta^{84} (1-\theta)^{24} = 1 \end{equation}\]
- en: 'We know what \(\int_{0}^{1} \theta^{84} (1-\theta)^{24}\) is; we just computed
    that value (called `AUC` in the R code above). So, the normalizing constant is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 \(\int_{0}^{1} \theta^{84} (1-\theta)^{24}\) 是什么；我们刚刚计算了那个值（在上述 R 代码中称为
    `AUC`）。因此，归一化常数是：
- en: \[\begin{equation} k = \frac{1}{\int_{0}^{1} \theta^{84} (1-\theta)^{24}} =
    \frac{1}{AUC} \end{equation}\]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} k = \frac{1}{\int_{0}^{1} \theta^{84} (1-\theta)^{24}} =
    \frac{1}{AUC} \end{equation}\]
- en: So, all that is needed to make the kernel \(\theta^{84} (1-\theta)^{24}\) into
    a proper probability distribution is to include a normalizing constant, which,
    according to the definition of the beta distribution (equation [(2.4)](ch-introBDA.html#eq:betach2)),
    would be \(B(84,24)\). This term is in fact the integral we computed above.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要将核 \(\theta^{84} (1-\theta)^{24}\) 转换为合适的概率分布，只需要包含一个归一化常数，根据贝塔分布的定义（方程
    [(2.4)](ch-introBDA.html#eq:betach2)），这个常数将是 \(B(84,24)\)。这个项实际上是我们上面计算过的积分。
- en: 'So, what we have is the distribution of \(\theta\) given the data, expressed
    as a PDF:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到的是给定数据的 \(\theta\) 分布，表示为 PDF：
- en: \[\begin{equation} p(\theta|n=100,k=80) = \frac{1}{B(84,24)} \theta^{84-1} (1-\theta)^{24-1}
    \end{equation}\]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta|n=100,k=80) = \frac{1}{B(84,24)} \theta^{84-1} (1-\theta)^{24-1}
    \end{equation}\]
- en: 'Now, this function will sum to one:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个函数的总和为 1：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 2.2.4 Summary of the procedure
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 程序总结
- en: To summarize, we started with data (\(n=100, k=80\)) and a binomial likelihood,
    multiplied it with the prior probability density function \(\theta \sim \mathit{Beta}(4,4)\),
    and obtained the posterior \(p(\theta|n,k) \sim \mathit{Beta}(84,24)\). The constants
    were ignored when carrying out the multiplication; we say that we computed the
    posterior *up to proportionality*. Finally, we showed how, in this simple example,
    the posterior can be rescaled to become a probability distribution, by including
    a proportionality constant.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们从一个数据集（\(n=100, k=80\)）和一个二项式似然开始，将其与先验概率密度函数 \(\theta \sim \mathit{Beta}(4,4)\)
    相乘，得到后验 \(p(\theta|n,k) \sim \mathit{Beta}(84,24)\)。在执行乘法时忽略了常数；我们说我们计算了后验 *到比例*。最后，我们展示了在这个简单例子中，如何通过包含一个比例常数将后验缩放为概率分布。
- en: 'The above example is a case of a *conjugate* analysis: the posterior on the
    parameter has the same form (belongs to the same family of probability distributions)
    as the prior. The above combination of likelihood and prior is called the beta-binomial
    conjugate case. There are several other such combinations of Likelihoods and Priors
    that yield a posterior that has a PDF that belongs to the same family as the PDF
    on the prior; some examples will appear in the exercises.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述例子是一个 *共轭* 分析的案例：参数的后验具有与先验相同的格式（属于同一族概率分布）。上述似然和先验的组合称为贝塔二项式共轭情况。还有其他几种似然和先验的组合，它们的后验具有与先验
    PDF 相同的 PDF；一些例子将在练习中出现。
- en: 'Formally, conjugacy is defined as follows: Given the likelihood \(p(y| \theta)\),
    if the prior \(p(\theta)\) results in a posterior \(p(\theta|y)\) that has the
    same form as \(p(\theta)\), then we call \(p(\theta)\) a conjugate prior.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，共轭性定义为以下内容：给定似然 \(p(y| \theta)\)，如果先验 \(p(\theta)\) 导致的后验 \(p(\theta|y)\)
    与 \(p(\theta)\) 具有相同的格式，那么我们称 \(p(\theta)\) 为共轭先验。
- en: 'For the beta-binomial conjugate case, we can derive a very general relationship
    between the likelihood, prior, and posterior. Given the binomial likelihood up
    to proportionality (ignoring the constant) \(\theta^k (1-\theta)^{n-k}\), and
    given the prior, also up to proportionality, \(\theta^{a-1} (1-\theta)^{b-1}\),
    their product will be:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于贝塔二项式共轭情况，我们可以推导出似然、先验和后验之间一个非常一般的关系。给定比例上的二项式似然（忽略常数）\(\theta^k (1-\theta)^{n-k}\)，以及先验，也到比例，\(\theta^{a-1}
    (1-\theta)^{b-1}\)，它们的乘积将是：
- en: \[\begin{equation} \theta^k (1-\theta)^{n-k} \theta^{a-1} (1-\theta)^{b-1} =
    \theta^{a+k-1} (1-\theta)^{b+n-k-1} \end{equation}\]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \theta^k (1-\theta)^{n-k} \theta^{a-1} (1-\theta)^{b-1} =
    \theta^{a+k-1} (1-\theta)^{b+n-k-1} \end{equation}\]
- en: Thus, given a \(\mathit{Binomial}(n,k|\theta)\) likelihood, and a \(\mathit{Beta}(a,b)\)
    prior on \(\theta\), the posterior will be \(\mathit{Beta}(a+k,b+n-k)\).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定一个 \(\mathit{Binomial}(n,k|\theta)\) 似然函数和一个 \(\mathit{Beta}(a,b)\) 先验分布，后验分布将是
    \(\mathit{Beta}(a+k,b+n-k)\)。
- en: 2.2.5 Visualizing the prior, likelihood, and posterior
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 可视化先验分布、似然函数和后验分布
- en: We established in the example above that the posterior is a beta distribution
    with parameters \(a = 84\), and \(b = 24\). We visualize the likelihood, prior,
    and the posterior side by side in Figure [2.2](ch-introBDA.html#fig:postbeta-viz).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们确定了后验分布是一个参数为 \(a = 84\) 和 \(b = 24\) 的贝塔分布。我们在图[2.2](ch-introBDA.html#fig:postbeta-viz)中并排可视化似然函数、先验分布和后验分布。
- en: '![The (scaled) likelihood, prior, and posterior in the beta-binomial conjugate
    example. The likelihood is scaled to integrate to 1 to make it easier to compare
    to the prior and posterior distributions.](../Images/39deb282fe143607e31431e44c6a63e0.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![在贝塔二项式共轭示例中（缩放后的）似然函数、先验分布和后验分布。似然函数被缩放以积分等于1，以便更容易与先验分布和后验分布进行比较。](../Images/39deb282fe143607e31431e44c6a63e0.png)'
- en: 'FIGURE 2.2: The (scaled) likelihood, prior, and posterior in the beta-binomial
    conjugate example. The likelihood is scaled to integrate to 1 to make it easier
    to compare to the prior and posterior distributions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：在贝塔二项式共轭示例中（缩放后的）似然函数、先验分布和后验分布。似然函数被缩放以积分等于1，以便更容易与先验分布和后验分布进行比较。
- en: 'We can summarize the posterior distribution either graphically as we did above,
    or summarize it by computing the mean and the variance. The mean gives us an estimate
    of the cloze probability of producing “umbrella” in that sentence (given the model,
    i.e., given the likelihood and prior):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像上面那样通过图形总结后验分布，或者通过计算均值和方差来总结。均值给出了在句子中产生“雨伞”的概率的估计（给定模型，即给定似然函数和先验）：
- en: \[\begin{equation} \operatorname{E}[\hat\theta] = \frac{84}{84+24}=0.78 \tag{2.10}
    \end{equation}\]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \operatorname{E}[\hat\theta] = \frac{84}{84+24}=0.78 \tag{2.10}
    \end{equation}\]
- en: \[\begin{equation} \operatorname{var}[\hat\theta]=\frac {84 \times 24 }{(84+24
    )^{2}(84+24 +1)}= 0.0016 \tag{2.11} \end{equation}\]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} \operatorname{var}[\hat\theta]=\frac {84 \times 24 }{(84+24
    )^{2}(84+24 +1)}= 0.0016 \tag{2.11} \end{equation}\]
- en: We could also display the 95% credible interval, the range over which we are
    95% certain that \(\theta\) lies, given the data and model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示95%的置信区间，即给定数据和模型，我们95%确信 \(\theta\) 落在这个范围内的范围。
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Typically, we would summarize the results of a Bayesian analysis by displaying
    the posterior distribution of the parameter (or parameters) graphically, along
    with the above summary statistics: the mean, the standard deviation or variance,
    and the 95% credible interval. You will see many examples of such summaries later.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会通过显示参数（或参数）的后验分布以及上述汇总统计信息（均值、标准差或方差、95%置信区间）来总结贝叶斯分析的结果。您将在后面的例子中看到许多此类汇总的例子。
- en: 2.2.6 The posterior distribution is a compromise between the prior and the likelihood
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 后验分布是先验分布和似然函数之间的折中
- en: Recall from the preceding sections that the \(a\) and \(b\) parameters in the
    beta distribution determine the shape of the prior distribution on the \(\theta\)
    parameter. Just for the sake of illustration, let’s take four different beta priors,
    which reflect increasing prior certainty about \(\theta\).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下前面的章节，贝塔分布中的 \(a\) 和 \(b\) 参数决定了 \(\theta\) 参数上的先验分布的形状。仅为了说明，让我们考虑四个不同的贝塔先验，它们反映了关于
    \(\theta\) 的先验确定性逐渐增加。
- en: \(\mathit{Beta}(a=2,b=2)\)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathit{Beta}(a=2,b=2)\)
- en: \(\mathit{Beta}(a=3,b=3)\)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathit{Beta}(a=3,b=3)\)
- en: \(\mathit{Beta}(a=6,b=6)\)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathit{Beta}(a=6,b=6)\)
- en: \(\mathit{Beta}(a=21,b=21)\)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\mathit{Beta}(a=21,b=21)\)
- en: 'Each of these priors reflects a belief that \(\theta=0.5\), but with varying
    degrees of (un)certainty. Given the general formula we developed above for the
    beta-binomial case, we just need to plug in the likelihood and the prior to get
    the posterior:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这些先验中的每一个都反映了对 \(\theta=0.5\) 的信念，但具有不同程度的（不）确定性。根据我们上面为贝塔二项式情况开发的通用公式，我们只需将似然函数和先验分布插入即可得到后验分布：
- en: \[\begin{equation} p(\theta | n,k) \propto p(k |n,\theta) p(\theta) \end{equation}\]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta | n,k) \propto p(k |n,\theta) p(\theta) \end{equation}\]
- en: 'The four corresponding posterior distributions would be:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的四个后验分布将是：
- en: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{2-1}(1-\theta)^{2-1}]
    = \theta^{82-1} (1-\theta)^{22-1} \end{equation}\]
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{2-1}(1-\theta)^{2-1}]
    = \theta^{82-1} (1-\theta)^{22-1} \end{equation}\]
- en: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{3-1}(1-\theta)^{3-1}]
    = \theta^{83-1} (1-\theta)^{23-1} \end{equation}\]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{3-1}(1-\theta)^{3-1}]
    = \theta^{83-1} (1-\theta)^{23-1} \end{equation}\]
- en: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{6-1}(1-\theta)^{6-1}]
    = \theta^{86-1} (1-\theta)^{26-1} \end{equation}\]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{6-1}(1-\theta)^{6-1}]
    = \theta^{86-1} (1-\theta)^{26-1} \end{equation}\]
- en: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{21-1}(1-\theta)^{21-1}]
    = \theta^{101-1} (1-\theta)^{41-1} \end{equation}\]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{21-1}(1-\theta)^{21-1}]
    = \theta^{101-1} (1-\theta)^{41-1} \end{equation}\]
- en: We can visualize each of these triplets of priors, likelihoods and posteriors;
    see Figure [2.3](ch-introBDA.html#fig:postbetavizvar).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化这些先验、似然和后验的三元组；见图[2.3](ch-introBDA.html#fig:postbetavizvar)。
- en: '![The (scaled) likelihood, prior, and posterior in the beta-binomial conjugate
    example, for different uncertainties in the prior. The likelihood is scaled to
    integrate to 1 to make its comparison easier.  ](../Images/e01b7219966e6918cc8688ecd2aa8646.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![在先验不确定性不同的情况下，beta-二项式共轭示例中的（缩放）似然、先验和后验。似然已缩放以积分到1，以便更容易比较。](../Images/e01b7219966e6918cc8688ecd2aa8646.png)'
- en: 'FIGURE 2.3: The (scaled) likelihood, prior, and posterior in the beta-binomial
    conjugate example, for different uncertainties in the prior. The likelihood is
    scaled to integrate to 1 to make its comparison easier.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：在先验不确定性不同的情况下，beta-二项式共轭示例中的（缩放）似然、先验和后验。似然已缩放以积分到1，以便更容易比较。
- en: 'Given some data and given a likelihood function, the tighter the prior, the
    greater the extent to which the posterior orients itself towards the prior. In
    general, we can say the following about the likelihood-prior-posterior relationship:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一些数据和似然函数，先验越紧，后验就越倾向于先验。一般来说，我们可以就似然-先验-后验关系说以下内容：
- en: The posterior distribution of a parameter is a compromise between the prior
    and the likelihood.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的后验分布是先验和似然性之间的折衷。
- en: For a given set of data, the greater the certainty in the prior, the more heavily
    will the posterior be influenced by the prior mean.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的一组数据，先验的确定性越大，后验就会受到先验均值的影响越大。
- en: Conversely, for a given set of data, the greater the *un*certainty in the prior,
    the more heavily will the posterior be influenced by the likelihood.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，对于给定的一组数据，先验的不确定性越大，后验就会受到似然性的更大影响。
- en: Another important observation emerges if we increase the sample size (here,
    the number of trials) from \(100\) to, say, \(1000000\). Suppose we still get
    a sample mean of \(0.8\) here, so that \(k=800000\). Now, the posterior mean will
    be influenced almost entirely by the sample mean. This is because, in the general
    form for the posterior \(\mathit{Beta}(a+k,b+n-k)\) that we computed above, the
    \(n\) and \(k\) become very large relative to the \(a\), \(b\) values, and dominate
    in determining the posterior mean.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将样本量（这里指试验次数）从100增加到，比如说，1000000，另一个重要的观察结果就会出现。假设我们在这里仍然得到样本均值为0.8，因此k=800000。现在，后验均值将几乎完全受到样本均值的影响。这是因为，在我们上面计算的后验的一般形式\(\mathit{Beta}(a+k,b+n-k)\)中，\(n\)和\(k\)相对于\(a\)、\(b\)值变得非常大，并在确定后验均值时占主导地位。
- en: Whenever we do a Bayesian analysis, it is good practice to check whether the
    parameter you are interested in estimating is sensitive to the prior specification.
    Such an investigation is called a *sensitivity analysis*. Later in this book,
    we will see many examples of sensitivity analyses in realistic data-analysis settings.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们进行贝叶斯分析时，检查你感兴趣估计的参数是否对先验指定敏感是一个好的做法。这种调查被称为*敏感性分析*。在本书的后面部分，我们将看到许多在现实数据分析设置中的敏感性分析的例子。
- en: 2.2.7 Incremental knowledge gain using prior knowledge
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 使用先验知识进行增量知识获取
- en: 'In the above example, we used an artificial example where we asked \(100\)
    subjects to complete the sentence shown at the beginning of the chapter, and then
    we counted the number of times that they produced “umbrella” vs. some other word
    as a continuation. Given \(80\) instances of “umbrella”, and using a \(\mathit{Beta}(4,4)\)
    prior, we derived the posterior to be \(\mathit{Beta}(84,24)\). We could now use
    this posterior as our prior for the next study. Suppose that we were to carry
    out a second experiment, again with \(100\) subjects, and this time \(60\) produced
    “umbrella.” We could now use our new prior (\(\mathit{Beta}(84,24)\)) to obtain
    an updated posterior. We have \(a=84, b=24, n=100, k=60\). This gives us as posterior:
    \(\mathit{Beta}(a+k,b+n-k) = \mathit{Beta}(84+60,24+100-60)=\mathit{Beta}(144,64)\).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述例子中，我们使用了一个人工示例，其中我们要求 \(100\) 个受试者完成本章开头显示的句子，然后我们计算了他们产生“雨伞”与其他词语作为后续的次数。给定
    \(80\) 个“雨伞”实例，并使用 \(\mathit{Beta}(4,4)\) 先验，我们推导出后验为 \(\mathit{Beta}(84,24)\)。现在我们可以使用这个后验作为下一项研究的先验。假设我们进行第二次实验，再次有
    \(100\) 个受试者，这次有 \(60\) 个产生了“雨伞”。现在我们可以使用我们新的先验（\(\mathit{Beta}(84,24)\)）来获得更新的后验。我们有
    \(a=84, b=24, n=100, k=60\)。这给我们带来了后验：\(\mathit{Beta}(a+k, b+n-k) = \mathit{Beta}(84+60,
    24+100-60)=\mathit{Beta}(144, 64)\)。
- en: Now, if we were to pool all our data that we have from the two experiments,
    then we would have as data \(n=200, k=140\). Suppose that we keep our initial
    prior of \(a=4,b=4\). Then, our posterior would be \(\mathit{Beta}(4+140,4+200-140)=\mathit{Beta}(144,64)\).
    This is exactly the same posterior that we got when first analyzed the first \(100\)
    subjects’ data, derived the posterior, and then used that posterior as a prior
    for the next \(100\) subjects’ data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们把从两个实验中获得的所有数据合并起来，那么我们就会有 \(n=200, k=140\) 的数据。假设我们保持初始先验 \(a=4, b=4\)。那么，我们的后验将是
    \(\mathit{Beta}(4+140, 4+200-140)=\mathit{Beta}(144, 64)\)。这正是我们在首次分析前 \(100\)
    个受试者的数据时得到的后验，推导出后验，然后将其作为下一个 \(100\) 个受试者数据的先验。
- en: This toy example illustrates an important point that has great practical importance
    for cognitive science. One can incrementally gain information about a research
    question by using information from previous studies and deriving a posterior,
    and then use that posterior as a prior for the next study. For practical examples
    from psycholinguistics showing how information can be pooled from previous studies,
    see Jäger, Engelmann, and Vasishth ([2017](#ref-JaegerEngelmannVasishth2017))
    and Nicenboim, Roettger, and Vasishth ([2018](#ref-NicenboimRoettgeretal)). Vasishth
    and Engelmann ([2022](#ref-VasishthEngelmann2022)) illustrates an example of how
    the posterior from a previous study or collection of studies can be used as a
    prior to compute the posterior from new data. This approach allows us to build
    on the information available from previous work.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个玩具示例说明了认知科学中一个具有重要实际意义的重要观点。通过使用先前研究的信息并推导出后验，然后使用该后验作为下一项研究的先验，可以逐步获取关于研究问题的信息。有关如何从先前研究中汇总信息的心理语言学实例，请参阅
    Jäger, Engelmann, 和 Vasishth ([2017](#ref-JaegerEngelmannVasishth2017)) 以及 Nicenboim,
    Roettger, 和 Vasishth ([2018](#ref-NicenboimRoettgeretal))。Vasishth 和 Engelmann
    ([2022](#ref-VasishthEngelmann2022)) 说明了如何将先前研究或研究集合的后验用作先验来计算新数据的后验。这种方法使我们能够从前人的工作中获取的信息中获益。
- en: 2.3 Summary
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 摘要
- en: 'In this chapter, we learned how to use Bayes’ rule in the specific case of
    a binomial likelihood, and a beta prior on the \(\theta\) parameter in the likelihood
    function. Our goal in any Bayesian analysis will follow the path we took in this
    simple example: decide on an appropriate likelihood function, decide on priors
    for all the parameters involved in the likelihood function, and use this model
    (i.e., the likelihood and the priors) to derive the posterior distribution of
    each parameter. Then we draw inferences about our research question based on the
    posterior distribution of the parameter.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在二项似然的具体情况下使用贝叶斯定理，以及在似然函数中对 \(\theta\) 参数使用贝塔先验。在任何贝叶斯分析中，我们的目标都将遵循我们在简单示例中采取的路径：选择一个合适的似然函数，为似然函数中涉及的所有参数选择先验，并使用这个模型（即似然函数和先验）来推导每个参数的后验分布。然后，我们根据参数的后验分布对研究问题进行推断。
- en: In the example discussed in this chapter, Bayesian analysis was easy. This was
    because we considered the simple conjugate case of the beta-binomial. In realistic
    data-analysis settings, our likelihood function will be very complex, and many
    parameters will be involved. Multiplying the likelihood function and the priors
    will become mathematically difficult or impossible. For such situations, we use
    computational methods to obtain samples from the posterior distributions of the
    parameters.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章讨论的例子中，贝叶斯分析很简单。这是因为我们考虑了 beta-二项分布的简单共轭情况。在现实的数据分析环境中，我们的似然函数将非常复杂，并且将涉及许多参数。乘以似然函数和先验分布将变得数学上困难或不可能。对于这种情况，我们使用计算方法从参数的后验分布中获取样本。
- en: 2.4 Further reading
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 进一步阅读
- en: Accessible introductions to conjugate Bayesian analysis are Lynch ([2007](#ref-lynch2007introduction)),
    and Lunn et al. ([2012](#ref-lunn2012bugs)). Somewhat more demanding discussions
    of conjugate analysis are in Lee ([2012](#ref-lee2012bayesian)), Carlin and Louis
    ([2008](#ref-carlin2008bayesian)), Christensen et al. ([2011](#ref-christensen2011)),
    O’Hagan and Forster ([2004](#ref-kendall2004)) and Bernardo and Smith ([2009](#ref-bernardosmith)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 关于共轭贝叶斯分析的易于理解的介绍有 Lynch ([2007](#ref-lynch2007introduction)) 和 Lunn 等人 ([2012](#ref-lunn2012bugs))。关于共轭分析的更深入讨论可以在
    Lee ([2012](#ref-lee2012bayesian))，Carlin 和 Louis ([2008](#ref-carlin2008bayesian))，Christensen
    等人 ([2011](#ref-christensen2011))，O’Hagan 和 Forster ([2004](#ref-kendall2004))
    以及 Bernardo 和 Smith ([2009](#ref-bernardosmith)) 的作品中找到。
- en: References
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bernardo, José M., and Adrian F. M. Smith. 2009\. *Bayesian Theory*. Vol. 405\.
    John Wiley & Sons.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Bernardo, José M., 和 Adrian F. M. Smith. 2009\. *贝叶斯理论*. 第 405 卷. 约翰·威利与 Sons.
- en: Carlin, Bradley P., and Thomas A Louis. 2008\. *Bayesian Methods for Data Analysis*.
    CRC Press.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Carlin, Bradley P., 和 Thomas A Louis. 2008\. *贝叶斯数据分析方法*. CRC 压力出版社。
- en: Christensen, Ronald, Wesley Johnson, Adam Branscum, and Timothy Hanson. 2011\.
    “Bayesian Ideas and Data Analysis.” CRC Press.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Christensen, Ronald, Wesley Johnson, Adam Branscum, 和 Timothy Hanson. 2011\.
    “贝叶斯思想和数据分析。” CRC 压力出版社。
- en: 'Jäger, Lena A., Felix Engelmann, and Shravan Vasishth. 2017\. “Similarity-Based
    Interference in Sentence Comprehension: Literature review and Bayesian meta-analysis.”
    *Journal of Memory and Language* 94: 316–39\. [https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004](https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jäger, Lena A., Felix Engelmann, 和 Shravan Vasishth. 2017\. “句子理解中的基于相似性的干扰：文献综述和贝叶斯元分析。”
    *记忆与语言杂志* 94: 316–39\. [https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004](https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004).'
- en: 'Kruschke, John K. 2014\. *Doing Bayesian Data Analysis: A tutorial with R,
    JAGS, and Stan*. Academic Press.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Kruschke, John K. 2014\. *贝叶斯数据分析教程：使用 R、JAGS 和 Stan*. 学术出版社。
- en: 'Lee, Peter M. 2012\. *Bayesian Statistics: An Introduction*. John Wiley & Sons.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Lee, Peter M. 2012\. *贝叶斯统计学：导论*. 约翰·威利与 Sons.
- en: 'Lunn, David J., Chris Jackson, David J. Spiegelhalter, Nichola G. Best, and
    Andrew Thomas. 2012\. *The BUGS Book: A Practical Introduction to Bayesian Analysis*.
    Vol. 98\. CRC Press.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Lunn, David J., Chris Jackson, David J. Spiegelhalter, Nichola G. Best, 和 Andrew
    Thomas. 2012\. *BUGS 书：贝叶斯分析的实用导论*. 第 98 卷. CRC 压力出版社。
- en: 'Lynch, Scott Michael. 2007\. *Introduction to Applied Bayesian Statistics and
    Estimation for Social Scientists*. New York, NY: Springer.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Lynch, Scott Michael. 2007\. *应用贝叶斯统计和社会科学家估计导论*. 纽约，纽约：斯普林格。
- en: 'Nicenboim, Bruno, Timo B. Roettger, and Shravan Vasishth. 2018\. “Using Meta-Analysis
    for Evidence Synthesis: The case of incomplete neutralization in German.” *Journal
    of Phonetics* 70: 39–55\. [https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001](https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 'Nicenboim, Bruno, Timo B. Roettger, 和 Shravan Vasishth. 2018\. “使用元分析进行证据综合：德国不完全中和的案例。”
    *语音学杂志* 70: 39–55\. [https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001](https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001).'
- en: 'O’Hagan, Anthony, and Jonathan J. Forster. 2004\. “Kendall’s Advanced Theory
    of Statistics, Vol. 2B: Bayesian Inference.” Wiley.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: O’Hagan, Anthony, 和 Jonathan J. Forster. 2004\. “肯德尔高级统计学理论，第 2B 卷：贝叶斯推理。” 威利。
- en: Ross, Sheldon. 2002\. *A First Course in Probability*. Pearson Education.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Ross, Sheldon. 2002\. *概率论入门*. Pearson 教育。
- en: 'Vasishth, Shravan, and Felix Engelmann. 2022\. *Sentence Comprehension as a
    Cognitive Process: A Computational Approach*. Cambridge, UK: Cambridge University
    Press. [https://books.google.de/books?id=6KZKzgEACAAJ](https://books.google.de/books?id=6KZKzgEACAAJ).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Vasishth, Shravan, 和 Felix Engelmann. 2022\. *句子理解作为认知过程：计算方法*. 英国剑桥：剑桥大学出版社。
    [https://books.google.de/books?id=6KZKzgEACAAJ](https://books.google.de/books?id=6KZKzgEACAAJ).
