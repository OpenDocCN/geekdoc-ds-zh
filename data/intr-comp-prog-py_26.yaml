- en: '25'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '25'
- en: CLUSTERING
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类
- en: Unsupervised learning involves finding hidden structure in unlabeled data. The
    most commonly used unsupervised machine learning technique is clustering.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习涉及在无标签数据中寻找隐藏结构。最常用的无监督机器学习技术是聚类。
- en: Clustering can be defined as the process of organizing objects into groups whose
    members are similar in some way. A key issue is defining the meaning of “similar.”
    Consider the plot in [Figure 25-1](#c25-fig-0001), which shows the height, weight,
    and shirt color for 13 people.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以定义为将对象组织成某种方式相似的组的过程。一个关键问题是定义“相似”的含义。考虑[图25-1](#c25-fig-0001)中的图示，显示了13个人的身高、体重和衬衫颜色。
- en: '![c25-fig-0001.jpg](../images/c25-fig-0001.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0001.jpg](../images/c25-fig-0001.jpg)'
- en: '[Figure 25-1](#c25-fig-0001a) Height, weight, and shirt color'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[图25-1](#c25-fig-0001a) 身高、体重和衬衫颜色'
- en: If we cluster people by height, there are two obvious clusters—delimited by
    the dotted horizontal line. If we cluster people by weight, there are two different
    obvious clusters—delimited by the solid vertical line. If we cluster people based
    on their shirts, there is yet a third clustering—delimited by the angled dashed
    lines. Notice, by the way, that this last division is not linear since we cannot
    separate the people by shirt color using a single straight line.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们按身高对人进行聚类，会出现两个明显的簇——由虚线水平线划分。如果我们按体重对人进行聚类，会出现两个不同的明显簇——由实线垂直线划分。如果我们根据他们的衬衫进行聚类，还有第三种聚类——由倾斜的虚线划分。顺便提一下，最后这种划分不是线性的，因为我们无法用一条直线根据衬衫颜色将人们分开。
- en: Clustering is an optimization problem. The goal is to find a set of clusters
    that optimizes an objective function, subject to some set of constraints. Given
    a distance metric that can be used to decide how close two examples are to each
    other, we need to define an objective function that minimizes the dissimilarity
    of the examples within a cluster.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一个优化问题。目标是找到一组簇，以优化目标函数，同时遵循一组约束条件。给定一个可以用来决定两个示例之间接近程度的距离度量，我们需要定义一个目标函数，以最小化簇内示例之间的不相似度。
- en: One measure, which we call variability (often called inertia in the literature),
    of how different the examples within a single cluster, *c*, are from each other
    is
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为变异性（在文献中通常称为惯性）的一个度量，表示单个簇内的示例*c*之间的差异性是
- en: '![c25-fig-5001.jpg](../images/c25-fig-5001.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-5001.jpg](../images/c25-fig-5001.jpg)'
- en: where *mean*(*c*) is the mean of the feature vectors of all the examples in
    the cluster. The mean of a set of vectors is computed component-wise. The corresponding
    elements are added, and the result divided by the number of vectors. If `v1` and
    `v2` are `arrays` of numbers, the value of the expression `(v1+v2)/2` is their
    **Euclidean mean**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*mean*(*c*)是簇内所有示例特征向量的均值。一个向量集的均值是逐组件计算的。对应元素相加，然后结果除以向量的数量。如果`v1`和`v2`是数字的`arrays`，表达式`(v1+v2)/2`的值是它们的**欧几里得均值**。
- en: What we are calling variability is similar to the notion of variance presented
    in Chapter 17\. The difference is that variability is not normalized by the size
    of the cluster, so clusters with more points are likely to look less cohesive
    according to this measure. If we want to compare the coherence of two clusters
    of different sizes, we need to divide the variability of each cluster by the size
    of the cluster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所称的变异性类似于第17章中提出的方差的概念。不同之处在于，变异性没有通过簇的大小进行归一化，因此根据该度量，具有更多点的簇看起来可能不那么凝聚。如果我们想比较两个不同大小簇的凝聚性，就需要将每个簇的变异性除以簇的大小。
- en: 'The definition of variability within a single cluster, *c*, can be extended
    to define a dissimilarity metric for a set of clusters, *C*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 单个簇*c*内的变异性的定义可以扩展为一组簇*C*的不相似度度量：
- en: '![c25-fig-5002.jpg](../images/c25-fig-5002.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-5002.jpg](../images/c25-fig-5002.jpg)'
- en: Notice that since we don't divide the variability by the size of the cluster,
    a large incoherent cluster increases the value of *dissimilarity(C)* more than
    a small incoherent cluster does. This is by design.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们不将变异性除以簇的大小，因此一个大的不凝聚簇会比一个小的不凝聚簇更大地增加*dissimilarity(C)*的值。这是有意设计的。
- en: So, is the optimization problem to find a set of clusters, *C*, such that *dissimilarity(C)*
    is minimized? Not exactly. It can easily be minimized by putting each example
    in its own cluster. We need to add some constraint. For example, we could put
    a constraint on the minimum distance between clusters or require that the maximum
    number of clusters be some constant *k*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，优化问题是否是找到一组簇 *C*，使得 *dissimilarity(C)* 被最小化？不完全是。通过将每个示例放在其自己的簇中，可以很容易地将其最小化。我们需要添加一些约束。例如，我们可以对簇之间的最小距离施加约束，或者要求最大簇数为某个常数
    *k*。
- en: In general, solving this optimization problem is computationally prohibitive
    for most interesting problems. Consequently, people rely on greedy algorithms
    that provide approximate solutions. In Section 25.2, we present one such algorithm,
    k-means clustering. But first we will introduce some abstractions that are useful
    for implementing that algorithm (and other clustering algorithms as well).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，解决这个优化问题在大多数有趣的问题上都是计算上不可行的。因此，人们依赖于提供近似解的贪心算法。在第 25.2 节中，我们介绍一种这样的算法，即
    k 均值聚类。但首先我们将介绍一些实现该算法（以及其他聚类算法）时有用的抽象。
- en: 25.1 Class Cluster
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.1 类 Cluster
- en: Class `Example` ([Figure 25-2](#c25-fig-0004)) will be used to build the samples
    to be clustered. Associated with each example is a name, a feature vector, and
    an optional label. The `distance` method returns the Euclidean distance between
    two examples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类 `Example` ([图 25-2](#c25-fig-0004)) 将用于构建要聚类的样本。与每个示例相关联的是一个名称、一个特征向量和一个可选标签。`distance`
    方法返回两个示例之间的欧几里得距离。
- en: '![c25-fig-0002.jpg](../images/c25-fig-0002.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0002.jpg](../images/c25-fig-0002.jpg)'
- en: '[Figure 25-2](#c25-fig-0004a) Class `Example`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-2](#c25-fig-0004a) 类 `Example`'
- en: Class `Cluster` ([Figure 25-3](#c25-fig-0005)) is slightly more complex. A cluster
    is a set of examples. The two interesting methods in `Cluster` are `compute_centroid`
    and `variability`. Think of the **centroid** of a cluster as its center of mass.
    The method `compute_centroid` returns an example with a feature vector equal to
    the Euclidean mean of the feature vectors of the examples in the cluster. The
    method `variability` provides a measure of the coherence of the cluster.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 类 `Cluster` ([图 25-3](#c25-fig-0005)) 稍微复杂一些。一个簇是一组示例。`Cluster` 中两个有趣的方法是 `compute_centroid`
    和 `variability`。将簇的 **质心** 视为其质心。方法 `compute_centroid` 返回一个示例，其特征向量等于簇中示例特征向量的欧几里得均值。方法
    `variability` 提供了簇的连贯性度量。
- en: '![c25-fig-0003.jpg](../images/c25-fig-0003.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0003.jpg](../images/c25-fig-0003.jpg)'
- en: '[Figure 25-3](#c25-fig-0005a) Class `Cluster`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-3](#c25-fig-0005a) 类 `Cluster`'
- en: '**Finger exercise**: Is the centroid of a cluster always one of the examples
    in the cluster?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**动手练习**：一个簇的质心是否总是该簇中的一个示例？'
- en: 25.2 K-means Clustering
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.2 K均值聚类
- en: '**K-means clustering** is probably the most widely used clustering method.[^(191)](#c25-fn-0001)
    Its goal is to partition a set of examples into `k` clusters such that'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**K均值聚类**可能是最广泛使用的聚类方法。[^(191)](#c25-fn-0001) 它的目标是将一组示例划分为 `k` 个簇，使得'
- en: Each example is in the cluster whose centroid is the closest centroid to that
    example.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个示例位于其质心最近的簇中。
- en: The dissimilarity of the set of clusters is minimized.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇集的相异性被最小化。
- en: Unfortunately, finding an optimal solution to this problem on a large data set
    is computationally intractable. Fortunately, there is an efficient greedy algorithm[^(192)](#c25-fn-0002)
    that can be used to find a useful approximation. It is described by the pseudocode
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在大数据集上找到该问题的最优解在计算上是不可行的。幸运的是，有一种高效的贪心算法[^(192)](#c25-fn-0002) 可用于找到有用的近似解。它由伪代码描述。
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The complexity of step 1 is order *θ*`(k*n*d)`, where `k` is the number of clusters,
    `n` is the number of examples, and `d` the time required to compute the distance
    between a pair of examples. The complexity of step 2 is *θ*`(n)`, and the complexity
    of step 3 is *θ*`(k)`. Hence, the complexity of a single iteration is *θ*`(k*n*d).`
    If the examples are compared using the Minkowski distance, `d` is linear in the
    length of the feature vector.[^(193)](#c25-fn-0003) Of course, the complexity
    of the entire algorithm depends upon the number of iterations. That is not easy
    to characterize, but suffice it to say that it is usually small.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1 的复杂度为 *θ*`(k*n*d)`，其中 `k` 是聚类的数量，`n` 是样本的数量，`d` 是计算一对样本之间距离所需的时间。步骤 2 的复杂度为
    *θ*`(n)`，步骤 3 的复杂度为 *θ*`(k)`。因此，单次迭代的复杂度为 *θ*`(k*n*d)`。如果使用闵可夫斯基距离比较样本，`d` 与特征向量的长度呈线性关系。[^(193)](#c25-fn-0003)
    当然，整个算法的复杂度取决于迭代的次数。这一点不容易表征，但可以说通常是较小的。
- en: '[Figure 25-4](#c25-fig-0006) contains a translation into Python of the pseudocode
    describing k-means. The only wrinkle is that it raises an exception if any iteration
    creates a cluster with no members. Generating an empty cluster is rare. It can''t
    occur on the first iteration, but it can occur on subsequent iterations. It usually
    results from choosing too large a `k` or an unlucky choice of initial centroids.
    Treating an empty cluster as an error is one of the options used by MATLAB. Another
    is creating a new cluster containing a single point—the point furthest from the
    centroid in the other clusters. We chose to treat it as an error to simplify the
    implementation.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-4](#c25-fig-0006) 包含描述 k-means 的伪代码的 Python 翻译。唯一的特殊之处在于，如果任何迭代创建了一个没有成员的聚类，则会抛出异常。生成空聚类是很少见的。它不会出现在第一次迭代中，但可能在后续迭代中出现。这通常是由于选择的
    `k` 过大或初始质心选择不幸。将空聚类视为错误是 MATLAB 使用的选项之一。另一个选项是创建一个只包含一个点的新聚类——该点是其他聚类中距离质心最远的点。我们选择将其视为错误，以简化实现。'
- en: One problem with the k-means algorithm is that the value returned depends upon
    the initial set of randomly chosen centroids. If a particularly unfortunate set
    of initial centroids is chosen, the algorithm might settle into a local optimum
    that is far from the global optimum. In practice, this problem is typically addressed
    by running k-means multiple times with randomly chosen initial centroids. We then
    choose the solution with the minimum dissimilarity of clusters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法的一个问题是返回的值依赖于初始随机选择的质心。如果选择了一组特别不幸的初始质心，算法可能会陷入一个远离全局最优解的局部最优解。在实践中，通常通过多次运行
    k-means 来解决这个问题，初始质心随机选择。然后，我们选择具有最小聚类相异度的解决方案。
- en: '[Figure 25-5](#c25-fig-0007) contains a function, `try_k_means`, that calls
    `k_means` `(`[Figure 25-4](#c25-fig-0006)`)` multiple times and selects the result
    with the lowest dissimilarity. If a trial fails because `k_means` generated an
    empty cluster and therefore raised an exception, `try_k_means` merely tries again—assuming
    that eventually `k_means` will choose an initial set of centroids that successfully
    converges.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-5](#c25-fig-0007) 包含一个函数 `try_k_means`，它多次调用 `k_means` `(`[图 25-4](#c25-fig-0006)`)`
    并选择相异度最低的结果。如果试验失败，因为 `k_means` 生成了一个空聚类并因此抛出了异常，`try_k_means` 仅仅是重新尝试——假设最终 `k_means`
    会选择一个成功收敛的初始质心集合。'
- en: '![c25-fig-0004.jpg](../images/c25-fig-0004.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0004.jpg](../images/c25-fig-0004.jpg)'
- en: '[Figure 25-4](#c25-fig-0006a) K-means clustering'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-4](#c25-fig-0006) K-means 聚类'
- en: '![c25-fig-0005.jpg](../images/c25-fig-0005.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0005.jpg](../images/c25-fig-0005.jpg)'
- en: '[Figure 25-5](#c25-fig-0007a) Finding the best k-means clustering'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-5](#c25-fig-0007a) 寻找最佳的 k-means 聚类'
- en: 25.3 A Contrived Example
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.3 一个人为的例子
- en: '[Figure 25-6](#c25-fig-0008) contains code that generates, plots, and clusters
    examples drawn from two distributions.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-6](#c25-fig-0008) 包含生成、绘制和聚类来自两个分布的样本的代码。'
- en: The function `gen_distributions` generates a list of `n` examples with two-dimensional
    feature vectors. The values of the elements of these feature vectors are drawn
    from normal distributions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `gen_distributions` 生成一个包含 `n` 个样本的列表，这些样本具有二维特征向量。这些特征向量元素的值来自正态分布。
- en: The function `plot_samples` plots the feature vectors of a set of examples.
    It uses `plt.annotate` to place text next to points on the plot. The first argument
    is the text, the second argument the point with which the text is associated,
    and the third argument the location of the text relative to the point with which
    it is associated.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`plot_samples`绘制一组示例的特征向量。它使用`plt.annotate`在绘图中的点旁边放置文本。第一个参数是文本，第二个参数是与文本相关联的点，第三个参数是文本相对于与其相关联的点的位置。
- en: The function `contrived_test` uses `gen_distributions` to create two distributions
    of 10 examples (each with the same standard deviation but different means), plots
    the examples using `plot_samples`, and then clusters them using `try_k_means`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`contrived_test`使用`gen_distributions`创建两个包含10个示例的分布（每个分布具有相同的标准差但不同的均值），使用`plot_samples`绘制示例，然后使用`try_k_means`对其进行聚类。
- en: '![c25-fig-0006.jpg](../images/c25-fig-0006.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0006.jpg](../images/c25-fig-0006.jpg)'
- en: '[Figure 25-6](#c25-fig-0008a) A test of k-means'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[图25-6](#c25-fig-0008a) k均值的测试'
- en: The call `contrived_test(1, 2, True)` produced the plot in [Figure 25-7](#c25-fig-0009)
    and printed the lines in [Figure 25-8](#c25-fig-0010). Notice that the initial
    (randomly chosen) centroids led to a highly skewed clustering in which a single
    cluster contained all but one of the points. By the fourth iteration, however,
    the centroids had moved to places such that the points from the two distributions
    were reasonably well separated into two clusters. The only “mistakes” were made
    on `A0` and `A8`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`contrived_test(1, 2, True)`生成了[图25-7](#c25-fig-0009)中的绘图，并打印了[图25-8](#c25-fig-0010)中的线条。注意，初始（随机选择的）质心导致了高度偏斜的聚类，其中一个聚类包含了除一个点以外的所有点。然而，到第四次迭代时，质心移动到使得两个分布的点合理分开为两个聚类的位置。唯一的“错误”发生在`A0`和`A8`上。
- en: '![c25-fig-0007.jpg](../images/c25-fig-0007.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0007.jpg](../images/c25-fig-0007.jpg)'
- en: '[Figure 25-7](#c25-fig-0009a) Examples from two distributions'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图25-7](#c25-fig-0009a) 来自两个分布的示例'
- en: '![c25-fig-0008.jpg](../images/c25-fig-0008.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0008.jpg](../images/c25-fig-0008.jpg)'
- en: '[Figure 25-8](#c25-fig-0010a) Lines printed by a call to `contrived_test(1,
    2, True)`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[图25-8](#c25-fig-0010a) 调用`contrived_test(1, 2, True)`打印的线条'
- en: When we tried `50` trials rather than `1,` by calling `contrived_test(50, 2,
    False)`, it printed
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试`50`次实验而不是`1`次时，通过调用`contrived_test(50, 2, False)`，它打印了
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`A0` is still mixed in with the `B`''s, but `A8` is not. If we try `1000` trials,
    we get the same result. That might surprise you, since a glance at [Figure 25-7](#c25-fig-0009)
    reveals that if `A0` and `B0` are chosen as the initial centroids (which would
    probably happen with `1000` trials), the first iteration will yield clusters that
    perfectly separate the `A`''s and `B`''s. However, in the second iteration new
    centroids will be computed, and `A0` will be assigned to a cluster with the `B`''s.
    Is this bad? Recall that clustering is a form of unsupervised learning that looks
    for structure in unlabeled data. Grouping `A0` with the `B`''s is not unreasonable.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`A0`仍然与`B`混在一起，但`A8`则没有。如果我们尝试`1000`次实验，结果也一样。这可能会让你感到惊讶，因为[图25-7](#c25-fig-0009)显示，如果`A0`和`B0`被选为初始质心（这在`1000`次实验中可能发生），第一次迭代将产生完美分离`A`和`B`的聚类。然而，在第二次迭代中将计算新的质心，`A0`将被分配到与`B`的一个聚类中。这不好吗？请记住，聚类是一种无监督学习形式，它在未标记数据中寻找结构。将`A0`与`B`分组并不不合理。'
- en: One of the key issues in using k-means clustering is choosing `k`. The function
    `contrived_test_2` in [Figure 25-9](#c25-fig-0011) generates, plots, and clusters
    points from three overlapping Gaussian distributions. We will use it to look at
    the results of clustering this data for various values of `k`. The data points
    are shown in [Figure 25-10](#c25-fig-0012).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k均值聚类的一个关键问题是选择`k`。[图25-9](#c25-fig-0011)中的函数`contrived_test_2`生成、绘制并聚类来自三个重叠高斯分布的点。我们将使用它来查看在不同`k`值下对该数据的聚类结果。数据点在[图25-10](#c25-fig-0012)中显示。
- en: '![c25-fig-0009.jpg](../images/c25-fig-0009.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0009.jpg](../images/c25-fig-0009.jpg)'
- en: '[Figure 25-9](#c25-fig-0011a) Generating points from three distributions'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图25-9](#c25-fig-0011a) 从三个分布生成点'
- en: '![c25-fig-0010.jpg](../images/c25-fig-0010.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0010.jpg](../images/c25-fig-0010.jpg)'
- en: '[Figure 25-10](#c25-fig-0012a) Points from three overlapping Gaussians'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[图25-10](#c25-fig-0012a) 来自三个重叠高斯的点'
- en: The invocation `contrived_test2(40, 2)` prints
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`contrived_test2(40, 2)`打印
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The invocation `contrived_test2(40, 3)` prints
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`contrived_test2(40, 3)`打印
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And the invocation `contrived_test2(40, 6)` prints
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`contrived_test2(40, 6)`打印
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The last clustering is the tightest fit, i.e., the clustering has the lowest
    dissimilarity (`11.441`). Does this mean that it is the “best” clustering? Not
    necessarily. Recall that when we looked at linear regression in Section 20.1.1,
    we observed that by increasing the degree of the polynomial we got a more complex
    model that provided a tighter fit to the data. We also observed that when we increased
    the degree of the polynomial, we ran the risk of finding a model with poor predictive
    value—because it overfit the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的聚类是最紧密的拟合，即聚类的不相似度最低（`11.441`）。这是否意味着这是“最佳”聚类？不一定。回想一下我们在20.1.1节中观察到的线性回归，通过增加多项式的次数，我们得到了一个更复杂的模型，从而更紧密地拟合了数据。我们还观察到，当我们增加多项式的次数时，我们有可能找到一个预测值较差的模型——因为它过拟合了数据。
- en: Choosing the right value for `k` is exactly analogous to choosing the right
    degree polynomial for a linear regression. By increasing `k`, we can decrease
    dissimilarity, at the risk of overfitting. (When `k` is equal to the number of
    examples to be clustered, the dissimilarity is 0!) If we have information about
    how the examples to be clustered were generated, e.g., chosen from `m` distributions,
    we can use that information to choose `k`. Absent such information, there are
    a variety of heuristic procedures for choosing `k`. Going into them is beyond
    the scope of this book.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的`k`值与为线性回归选择合适的多项式次数完全类似。通过增加`k`，我们可以减少不相似度，但有过拟合的风险。（当`k`等于待聚类样本数量时，不相似度为0！）如果我们知道待聚类样本的生成方式，例如从`m`个分布中选择，我们可以利用这些信息来选择`k`。在缺乏此类信息的情况下，有多种启发式方法可以选择`k`。深入讨论这些超出了本书的范围。
- en: 25.4 A Less Contrived Example
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.4 一个不那么复杂的例子
- en: Different species of mammals have different eating habits. Some species (e.g.,
    elephants and beavers) eat only plants, others (e.g., lions and tigers) eat only
    meat, and some (e.g., pigs and humans) eat anything they can get into their mouths.
    The vegetarian species are called herbivores, the meat eaters are called carnivores,
    and those species that eat both plants and animals are called omnivores.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不同物种的哺乳动物有不同的饮食习惯。一些物种（例如，大象和海狸）只吃植物，其他物种（例如，狮子和老虎）只吃肉，还有一些物种（例如，猪和人类）则吃任何能放进嘴里的东西。素食物种称为草食动物，肉食动物称为肉食动物，而那些既吃植物又吃动物的物种称为杂食动物。
- en: Over the millennia, evolution (or, if you prefer, some other mysterious process)
    has equipped species with teeth suitable for consumption of their preferred foods.[^(194)](#c25-fn-0004)
    That raises the question of whether clustering mammals based on their dentition
    produces clusters that have some relation to their diets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在千百年的演化过程中（或者，如果你愿意，可以认为是某种神秘的过程），物种的牙齿被赋予了适合其偏好食物的形态。[^(194)](#c25-fn-0004)
    这引出了一个问题，即基于哺乳动物的牙齿结构进行聚类是否会产生与其饮食相关的聚类。
- en: '[Figure 25-11](#c25-fig-0013) shows the contents of a file listing some species
    of mammals, their dental formulas (the first `8` numbers), and their average adult
    weight in pounds.[^(195)](#c25-fn-0005) The comments at the top describe the items
    associated with each mammal, e.g., the first item following the name is the number
    of top incisors.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-11](#c25-fig-0013) 显示了一个文件的内容，列出了某些哺乳动物的物种、其牙齿公式（前`8`个数字）以及其平均成年体重（磅）。[^(195)](#c25-fn-0005)
    文件顶部的注释描述了与每种哺乳动物相关的项目，例如，名称后第一个项目是顶端门牙的数量。'
- en: '![c25-fig-0011.jpg](../images/c25-fig-0011.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0011.jpg](../images/c25-fig-0011.jpg)'
- en: '[Figure 25-11](#c25-fig-0013a) Mammal dentition in `dentalFormulas.csv`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-11](#c25-fig-0013a) 哺乳动物的牙齿结构在`dentalFormulas.csv`中'
- en: '[Figure 25-12](#c25-fig-0014) contains three functions. The function `read_mammal_data`
    first reads a CSV file, formatted like the one in [Figure 25-11](#c25-fig-0013),
    to create a DataFrame. The keyword argument `comment` is used to instruct `read_csv`
    to ignore lines starting with `#`. If the parameter `scale_method` is not equal
    to `None`, it then scales each column in the DataFrame using `scale_method`. Finally,
    it creates and returns a dictionary mapping species names to feature vectors.
    The function `build_mammal_examples` uses the dictionary returned by `read_mammal_data`
    to produce and return a set of examples. The function `test_teeth` produces and
    prints a clustering.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-12](#c25-fig-0014)包含三个函数。`read_mammal_data`函数首先读取一个CSV文件，格式如[图 25-11](#c25-fig-0013)所示，以创建一个数据框。关键字参数`comment`用于指示`read_csv`忽略以`#`开头的行。如果参数`scale_method`不等于`None`，则使用`scale_method`缩放数据框中的每一列。最后，它创建并返回一个将物种名称映射到特征向量的字典。`build_mammal_examples`函数使用`read_mammal_data`返回的字典生成并返回一组示例。`test_teeth`函数生成并打印聚类。'
- en: '![c25-fig-0012.jpg](../images/c25-fig-0012.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0012.jpg](../images/c25-fig-0012.jpg)'
- en: '[Figure 25-12](#c25-fig-0014a) Read and process CSV file'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-12](#c25-fig-0014) 读取和处理CSV文件'
- en: "The call `\uFEFFtest_teeth('dentalFormulas.csv', 3, 40)` prints"
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`test_teeth('dentalFormulas.csv', 3, 40)`打印
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A cursory inspection suggests that we have a clustering totally dominated by
    the weights of the animals. The problem is that the range of weights is much larger
    than the range of any of the other features. Therefore, when the Euclidean distance
    between examples is computed, the only feature that truly matters is weight.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略检查表明，我们的聚类完全被动物的体重主导。问题在于，体重的范围远大于其他任何特征的范围。因此，当计算样本之间的欧几里得距离时，唯一真正重要的特征是体重。
- en: We encountered a similar problem in Section 24.2 when we found that the distance
    between animals was dominated by the number of legs. We solved the problem there
    by turning the number of legs into a binary feature (legged or legless). That
    was fine for that data set, because all of the animals happened to have either
    zero or four legs. Here, however, there is no obvious way to turn weight into
    a single binary feature without losing a great deal of information.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第24.2节遇到过类似的问题，当时发现动物之间的距离主要由腿的数量主导。我们通过将腿的数量转换为二元特征（有腿或无腿）来解决了那个问题。那对于该数据集是可行的，因为所有动物的腿数恰好是零或四条。然而，在这里，毫无疑问地将体重转换为单一二元特征而不损失大量信息是不现实的。
- en: This is a common problem, which is often addressed by scaling the features so
    that each feature has a mean of `0` and a standard deviation of `1`,[^(196)](#c25-fn-0006)
    as done by the function `z_scale` in [Figure 25-13](#c25-fig-0015). It's easy
    to see why the statement `result = result - mean` ensures that the mean of the
    returned array will always be close to `0`.[^(197)](#c25-fn-0007) That the standard
    deviation will always be `1` is not obvious. It can be shown by a long and tedious
    chain of algebraic manipulations, which we will not bore you with. This kind of
    scaling is often called **z-scaling** because the standard normal distribution
    is sometimes referred to as the Z-distribution.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常见的问题，通常通过对特征进行缩放来解决，使每个特征的均值为`0`，标准差为`1`，[^(196)](#c25-fn-0006)，就像在[图 25-13](#c25-fig-0015)中`z_scale`函数所做的那样。很容易看出，语句`result
    = result - mean`确保返回数组的均值总是接近`0`。[^(197)](#c25-fn-0007) 标准差总是为`1`并不明显，可以通过一系列繁琐的代数变换证明，但我们不想让你感到乏味。这种缩放通常被称为**z-缩放**，因为标准正态分布有时被称为Z分布。
- en: Another common approach to scaling is to map the minimum feature value to `0`,
    map the maximum feature value to `1`, and use **linear scaling** in between, as
    done by the function `linear_scale` in [Figure 25-13](#c25-fig-0015). This is
    often called **min-max scaling**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的缩放方法是将最小特征值映射到`0`，将最大特征值映射到`1`，并在其间使用**线性缩放**，就像在[图 25-13](#c25-fig-0015)中`linear_scale`函数所做的那样。这通常被称为**最小-最大缩放**。
- en: '![c25-fig-0013.jpg](../images/c25-fig-0013.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0013.jpg](../images/c25-fig-0013.jpg)'
- en: '[Figure 25-13](#c25-fig-0015a) Scaling attributes'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-13](#c25-fig-0015a) 缩放属性'
- en: "The call \uFEFF`test_teeth('dentalFormulas.csv', 3, 40, z_scale)` prints"
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`test_teeth('dentalFormulas.csv', 3, 40, z_scale)`打印
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It's not immediately obvious how this clustering relates to the features associated
    with each of these mammals, but at least it is not merely grouping the mammals
    by weight.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚类如何与这些哺乳动物相关的特征并不明显，但至少它并不是仅仅通过体重对哺乳动物进行分组。
- en: Recall that we started this section by hypothesizing that there was a relationship
    between a mammal's dentition and its diet. [Figure 25-14](#c25-fig-0016) contains
    an excerpt of a CSV file, `diet.csv`, that associates mammals with their dietary
    preference.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们在本节开始时假设哺乳动物的牙齿特征与其饮食之间存在关系。 [图 25-14](#c25-fig-0016)包含了一个CSV文件`diet.csv`的摘录，关联了哺乳动物及其饮食偏好。
- en: '![c25-fig-0014.jpg](../images/c25-fig-0014.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0014.jpg](../images/c25-fig-0014.jpg)'
- en: '[Figure 25-14](#c25-fig-0016a) Start of CSV file classifying mammals by diet'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-14](#c25-fig-0016a) CSV文件的开始，用于按饮食分类哺乳动物'
- en: We can use information in `diet.csv` to see to what extent the clusterings we
    have produced are related to diet. The code in [Figure 25-15](#c25-fig-0017) does
    exactly that.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`diet.csv`中的信息来看我们生成的聚类与饮食之间的关系。 [图 25-15](#c25-fig-0017)中的代码正是这样做的。
- en: '![c25-fig-0015.jpg](../images/c25-fig-0015.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![c25-fig-0015.jpg](../images/c25-fig-0015.jpg)'
- en: '[Figure 25-15](#c25-fig-0017a) Relating clustering to labels'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 25-15](#c25-fig-0017a) 将聚类与标签相关联'
- en: "When `\uFEFFtest_teeth_diet('dentalFormulas.csv', ‘diet.csv', 3, 40, z_scale)`\
    \ was run, it printed"
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: "当运行`\uFEFFtest_teeth_diet('dentalFormulas.csv', ‘diet.csv', 3, 40, z_scale)`时，它打印了"
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The clustering with z-scaling (linear scaling yields the same clusters) does
    not perfectly partition the animals based upon their eating habits, but it is
    certainly correlated with what they eat. It does a good job of separating the
    carnivores from the herbivores, but there is no obvious pattern in where the omnivores
    appear. This suggests that perhaps features other than dentition and weight might
    be needed to separate omnivores from herbivores and carnivores.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用z缩放的聚类（线性缩放产生相同的聚类）并没有完美地根据动物的饮食习惯进行划分，但它与它们的饮食确实相关。它很好地将食肉动物与草食动物分开，但杂食动物出现的地方没有明显的模式。这表明，除了牙齿和体重外，可能还需要其他特征来区分杂食动物与草食动物和食肉动物。
- en: 25.5 Terms Introduced in Chapter
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 25.5 本章引入的术语
- en: Euclidean mean
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得均值
- en: dissimilarity
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不相似性
- en: centroid
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重心
- en: k-means clustering
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值聚类
- en: standard normal distribution
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准正态分布
- en: z-scaling
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: z缩放
- en: linear scaling
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性缩放
- en: min-max scaling
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小-最大缩放
- en: linear interpolation
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性插值
