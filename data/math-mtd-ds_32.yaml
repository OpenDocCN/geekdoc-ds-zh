- en: '4.6\. Further applications of the SVD: low-rank approximations and ridge regression#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.6\. SVD的进一步应用：低秩逼近和岭回归#
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html](https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html](https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html)
- en: In this section, we discuss further properties of the SVD. We first introduce
    additional matrix norms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进一步讨论SVD的性质。我们首先介绍额外的矩阵范数。
- en: 4.6.1\. Matrix norms[#](#matrix-norms "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6.1\. 矩阵范数[#](#matrix-norms "链接到本标题")
- en: Recall that the Frobenius norm\(\idx{Frobenius norm}\xdi\) of an \(n \times
    m\) matrix \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) is defined as
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，\(n \times m\)矩阵\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)的Frobenius范数\(\idx{Frobenius
    norm}\xdi\)定义为
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}. \]
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_F = \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}. \]
- en: Here we introduce a different notion of matrix norm that has many uses in data
    science (and beyond).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了一种不同的矩阵范数概念，它在数据科学（及其他领域）中有许多用途。
- en: '**Induced norm** The Frobenius norm does not directly relate to \(A\) as a
    representation of a [linear map](https://en.wikipedia.org/wiki/Linear_map). In
    particular, it is desirable in many contexts to quantify how two matrices differ
    in terms of how they act on vectors. For instance, one is often interested in
    bounding quantities of the following form. Let \(B, B'' \in \mathbb{R}^{n \times
    m}\) and let \(\mathbf{x} \in \mathbb{R}^m\) be of unit norm. What can be said
    about \(\|B \mathbf{x} - B'' \mathbf{x}\|\)? Intuitively, what we would like is
    this: if the norm of \(B - B''\) is small then \(B\) is close to \(B''\) as a
    linear map, that is, the vector norm \(\|B \mathbf{x} - B'' \mathbf{x}\|\) is
    small for any unit vector \(\mathbf{x}\). The following definition provides us
    with such a notion. Define the unit sphere \(\mathbb{S}^{m-1} = \{\mathbf{x} \in
    \mathbb{R}^m\,:\,\|\mathbf{x}\| = 1\}\) in \(m\) dimensions.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**诱导范数** Frobenius范数并不直接与\(A\)作为[线性映射](https://en.wikipedia.org/wiki/Linear_map)的代表相关。在许多情况下，我们希望量化两个矩阵在如何作用于向量方面的差异。例如，人们通常对以下形式的量感兴趣。设\(B,
    B'' \in \mathbb{R}^{n \times m}\)，设\(\mathbf{x} \in \mathbb{R}^m\)为单位范数。关于\(\|B
    \mathbf{x} - B'' \mathbf{x}\|\)可以说什么？直观上，我们希望的是：如果\(B - B''\)的范数很小，那么\(B\)在作为线性映射时接近\(B''\)，也就是说，对于任何单位向量\(\mathbf{x}\)，向量范数\(\|B
    \mathbf{x} - B'' \mathbf{x}\|\)都很小。以下定义为我们提供了这样的概念。定义\(m\)维度的单位球\(\mathbb{S}^{m-1}
    = \{\mathbf{x} \in \mathbb{R}^m\,:\,\|\mathbf{x}\| = 1\}\)。'
- en: '**DEFINITION** **(\(2\)-Norm)** The \(2\)-norm of a matrix\(\idx{2-norm}\xdi\)
    \(A \in \mathbb{R}^{n \times m}\) is'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(\(2\)-范数)** 矩阵\(A \in \mathbb{R}^{n \times m}\)的\(2\)-范数\(\idx{2-norm}\xdi\)定义为'
- en: \[ \|A\|_2 := \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|}
    = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|. \]
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_2 := \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|}
    = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|. \]
- en: \(\natural\)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: The equality in the definition uses the absolute homogeneity of the vector norm.
    Also the definition implicitly uses the *Extreme Value Theorem*. In this case,
    we use the fact that the function \(f(\mathbf{x}) = \|A \mathbf{x}\|\) is continuous
    and the set \(\mathbb{S}^{m-1}\) is closed and bounded to conclude that there
    exists \(\mathbf{x}^* \in \mathbb{S}^{m-1}\) such that \(f(\mathbf{x}^*) \geq
    f(\mathbf{x})\) for all \(\mathbf{x} \in \mathbb{S}^{m-1}\).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 定义中的等式使用了向量范数的绝对齐次性。同时，定义隐含地使用了*极值定理*。在这种情况下，我们使用函数\(f(\mathbf{x}) = \|A \mathbf{x}\|\)是连续的，集合\(\mathbb{S}^{m-1}\)是闭且有界的这一事实，得出存在\(\mathbf{x}^*
    \in \mathbb{S}^{m-1}\)，使得对于所有\(\mathbf{x} \in \mathbb{S}^{m-1}\)，有\(f(\mathbf{x}^*)
    \geq f(\mathbf{x})\)。
- en: The \(2\)-norm of a matrix has many other useful properties. The first four
    below are what makes it a [norm](https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的\(2\)-范数具有许多其他有用的性质。下面列出的前四个性质是使其成为[范数](https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries)的原因。
- en: '**LEMMA** **(Properties of the \(2\)-Norm)** Let \(A, B \in \mathbb{R}^{n \times
    m}\) and \(\alpha \in \mathbb{R}\). The following hold:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(\(2\)-范数的性质)** 设\(A, B \in \mathbb{R}^{n \times m}\)且\(\alpha \in
    \mathbb{R}\)。以下性质成立：'
- en: a) \(\|A\|_2 \geq 0\)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\|A\|_2 \geq 0\)
- en: b) \(\|A\|_2 = 0\) if and only if \(A = 0\)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\|A\|_2 = 0\)当且仅当\(A = 0\)
- en: c) \(\|\alpha A\|_2 = |\alpha| \|A\|_2\)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\|\alpha A\|_2 = |\alpha| \|A\|_2\)
- en: d) \(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)
- en: e) \(\|A B \|_2 \leq \|A\|_2 \|B\|_2\).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: e) \(\|A B \|_2 \leq \|A\|_2 \|B\|_2\).
- en: f) \(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\), \(\forall \mathbf{0} \neq
    \mathbf{x} \in \mathbb{R}^m\)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: f) \(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\), \(\forall \mathbf{0} \neq
    \mathbf{x} \in \mathbb{R}^m\)
- en: \(\flat\)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* These properties all follow from the definition of the \(2\)-norm
    and the corresponding properties for the vector norm:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 这些性质都遵循 \(2\)-范数的定义以及向量范数的相应性质：'
- en: Claims a) and f) are immediate from the definition.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明 a) 和 f) 是直接的。
- en: For b) note that \(\|A\|_2 = 0\) implies \(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\), so that \(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\). In particular, \(a_{ij} = \mathbf{e}_i^T A \mathbf{e}_j
    = 0, \forall i,j\).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 b)，注意 \(\|A\|_2 = 0\) 意味着 \(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x} \in
    \mathbb{S}^{m-1}\)，因此 \(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x} \in \mathbb{S}^{m-1}\)。特别是，\(a_{ij}
    = \mathbf{e}_i^T A \mathbf{e}_j = 0, \forall i,j\).
- en: For c), d), e), observe that for all \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 c)，d)，e)，观察对于所有 \(\mathbf{x} \in \mathbb{S}^{m-1}\)
- en: \[ \|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|, \]\[\|(A+B)\mathbf{x}\|
    = \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\| \leq \|A\|_2
    + \|B\|_2 \]\[ \|(AB)\mathbf{x}\| = \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
    \leq \|A\|_2 \|B\|_2.\]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|, \]\[\|(A+B)\mathbf{x}\|
    = \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\| \leq \|A\|_2
    + \|B\|_2 \]\[ \|(AB)\mathbf{x}\| = \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
    \leq \|A\|_2 \|B\|_2.\]
- en: Then apply the definition of \(2\)-norm. For example, for ©,
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后应用 \(2\)-范数的定义。例如，对于 ©，
- en: \[\begin{align*} \|\alpha A\|_2 &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha
    A \mathbf{x}\|\\ &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
    &= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\ &= |\alpha|
    \|A\|_2, \end{align*}\]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|\alpha A\|_2 &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha
    A \mathbf{x}\|\\ &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
    &= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\ &= |\alpha|
    \|A\|_2, \end{align*}\]
- en: where we used that \(|\alpha|\) does not depend on \(\mathbf{x}\). \(\square\)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(|\alpha|\) 不依赖于 \(\mathbf{x}\)。\(\square\)
- en: '**NUMERICAL CORNER:** In NumPy, the Frobenius norm of a matrix can be computed
    using the default of the function `numpy.linalg.norm` while the induced norm can
    be computed using the same function with [`ord` parameter set to `2`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落:** 在 NumPy 中，可以使用函数 `numpy.linalg.norm` 的默认值来计算矩阵的 Frobenius 范数，而诱导范数可以通过将相同的函数的
    `ord` 参数设置为 `2` 来计算（[链接](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)）。'
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: \(\unlhd\)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Matrix norms and SVD** As it turns out, the two notions of matrix norms we
    have introduced admit simple expressions in terms of the singular values of the
    matrix.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵范数与 SVD** 如我们所见，我们引入的两个矩阵范数的概念可以用矩阵的奇异值简单地表示。'
- en: '**LEMMA** **(Matrix Norms and Singular Values)** \(\idx{matrix norms and singular
    values lemma}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with compact
    SVD'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(矩阵范数与奇异值)** \(\idx{matrix norms and singular values lemma}\xdi\)
    设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有紧奇异值分解的矩阵'
- en: \[ A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T \]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T \]
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\). Then
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中回忆一下 \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\)。然后
- en: \[ \|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2 \]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2 \]
- en: and
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[ \|A\|^2_2 = \sigma_{1}^2. \]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|^2_2 = \sigma_{1}^2. \]
- en: \(\flat\)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* We will use the notation \(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\).
    Using that the squared Frobenius norm of \(A\) is the sum of the squared norms
    of its columns, we have'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们将使用符号 \(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\)。利用 \(A\)
    的平方 Frobenius 范数是其列的平方范数的和，我们有'
- en: \[ \|A\|^2_F = \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
    = \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell \right\|^2.
    \]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|^2_F = \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
    = \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell \right\|^2.
    \]
- en: Because the \(\mathbf{u}_\ell\)’s are orthonormal, this is
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathbf{u}_\ell\) 是正交归一的，所以这是
- en: \[ \sum_{j=1}^m \sum_{\ell=1}^r \sigma_\ell^2 v_{\ell,j}^2 = \sum_{\ell=1}^r
    \sigma_\ell^2 \left(\sum_{j=1}^m v_{\ell,j}^2\right) = \sum_{\ell=1}^r \sigma_\ell^2
    \|\mathbf{v}_{\ell}\|^2 = \sum_{\ell=1}^r \sigma_\ell^2, \]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^m \sum_{\ell=1}^r \sigma_\ell^2 v_{\ell,j}^2 = \sum_{\ell=1}^r
    \sigma_\ell^2 \left(\sum_{j=1}^m v_{\ell,j}^2\right) = \sum_{\ell=1}^r \sigma_\ell^2
    \|\mathbf{v}_{\ell}\|^2 = \sum_{\ell=1}^r \sigma_\ell^2, \]
- en: where we used that the \(\mathbf{v}_\ell\)’s are also orthonormal.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\mathbf{v}_\ell\) 也是正交归一的。
- en: For the second claim, recall that the \(2\)-norm is defined as
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个断言，回忆一下，\(2\)范数定义为
- en: \[ \|A\|_2^2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2. \]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_2^2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2. \]
- en: We have shown previously that \(\mathbf{v}_1\) solves this problem. Hence \(\|A\|_2^2
    = \|A \mathbf{v}_1\|^2 = \sigma_1^2\). \(\square\)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经证明\(\mathbf{v}_1\)解决了这个问题。因此\(\|A\|_2^2 = \|A \mathbf{v}_1\|^2 = \sigma_1^2\)。\(\square\)
- en: 4.6.2\. Low-rank approximation[#](#low-rank-approximation "Link to this heading")
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6.2\. 低秩近似[#](#low-rank-approximation "链接到这个标题")
- en: Now that we have defined a notion of distance between matrices, we will consider
    the problem of finding a good approximation to a matrix \(A\) among all matrices
    of rank at most \(k\). We will start with the Frobenius norm, which is easier
    to work with, and we will show later on that the solution is the same under the
    induced norm. The solution to this problem will be familiar. In essence, we will
    re-interpret our solution to the best approximating subspace as a low-rank approximation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了矩阵之间距离的概念，我们将考虑在所有秩最多为\(k\)的矩阵中寻找矩阵\(A\)的良好近似的问题。我们将从更容易处理的Frobenius范数开始，稍后我们将证明在诱导范数下解是相同的。这个问题的解将是熟悉的。本质上，我们将重新解释我们对于最佳逼近子空间的解作为低秩近似。
- en: '**Low-rank approximation in the Frobenius norm** \(\idx{low-rank approximation}\xdi\)
    From the proof of the *Row Rank Equals Column Rank Lemma*, it follows that a rank-\(r\)
    matrix \(A\) can be written as a sum of \(r\) rank-\(1\) matrices'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Frobenius范数下的低秩近似** \(\idx{low-rank approximation}\xdi\) 从 *行秩等于列秩引理* 的证明中可以得出，秩为\(r\)的矩阵\(A\)可以表示为\(r\)个秩为\(1\)的矩阵之和'
- en: \[ A = \sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T. \]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T. \]
- en: We will now consider the problem of finding a “simpler” approximation to \(A\)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考虑寻找\(A\)的“更简单”近似的问题
- en: \[ A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T \]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T \]
- en: where \(k < r\). Here we measure the quality of this approximation using a matrix
    norm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(k < r\)。在这里，我们使用矩阵范数来衡量这个近似的质量。
- en: We are ready to state our key observation. In words, the best rank-\(k\) approximation
    to \(A\) in Frobenius norm is obtained by projecting the rows of \(A\) onto a
    linear subspace of dimension \(k\). We will come back to how one finds the best
    such subspace below. (*Hint:* We have already solved this problem.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备陈述我们的关键观察。换句话说，在Frobenius范数下，\(A\)的最佳秩\(k\)近似是通过将\(A\)的行投影到维度为\(k\)的线性子空间中获得的。我们将在下面回到如何找到这样的最佳子空间。(*提示:*
    我们已经解决了这个问题。)
- en: '**LEMMA** **(Projection and Rank-\(k\) Approximation)** \(\idx{projection and
    rank-k approximation lemma}\xdi\) Let \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n
    \times m}\). For any matrix \(B = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)
    of rank \(k \leq \min\{n,m\}\),'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(投影和秩-\(k\)近似)** \(\idx{projection and rank-k approximation lemma}\xdi\)
    设\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)。对于任何秩\(k \leq \min\{n,m\}\)的矩阵\(B
    = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)，'
- en: \[ \|A - B_{\perp}\|_F \leq \|A - B\|_F \]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - B_{\perp}\|_F \leq \|A - B\|_F \]
- en: where \(B_{\perp} \in \mathbb{R}^{n \times m}\) is the matrix of rank at most
    \(k\) obtained as follows. Denote row \(i\) of \(A\), \(B\) and \(B_{\perp}\)
    respectively by \(\boldsymbol{\alpha}_i^T\), \(\mathbf{b}_{i}^T\) and \(\mathbf{b}_{\perp,i}^T\),
    \(i=1,\ldots, n\). Set \(\mathbf{b}_{\perp,i}\) to be the orthogonal projection
    of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z} = \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\).
    \(\flat\)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(B_{\perp} \in \mathbb{R}^{n \times m}\)是秩最多为\(k\)的矩阵，其获得方式如下。分别用\(\boldsymbol{\alpha}_i^T\)、\(\mathbf{b}_{i}^T\)和\(\mathbf{b}_{\perp,i}^T\)表示\(A\)、\(B\)和\(B_{\perp}\)的行，\(i=1,\ldots,
    n\)。将\(\mathbf{b}_{\perp,i}\)设置为\(\boldsymbol{\alpha}_i\)在\(\mathcal{Z} = \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\)上的正交投影。\(\flat\)
- en: '*Proof idea:* The square of the Frobenius norm decomposes as a sum of squared
    row norms. Each term in the sum is minimized by the orthogonal projection.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* Frobenius范数的平方可以分解为平方行范数的和。和中的每一项都通过正交投影最小化。'
- en: '*Proof:* By definition of the Frobenius norm, we note that'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 根据Frobenius范数的定义，我们注意到'
- en: \[ \|A - B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 = \sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 \]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 = \sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 \]
- en: 'and similarly for \(\|A - B_{\perp}\|_F\). We make two observations:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地对于\(\|A - B_{\perp}\|_F\)。我们做出两个观察：
- en: Because the orthogonal projection of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z}\)
    minimizes the distance to \(\mathcal{Z}\), it follows that term by term \(\|\boldsymbol{\alpha}_i
    - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|\) so
    that
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为 \(\boldsymbol{\alpha}_i\) 在 \(\mathcal{Z}\) 上的正交投影最小化了到 \(\mathcal{Z}\) 的距离，所以逐项
    \(\|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i
    - \mathbf{b}_{i}\|\)，因此
- en: \[ \|A - B_\perp\|_F^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
    \leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 = \|A - B\|_F^2.
    \]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - B_\perp\|_F^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
    \leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 = \|A - B\|_F^2.
    \]
- en: Moreover, because the projections satisfy \(\mathbf{b}_{\perp,i} \in \mathcal{Z}\)
    for all \(i\), \(\mathrm{row}(B_\perp) \subseteq \mathrm{row}(B)\) and, hence,
    the rank of \(B_\perp\) is at most the rank of \(B\).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，因为投影满足 \(\mathbf{b}_{\perp,i} \in \mathcal{Z}\) 对于所有 \(i\)，\(\mathrm{row}(B_\perp)
    \subseteq \mathrm{row}(B)\) 并且因此 \(B_\perp\) 的秩至多是 \(B\) 的秩。
- en: That concludes the proof. \(\square\)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了证明。 \(\square\)
- en: Recall the approximating subspace problem. That is, think of the rows \(\boldsymbol{\alpha}_i^T\)
    of \(A \in \mathbb{R}^{n \times m}\) as a collection of \(n\) data points in \(\mathbb{R}^m\).
    We are looking for a linear subspace \(\mathcal{Z}\) that minimizes \(\sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)
    over all linear subspaces of \(\mathbb{R}^m\) of dimension at most \(k\). By the
    *Projection and Rank-\(k\) Approximation Lemma*, this problem is equivalent to
    finding a matrix \(B\) that minimizes \(\|A - B\|_F\) among all matrices in \(\mathbb{R}^{n
    \times m}\) of rank at most \(k\). Of course we have solved this problem before.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下逼近子空间问题。也就是说，将 \(A \in \mathbb{R}^{n \times m}\) 的行 \(\boldsymbol{\alpha}_i^T\)
    视为 \(\mathbb{R}^m\) 中的 \(n\) 个数据点的集合。我们正在寻找一个线性子空间 \(\mathcal{Z}\)，它在所有维度至多为 \(k\)
    的 \(\mathbb{R}^m\) 线性子空间中，最小化 \(\sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)。根据**投影和秩-\(k\)
    逼近引理**，这个问题等价于找到一个矩阵 \(B\)，它在所有秩至多为 \(k\) 的 \(\mathbb{R}^{n \times m}\) 矩阵中，最小化
    \(\|A - B\|_F\)。当然，我们之前已经解决了这个问题。
- en: Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). For \(k < r\), truncate the sum at the
    \(k\)-th term \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). The
    rank of \(A_k\) is exactly \(k\). Indeed, by construction,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有奇异值分解 \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) 的矩阵。对于 \(k < r\)，截断和到第 \(k\) 项 \(A_k = \sum_{j=1}^k \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\)。\(A_k\) 的秩正好是 \(k\)。实际上，通过构造，
- en: the vectors \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) are orthonormal, and
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量 \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) 是正交归一的，并且
- en: since \(\sigma_j > 0\) for \(j=1,\ldots,k\) and the vectors \(\{\mathbf{v}_j\,:\,j
    = 1,\ldots,k\}\) are orthonormal, \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) spans
    the column space of \(A_k\).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 \(\sigma_j > 0\) 对于 \(j=1,\ldots,k\)，并且向量 \(\{\mathbf{v}_j\,:\,j = 1,\ldots,k\}\)
    是正交归一的，\(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) 张成 \(A_k\) 的列空间。
- en: We have shown before that \(A_k\) is the best approximation to \(A\) among matrices
    of rank at most \(k\) in Frobenius norm. Specifically, the *Greedy Finds Best
    Fit Theorem* implies that, for any matrix \(B \in \mathbb{R}^{n \times m}\) of
    rank at most \(k\),
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经证明 \(A_k\) 是在 Frobenius 范数下，在秩至多为 \(k\) 的矩阵中 \(A\) 的最佳逼近。具体来说，**贪婪最佳拟合定理**意味着，对于任何秩至多为
    \(k\) 的矩阵 \(B \in \mathbb{R}^{n \times m}\)，
- en: \[ \|A - A_k\|_F \leq \|A - B\|_F. \]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|_F \leq \|A - B\|_F. \]
- en: This result is known as the *Eckart-Young Theorem*\(\idx{Eckart-Young theorem}\xdi\).
    It also holds in the induced \(2\)-norm, as we show next.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果被称为**Eckart-Young 定理**(\(\idx{Eckart-Young theorem}\xdi\))。它也适用于诱导的 \(2\)-范数，正如我们接下来要展示的。
- en: '**Low-rank approximation in the induced norm** We show in this section that
    the same holds in the induced norm. First, some observations.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**诱导范数中的低秩逼近** 在本节中，我们展示在诱导范数中同样成立。首先，一些观察。'
- en: '**LEMMA** **(Matrix Norms and Singular Values: Truncation)** Let \(A \in \mathbb{R}^{n
    \times m}\) be a matrix with SVD'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(矩阵范数和奇异值：截断)** 设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有奇异值分解的矩阵。'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\) and let
    \(A_k\) be the truncation defined above. Then
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，回想一下 \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\)，并让 \(A_k\) 是上面定义的截断。然后
- en: \[ \|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2 \]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2 \]
- en: and
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \|A - A_k\|^2_2 = \sigma_{k+1}^2. \]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|^2_2 = \sigma_{k+1}^2. \]
- en: \(\flat\)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* For the first claim, by definition, summing over the columns of \(A
    - A_k\)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 对于第一个断言，根据定义，对 \(A - A_k\) 的列求和'
- en: \[ \|A - A_k\|^2_F = \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
    = \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j \right\|^2.
    \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|^2_F = \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
    = \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j \right\|^2.
    \]
- en: Because the \(\mathbf{u}_j\)’s are orthonormal, this is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathbf{u}_j\) 是正交归一的，所以这是
- en: \[ \sum_{i=1}^m \sum_{j=k+1}^r \sigma_j^2 v_{j,i}^2 = \sum_{j=k+1}^r \sigma_j^2
    \left(\sum_{i=1}^m v_{j,i}^2\right) = \sum_{j=k+1}^r \sigma_j^2 \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^m \sum_{j=k+1}^r \sigma_j^2 v_{j,i}^2 = \sum_{j=k+1}^r \sigma_j^2
    \left(\sum_{i=1}^m v_{j,i}^2\right) = \sum_{j=k+1}^r \sigma_j^2 \]
- en: where we used that the \(\mathbf{v}_j\)’s are also orthonormal.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\mathbf{v}_j\) 也是正交归一的。
- en: For the second claim, recall that the induced norm is defined as
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个断言，回忆一下诱导范数的定义
- en: \[ \|B\|_2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|. \]
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|B\|_2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|. \]
- en: For any \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 \(\mathbf{x} \in \mathbb{S}^{m-1}\)
- en: \[ \left\|(A - A_k)\mathbf{x}\right\|^2 = \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j
    (\mathbf{v}_j^T \mathbf{x}) \right\|^2 = \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j,
    \mathbf{x}\rangle^2. \]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left\|(A - A_k)\mathbf{x}\right\|^2 = \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j
    (\mathbf{v}_j^T \mathbf{x}) \right\|^2 = \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j,
    \mathbf{x}\rangle^2. \]
- en: Because the \(\sigma_j\)’s are in decreasing order, this is maximized when \(\langle
    \mathbf{v}_j, \mathbf{x}\rangle = 1\) if \(j=k+1\) and \(0\) otherwise. That is,
    we take \(\mathbf{x} = \mathbf{v}_{k+1}\) and the norm is then \(\sigma_{k+1}^2\),
    as claimed. \(\square\)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\sigma_j\) 是递减的，所以当 \(j=k+1\) 时，\(\langle \mathbf{v}_j, \mathbf{x}\rangle
    = 1\)，否则为 \(0\) 时，这是最大化的。也就是说，我们取 \(\mathbf{x} = \mathbf{v}_{k+1}\)，此时范数为 \(\sigma_{k+1}^2\)，正如所声称的。
    \(\square\)
- en: '**THEOREM** **(Low-Rank Approximation in the Induced Norm)** \(\idx{low-rank
    approximation in the induced norm theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times
    m}\) be a matrix with SVD'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(诱导范数中的低秩逼近)** \(\idx{low-rank approximation in the induced norm theorem}\xdi\)
    设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有奇异值分解的矩阵'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: and let \(A_k\) be the truncation defined above with \(k < r\). For any matrix
    \(B \in \mathbb{R}^{n \times m}\) of rank at most \(k\),
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 并令 \(A_k\) 为上面定义的截断，其中 \(k < r\)。对于任何秩最多为 \(k\) 的矩阵 \(B \in \mathbb{R}^{n \times
    m}\)
- en: \[ \|A - A_k\|_2 \leq \|A - B\|_2. \]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|_2 \leq \|A - B\|_2. \]
- en: \(\sharp\)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: '*Proof idea:* We know that \(\|A - A_k\|_2^2 = \sigma_{k+1}^2\). So we want
    to lower bound \(\|A - B\|_2^2\) by \(\sigma_{k+1}^2\). For that, we have to find
    an appropriate \(\mathbf{z}\) for any given \(B\) of rank at most \(k\). The idea
    is to take a vector \(\mathbf{z}\) in the intersection of the null space of \(B\)
    and the span of the singular vectors \(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\).
    By the former, the squared norm of \((A - B)\mathbf{z}\) is equal to the squared
    norm of \(A\mathbf{z}\) which lower bounds \(\|A\|_2^2\). By the latter, \(\|A
    \mathbf{z}\|^2\) is at least \(\sigma_{k+1}^2\).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们知道 \(\|A - A_k\|_2^2 = \sigma_{k+1}^2\). 因此，我们希望用 \(\sigma_{k+1}^2\)
    来下界 \(\|A - B\|_2^2\). 为了做到这一点，我们必须为任何给定的秩最多为 \(k\) 的 \(B\) 找到一个合适的 \(\mathbf{z}\).
    策略是取一个向量 \(\mathbf{z}\)，它在 \(B\) 的零空间和奇异向量 \(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\)
    的张成空间中。通过前者，\((A - B)\mathbf{z}\) 的平方范数等于 \(A\mathbf{z}\) 的平方范数，这为 \(\|A\|_2^2\)
    提供了下界。通过后者，\(\|A \mathbf{z}\|^2\) 至少为 \(\sigma_{k+1}^2\).'
- en: '*Proof:* By the *Rank-Nullity Theorem*, the dimension of \(\mathrm{null}(B)\)
    is at least \(m-k\) so there is a unit vector \(\mathbf{z}\) in the intersection'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过 *秩-零度定理*，\(\mathrm{null}(B)\) 的维度至少为 \(m-k\)，因此存在一个单位向量 \(\mathbf{z}\)
    在交集中'
- en: \[ \mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
    \]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
    \]
- en: (Prove it!) Then \((A-B)\mathbf{z} = A\mathbf{z}\) since \(\mathbf{z} \in \mathrm{null}(B)\).
    Also since \(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\),
    and therefore orthogonal to \(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\), we have
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (证明它!) 然后 \((A-B)\mathbf{z} = A\mathbf{z}\) 因为 \(\mathbf{z} \in \mathrm{null}(B)\).
    同时，由于 \(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\)，并且因此与
    \(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\) 正交，我们有
- en: \[\begin{align*} \|(A-B)\mathbf{z}\|^2 &= \|A\mathbf{z}\|^2\\ &= \left\|\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \left\|\sum_{j=1}^{k+1}
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \sum_{j=1}^{k+1}
    \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1}
    \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &= \sigma_{k+1}^2. \end{align*}\]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|(A-B)\mathbf{z}\|^2 &= \|A\mathbf{z}\|^2\\ &= \left\|\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \left\|\sum_{j=1}^{k+1}
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \sum_{j=1}^{k+1}
    \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1}
    \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &= \sigma_{k+1}^2. \end{align*}\]
- en: By the previous lemma, \(\sigma_{k+1}^2 = \|A - A_k\|_2\) and we are done. \(\square\)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的引理，\(\sigma_{k+1}^2 = \|A - A_k\|_2\)，这就完成了。 \(\square\)
- en: '**An application: Why project?** We return to \(k\)-means clustering and why
    projecting to a lower-dimensional subspace can produce better results. We prove
    a simple inequality that provides some insight. Quoting [BHK, Section 7.5.1]:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用：为什么进行投影？** 我们回到 \(k\) 均值聚类，并探讨为什么将投影到低维子空间可以产生更好的结果。我们证明一个简单的不等式，提供一些见解。引用
    [BHK, 第 7.5.1 节]：'
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[…] 让我们理解进行投影到[前 \(k\) 个右奇异向量]的中心优势。这很简单，对于任何合理的（未知的）数据点聚类，投影会使数据点更接近其簇中心。'
- en: To elaborate, suppose we have \(n\) data points in \(d\) dimensions in the form
    of the rows \(\boldsymbol{\alpha}_i^T\), \(i=1\ldots, n\), of matrix \(A \in \mathbb{A}^{n
    \times d}\), where we assume that \(n > d\) and that \(A\) has full column rank.
    Imagine these data points come from an unknown ground-truth \(k\)-clustering assignment
    \(g(i) \in [k]\), \(i = 1,\ldots, n\), with corresponding unknown centers \(\mathbf{c}_j\),
    \(j = 1,\ldots, k\). Let \(C \in \mathbb{R}^{n \times d}\) be the corresponding
    matrix, that is, row \(i\) of \(C\) is \(\mathbf{c}_j^T\) if \(g(i) = j\). The
    \(k\)-means objective of the true clustering is then
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了详细说明，假设我们有 \(n\) 个数据点在 \(d\) 维中，形式为矩阵 \(A \in \mathbb{A}^{n \times d}\) 的行
    \(\boldsymbol{\alpha}_i^T\)，\(i=1\ldots, n\)，其中我们假设 \(n > d\) 且 \(A\) 具有满列秩。想象这些数据点来自一个未知的真实
    \(k\) 聚类分配 \(g(i) \in [k]\)，\(i = 1,\ldots, n\)，以及相应的未知中心 \(\mathbf{c}_j\)，\(j
    = 1,\ldots, k\)。设 \(C \in \mathbb{R}^{n \times d}\) 为相应的矩阵，即如果 \(g(i) = j\)，则
    \(C\) 的第 \(i\) 行是 \(\mathbf{c}_j^T\)。真实聚类的 \(k\) 均值目标函数是
- en: \[\begin{align*} \sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
    &= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
    &= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\ &= \|A - C\|_F^2.
    \end{align*}\]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
    &= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
    &= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\ &= \|A - C\|_F^2.
    \end{align*}\]
- en: The matrix \(A\) has an SVD \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)
    and for \(k < r\) we have the truncation \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\). It corresponds to projecting each row of \(A\) onto the linear
    subspace spanned by the first \(k\) right singular vectors \(\mathbf{v}_1, \ldots,
    \mathbf{v}_k\). To see this, note that the \(i\)-th row of \(A\) is \(\boldsymbol{\alpha}_i^T
    = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\) and that, because the \(\mathbf{v}_j\)’s
    are linearly independent and in particular \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    is an orthonormal basis of its span, the projection of \(\boldsymbol{\alpha}_i\)
    onto \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) is
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(A\) 具有奇异值分解 \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)，对于
    \(k < r\)，我们有截断 \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)。这对应于将
    \(A\) 的每一行投影到由前 \(k\) 个右奇异向量 \(\mathbf{v}_1, \ldots, \mathbf{v}_k\) 张成的线性子空间。为了理解这一点，请注意
    \(A\) 的第 \(i\) 行是 \(\boldsymbol{\alpha}_i^T = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\)，并且由于
    \(\mathbf{v}_j\) 是线性无关的，特别是 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 是其张成的正交基，因此 \(\boldsymbol{\alpha}_i\)
    在 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) 上的投影是
- en: \[ \sum_{\ell=1}^k \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i}
    \mathbf{v}_j,\mathbf{v}_\ell\right\rangle = \sum_{\ell=1}^k \sigma_\ell u_{\ell,i}
    \mathbf{v}_\ell \]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{\ell=1}^k \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i}
    \mathbf{v}_j,\mathbf{v}_\ell\right\rangle = \sum_{\ell=1}^k \sigma_\ell u_{\ell,i}
    \mathbf{v}_\ell \]
- en: which is the \(i\)-th row of \(A_k\). The \(k\)-means objective of \(A_k\) with
    respect to the ground-truth centers \(\mathbf{c}_j\), \(j=1,\ldots,k\), is \(\|A_k
    - C\|_F^2\).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 \(A_k\) 的第 \(i\) 行。相对于真实中心 \(\mathbf{c}_j\)，\(j=1,\ldots,k\) 的 \(A_k\) 的
    \(k\) 均值目标是 \(\|A_k - C\|_F^2\)。
- en: 'One more observation: the rank of \(C\) is at most \(k\). Indeed, there are
    \(k\) different rows in \(C\) so its row rank is \(k\) if these different rows
    are linearly independent and less than \(k\) otherwise.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个额外的观察：\(C\) 的秩最多为 \(k\)。确实，\(C\) 中有 \(k\) 个不同的行，所以如果这些不同的行线性无关，则其行秩为 \(k\)，否则小于
    \(k\)。
- en: '**THEOREM** **(Why Project)** \(\idx{why project theorem}\xdi\) Let \(A \in
    \mathbb{A}^{n \times d}\) be a matrix and let \(A_k\) be the truncation above.
    For any matrix \(C \in \mathbb{R}^{n \times d}\) of rank \(\leq k\),'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（为什么投影）** \(\idx{why project theorem}\xdi\) 设 \(A \in \mathbb{A}^{n
    \times d}\) 为一个矩阵，设 \(A_k\) 为上述截断。对于任何秩 \(\leq k\) 的矩阵 \(C \in \mathbb{R}^{n \times
    d}\)，'
- en: \[ \|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2. \]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2. \]
- en: \(\sharp\)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Observe that we used different matrix norms on the different sides of the inequality.
    The content of this inequality is the following. The quantity \(\|A_k - C\|_F^2\)
    is the \(k\)-means objective of the projection \(A_k\) with respect to the true
    centers, that is, the sum of the squared distances to the centers. By the *Matrix
    Norms and Singular Values Lemma*, the inequality above gives that
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们在不等式的两边使用了不同的矩阵范数。这个不等式的内容如下。量 \(\|A_k - C\|_F^2\) 是投影 \(A_k\) 相对于真实中心的
    \(k\) 均值目标，即到中心的平方距离之和。根据 *矩阵范数和奇异值引理*，上述不等式给出
- en: \[ \|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2, \]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2, \]
- en: where \(\sigma_j(A - C)\) is the \(j\)-th singular value of \(A - C\). On the
    other hand, by the same lemma, the \(k\)-means objective of the un-projected data
    is
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_j(A - C)\) 是 \(A - C\) 的第 \(j\) 个奇异值。另一方面，根据相同的引理，未投影数据的 \(k\) 均值目标函数是
- en: \[ \|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2. \]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2. \]
- en: If the rank of \(A-C\) is much larger than \(k\) and the singular values of
    \(A-C\) decay slowly, then the latter quantity may be much larger. In other words,
    projecting may bring the data points closer to their true centers, potentially
    making it easier to cluster them.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 \(A-C\) 的秩远大于 \(k\) 且 \(A-C\) 的奇异值衰减缓慢，那么后一个量可能要大得多。换句话说，投影可能将数据点更靠近它们的真实中心，从而可能使聚类更容易。
- en: '*Proof:* *(Why Project)* We have shown previously that, for any matrices \(A,
    B \in \mathbb{R}^{n \times m}\), the rank of their sum is less or equal than the
    sum of their ranks, that is, \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
    So the rank of the difference \(A_k - C\) is at most the sum of the ranks'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明** *(为什么投影)* 我们之前已经证明，对于任何矩阵 \(A, B \in \mathbb{R}^{n \times m}\)，它们的和的秩小于或等于它们秩的和，即
    \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\)。因此，差 \(A_k - C\) 的秩最多是它们秩的和'
- en: \[ \mathrm{rk}(A_k - C) \leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k \]
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{rk}(A_k - C) \leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k \]
- en: where we used that the rank of \(A_k\) is \(k\) and the rank of \(C\) is \(\leq
    k\) since it has \(k\) distinct rows. By the *Matrix Norms and Singular Values
    Lemma*,
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(A_k\) 的秩为 \(k\) 以及 \(C\) 的秩 \(\leq k\)，因为它有 \(k\) 个不同的行。根据 *矩阵范数和奇异值引理*，
- en: \[ \|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2\. \]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2\. \]
- en: By the triangle inequality for matrix norms,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 根据矩阵范数的三角不等式，
- en: \[ \|A_k - C\|_2 \leq \|A_k - A\|_2 + \|A - C\|_2. \]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_2 \leq \|A_k - A\|_2 + \|A - C\|_2. \]
- en: By the *Low-Rank Approximation in the Induced Norm Theorem*,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据诱导范数中的低秩逼近定理，
- en: \[ \|A - A_k\|_2 \leq \|A - C\|_2 \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|_2 \leq \|A - C\|_2 \]
- en: since \(C\) has rank at most \(k\). Putting these three inequalities together,
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(C\) 的秩最多为 \(k\)。将这三个不等式结合起来，
- en: \[ \|A_k - C\|_F^2 \leq 2k (2 \|A - C\|_2)^2 = 8k \|A - C\|_2^2\. \]
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 2k (2 \|A - C\|_2)^2 = 8k \|A - C\|_2^2\. \]
- en: \(\square\)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: '**NUMERICAL CORNER:** We return to our example with the two Gaussian clusters.
    We use function producing two separate clusters.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**：我们回到我们的两个高斯簇的例子。我们使用产生两个单独簇的函数。'
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We first generate the data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先生成数据。
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In reality, we cannot compute the matrix norms of \(X-C\) and \(X_k-C\) as
    the true centers are not known. But, because this is simulated data, we happen
    to know the truth and we can check the validity of our results in this case. The
    centers are:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，我们无法计算 \(X-C\) 和 \(X_k-C\) 的矩阵范数，因为真正的中心是未知的。但由于这是模拟数据，我们恰好知道真相，并且我们可以检查在这种情况下我们结果的有效性。中心是：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We use [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function to compute the norms from the formulas in the *Matrix Norms and Singular
    Values Lemma*. First, we observe that the singular values of \(X-C\) are decaying
    slowly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `numpy.linalg.svd` 函数从 *矩阵范数和奇异值引理* 中的公式计算范数。首先，我们观察到 \(X-C\) 的奇异值衰减缓慢。
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png](../Images/23b4327417d560c241971d818c1583cb.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png](../Images/23b4327417d560c241971d818c1583cb.png)'
- en: 'The \(k\)-means objective with respect to the true centers under the full-dimensional
    data is:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在全维数据下，关于真实中心的 \(k\)-means 目标函数是：
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'while the square of the top singular value of \(X-C\) is only:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 而且在 \(X-C\) 的最大奇异值的平方仅为：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we compute the \(k\)-means objective with respect to the true centers
    under the projected one-dimensional data:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在投影一维数据下计算关于真实中心的 \(k\)-means 目标函数：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: \(\unlhd\)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the applications of SVD
    in recommendation systems. How is it used to predict user preferences? \(\ddagger\)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向你最喜欢的 AI 聊天机器人询问 SVD 在推荐系统中的应用。它是如何用来预测用户偏好的？\(\ddagger\)'
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about nonnegative matrix factorization
    (NMF) and how it compares to SVD. What are the key differences in their constraints
    and applications? How does NMF handle interpretability in topics like text analysis
    or image processing? Explore some algorithms used to compute NMF. \(\ddagger\)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向你最喜欢的 AI 聊天机器人询问非负矩阵分解 (NMF) 以及它与 SVD 的比较。它们在约束和应用中的关键区别是什么？NMF
    在文本分析或图像处理等主题中如何处理可解释性？探索一些用于计算 NMF 的算法。\(\ddagger\)'
- en: 4.6.3\. Ridge regression[#](#ridge-regression "Link to this heading")
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6.3\. 岭回归[#](#ridge-regression "链接到这个标题")
- en: Here we consider what is called Tikhonov regularization\(\idx{Tikhonov regularization}\xdi\),
    an idea that turns out to be useful in overdetermined linear systems, particularly
    when the columns of the matrix \(A\) are linearly dependent or close to linearly
    dependent (which is sometimes referred to as [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\(\idx{multicollinearity}\xdi\)
    in statistics). It trades off minimizing the fit to the data versus minimizing
    the norm of the solution. More precisely, for a parameter \(\lambda > 0\) to be
    chosen, we solve\(\idx{ridge regression}\xdi\)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们考虑所谓的 Tikhonov 正则化\(\idx{Tikhonov regularization}\xdi\)，这个想法最终证明在过定线性系统中很有用，尤其是在矩阵
    \(A\) 的列线性相关或接近线性相关（在统计学中有时被称为 [多重共线性](https://en.wikipedia.org/wiki/Multicollinearity)\(\idx{multicollinearity}\xdi\)）时。它是在最小化数据拟合与最小化解的范数之间进行权衡。更精确地说，为了选择参数
    \(\lambda > 0\)，我们解决\(\idx{ridge regression}\xdi\)
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2. \]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2. \]
- en: The second term is referred to as an \(L_2\)-regularizer\(\idx{L2-regularization}\xdi\).
    Here \(A \in \mathbb{R}^{n\times m}\) with \(n \geq m\) and \(\mathbf{b} \in \mathbb{R}^n\).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项被称为 \(L_2\)-正则化\(\idx{L2-regularization}\xdi\)。这里 \(A \in \mathbb{R}^{n\times
    m}\) 且 \(n \geq m\)，\(\mathbf{b} \in \mathbb{R}^n\)。
- en: To solve this optimization problem, we show that the objective function is strongly
    convex. We then find its unique stationary point. Rewriting the objective in quadratic
    function form
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个优化问题，我们证明目标函数是强凸的。然后我们找到它的唯一驻点。将目标函数重写为二次函数形式
- en: \[\begin{align*} f(\mathbf{x}) &= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2\\ &= \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x}
    + \mathbf{b}^T \mathbf{b} + \lambda \mathbf{x}^T \mathbf{x}\\ &= \mathbf{x}^T
    (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T
    \mathbf{b}\\ &= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \end{align*}\]
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x}) &= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2\\ &= \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x}
    + \mathbf{b}^T \mathbf{b} + \lambda \mathbf{x}^T \mathbf{x}\\ &= \mathbf{x}^T
    (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T
    \mathbf{b}\\ &= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \end{align*}\]
- en: where \(P = 2 (A^T A + \lambda I_{m \times m})\) is symmetric, \(\mathbf{q}
    = - 2 A^T \mathbf{b}\), and \(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(P = 2 (A^T A + \lambda I_{m \times m})\) 是对称的，\(\mathbf{q} = - 2 A^T \mathbf{b}\)，且
    \(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\)。
- en: As we previously computed, the Hessian of \(f\) is \(H_f(\mathbf{x})= P\). Now
    comes a key observation. The matrix \(P\) is positive definite whenever \(\lambda
    > 0\). Indeed, for any \(\mathbf{z} \in \mathbb{R}^m\),
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前计算的，函数 \(f\) 的Hessian矩阵 \(H_f(\mathbf{x})= P\)。现在有一个关键观察。当 \(\lambda >
    0\) 时，矩阵 \(P\) 总是正定的。确实，对于任何 \(\mathbf{z} \in \mathbb{R}^m\)，
- en: \[ \mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z} = 2 \|A \mathbf{z}\|_2^2
    + 2 \lambda \|\mathbf{z}\|_2^2 > 0. \]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z} = 2 \|A \mathbf{z}\|_2^2
    + 2 \lambda \|\mathbf{z}\|_2^2 > 0. \]
- en: Let \(\mu = 2 \lambda > 0\). Then \(f\) is \(\mu\)-strongly convex. This holds
    whether or not the columns of \(A\) are linearly independent.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 令 \(\mu = 2 \lambda > 0\)。那么 \(f\) 是 \(\mu\)-强凸的。这无论 \(A\) 的列是否线性无关都成立。
- en: The stationary points are easily characterized. Recall that the gradient is
    \(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\). Equating to \(\mathbf{0}\)
    leads to the system
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 站点很容易被描述。回想一下梯度是 \(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\)。等于 \(\mathbf{0}\)
    导致以下系统
- en: \[ 2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
    \]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[ 2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
    \]
- en: that is
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 即
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}. \]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}. \]
- en: The matrix in parenthesis is invertible as it is \(1/2\) of the Hessian, which
    is positive definite.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的矩阵是可逆的，因为它等于Hessian的一半，而Hessian是正定的。
- en: '**Connection to SVD** Expressing the solution in terms of a compact SVD \(A
    = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) provides
    some insights into how ridge regression works. Suppose that \(A\) has full column
    rank. That implies that \(V V^T = I_{m \times m}\). Then observe that'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**与SVD的联系** 将解表示为紧凑的SVD \(A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\)，这提供了一些关于岭回归如何工作的见解。假设 \(A\) 具有满列秩。这意味着 \(V V^T = I_{m \times
    m}\)。然后观察'
- en: \[\begin{align*} (A^T A + \lambda I_{m \times m})^{-1} &= (V \Sigma U^T U \Sigma
    V^T + \lambda I_{m \times m})^{-1}\\ &= (V \Sigma^2 V^T + \lambda I_{m \times
    m})^{-1}\\ &= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\ &= (V [\Sigma^2
    + \lambda I_{m \times m}] V^T)^{-1}\\ &= V (\Sigma^2 + \lambda I_{m \times m})^{-1}
    V^T. \end{align*}\]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (A^T A + \lambda I_{m \times m})^{-1} &= (V \Sigma U^T U \Sigma
    V^T + \lambda I_{m \times m})^{-1}\\ &= (V \Sigma^2 V^T + \lambda I_{m \times
    m})^{-1}\\ &= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\ &= (V [\Sigma^2
    + \lambda I_{m \times m}] V^T)^{-1}\\ &= V (\Sigma^2 + \lambda I_{m \times m})^{-1}
    V^T. \end{align*}\]
- en: Hence
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b} =
    V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b} = V (\Sigma^2
    + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}. \]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b} =
    V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b} = V (\Sigma^2
    + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}. \]
- en: Our predictions are
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测是
- en: \[\begin{align*} A \mathbf{x}^{**} &= U \Sigma V^T V (\Sigma^2 + \lambda I_{m
    \times m})^{-1} \Sigma U^T \mathbf{b}\\ &= U \Sigma (\Sigma^2 + \lambda I_{m \times
    m})^{-1} \Sigma U^T \mathbf{b}\\ &= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2
    + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}. \end{align*}\]
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{x}^{**} &= U \Sigma V^T V (\Sigma^2 + \lambda I_{m
    \times m})^{-1} \Sigma U^T \mathbf{b}\\ &= U \Sigma (\Sigma^2 + \lambda I_{m \times
    m})^{-1} \Sigma U^T \mathbf{b}\\ &= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2
    + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}. \end{align*}\]
- en: Note that the terms in curly brackets are \(< 1\) when \(\lambda > 0\).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当 \(\lambda > 0\) 时，花括号内的术语 \(< 1\)。
- en: Compare this to the unregularized least squares solution, which is obtained
    simply by setting \(\lambda = 0\) above,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与未正则化的最小二乘解进行比较，该解简单地通过将上面的 \(\lambda\) 设置为 0 来获得，
- en: \[\begin{align*} A \mathbf{x}^* &= \sum_{j=1}^r \mathbf{u}_j \mathbf{u}_j^T
    \mathbf{b}. \end{align*}\]
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{x}^* &= \sum_{j=1}^r \mathbf{u}_j \mathbf{u}_j^T
    \mathbf{b}. \end{align*}\]
- en: The difference is that the regularized solution reduces the contributions from
    the left singular vectors corresponding to small singular values.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 差异在于正则化解减少了对应于小奇异值的左奇异向量的贡献。
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由克劳德、双子座和ChatGPT协助)*'
- en: '**1** Which of the following best describes the Frobenius norm of a matrix
    \(A \in \mathbb{R}^{n \times m}\)?'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 以下哪个选项最好地描述了矩阵 \(A \in \mathbb{R}^{n \times m}\) 的Frobenius范数？'
- en: a) The maximum singular value of \(A\).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(A\) 的最大奇异值。
- en: b) The square root of the sum of the squares of all entries in \(A\).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(A\) 中所有元素平方和的平方根。
- en: c) The maximum absolute row sum of \(A\).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(A\) 的最大绝对行和。
- en: d) The maximum absolute column sum of \(A\).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(A\) 的最大绝对列和。
- en: '**2** Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) and let \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) be the truncated SVD with \(k < r\). Which of the following is
    true about the Frobenius norm of \(A - A_k\)?'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 设 \(A \in \mathbb{R}^{n \times m}\) 为一个具有奇异值分解 \(A = \sum_{j=1}^r \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\) 的矩阵，设 \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) 为截断奇异值分解，其中 \(k < r\)。关于 \(A - A_k\) 的Frobenius范数，以下哪个是正确的？'
- en: a) \(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)
- en: b) \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)
- en: c) \(\|A - A_k\|_F^2 = \sigma_k^2\)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\|A - A_k\|_F^2 = \sigma_k^2\)
- en: d) \(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)
- en: '**3** The ridge regression problem is formulated as \(\min_{\mathbf{x} \in
    \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\).
    What is the role of the parameter \(\lambda\)?'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 岭回归问题被表述为 \(\min_{\mathbf{x} \in \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2
    + \lambda \|\mathbf{x}\|_2^2\)。参数 \(\lambda\) 的作用是什么？'
- en: a) It controls the trade-off between fitting the data and minimizing the norm
    of the solution.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它控制着拟合数据和最小化解的范数之间的权衡。
- en: b) It determines the rank of the matrix \(A\).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它决定了矩阵 \(A\) 的秩。
- en: c) It is the smallest singular value of \(A\).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它是 \(A\) 的最小奇异值。
- en: d) It is the largest singular value of \(A\).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它是 \(A\) 的最大奇异值。
- en: '**4** Let \(A\) be an \(n \times m\) matrix with compact SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). How does the ridge regression solution
    \(\mathbf{x}^{**}\) compare to the least squares solution \(\mathbf{x}^*\)?'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 设 \(A\) 为一个 \(n \times m\) 的矩阵，具有紧凑的奇异值分解 \(A = \sum_{j=1}^r \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\)。岭回归解 \(\mathbf{x}^{**}\) 与最小二乘解 \(\mathbf{x}^*\)
    如何比较？'
- en: a) \(\mathbf{x}^{**}\) has larger components along the left singular vectors
    corresponding to small singular values.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbf{x}^{**}\) 在对应于小奇异值的左奇异向量上的分量更大。
- en: b) \(\mathbf{x}^{**}\) has smaller components along the left singular vectors
    corresponding to small singular values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{x}^{**}\) 在对应于小奇异值的左奇异向量上的分量更小。
- en: c) \(\mathbf{x}^{**}\) is identical to \(\mathbf{x}^*\).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathbf{x}^{**}\) 与 \(\mathbf{x}^*\) 相同。
- en: d) None of the above.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: d) 以上都不对。
- en: '**5** (*Note:* Refers to online supplementary materials.) Let \(A \in \mathbb{R}^{n
    \times n}\) be a square nonsingular matrix with compact SVD \(A = \sum_{j=1}^n
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). Which of the following is true about the
    induced 2-norm of the inverse \(A^{-1}\)?'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** (*注意：* 指的是在线补充材料。) 设 \(A \in \mathbb{R}^{n \times n}\) 为一个方阵，具有紧凑的奇异值分解
    \(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)。关于 \(A^{-1}\) 的诱导 2-范数，以下哪个是正确的？'
- en: a) \(\|A^{-1}\|_2 = \sigma_1\)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\|A^{-1}\|_2 = \sigma_1\)
- en: b) \(\|A^{-1}\|_2 = \sigma_n\)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\|A^{-1}\|_2 = \sigma_n\)
- en: c) \(\|A^{-1}\|_2 = \sigma_1^{-1}\)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\|A^{-1}\|_2 = \sigma_1^{-1}\)
- en: d) \(\|A^{-1}\|_2 = \sigma_n^{-1}\)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\|A^{-1}\|_2 = \sigma_n^{-1}\)
- en: 'Answer for 1: b. Justification: The text defines the Frobenius norm of an \(n
    \times m\) matrix \(A\) as \(\|A\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{m} a_{ij}^2}\).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：b. 理由：文本定义了一个 \(n \times m\) 矩阵 \(A\) 的Frobenius范数为 \(\|A\|_F = \sqrt{\sum_{i=1}^{n}
    \sum_{j=1}^{m} a_{ij}^2}\)。
- en: 'Answer for 2: b. Justification: The text proves that \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r
    \sigma_j^2\) in the Matrix Norms and Singular Values: Truncation Lemma.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：b. 理由：文本在矩阵范数和奇异值：截断引理中证明了 \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)。
- en: 'Answer for 3: a. Justification: The text explains that ridge regression “trades
    off minimizing the fit to the data versus minimizing the norm of the solution,”
    and \(\lambda\) is the parameter that controls this trade-off.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：a. 理由：文本解释说，岭回归“在最小化数据拟合与最小化解的范数之间进行权衡”，而 \(\lambda\) 是控制这种权衡的参数。
- en: 'Answer for 4: b. Justification: The text notes that the ridge regression solution
    “reduces the contributions from the left singular vectors corresponding to small
    singular values.”'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：b. 理由：文本指出，岭回归解“减少了对应于小奇异值的左奇异向量的贡献。”
- en: 'Answer for 5: d. Justification: The text shows in an example that for a square
    nonsingular matrix \(A\), \(\|A^{-1}\|_2 = \sigma_n^{-1}\), where \(\sigma_n\)
    is the smallest singular value of \(A\).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：d. 理由：文本通过一个例子表明，对于一个方阵 \(A\)，\(\|A^{-1}\|_2 = \sigma_n^{-1}\)，其中 \(\sigma_n\)
    是 \(A\) 的最小奇异值。
- en: 4.6.1\. Matrix norms[#](#matrix-norms "Link to this heading")
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6.1\. 矩阵范数[#](#matrix-norms "链接到本标题")
- en: Recall that the Frobenius norm\(\idx{Frobenius norm}\xdi\) of an \(n \times
    m\) matrix \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) is defined as
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，\(n \times m\) 矩阵 \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\) 的**弗罗贝尼乌斯范数**\(\idx{Frobenius
    norm}\xdi\) 定义为
- en: \[ \|A\|_F = \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}. \]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_F = \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}. \]
- en: Here we introduce a different notion of matrix norm that has many uses in data
    science (and beyond).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了一种不同的矩阵范数概念，它在数据科学（及其他领域）中有许多应用。
- en: '**Induced norm** The Frobenius norm does not directly relate to \(A\) as a
    representation of a [linear map](https://en.wikipedia.org/wiki/Linear_map). In
    particular, it is desirable in many contexts to quantify how two matrices differ
    in terms of how they act on vectors. For instance, one is often interested in
    bounding quantities of the following form. Let \(B, B'' \in \mathbb{R}^{n \times
    m}\) and let \(\mathbf{x} \in \mathbb{R}^m\) be of unit norm. What can be said
    about \(\|B \mathbf{x} - B'' \mathbf{x}\|\)? Intuitively, what we would like is
    this: if the norm of \(B - B''\) is small then \(B\) is close to \(B''\) as a
    linear map, that is, the vector norm \(\|B \mathbf{x} - B'' \mathbf{x}\|\) is
    small for any unit vector \(\mathbf{x}\). The following definition provides us
    with such a notion. Define the unit sphere \(\mathbb{S}^{m-1} = \{\mathbf{x} \in
    \mathbb{R}^m\,:\,\|\mathbf{x}\| = 1\}\) in \(m\) dimensions.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**诱导范数** 弗罗贝尼乌斯范数并不直接与 \(A\) 作为 [线性映射](https://en.wikipedia.org/wiki/Linear_map)
    的表示相关。特别是在许多情况下，我们希望量化两个矩阵在如何作用于向量方面的差异。例如，人们通常对以下形式的量进行界定。设 \(B, B'' \in \mathbb{R}^{n
    \times m}\) 和 \(\mathbf{x} \in \mathbb{R}^m\) 是单位范数。关于 \(\|B \mathbf{x} - B''
    \mathbf{x}\|\) 有什么可以说的？直观上，我们希望的是：如果 \(B - B''\) 的范数很小，那么 \(B\) 作为线性映射接近 \(B''\)，也就是说，对于任何单位向量
    \(\mathbf{x}\)，向量范数 \(\|B \mathbf{x} - B'' \mathbf{x}\|\) 都很小。下面的定义为我们提供了这样的概念。定义
    \(m\) 维度中的单位球 \(\mathbb{S}^{m-1} = \{\mathbf{x} \in \mathbb{R}^m\,:\,\|\mathbf{x}\|
    = 1\}\)。'
- en: '**DEFINITION** **(\(2\)-Norm)** The \(2\)-norm of a matrix\(\idx{2-norm}\xdi\)
    \(A \in \mathbb{R}^{n \times m}\) is'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(二范数)** 矩阵 \(A \in \mathbb{R}^{n \times m}\) 的 \(2\)-范数\(\idx{2-norm}\xdi\)
    定义为'
- en: \[ \|A\|_2 := \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|}
    = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|. \]
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_2 := \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|}
    = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|. \]
- en: \(\natural\)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: The equality in the definition uses the absolute homogeneity of the vector norm.
    Also the definition implicitly uses the *Extreme Value Theorem*. In this case,
    we use the fact that the function \(f(\mathbf{x}) = \|A \mathbf{x}\|\) is continuous
    and the set \(\mathbb{S}^{m-1}\) is closed and bounded to conclude that there
    exists \(\mathbf{x}^* \in \mathbb{S}^{m-1}\) such that \(f(\mathbf{x}^*) \geq
    f(\mathbf{x})\) for all \(\mathbf{x} \in \mathbb{S}^{m-1}\).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 定义中的等式使用了向量范数的绝对齐次性。同时，定义隐含地使用了**极值定理**。在这种情况下，我们利用函数 \(f(\mathbf{x}) = \|A
    \mathbf{x}\|\) 的连续性和集合 \(\mathbb{S}^{m-1}\) 的闭有界性，得出存在 \(\mathbf{x}^* \in \mathbb{S}^{m-1}\)
    使得对于所有 \(\mathbf{x} \in \mathbb{S}^{m-1}\)，都有 \(f(\mathbf{x}^*) \geq f(\mathbf{x})\)。
- en: The \(2\)-norm of a matrix has many other useful properties. The first four
    below are what makes it a [norm](https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的 \(2\)-范数有许多其他有用的性质。下面列出的前四个性质使其成为一个 [范数](https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries)。
- en: '**LEMMA** **(Properties of the \(2\)-Norm)** Let \(A, B \in \mathbb{R}^{n \times
    m}\) and \(\alpha \in \mathbb{R}\). The following hold:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(二范数的性质)** 设 \(A, B \in \mathbb{R}^{n \times m}\) 和 \(\alpha \in \mathbb{R}\)。以下性质成立：'
- en: a) \(\|A\|_2 \geq 0\)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\|A\|_2 \geq 0\)
- en: b) \(\|A\|_2 = 0\) if and only if \(A = 0\)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\|A\|_2 = 0\) 当且仅当 \(A = 0\)
- en: c) \(\|\alpha A\|_2 = |\alpha| \|A\|_2\)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\|\alpha A\|_2 = |\alpha| \|A\|_2\)
- en: d) \(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)
- en: e) \(\|A B \|_2 \leq \|A\|_2 \|B\|_2\).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: e) \(\|A B \|_2 \leq \|A\|_2 \|B\|_2\).
- en: f) \(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\), \(\forall \mathbf{0} \neq
    \mathbf{x} \in \mathbb{R}^m\)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: f) \(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\)，\(\forall \mathbf{0} \neq
    \mathbf{x} \in \mathbb{R}^m\)
- en: \(\flat\)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* These properties all follow from the definition of the \(2\)-norm
    and the corresponding properties for the vector norm:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 这些性质都遵循 \(2\)-范数的定义和向量范数相应性质：'
- en: Claims a) and f) are immediate from the definition.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命题 a) 和 f) 直接从定义中得出。
- en: For b) note that \(\|A\|_2 = 0\) implies \(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\), so that \(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x}
    \in \mathbb{S}^{m-1}\). In particular, \(a_{ij} = \mathbf{e}_i^T A \mathbf{e}_j
    = 0, \forall i,j\).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 b) 注意到 \(\|A\|_2 = 0\) 意味着 \(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x} \in
    \mathbb{S}^{m-1}\)，因此 \(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x} \in \mathbb{S}^{m-1}\)。特别是，\(a_{ij}
    = \mathbf{e}_i^T A \mathbf{e}_j = 0, \forall i,j\).
- en: For c), d), e), observe that for all \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 c), d), e)，观察对于所有 \(\mathbf{x} \in \mathbb{S}^{m-1}\)
- en: \[ \|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|, \]\[\|(A+B)\mathbf{x}\|
    = \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\| \leq \|A\|_2
    + \|B\|_2 \]\[ \|(AB)\mathbf{x}\| = \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
    \leq \|A\|_2 \|B\|_2.\]
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|, \]\[\|(A+B)\mathbf{x}\|
    = \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\| \leq \|A\|_2
    + \|B\|_2 \]\[ \|(AB)\mathbf{x}\| = \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
    \leq \|A\|_2 \|B\|_2.\]
- en: Then apply the definition of \(2\)-norm. For example, for ©,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后应用 \(2\)-范数的定义。例如，对于 ©,
- en: \[\begin{align*} \|\alpha A\|_2 &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha
    A \mathbf{x}\|\\ &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
    &= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\ &= |\alpha|
    \|A\|_2, \end{align*}\]
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|\alpha A\|_2 &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha
    A \mathbf{x}\|\\ &= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
    &= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\ &= |\alpha|
    \|A\|_2, \end{align*}\]
- en: where we used that \(|\alpha|\) does not depend on \(\mathbf{x}\). \(\square\)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(|\alpha|\) 不依赖于 \(\mathbf{x}\). \(\square\)
- en: '**NUMERICAL CORNER:** In NumPy, the Frobenius norm of a matrix can be computed
    using the default of the function `numpy.linalg.norm` while the induced norm can
    be computed using the same function with [`ord` parameter set to `2`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 在NumPy中，可以使用函数 `numpy.linalg.norm` 的默认值来计算矩阵的Frobenius范数，而诱导范数可以通过将相同的函数的
    `ord` 参数设置为 `2` 来计算（[链接](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)）。'
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: \(\unlhd\)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Matrix norms and SVD** As it turns out, the two notions of matrix norms we
    have introduced admit simple expressions in terms of the singular values of the
    matrix.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵范数和奇异值分解** 如我们所见，我们引入的两个矩阵范数的概念可以用矩阵的奇异值简单地表示。'
- en: '**LEMMA** **(Matrix Norms and Singular Values)** \(\idx{matrix norms and singular
    values lemma}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with compact
    SVD'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(矩阵范数和奇异值)** \(\idx{matrix norms and singular values lemma}\xdi\)
    设 \(A \in \mathbb{R}^{n \times m}\) 是一个具有紧奇异值分解的矩阵'
- en: \[ A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T \]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T \]
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\). Then
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其中记住 \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\). 然后
- en: \[ \|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2 \]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2 \]
- en: and
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \|A\|^2_2 = \sigma_{1}^2. \]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|^2_2 = \sigma_{1}^2. \]
- en: \(\flat\)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* We will use the notation \(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\).
    Using that the squared Frobenius norm of \(A\) is the sum of the squared norms
    of its columns, we have'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 我们将使用符号 \(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\). 利用 \(A\)
    的Frobenius范数的平方是其列的平方范数的和，我们有'
- en: \[ \|A\|^2_F = \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
    = \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell \right\|^2.
    \]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|^2_F = \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
    = \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell \right\|^2.
    \]
- en: Because the \(\mathbf{u}_\ell\)’s are orthonormal, this is
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathbf{u}_\ell\) 是正交归一的，所以这是
- en: \[ \sum_{j=1}^m \sum_{\ell=1}^r \sigma_\ell^2 v_{\ell,j}^2 = \sum_{\ell=1}^r
    \sigma_\ell^2 \left(\sum_{j=1}^m v_{\ell,j}^2\right) = \sum_{\ell=1}^r \sigma_\ell^2
    \|\mathbf{v}_{\ell}\|^2 = \sum_{\ell=1}^r \sigma_\ell^2, \]
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^m \sum_{\ell=1}^r \sigma_\ell^2 v_{\ell,j}^2 = \sum_{\ell=1}^r
    \sigma_\ell^2 \left(\sum_{j=1}^m v_{\ell,j}^2\right) = \sum_{\ell=1}^r \sigma_\ell^2
    \|\mathbf{v}_{\ell}\|^2 = \sum_{\ell=1}^r \sigma_\ell^2, \]
- en: where we used that the \(\mathbf{v}_\ell\)’s are also orthonormal.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\mathbf{v}_\ell\) 也是正交归一的性质。
- en: For the second claim, recall that the \(2\)-norm is defined as
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个断言，回忆一下 \(2\)-范数的定义
- en: \[ \|A\|_2^2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2. \]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A\|_2^2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2. \]
- en: We have shown previously that \(\mathbf{v}_1\) solves this problem. Hence \(\|A\|_2^2
    = \|A \mathbf{v}_1\|^2 = \sigma_1^2\). \(\square\)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经证明 \(\mathbf{v}_1\) 解决了这个问题。因此 \(\|A\|_2^2 = \|A \mathbf{v}_1\|^2 = \sigma_1^2\).
    \(\square\)
- en: 4.6.2\. Low-rank approximation[#](#low-rank-approximation "Link to this heading")
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6.2\. 低秩近似[#](#low-rank-approximation "链接到这个标题")
- en: Now that we have defined a notion of distance between matrices, we will consider
    the problem of finding a good approximation to a matrix \(A\) among all matrices
    of rank at most \(k\). We will start with the Frobenius norm, which is easier
    to work with, and we will show later on that the solution is the same under the
    induced norm. The solution to this problem will be familiar. In essence, we will
    re-interpret our solution to the best approximating subspace as a low-rank approximation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了矩阵之间距离的概念，我们将考虑在所有秩最多为 \(k\) 的矩阵中找到一个好的矩阵 \(A\) 的近似的问题。我们将从更容易处理的Frobenius范数开始，稍后我们将表明在诱导范数下解是相同的。这个问题的解将是熟悉的。本质上，我们将重新解释我们对于最佳逼近子空间的解决方案，作为低秩近似。
- en: '**Low-rank approximation in the Frobenius norm** \(\idx{low-rank approximation}\xdi\)
    From the proof of the *Row Rank Equals Column Rank Lemma*, it follows that a rank-\(r\)
    matrix \(A\) can be written as a sum of \(r\) rank-\(1\) matrices'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**Frobenius范数下的低秩近似** \(\idx{low-rank approximation}\xdi\) 从 *行秩等于列秩引理* 的证明中可以得出，秩-\(r\)
    矩阵 \(A\) 可以写成 \(r\) 个秩-\(1\) 矩阵的和'
- en: \[ A = \sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T. \]
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T. \]
- en: We will now consider the problem of finding a “simpler” approximation to \(A\)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考虑找到 \(A\) 的“更简单”近似的问题
- en: \[ A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T \]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T \]
- en: where \(k < r\). Here we measure the quality of this approximation using a matrix
    norm.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(k < r\)。在这里，我们使用矩阵范数来衡量这个近似的质量。
- en: We are ready to state our key observation. In words, the best rank-\(k\) approximation
    to \(A\) in Frobenius norm is obtained by projecting the rows of \(A\) onto a
    linear subspace of dimension \(k\). We will come back to how one finds the best
    such subspace below. (*Hint:* We have already solved this problem.)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以陈述我们的关键观察。换句话说，在Frobenius范数下，\(A\) 的最佳秩-\(k\) 近似是通过将 \(A\) 的行投影到维度为 \(k\)
    的线性子空间中获得的。我们将在下面回到如何找到这样的最佳子空间。(*提示：* 我们已经解决了这个问题。)
- en: '**LEMMA** **(Projection and Rank-\(k\) Approximation)** \(\idx{projection and
    rank-k approximation lemma}\xdi\) Let \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n
    \times m}\). For any matrix \(B = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)
    of rank \(k \leq \min\{n,m\}\),'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(投影和秩-\(k\) 近似)** \(\idx{projection and rank-k approximation lemma}\xdi\)
    设 \(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)。对于任何秩 \(k \leq \min\{n,m\}\)
    的矩阵 \(B = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)，'
- en: \[ \|A - B_{\perp}\|_F \leq \|A - B\|_F \]
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - B_{\perp}\|_F \leq \|A - B\|_F \]
- en: where \(B_{\perp} \in \mathbb{R}^{n \times m}\) is the matrix of rank at most
    \(k\) obtained as follows. Denote row \(i\) of \(A\), \(B\) and \(B_{\perp}\)
    respectively by \(\boldsymbol{\alpha}_i^T\), \(\mathbf{b}_{i}^T\) and \(\mathbf{b}_{\perp,i}^T\),
    \(i=1,\ldots, n\). Set \(\mathbf{b}_{\perp,i}\) to be the orthogonal projection
    of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z} = \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\).
    \(\flat\)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(B_{\perp} \in \mathbb{R}^{n \times m}\) 是通过以下方式获得的秩最多为 \(k\) 的矩阵。分别用 \(\boldsymbol{\alpha}_i^T\)、\(\mathbf{b}_{i}^T\)
    和 \(\mathbf{b}_{\perp,i}^T\) 表示 \(A\)、\(B\) 和 \(B_{\perp}\) 的第 \(i\) 行，\(i=1,\ldots,
    n\)。将 \(\mathbf{b}_{\perp,i}\) 设置为 \(\boldsymbol{\alpha}_i\) 在 \(\mathcal{Z} =
    \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\) 上的正交投影。 \(\flat\)
- en: '*Proof idea:* The square of the Frobenius norm decomposes as a sum of squared
    row norms. Each term in the sum is minimized by the orthogonal projection.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* Frobenius范数的平方可以分解为行范数平方的和。和中的每一项都通过正交投影最小化。'
- en: '*Proof:* By definition of the Frobenius norm, we note that'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 根据Frobenius范数的定义，我们注意到'
- en: \[ \|A - B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 = \sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 \]
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - B\|_F^2 = \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 = \sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathbf{b}_{i,j}\|^2 \]
- en: 'and similarly for \(\|A - B_{\perp}\|_F\). We make two observations:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 以及对于 \(\|A - B_{\perp}\|_F\) 也是如此。我们做出两个观察：
- en: Because the orthogonal projection of \(\boldsymbol{\alpha}_i\) onto \(\mathcal{Z}\)
    minimizes the distance to \(\mathcal{Z}\), it follows that term by term \(\|\boldsymbol{\alpha}_i
    - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|\) so
    that
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为 \(\boldsymbol{\alpha}_i\) 在 \(\mathcal{Z}\) 上的正交投影最小化了到 \(\mathcal{Z}\) 的距离，所以逐项
    \(\|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i
    - \mathbf{b}_{i,j}\|\)，从而
- en: \[ \|A - B_\perp\|_F^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
    \leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 = \|A - B\|_F^2.
    \]
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - B_\perp\|_F^2 = \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
    \leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2 = \|A - B\|_F^2.
    \]
- en: Moreover, because the projections satisfy \(\mathbf{b}_{\perp,i} \in \mathcal{Z}\)
    for all \(i\), \(\mathrm{row}(B_\perp) \subseteq \mathrm{row}(B)\) and, hence,
    the rank of \(B_\perp\) is at most the rank of \(B\).
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，因为投影满足对所有 \(i\)，\(\mathbf{b}_{\perp,i} \in \mathcal{Z}\)，所以 \(\mathrm{row}(B_\perp)
    \subseteq \mathrm{row}(B)\)，因此 \(B_\perp\) 的秩至多是 \(B\) 的秩。
- en: That concludes the proof. \(\square\)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了证明。 \(\square\)
- en: Recall the approximating subspace problem. That is, think of the rows \(\boldsymbol{\alpha}_i^T\)
    of \(A \in \mathbb{R}^{n \times m}\) as a collection of \(n\) data points in \(\mathbb{R}^m\).
    We are looking for a linear subspace \(\mathcal{Z}\) that minimizes \(\sum_{i=1}^n
    \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)
    over all linear subspaces of \(\mathbb{R}^m\) of dimension at most \(k\). By the
    *Projection and Rank-\(k\) Approximation Lemma*, this problem is equivalent to
    finding a matrix \(B\) that minimizes \(\|A - B\|_F\) among all matrices in \(\mathbb{R}^{n
    \times m}\) of rank at most \(k\). Of course we have solved this problem before.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下逼近子空间问题。也就是说，将 \(A \in \mathbb{R}^{n \times m}\) 的行 \(\boldsymbol{\alpha}_i^T\)
    视为 \(\mathbb{R}^m\) 中的 \(n\) 个数据点的集合。我们正在寻找一个线性子空间 \(\mathcal{Z}\)，它在所有维度不超过 \(k\)
    的 \(\mathbb{R}^m\) 线性子空间中使 \(\sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)
    最小。根据 *投影和秩-\(k\) 近似引理*，这个问题等价于在所有秩不超过 \(k\) 的 \(\mathbb{R}^{n \times m}\) 矩阵中找到一个矩阵
    \(B\)，使得 \(\|A - B\|_F\) 最小。当然，我们之前已经解决了这个问题。
- en: Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). For \(k < r\), truncate the sum at the
    \(k\)-th term \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). The
    rank of \(A_k\) is exactly \(k\). Indeed, by construction,
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(A \in \mathbb{R}^{n \times m}\) 为一个矩阵，其奇异值分解为 \(A = \sum_{j=1}^r \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\)。对于 \(k < r\)，截断到第 \(k\) 项 \(A_k = \sum_{j=1}^k \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\)。\(A_k\) 的秩正好是 \(k\)。实际上，通过构造，
- en: the vectors \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) are orthonormal, and
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量 \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) 是正交归一的，并且
- en: since \(\sigma_j > 0\) for \(j=1,\ldots,k\) and the vectors \(\{\mathbf{v}_j\,:\,j
    = 1,\ldots,k\}\) are orthonormal, \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) spans
    the column space of \(A_k\).
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 \(j=1,\ldots,k\) 时 \(\sigma_j > 0\)，且向量 \(\{\mathbf{v}_j\,:\,j = 1,\ldots,k\}\)
    是正交归一的，因此 \(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\) 张成 \(A_k\) 的列空间。
- en: We have shown before that \(A_k\) is the best approximation to \(A\) among matrices
    of rank at most \(k\) in Frobenius norm. Specifically, the *Greedy Finds Best
    Fit Theorem* implies that, for any matrix \(B \in \mathbb{R}^{n \times m}\) of
    rank at most \(k\),
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经表明 \(A_k\) 是在 Frobenius 范数下 \(A\) 在秩不超过 \(k\) 的矩阵中的最佳逼近。具体来说，*贪婪寻找最佳拟合定理*
    意味着，对于任何秩不超过 \(k\) 的矩阵 \(B \in \mathbb{R}^{n \times m}\)，
- en: \[ \|A - A_k\|_F \leq \|A - B\|_F. \]
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|_F \leq \|A - B\|_F. \]
- en: This result is known as the *Eckart-Young Theorem*\(\idx{Eckart-Young theorem}\xdi\).
    It also holds in the induced \(2\)-norm, as we show next.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果被称为 *Eckart-Young 定理*\(\idx{Eckart-Young theorem}\xdi\)。它也适用于诱导的 \(2\)-范数，我们将在下文中展示。
- en: '**Low-rank approximation in the induced norm** We show in this section that
    the same holds in the induced norm. First, some observations.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**诱导范数中的低秩逼近** 在本节中，我们表明在诱导范数中也是如此。首先，一些观察。'
- en: '**LEMMA** **(Matrix Norms and Singular Values: Truncation)** Let \(A \in \mathbb{R}^{n
    \times m}\) be a matrix with SVD'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(矩阵范数和奇异值：截断)** 设 \(A \in \mathbb{R}^{n \times m}\) 为一个矩阵，其奇异值分解为'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: where recall that \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\) and let
    \(A_k\) be the truncation defined above. Then
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，回忆一下 \(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0\)，并且令 \(A_k\) 为上面定义的截断。然后
- en: \[ \|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2 \]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2 \]
- en: and
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \|A - A_k\|^2_2 = \sigma_{k+1}^2. \]
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|^2_2 = \sigma_{k+1}^2. \]
- en: \(\flat\)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* For the first claim, by definition, summing over the columns of \(A
    - A_k\)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 对于第一个断言，根据定义，对 \(A - A_k\) 的列求和'
- en: \[ \|A - A_k\|^2_F = \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
    = \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j \right\|^2.
    \]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|^2_F = \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
    = \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j \right\|^2.
    \]
- en: Because the \(\mathbf{u}_j\)’s are orthonormal, this is
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathbf{u}_j\) 是正交归一的，所以这是
- en: \[ \sum_{i=1}^m \sum_{j=k+1}^r \sigma_j^2 v_{j,i}^2 = \sum_{j=k+1}^r \sigma_j^2
    \left(\sum_{i=1}^m v_{j,i}^2\right) = \sum_{j=k+1}^r \sigma_j^2 \]
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^m \sum_{j=k+1}^r \sigma_j^2 v_{j,i}^2 = \sum_{j=k+1}^r \sigma_j^2
    \left(\sum_{i=1}^m v_{j,i}^2\right) = \sum_{j=k+1}^r \sigma_j^2 \]
- en: where we used that the \(\mathbf{v}_j\)’s are also orthonormal.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\mathbf{v}_j\) 也是正交归一的性质。
- en: For the second claim, recall that the induced norm is defined as
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个断言，回忆一下诱导范数的定义
- en: \[ \|B\|_2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|. \]
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|B\|_2 = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|. \]
- en: For any \(\mathbf{x} \in \mathbb{S}^{m-1}\)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意 \(\mathbf{x} \in \mathbb{S}^{m-1}\)
- en: \[ \left\|(A - A_k)\mathbf{x}\right\|^2 = \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j
    (\mathbf{v}_j^T \mathbf{x}) \right\|^2 = \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j,
    \mathbf{x}\rangle^2. \]
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left\|(A - A_k)\mathbf{x}\right\|^2 = \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j
    (\mathbf{v}_j^T \mathbf{x}) \right\|^2 = \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j,
    \mathbf{x}\rangle^2. \]
- en: Because the \(\sigma_j\)’s are in decreasing order, this is maximized when \(\langle
    \mathbf{v}_j, \mathbf{x}\rangle = 1\) if \(j=k+1\) and \(0\) otherwise. That is,
    we take \(\mathbf{x} = \mathbf{v}_{k+1}\) and the norm is then \(\sigma_{k+1}^2\),
    as claimed. \(\square\)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\sigma_j\) 是递减的，所以当 \(j=k+1\) 时，\(\langle \mathbf{v}_j, \mathbf{x}\rangle
    = 1\)，否则为 \(0\) 时，这是最大的。也就是说，我们取 \(\mathbf{x} = \mathbf{v}_{k+1}\)，那么范数就是 \(\sigma_{k+1}^2\)，正如所声称的。
    \(\square\)
- en: '**THEOREM** **(Low-Rank Approximation in the Induced Norm)** \(\idx{low-rank
    approximation in the induced norm theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times
    m}\) be a matrix with SVD'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(诱导范数中的低秩近似)** \(\idx{诱导范数中的低秩近似定理}\xdi\) 设 \(A \in \mathbb{R}^{n
    \times m}\) 是一个具有奇异值分解的矩阵'
- en: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T \]
- en: and let \(A_k\) be the truncation defined above with \(k < r\). For any matrix
    \(B \in \mathbb{R}^{n \times m}\) of rank at most \(k\),
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 并令 \(A_k\) 为上面定义的截断，其中 \(k < r\)。对于任何秩最多为 \(k\) 的矩阵 \(B \in \mathbb{R}^{n \times
    m}\)
- en: \[ \|A - A_k\|_2 \leq \|A - B\|_2. \]
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|_2 \leq \|A - B\|_2. \]
- en: \(\sharp\)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: '*Proof idea:* We know that \(\|A - A_k\|_2^2 = \sigma_{k+1}^2\). So we want
    to lower bound \(\|A - B\|_2^2\) by \(\sigma_{k+1}^2\). For that, we have to find
    an appropriate \(\mathbf{z}\) for any given \(B\) of rank at most \(k\). The idea
    is to take a vector \(\mathbf{z}\) in the intersection of the null space of \(B\)
    and the span of the singular vectors \(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\).
    By the former, the squared norm of \((A - B)\mathbf{z}\) is equal to the squared
    norm of \(A\mathbf{z}\) which lower bounds \(\|A\|_2^2\). By the latter, \(\|A
    \mathbf{z}\|^2\) is at least \(\sigma_{k+1}^2\).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们知道 \(\|A - A_k\|_2^2 = \sigma_{k+1}^2\)。因此，我们希望用 \(\sigma_{k+1}^2\)
    来下界 \(\|A - B\|_2^2\)。为了做到这一点，我们必须为任何给定的秩最多为 \(k\) 的 \(B\) 找到一个合适的 \(\mathbf{z}\)。思路是取一个向量
    \(\mathbf{z}\)，它在 \(B\) 的零空间和奇异向量 \(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\) 的张成空间中。通过前者，\((A
    - B)\mathbf{z}\) 的平方范数等于 \(A\mathbf{z}\) 的平方范数，这下界 \(\|A\|_2^2\)。通过后者，\(\|A \mathbf{z}\|^2\)
    至少是 \(\sigma_{k+1}^2\)。'
- en: '*Proof:* By the *Rank-Nullity Theorem*, the dimension of \(\mathrm{null}(B)\)
    is at least \(m-k\) so there is a unit vector \(\mathbf{z}\) in the intersection'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 根据秩-零度定理，\(\mathrm{null}(B)\) 的维度至少是 \(m-k\)，因此存在一个单位向量 \(\mathbf{z}\)
    在它们的交集中'
- en: \[ \mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
    \]
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
    \]
- en: (Prove it!) Then \((A-B)\mathbf{z} = A\mathbf{z}\) since \(\mathbf{z} \in \mathrm{null}(B)\).
    Also since \(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\),
    and therefore orthogonal to \(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\), we have
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: (证明它!) 然后 \((A-B)\mathbf{z} = A\mathbf{z}\) 因为 \(\mathbf{z} \in \mathrm{null}(B)\)。同样，由于
    \(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\)，因此与 \(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\)
    正交，我们有
- en: \[\begin{align*} \|(A-B)\mathbf{z}\|^2 &= \|A\mathbf{z}\|^2\\ &= \left\|\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \left\|\sum_{j=1}^{k+1}
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \sum_{j=1}^{k+1}
    \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1}
    \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &= \sigma_{k+1}^2. \end{align*}\]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|(A-B)\mathbf{z}\|^2 &= \|A\mathbf{z}\|^2\\ &= \left\|\sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \left\|\sum_{j=1}^{k+1}
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\ &= \sum_{j=1}^{k+1}
    \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1}
    \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\ &= \sigma_{k+1}^2. \end{align*}\]
- en: By the previous lemma, \(\sigma_{k+1}^2 = \|A - A_k\|_2\) and we are done. \(\square\)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的引理，\(\sigma_{k+1}^2 = \|A - A_k\|_2\)，我们完成了证明。 \(\square\)
- en: '**An application: Why project?** We return to \(k\)-means clustering and why
    projecting to a lower-dimensional subspace can produce better results. We prove
    a simple inequality that provides some insight. Quoting [BHK, Section 7.5.1]:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用：为什么投影？** 我们回到 \(k\)-means 聚类并探讨为什么将数据投影到低维子空间可以产生更好的结果。我们证明了一个简单的不等式，提供了一些见解。引用
    [BHK, 第 7.5.1 节]：'
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  id: totrans-318
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[…] 让我们理解进行投影到 [前 \(k\) 个右奇异向量] 的中心优势。这很简单，对于任何合理（未知）的数据点聚类，投影将数据点更靠近它们的簇中心。'
- en: To elaborate, suppose we have \(n\) data points in \(d\) dimensions in the form
    of the rows \(\boldsymbol{\alpha}_i^T\), \(i=1\ldots, n\), of matrix \(A \in \mathbb{A}^{n
    \times d}\), where we assume that \(n > d\) and that \(A\) has full column rank.
    Imagine these data points come from an unknown ground-truth \(k\)-clustering assignment
    \(g(i) \in [k]\), \(i = 1,\ldots, n\), with corresponding unknown centers \(\mathbf{c}_j\),
    \(j = 1,\ldots, k\). Let \(C \in \mathbb{R}^{n \times d}\) be the corresponding
    matrix, that is, row \(i\) of \(C\) is \(\mathbf{c}_j^T\) if \(g(i) = j\). The
    \(k\)-means objective of the true clustering is then
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为了详细说明，假设我们有一个 \(n\) 维数据点，形式为矩阵 \(A \in \mathbb{A}^{n \times d}\) 的行 \(\boldsymbol{\alpha}_i^T\)，\(i=1\ldots,
    n\)，其中我们假设 \(n > d\) 且 \(A\) 具有满列秩。想象这些数据点来自一个未知的真实 \(k\)-聚类分配 \(g(i) \in [k]\)，\(i
    = 1,\ldots, n\)，对应于未知的中心 \(\mathbf{c}_j\)，\(j = 1,\ldots, k\)。让 \(C \in \mathbb{R}^{n
    \times d}\) 是相应的矩阵，即如果 \(g(i) = j\)，则 \(C\) 的第 \(i\) 行是 \(\mathbf{c}_j^T\)。因此，真实聚类的
    \(k\)-means 目标是
- en: \[\begin{align*} \sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
    &= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
    &= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\ &= \|A - C\|_F^2.
    \end{align*}\]
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
    &= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
    &= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\ &= \|A - C\|_F^2.
    \end{align*}\]
- en: The matrix \(A\) has an SVD \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)
    and for \(k < r\) we have the truncation \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\). It corresponds to projecting each row of \(A\) onto the linear
    subspace spanned by the first \(k\) right singular vectors \(\mathbf{v}_1, \ldots,
    \mathbf{v}_k\). To see this, note that the \(i\)-th row of \(A\) is \(\boldsymbol{\alpha}_i^T
    = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\) and that, because the \(\mathbf{v}_j\)’s
    are linearly independent and in particular \(\mathbf{v}_1,\ldots,\mathbf{v}_k\)
    is an orthonormal basis of its span, the projection of \(\boldsymbol{\alpha}_i\)
    onto \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) is
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(A\) 有一个奇异值分解 \(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)，对于
    \(k < r\)，我们有截断 \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)。这对应于将
    \(A\) 的每一行投影到由前 \(k\) 个右奇异向量 \(\mathbf{v}_1, \ldots, \mathbf{v}_k\) 张成的线性子空间。为了理解这一点，请注意
    \(A\) 的第 \(i\) 行是 \(\boldsymbol{\alpha}_i^T = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\)，并且由于
    \(\mathbf{v}_j\) 是线性无关的，特别是 \(\mathbf{v}_1,\ldots,\mathbf{v}_k\) 是其张成的正交基，因此 \(\boldsymbol{\alpha}_i\)
    投影到 \(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\) 是
- en: \[ \sum_{\ell=1}^k \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i}
    \mathbf{v}_j,\mathbf{v}_\ell\right\rangle = \sum_{\ell=1}^k \sigma_\ell u_{\ell,i}
    \mathbf{v}_\ell \]
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{\ell=1}^k \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i}
    \mathbf{v}_j,\mathbf{v}_\ell\right\rangle = \sum_{\ell=1}^k \sigma_\ell u_{\ell,i}
    \mathbf{v}_\ell \]
- en: which is the \(i\)-th row of \(A_k\). The \(k\)-means objective of \(A_k\) with
    respect to the ground-truth centers \(\mathbf{c}_j\), \(j=1,\ldots,k\), is \(\|A_k
    - C\|_F^2\).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 \(A_k\) 的第 \(i\) 行。\(A_k\) 关于真实中心 \(\mathbf{c}_j\)，\(j=1,\ldots,k\) 的 \(k\)-means
    目标是 \(\|A_k - C\|_F^2\)。
- en: 'One more observation: the rank of \(C\) is at most \(k\). Indeed, there are
    \(k\) different rows in \(C\) so its row rank is \(k\) if these different rows
    are linearly independent and less than \(k\) otherwise.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一个额外的观察：矩阵 \(C\) 的秩最多为 \(k\)。实际上，\(C\) 中有 \(k\) 个不同的行，所以如果这些不同的行线性无关，其行秩为 \(k\)，否则小于
    \(k\)。
- en: '**THEOREM** **(Why Project)** \(\idx{why project theorem}\xdi\) Let \(A \in
    \mathbb{A}^{n \times d}\) be a matrix and let \(A_k\) be the truncation above.
    For any matrix \(C \in \mathbb{R}^{n \times d}\) of rank \(\leq k\),'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **（为什么投影）** \(\idx{why project theorem}\xdi\) 设 \(A \in \mathbb{A}^{n
    \times d}\) 为一个矩阵，设 \(A_k\) 为上述截断。对于任何秩 \(\leq k\) 的矩阵 \(C \in \mathbb{R}^{n \times
    d}\)，'
- en: \[ \|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2. \]
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2. \]
- en: \(\sharp\)
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Observe that we used different matrix norms on the different sides of the inequality.
    The content of this inequality is the following. The quantity \(\|A_k - C\|_F^2\)
    is the \(k\)-means objective of the projection \(A_k\) with respect to the true
    centers, that is, the sum of the squared distances to the centers. By the *Matrix
    Norms and Singular Values Lemma*, the inequality above gives that
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们在不等式的不同侧使用了不同的矩阵范数。这个不等式的含义如下。量 \(\|A_k - C\|_F^2\) 是 \(A_k\) 关于真实中心的 \(k\)-means
    目标，即到中心的平方距离之和。根据 *矩阵范数和奇异值引理*，上述不等式给出
- en: \[ \|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2, \]
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2, \]
- en: where \(\sigma_j(A - C)\) is the \(j\)-th singular value of \(A - C\). On the
    other hand, by the same lemma, the \(k\)-means objective of the un-projected data
    is
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_j(A - C)\) 是 \(A - C\) 的第 \(j\) 个奇异值。另一方面，根据同样的引理，未投影数据的 \(k\)-means
    目标是
- en: \[ \|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2. \]
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2. \]
- en: If the rank of \(A-C\) is much larger than \(k\) and the singular values of
    \(A-C\) decay slowly, then the latter quantity may be much larger. In other words,
    projecting may bring the data points closer to their true centers, potentially
    making it easier to cluster them.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 \(A-C\) 的秩远大于 \(k\) 并且 \(A-C\) 的奇异值衰减缓慢，那么后者可能要大得多。换句话说，投影可能将数据点更靠近它们的真实中心，从而可能使聚类更容易。
- en: '*Proof:* *(Why Project)* We have shown previously that, for any matrices \(A,
    B \in \mathbb{R}^{n \times m}\), the rank of their sum is less or equal than the
    sum of their ranks, that is, \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\).
    So the rank of the difference \(A_k - C\) is at most the sum of the ranks'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(为什么投影)* 我们之前已经证明，对于任何 \(A, B \in \mathbb{R}^{n \times m}\) 的矩阵，它们的和的秩小于或等于它们秩的和，即
    \(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\)。所以 \(A_k - C\) 的秩最多是它们秩的和'
- en: \[ \mathrm{rk}(A_k - C) \leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k \]
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{rk}(A_k - C) \leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k \]
- en: where we used that the rank of \(A_k\) is \(k\) and the rank of \(C\) is \(\leq
    k\) since it has \(k\) distinct rows. By the *Matrix Norms and Singular Values
    Lemma*,
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(A_k\) 的秩是 \(k\) 以及 \(C\) 的秩是 \(\leq k\)，因为它有 \(k\) 个不同的行。根据 *矩阵范数和奇异值引理*，
- en: \[ \|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2\. \]
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2\. \]
- en: By the triangle inequality for matrix norms,
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 根据矩阵范数的三角不等式，
- en: \[ \|A_k - C\|_2 \leq \|A_k - A\|_2 + \|A - C\|_2. \]
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_2 \leq \|A_k - A\|_2 + \|A - C\|_2. \]
- en: By the *Low-Rank Approximation in the Induced Norm Theorem*,
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 *诱导范数低秩近似定理*，
- en: \[ \|A - A_k\|_2 \leq \|A - C\|_2 \]
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A - A_k\|_2 \leq \|A - C\|_2 \]
- en: since \(C\) has rank at most \(k\). Putting these three inequalities together,
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(C\) 的秩最多为 \(k\)。将这三个不等式放在一起，
- en: \[ \|A_k - C\|_F^2 \leq 2k (2 \|A - C\|_2)^2 = 8k \|A - C\|_2^2\. \]
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|A_k - C\|_F^2 \leq 2k (2 \|A - C\|_2)^2 = 8k \|A - C\|_2^2\. \]
- en: \(\square\)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \(\square\)
- en: '**NUMERICAL CORNER:** We return to our example with the two Gaussian clusters.
    We use function producing two separate clusters.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们回到我们的例子，有两个高斯聚类。我们使用产生两个独立聚类的函数。'
- en: '[PRE22]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We first generate the data.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先生成数据。
- en: '[PRE23]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In reality, we cannot compute the matrix norms of \(X-C\) and \(X_k-C\) as
    the true centers are not known. But, because this is simulated data, we happen
    to know the truth and we can check the validity of our results in this case. The
    centers are:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，我们无法计算 \(X-C\) 和 \(X_k-C\) 的矩阵范数，因为真正的中心是未知的。但是，因为这是模拟数据，我们恰好知道真相，并且可以检查我们结果的正确性。中心是：
- en: '[PRE24]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We use [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function to compute the norms from the formulas in the *Matrix Norms and Singular
    Values Lemma*. First, we observe that the singular values of \(X-C\) are decaying
    slowly.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `numpy.linalg.svd` 函数从 *矩阵范数和奇异值引理* 中的公式计算范数。首先，我们观察到 \(X-C\) 的奇异值衰减缓慢。
- en: '[PRE25]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png](../Images/23b4327417d560c241971d818c1583cb.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png](../Images/23b4327417d560c241971d818c1583cb.png)'
- en: 'The \(k\)-means objective with respect to the true centers under the full-dimensional
    data is:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在全维数据下，关于真实中心的 \(k\)-means 目标是：
- en: '[PRE26]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'while the square of the top singular value of \(X-C\) is only:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 而顶奇异值 \(X-C\) 的平方只有：
- en: '[PRE28]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we compute the \(k\)-means objective with respect to the true centers
    under the projected one-dimensional data:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在投影的一维数据下计算关于真实中心的 \(k\)-means 目标函数：
- en: '[PRE30]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: \(\unlhd\)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the applications of SVD
    in recommendation systems. How is it used to predict user preferences? \(\ddagger\)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向您喜欢的AI聊天机器人询问SVD在推荐系统中的应用。它是如何用来预测用户偏好的？\(\ddagger\)'
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about nonnegative matrix factorization
    (NMF) and how it compares to SVD. What are the key differences in their constraints
    and applications? How does NMF handle interpretability in topics like text analysis
    or image processing? Explore some algorithms used to compute NMF. \(\ddagger\)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向您喜欢的AI聊天机器人询问非负矩阵分解（NMF）及其与SVD的比较。它们在约束和应用程序中的关键差异是什么？NMF在文本分析或图像处理等主题中如何处理可解释性？探索一些用于计算NMF的算法。\(\ddagger\)'
- en: 4.6.3\. Ridge regression[#](#ridge-regression "Link to this heading")
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6.3\. 岭回归[#](#ridge-regression "链接到这个标题")
- en: Here we consider what is called Tikhonov regularization\(\idx{Tikhonov regularization}\xdi\),
    an idea that turns out to be useful in overdetermined linear systems, particularly
    when the columns of the matrix \(A\) are linearly dependent or close to linearly
    dependent (which is sometimes referred to as [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\(\idx{multicollinearity}\xdi\)
    in statistics). It trades off minimizing the fit to the data versus minimizing
    the norm of the solution. More precisely, for a parameter \(\lambda > 0\) to be
    chosen, we solve\(\idx{ridge regression}\xdi\)
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们考虑所谓的Tikhonov正则化\(\idx{Tikhonov正则化}\xdi\)，这个想法最终证明在过定线性系统中很有用，尤其是在矩阵 \(A\)
    的列线性相关或接近线性相关时（在统计学中有时被称为[多重共线性](https://en.wikipedia.org/wiki/Multicollinearity)\(\idx{多重共线性}\xdi\)）。它权衡了最小化数据拟合与最小化解的范数。更精确地说，为了选择一个参数
    \(\lambda > 0\)，我们解决\(\idx{岭回归}\xdi\)
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2. \]
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2. \]
- en: The second term is referred to as an \(L_2\)-regularizer\(\idx{L2-regularization}\xdi\).
    Here \(A \in \mathbb{R}^{n\times m}\) with \(n \geq m\) and \(\mathbf{b} \in \mathbb{R}^n\).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项被称为 \(L_2\)-正则化器\(\idx{L2-正则化}\xdi\)。在这里 \(A \in \mathbb{R}^{n\times m}\)
    且 \(n \geq m\)，\(\mathbf{b} \in \mathbb{R}^n\)。
- en: To solve this optimization problem, we show that the objective function is strongly
    convex. We then find its unique stationary point. Rewriting the objective in quadratic
    function form
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个优化问题，我们证明目标函数是强凸的。然后我们找到它的唯一驻点。将目标函数重写为二次函数形式
- en: \[\begin{align*} f(\mathbf{x}) &= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2\\ &= \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x}
    + \mathbf{b}^T \mathbf{b} + \lambda \mathbf{x}^T \mathbf{x}\\ &= \mathbf{x}^T
    (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T
    \mathbf{b}\\ &= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \end{align*}\]
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x}) &= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda
    \|\mathbf{x}\|_2^2\\ &= \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x}
    + \mathbf{b}^T \mathbf{b} + \lambda \mathbf{x}^T \mathbf{x}\\ &= \mathbf{x}^T
    (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T
    \mathbf{b}\\ &= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \end{align*}\]
- en: where \(P = 2 (A^T A + \lambda I_{m \times m})\) is symmetric, \(\mathbf{q}
    = - 2 A^T \mathbf{b}\), and \(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(P = 2 (A^T A + \lambda I_{m \times m})\) 是对称的，\(\mathbf{q} = - 2 A^T \mathbf{b}\)，且
    \(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\)。
- en: As we previously computed, the Hessian of \(f\) is \(H_f(\mathbf{x})= P\). Now
    comes a key observation. The matrix \(P\) is positive definite whenever \(\lambda
    > 0\). Indeed, for any \(\mathbf{z} \in \mathbb{R}^m\),
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前计算的，\(f\) 的Hessian矩阵是 \(H_f(\mathbf{x})= P\)。现在有一个关键观察。当 \(\lambda > 0\)
    时，矩阵 \(P\) 总是正定的。确实，对于任何 \(\mathbf{z} \in \mathbb{R}^m\)，
- en: \[ \mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z} = 2 \|A \mathbf{z}\|_2^2
    + 2 \lambda \|\mathbf{z}\|_2^2 > 0. \]
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z} = 2 \|A \mathbf{z}\|_2^2
    + 2 \lambda \|\mathbf{z}\|_2^2 > 0. \]
- en: Let \(\mu = 2 \lambda > 0\). Then \(f\) is \(\mu\)-strongly convex. This holds
    whether or not the columns of \(A\) are linearly independent.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\mu = 2 \lambda > 0\)。那么 \(f\) 是 \(\mu\)-强凸的。这无论 \(A\) 的列是否线性无关都成立。
- en: The stationary points are easily characterized. Recall that the gradient is
    \(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\). Equating to \(\mathbf{0}\)
    leads to the system
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 驻点很容易被描述。回想一下梯度是 \(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\)。将其等于 \(\mathbf{0}\)
    导致以下系统
- en: \[ 2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
    \]
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: \[ 2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
    \]
- en: that is
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是说
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}. \]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}. \]
- en: The matrix in parenthesis is invertible as it is \(1/2\) of the Hessian, which
    is positive definite.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的矩阵是可逆的，因为它等于Hessian矩阵的一半，而Hessian矩阵是正定的。
- en: '**Connection to SVD** Expressing the solution in terms of a compact SVD \(A
    = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) provides
    some insights into how ridge regression works. Suppose that \(A\) has full column
    rank. That implies that \(V V^T = I_{m \times m}\). Then observe that'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**与SVD的联系** 将解用紧凑的SVD \(A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) 表示，可以提供一些关于岭回归如何工作的见解。假设 \(A\) 具有满列秩。这意味着 \(V V^T = I_{m \times
    m}\)。然后观察'
- en: \[\begin{align*} (A^T A + \lambda I_{m \times m})^{-1} &= (V \Sigma U^T U \Sigma
    V^T + \lambda I_{m \times m})^{-1}\\ &= (V \Sigma^2 V^T + \lambda I_{m \times
    m})^{-1}\\ &= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\ &= (V [\Sigma^2
    + \lambda I_{m \times m}] V^T)^{-1}\\ &= V (\Sigma^2 + \lambda I_{m \times m})^{-1}
    V^T. \end{align*}\]
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (A^T A + \lambda I_{m \times m})^{-1} &= (V \Sigma U^T U \Sigma
    V^T + \lambda I_{m \times m})^{-1}\\ &= (V \Sigma^2 V^T + \lambda I_{m \times
    m})^{-1}\\ &= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\ &= (V [\Sigma^2
    + \lambda I_{m \times m}] V^T)^{-1}\\ &= V (\Sigma^2 + \lambda I_{m \times m})^{-1}
    V^T. \end{align*}\]
- en: Hence
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b} =
    V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b} = V (\Sigma^2
    + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}. \]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{**} = (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b} =
    V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b} = V (\Sigma^2
    + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}. \]
- en: Our predictions are
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测是
- en: \[\begin{align*} A \mathbf{x}^{**} &= U \Sigma V^T V (\Sigma^2 + \lambda I_{m
    \times m})^{-1} \Sigma U^T \mathbf{b}\\ &= U \Sigma (\Sigma^2 + \lambda I_{m \times
    m})^{-1} \Sigma U^T \mathbf{b}\\ &= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2
    + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}. \end{align*}\]
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{x}^{**} &= U \Sigma V^T V (\Sigma^2 + \lambda I_{m
    \times m})^{-1} \Sigma U^T \mathbf{b}\\ &= U \Sigma (\Sigma^2 + \lambda I_{m \times
    m})^{-1} \Sigma U^T \mathbf{b}\\ &= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2
    + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}. \end{align*}\]
- en: Note that the terms in curly brackets are \(< 1\) when \(\lambda > 0\).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当 \(\lambda > 0\) 时，括号中的项是 \(< 1\) 的。
- en: Compare this to the unregularized least squares solution, which is obtained
    simply by setting \(\lambda = 0\) above,
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与未正则化的最小二乘解进行比较，该解简单地通过将上式中的 \(\lambda\) 设置为 0 来获得，
- en: \[\begin{align*} A \mathbf{x}^* &= \sum_{j=1}^r \mathbf{u}_j \mathbf{u}_j^T
    \mathbf{b}. \end{align*}\]
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} A \mathbf{x}^* &= \sum_{j=1}^r \mathbf{u}_j \mathbf{u}_j^T
    \mathbf{b}. \end{align*}\]
- en: The difference is that the regularized solution reduces the contributions from
    the left singular vectors corresponding to small singular values.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 差异在于正则化解减少了对应于小奇异值的左奇异向量的贡献。
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude、Gemini和ChatGPT协助)*'
- en: '**1** Which of the following best describes the Frobenius norm of a matrix
    \(A \in \mathbb{R}^{n \times m}\)?'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 以下哪项最好地描述了矩阵 \(A \in \mathbb{R}^{n \times m}\) 的Frobenius范数？'
- en: a) The maximum singular value of \(A\).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: a) 矩阵 \(A\) 的最大奇异值。
- en: b) The square root of the sum of the squares of all entries in \(A\).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: b) 矩阵 \(A\) 中所有元素平方和的平方根。
- en: c) The maximum absolute row sum of \(A\).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: c) 矩阵 \(A\) 的最大绝对行和。
- en: d) The maximum absolute column sum of \(A\).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: d) 矩阵 \(A\) 的最大绝对列和。
- en: '**2** Let \(A \in \mathbb{R}^{n \times m}\) be a matrix with SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\) and let \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) be the truncated SVD with \(k < r\). Which of the following is
    true about the Frobenius norm of \(A - A_k\)?'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 设 \(A \in \mathbb{R}^{n \times m}\) 为一个矩阵，其奇异值分解为 \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)，并且设 \(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j
    \mathbf{v}_j^T\) 为截断奇异值分解，其中 \(k < r\)。以下关于 \(A - A_k\) 的Frobenius范数的说法哪一个是正确的？'
- en: a) \(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)
- en: b) \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)
- en: c) \(\|A - A_k\|_F^2 = \sigma_k^2\)
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\|A - A_k\|_F^2 = \sigma_k^2\)
- en: d) \(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)
- en: '**3** The ridge regression problem is formulated as \(\min_{\mathbf{x} \in
    \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\).
    What is the role of the parameter \(\lambda\)?'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 岭回归问题被表述为 \(\min_{\mathbf{x} \in \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2
    + \lambda \|\mathbf{x}\|_2^2\)。参数 \(\lambda\) 的作用是什么？'
- en: a) It controls the trade-off between fitting the data and minimizing the norm
    of the solution.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它控制着拟合数据和最小化解的范数之间的权衡。
- en: b) It determines the rank of the matrix \(A\).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它决定了矩阵 \(A\) 的秩。
- en: c) It is the smallest singular value of \(A\).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它是 \(A\) 的最小奇异值。
- en: d) It is the largest singular value of \(A\).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它是 \(A\) 的最大奇异值。
- en: '**4** Let \(A\) be an \(n \times m\) matrix with compact SVD \(A = \sum_{j=1}^r
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). How does the ridge regression solution
    \(\mathbf{x}^{**}\) compare to the least squares solution \(\mathbf{x}^*\)?'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 设 \(A\) 为一个 \(n \times m\) 的矩阵，具有紧凑奇异值分解 \(A = \sum_{j=1}^r \sigma_j
    \mathbf{u}_j \mathbf{v}_j^T\)。岭回归解 \(\mathbf{x}^{**}\) 与最小二乘解 \(\mathbf{x}^*\)
    如何比较？'
- en: a) \(\mathbf{x}^{**}\) has larger components along the left singular vectors
    corresponding to small singular values.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbf{x}^{**}\) 在对应于小奇异值的左奇异向量上的分量更大。
- en: b) \(\mathbf{x}^{**}\) has smaller components along the left singular vectors
    corresponding to small singular values.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{x}^{**}\) 在对应于小奇异值的左奇异向量上的分量更小。
- en: c) \(\mathbf{x}^{**}\) is identical to \(\mathbf{x}^*\).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathbf{x}^{**}\) 与 \(\mathbf{x}^*\) 相同。
- en: d) None of the above.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: d) 以上皆非。
- en: '**5** (*Note:* Refers to online supplementary materials.) Let \(A \in \mathbb{R}^{n
    \times n}\) be a square nonsingular matrix with compact SVD \(A = \sum_{j=1}^n
    \sigma_j \mathbf{u}_j \mathbf{v}_j^T\). Which of the following is true about the
    induced 2-norm of the inverse \(A^{-1}\)?'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** (*注意：指代在线补充材料。) 设 \(A \in \mathbb{R}^{n \times n}\) 为一个具有紧凑奇异值分解的方阵 \(A
    = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)。以下关于 \(A^{-1}\) 的诱导 2-范数的说法哪一个是正确的？'
- en: a) \(\|A^{-1}\|_2 = \sigma_1\)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\|A^{-1}\|_2 = \sigma_1\)
- en: b) \(\|A^{-1}\|_2 = \sigma_n\)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\|A^{-1}\|_2 = \sigma_n\)
- en: c) \(\|A^{-1}\|_2 = \sigma_1^{-1}\)
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\|A^{-1}\|_2 = \sigma_1^{-1}\)
- en: d) \(\|A^{-1}\|_2 = \sigma_n^{-1}\)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\|A^{-1}\|_2 = \sigma_n^{-1}\)
- en: 'Answer for 1: b. Justification: The text defines the Frobenius norm of an \(n
    \times m\) matrix \(A\) as \(\|A\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{m} a_{ij}^2}\).'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 1题的答案：b. 理由：文本定义了一个 \(n \times m\) 矩阵 \(A\) 的Frobenius范数为 \(\|A\|_F = \sqrt{\sum_{i=1}^{n}
    \sum_{j=1}^{m} a_{ij}^2}\)。
- en: 'Answer for 2: b. Justification: The text proves that \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r
    \sigma_j^2\) in the Matrix Norms and Singular Values: Truncation Lemma.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 2题的答案：b. 理由：文本在矩阵范数和奇异值：截断引理中证明了 \(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)。
- en: 'Answer for 3: a. Justification: The text explains that ridge regression “trades
    off minimizing the fit to the data versus minimizing the norm of the solution,”
    and \(\lambda\) is the parameter that controls this trade-off.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 3题的答案：a. 理由：文本解释说岭回归“在最小化拟合数据和最小化解的范数之间进行权衡”，而 \(\lambda\) 是控制这种权衡的参数。
- en: 'Answer for 4: b. Justification: The text notes that the ridge regression solution
    “reduces the contributions from the left singular vectors corresponding to small
    singular values.”'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 4题的答案：b. 理由：文本指出岭回归解“减少了对应于小奇异值的左奇异向量的贡献。”
- en: 'Answer for 5: d. Justification: The text shows in an example that for a square
    nonsingular matrix \(A\), \(\|A^{-1}\|_2 = \sigma_n^{-1}\), where \(\sigma_n\)
    is the smallest singular value of \(A\).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 5题的答案：d. 理由：文本通过一个例子表明，对于方阵 \(A\)，\(\|A^{-1}\|_2 = \sigma_n^{-1}\)，其中 \(\sigma_n\)
    是 \(A\) 的最小奇异值。
