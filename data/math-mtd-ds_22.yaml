- en: 3.5\. Gradient descent and its convergence analysis#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.5\. 梯度下降及其收敛性分析#
- en: 原文：[https://mmids-textbook.github.io/chap03_opt/05_gd/roch-mmids-opt-gd.html](https://mmids-textbook.github.io/chap03_opt/05_gd/roch-mmids-opt-gd.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap03_opt/05_gd/roch-mmids-opt-gd.html](https://mmids-textbook.github.io/chap03_opt/05_gd/roch-mmids-opt-gd.html)
- en: 'We consider a natural approach for solving optimization problems numerically:
    a class of algorithms known as descent methods.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一种自然的方法来解决优化问题的数值问题：一类被称为下降法的算法。
- en: 'Let \(f : \mathbb{R}^d \to \mathbb{R}\) be continuously differentiable. We
    restrict ourselves to unconstrained minimization problems of the form'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是连续可微的。我们限制自己于无约束最小化问题，形式如下'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}). \]
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}). \]
- en: Ideally one would like to identify a global minimizer of \(f\). A naive approach
    might be to evaluate \(f\) at a large number of points \(\mathbf{x}\), say on
    a dense grid. However, even if we were satisfied with an approximate solution
    and limited ourselves to a bounded subset of the domain of \(f\), this type of
    [exhaustive search](https://en.wikipedia.org/wiki/Brute-force_search) is wasteful
    and impractical in large dimension \(d\), as the number of points interrogated
    grows exponentially with \(d\).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，人们希望识别 \(f\) 的全局最小值。一种天真（naive）的方法可能是评估 \(f\) 在大量点 \(\mathbf{x}\) 上的值，比如在一个密集的网格上。然而，即使我们满足于一个近似解，并限制自己只考虑
    \(f\) 的定义域的一个有界子集，这种类型的 [穷举搜索](https://en.wikipedia.org/wiki/Brute-force_search)
    在高维 \(d\) 的情况下也是浪费时间和不切实际的，因为被查询的点数会随着 \(d\) 的增长而指数级增长。
- en: A less naive approach might be to find all stationary points of \(f\), that
    is, those \(\mathbf{x}\) such that \(\nabla f(\mathbf{x}) = \mathbf{0}\). And
    then choose an \(\mathbf{x}\) among them that produces the smallest value of \(f(\mathbf{x})\).
    This indeed works in many problems, like the following example we have encountered
    previously.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不太天真（naive）的方法可能是找到 \(f\) 的所有驻点，即那些 \(\mathbf{x}\) 使得 \(\nabla f(\mathbf{x})
    = \mathbf{0}\)。然后从这些中选一个 \(\mathbf{x}\)，使得 \(f(\mathbf{x})\) 的值最小。这在许多问题中确实有效，例如我们之前遇到的一个例子。
- en: '**EXAMPLE:** **(Least Squares)** Consider again the least squares problem\(\idx{least
    squares problem}\xdi\)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**例（最小二乘法）** 再次考虑最小二乘问题（least squares problem）'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \|A \mathbf{x} - \mathbf{b}\|^2 \]
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \|A \mathbf{x} - \mathbf{b}\|^2 \]
- en: where \(A \in \mathbb{R}^{n \times d}\) has full column rank and \(\mathbf{b}
    \in \mathbb{R}^n\). In particular, \(d \leq n\). We saw in a previous example
    that the objective function is a quadratic function
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(A \in \mathbb{R}^{n \times d}\) 具有满列秩，\(\mathbf{b} \in \mathbb{R}^n\)。特别是，\(d
    \leq n\)。在先前的例子中，我们看到了目标函数是一个二次函数
- en: \[ f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x}
    + r, \]
- en: where \(P = 2 A^T A\) is symmetric, \(\mathbf{q} = - 2 A^T \mathbf{b}\), and
    \(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\). We also showed that \(f\) is
    \(\mu\)-strongly convex. So there is a unique global minimizer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(P = 2 A^T A\) 是对称的，\(\mathbf{q} = - 2 A^T \mathbf{b}\)，且 \(r= \mathbf{b}^T
    \mathbf{b} = \|\mathbf{b}\|^2\)。我们还证明了 \(f\) 是 \(\mu\)-强凸的。因此存在唯一的全局最小值。
- en: By a previous calculation,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过之前的计算，
- en: \[ \nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q} = 2 A^T A \mathbf{x} - 2
    A^T \mathbf{b}. \]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q} = 2 A^T A \mathbf{x} - 2
    A^T \mathbf{b}. \]
- en: So the stationary points satisfy
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以驻点满足
- en: \[ A^T A \mathbf{x} = A^T \mathbf{b} \]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{x} = A^T \mathbf{b} \]
- en: which you may recognize as the normal equations\(\idx{normal equations}\xdi\)
    for the least-squares problem. \(\lhd\)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能被你识别为最小二乘问题（least squares problem）的正则方程（normal equations）。\(\lhd\)
- en: Unfortunately, identifying stationary points often leads to systems of nonlinear
    equations that do not have explicit solutions. Hence we resort to a different
    approach.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，识别驻点通常会导致没有显式解的非线性方程组。因此我们采取不同的方法。
- en: 3.5.1\. Gradient descent[#](#gradient-descent "Link to this heading")
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5.1\. 梯度下降[#](#gradient-descent "链接到本标题")
- en: In gradient descent, we attempt to find smaller values of \(f\) by successively
    following directions in which \(f\) decreases locally. As we have seen in the
    proof of the *First-Order Necessary Optimality Condition*, \(- \nabla f\) provides
    such a direction. In fact, it is the direction of steepest descent in the following
    sense.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降中，我们试图通过依次跟随 \(f\) 在局部减少的方向来找到 \(f\) 的较小值。正如我们在 *一阶必要最优性条件* 的证明中所看到的，\(-
    \nabla f\) 提供了这样的方向。实际上，它是在以下意义上的最速下降方向。
- en: Recall from the *Descent Direction and Directional Derivative Lemma* that \(\mathbf{v}\)
    is a descent direction at \(\mathbf{x}_0\) if the directional derivative of \(f\)
    at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is negative.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下从**下降方向和方向导数引理**中得知，如果 \(f\) 在 \(\mathbf{x}_0\) 处沿 \(\mathbf{v}\) 方向的方向导数是负的，那么
    \(\mathbf{v}\) 是 \(\mathbf{x}_0\) 处的一个下降方向。
- en: '**LEMMA** **(Steepest Descent)** \(\idx{steepest descent lemma}\xdi\) Let \(f
    : \mathbb{R}^d \to \mathbb{R}\) be continuously differentiable at \(\mathbf{x}_0\).
    For any unit vector \(\mathbf{v} \in \mathbb{R}^d\),'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(最速下降)** \(\idx{steepest descent lemma}\xdi\) 设 \(f : \mathbb{R}^d
    \to \mathbb{R}\) 在 \(\mathbf{x}_0\) 处是连续可微的。对于 \(\mathbb{R}^d\) 中的任意单位向量 \(\mathbf{v}
    \in \mathbb{R}^d\)，'
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} \geq \frac{\partial
    f (\mathbf{x}_0)}{\partial \mathbf{v}^*} \]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} \geq \frac{\partial
    f (\mathbf{x}_0)}{\partial \mathbf{v}^*} \]
- en: where
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[ \mathbf{v}^* = - \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}.
    \]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}^* = - \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}.
    \]
- en: \(\flat\)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* This is an immediate application of the *Cauchy-Schwarz inequality*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 这是对**柯西-施瓦茨不等式**的直接应用。'
- en: '*Proof:* By the *Cauchy-Schwarz inequality*, since \(\mathbf{v}\) has unit
    norm,'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过**柯西-施瓦茨不等式**，由于 \(\mathbf{v}\) 的范数为单位，'
- en: \[\begin{align*} \left|\frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}}\right|
    &= \left|\nabla f(\mathbf{x}_0)^T \mathbf{v}\right|\\ &\leq \|\nabla f(\mathbf{x}_0)\|
    \|\mathbf{v}\|\\ &= \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \left|\frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}}\right|
    &= \left|\nabla f(\mathbf{x}_0)^T \mathbf{v}\right|\\ &\leq \|\nabla f(\mathbf{x}_0)\|
    \|\mathbf{v}\|\\ &= \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
- en: Or, put differently,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，换一种说法，
- en: \[ - \|\nabla f(\mathbf{x}_0)\| \leq \frac{\partial f (\mathbf{x}_0)}{\partial
    \mathbf{v}} \leq \|\nabla f(\mathbf{x}_0)\|. \]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - \|\nabla f(\mathbf{x}_0)\| \leq \frac{\partial f (\mathbf{x}_0)}{\partial
    \mathbf{v}} \leq \|\nabla f(\mathbf{x}_0)\|. \]
- en: On the other hand, by the choice of \(\mathbf{v}^*\),
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，根据 \(\mathbf{v}^*\) 的选择，
- en: \[\begin{align*} \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}^*} &=
    \nabla f(\mathbf{x}_0)^T \left(- \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\nabla f(\mathbf{x}_0)^T \nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\|\nabla f(\mathbf{x}_0)\|^2}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}^*} &=
    \nabla f(\mathbf{x}_0)^T \left(- \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\nabla f(\mathbf{x}_0)^T \nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\|\nabla f(\mathbf{x}_0)\|^2}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
- en: The last two displays combined give the result. \(\square\)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个显示的结果合并给出。\(\square\)
- en: At each iteration of gradient descent, we take a step in the direction of the
    negative of the gradient, that is,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降的每次迭代中，我们沿着梯度的负方向迈出一步，即，
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t), \quad
    t=0,1,2\ldots \]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t), \quad
    t=0,1,2\ldots \]
- en: for a sequence of step sizes \(\alpha_t > 0\). Choosing the right step size
    (also known as steplength or learning rate) is a large subject in itself. We will
    only consider the case of fixed step size here.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一系列步长 \(\alpha_t > 0\)。选择合适的步长（也称为步长或学习率）本身就是一个很大的主题。在这里，我们只考虑固定步长的情况。
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the different approaches
    for selecting a step size in gradient descent methods. \(\ddagger\)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向你最喜欢的 AI 聊天机器人询问梯度下降方法中选择步长不同方法的细节。\(\ddagger\)'
- en: 'In general, we will not be able to guarantee that a global minimizer is reached
    in the limit, even if one exists. Our goal for now is more modest: to find a point
    where the gradient of \(f\) approximately vanishes.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们无法保证在极限情况下能够达到全局最小值，即使它确实存在。我们现在的目标更为谦逊：找到梯度 \(f\) 大致消失的点。
- en: We implement gradient descent\(\idx{gradient descent}\xdi\) in Python. We assume
    that a function `f` and its gradient `grad_f` are provided. We first code the
    basic steepest descent step with a step size\(\idx{step size}\xdi\) \(\idx{learning
    rate}\xdi\) \(\alpha =\) `alpha`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现了梯度下降\(\idx{gradient descent}\xdi\)。我们假设提供了一个函数 `f` 和它的梯度 `grad_f`。我们首先用步长\(\idx{step
    size}\xdi\) \(\idx{learning rate}\xdi\) \(\alpha =\) `alpha` 编写基本的梯度下降步骤。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**NUMERICAL CORNER:** We illustrate on a simple example.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们用一个简单的例子来说明。'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![../../_images/0697c218312ddc584b4a0edc5e583702b6afa69be250f4eac766440ceda0f7d2.png](../Images/8fbfd5ba011f320f65a37c311822755c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/0697c218312ddc584b4a0edc5e583702b6afa69be250f4eac766440ceda0f7d2.png](../Images/8fbfd5ba011f320f65a37c311822755c.png)'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We found a global minmizer in this case.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们找到了一个全局最小值。
- en: The next example shows that a different local minimizer may be reached depending
    on the starting point.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个例子表明，根据起始点不同，可能达到不同的局部最小值。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![../../_images/48729baf93e6b40366984a812582a50a90890a4c4e42f79a718f921c414b5b55.png](../Images/c2b85b3ea7eba3a62567ae4ff6962fca.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/48729baf93e6b40366984a812582a50a90890a4c4e42f79a718f921c414b5b55.png](../Images/c2b85b3ea7eba3a62567ae4ff6962fca.png)'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**TRY IT!** In this last example, does changing the step size affect the outcome?
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试一下！** 在这个最后一个例子中，改变步长会影响结果吗？ ([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
- en: In the final example, we end up at a stationary point that is not a local minimizer.
    Here both the first and second derivatives are zero. This is known as a [saddle
    point](https://en.wikipedia.org/wiki/Saddle_point).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个例子中，我们最终到达一个不是局部最小值的驻点。这里一阶和二阶导数都是零。这被称为 [鞍点](https://en.wikipedia.org/wiki/Saddle_point)。
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![../../_images/68a876bde430631abb6621a64c476ac59b86424854df7da519e2a8ba40834bcf.png](../Images/b3363868479c1ce17160f5f595b4b954.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/68a876bde430631abb6621a64c476ac59b86424854df7da519e2a8ba40834bcf.png](../Images/b3363868479c1ce17160f5f595b4b954.png)'
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: \(\unlhd\)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 3.5.2\. Convergence analysis[#](#convergence-analysis "Link to this heading")
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5.2\. 收敛性分析[#](#convergence-analysis "链接到这个标题")
- en: In this section, we prove some results about the convergence\(\idx{convergence
    analysis}\xdi\) of gradient descent. We start with the smooth case.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们证明关于梯度下降的收敛性\(\idx{convergence analysis}\xdi\)的一些结果。我们首先从光滑情况开始。
- en: '**Smooth case** Informally, a function is smooth if its gradient does not change
    too fast. The formal definition we will use here follows. We restrict ourselves
    to the twice continuously differentiable case.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**光滑情况** 非正式地说，如果一个函数的梯度变化不是太快，那么这个函数是光滑的。我们在这里将使用以下正式定义。我们限制自己到二阶连续可微的情况。'
- en: '**DEFINITION** **(Smooth Function)** \(\idx{smooth function}\xdi\) Let \(f
    : \mathbb{R}^d \to \mathbb{R}\) be twice continuously differentiable. We say that
    \(f\) is \(L\)-smooth if'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（光滑函数）** \(\idx{smooth function}\xdi\) 设 \(f : \mathbb{R}^d \to \mathbb{R}\)
    是二阶连续可微的。我们说 \(f\) 是 \(L\)-光滑的，如果'
- en: \[ - L I_{d \times d} \preceq H_f(\mathbf{x}) \preceq L I_{d \times d}, \quad
    \forall \mathbf{x} \in \mathbb{R}^d. \]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L I_{d \times d} \preceq H_f(\mathbf{x}) \preceq L I_{d \times d}, \quad
    \forall \mathbf{x} \in \mathbb{R}^d. \]
- en: \(\natural\)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: In the single-variable case, this reduces to \(- L \leq f''(x) \leq L\) for
    all \(x \in \mathbb{R}\). More generally, recall that
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量情况下，这可以简化为对于所有 \(x \in \mathbb{R}\)，有 \(- L \leq f''(x) \leq L\)。更一般地，回忆一下
- en: \[ A \preceq B \iff \mathbf{z}^T A\mathbf{z} \leq \mathbf{z}^T B\mathbf{z},
    \qquad \forall \mathbf{z} \in \mathbb{R}^{d}. \]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \preceq B \iff \mathbf{z}^T A\mathbf{z} \leq \mathbf{z}^T B\mathbf{z},
    \qquad \forall \mathbf{z} \in \mathbb{R}^{d}. \]
- en: So the condition above is equivalent to
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上述条件等价于
- en: \[ - L \|\mathbf{z}\|^2 \leq \mathbf{z}^T H_f(\mathbf{x}) \,\mathbf{z} \leq
    L \|\mathbf{z}\|^2, \quad \forall \mathbf{x}, \mathbf{z} \in \mathbb{R}^d. \]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L \|\mathbf{z}\|^2 \leq \mathbf{z}^T H_f(\mathbf{x}) \,\mathbf{z} \leq
    L \|\mathbf{z}\|^2, \quad \forall \mathbf{x}, \mathbf{z} \in \mathbb{R}^d. \]
- en: A different way to put this is that the second directional derivative satisfies
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表述方式是，二阶方向导数满足
- en: \[ - L \leq \frac{\partial^2 f (\mathbf{x})}{\partial \mathbf{v}^2} \leq L \]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L \leq \frac{\partial^2 f (\mathbf{x})}{\partial \mathbf{v}^2} \leq L \]
- en: for all \(\mathbf{x} \in \mathbb{R}^d\) and all unit vectors \(\mathbf{v} \in
    \mathbb{R}^d\).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(\mathbf{x} \in \mathbb{R}^d\) 和所有单位向量 \(\mathbf{v} \in \mathbb{R}^d\).
- en: Combined with *Taylor’s Theorem*, this gives immediately the following.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 结合 *泰勒定理*，这立即给出以下结论。
- en: '**LEMMA** **(Quadratic Bound for Smooth Functions)** \(\idx{quadratic bound
    for smooth functions}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) be twice continuously
    differentiable. Then \(f\) is \(L\)-smooth if and only if for all \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^d\) it holds that'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（光滑函数的二次界限）** \(\idx{quadratic bound for smooth functions}\xdi\) 设
    \(f : \mathbb{R}^d \to \mathbb{R}\) 是二阶连续可微的。那么 \(f\) 是 \(L\)-光滑的，当且仅当对于所有 \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^d\)，它满足'
- en: \[ \left|f(\mathbf{y}) - \{f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{y}
    - \mathbf{x})\}\right| \leq \frac{L}{2} \|\mathbf{y} - \mathbf{x}\|^2. \]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left|f(\mathbf{y}) - \{f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{y}
    - \mathbf{x})\}\right| \leq \frac{L}{2} \|\mathbf{y} - \mathbf{x}\|^2. \]
- en: \(\flat\)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* We apply the *Taylor’s Theorem*, then bound the second-order
    term.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* 我们应用 *泰勒定理*，然后对二阶项进行界定。'
- en: '*Proof:* By *Taylor’s Theorem*, for any \(\alpha > 0\) there is \(\xi_\alpha
    \in (0,1)\)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过 *泰勒定理*，对于任意 \(\alpha > 0\)，存在 \(\xi_\alpha \in (0,1)\)'
- en: \[ f(\mathbf{x} + \alpha \mathbf{p}) = f(\mathbf{x}) + \alpha \nabla f(\mathbf{x})^T
    \mathbf{p} + \frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p} \]
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x} + \alpha \mathbf{p}) = f(\mathbf{x}) + \alpha \nabla f(\mathbf{x})^T
    \mathbf{p} + \frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p} \]
- en: where \(\mathbf{p} = \mathbf{y} - \mathbf{x}\).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{p} = \mathbf{y} - \mathbf{x}\).
- en: If \(f\) is \(L\)-smooth, then at \(\alpha = 1\) by the observation above
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 \(f\) 是 \(L\)-平滑的，那么在 \(\alpha = 1\) 时，通过上述观察
- en: \[ - L \|\mathbf{p}\|^2 \leq \mathbf{p}^T \,H_f(\mathbf{x} + \xi_1 \mathbf{p})
    \,\mathbf{p} \leq L \|\mathbf{p}\|^2. \]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L \|\mathbf{p}\|^2 \leq \mathbf{p}^T \,H_f(\mathbf{x} + \xi_1 \mathbf{p})
    \,\mathbf{p} \leq L \|\mathbf{p}\|^2. \]
- en: That implies the inequality in the statement.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着陈述中的不等式。
- en: On the other hand, if that inequality holds, by combining with the Taylor expansion
    above we get
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果这个不等式成立，通过结合上述泰勒展开，我们得到
- en: \[ \left|\,\frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p}\,\right| \leq \frac{L}{2} \alpha^2 \|\mathbf{p}\|^2 \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left|\,\frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p}\,\right| \leq \frac{L}{2} \alpha^2 \|\mathbf{p}\|^2 \]
- en: where we used that \(\|\alpha \mathbf{p}\| = \alpha \|\mathbf{p}\|\) by absolute
    homogeneity of the norm. Dividing by \(\alpha^2/2\), then taking \(\alpha \to
    0\) and using the continuity of the Hessian gives
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了由范数的绝对齐次性得出的 \(\|\alpha \mathbf{p}\| = \alpha \|\mathbf{p}\|\)。除以 \(\alpha^2/2\)，然后取
    \(\alpha \to 0\) 并使用 Hessian 的连续性给出
- en: \[ \left|\, \mathbf{p}^T \,H_f(\mathbf{x}) \,\mathbf{p} \,\right| \leq L \|\mathbf{p}\|^2.
    \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left|\, \mathbf{p}^T \,H_f(\mathbf{x}) \,\mathbf{p} \,\right| \leq L \|\mathbf{p}\|^2.
    \]
- en: By the observation above again, that implies that \(f\) is \(L\)-smooth. \(\square\)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述观察再次，这意味着 \(f\) 是 \(L\)-平滑的。 \(\square\)
- en: We show next that, in the smooth case, steepest descent with an appropriately
    chosen step size produces a sequence of points whose objective values decrease
    (or stay the same) and whose gradients vanish in the limit. We also give a quantitative
    convergence rate. Note that this result does not imply convergence to a local
    (or global) minimizer.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将证明，在平滑的情况下，使用适当选择的步长进行最速下降法会产生一个目标值递减（或保持不变）且梯度在极限中消失的点的序列。我们还将给出一个定量的收敛率。请注意，这个结果并不意味着收敛到局部（或全局）最小值。
- en: '**THEOREM** **(Convergence of Gradient Descent in the Smooth Case)** \(\idx{convergence
    of gradient descent in the smooth case}\xdi\) Suppose that \(f : \mathbb{R}^d
    \to \mathbb{R}\) is \(L\)-smooth and bounded from below, that is, there is \(\bar{f}
    > - \infty\) such that \(f(\mathbf{x}) \geq \bar{f}\), \(\forall \mathbf{x} \in
    \mathbb{R}^d\). Then gradient descent with step size \(\alpha_t = \alpha := 1/L\)
    started from any \(\mathbf{x}^0\) produces a sequence \(\mathbf{x}^t\), \(t=1,2,\ldots\)
    such that'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(平滑情况下梯度下降的收敛性)** \(\idx{convergence of gradient descent in the smooth
    case}\xdi\) 假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-平滑且有下界，即存在 \(\bar{f}
    > - \infty\) 使得 \(f(\mathbf{x}) \geq \bar{f}\)，\(\forall \mathbf{x} \in \mathbb{R}^d\)。那么从任意
    \(\mathbf{x}^0\) 开始的梯度下降法，步长为 \(\alpha_t = \alpha := 1/L\)，将产生一个序列 \(\mathbf{x}^t\)，\(t=1,2,\ldots\)，使得'
- en: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t), \quad \forall t \]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t), \quad \forall t \]
- en: and
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: \[ \lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0. \]
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0. \]
- en: Moreover, after \(S\) steps, there is a \(t\) in \(\{0,\ldots,S\}\) such that
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 \(S\) 步之后，存在一个 \(t\) 在 \(\{0,\ldots,S\}\) 中，使得
- en: \[ \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L \left[\,f(\mathbf{x}^0) -
    \bar{f}\,\right]}{S}}. \]
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L \left[\,f(\mathbf{x}^0) -
    \bar{f}\,\right]}{S}}. \]
- en: \(\sharp\)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: The assumption that a lower bound on \(f\) is known may seem far-fetched. But
    there are in fact many settings where this is natural. For instance, in the case
    of the least-squares problem, the objective function \(f\) is non-negative by
    definition and therefore we can take \(\bar{f} = 0\).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 \(f\) 的下界是已知的，这似乎有些牵强。但实际上，有许多情况下这是自然的。例如，在最小二乘问题的案例中，目标函数 \(f\) 由定义是非负的，因此我们可以取
    \(\bar{f} = 0\)。
- en: A different way to put the claim above regarding the convergence rate is the
    following. Take any \(\epsilon > 0\). If our goal is to find a point \(\mathbf{x}\)
    such that \(\|\nabla f(\mathbf{x})\| \leq \epsilon\), then we are guaranteed to
    find one if we perform \(S\) steps such that
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关于上述收敛率的另一种表述方式如下。取任意 \(\epsilon > 0\)。如果我们的目标是找到一个点 \(\mathbf{x}\)，使得 \(\|\nabla
    f(\mathbf{x})\| \leq \epsilon\)，那么如果我们执行 \(S\) 步，就能保证找到这样一个点。
- en: \[ \min_{t = 0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L
    \left[\,f(\mathbf{x}^0) - \bar{f}\,\right]}{S}} \leq \epsilon, \]
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{t = 0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L
    \left[\,f(\mathbf{x}^0) - \bar{f}\,\right]}{S}} \leq \epsilon, \]
- en: that is, after rearranging,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 即，重新排列后，
- en: \[ S \geq \frac{2L [f(\mathbf{x}^0) - \bar{f}]}{\epsilon^2}. \]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \[ S \geq \frac{2L [f(\mathbf{x}^0) - \bar{f}]}{\epsilon^2}. \]
- en: The heart of the proof is the following fundamental inequality. It also informs
    the choice of step size.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 证明的核心是以下基本不等式。它还说明了步长的选择。
- en: '**LEMMA** **(Descent Guarantee in the Smooth Case)** \(\idx{descent guarantee
    in the smooth case}\xdi\) Suppose that \(f : \mathbb{R}^d \to \mathbb{R}\) is
    \(L\)-smooth. For any \(\mathbf{x} \in \mathbb{R}^d\),'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(光滑情况下的下降保证)** \(\idx{descent guarantee in the smooth case}\xdi\)
    假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-光滑的。对于任何 \(\mathbf{x} \in \mathbb{R}^d\)，'
- en: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2 L} \|\nabla f(\mathbf{x})\|^2. \]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2 L} \|\nabla f(\mathbf{x})\|^2. \]
- en: \(\flat\)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea (Descent Guarantee in the Smooth Case):* Intuitively, the *Quadratic
    Bound for Smooth Functions* shows that \(f\) is well approximated by a quadratic
    function in a neighborhood of \(\mathbf{x}\) whose size depends on the smoothness
    parameter \(L\). Choosing a step size that minimizes this approximation leads
    to a guaranteed improvement. The approach taken here is a special case of what
    is referred to as [Majorize-Minimization (MM)](https://en.wikipedia.org/wiki/MM_algorithm)\(\idx{Majorize-Minimization}\xdi\).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路（光滑情况下的下降保证）：* 直观上，*光滑函数的二次界*表明 \(f\) 在 \(\mathbf{x}\) 的邻域内被一个二次函数很好地近似，其大小取决于光滑参数
    \(L\)。选择最小化这种近似的步长可以保证改进。这里采取的方法是所谓的[主元最小化（MM）算法](https://en.wikipedia.org/wiki/MM_algorithm)\(\idx{Majorize-Minimization}\xdi\)的一个特例。'
- en: '*Proof:* *(Descent Guarantee in the Smooth Case)* By the *Quadratic Bound for
    Smooth Functions*, letting \(\mathbf{p} = - \nabla f(\mathbf{x})\)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(光滑情况下的下降保证)* 根据 *光滑函数的二次界*，令 \(\mathbf{p} = - \nabla f(\mathbf{x})\)'
- en: \[\begin{align*} f(\mathbf{x} + \alpha \mathbf{p}) &\leq f(\mathbf{x}) + \nabla
    f(\mathbf{x})^T (\alpha \mathbf{p}) + \frac{L}{2} \|\alpha \mathbf{p}\|^2\\ &=
    f(\mathbf{x}) - \alpha \|\nabla f(\mathbf{x})\|^2 + \alpha^2 \frac{L}{2} \|\nabla
    f(\mathbf{x})\|^2\\ &= f(\mathbf{x}) + \left( - \alpha + \alpha^2 \frac{L}{2}
    \right) \|\nabla f(\mathbf{x})\|^2. \end{align*}\]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x} + \alpha \mathbf{p}) &\leq f(\mathbf{x}) + \nabla
    f(\mathbf{x})^T (\alpha \mathbf{p}) + \frac{L}{2} \|\alpha \mathbf{p}\|^2\\ &=
    f(\mathbf{x}) - \alpha \|\nabla f(\mathbf{x})\|^2 + \alpha^2 \frac{L}{2} \|\nabla
    f(\mathbf{x})\|^2\\ &= f(\mathbf{x}) + \left( - \alpha + \alpha^2 \frac{L}{2}
    \right) \|\nabla f(\mathbf{x})\|^2. \end{align*}\]
- en: The quadratic function in parentheses is convex and minimized at the stationary
    point \(\alpha\) satisfying
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的二次函数是凸函数，并在满足 \(\alpha\) 的驻点处最小化
- en: \[ \frac{\mathrm{d}}{\mathrm{d} \alpha}\left( - \alpha + \alpha^2 \frac{L}{2}
    \right) = -1 + \alpha L = 0\. \]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\mathrm{d}}{\mathrm{d} \alpha}\left( - \alpha + \alpha^2 \frac{L}{2}
    \right) = -1 + \alpha L = 0\. \]
- en: Taking \(\alpha = 1/L\), where \(-\alpha + \alpha^2 \frac{L}{2} = - \frac{1}{2L}\),
    and replacing in the inequality above gives
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 取 \(\alpha = 1/L\)，其中 \(-\alpha + \alpha^2 \frac{L}{2} = - \frac{1}{2L}\)，并将此代入上述不等式中给出
- en: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2L}\|\nabla f(\mathbf{x})\|^2, \]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2L}\|\nabla f(\mathbf{x})\|^2, \]
- en: as claimed. \(\square\)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。 \(\square\)
- en: We return to the proof of the theorem.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到定理的证明。
- en: '*Proof idea (Convergence of Gradient Descent in the Smooth Case):* We use a
    telescoping argument to write \(f(\mathbf{x}^S)\) as a sum of stepwise increments,
    each of which can be bounded by the previous lemma. Because \(f(\mathbf{x}^S)\)
    is bounded from below, it then follows that the gradients must vanish in the limit.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路（光滑情况下的梯度下降收敛）：* 我们使用望远镜论证将 \(f(\mathbf{x}^S)\) 写成一系列步进增量之和，每个增量都可以由前面的引理进行界定。因为
    \(f(\mathbf{x}^S)\) 从下限是有界的，因此可以得出梯度必须在极限中消失的结论。'
- en: '*Proof:* *(Convergence of Gradient Descent in the Smooth Case)* By the *Descent
    Guarantee in the Smooth Case*,'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(光滑情况下的梯度下降收敛)* 根据 *光滑情况下的下降保证*，'
- en: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2 L}\|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t), \quad \forall t. \]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2 L}\|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t), \quad \forall t. \]
- en: Furthermore, using a telescoping sum, we get
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用望远镜求和，我们得到
- en: \[\begin{align*} f(\mathbf{x}^S) &= f(\mathbf{x}^0) + \sum_{t=0}^{S-1} [f(\mathbf{x}^{t+1})
    - f(\mathbf{x}^t)]\\ &\leq f(\mathbf{x}^0) - \frac{1}{2 L}\sum_{t=0}^{S-1} \|\nabla
    f(\mathbf{x}^t)\|^2\. \end{align*}\]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x}^S) &= f(\mathbf{x}^0) + \sum_{t=0}^{S-1} [f(\mathbf{x}^{t+1})
    - f(\mathbf{x}^t)]\\ &\leq f(\mathbf{x}^0) - \frac{1}{2 L}\sum_{t=0}^{S-1} \|\nabla
    f(\mathbf{x}^t)\|^2\. \end{align*}\]
- en: Rearranging and using \(f(\mathbf{x}^S) \geq \bar{f}\) leads to
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新排列并使用 \(f(\mathbf{x}^S) \geq \bar{f}\) 得到
- en: \[ \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2 \leq 2L [f(\mathbf{x}^0) -
    \bar{f}]. \]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2 \leq 2L [f(\mathbf{x}^0) -
    \bar{f}]. \]
- en: We get the quantitative bound
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个定量的界
- en: \[\begin{align*} \min_{t=0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\|^2 & \leq
    \frac{1}{S} \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2\\ &\leq \frac{2L [f(\mathbf{x}^0)
    - \bar{f}]}{S} \end{align*}\]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \min_{t=0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\|^2 & \leq
    \frac{1}{S} \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2\\ &\leq \frac{2L [f(\mathbf{x}^0)
    - \bar{f}]}{S} \end{align*}\]
- en: as the minimum is necessarily less or equal than the average. Moreover, as \(S
    \to +\infty\), we must have \(\|\nabla f(\mathbf{x}^S)\|^2 \to 0\) by standard
    [analytical](https://math.stackexchange.com/questions/62389/relationships-between-bounded-and-convergent-series)
    [arguments](https://math.stackexchange.com/questions/107961/if-a-series-converges-then-the-sequence-of-terms-converges-to-0).
    That proves the claim. \(\square\)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因为最小值必然小于或等于平均值。此外，当 \(S \to +\infty\) 时，根据标准的 [分析](https://math.stackexchange.com/questions/62389/relationships-between-bounded-and-convergent-series)
    [论证](https://math.stackexchange.com/questions/107961/if-a-series-converges-then-the-sequence-of-terms-converges-to-0)，我们必须有
    \(\|\nabla f(\mathbf{x}^S)\|^2 \to 0\)。这证明了该命题。 \(\square\)
- en: '**Smooth and strongly convex case** With stronger assumptions, we obtain stronger
    convergence results. One such assumption is strong convexity, which we defined
    in the previous section for twice continuously differentiable functions.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**光滑且强凸的情况** 在更强的假设下，我们可以获得更强的收敛结果。其中一个假设是强凸性，我们在上一节中为二阶连续可微函数定义了它。'
- en: We prove a convergence result for smooth, strongly convex functions. We show
    something stronger this time. We control the value of \(f\) itself and obtain
    a much faster rate of convergence. If \(f\) is \(m\)-strongly convex and has a
    global minimizer \(\mathbf{x}^*\), then the global minimizer is unique and characterized
    by \(\nabla f(\mathbf{x}^*) = \mathbf{0}\). Strong convexity allows us to relate
    the value of the function at a point \(\mathbf{x}\) and the gradient of \(f\)
    at that point. This is proved in the following lemma, which is key to our convergence
    result.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为光滑、强凸函数证明了一个收敛结果。这次我们展示了更强的事实。我们控制了 \(f\) 的值并获得了更快的收敛速度。如果 \(f\) 是 \(m\)-强凸的并且有一个全局最小值
    \(\mathbf{x}^*\)，那么全局最小值是唯一的，并且由 \(\nabla f(\mathbf{x}^*) = \mathbf{0}\) 来描述。强凸性允许我们将函数在点
    \(\mathbf{x}\) 的值与该点 \(f\) 的梯度联系起来。这将在以下引理中证明，它是我们收敛结果的关键。
- en: '**LEMMA** **(Relating a function and its gradient)** \(\idx{relating a function
    and its gradient lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) be twice
    continuously differentiable, \(m\)-strongly convex with a global minimizer at
    \(\mathbf{x}^*\). Then for any \(\mathbf{x} \in \mathbb{R}^d\)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(函数及其梯度的关系)** \(\idx{relating a function and its gradient lemma}\xdi\)
    设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是二阶连续可微的，\(m\)-强凸，并在 \(\mathbf{x}^*\) 处有全局最小值。那么对于任意的
    \(\mathbf{x} \in \mathbb{R}^d\)'
- en: \[ f(\mathbf{x}) - f(\mathbf{x}^*) \leq \frac{\|\nabla f(\mathbf{x})\|^2}{2
    m}. \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}) - f(\mathbf{x}^*) \leq \frac{\|\nabla f(\mathbf{x})\|^2}{2
    m}. \]
- en: \(\flat\)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* By the *Quadratic Bound for Strongly Convex Functions*,'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 根据 *强凸函数的二次界*，'
- en: '\[\begin{align*} f(\mathbf{x}^*) &\geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    (\mathbf{x}^* - \mathbf{x}) + \frac{m}{2} \|\mathbf{x}^* - \mathbf{x}\|^2\\ &=
    f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T (m
    I_{d \times d}) \,\mathbf{w}\\ &=: r + \mathbf{q}^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T
    P \,\mathbf{w} \end{align*}\]'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{align*} f(\mathbf{x}^*) &\geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    (\mathbf{x}^* - \mathbf{x}) + \frac{m}{2} \|\mathbf{x}^* - \mathbf{x}\|^2\\ &=
    f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T (m
    I_{d \times d}) \,\mathbf{w}\\ &=: r + \mathbf{q}^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T
    P \,\mathbf{w} \end{align*}\]'
- en: where on the second line we defined \(\mathbf{w} = \mathbf{x}^* - \mathbf{x}\).
    The right-hand side is a quadratic function in \(\mathbf{w}\) (for \(\mathbf{x}\)
    fixed), and on the third line we used our previous notation \(P\), \(\mathbf{q}\)
    and \(r\) for such a function. So the inequality is still valid if we replace
    \(\mathbf{w}\) with the global minimizer \(\mathbf{w}^*\) of that quadratic function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中在第二行我们定义了 \(\mathbf{w} = \mathbf{x}^* - \mathbf{x}\)。右侧是一个关于 \(\mathbf{w}\)
    的二次函数（对于固定的 \(\mathbf{x}\)），在第三行我们使用了我们之前的符号 \(P\)，\(\mathbf{q}\) 和 \(r\) 来表示这样的函数。因此，如果我们用全局最小值
    \(\mathbf{w}^*\) 替换 \(\mathbf{w}\)，不等式仍然成立。
- en: The matrix \(P = m I_{d \times d}\) is positive definite. By a previous example,
    we know that the minimizer is achieved when the gradient \(\frac{1}{2}[P + P^T]\mathbf{w}^*
    + \mathbf{q} = \mathbf{0}\), which is equivalent to
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(P = m I_{d \times d}\) 是正定的。通过之前的例子，我们知道当梯度 \(\frac{1}{2}[P + P^T]\mathbf{w}^*
    + \mathbf{q} = \mathbf{0}\) 时，最小值被达到，这等价于
- en: \[ \mathbf{w}^* = - (m I_{d \times d})^{-1} \nabla f(\mathbf{x}) = - (m^{-1}
    I_{d \times d}) \nabla f(\mathbf{x}) = - \frac{1}{m} \nabla f(\mathbf{x}). \]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^* = - (m I_{d \times d})^{-1} \nabla f(\mathbf{x}) = - (m^{-1}
    I_{d \times d}) \nabla f(\mathbf{x}) = - \frac{1}{m} \nabla f(\mathbf{x}). \]
- en: So, replacing \(\mathbf{w}\) with \(\mathbf{w}^*\), we have the inequality
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用 \(\mathbf{w}^*\) 替换 \(\mathbf{w}\)，我们得到不等式
- en: \[\begin{align*} f(\mathbf{x}^*) & \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & \quad \quad+ \frac{1}{2}
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}^T (m I_{d \times d}) \left\{-
    \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & = f(\mathbf{x}) - \frac{1}{2m} \|\nabla
    f(\mathbf{x})\|^2. \end{align*}\]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x}^*) & \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & \quad \quad+ \frac{1}{2}
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}^T (m I_{d \times d}) \left\{-
    \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & = f(\mathbf{x}) - \frac{1}{2m} \|\nabla
    f(\mathbf{x})\|^2. \end{align*}\]
- en: Rearranging gives the claim. \(\square\)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列后得到结论。 \(\square\)
- en: We can now state our convergence result.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以陈述我们的收敛结果。
- en: '**THEOREM** **(Convergence of Gradient Descent in the Strongly Convex Case)**
    \(\idx{convergence of gradient descent in the strongly convex case}\xdi\) Suppose
    that \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and \(m\)-strongly convex
    with a global minimizer at \(\mathbf{x}^*\). Then gradient descent with step size
    \(\alpha = 1/L\) started from any \(\mathbf{x}^0\) produces a sequence \(\mathbf{x}^t\),
    \(t=1,2,\ldots\) such that'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(强凸情况下梯度下降的收敛性)** \(\idx{强凸情况下梯度下降的收敛性}\xdi\) 假设 \(f : \mathbb{R}^d
    \to \mathbb{R}\) 是 \(L\)-光滑和 \(m\)-强凸的，并且全局最小值在 \(\mathbf{x}^*\) 处。那么从任意 \(\mathbf{x}^0\)
    开始的步长为 \(\alpha = 1/L\) 的梯度下降算法会产生一个序列 \(\mathbf{x}^t\)，\(t=1,2,\ldots\)，使得'
- en: \[ \lim_{t \to +\infty} f(\mathbf{x}^t) = f(\mathbf{x}^*). \]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{t \to +\infty} f(\mathbf{x}^t) = f(\mathbf{x}^*). \]
- en: Moreover, after \(S\) steps, we have
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，经过 \(S\) 次迭代后，我们有
- en: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]. \]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]. \]
- en: \(\sharp\)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Observe that \(f(\mathbf{x}^S) - f(\mathbf{x}^*)\) decreases exponentially fast
    in \(S\). A related bound can be proved for \(\|\mathbf{x}^S - \mathbf{x}^*\|\).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(f(\mathbf{x}^S) - f(\mathbf{x}^*)\) 在 \(S\) 上以指数速度下降。可以证明 \(\|\mathbf{x}^S
    - \mathbf{x}^*\|\) 的一个相关界限。
- en: Put differently, fix any \(\epsilon > 0\). If our goal is to find a point \(\mathbf{x}\)
    such that \(f(\mathbf{x}) - f(\mathbf{x}^*) \leq \epsilon\), then we are guaranteed
    to find one if we perform \(S\) steps such that
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，固定任意 \(\epsilon > 0\)。如果我们的目标是找到一个点 \(\mathbf{x}\)，使得 \(f(\mathbf{x}) -
    f(\mathbf{x}^*) \leq \epsilon\)，那么如果我们执行 \(S\) 次迭代，我们就能保证找到这样一个点
- en: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)] \leq \epsilon \]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)] \leq \epsilon \]
- en: that is, after rearranging,
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 即，重新排列后，
- en: \[ S \geq \frac{\log \epsilon^{-1} + \log(f(\mathbf{x}^0) - \bar{f})}{\log \left(1
    - \frac{m}{L}\right)^{-1}}. \]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: \[ S \geq \frac{\log \epsilon^{-1} + \log(f(\mathbf{x}^0) - \bar{f})}{\log \left(1
    - \frac{m}{L}\right)^{-1}}. \]
- en: '*Proof idea (Convergence of Gradient Descent in the Strongly Convex Case):*
    We apply the *Descent Guarantee for Smooth Functions* together with the lemma
    above.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路（强凸情况下梯度下降的收敛性）:* 我们将应用 *光滑函数的下降保证* 以及上述引理。'
- en: '*Proof:* *(Convergence of Gradient Descent in the Strongly Convex Case)* By
    the *Descent Guarantee for Smooth Functions* together with the lemma above, we
    have for all \(t\)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* *(强凸情况下梯度下降的收敛性)* 通过 *光滑函数的下降保证* 以及上述引理，对于所有 \(t\)，我们有'
- en: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2L} \|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t) - \frac{m}{L} [f(\mathbf{x}^t) - f(\mathbf{x}^*)]. \]
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2L} \|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t) - \frac{m}{L} [f(\mathbf{x}^t) - f(\mathbf{x}^*)]. \]
- en: Subtracting \(f(\mathbf{x}^*)\) on both sides gives
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 两边减去 \(f(\mathbf{x}^*)\) 得到
- en: \[ f(\mathbf{x}^{t+1}) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)[f(\mathbf{x}^t)
    - f(\mathbf{x}^*)]. \]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)[f(\mathbf{x}^t)
    - f(\mathbf{x}^*)]. \]
- en: Recursing this is
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 递归这个过程
- en: \[ \leq \left(1 - \frac{m}{L}\right)^2[f(\mathbf{x}^{t-1}) - f(\mathbf{x}^*)],
    \]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \leq \left(1 - \frac{m}{L}\right)^2[f(\mathbf{x}^{t-1}) - f(\mathbf{x}^*)],
    \]
- en: and so on. That gives the claim. \(\square\)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。这给出了结论。 \(\square\)
- en: '**NUMERICAL CORNER:** We revisit our first simple single-variable example.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们重新审视我们的第一个简单的单变量例子。'
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The second derivative is \(f''(x) = 2\). Hence, this \(f\) is \(L\)-smooth and
    \(m\)-strongly convex with \(L = m = 2\). The theory we developed suggests taking
    step size \(\alpha_t = \alpha = 1/L = 1/2\). It also implies that
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数是 \(f''(x) = 2\)。因此，这个 \(f\) 是 \(L\)-平滑和 \(m\)-强凸的，\(L = m = 2\)。我们发展的理论建议取步长
    \(\alpha_t = \alpha = 1/L = 1/2\)。这也意味着
- en: \[ f(x^1) - f(x^*) \leq \left(1 - \frac{m}{L}\right) [f(x^0) - f(x^*)] = 0.
    \]
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x^1) - f(x^*) \leq \left(1 - \frac{m}{L}\right) [f(x^0) - f(x^*)] = 0.
    \]
- en: We converge in one step! And that holds for any starting point \(x^0\).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一步就收敛了！这对于任何起始点 \(x^0\) 都成立。
- en: Let’s try this!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试这个！
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let’s try a different starting point.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个不同的起始点。
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: \(\unlhd\)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** In the gradient descent update rule \(\mathbf{x}^{t+1} = \mathbf{x}^t
    - \alpha_t \nabla f(\mathbf{x}^t)\), what does \(\alpha_t\) represent?'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在梯度下降更新规则 \(\mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t)\)
    中，\(\alpha_t\) 代表什么？'
- en: a) The gradient of \(f\) at \(\mathbf{x}^t\)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: a) 在 \(\mathbf{x}^t\) 处 \(f\) 的梯度
- en: b) The step size or learning rate
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: b) 步长或学习率
- en: c) The direction of steepest ascent
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: c) 最速上升方向
- en: d) The Hessian matrix of \(f\) at \(\mathbf{x}^t\)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: d) 在 \(\mathbf{x}^t\) 处 \(f\) 的Hessian矩阵
- en: '**2** A function \(f : \mathbb{R}^d \to \mathbb{R}\) is said to be \(L\)-smooth
    if:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 如果一个函数 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-平滑的，那么：'
- en: a) \(\|\nabla f(\mathbf{x})\| \leq L\) for all \(\mathbf{x} \in \mathbb{R}^d\)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对于所有 \(\mathbf{x} \in \mathbb{R}^d\)，有 \(\|\nabla f(\mathbf{x})\| \leq L\)
- en: b) \(-LI_{d\times d} \preceq \mathbf{H}_f(\mathbf{x}) \preceq LI_{d\times d}\)
    for all \(\mathbf{x} \in \mathbb{R}^d\)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: b) \( -LI_{d\times d} \preceq \mathbf{H}_f(\mathbf{x}) \preceq LI_{d\times d}
    \) 对于所有 \(\mathbf{x} \in \mathbb{R}^d\)
- en: c) \(f(\mathbf{y}) \leq f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{y} -
    \mathbf{x}) + \frac{L}{2}\|\mathbf{y} - \mathbf{x}\|^2\) for all \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^d\)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(f(\mathbf{y}) \leq f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{y} -
    \mathbf{x}) + \frac{L}{2}\|\mathbf{y} - \mathbf{x}\|^2\) 对于所有 \(\mathbf{x}, \mathbf{y}
    \in \mathbb{R}^d\)
- en: d) Both b) and c)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: d) b) 和 c) 都对
- en: '**3** Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and bounded
    from below. According to the *Convergence of Gradient Descent in the Smooth Case*
    theorem, gradient descent with step size \(\alpha_t = 1/L\) started from any \(\mathbf{x}^0\)
    produces a sequence \(\{\mathbf{x}^t\}\) such that'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-平滑且有下界。根据 *平滑情况下的梯度下降收敛定理*，以步长
    \(\alpha_t = 1/L\) 从任意 \(\mathbf{x}^0\) 开始的梯度下降产生一个序列 \(\{\mathbf{x}^t\}\)，使得'
- en: a) \(\lim_{t \to +\infty} f(\mathbf{x}^t) = 0\)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\lim_{t \to +\infty} f(\mathbf{x}^t) = 0\)
- en: b) \(\lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0\)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0\)
- en: c) \(\min_{t=0,\ldots,S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2L[f(\mathbf{x}^0)
    - \bar{f}]}{S}}\) after \(S\) steps
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: c) 在 \(S\) 步之后 \(\min_{t=0,\ldots,S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2L[f(\mathbf{x}^0)
    - \bar{f}]}{S}}\)
- en: d) Both b) and c)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: d) b) 和 c) 都对
- en: '**4** Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and \(m\)-strongly
    convex with a global minimizer at \(\mathbf{x}^*\). According to the *Convergence
    of Gradient Descent in th Strongly Convex Case* theorem, gradient descent with
    step size \(\alpha = 1/L\) started from any \(\mathbf{x}^0\) produces a sequence
    \(\{\mathbf{x}^t\}\) such that after \(S\) steps:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-平滑和 \(m\)-强凸的，并且全局最小值在
    \(\mathbf{x}^*\)。根据 *强凸情况下的梯度下降收敛定理*，以步长 \(\alpha = 1/L\) 从任意 \(\mathbf{x}^0\)
    开始的梯度下降产生一个序列 \(\{\mathbf{x}^t\}\)，在 \(S\) 步之后：'
- en: a) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: b) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: c) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: d) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: '**5** If a function \(f\) is \(m\)-strongly convex, what can we say about its
    global minimizer?'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 如果一个函数 \(f\) 是 \(m\)-强凸的，我们可以对其全局最小值说些什么？'
- en: a) It may not exist.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它可能不存在。
- en: b) It exists and is unique.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它存在且是唯一的。
- en: c) It exists but may not be unique.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它存在，但不一定是唯一的。
- en: d) It always occurs at the origin.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它总是在原点发生。
- en: 'Answer for 1: b. Justification: The text states “At each iteration of gradient
    descent, we take a step in the direction of the negative of the gradient, that
    is, \(\mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t), t = 0,
    1, 2, \ldots\) for a sequence of step sizes \(\alpha_t > 0\).”'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 1的答案：b. 证明：文本中提到“在梯度下降的每次迭代中，我们沿着梯度的负方向迈出一步，即 \(\mathbf{x}^{t+1} = \mathbf{x}^t
    - \alpha_t \nabla f(\mathbf{x}^t), t = 0, 1, 2, \ldots\) 对于一系列步长 \(\alpha_t >
    0\)。”
- en: 'Answer for 2: d. Justification: The text provides both the definition in terms
    of the Hessian matrix (option b) and the equivalent characterization in terms
    of the quadratic bound (option c).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 2的答案：d. 证明：文本提供了关于Hessian矩阵的定义（选项b）以及关于二次界限的等价描述（选项c）。
- en: 'Answer for 3: d. Justification: The theorem states both the asymptotic convergence
    of the gradients to zero (option b) and the quantitative bound on the minimum
    gradient norm after \(S\) steps (option c).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 3的答案：d. 证明：定理既说明了梯度趋于零的渐近收敛（选项b），也说明了 \(S\) 次迭代后最小梯度范数的定量界限（选项c）。
- en: 'Answer for 4: a. Justification: This is the convergence rate stated in the
    theorem.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 4的答案：a. 证明：这是定理中提到的收敛速度。
- en: 'Answer for 5: b. Justification: The text states that if \(f\) is \(m\)-strongly
    convex, then “the global minimizer is unique.”'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 5的答案：b. 证明：文本中提到，如果 \(f\) 是 \(m\)-强凸的，那么“全局最小值是唯一的。”
- en: 3.5.1\. Gradient descent[#](#gradient-descent "Link to this heading")
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5.1\. 梯度下降[#](#gradient-descent "链接到这个标题")
- en: In gradient descent, we attempt to find smaller values of \(f\) by successively
    following directions in which \(f\) decreases locally. As we have seen in the
    proof of the *First-Order Necessary Optimality Condition*, \(- \nabla f\) provides
    such a direction. In fact, it is the direction of steepest descent in the following
    sense.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降中，我们试图通过依次跟随 \(f\) 局部减少的方向来找到 \(f\) 的较小值。正如我们在**一阶必要最优性条件**的证明中所看到的，\(-
    \nabla f\) 提供了这样的方向。实际上，它是在以下意义上的最速下降方向。
- en: Recall from the *Descent Direction and Directional Derivative Lemma* that \(\mathbf{v}\)
    is a descent direction at \(\mathbf{x}_0\) if the directional derivative of \(f\)
    at \(\mathbf{x}_0\) in the direction \(\mathbf{v}\) is negative.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下从**下降方向和方向导数引理**中，\(\mathbf{v}\) 是 \(\mathbf{x}_0\) 处的下降方向，如果 \(f\) 在 \(\mathbf{x}_0\)
    处沿 \(\mathbf{v}\) 方向的方向导数是负的。
- en: '**LEMMA** **(Steepest Descent)** \(\idx{steepest descent lemma}\xdi\) Let \(f
    : \mathbb{R}^d \to \mathbb{R}\) be continuously differentiable at \(\mathbf{x}_0\).
    For any unit vector \(\mathbf{v} \in \mathbb{R}^d\),'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(最速下降)** \(\idx{steepest descent lemma}\xdi\) 设 \(f : \mathbb{R}^d
    \to \mathbb{R}\) 在 \(\mathbf{x}_0\) 处是连续可微的。对于任何单位向量 \(\mathbf{v} \in \mathbb{R}^d\)，'
- en: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} \geq \frac{\partial
    f (\mathbf{x}_0)}{\partial \mathbf{v}^*} \]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}} \geq \frac{\partial
    f (\mathbf{x}_0)}{\partial \mathbf{v}^*} \]
- en: where
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在哪里
- en: \[ \mathbf{v}^* = - \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}.
    \]
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}^* = - \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}.
    \]
- en: \(\flat\)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* This is an immediate application of the *Cauchy-Schwarz inequality*.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 这是对**柯西-施瓦茨不等式**的直接应用。'
- en: '*Proof:* By the *Cauchy-Schwarz inequality*, since \(\mathbf{v}\) has unit
    norm,'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过**柯西-施瓦茨不等式**，由于 \(\mathbf{v}\) 的范数为单位，'
- en: \[\begin{align*} \left|\frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}}\right|
    &= \left|\nabla f(\mathbf{x}_0)^T \mathbf{v}\right|\\ &\leq \|\nabla f(\mathbf{x}_0)\|
    \|\mathbf{v}\|\\ &= \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \left|\frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}}\right|
    &= \left|\nabla f(\mathbf{x}_0)^T \mathbf{v}\right|\\ &\leq \|\nabla f(\mathbf{x}_0)\|
    \|\mathbf{v}\|\\ &= \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
- en: Or, put differently,
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，换一种说法，
- en: \[ - \|\nabla f(\mathbf{x}_0)\| \leq \frac{\partial f (\mathbf{x}_0)}{\partial
    \mathbf{v}} \leq \|\nabla f(\mathbf{x}_0)\|. \]
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - \|\nabla f(\mathbf{x}_0)\| \leq \frac{\partial f (\mathbf{x}_0)}{\partial
    \mathbf{v}} \leq \|\nabla f(\mathbf{x}_0)\|. \]
- en: On the other hand, by the choice of \(\mathbf{v}^*\),
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，根据 \(\mathbf{v}^*\) 的选择，
- en: \[\begin{align*} \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}^*} &=
    \nabla f(\mathbf{x}_0)^T \left(- \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\nabla f(\mathbf{x}_0)^T \nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\|\nabla f(\mathbf{x}_0)\|^2}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f (\mathbf{x}_0)}{\partial \mathbf{v}^*} &=
    \nabla f(\mathbf{x}_0)^T \left(- \frac{\nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\nabla f(\mathbf{x}_0)^T \nabla f(\mathbf{x}_0)}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \left(\frac{\|\nabla f(\mathbf{x}_0)\|^2}{\|\nabla f(\mathbf{x}_0)\|}\right)\\
    &= - \|\nabla f(\mathbf{x}_0)\|. \end{align*}\]
- en: The last two displays combined give the result. \(\square\)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个显示结合给出了结果。\(\square\)
- en: At each iteration of gradient descent, we take a step in the direction of the
    negative of the gradient, that is,
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降的每次迭代中，我们沿着梯度的负方向迈出一步，即，
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t), \quad
    t=0,1,2\ldots \]
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t), \quad
    t=0,1,2\ldots \]
- en: for a sequence of step sizes \(\alpha_t > 0\). Choosing the right step size
    (also known as steplength or learning rate) is a large subject in itself. We will
    only consider the case of fixed step size here.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一系列步长\(\alpha_t > 0\)。选择合适的步长（也称为步长或学习率）本身就是一个很大的主题。在这里，我们只考虑固定步长的情况。
- en: '**CHAT & LEARN** Ask your favorite AI chatbot about the different approaches
    for selecting a step size in gradient descent methods. \(\ddagger\)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向你喜欢的AI聊天机器人询问梯度下降法中选择步长不同方法的细节。\(\ddagger\)'
- en: 'In general, we will not be able to guarantee that a global minimizer is reached
    in the limit, even if one exists. Our goal for now is more modest: to find a point
    where the gradient of \(f\) approximately vanishes.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们无法保证在极限情况下达到全局最小值，即使存在这样的最小值。我们现在的目标是更谦虚的：找到一个梯度大约为零的点。
- en: We implement gradient descent\(\idx{gradient descent}\xdi\) in Python. We assume
    that a function `f` and its gradient `grad_f` are provided. We first code the
    basic steepest descent step with a step size\(\idx{step size}\xdi\) \(\idx{learning
    rate}\xdi\) \(\alpha =\) `alpha`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Python中实现了梯度下降\(\idx{梯度下降}\xdi\)。我们假设提供了一个函数`f`及其梯度`grad_f`。我们首先用步长\(\idx{步长}\xdi\)
    \(\idx{学习率}\xdi\) \(\alpha =\) `alpha`编写基本的梯度下降步骤。
- en: '[PRE19]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**NUMERICAL CORNER:** We illustrate on a simple example.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落:** 我们用一个简单的例子来说明。'
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![../../_images/0697c218312ddc584b4a0edc5e583702b6afa69be250f4eac766440ceda0f7d2.png](../Images/8fbfd5ba011f320f65a37c311822755c.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/0697c218312ddc584b4a0edc5e583702b6afa69be250f4eac766440ceda0f7d2.png](../Images/8fbfd5ba011f320f65a37c311822755c.png)'
- en: '[PRE21]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We found a global minmizer in this case.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们找到了全局最小值。
- en: The next example shows that a different local minimizer may be reached depending
    on the starting point.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个例子表明，根据起始点，可能达到不同的局部最小值。
- en: '[PRE23]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![../../_images/48729baf93e6b40366984a812582a50a90890a4c4e42f79a718f921c414b5b55.png](../Images/c2b85b3ea7eba3a62567ae4ff6962fca.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/48729baf93e6b40366984a812582a50a90890a4c4e42f79a718f921c414b5b55.png](../Images/c2b85b3ea7eba3a62567ae4ff6962fca.png)'
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**TRY IT!** In this last example, does changing the step size affect the outcome?
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**TRY IT!** 在这个最后的例子中，改变步长是否会影响结果？（[在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
- en: In the final example, we end up at a stationary point that is not a local minimizer.
    Here both the first and second derivatives are zero. This is known as a [saddle
    point](https://en.wikipedia.org/wiki/Saddle_point).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的例子中，我们最终到达一个不是局部最小值点的平衡点。在这里，一阶和二阶导数都为零。这被称为[saddle point](https://en.wikipedia.org/wiki/Saddle_point)。
- en: '[PRE28]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![../../_images/68a876bde430631abb6621a64c476ac59b86424854df7da519e2a8ba40834bcf.png](../Images/b3363868479c1ce17160f5f595b4b954.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/68a876bde430631abb6621a64c476ac59b86424854df7da519e2a8ba40834bcf.png](../Images/b3363868479c1ce17160f5f595b4b954.png)'
- en: '[PRE29]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: \(\unlhd\)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 3.5.2\. Convergence analysis[#](#convergence-analysis "Link to this heading")
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5.2\. 收敛性分析[#](#convergence-analysis "链接到这个标题")
- en: In this section, we prove some results about the convergence\(\idx{convergence
    analysis}\xdi\) of gradient descent. We start with the smooth case.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们证明了一些关于梯度下降\(\idx{收敛性分析}\xdi\)的结论。我们首先从光滑情况开始。
- en: '**Smooth case** Informally, a function is smooth if its gradient does not change
    too fast. The formal definition we will use here follows. We restrict ourselves
    to the twice continuously differentiable case.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**光滑情况** 非正式地说，如果一个函数的梯度变化不是太快，那么这个函数是光滑的。我们在这里将使用正式的定义。我们限制自己到二阶连续可微的情况。'
- en: '**DEFINITION** **(Smooth Function)** \(\idx{smooth function}\xdi\) Let \(f
    : \mathbb{R}^d \to \mathbb{R}\) be twice continuously differentiable. We say that
    \(f\) is \(L\)-smooth if'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（光滑函数）** \(\idx{smooth function}\xdi\) 设 \(f : \mathbb{R}^d \to \mathbb{R}\)
    是二阶连续可微的。我们说 \(f\) 是 \(L\)-光滑的，如果'
- en: \[ - L I_{d \times d} \preceq H_f(\mathbf{x}) \preceq L I_{d \times d}, \quad
    \forall \mathbf{x} \in \mathbb{R}^d. \]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L I_{d \times d} \preceq H_f(\mathbf{x}) \preceq L I_{d \times d}, \quad
    \forall \mathbf{x} \in \mathbb{R}^d. \]
- en: \(\natural\)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: In the single-variable case, this reduces to \(- L \leq f''(x) \leq L\) for
    all \(x \in \mathbb{R}\). More generally, recall that
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量情况下，这简化为对于所有 \(x \in \mathbb{R}\)，有 \(- L \leq f''(x) \leq L\)。更一般地，回忆一下
- en: \[ A \preceq B \iff \mathbf{z}^T A\mathbf{z} \leq \mathbf{z}^T B\mathbf{z},
    \qquad \forall \mathbf{z} \in \mathbb{R}^{d}. \]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \preceq B \iff \mathbf{z}^T A\mathbf{z} \leq \mathbf{z}^T B\mathbf{z},
    \qquad \forall \mathbf{z} \in \mathbb{R}^{d}. \]
- en: So the condition above is equivalent to
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上述条件等价于
- en: \[ - L \|\mathbf{z}\|^2 \leq \mathbf{z}^T H_f(\mathbf{x}) \,\mathbf{z} \leq
    L \|\mathbf{z}\|^2, \quad \forall \mathbf{x}, \mathbf{z} \in \mathbb{R}^d. \]
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L \|\mathbf{z}\|^2 \leq \mathbf{z}^T H_f(\mathbf{x}) \,\mathbf{z} \leq
    L \|\mathbf{z}\|^2, \quad \forall \mathbf{x}, \mathbf{z} \in \mathbb{R}^d. \]
- en: A different way to put this is that the second directional derivative satisfies
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表述方式是，二阶方向导数满足
- en: \[ - L \leq \frac{\partial^2 f (\mathbf{x})}{\partial \mathbf{v}^2} \leq L \]
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L \leq \frac{\partial^2 f (\mathbf{x})}{\partial \mathbf{v}^2} \leq L \]
- en: for all \(\mathbf{x} \in \mathbb{R}^d\) and all unit vectors \(\mathbf{v} \in
    \mathbb{R}^d\).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(\mathbf{x} \in \mathbb{R}^d\) 和所有单位向量 \(\mathbf{v} \in \mathbb{R}^d\)。
- en: Combined with *Taylor’s Theorem*, this gives immediately the following.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 结合 **泰勒定理**，这立即给出以下结论。
- en: '**LEMMA** **(Quadratic Bound for Smooth Functions)** \(\idx{quadratic bound
    for smooth functions}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) be twice continuously
    differentiable. Then \(f\) is \(L\)-smooth if and only if for all \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^d\) it holds that'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（光滑函数的二次界定）** \(\idx{quadratic bound for smooth functions}\xdi\) 设
    \(f : \mathbb{R}^d \to \mathbb{R}\) 是二阶连续可微的。那么 \(f\) 是 \(L\)-光滑的，当且仅当对于所有 \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^d\)，它满足'
- en: \[ \left|f(\mathbf{y}) - \{f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{y}
    - \mathbf{x})\}\right| \leq \frac{L}{2} \|\mathbf{y} - \mathbf{x}\|^2. \]
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left|f(\mathbf{y}) - \{f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{y}
    - \mathbf{x})\}\right| \leq \frac{L}{2} \|\mathbf{y} - \mathbf{x}\|^2. \]
- en: \(\flat\)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* We apply the *Taylor’s Theorem*, then bound the second-order
    term.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明思路**：我们应用 **泰勒定理**，然后对二阶项进行界定。'
- en: '*Proof:* By *Taylor’s Theorem*, for any \(\alpha > 0\) there is \(\xi_\alpha
    \in (0,1)\)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：通过 **泰勒定理**，对于任何 \(\alpha > 0\)，存在 \(\xi_\alpha \in (0,1)\)'
- en: \[ f(\mathbf{x} + \alpha \mathbf{p}) = f(\mathbf{x}) + \alpha \nabla f(\mathbf{x})^T
    \mathbf{p} + \frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p} \]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x} + \alpha \mathbf{p}) = f(\mathbf{x}) + \alpha \nabla f(\mathbf{x})^T
    \mathbf{p} + \frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p} \]
- en: where \(\mathbf{p} = \mathbf{y} - \mathbf{x}\).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{p} = \mathbf{y} - \mathbf{x}\)。
- en: If \(f\) is \(L\)-smooth, then at \(\alpha = 1\) by the observation above
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 \(f\) 是 \(L\)-光滑的，那么在 \(\alpha = 1\) 时，根据上述观察
- en: \[ - L \|\mathbf{p}\|^2 \leq \mathbf{p}^T \,H_f(\mathbf{x} + \xi_1 \mathbf{p})
    \,\mathbf{p} \leq L \|\mathbf{p}\|^2. \]
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: \[ - L \|\mathbf{p}\|^2 \leq \mathbf{p}^T \,H_f(\mathbf{x} + \xi_1 \mathbf{p})
    \,\mathbf{p} \leq L \|\mathbf{p}\|^2. \]
- en: That implies the inequality in the statement.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着陈述中的不等式。
- en: On the other hand, if that inequality holds, by combining with the Taylor expansion
    above we get
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果该不等式成立，通过结合上述泰勒展开，我们得到
- en: \[ \left|\,\frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p}\,\right| \leq \frac{L}{2} \alpha^2 \|\mathbf{p}\|^2 \]
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left|\,\frac{1}{2} \alpha^2 \mathbf{p}^T \,H_f(\mathbf{x} + \xi_\alpha \alpha
    \mathbf{p}) \,\mathbf{p}\,\right| \leq \frac{L}{2} \alpha^2 \|\mathbf{p}\|^2 \]
- en: where we used that \(\|\alpha \mathbf{p}\| = \alpha \|\mathbf{p}\|\) by absolute
    homogeneity of the norm. Dividing by \(\alpha^2/2\), then taking \(\alpha \to
    0\) and using the continuity of the Hessian gives
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了范数的绝对齐次性，即 \(\|\alpha \mathbf{p}\| = \alpha \|\mathbf{p}\|\)。除以 \(\alpha^2/2\)，然后取
    \(\alpha \to 0\) 并利用 Hessian 的连续性给出
- en: \[ \left|\, \mathbf{p}^T \,H_f(\mathbf{x}) \,\mathbf{p} \,\right| \leq L \|\mathbf{p}\|^2.
    \]
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left|\, \mathbf{p}^T \,H_f(\mathbf{x}) \,\mathbf{p} \,\right| \leq L \|\mathbf{p}\|^2.
    \]
- en: By the observation above again, that implies that \(f\) is \(L\)-smooth. \(\square\)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述观察，这意味着 \(f\) 是 \(L\)-光滑。 \(\square\)
- en: We show next that, in the smooth case, steepest descent with an appropriately
    chosen step size produces a sequence of points whose objective values decrease
    (or stay the same) and whose gradients vanish in the limit. We also give a quantitative
    convergence rate. Note that this result does not imply convergence to a local
    (or global) minimizer.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来证明，在光滑的情况下，使用适当选择的步长进行最速下降法会产生一个目标值递减（或保持不变）且梯度在极限下消失的点序列。我们还给出了一个定量的收敛速度。请注意，这个结果并不意味着收敛到局部（或全局）最小值。
- en: '**THEOREM** **(Convergence of Gradient Descent in the Smooth Case)** \(\idx{convergence
    of gradient descent in the smooth case}\xdi\) Suppose that \(f : \mathbb{R}^d
    \to \mathbb{R}\) is \(L\)-smooth and bounded from below, that is, there is \(\bar{f}
    > - \infty\) such that \(f(\mathbf{x}) \geq \bar{f}\), \(\forall \mathbf{x} \in
    \mathbb{R}^d\). Then gradient descent with step size \(\alpha_t = \alpha := 1/L\)
    started from any \(\mathbf{x}^0\) produces a sequence \(\mathbf{x}^t\), \(t=1,2,\ldots\)
    such that'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(光滑情况下梯度下降的收敛性)** \(\idx{光滑情况下梯度下降的收敛性}\xdi\) 假设 \(f : \mathbb{R}^d
    \to \mathbb{R}\) 是 \(L\)-光滑且有下界，即存在 \(\bar{f} > - \infty\) 使得 \(f(\mathbf{x})
    \geq \bar{f}\)，对所有 \(\mathbf{x} \in \mathbb{R}^d\) 成立。那么从任意 \(\mathbf{x}^0\) 开始的步长为
    \(\alpha_t = \alpha := 1/L\) 的梯度下降法会产生一个序列 \(\mathbf{x}^t\)，\(t=1,2,\ldots\)，使得'
- en: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t), \quad \forall t \]
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t), \quad \forall t \]
- en: and
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: and
- en: \[ \lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0. \]
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0. \]
- en: Moreover, after \(S\) steps, there is a \(t\) in \(\{0,\ldots,S\}\) such that
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，经过 \(S\) 步后，存在一个 \(t\) 在 \(\{0,\ldots,S\}\) 中，使得
- en: \[ \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L \left[\,f(\mathbf{x}^0) -
    \bar{f}\,\right]}{S}}. \]
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L \left[\,f(\mathbf{x}^0) -
    \bar{f}\,\right]}{S}}. \]
- en: \(\sharp\)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: The assumption that a lower bound on \(f\) is known may seem far-fetched. But
    there are in fact many settings where this is natural. For instance, in the case
    of the least-squares problem, the objective function \(f\) is non-negative by
    definition and therefore we can take \(\bar{f} = 0\).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 假设已知 \(f\) 的下界可能看起来有些牵强。但实际上，有许多情况下这是自然的。例如，在最小二乘问题的案例中，目标函数 \(f\) 由定义是非负的，因此我们可以取
    \(\bar{f} = 0\)。
- en: A different way to put the claim above regarding the convergence rate is the
    following. Take any \(\epsilon > 0\). If our goal is to find a point \(\mathbf{x}\)
    such that \(\|\nabla f(\mathbf{x})\| \leq \epsilon\), then we are guaranteed to
    find one if we perform \(S\) steps such that
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 关于收敛速度的上述断言的另一种表述方式如下。取任意 \(\epsilon > 0\)。如果我们的目标是找到一个点 \(\mathbf{x}\) 使得 \(\|\nabla
    f(\mathbf{x})\| \leq \epsilon\)，那么如果我们执行 \(S\) 步，我们就能保证找到这样一个点，
- en: \[ \min_{t = 0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L
    \left[\,f(\mathbf{x}^0) - \bar{f}\,\right]}{S}} \leq \epsilon, \]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{t = 0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2 L
    \left[\,f(\mathbf{x}^0) - \bar{f}\,\right]}{S}} \leq \epsilon, \]
- en: that is, after rearranging,
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 即，经过重新排列后，
- en: \[ S \geq \frac{2L [f(\mathbf{x}^0) - \bar{f}]}{\epsilon^2}. \]
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: \[ S \geq \frac{2L [f(\mathbf{x}^0) - \bar{f}]}{\epsilon^2}. \]
- en: The heart of the proof is the following fundamental inequality. It also informs
    the choice of step size.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 证明的核心是以下基本不等式。它也说明了步长的选择。
- en: '**LEMMA** **(Descent Guarantee in the Smooth Case)** \(\idx{descent guarantee
    in the smooth case}\xdi\) Suppose that \(f : \mathbb{R}^d \to \mathbb{R}\) is
    \(L\)-smooth. For any \(\mathbf{x} \in \mathbb{R}^d\),'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(光滑情况下的下降保证)** \(\idx{光滑情况下的下降保证}\xdi\) 假设 \(f : \mathbb{R}^d \to
    \mathbb{R}\) 是 \(L\)-光滑。对于任意 \(\mathbf{x} \in \mathbb{R}^d\)，'
- en: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2 L} \|\nabla f(\mathbf{x})\|^2. \]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2 L} \|\nabla f(\mathbf{x})\|^2. \]
- en: \(\flat\)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea (Descent Guarantee in the Smooth Case):* Intuitively, the *Quadratic
    Bound for Smooth Functions* shows that \(f\) is well approximated by a quadratic
    function in a neighborhood of \(\mathbf{x}\) whose size depends on the smoothness
    parameter \(L\). Choosing a step size that minimizes this approximation leads
    to a guaranteed improvement. The approach taken here is a special case of what
    is referred to as [Majorize-Minimization (MM)](https://en.wikipedia.org/wiki/MM_algorithm)\(\idx{Majorize-Minimization}\xdi\).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路（光滑情况下的下降保证）:* 直观上，*光滑函数的二次界限*表明，\(f\) 在 \(\mathbf{x}\) 的邻域内被一个二次函数很好地逼近，其大小取决于光滑性参数
    \(L\)。选择一个最小化这种逼近的步长会导致保证的改进。这里采取的方法是所谓的 [主元最小化 (MM)](https://en.wikipedia.org/wiki/MM_algorithm)\(\idx{Majorize-Minimization}\xdi\)
    的一个特例。'
- en: '*Proof:* *(Descent Guarantee in the Smooth Case)* By the *Quadratic Bound for
    Smooth Functions*, letting \(\mathbf{p} = - \nabla f(\mathbf{x})\)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* *(光滑情况下的下降保证)* 根据 *光滑函数的二次界限*，令 \(\mathbf{p} = - \nabla f(\mathbf{x})\)'
- en: \[\begin{align*} f(\mathbf{x} + \alpha \mathbf{p}) &\leq f(\mathbf{x}) + \nabla
    f(\mathbf{x})^T (\alpha \mathbf{p}) + \frac{L}{2} \|\alpha \mathbf{p}\|^2\\ &=
    f(\mathbf{x}) - \alpha \|\nabla f(\mathbf{x})\|^2 + \alpha^2 \frac{L}{2} \|\nabla
    f(\mathbf{x})\|^2\\ &= f(\mathbf{x}) + \left( - \alpha + \alpha^2 \frac{L}{2}
    \right) \|\nabla f(\mathbf{x})\|^2. \end{align*}\]
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x} + \alpha \mathbf{p}) &\leq f(\mathbf{x}) + \nabla
    f(\mathbf{x})^T (\alpha \mathbf{p}) + \frac{L}{2} \|\alpha \mathbf{p}\|^2\\ &=
    f(\mathbf{x}) - \alpha \|\nabla f(\mathbf{x})\|^2 + \alpha^2 \frac{L}{2} \|\nabla
    f(\mathbf{x})\|^2\\ &= f(\mathbf{x}) + \left( - \alpha + \alpha^2 \frac{L}{2}
    \right) \|\nabla f(\mathbf{x})\|^2. \end{align*}\]
- en: The quadratic function in parentheses is convex and minimized at the stationary
    point \(\alpha\) satisfying
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的二次函数是凸函数，并在满足
- en: \[ \frac{\mathrm{d}}{\mathrm{d} \alpha}\left( - \alpha + \alpha^2 \frac{L}{2}
    \right) = -1 + \alpha L = 0\. \]
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\mathrm{d}}{\mathrm{d} \alpha}\left( - \alpha + \alpha^2 \frac{L}{2}
    \right) = -1 + \alpha L = 0\. \]
- en: Taking \(\alpha = 1/L\), where \(-\alpha + \alpha^2 \frac{L}{2} = - \frac{1}{2L}\),
    and replacing in the inequality above gives
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 取 \(\alpha = 1/L\)，其中 \(-\alpha + \alpha^2 \frac{L}{2} = - \frac{1}{2L}\)，并将此代入上述不等式中给出
- en: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2L}\|\nabla f(\mathbf{x})\|^2, \]
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f\left(\mathbf{x} - \frac{1}{L} \nabla f(\mathbf{x})\right) \leq f(\mathbf{x})
    - \frac{1}{2L}\|\nabla f(\mathbf{x})\|^2, \]
- en: as claimed. \(\square\)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。 \(\square\)
- en: We return to the proof of the theorem.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到定理的证明。
- en: '*Proof idea (Convergence of Gradient Descent in the Smooth Case):* We use a
    telescoping argument to write \(f(\mathbf{x}^S)\) as a sum of stepwise increments,
    each of which can be bounded by the previous lemma. Because \(f(\mathbf{x}^S)\)
    is bounded from below, it then follows that the gradients must vanish in the limit.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路（光滑情况下的梯度下降收敛）:* 我们使用望远镜论证将 \(f(\mathbf{x}^S)\) 写成一系列步进增量之和，每个增量都可以由前面的引理进行界定。因为
    \(f(\mathbf{x}^S)\) 从下限有界，因此可以得出结论，梯度在极限中必须消失。'
- en: '*Proof:* *(Convergence of Gradient Descent in the Smooth Case)* By the *Descent
    Guarantee in the Smooth Case*,'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* *(光滑情况下的梯度下降收敛)* 根据 *光滑情况下的下降保证*，'
- en: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2 L}\|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t), \quad \forall t. \]
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2 L}\|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t), \quad \forall t. \]
- en: Furthermore, using a telescoping sum, we get
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用望远镜求和，我们得到
- en: \[\begin{align*} f(\mathbf{x}^S) &= f(\mathbf{x}^0) + \sum_{t=0}^{S-1} [f(\mathbf{x}^{t+1})
    - f(\mathbf{x}^t)]\\ &\leq f(\mathbf{x}^0) - \frac{1}{2 L}\sum_{t=0}^{S-1} \|\nabla
    f(\mathbf{x}^t)\|^2\. \end{align*}\]
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x}^S) &= f(\mathbf{x}^0) + \sum_{t=0}^{S-1} [f(\mathbf{x}^{t+1})
    - f(\mathbf{x}^t)]\\ &\leq f(\mathbf{x}^0) - \frac{1}{2 L}\sum_{t=0}^{S-1} \|\nabla
    f(\mathbf{x}^t)\|^2\. \end{align*}\]
- en: Rearranging and using \(f(\mathbf{x}^S) \geq \bar{f}\) leads to
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列并使用 \(f(\mathbf{x}^S) \geq \bar{f}\) 导致
- en: \[ \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2 \leq 2L [f(\mathbf{x}^0) -
    \bar{f}]. \]
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2 \leq 2L [f(\mathbf{x}^0) -
    \bar{f}]. \]
- en: We get the quantitative bound
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到定量界限
- en: \[\begin{align*} \min_{t=0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\|^2 & \leq
    \frac{1}{S} \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2\\ &\leq \frac{2L [f(\mathbf{x}^0)
    - \bar{f}]}{S} \end{align*}\]
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \min_{t=0,\ldots, S-1} \|\nabla f(\mathbf{x}^t)\|^2 & \leq
    \frac{1}{S} \sum_{t=0}^{S-1} \|\nabla f(\mathbf{x}^t)\|^2\\ &\leq \frac{2L [f(\mathbf{x}^0)
    - \bar{f}]}{S} \end{align*}\]
- en: as the minimum is necessarily less or equal than the average. Moreover, as \(S
    \to +\infty\), we must have \(\|\nabla f(\mathbf{x}^S)\|^2 \to 0\) by standard
    [analytical](https://math.stackexchange.com/questions/62389/relationships-between-bounded-and-convergent-series)
    [arguments](https://math.stackexchange.com/questions/107961/if-a-series-converges-then-the-sequence-of-terms-converges-to-0).
    That proves the claim. \(\square\)
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 因为最小值必然小于或等于平均值。此外，当 \(S \to +\infty\) 时，根据标准的 [分析](https://math.stackexchange.com/questions/62389/relationships-between-bounded-and-convergent-series)
    [论证](https://math.stackexchange.com/questions/107961/if-a-series-converges-then-the-sequence-of-terms-converges-to-0)，我们必须有
    \(\|\nabla f(\mathbf{x}^S)\|^2 \to 0\)。这证明了该命题。 \(\square\)
- en: '**Smooth and strongly convex case** With stronger assumptions, we obtain stronger
    convergence results. One such assumption is strong convexity, which we defined
    in the previous section for twice continuously differentiable functions.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**平滑且强凸情况** 在更强的假设下，我们获得了更强的收敛结果。其中一个假设是强凸性，我们在上一节中为二阶连续可微函数定义了它。'
- en: We prove a convergence result for smooth, strongly convex functions. We show
    something stronger this time. We control the value of \(f\) itself and obtain
    a much faster rate of convergence. If \(f\) is \(m\)-strongly convex and has a
    global minimizer \(\mathbf{x}^*\), then the global minimizer is unique and characterized
    by \(\nabla f(\mathbf{x}^*) = \mathbf{0}\). Strong convexity allows us to relate
    the value of the function at a point \(\mathbf{x}\) and the gradient of \(f\)
    at that point. This is proved in the following lemma, which is key to our convergence
    result.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为平滑、强凸函数证明了收敛结果。这次我们展示了更强的一点。我们控制了 \(f\) 的值并获得了更快的收敛速度。如果 \(f\) 是 \(m\)-强凸的并且有一个全局最小值
    \(\mathbf{x}^*\)，那么全局最小值是唯一的，并且由 \(\nabla f(\mathbf{x}^*) = \mathbf{0}\) 来描述。强凸性允许我们将函数在点
    \(\mathbf{x}\) 的值与该点 \(f\) 的梯度联系起来。这将在以下引理中证明，它是我们收敛结果的关键。
- en: '**LEMMA** **(Relating a function and its gradient)** \(\idx{relating a function
    and its gradient lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) be twice
    continuously differentiable, \(m\)-strongly convex with a global minimizer at
    \(\mathbf{x}^*\). Then for any \(\mathbf{x} \in \mathbb{R}^d\)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(函数及其梯度的关系)** \(\idx{relating a function and its gradient lemma}\xdi\)
    设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是二阶连续可微的，\(m\)-强凸，并在 \(\mathbf{x}^*\) 处有全局最小值。那么对于任何
    \(\mathbf{x} \in \mathbb{R}^d\)'
- en: \[ f(\mathbf{x}) - f(\mathbf{x}^*) \leq \frac{\|\nabla f(\mathbf{x})\|^2}{2
    m}. \]
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}) - f(\mathbf{x}^*) \leq \frac{\|\nabla f(\mathbf{x})\|^2}{2
    m}. \]
- en: \(\flat\)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* By the *Quadratic Bound for Strongly Convex Functions*,'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 通过 *强凸函数的二次界*，'
- en: '\[\begin{align*} f(\mathbf{x}^*) &\geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    (\mathbf{x}^* - \mathbf{x}) + \frac{m}{2} \|\mathbf{x}^* - \mathbf{x}\|^2\\ &=
    f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T (m
    I_{d \times d}) \,\mathbf{w}\\ &=: r + \mathbf{q}^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T
    P \,\mathbf{w} \end{align*}\]'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{align*} f(\mathbf{x}^*) &\geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    (\mathbf{x}^* - \mathbf{x}) + \frac{m}{2} \|\mathbf{x}^* - \mathbf{x}\|^2\\ &=
    f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T (m
    I_{d \times d}) \,\mathbf{w}\\ &=: r + \mathbf{q}^T \mathbf{w} + \frac{1}{2} \mathbf{w}^T
    P \,\mathbf{w} \end{align*}\]'
- en: where on the second line we defined \(\mathbf{w} = \mathbf{x}^* - \mathbf{x}\).
    The right-hand side is a quadratic function in \(\mathbf{w}\) (for \(\mathbf{x}\)
    fixed), and on the third line we used our previous notation \(P\), \(\mathbf{q}\)
    and \(r\) for such a function. So the inequality is still valid if we replace
    \(\mathbf{w}\) with the global minimizer \(\mathbf{w}^*\) of that quadratic function.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二行中，我们定义了 \(\mathbf{w} = \mathbf{x}^* - \mathbf{x}\)。右侧是 \(\mathbf{w}\) 的二次函数（对于固定的
    \(\mathbf{x}\)），在第三行中，我们使用了我们之前为这种函数定义的 \(P\)、\(\mathbf{q}\) 和 \(r\)。因此，如果我们用那个二次函数的全局最小值
    \(\mathbf{w}^*\) 替换 \(\mathbf{w}\)，不等式仍然成立。
- en: The matrix \(P = m I_{d \times d}\) is positive definite. By a previous example,
    we know that the minimizer is achieved when the gradient \(\frac{1}{2}[P + P^T]\mathbf{w}^*
    + \mathbf{q} = \mathbf{0}\), which is equivalent to
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(P = m I_{d \times d}\) 是正定的。通过之前的例子，我们知道当梯度 \(\frac{1}{2}[P + P^T]\mathbf{w}^*
    + \mathbf{q} = \mathbf{0}\) 时，最小值被实现，这等价于
- en: \[ \mathbf{w}^* = - (m I_{d \times d})^{-1} \nabla f(\mathbf{x}) = - (m^{-1}
    I_{d \times d}) \nabla f(\mathbf{x}) = - \frac{1}{m} \nabla f(\mathbf{x}). \]
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^* = - (m I_{d \times d})^{-1} \nabla f(\mathbf{x}) = - (m^{-1}
    I_{d \times d}) \nabla f(\mathbf{x}) = - \frac{1}{m} \nabla f(\mathbf{x}). \]
- en: So, replacing \(\mathbf{w}\) with \(\mathbf{w}^*\), we have the inequality
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用 \(\mathbf{w}^*\) 替换 \(\mathbf{w}\)，我们得到不等式
- en: \[\begin{align*} f(\mathbf{x}^*) & \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & \quad \quad+ \frac{1}{2}
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}^T (m I_{d \times d}) \left\{-
    \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & = f(\mathbf{x}) - \frac{1}{2m} \|\nabla
    f(\mathbf{x})\|^2. \end{align*}\]
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{x}^*) & \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & \quad \quad+ \frac{1}{2}
    \left\{- \frac{1}{m} \nabla f(\mathbf{x})\right\}^T (m I_{d \times d}) \left\{-
    \frac{1}{m} \nabla f(\mathbf{x})\right\}\\ & = f(\mathbf{x}) - \frac{1}{2m} \|\nabla
    f(\mathbf{x})\|^2. \end{align*}\]
- en: Rearranging gives the claim. \(\square\)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列后给出结论。 \(\square\)
- en: We can now state our convergence result.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以陈述我们的收敛结果。
- en: '**THEOREM** **(Convergence of Gradient Descent in the Strongly Convex Case)**
    \(\idx{convergence of gradient descent in the strongly convex case}\xdi\) Suppose
    that \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and \(m\)-strongly convex
    with a global minimizer at \(\mathbf{x}^*\). Then gradient descent with step size
    \(\alpha = 1/L\) started from any \(\mathbf{x}^0\) produces a sequence \(\mathbf{x}^t\),
    \(t=1,2,\ldots\) such that'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(强凸情况下梯度下降的收敛性)** \(\idx{梯度下降在强凸情况下的收敛性}\xdi\) 假设 \(f : \mathbb{R}^d
    \to \mathbb{R}\) 是 \(L\)-光滑和 \(m\)-强凸的，并且全局最小值在 \(\mathbf{x}^*\) 处。那么从任意 \(\mathbf{x}^0\)
    开始的步长为 \(\alpha = 1/L\) 的梯度下降会产生一个序列 \(\mathbf{x}^t\)，\(t=1,2,\ldots\)，使得'
- en: \[ \lim_{t \to +\infty} f(\mathbf{x}^t) = f(\mathbf{x}^*). \]
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lim_{t \to +\infty} f(\mathbf{x}^t) = f(\mathbf{x}^*). \]
- en: Moreover, after \(S\) steps, we have
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，经过 \(S\) 步后，我们有
- en: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]. \]
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]. \]
- en: \(\sharp\)
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Observe that \(f(\mathbf{x}^S) - f(\mathbf{x}^*)\) decreases exponentially fast
    in \(S\). A related bound can be proved for \(\|\mathbf{x}^S - \mathbf{x}^*\|\).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(f(\mathbf{x}^S) - f(\mathbf{x}^*)\) 在 \(S\) 中以指数速度下降。对于 \(\|\mathbf{x}^S
    - \mathbf{x}^*\|\) 也可以证明一个相关的界限。
- en: Put differently, fix any \(\epsilon > 0\). If our goal is to find a point \(\mathbf{x}\)
    such that \(f(\mathbf{x}) - f(\mathbf{x}^*) \leq \epsilon\), then we are guaranteed
    to find one if we perform \(S\) steps such that
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，固定任意 \(\epsilon > 0\)。如果我们的目标是找到一个点 \(\mathbf{x}\)，使得 \(f(\mathbf{x}) -
    f(\mathbf{x}^*) \leq \epsilon\)，那么如果我们执行 \(S\) 步，我们就能保证找到这样一个点，
- en: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)] \leq \epsilon \]
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)^S [f(\mathbf{x}^0)
    - f(\mathbf{x}^*)] \leq \epsilon \]
- en: that is, after rearranging,
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 即，重新排列后，
- en: \[ S \geq \frac{\log \epsilon^{-1} + \log(f(\mathbf{x}^0) - \bar{f})}{\log \left(1
    - \frac{m}{L}\right)^{-1}}. \]
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: \[ S \geq \frac{\log \epsilon^{-1} + \log(f(\mathbf{x}^0) - \bar{f})}{\log \left(1
    - \frac{m}{L}\right)^{-1}}. \]
- en: '*Proof idea (Convergence of Gradient Descent in the Strongly Convex Case):*
    We apply the *Descent Guarantee for Smooth Functions* together with the lemma
    above.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路（强凸情况下梯度下降的收敛性）：* 我们应用平滑函数的下降保证和上述引理。'
- en: '*Proof:* *(Convergence of Gradient Descent in the Strongly Convex Case)* By
    the *Descent Guarantee for Smooth Functions* together with the lemma above, we
    have for all \(t\)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(强凸情况下梯度下降的收敛性)* 通过结合平滑函数的下降保证和上述引理，对于所有的 \(t\)，我们有'
- en: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2L} \|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t) - \frac{m}{L} [f(\mathbf{x}^t) - f(\mathbf{x}^*)]. \]
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) \leq f(\mathbf{x}^t) - \frac{1}{2L} \|\nabla f(\mathbf{x}^t)\|^2
    \leq f(\mathbf{x}^t) - \frac{m}{L} [f(\mathbf{x}^t) - f(\mathbf{x}^*)]. \]
- en: Subtracting \(f(\mathbf{x}^*)\) on both sides gives
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 两边减去 \(f(\mathbf{x}^*)\) 得到
- en: \[ f(\mathbf{x}^{t+1}) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)[f(\mathbf{x}^t)
    - f(\mathbf{x}^*)]. \]
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^{t+1}) - f(\mathbf{x}^*) \leq \left(1 - \frac{m}{L}\right)[f(\mathbf{x}^t)
    - f(\mathbf{x}^*)]. \]
- en: Recursing this is
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 递归这个不等式，
- en: \[ \leq \left(1 - \frac{m}{L}\right)^2[f(\mathbf{x}^{t-1}) - f(\mathbf{x}^*)],
    \]
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \leq \left(1 - \frac{m}{L}\right)^2[f(\mathbf{x}^{t-1}) - f(\mathbf{x}^*)],
    \]
- en: and so on. That gives the claim. \(\square\)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。这证明了我们的结论。 \(\square\)
- en: '**NUMERICAL CORNER:** We revisit our first simple single-variable example.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们重新审视我们的第一个简单的单变量例子。'
- en: '[PRE33]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The second derivative is \(f''(x) = 2\). Hence, this \(f\) is \(L\)-smooth and
    \(m\)-strongly convex with \(L = m = 2\). The theory we developed suggests taking
    step size \(\alpha_t = \alpha = 1/L = 1/2\). It also implies that
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数是 \(f''(x) = 2\)。因此，这个 \(f\) 是 \(L\)-光滑和 \(m\)-强凸的，其中 \(L = m = 2\)。我们发展的理论建议取步长
    \(\alpha_t = \alpha = 1/L = 1/2\)。这也意味着
- en: \[ f(x^1) - f(x^*) \leq \left(1 - \frac{m}{L}\right) [f(x^0) - f(x^*)] = 0.
    \]
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x^1) - f(x^*) \leq \left(1 - \frac{m}{L}\right) [f(x^0) - f(x^*)] = 0.
    \]
- en: We converge in one step! And that holds for any starting point \(x^0\).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一步就收敛了！这对于任何起始点 \(x^0\) 都成立。
- en: Let’s try this!
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试这个！
- en: '[PRE34]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Let’s try a different starting point.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个不同的起始点。
- en: '[PRE36]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: \(\unlhd\)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由 Claude、Gemini 和 ChatGPT 协助)*'
- en: '**1** In the gradient descent update rule \(\mathbf{x}^{t+1} = \mathbf{x}^t
    - \alpha_t \nabla f(\mathbf{x}^t)\), what does \(\alpha_t\) represent?'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在梯度下降更新规则 \(\mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t)\)
    中，\(\alpha_t\) 代表什么？'
- en: a) The gradient of \(f\) at \(\mathbf{x}^t\)
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(f\) 在 \(\mathbf{x}^t\) 处的梯度
- en: b) The step size or learning rate
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: b) 步长或学习率
- en: c) The direction of steepest ascent
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: c) 最速上升方向
- en: d) The Hessian matrix of \(f\) at \(\mathbf{x}^t\)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(f\) 在 \(\mathbf{x}^t\) 处的 Hessian 矩阵
- en: '**2** A function \(f : \mathbb{R}^d \to \mathbb{R}\) is said to be \(L\)-smooth
    if:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 如果一个函数 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-光滑的，那么它满足以下哪个条件？'
- en: a) \(\|\nabla f(\mathbf{x})\| \leq L\) for all \(\mathbf{x} \in \mathbb{R}^d\)
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对于所有 \(\mathbf{x} \in \mathbb{R}^d\)，有 \(\|\nabla f(\mathbf{x})\| \leq L\)
- en: b) \(-LI_{d\times d} \preceq \mathbf{H}_f(\mathbf{x}) \preceq LI_{d\times d}\)
    for all \(\mathbf{x} \in \mathbb{R}^d\)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: b) 对于所有 \(\mathbf{x} \in \mathbb{R}^d\)，有 \(-LI_{d\times d} \preceq \mathbf{H}_f(\mathbf{x})
    \preceq LI_{d\times d}\)
- en: c) \(f(\mathbf{y}) \leq f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{y} -
    \mathbf{x}) + \frac{L}{2}\|\mathbf{y} - \mathbf{x}\|^2\) for all \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^d\)
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: c) 对于所有 \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\)，有 \(f(\mathbf{y}) \leq f(\mathbf{x})
    + \nabla f(\mathbf{x})^T(\mathbf{y} - \mathbf{x}) + \frac{L}{2}\|\mathbf{y} -
    \mathbf{x}\|^2\)
- en: d) Both b) and c)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: d) b) 和 c)
- en: '**3** Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and bounded
    from below. According to the *Convergence of Gradient Descent in the Smooth Case*
    theorem, gradient descent with step size \(\alpha_t = 1/L\) started from any \(\mathbf{x}^0\)
    produces a sequence \(\{\mathbf{x}^t\}\) such that'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-光滑且有下界。根据 *光滑情况下梯度下降的收敛定理*，以步长
    \(\alpha_t = 1/L\) 从任意 \(\mathbf{x}^0\) 开始的梯度下降产生一个序列 \(\{\mathbf{x}^t\}\)，使得：'
- en: a) \(\lim_{t \to +\infty} f(\mathbf{x}^t) = 0\)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\lim_{t \to +\infty} f(\mathbf{x}^t) = 0\)
- en: b) \(\lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0\)
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\lim_{t \to +\infty} \|\nabla f(\mathbf{x}^t)\| = 0\)
- en: c) \(\min_{t=0,\ldots,S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2L[f(\mathbf{x}^0)
    - \bar{f}]}{S}}\) after \(S\) steps
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: c) 经过 \(S\) 步后，\(\min_{t=0,\ldots,S-1} \|\nabla f(\mathbf{x}^t)\| \leq \sqrt{\frac{2L[f(\mathbf{x}^0)
    - \bar{f}]}{S}}\)
- en: d) Both b) and c)
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: d) b) 和 c)
- en: '**4** Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-smooth and \(m\)-strongly
    convex with a global minimizer at \(\mathbf{x}^*\). According to the *Convergence
    of Gradient Descent in th Strongly Convex Case* theorem, gradient descent with
    step size \(\alpha = 1/L\) started from any \(\mathbf{x}^0\) produces a sequence
    \(\{\mathbf{x}^t\}\) such that after \(S\) steps:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-光滑和 \(m\)-强凸的，并且在 \(\mathbf{x}^*\)
    处有全局最小值。根据 *强凸情况下梯度下降的收敛定理*，以步长 \(\alpha = 1/L\) 从任意 \(\mathbf{x}^0\) 开始的梯度下降产生一个序列
    \(\{\mathbf{x}^t\}\)，在 \(S\) 步后：'
- en: a) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: b) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 - \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: c) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \leq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: d) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(f(\mathbf{x}^S) - f(\mathbf{x}^*) \geq (1 + \frac{m}{L})^S[f(\mathbf{x}^0)
    - f(\mathbf{x}^*)]\)
- en: '**5** If a function \(f\) is \(m\)-strongly convex, what can we say about its
    global minimizer?'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 如果一个函数 \(f\) 是 \(m\)-强凸的，我们可以对其全局最小值说些什么？'
- en: a) It may not exist.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它可能不存在。
- en: b) It exists and is unique.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它存在且唯一。
- en: c) It exists but may not be unique.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它存在，但不一定唯一。
- en: d) It always occurs at the origin.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它总是出现在原点。
- en: 'Answer for 1: b. Justification: The text states “At each iteration of gradient
    descent, we take a step in the direction of the negative of the gradient, that
    is, \(\mathbf{x}^{t+1} = \mathbf{x}^t - \alpha_t \nabla f(\mathbf{x}^t), t = 0,
    1, 2, \ldots\) for a sequence of step sizes \(\alpha_t > 0\).”'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：b. 理由：文本中提到“在梯度下降的每次迭代中，我们沿着梯度的负方向迈出一步，即 \(\mathbf{x}^{t+1} = \mathbf{x}^t
    - \alpha_t \nabla f(\mathbf{x}^t), t = 0, 1, 2, \ldots\) 对于一系列步长 \(\alpha_t >
    0\)。”
- en: 'Answer for 2: d. Justification: The text provides both the definition in terms
    of the Hessian matrix (option b) and the equivalent characterization in terms
    of the quadratic bound (option c).'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 答案2：d. 理由：文本提供了关于Hessian矩阵的定义（选项b）以及关于二次界限的等价描述（选项c）。
- en: 'Answer for 3: d. Justification: The theorem states both the asymptotic convergence
    of the gradients to zero (option b) and the quantitative bound on the minimum
    gradient norm after \(S\) steps (option c).'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 答案3：d. 理由：定理既说明了梯度趋于零的渐近收敛（选项b），也说明了在 \(S\) 步之后最小梯度范数的定量界限（选项c）。
- en: 'Answer for 4: a. Justification: This is the convergence rate stated in the
    theorem.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：a. 理由：这是定理中指出的收敛速率。
- en: 'Answer for 5: b. Justification: The text states that if \(f\) is \(m\)-strongly
    convex, then “the global minimizer is unique.”'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：b. 理由：文本指出，如果 \(f\) 是 \(m\)-强凸的，那么“全局最小值是唯一的。”
