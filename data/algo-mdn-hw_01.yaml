- en: Complexity Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复杂性模型
- en: 原文：[https://en.algorithmica.org/hpc/complexity/](https://en.algorithmica.org/hpc/complexity/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://en.algorithmica.org/hpc/complexity/](https://en.algorithmica.org/hpc/complexity/)'
- en: If you ever opened a computer science textbook, it probably introduced *computational
    complexity* somewhere in the very beginning. Simply put, it is the total count
    of *elementary operations* (additions, multiplications, reads, writes…) that are
    executed during a computation, optionally weighted by their *costs*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经打开过一本计算机科学教科书，它可能在非常开头的地方介绍了*计算复杂性*。简单来说，它是在计算过程中执行的总数*基本操作*（加法、乘法、读取、写入等），这些操作可以按其*成本*进行加权。
- en: Complexity is an old concept. It was [systematically formulated](http://www.cs.albany.edu/~res/comp_complexity_ams_1965.pdf)
    in the early 1960s, and since then it has been universally used as the cost function
    for designing algorithms. The reason this model was so quickly adopted is that
    it was a good approximation of how computers worked at the time.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性是一个古老的概念。它在20世纪60年代初被[系统地阐述](http://www.cs.albany.edu/~res/comp_complexity_ams_1965.pdf)，自那时起，它一直被普遍用作设计算法的成本函数。这个模型之所以能迅速被采纳，是因为它很好地近似了当时计算机的工作方式。
- en: Classical Complexity Theory
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经典复杂性理论
- en: The “elementary operations” of a CPU are called *instructions*, and their “costs”
    are called *latencies*. Instructions are stored in *memory* and executed one by
    one by the processor, which has some internal *state* stored in a number of *registers*.
    One of these registers is the *instruction pointer*, which indicates the address
    of the next instruction to read and execute. Each instruction changes the state
    of the processor in a certain way (including moving the instruction pointer),
    possibly modifies the main memory, and takes a different number of *CPU cycles*
    to complete before the next one can be started.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: CPU的“基本操作”被称为*指令*，它们的“成本”被称为*延迟*。指令存储在*内存*中，并由处理器逐个执行，处理器在多个*寄存器*中存储一些内部*状态*。这些寄存器之一是*指令指针*，它指示下一个要读取和执行的指令的地址。每个指令以某种方式改变处理器的状态（包括移动指令指针），可能修改主内存，并在下一个指令可以开始之前需要不同数量的*CPU周期*来完成。
- en: To estimate the real running time of a program, you need to sum all latencies
    for its executed instructions and divide it by the *clock frequency*, that is,
    the number of cycles a particular CPU does per second.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计程序的真正运行时间，你需要将其执行指令的所有延迟相加，然后除以*时钟频率*，即特定CPU每秒完成的周期数。
- en: '![](../Images/ad0d0292006a4cdf566b13f89ca87ea1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad0d0292006a4cdf566b13f89ca87ea1.png)'
- en: The clock frequency is a volatile and often unknown variable that depends on
    the CPU model, operating system settings, current microchip temperature, power
    usage of other components, and quite a few other things. In contrast, instruction
    latencies are static and even somewhat consistent across different CPUs when expressed
    in clock cycles, so counting them instead is much more useful for analytical purposes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 时钟频率是一个波动且往往未知变量，它取决于CPU型号、操作系统设置、当前微芯片温度、其他组件的功耗以及许多其他因素。相比之下，指令延迟是静态的，甚至在不同的CPU中，当以时钟周期表示时，它们甚至有一定的稳定性，因此计数它们对于分析目的来说更有用。
- en: 'For example, the by-definition matrix multiplication algorithm requires the
    total of $n^2 \cdot (n + n - 1)$ arithmetic operations: specifically, $n^3$ multiplications
    and $n^2 \cdot (n - 1)$ additions. If we look up the latencies for these instructions
    (in special documents called *instruction tables*, like [this one](https://www.agner.org/optimize/instruction_tables.pdf)),
    we can find that, e.g., multiplication takes 3 cycles, while addition takes 1,
    so we need a total of $3 \cdot n^3 + n^2 \cdot (n - 1) = 4 \cdot n^3 - n^2$ clock
    cycles for the entire computation (bluntly ignoring everything else that needs
    to be done to “feed” these instructions with the right data).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，按定义的矩阵乘法算法需要总共 $n^2 \cdot (n + n - 1)$ 次算术运算：具体来说，$n^3$ 次乘法和 $n^2 \cdot (n
    - 1)$ 次加法。如果我们查阅这些指令的延迟（在称为*指令表*的特殊文档中，如[这个](https://www.agner.org/optimize/instruction_tables.pdf)），我们可以发现，例如，乘法需要3个周期，而加法需要1个周期，因此整个计算需要总共
    $3 \cdot n^3 + n^2 \cdot (n - 1) = 4 \cdot n^3 - n^2$ 个时钟周期（粗略地忽略所有其他需要完成以“喂”这些指令正确数据的操作）。
- en: Similar to how the sum of instruction latencies can be used as a clock-independent
    proxy for total execution time, computational complexity can be used to quantify
    the intrinsic time requirements of an abstract algorithm, without relying on the
    choice of a specific computer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于如何使用指令延迟的总和作为总执行时间的时钟无关的代理，计算复杂度可以用来量化抽象算法的内在时间需求，而不依赖于特定计算机的选择。
- en: Asymptotic Complexity
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渐进复杂度
- en: The idea to express execution time as a function of input size seems obvious
    now, but it wasn’t so in the 1960s. Back then, [typical computers](https://en.wikipedia.org/wiki/CDC_1604)
    cost millions of dollars, were so large that they required a separate room, and
    had clock rates measured in kilohertz. They were used for practical tasks at hand,
    like predicting the weather, sending rockets into space, or figuring out how far
    a Soviet nuclear missile can fly from the coast of Cuba — all of which are finite-length
    problems. Engineers of that era were mainly concerned with how to multiply $3
    \times 3$ matrices rather than $n \times n$ ones.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将执行时间表示为输入大小的函数的想法现在看起来很显然，但在20世纪60年代并不是这样。当时，[典型计算机](https://en.wikipedia.org/wiki/CDC_1604)价值数百万美元，如此之大以至于需要单独的房间，时钟速度以千赫兹为单位。它们被用于手头的实际任务，如预测天气，将火箭送入太空，或计算出苏联核导弹从古巴海岸飞行的距离——所有这些都是有限长度的问题。那个时代的工程师主要关心的是如何乘以$3
    \times 3$矩阵，而不是$n \times n$矩阵。
- en: What caused the shift was the acquired confidence among computer scientists
    that computers will continue to become faster — and indeed they have. Over time,
    people stopped counting execution time, then stopped counting cycles, and then
    even stopped counting operations exactly, replacing it with an *estimate* that,
    on sufficiently large inputs, is only off by no more than a constant factor. With
    *asymptotic complexity*, verbose “$4 \cdot n^3 - n^2$ operations” turns into plain
    “$\Theta(n^3)$,” hiding the initial costs of individual operations in the “Big
    O,” along with all the other intricacies of the hardware.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 导致这种转变的原因是计算机科学家们获得的信心，即计算机将继续变得更快——事实上也是如此。随着时间的推移，人们停止了计算执行时间，然后停止了计算周期，甚至停止了精确计算操作，用*估计*来代替，在足够大的输入下，这个估计只会比常数因子多。在*渐进复杂度*下，冗长的“$4
    \cdot n^3 - n^2$操作”变成了简单的“$\Theta(n^3)$”，在“大O”符号中隐藏了单个操作的初始成本，以及所有其他硬件的复杂性。
- en: '![](../Images/90369c2580760589832d77c92444c4c9.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90369c2580760589832d77c92444c4c9.png)'
- en: The reason we use asymptotic complexity is that it provides simplicity while
    still being just precise enough to yield useful results about relative algorithm
    performance on large datasets. Under the promise that computers will eventually
    become fast enough to handle any *sufficiently large* input in a reasonable amount
    of time, asymptotically faster algorithms will always be faster in real-time too,
    regardless of the hidden constant.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用渐进复杂度的原因是它提供了简洁性，同时仍然足够精确，可以产生关于相对算法性能在大数据集上的有用结果。在承诺计算机最终将足够快，能够在合理的时间内处理任何*足够大的*输入的情况下，渐进更快的算法在实时情况下也将更快，无论隐藏的常数是多少。
- en: But this promise turned out to be not true — at least not in terms of clock
    speeds and instruction latencies — and in this chapter, we will try to explain
    why and how to deal with it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个承诺最终证明是不真实的——至少在时钟速度和指令延迟方面不是——在这一章中，我们将尝试解释为什么以及如何处理这个问题。
- en: '[Modern Hardware →](https://en.algorithmica.org/hpc/complexity/hardware/)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[现代硬件 →](https://en.algorithmica.org/hpc/complexity/hardware/)'
- en: '[../Computer Architecture →](https://en.algorithmica.org/hpc/architecture/)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[../计算机体系结构 →](https://en.algorithmica.org/hpc/architecture/)'
