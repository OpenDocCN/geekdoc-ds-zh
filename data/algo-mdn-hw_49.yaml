- en: Eviction Policies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驱逐策略
- en: 原文：[https://en.algorithmica.org/hpc/external-memory/policies/](https://en.algorithmica.org/hpc/external-memory/policies/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/external-memory/policies/](https://en.algorithmica.org/hpc/external-memory/policies/)
- en: You can control the I/O operations of your program manually, but most of the
    time people just rely on automatic bufferization and caching, either due to laziness
    or because of the computing environment limitations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以手动控制程序的数据输入/输出操作，但大多数时候人们只是依赖于自动缓冲和缓存，要么是因为懒惰，要么是因为计算环境限制。
- en: But automatic caching comes with its own challenges. When a program runs out
    of working memory to store its intermediate data, it needs to get rid of one block
    to make space for a new one. A concrete rule for deciding which data to retain
    in the cache in case of conflicts is called an *eviction policy*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但自动缓存也带来了自己的挑战。当程序耗尽工作内存来存储其中间数据时，它需要移除一个块来为新块腾出空间。在冲突情况下决定保留缓存中哪些数据的具体规则称为*驱逐策略*。
- en: 'This rule can be arbitrary, but there are several popular choices:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这条规则可以是任意的，但有几个流行的选择：
- en: 'First in first out (FIFO): simply evict the earliest added block, without any
    regard to how often it was accessed before (the same way as a FIFO queue).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先进先出（FIFO）：简单地移除最早添加的块，不考虑它之前被访问的频率（与FIFO队列相同的方式）。
- en: 'Least recently used (LRU): evict the block that has not been accessed for the
    longest period of time.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近最少使用（LRU）：移除最长时间未被访问的块。
- en: 'Last in first out (LIFO) and most recently used (MRU): the opposite of the
    previous two. It seems harmful to delete the hottest blocks, but there are scenarios
    where these policies are optimal, such as repeatedly looping around a file in
    a cycle.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后进先出（LIFO）和最近最少使用（MRU）：与前两种策略相反。删除最热块似乎是有害的，但在某些场景中，这些策略是最佳的，例如在循环中反复遍历文件。
- en: 'Least-frequently used (LFU): counts how often each block has been requested
    and discards the one used least often. Some variations also account for changing
    access patterns over time, such as using a time window to only consider the last
    $n$ accesses or using exponential averaging to give recent accesses more weight.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最少使用（LFU）：统计每个块被请求的频率，并丢弃使用最少的块。一些变体还考虑了随时间变化的访问模式，例如使用时间窗口只考虑最近的$n$次访问或使用指数平均给最近访问更多的权重。
- en: 'Random replacement (RR): discard a block randomly. The advantage is that it
    does not need to maintain any data structures with block information.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机替换（RR）：随机丢弃一个块。其优点是不需要维护任何包含块信息的结构。
- en: There is a natural trade-off between the accuracy of eviction policies and the
    additional overhead due to the complexity of their implementations. For a CPU
    cache, you need a simple policy that can be easily implemented in hardware with
    next-to-zero latency, while in more slow-paced and plannable settings such as
    Netflix deciding in which data centers to store their movies or Google Drive optimizing
    where to store user data, it makes sense to use more complex policies, possibly
    involving some machine learning to predict when the data is going to be accessed
    next.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 驱逐策略的准确性与其实现复杂性的额外开销之间存在自然权衡。对于CPU缓存，你需要一个简单到可以在硬件中轻松实现且延迟接近零的策略，而在更慢速和可计划的设置中，例如Netflix决定在哪些数据中心存储他们的电影或Google
    Drive优化用户数据存储的位置，使用更复杂的策略是有意义的，可能涉及一些机器学习来预测数据何时将被下一次访问。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/policies/#optimal-caching)Optimal
    Caching'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/policies/#optimal-caching)最优缓存'
- en: Apart from the aforementioned strategies, there is also the theoretical *optimal
    policy*, denoted as $OPT$ or $MIN$, which determines, for a given sequence of
    queries, which blocks should be retained to minimize the total number of cache
    misses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述策略之外，还存在一个理论上的*最优策略*，表示为$OPT$或$MIN$，它确定对于给定的查询序列，应该保留哪些块以最小化总的缓存未命中数。
- en: 'These decisions can be made using a simple greedy approach called *Bélády algorithm*:
    we can just keep the *latest-to-be-used* block, and it can be shown by contradiction
    that doing so is always one of the optimal solutions. The downside of this method
    is that you either need to have these queries in advance or somehow be able to
    predict the future.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些决策可以通过一种简单的贪婪方法，称为*Bélády算法*来做出：我们只需保留*最近使用过的*块，并且可以通过反证法证明这样做总是最优解之一。这种方法的不利之处在于，你可能需要提前有这些查询，或者以某种方式预测未来。
- en: The good thing is that, in terms of asymptotic complexity, it doesn’t really
    matter which particular method is used. [Sleator & Tarjan showed](https://www.cs.cmu.edu/~sleator/papers/amortized-efficiency.pdf)
    that in most cases, the performance of popular policies such as $LRU$ differs
    from $OPT$ just by a constant factor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好事是，从渐近复杂性的角度来看，实际上并不重要使用哪种特定的方法。[Sleator & Tarjan证明了](https://www.cs.cmu.edu/~sleator/papers/amortized-efficiency.pdf)，在大多数情况下，流行的策略如LRU的性能与OPT只相差一个常数因子。
- en: '**Theorem.** Let $LRU_M$ and $OPT_M$ denote the number of blocks a computer
    with $M$ internal memory would need to access while executing the same algorithm
    following the least recently used cache replacement policy and the theoretical
    minimum respectively. Then:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理。**设$LRU_M$和$OPT_M$分别表示具有$M$内部内存的计算机在执行相同算法时，遵循最少最近使用缓存替换策略和理论最小值所需的块数。那么：'
- en: $$ LRU_M \leq 2 \cdot OPT_{M/2} $$
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: $$ LRU_M \leq 2 \cdot OPT_{M/2} $$
- en: 'The main idea of the proof is to consider the worst case scenario. For LRU
    it would be the repeating series of $\frac{M}{B}$ distinct blocks: each block
    is new and so LRU has 100% cache misses. Meanwhile, $OPT_{M/2}$ would be able
    to cache half of them (but not more, because it only has half the memory). Thus
    $LRU_M$ needs to fetch double the number of blocks that $OPT_{M/2}$ does, which
    is basically what is expressed in the inequality, and anything better for $LRU$
    would only weaken it.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 证明的主要思想是考虑最坏的情况。对于LRU来说，将是重复的$\frac{M}{B}$个不同块的序列：每个块都是新的，因此LRU有100%的缓存未命中。同时，$OPT_{M/2}$能够缓存其中的一半（但不能更多，因为它只有一半的内存）。因此，$LRU_M$需要获取的块数是$OPT_{M/2}$的两倍，这正是不等式所表达的内容，对于LRU来说，任何更好的方法只会削弱它。
- en: '![](../Images/d2d2701c4bc514fa00fdf4a03c4c1436.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/d2d2701c4bc514fa00fdf4a03c4c1436.png)'
- en: Dimmed are the blocks cached by OPT (but not cached by LRU)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 被OPT（但不是LRU）缓存的块被暗淡显示
- en: This is a very relieving result. It means that, at least in terms of asymptotic
    I/O complexity, you can just assume that the eviction policy is either LRU or
    OPT — whichever is easier for you — do complexity analysis with it, and the result
    you get will normally transfer to any other reasonable cache replacement policy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常令人欣慰的结果。这意味着，至少在渐近I/O复杂性的意义上，你只需假设驱逐策略是LRU或OPT——哪个对你来说更容易——用它来做复杂度分析，你得到的结果通常可以转移到任何其他合理的缓存替换策略上。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/policies/#implementing-caching)Implementing
    Caching'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/policies/#implementing-caching)
    实现缓存'
- en: This is not always a trivial task to find the right block to evict in a reasonable
    time. While CPU caches are implemented in hardware (usually as some variation
    of LRU), higher-level eviction policies have to rely on software to store certain
    statistics about the blocks and maintain data structures on top of them to speed
    up the process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在合理的时间内找到正确的块进行驱逐并不是一件简单的事情。虽然CPU缓存是在硬件中实现的（通常是LRU的一些变体），但更高级别的驱逐策略必须依赖于软件来存储有关块的某些统计信息，并在其之上维护数据结构以加快处理过程。
- en: Let’s think about what we need to implement an LRU cache. Assume we are storing
    some moderately large objects — say, we need to develop a cache for a database,
    there both the requests and replies are medium-sized strings in some SQL dialect,
    so the overhead of our structure is small but non-negligible.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下实现LRU缓存需要什么。假设我们正在存储一些中等大小的对象——比如说，我们需要为数据库开发一个缓存，其中请求和回复都是某种SQL方言中的中等大小的字符串，因此我们结构的开销很小但并非微不足道。
- en: First of all, we need a hash table to find the data itself. Since we are working
    with large variable-length strings, it makes sense to use the hash of the query
    as the key and a pointer to a heap-allocated result string as the value.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个哈希表来找到数据本身。由于我们正在处理大型的可变长度字符串，使用查询的哈希值作为键，并将指向堆分配的结果字符串的指针作为值是有意义的。
- en: To implement the LRU logic, the simplest approach would be to create a queue
    where we put the current time and IDs/keys of objects when we access them, and
    also store when each object was accessed the last time (not necessarily as a timestamp
    — any increasing counter will suffice).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现LRU逻辑，最简单的方法是创建一个队列，我们在访问对象时将当前时间和对象的ID/键放入队列中，并存储每个对象最后访问的时间（不一定是时间戳——任何递增计数器都足够）。
- en: Now, when we need to free up space, we can find the least recently used object
    by popping elements from the front of the queue. We can’t just delete them, because
    it may be that they were accessed again since their record was added to the queue.
    So we need to check if the time of when we put them in queue matches the time
    of when they were last accessed, and only then free up the memory.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们需要释放空间时，我们可以通过从队列前端弹出元素来找到最近最少使用的对象。我们不能简单地删除它们，因为它们可能自被添加到队列以来又被访问过。因此，我们需要检查我们将它们放入队列的时间是否与它们最后一次被访问的时间相匹配，然后才能释放内存。
- en: The only remaining issue here is that we add an entry to the queue each time
    a block is accessed, and only remove entries when we have a cache miss and start
    popping them off from the front until we have a match. This may lead to the queue
    overflowing, and to mitigate this, instead of adding an entry and forgetting about
    it, we can move it to the end of the queue on a cache hit right away.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一剩下的问题是，每次访问一个块时，我们都会向队列中添加一个条目，并且只有在发生缓存未命中并开始从队列前端弹出条目直到找到匹配项时才移除条目。这可能会导致队列溢出，为了减轻这种情况，我们可以在缓存命中时立即将条目移动到队列的末尾，而不是添加条目后忘记它。
- en: To support this, we need to implement the queue over a doubly-linked list and
    store a pointer to the block’s node in the queue in the hash table. Then, when
    we have a cache hit, we follow the pointer and remove the node from the linked
    list in constant time, and add a newer node to the end of the queue. This way,
    at any point in time, there would be exactly as many nodes in the queue as we
    have objects, and the memory overhead will be guaranteed to be constant per cache
    entry.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这一点，我们需要在双链表上实现队列，并在哈希表中存储指向块节点的指针。然后，当我们发生缓存命中时，我们跟随指针并在常数时间内从链表中移除节点，并将一个较新的节点添加到队列的末尾。这样，在任何时候，队列中的节点数量将正好与我们拥有的对象数量相同，并且内存开销将保证每个缓存条目都是常数。
- en: As an exercise, try to think about ways to implement other caching strategies.
    [← List Ranking](https://en.algorithmica.org/hpc/external-memory/list-ranking/)[Cache-Oblivious
    Algorithms →](https://en.algorithmica.org/hpc/external-memory/oblivious/)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试思考实现其他缓存策略的方法。[← 列表排名](https://en.algorithmica.org/hpc/external-memory/list-ranking/)[无缓存意识算法
    →](https://en.algorithmica.org/hpc/external-memory/oblivious/)
