- en: Non-portable kernel-based models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于不可移植内核的模型
- en: 原文：[https://enccs.github.io/gpu-programming/7-non-portable-kernel-models/](https://enccs.github.io/gpu-programming/7-non-portable-kernel-models/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://enccs.github.io/gpu-programming/7-non-portable-kernel-models/](https://enccs.github.io/gpu-programming/7-non-portable-kernel-models/)
- en: '*[GPU programming: why, when and how?](../)* **   Non-portable kernel-based
    models'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*[GPU编程：为什么、何时以及如何？](../)* **   基于不可移植内核的模型'
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/7-non-portable-kernel-models.rst)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在GitHub上编辑](https://github.com/ENCCS/gpu-programming/blob/main/content/7-non-portable-kernel-models.rst)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Questions
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 问题
- en: How to program GPUs with CUDA and HIP?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用CUDA和HIP编程GPU？
- en: What optimizations are possible when programming with CUDA and HIP?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA和HIP编程时，可能进行哪些优化？
- en: Objectives
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Be able to use CUDA and HIP to write basic codes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用CUDA和HIP编写基本代码
- en: Understand how the execution is done and how to do optimizations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解执行过程以及如何进行优化
- en: Instructor note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 讲师备注
- en: 45 min teaching
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 45分钟教学
- en: 30 min exercises
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 30分钟练习
- en: Fundamentals of GPU programming with CUDA and HIP
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CUDA和HIP进行GPU编程的基础知识
- en: Unlike some cross-platform portability ecosystems, such as alpaka, Kokkos, OpenCL,
    RAJA, and SYCL, which cater to multiple architectures, CUDA and HIP are solely
    focused on GPUs. They provide extensive libraries, APIs, and compiler toolchains
    that optimize code execution on NVIDIA GPUs (in the case of CUDA) and both NVIDIA
    and AMD GPUs (in the case of HIP). Because they are developed by the device producers,
    these programming models provide high-performance computing capabilities and offer
    advanced features like shared memory, thread synchronization, and memory management
    specific to GPU architectures.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与一些面向多个架构的跨平台可移植性生态系统（如alpaka、Kokkos、OpenCL、RAJA和SYCL）不同，CUDA和HIP专注于GPU。它们提供了广泛的库、API和编译器工具链，优化了代码在NVIDIA
    GPU（在CUDA的情况下）和NVIDIA及AMD GPU（在HIP的情况下）上的执行。由于它们是由设备制造商开发的，这些编程模型提供了高性能计算能力，并提供了针对GPU架构的共享内存、线程同步和内存管理等高级功能。
- en: CUDA, developed by NVIDIA, has gained significant popularity and is widely used
    for GPU programming. It offers a comprehensive ecosystem that includes not only
    the CUDA programming model but also a vast collection of GPU-accelerated libraries.
    Developers can write CUDA kernels using C++ and seamlessly integrate them into
    their applications to harness the massive parallelism of GPUs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由NVIDIA开发的CUDA已经获得了显著的流行度，并被广泛用于GPU编程。它提供了一个全面的生态系统，不仅包括CUDA编程模型，还包括大量的GPU加速库。开发者可以使用C++编写CUDA内核，并将其无缝集成到他们的应用程序中，以利用GPU的巨大并行性。
- en: HIP, on the other hand, is an open-source project that aims to provide a more
    “portable” GPU programming interface. It allows developers to write GPU code in
    a syntax similar to CUDA and provides a translation layer that enables the same
    code to run on both NVIDIA and AMD GPUs. This approach minimizes the effort required
    to port CUDA code to different GPU architectures and provides flexibility for
    developers to target multiple platforms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，HIP是一个开源项目，旨在提供一个更“可移植”的GPU编程接口。它允许开发者使用类似于CUDA的语法编写GPU代码，并提供一个翻译层，使得相同的代码可以在NVIDIA和AMD
    GPU上运行。这种方法最小化了将CUDA代码移植到不同GPU架构所需的努力，并为开发者提供了针对多个平台的灵活性。
- en: By being closely tied to the GPU hardware, CUDA and HIP provide a level of performance
    optimization that may not be achievable with cross-platform portability ecosystems.
    The libraries and toolchains offered by these programming models are specifically
    designed to exploit the capabilities of the underlying GPU architectures, enabling
    developers to achieve high performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与GPU硬件紧密相连，CUDA和HIP提供了一种可能无法通过跨平台可移植性生态系统实现的性能优化级别。这些编程模型提供的库和工具链专门设计用于利用底层GPU架构的能力，使开发者能够实现高性能。
- en: Developers utilizing CUDA or HIP can tap into an extensive ecosystem of GPU-accelerated
    libraries, covering various domains, including linear algebra, signal processing,
    image processing, machine learning, and more. These libraries are highly optimized
    to take advantage of the parallelism and computational power offered by GPUs,
    allowing developers to accelerate their applications without having to implement
    complex algorithms from scratch.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 利用CUDA或HIP的开发者可以访问一个广泛的GPU加速库生态系统，涵盖线性代数、信号处理、图像处理、机器学习等多个领域。这些库高度优化，以利用GPU提供的并行性和计算能力，允许开发者加速他们的应用程序，而无需从头实现复杂的算法。
- en: As mentioned before, CUDA and HIP are very similar so it makes sense to cover
    both at the same time.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CUDA 和 HIP 非常相似，因此同时介绍两者是有意义的。
- en: Comparison to portable kernel-based models
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与可移植内核模型比较
- en: In code examples below, we will also show examples in the portable kernel-based
    frameworks Kokkos, SYCL and OpenCL, which will be covered in the next episode.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们还将展示在可移植的基于内核的框架 Kokkos、SYCL 和 OpenCL 中的示例，这些将在下一集中介绍。
- en: Hello World
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hello World
- en: 'Below we have the most basic example of CUDA and HIP, the “Hello World” program:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们对 CUDA 和 HIP 的最基本示例，即“Hello World”程序：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In both versions, we include the necessary headers: **cuda_runtime.h** and
    **cuda.h** for CUDA, and **hip_runtime.h** for HIP. These headers provide the
    required functionality for GPU programming.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个版本中，我们都包含了必要的头文件：CUDA 的 **cuda_runtime.h** 和 **cuda.h**，以及 HIP 的 **hip_runtime.h**。这些头文件提供了
    GPU 编程所需的函数。
- en: To retrieve information about the available devices, we use the functions **<cuda/hip>GetDeviceCount**
    and **<cuda/hip>GetDevice**. These functions allow us to determine the total number
    of GPUs and the index of the currently used device. In the code examples, we default
    to using device 0.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索有关可用设备的信息，我们使用函数 **<cuda/hip>GetDeviceCount** 和 **<cuda/hip>GetDevice**。这些函数允许我们确定
    GPU 的总数以及当前使用设备的索引。在代码示例中，我们默认使用设备 0。
- en: 'As an exercise, modify the “Hello World” code to explicitly use a specific
    GPU. Do this by using the **<cuda/hip>SetDevice** function, which allows to set
    the desired GPU device. Note that the device number provided has to be within
    the range of available devices, otherwise, the program may fail to run or produce
    unexpected results. To experiment with different GPUs, modify the code to include
    the following line before retrieving device information:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，修改“Hello World”代码以显式使用特定的 GPU。通过使用 **<cuda/hip>SetDevice** 函数来实现，该函数允许设置所需的
    GPU 设备。请注意，提供的设备号必须在可用设备范围内，否则程序可能无法运行或产生意外的结果。为了实验不同的 GPU，在检索设备信息之前修改代码以包含以下行：
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Replace **deviceNumber** with the desired GPU device index. Run the code with
    different device numbers to observe the output (more examples for the “Hello World”
    program are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    subdirectory of this lesson repository).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将 **deviceNumber** 替换为所需的 GPU 设备索引。使用不同的设备号运行代码以观察输出（本课程仓库的 [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    子目录中提供了“Hello World”程序的更多示例）。
- en: Vector Addition
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量加法
- en: To demonstrate the fundamental features of CUDA/HIP programming, let’s begin
    with a straightforward task of element-wise vector addition. The code snippet
    below demonstrates how to utilize CUDA and HIP for efficiently executing this
    operation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 CUDA/HIP 编程的基本功能，让我们从一项简单的任务开始，即逐元素向量加法。下面的代码片段展示了如何利用 CUDA 和 HIP 高效地执行此操作。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, the CUDA and HIP codes are equivalent one to one so we will only
    refer to the CUDA version. The CUDA and HIP programming model are host centric
    programming models. The main program is executed on CPU and controls all the operations,
    memory allocations, data transfers between CPU and GPU, and launches the kernels
    to be executed on the GPU. The code starts with defining the GPU kernel function
    called **vector_add** with attribute **___global__**. It takes three input arrays
    A, B, and C along with the array size n. The kernel function contains the actually
    code which is executed on the GPU by multiple threads in parallel.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，CUDA 和 HIP 代码是逐个对应的，因此我们只参考 CUDA 版本。CUDA 和 HIP 编程模型是主机中心编程模型。主程序在 CPU
    上执行并控制所有操作、内存分配、CPU 和 GPU 之间的数据传输，并启动在 GPU 上执行的内核。代码从定义具有 **___global__** 属性的
    GPU 内核函数 **vector_add** 开始。它包含三个输入数组 A、B 和 C 以及数组大小 n。内核函数包含实际上在 GPU 上由多个线程并行执行的代码。
- en: Accelerators in general and GPUs in particular usually have their own dedicated
    memory separate from the system memory (AMD MI300A is one exception, using the
    same memory for both CPU and GPU). When programming for GPUs, there are two sets
    of pointers involved and it’s necessary to manage data movement between the host
    memory and the accelerator memory. Data needs to be explicitly copied from the
    host memory to the accelerator memory before it can be processed by the accelerator.
    Similarly, results or modified data may need to be copied back from the accelerator
    memory to the host memory to make them accessible to the CPU.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，加速器（尤其是GPU）都有自己的专用内存，与系统内存分开（AMD MI300A 是一个例外，它使用相同的内存为 CPU 和 GPU）。当为 GPU
    编程时，涉及两组指针，并且需要管理主机内存和加速器内存之间的数据移动。在加速器处理之前，数据需要显式地从主机内存复制到加速器内存。同样，结果或修改后的数据可能需要从加速器内存复制回主机内存，以便使它们对
    CPU 可访问。
- en: The main function of the code initializes the input arrays Ah, Bh on the CPU
    and computes the reference array Cref. It then allocates memory on the GPU for
    the input and output arrays Ad, Bd, and Cd using **cudaMalloc**. Herein, h is
    for the ‘host’ (CPU) and d for the ‘device’ (GPU). The data is transferred from
    the CPU to the GPU using hipMemcpy, and then the GPU kernel is launched using
    the <<<.>>> syntax. All kernels launch are asynchronous. After launch the control
    returns to the main() and the code proceeds to the next instructions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的主函数在 CPU 上初始化输入数组 Ah、Bh，并计算参考数组 Cref。然后使用 **cudaMalloc** 在 GPU 上为输入和输出数组
    Ad、Bd、Cd 分配内存。在这里，h 代表 ‘host’（CPU），d 代表 ‘device’（GPU）。数据使用 hipMemcpy 从 CPU 传输到
    GPU，然后使用 <<<.>>> 语法启动 GPU 内核。所有启动的内核都是异步的。启动后，控制权返回到 main()，代码继续执行下一行指令。
- en: After the kernel execution, the result array Cd is copied back to the CPU using
    **cudaMemcpy**. The code then prints the reference and result arrays, calculates
    the error by comparing the reference and result arrays. Finally, the GPU and CPU
    memory are deallocated using **cudaFree** and **free** functions, respectively.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核执行后，使用 **cudaMemcpy** 将结果数组 Cd 复制回 CPU。然后代码打印参考和结果数组，通过比较参考和结果数组来计算误差。最后，使用
    **cudaFree** 和 **free** 函数分别释放 GPU 和 CPU 内存。
- en: The host functions **cudaSetDevice**, **cudaMalloc**, **cudaMemcpy**, and **cudaFree**
    are blocking, i.e. the code does not continues to next instructions until the
    operations are completed. However this is not the default behaviour, for many
    operations there are asynchronous equivalents and there are as well many library
    calls return the control to the main() after calling. This allows the developers
    to launch independent operations and overlap them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主机函数 **cudaSetDevice**、**cudaMalloc**、**cudaMemcpy** 和 **cudaFree** 是阻塞的，即代码不会继续执行到下一行指令，直到操作完成。然而，这并非默认行为，对于许多操作，存在异步等效操作，以及许多库调用在调用后返回控制权到
    main()。这允许开发者启动独立操作并将它们重叠。
- en: In short, this code demonstrates how to utilize the CUDA and HIP to perform
    vector addition on a GPU, showcasing the steps involved in allocating memory,
    transferring data between the CPU and GPU, launching a kernel function, and handling
    the results. It serves as a starting point for GPU-accelerated computations using
    CUDA and HIP. More examples for the vector (array) addition program are available
    at [content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，此代码演示了如何利用 CUDA 和 HIP 在 GPU 上执行向量加法，展示了涉及分配内存、在 CPU 和 GPU 之间传输数据、启动内核函数和处理结果的步骤。它作为使用
    CUDA 和 HIP 进行 GPU 加速计算的起点。有关向量（数组）加法程序的更多示例，请参阅 [content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples)。
- en: In order to practice the concepts shown above, edit the skeleton code in the
    repository and the code corresponding to setting the device, memory allocations
    and transfers, and the kernel execution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实践上述概念，请编辑存储库中的骨架代码以及与设置设备、内存分配和传输、内核执行相对应的代码。
- en: Vector Addition with Unified Memory
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用统一内存的向量加法
- en: 'For a while already GPUs support unified memory, which allows to use the same
    pointer for both CPU and GPU data. This simplifies developing codes by removing
    the explicit data transfers. The data resides on CPU until it is needed on GPU
    or vice-versa. However the data transfers still happens “under the hood” and the
    developer needs to construct the code to avoid unnecessary transfers. Below one
    can see the modified vector addition codes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一段时间了，GPU已经支持统一内存，这允许使用相同的指针来访问CPU和GPU数据。这通过消除显式的数据传输简化了代码的开发。数据驻留在CPU上，直到需要在GPU上使用，反之亦然。然而，数据传输仍然“在幕后”发生，开发者需要构建代码以避免不必要的传输。下面可以看到修改后的向量加法代码：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now the arrays Ah, Bh, Ch, and Cref are using cudaMallocManaged to allocate
    Unified Memory. The **vector_add kernel** is launched by passing these Unified
    Memory pointers directly. After the kernel launch, **cudaDeviceSynchronize** is
    used to wait for the kernel to complete execution. Finally, **cudaFree** is used
    to free the Unified Memory arrays. The Unified Memory allows for transparent data
    migration between CPU and GPU, eliminating the need for explicit data transfers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数组Ah、Bh、Ch和Cref使用cudaMallocManaged来分配统一内存。**向量加法内核**通过传递这些统一内存指针直接启动。内核启动后，使用**cudaDeviceSynchronize**等待内核完成执行。最后，使用**cudaFree**释放统一内存数组。统一内存允许在CPU和GPU之间透明地迁移数据，消除了显式数据传输的需要。
- en: As an exercise modify the skeleton code for vector addition to use Unified Memory.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，修改向量加法的骨架代码以使用统一内存。
- en: Basics - In short
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基础 - 简而言之
- en: CUDA is developed by NVIDIA, while HIP is an open-source project (from AMD)
    for multi-platform GPU programming.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA由NVIDIA开发，而HIP是AMD的一个开源项目（用于多平台GPU编程）。
- en: CUDA and HIP are GPU-focused programming models for optimized code execution
    on NVIDIA and AMD GPUs.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是针对NVIDIA和AMD GPU上优化代码执行的GPU-focused编程模型。
- en: CUDA and HIP are similar, allowing developers to write GPU code in a syntax
    similar to CUDA and target multiple platforms.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP相似，允许开发者使用类似于CUDA的语法编写GPU代码，并针对多个平台进行目标定位。
- en: CUDA and HIP are programming models focused solely on GPUs
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是仅针对GPU的编程模型。
- en: CUDA and HIP offer high-performance computing capabilities and advanced features
    specific to GPU architectures, such as shared memory and memory management.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP提供了针对GPU架构的高性能计算能力和高级功能，例如共享内存和内存管理。
- en: They provide highly GPU-accelerated libraries in various domains like linear
    algebra, signal processing, image processing, and machine learning.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们提供了在各个领域（如线性代数、信号处理、图像处理和机器学习）中高度GPU加速的库。
- en: Programming for GPUs involves managing data movement between host and accelerator
    memory.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为GPU编程涉及在主机和加速器内存之间管理数据移动。
- en: Unified Memory simplifies data transfers by using the same pointer for CPU and
    GPU data, but code optimization is still necessary.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一内存通过使用相同的指针来简化CPU和GPU数据之间的数据传输，但代码优化仍然是必要的。
- en: Memory Optimizations
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存优化
- en: Vector addition is a relatively simple, straight forward case. Each thread reads
    data from memory, does an addition and then saves the result. Two adjacent threads
    access memory location in memory close to each other. Also the data is used only
    once. In practice this not the case. Also sometimes the same data is used several
    times resulting in additional memory accesses.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 向量加法是一个相对简单、直接的案例。每个线程从内存中读取数据，进行加法运算，然后将结果保存。两个相邻的线程访问内存中彼此靠近的位置。此外，数据只使用一次。在实践中，情况并非如此。有时，相同的数据被多次使用，导致额外的内存访问。
- en: Memory optimization is one of the most important type of optimization done to
    efficiently use the GPUs. Before looking how it is done in practice let’s revisit
    some basic concepts about GPUs and execution model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 内存优化是用于高效使用GPU的最重要类型的优化之一。在探讨实际操作之前，让我们回顾一下关于GPU和执行模型的一些基本概念。
- en: GPUs are comprised many light cores, the so-called Streaming Processors (SP)
    in CUDA, which are physically group together in units, i.e. Streaming Multi-Processors
    (SMP) in CUDA architecture (note that in AMD the equivalent is called Computing
    Units, while in Intel GPUs they are Execution Units). The work is done on GPUs
    by launching many threads each executing an instance of the same kernel. The order
    of execution is not defined, and the threads can only exchange information in
    specific conditions. Because of the way the SPs are grouped the threads are also
    grouped in **blocks**. Each **block** is assigned to an SMP, and can not be split.
    An SMP can have more than block residing at a moment, however there is no communications
    between the threads in different blocks. In addition to the SPs, each SMP contains
    very fast memory which in CUDA is referred to as shared memory. The threads in
    a block can read and write to the shared memory and use it as a user controlled
    cache. One thread can for example write to a location in the shared memory while
    another thread in the same block can read and use that data. In order to be sure
    that all threads in the block completed writing **__syncthreads()** function has
    to be used to make the threads in the block wait until all of them reached the
    specific place in the kernel. Another important aspect in the GPU programming
    model is that the threads in the block are not executed independently. The threads
    in a block are physically grouped in warps of size 32 in NVIDIA devices or wavefronts
    of size 32 or 64 in AMD devices (depending on device architecture). Intel devices
    are notable in that the warp size, called SIMD width, is highly configurable,
    with typical possible values of 8, 16, or 32 (depends on the hardware). All memory
    accesses of the global GPU memory are done per warp. When data is needed for some
    calculations a warp loads from the GPU memory blocks of specific size (64 or 128
    Bytes). These operation is very expensive, it has a latency of hundreds of cycles.
    This means that the threads in a warp should work with elements of the data located
    close in the memory. In the vector addition two threads near each other, of index
    tid and tid+1, access elements adjacent in the GPU memory.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GPU由许多轻量级核心组成，在CUDA中称为流处理器（SP），它们在物理上以单元的形式组合在一起，即CUDA架构中的流多处理器（SMP）（注意，在AMD中，等效的部分称为计算单元，而在英特尔GPU中，它们称为执行单元）。GPU上的工作是通过启动许多线程来完成的，每个线程执行相同内核的一个实例。执行顺序未定义，线程只能在特定条件下交换信息。由于SPs的组合方式，线程也被组合成**块**。每个**块**被分配给一个SMP，并且不能分割。一个SMP可以同时容纳多个块，然而不同块之间的线程没有通信。除了SPs之外，每个SMP还包含非常快速的内存，在CUDA中称为共享内存。块中的线程可以读写共享内存，并将其用作用户控制的缓存。例如，一个线程可以写入共享内存中的某个位置，而同一块中的另一个线程可以读取并使用这些数据。为了确保块中的所有线程都完成了写入，必须使用**__syncthreads()**函数来使块中的线程等待，直到它们都到达内核中的特定位置。在GPU编程模型中另一个重要方面是，块中的线程不是独立执行的。在NVIDIA设备中，块中的线程以大小为32的warp形式物理组合，在AMD设备中，以大小为32或64的wavefront形式组合（取决于设备架构）。英特尔设备值得注意的是，warp的大小，称为SIMD宽度，可以高度配置，典型的可能值为8、16或32（取决于硬件）。所有全局GPU内存的内存访问都是按warp进行的。当需要某些计算的数据时，warp从GPU内存中加载特定大小的块（64或128字节）。这些操作非常昂贵，具有数百个周期的延迟。这意味着warp中的线程应该与内存中位置接近的数据元素一起工作。在向量加法中，两个相邻的线程，索引为tid和tid+1，访问GPU内存中相邻的元素。
- en: The shared memory can be used to improve performance in two ways. It is possible
    to avoid extra reads from the memory when several threads in the same block need
    the same data (see [stencil](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/stencil)
    code) or it can be used to improve the memory access patterns like in the case
    of matrix transpose.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存可以通过两种方式来提高性能。当同一块中的多个线程需要相同的数据时，可以避免从内存中进行额外的读取（参见[stencil](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/stencil)代码）或者它可以用来改进内存访问模式，如矩阵转置的情况。
- en: Memory, Execution - In short
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 内存、执行 - 简要来说
- en: GPUs consist of streaming processors (SPs) grouped together in units, such as
    Streaming Multi-Processors (SMPs) in CUDA architecture.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU由流处理器（SPs）组成，这些处理器以单元的形式组合在一起，例如在CUDA架构中的流多处理器（SMPs）。
- en: Work on GPUs is done by launching threads, with each thread executing an instance
    of the same kernel, and the execution order is not defined.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上进行的操作是通过启动线程来完成的，每个线程执行相同内核的一个实例，执行顺序未定义。
- en: Threads are organized into blocks, assigned to an SMP, and cannot be split,
    and there is no communication between threads in different blocks.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程被组织成块，分配给SMP，并且不能被分割，并且不同块之间的线程没有通信。
- en: Each SMP contains shared memory, which acts as a user-controlled cache for threads
    within a block, allowing efficient data sharing and synchronization.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个SMP包含共享内存，它作为块内线程的用户可控缓存，允许高效的数据共享和同步。
- en: The shared memory can be used to avoid extra memory reads when multiple threads
    in the same block need the same data or to improve memory access patterns, such
    as in matrix transpose operations.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以用来避免当同一块中的多个线程需要相同数据时进行额外的内存读取，或者改善内存访问模式，例如在矩阵转置操作中。
- en: Memory accesses from global GPU memory are performed per warp (groups of threads),
    and loading data from GPU memory has high latency.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从全局GPU内存进行的内存访问是按warp（线程组）进行的，从GPU内存加载数据具有高延迟。
- en: To optimize memory access, threads within a warp should work with adjacent elements
    in memory to reduce latency.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了优化内存访问，线程组内的线程应该与内存中的相邻元素一起工作，以减少延迟。
- en: Proper utilization of shared memory can improve performance by reducing memory
    reads and enhancing memory access patterns.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确利用共享内存可以通过减少内存读取和增强内存访问模式来提高性能。
- en: Matrix Transpose
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵转置
- en: Matrix transpose is a classic example where shared memory can significantly
    improve the performance. The use of shared memory reduces global memory accesses
    and exploits the high bandwidth and low latency of shared memory.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵转置是一个共享内存可以显著提高性能的经典例子。使用共享内存减少了全局内存访问，并利用了共享内存的高带宽和低延迟。
- en: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
- en: First as a reference we use a simple kernel which copy the data from one array
    to the other.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用一个简单的内核作为参考，该内核将数据从一个数组复制到另一个数组。
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We note that this code does not do any calculations. Each thread reads one element
    and then writes it to another locations. By measuring the execution time of the
    kernel we can compute the effective bandwidth achieve by this kernel. We can measure
    the time using **rocprof** or **cuda/hip events**. On a NVIDIA V100 GPU this code
    achieves 717 GB/s out of the theoretical peak 900 GB/s.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到这段代码没有进行任何计算。每个线程读取一个元素，然后将其写入另一个位置。通过测量内核的执行时间，我们可以计算出该内核的有效带宽。我们可以使用**rocprof**或**cuda/hip
    events**来测量时间。在NVIDIA V100 GPU上，此代码实现了717 GB/s，理论峰值900 GB/s。
- en: Now we do the first iteration of the code, a naive transpose. The reads have
    a nice coalesced access pattern, but the writing is now very inefficient.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进行代码的第一轮迭代，一个简单的转置。读取具有很好的归约访问模式，但写入现在非常低效。
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Checking the index in_index we see that two adjacent threads (threadIx.x, threadIdx.x+1)
    access location in memory near each other. However the writes are not. Threads
    access data which in a strided way. Two adjacent threads access data separated
    by height elements. This practically results in 32 memory operations, however
    due to under the hood optimizations the achieved bandwidth is 311 GB/s.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 检查索引in_index，我们看到两个相邻的线程（threadIx.x，threadIdx.x+1）访问内存中的相邻位置。然而，写入操作不是这样的。线程以步进方式访问数据。两个相邻的线程访问的数据之间相隔height个元素。这实际上导致了32次内存操作，但由于底层的优化，实现的带宽为311
    GB/s。
- en: We can improve the code by reading the data in a coalesced way, save it in the
    shared memory row by row and then write in the global memory column by column.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以归约的方式读取数据，逐行将其保存到共享内存中，然后按列写入全局内存来改进代码。
- en: '[PRE18]'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We define a **tile_dim** constant to determine the size of the shared memory
    tile. The matrix transpose kernel uses a 2D grid of thread blocks, where each
    thread block operates on a tile_dim x tile_dim tile of the input matrix.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个**tile_dim**常量来确定共享内存块的大小。矩阵转置内核使用一个二维线程块网格，其中每个线程块操作输入矩阵的tile_dim x tile_dim块。
- en: The kernel first loads data from the global memory into the shared memory tile.
    Each thread loads a single element from the input matrix into the shared memory
    tile. Then, a **__syncthreads()** barrier ensures that all threads have finished
    loading data into shared memory before proceeding.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 内核首先将全局内存中的数据加载到共享内存块中。每个线程从输入矩阵中加载单个元素到共享内存块中。然后，一个**__syncthreads()**屏障确保所有线程在继续之前已经将数据加载到共享内存中。
- en: Next, the kernel writes the transposed data from the shared memory tile back
    to the output matrix in global memory. Each thread writes a single element from
    the shared memory tile to the output matrix. By using shared memory, this optimized
    implementation reduces global memory accesses and exploits memory coalescence,
    resulting in improved performance compared to a naive transpose implementation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，内核将共享内存瓷砖中的转置数据写回全局内存中的输出矩阵。每个线程将共享内存瓷砖中的一个元素写入输出矩阵。通过使用共享内存，这种优化实现减少了全局内存访问并利用了内存归约，与简单的转置实现相比，性能得到提升。
- en: This kernel achieved on NVIDIA V100 674 GB/s.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此内核在NVIDIA V100上达到了674 GB/s。
- en: This is pretty close to the bandwidth achieved by the simple copy kernel, but
    there is one more thing to improve.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎达到了简单复制内核所达到的带宽，但还有一点可以改进。
- en: Shared memory is composed of banks. Each banks can service only one request
    at the time. Bank conflicts happen when more than 1 thread in a specific warp
    try to access data in bank. The bank conflicts are resolved by serializing the
    accesses resulting in less performance. In the above example when data is saved
    to the shared memory, each thread in the warp will save an element of the data
    in a different one. Assuming that shared memory has 16 banks after writing each
    bank will contain one column. At the last step when we write from the shared memory
    to the global memory each warp load data from the same bank. A simple way to avoid
    this is by just padding the temporary array.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存由银行组成。每个银行一次只能服务一个请求。当特定战中的一个线程尝试访问银行中的数据时，就会发生银行冲突。通过序列化访问来解决银行冲突，导致性能降低。在上面的例子中，当数据保存到共享内存时，战中的每个线程将保存数据的一个元素到不同的位置。假设共享内存有16个银行，在写入后，每个银行将包含一列。在最后一步，当我们从共享内存写入全局内存时，每个战将从相同的银行加载数据。避免这种情况的一个简单方法是通过填充临时数组。
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: By padding the array the data is slightly shifting it resulting in no bank conflicts.
    The effective bandwidth for this kernel is 697 GB/s.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过填充数组，数据会略微偏移，从而避免了银行冲突。此内核的有效带宽为697 GB/s。
- en: Using sharing memory as a cache - In short
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享内存作为缓存 - 简而言之
- en: Shared memory can significantly improve performance in operations like matrix
    transpose.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以显著提高矩阵转置等操作的性能。
- en: Shared memory reduces global memory accesses and exploits the high bandwidth
    and low latency of shared memory.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存减少了全局内存访问并利用了共享内存的高带宽和低延迟。
- en: An optimized implementation utilizes shared memory, loads data coalescedly,
    and performs transpose operations.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的实现利用共享内存，以归约方式加载数据，并执行转置操作。
- en: The optimized implementation uses a 2D grid of thread blocks and a shared memory
    tile size determined by a constant.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的实现使用了一个二维的线程块网格和一个由常量确定的共享内存瓷砖大小。
- en: The kernel loads data from global memory into the shared memory tile and uses
    a synchronization barrier.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核将数据从全局内存加载到共享内存瓷砖，并使用同步屏障。
- en: To avoid bank conflicts in shared memory, padding the temporary array is a simple
    solution.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免共享内存中的银行冲突，填充临时数组是一个简单的解决方案。
- en: Reductions
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 归约
- en: Reductions refer to operations in which the elements of an array are aggregated
    in a single value through operations such as summing, finding the maximum or minimum,
    or performing logical operations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 归约指的是通过求和、查找最大或最小值或执行逻辑运算等操作将数组元素聚合为单个值的操作。
- en: In the serial approach, the reduction is performed sequentially by iterating
    through the collection of values and accumulating the result step by step. This
    will be enough for small sizes, but for big problems this results in significant
    time spent in this part of an application. On a GPU, this approach is not feasible.
    Using just one thread to do this operation means the rest of the GPU is wasted.
    Doing reduction in parallel is a little tricky. In order for a thread to do work,
    it needs to have some partial result to use. If we launch, for example, a kernel
    performing a simple vector summation, `sum[0]+=a[tid]`, with N threads we notice
    that this would result in undefined behaviour. GPUs have mechanisms to access
    the memory and lock the access for other threads while 1 thread is doing some
    operations to a given data via **atomics**, however this means that the memory
    access gets again to be serialized. There is not much gain. We note that when
    doing reductions the order of the iterations is not important (barring the typical
    non-associative behavior of floating-point operations). Also we can we might have
    to divide our problem in several subsets and do the reduction operation for each
    subset separately. On the GPUs, since the GPU threads are grouped in blocks, the
    size of the subset based on that. Inside the block, threads can cooperate with
    each other, they can share data via the shared memory and can be synchronized
    as well. All threads read the data to be reduced, but now we have significantly
    less partial results to deal with. In general, the size of the block ranges from
    256 to 1024 threads. In case of very large problems, after this procedure if we
    are left too many partial results this step can be repeated.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在顺序方法中，减少操作是通过迭代值集合并逐步累积结果来顺序执行的。这对于小规模问题来说足够了，但对于大问题，这会导致在这个应用程序的这一部分花费大量时间。在GPU上，这种方法是不可行的。仅使用一个线程来完成这个操作意味着GPU的其余部分都被浪费了。并行执行减少操作有点棘手。为了线程能够进行工作，它需要有一些部分结果来使用。例如，如果我们启动一个执行简单向量求和的内核，`sum[0]+=a[tid]`，使用N个线程，我们会注意到这将导致未定义的行为。GPU有机制来访问内存并在一个线程对给定数据进行某些操作时锁定其他线程的访问，通过**原子操作**，但这意味着内存访问再次被序列化。这并没有带来太多好处。我们注意到，在执行减少操作时，迭代顺序并不重要（除了浮点操作的典型非结合行为）。此外，我们可能需要将问题分成几个子集，并对每个子集分别执行减少操作。在GPU上，由于GPU线程被分组在块中，子集的大小基于这一点。在块内部，线程可以相互合作，可以通过共享内存共享数据，也可以进行同步。所有线程都读取要减少的数据，但现在我们处理的部分结果显著减少。一般来说，块的大小从256到1024个线程不等。在非常大的问题中，如果在这个过程之后我们留下了太多的部分结果，这一步可以重复进行。
- en: 'At the block level we still have to perform a reduction in an efficient way.
    Doing it serially means that we are not using all GPU cores (roughly 97% of the
    computing capacity is wasted). Doing it naively parallel using **atomics**, but
    on the shared memory is also not a good option. Going back back to the fact the
    reduction operations are commutative and associative we can set each thread to
    “reduce” two elements of the local part of the array. Shared memory can be used
    to store the partial “reductions” as shown below in the code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在块级别，我们仍然需要以高效的方式执行减少操作。如果是顺序执行，意味着我们没有使用所有GPU核心（大约97%的计算能力被浪费了）。如果使用**原子操作**进行简单的并行操作，但在共享内存上也不是一个好的选择。回到这样一个事实，即减少操作是交换律和结合律的，我们可以让每个线程“减少”数组局部部分的两个元素。共享内存可以用来存储如以下代码所示的部分“减少”结果：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the kernel we have each GPU performing thread a reduction of two elements
    from the local portion of the array. If we have tpb GPU threads per block, we
    utilize them to store 2xtpb elements in the local shared memory. To ensure synchronization
    until all data is available in the shared memory, we employ the syncthreads()
    function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，每个GPU线程都对数组局部部分的两个元素进行减少操作。如果我们有每个块tpb个GPU线程，我们利用它们在局部共享内存中存储2xtpb个元素。为了确保同步，直到所有数据都可用在共享内存中，我们使用syncthreads()函数。
- en: Next, we instruct each thread to “reduce” the element in the array at threadIdx.x
    with the element at threadIdx.x+tpb. As this operation saves the result back into
    the shared memory, we once again employ syncthreads(). By doing this, we effectively
    halve the number of elements to be reduced.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指导每个线程将数组中位于threadIdx.x位置的元素与threadIdx.x+tpb位置的元素进行“减少”操作。由于这个操作将结果保存回共享内存，我们再次使用syncthreads()。通过这样做，我们有效地将需要减少的元素数量减半。
- en: This procedure can be repeated, but now we only utilize tpb/2 threads. Each
    thread is responsible for “reducing” the element in the array at threadIdx.x with
    the element at threadIdx.x+tpb/2. After this step, we are left with tpb/4 numbers
    to be reduced. We continue applying this procedure until only one number remains.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以重复，但现在我们只利用 tpb/2 个线程。每个线程负责使用 threadIdx.x 中的元素和 threadIdx.x+tpb/2 中的元素“减少”数组中位于
    threadIdx.x 的元素。在此步骤之后，我们剩下 tpb/4 个数字需要减少。我们继续应用此过程，直到只剩下一个数字。
- en: At this point, we can either “reduce” the final number with a global partial
    result using atomic read and write operations, or we can save it into an array
    for further processing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用全局部分结果和原子读写操作来“减少”最终数字，或者我们可以将其保存到数组中以便进一步处理。
- en: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
- en: Schematic representation on the reduction algorithm with 8 GPU threads.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 8 个 GPU 线程的减少算法的示意图。
- en: For a detail analysis of how to optimize reduction operations in CUDA/HIP check
    this presentation [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CUDA/HIP 中如何优化减少操作的详细分析，请查看这个演示文稿 [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
- en: Reductions - In short
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 减少操作 - 简而言之
- en: Reductions refer to aggregating elements of an array into a single value through
    operations like summing, finding maximum or minimum, or performing logical operations.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少操作指的是通过求和、查找最大或最小值或执行逻辑运算等操作，将数组中的元素聚合为一个单一值。
- en: Performing reductions sequentially in a serial approach is inefficient for large
    problems, while parallel reduction on GPUs offers better performance.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大型问题，按顺序进行串行减少操作效率低下，而 GPU 上的并行减少操作提供了更好的性能。
- en: Parallel reduction on GPUs involves dividing the problem into subsets, performing
    reductions within blocks of threads using shared memory, and repeatedly reducing
    the number of elements (two per GPU thread) until only one remains.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GPU 上进行并行减少操作涉及将问题划分为子集，使用共享内存在每个线程块内执行减少操作，并重复减少元素的数量（每个 GPU 线程两个），直到只剩下一个元素。
- en: Overlapping Computations and Memory transfer. CUDA/HIP Streams
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重叠计算和内存传输。CUDA/HIP 流
- en: Modern GPUs can overlap independent operations. They can do transfers between
    CPU and GPU and execute kernels in the same time, or they can execute kernels
    concurrently. CUDA/HIP streams are independent execution units, a sequence of
    operations that execute in issue-order on the GPU. The operations issue in different
    streams can be executed concurrently.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 GPU 可以重叠独立操作。它们可以在 CPU 和 GPU 之间进行传输并在同一时间执行内核，或者它们可以并发执行内核。CUDA/HIP 流是独立的执行单元，是一系列在
    GPU 上按发布顺序执行的运算。不同流中的运算可以并发执行。
- en: Consider the previous case of vector addition, which involves copying data from
    CPU to GPU, computations and then copying back the result to GPU. In this way
    nothing can be overlap.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑之前的向量加法案例，它涉及从 CPU 到 GPU 的数据复制、计算然后将结果复制回 GPU。这样就不能重叠。
- en: We can improve the performance by dividing the problem in smaller independent
    parts. Let’s consider 5 streams and consider the case where copy in one direction
    and computation take the same amount of time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将问题划分为更小的独立部分来提高性能。让我们考虑 5 个流，并考虑复制和计算花费相同时间的案例。
- en: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
- en: After the first and second stream copy data to the GPU, the GPU is practically
    occupied all time. We can see that significant performance improvements can be
    obtained by eliminating the time in which the GPU is idle, waiting for data to
    arrive from the CPU. This very useful for problems where there is often communication
    to the CPU because the GPU memory can not fit all the problem or the application
    runs in a multi-GPU set up and communication is needed often.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个和第二个流将数据复制到 GPU 后，GPU 几乎一直处于忙碌状态。我们可以看到，通过消除 GPU 空闲等待 CPU 数据到达的时间，可以获得显著的性能提升。这对于经常需要与
    CPU 通信的问题非常有用，因为 GPU 内存无法容纳整个问题或应用程序在多 GPU 设置中运行且需要频繁通信。
- en: We can apply this to the vector addition problem above.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此应用于上述的向量加法问题。
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Instead of having one copy to gpu, one execution of the kernel and one copy
    back, we now have several of these calls independent of each other.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是只有一个复制到 GPU、一次内核执行和一次复制回的操作，我们现在有多个这样的调用，彼此独立。
- en: Note that even when streams are not explicitly used it is possible to launch
    all the GPU operations asynchronous and overlap CPU operations (such I/O) and
    GPU operations. In order to learn more about how to improve performance using
    streams check the NVIDIA blog [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使没有明确使用流，也可以异步启动所有GPU操作，重叠CPU操作（如I/O）和GPU操作。要了解更多关于如何使用流来提高性能的信息，请查看NVIDIA博客[如何在CUDA
    C/C++中重叠数据传输](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)。
- en: Streams - In short
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 流 - 简而言之
- en: CUDA/HIP streams are independent execution contexts on the GPU that allow for
    concurrent execution of operations issued in different streams.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA/HIP流是GPU上的独立执行上下文，允许并发执行不同流中发出的操作。
- en: Using streams can improve GPU performance by overlapping operations such as
    data transfers between CPU and GPU and kernel executions.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流可以改善GPU性能，通过重叠操作，例如CPU和GPU之间的数据传输和内核执行。
- en: By dividing a problem into smaller independent parts and utilizing multiple
    streams, the GPU can avoid idle time, resulting in significant performance improvements,
    especially for problems with frequent CPU communication or multi-GPU setups.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将问题分解为更小的独立部分并利用多个流，GPU可以避免空闲时间，从而实现显著的性能提升，特别是对于频繁进行CPU通信或多GPU设置的程序。
- en: Pros and cons of native programming models
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原生编程模型的优缺点
- en: 'There are advantages and limitations to CUDA and HIP:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA和HIP都有优点和局限性：
- en: 'CUDA Pros:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA优点：
- en: 'Performance Boost: CUDA is designed for NVIDIA GPUs and delivers excellent
    performance.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能提升：CUDA是为NVIDIA GPU设计的，并提供了卓越的性能。
- en: 'Wide Adoption: CUDA is popular, with many resources and tools available.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广泛采用：CUDA很受欢迎，有许多资源和工具可用。
- en: 'Mature Ecosystem: NVIDIA provides comprehensive libraries and tools for CUDA
    programming.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成熟的生态系统：NVIDIA为CUDA编程提供了全面的库和工具。
- en: 'HIP Pros:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: HIP优点：
- en: 'Portability: HIP is portable across different GPU architectures.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可移植性：HIP可以在不同的GPU架构之间移植。
- en: 'Open Standards: HIP is based on open standards, making it more accessible.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开放标准：HIP基于开放标准，使其更易于访问。
- en: 'Growing Community: The HIP community is growing, providing more resources and
    support.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社区增长：HIP社区正在增长，提供更多资源和支持。
- en: 'Cons:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Exclusive for GPUs
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专为GPU设计
- en: 'Vendor Lock-in: CUDA is exclusive to NVIDIA GPUs, limiting compatibility.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 供应商锁定：CUDA仅限于NVIDIA GPU，限制了兼容性。
- en: 'Learning Curve: Both CUDA and HIP require learning GPU programming concepts.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习曲线：CUDA和HIP都需要学习GPU编程概念。
- en: 'Limited Hardware Support: HIP may face limitations on older or less common
    GPUs.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件支持有限：HIP可能在较老或不太常见的GPU上面临限制。
- en: Keypoints
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: CUDA and HIP are two GPU programming models
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是两种GPU编程模型
- en: Memory optimizations are very important
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存优化非常重要
- en: Asynchronous launching can be used to overlap operations and avoid idle GPU
    [Previous](../6-directive-based-models/ "Directive-based models") [Next](../8-portable-kernel-models/
    "Portable kernel-based models")
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步启动可用于重叠操作并避免GPU空闲[上一页](../6-directive-based-models/ "基于指令的模型") [下一页](../8-portable-kernel-models/
    "基于可移植内核的模型")
- en: '* * *'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: © Copyright 2023-2024, The contributors.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: © 版权所有 2023-2024，贡献者。
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Apache Sphinx](https://www.sphinx-doc.org/)构建，并使用[主题](https://github.com/readthedocs/sphinx_rtd_theme)由[Read
    the Docs](https://readthedocs.org)提供。问题
- en: How to program GPUs with CUDA and HIP?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用CUDA和HIP编程GPU？
- en: What optimizations are possible when programming with CUDA and HIP?
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA和HIP编程时可能进行哪些优化？
- en: Objectives
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Be able to use CUDA and HIP to write basic codes
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用CUDA和HIP编写基本代码
- en: Understand how the execution is done and how to do optimizations
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解执行过程以及如何进行优化
- en: Instructor note
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 教师备注
- en: 45 min teaching
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 45分钟教学
- en: 30 min exercises
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 30分钟练习
- en: Fundamentals of GPU programming with CUDA and HIP
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CUDA和HIP的GPU编程基础
- en: Unlike some cross-platform portability ecosystems, such as alpaka, Kokkos, OpenCL,
    RAJA, and SYCL, which cater to multiple architectures, CUDA and HIP are solely
    focused on GPUs. They provide extensive libraries, APIs, and compiler toolchains
    that optimize code execution on NVIDIA GPUs (in the case of CUDA) and both NVIDIA
    and AMD GPUs (in the case of HIP). Because they are developed by the device producers,
    these programming models provide high-performance computing capabilities and offer
    advanced features like shared memory, thread synchronization, and memory management
    specific to GPU architectures.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与一些面向多个架构的跨平台可移植性生态系统，如alpaka、Kokkos、OpenCL、RAJA和SYCL不同，CUDA和HIP仅专注于GPU。它们提供了广泛的库、API和编译器工具链，这些工具链优化了代码在NVIDIA
    GPU（在CUDA的情况下）和NVIDIA及AMD GPU（在HIP的情况下）上的执行。由于它们是由设备制造商开发的，这些编程模型提供了高性能计算能力，并提供了针对GPU架构的共享内存、线程同步和内存管理等高级功能。
- en: CUDA, developed by NVIDIA, has gained significant popularity and is widely used
    for GPU programming. It offers a comprehensive ecosystem that includes not only
    the CUDA programming model but also a vast collection of GPU-accelerated libraries.
    Developers can write CUDA kernels using C++ and seamlessly integrate them into
    their applications to harness the massive parallelism of GPUs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由NVIDIA开发的CUDA已经获得了显著的流行度，并被广泛用于GPU编程。它提供了一个全面的生态系统，不仅包括CUDA编程模型，还包括大量的GPU加速库。开发者可以使用C++编写CUDA内核，并将其无缝集成到他们的应用程序中，以利用GPU的巨大并行性。
- en: HIP, on the other hand, is an open-source project that aims to provide a more
    “portable” GPU programming interface. It allows developers to write GPU code in
    a syntax similar to CUDA and provides a translation layer that enables the same
    code to run on both NVIDIA and AMD GPUs. This approach minimizes the effort required
    to port CUDA code to different GPU architectures and provides flexibility for
    developers to target multiple platforms.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，HIP是一个开源项目，旨在提供一个更“可移植”的GPU编程接口。它允许开发者使用类似于CUDA的语法编写GPU代码，并提供一个翻译层，使得相同的代码可以在NVIDIA和AMD
    GPU上运行。这种方法最小化了将CUDA代码移植到不同GPU架构所需的努力，并为开发者提供了针对多个平台的灵活性。
- en: By being closely tied to the GPU hardware, CUDA and HIP provide a level of performance
    optimization that may not be achievable with cross-platform portability ecosystems.
    The libraries and toolchains offered by these programming models are specifically
    designed to exploit the capabilities of the underlying GPU architectures, enabling
    developers to achieve high performance.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CUDA和HIP与GPU硬件紧密相连，它们提供了可能无法通过跨平台可移植性生态系统实现的性能优化级别。这些编程模型提供的库和工具链专门设计用于利用底层GPU架构的能力，使得开发者能够实现高性能。
- en: Developers utilizing CUDA or HIP can tap into an extensive ecosystem of GPU-accelerated
    libraries, covering various domains, including linear algebra, signal processing,
    image processing, machine learning, and more. These libraries are highly optimized
    to take advantage of the parallelism and computational power offered by GPUs,
    allowing developers to accelerate their applications without having to implement
    complex algorithms from scratch.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 利用CUDA或HIP的开发者可以访问一个广泛的GPU加速库生态系统，涵盖包括线性代数、信号处理、图像处理、机器学习等多个领域。这些库高度优化，以充分利用GPU提供的并行性和计算能力，使得开发者能够在不从头实现复杂算法的情况下加速他们的应用程序。
- en: As mentioned before, CUDA and HIP are very similar so it makes sense to cover
    both at the same time.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CUDA和HIP非常相似，因此同时介绍两者是有意义的。
- en: Comparison to portable kernel-based models
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于内核的可移植模型的比较
- en: In code examples below, we will also show examples in the portable kernel-based
    frameworks Kokkos, SYCL and OpenCL, which will be covered in the next episode.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们还将展示在可移植的基于内核的框架Kokkos、SYCL和OpenCL中的示例，这些将在下一集中介绍。
- en: Hello World
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hello World
- en: 'Below we have the most basic example of CUDA and HIP, the “Hello World” program:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，我们有CUDA和HIP最基础的示例，即“Hello World”程序：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In both versions, we include the necessary headers: **cuda_runtime.h** and
    **cuda.h** for CUDA, and **hip_runtime.h** for HIP. These headers provide the
    required functionality for GPU programming.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个版本中，我们都包含了必要的头文件：**cuda_runtime.h**和**cuda.h**用于CUDA，以及**hip_runtime.h**用于HIP。这些头文件提供了GPU编程所需的功能。
- en: To retrieve information about the available devices, we use the functions **<cuda/hip>GetDeviceCount**
    and **<cuda/hip>GetDevice**. These functions allow us to determine the total number
    of GPUs and the index of the currently used device. In the code examples, we default
    to using device 0.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索有关可用设备的信息，我们使用函数**<cuda/hip>GetDeviceCount**和**<cuda/hip>GetDevice**。这些函数允许我们确定GPU的总数和当前使用设备的索引。在代码示例中，我们默认使用设备0。
- en: 'As an exercise, modify the “Hello World” code to explicitly use a specific
    GPU. Do this by using the **<cuda/hip>SetDevice** function, which allows to set
    the desired GPU device. Note that the device number provided has to be within
    the range of available devices, otherwise, the program may fail to run or produce
    unexpected results. To experiment with different GPUs, modify the code to include
    the following line before retrieving device information:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，修改“Hello World”代码以显式使用特定的GPU。通过使用**<cuda/hip>SetDevice**函数来实现，该函数允许设置所需的GPU设备。请注意，提供的设备号必须在可用设备范围内，否则程序可能无法运行或产生意外的结果。为了实验不同的GPU，在检索设备信息之前修改代码以包含以下行：
- en: '[PRE31]'
  id: totrans-191
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Replace **deviceNumber** with the desired GPU device index. Run the code with
    different device numbers to observe the output (more examples for the “Hello World”
    program are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    subdirectory of this lesson repository).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将**deviceNumber**替换为所需的GPU设备索引。使用不同的设备号运行代码以观察输出（本课程仓库的[content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)子目录中提供了“Hello
    World”程序的更多示例）。
- en: Vector Addition
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量加法
- en: To demonstrate the fundamental features of CUDA/HIP programming, let’s begin
    with a straightforward task of element-wise vector addition. The code snippet
    below demonstrates how to utilize CUDA and HIP for efficiently executing this
    operation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示CUDA/HIP编程的基本特性，让我们从一项简单的任务——逐元素向量加法——开始。下面的代码片段展示了如何利用CUDA和HIP高效地执行此操作。
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In this case, the CUDA and HIP codes are equivalent one to one so we will only
    refer to the CUDA version. The CUDA and HIP programming model are host centric
    programming models. The main program is executed on CPU and controls all the operations,
    memory allocations, data transfers between CPU and GPU, and launches the kernels
    to be executed on the GPU. The code starts with defining the GPU kernel function
    called **vector_add** with attribute **___global__**. It takes three input arrays
    A, B, and C along with the array size n. The kernel function contains the actually
    code which is executed on the GPU by multiple threads in parallel.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，CUDA和HIP代码是逐个对应的，因此我们只参考CUDA版本。CUDA和HIP编程模型是主机中心编程模型。主程序在CPU上执行并控制所有操作、内存分配、CPU和GPU之间的数据传输，并启动在GPU上执行的内核。代码从定义具有**___global__**属性的GPU内核函数**vector_add**开始。它包含三个输入数组A、B和C以及数组大小n。内核函数包含实际上在GPU上由多个线程并行执行的代码。
- en: Accelerators in general and GPUs in particular usually have their own dedicated
    memory separate from the system memory (AMD MI300A is one exception, using the
    same memory for both CPU and GPU). When programming for GPUs, there are two sets
    of pointers involved and it’s necessary to manage data movement between the host
    memory and the accelerator memory. Data needs to be explicitly copied from the
    host memory to the accelerator memory before it can be processed by the accelerator.
    Similarly, results or modified data may need to be copied back from the accelerator
    memory to the host memory to make them accessible to the CPU.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，加速器（尤其是GPU）都有自己的专用内存，与系统内存分开（AMD MI300A是一个例外，它使用相同的内存为CPU和GPU）。当为GPU编程时，涉及两组指针，并且需要在主机内存和加速器内存之间管理数据移动。在加速器处理之前，数据需要显式地从主机内存复制到加速器内存。同样，结果或修改后的数据可能需要从加速器内存复制回主机内存，以便使它们对CPU可访问。
- en: The main function of the code initializes the input arrays Ah, Bh on the CPU
    and computes the reference array Cref. It then allocates memory on the GPU for
    the input and output arrays Ad, Bd, and Cd using **cudaMalloc**. Herein, h is
    for the ‘host’ (CPU) and d for the ‘device’ (GPU). The data is transferred from
    the CPU to the GPU using hipMemcpy, and then the GPU kernel is launched using
    the <<<.>>> syntax. All kernels launch are asynchronous. After launch the control
    returns to the main() and the code proceeds to the next instructions.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的主函数在 CPU 上初始化输入数组 Ah 和 Bh，并计算参考数组 Cref。然后使用 **cudaMalloc** 在 GPU 上为输入和输出数组
    Ad、Bd 和 Cd 分配内存。在这里，h 代表 ‘host’（CPU），d 代表 ‘device’（GPU）。使用 hipMemcpy 将数据从 CPU
    传输到 GPU，然后使用 <<<...>>> 语法启动 GPU 内核。所有内核启动都是异步的。启动后，控制权返回到 main()，代码继续执行下一行指令。
- en: After the kernel execution, the result array Cd is copied back to the CPU using
    **cudaMemcpy**. The code then prints the reference and result arrays, calculates
    the error by comparing the reference and result arrays. Finally, the GPU and CPU
    memory are deallocated using **cudaFree** and **free** functions, respectively.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核执行之后，结果数组 Cd 通过 **cudaMemcpy** 被复制回 CPU。然后代码打印出参考数组和结果数组，通过比较参考数组和结果数组来计算误差。最后，使用
    **cudaFree** 和 **free** 函数分别释放 GPU 和 CPU 内存。
- en: The host functions **cudaSetDevice**, **cudaMalloc**, **cudaMemcpy**, and **cudaFree**
    are blocking, i.e. the code does not continues to next instructions until the
    operations are completed. However this is not the default behaviour, for many
    operations there are asynchronous equivalents and there are as well many library
    calls return the control to the main() after calling. This allows the developers
    to launch independent operations and overlap them.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数中的 **cudaSetDevice**、**cudaMalloc**、**cudaMemcpy** 和 **cudaFree** 是阻塞的，即代码不会继续执行下一行指令，直到操作完成。然而，这并不是默认行为，对于许多操作，都有异步等效操作，以及许多库调用在调用后返回控制权到
    main()。这允许开发者启动独立操作并将它们重叠。
- en: In short, this code demonstrates how to utilize the CUDA and HIP to perform
    vector addition on a GPU, showcasing the steps involved in allocating memory,
    transferring data between the CPU and GPU, launching a kernel function, and handling
    the results. It serves as a starting point for GPU-accelerated computations using
    CUDA and HIP. More examples for the vector (array) addition program are available
    at [content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，此代码演示了如何利用 CUDA 和 HIP 在 GPU 上执行向量加法，展示了分配内存、在 CPU 和 GPU 之间传输数据、启动内核函数和处理结果的步骤。它作为使用
    CUDA 和 HIP 进行 GPU 加速计算的起点。有关向量（数组）加法程序的更多示例，请参阅 [content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples)。
- en: In order to practice the concepts shown above, edit the skeleton code in the
    repository and the code corresponding to setting the device, memory allocations
    and transfers, and the kernel execution.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习上述概念，请编辑存储库中的骨架代码以及与设置设备、内存分配和传输以及内核执行相对应的代码。
- en: Vector Addition with Unified Memory
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用统一内存的向量加法
- en: 'For a while already GPUs support unified memory, which allows to use the same
    pointer for both CPU and GPU data. This simplifies developing codes by removing
    the explicit data transfers. The data resides on CPU until it is needed on GPU
    or vice-versa. However the data transfers still happens “under the hood” and the
    developer needs to construct the code to avoid unnecessary transfers. Below one
    can see the modified vector addition codes:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有一段时间了，GPU 支持统一内存，这允许使用相同的指针来访问 CPU 和 GPU 数据。这通过消除显式数据传输简化了代码开发。数据驻留在 CPU
    上，直到需要在 GPU 上使用，反之亦然。然而，数据传输仍然“在幕后”发生，并且开发者需要构建代码以避免不必要的传输。以下是可以看到修改后的向量加法代码：
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now the arrays Ah, Bh, Ch, and Cref are using cudaMallocManaged to allocate
    Unified Memory. The **vector_add kernel** is launched by passing these Unified
    Memory pointers directly. After the kernel launch, **cudaDeviceSynchronize** is
    used to wait for the kernel to complete execution. Finally, **cudaFree** is used
    to free the Unified Memory arrays. The Unified Memory allows for transparent data
    migration between CPU and GPU, eliminating the need for explicit data transfers.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数组 Ah、Bh、Ch 和 Cref 使用 cudaMallocManaged 来分配统一内存。**vector_add 内核**通过传递这些统一内存指针直接启动。内核启动后，使用
    **cudaDeviceSynchronize** 等待内核完成执行。最后，使用 **cudaFree** 释放统一内存数组。统一内存允许在 CPU 和 GPU
    之间透明地迁移数据，消除了显式数据传输的需要。
- en: As an exercise modify the skeleton code for vector addition to use Unified Memory.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，修改向量加法的骨架代码以使用统一内存。
- en: Basics - In short
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基础 - 简而言之
- en: CUDA is developed by NVIDIA, while HIP is an open-source project (from AMD)
    for multi-platform GPU programming.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA由NVIDIA开发，而HIP是AMD的一个开源项目（用于多平台GPU编程）。
- en: CUDA and HIP are GPU-focused programming models for optimized code execution
    on NVIDIA and AMD GPUs.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是针对NVIDIA和AMD GPU上优化代码执行的GPU-focused编程模型。
- en: CUDA and HIP are similar, allowing developers to write GPU code in a syntax
    similar to CUDA and target multiple platforms.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP相似，允许开发者使用类似于CUDA的语法编写GPU代码，并针对多个平台进行目标定位。
- en: CUDA and HIP are programming models focused solely on GPUs
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是专注于GPU的编程模型
- en: CUDA and HIP offer high-performance computing capabilities and advanced features
    specific to GPU architectures, such as shared memory and memory management.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP提供了针对GPU架构的高性能计算能力和高级功能，例如共享内存和内存管理。
- en: They provide highly GPU-accelerated libraries in various domains like linear
    algebra, signal processing, image processing, and machine learning.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们提供了高度GPU加速的库，涵盖各个领域，如线性代数、信号处理、图像处理和机器学习。
- en: Programming for GPUs involves managing data movement between host and accelerator
    memory.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为GPU编程涉及在主机和加速器内存之间管理数据移动。
- en: Unified Memory simplifies data transfers by using the same pointer for CPU and
    GPU data, but code optimization is still necessary.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一内存通过使用相同的指针来简化CPU和GPU数据之间的数据传输，但代码优化仍然是必要的。
- en: Memory Optimizations
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存优化
- en: Vector addition is a relatively simple, straight forward case. Each thread reads
    data from memory, does an addition and then saves the result. Two adjacent threads
    access memory location in memory close to each other. Also the data is used only
    once. In practice this not the case. Also sometimes the same data is used several
    times resulting in additional memory accesses.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 向量加法是一个相对简单、直接的情况。每个线程从内存中读取数据，进行加法运算，然后将结果保存。两个相邻的线程访问内存中彼此靠近的位置。此外，数据只使用一次。实际上并非如此。有时相同的数據被多次使用，导致额外的内存访问。
- en: Memory optimization is one of the most important type of optimization done to
    efficiently use the GPUs. Before looking how it is done in practice let’s revisit
    some basic concepts about GPUs and execution model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 内存优化是用于高效使用GPU的最重要类型的优化之一。在查看实际操作之前，让我们回顾一下关于GPU和执行模型的一些基本概念。
- en: GPUs are comprised many light cores, the so-called Streaming Processors (SP)
    in CUDA, which are physically group together in units, i.e. Streaming Multi-Processors
    (SMP) in CUDA architecture (note that in AMD the equivalent is called Computing
    Units, while in Intel GPUs they are Execution Units). The work is done on GPUs
    by launching many threads each executing an instance of the same kernel. The order
    of execution is not defined, and the threads can only exchange information in
    specific conditions. Because of the way the SPs are grouped the threads are also
    grouped in **blocks**. Each **block** is assigned to an SMP, and can not be split.
    An SMP can have more than block residing at a moment, however there is no communications
    between the threads in different blocks. In addition to the SPs, each SMP contains
    very fast memory which in CUDA is referred to as shared memory. The threads in
    a block can read and write to the shared memory and use it as a user controlled
    cache. One thread can for example write to a location in the shared memory while
    another thread in the same block can read and use that data. In order to be sure
    that all threads in the block completed writing **__syncthreads()** function has
    to be used to make the threads in the block wait until all of them reached the
    specific place in the kernel. Another important aspect in the GPU programming
    model is that the threads in the block are not executed independently. The threads
    in a block are physically grouped in warps of size 32 in NVIDIA devices or wavefronts
    of size 32 or 64 in AMD devices (depending on device architecture). Intel devices
    are notable in that the warp size, called SIMD width, is highly configurable,
    with typical possible values of 8, 16, or 32 (depends on the hardware). All memory
    accesses of the global GPU memory are done per warp. When data is needed for some
    calculations a warp loads from the GPU memory blocks of specific size (64 or 128
    Bytes). These operation is very expensive, it has a latency of hundreds of cycles.
    This means that the threads in a warp should work with elements of the data located
    close in the memory. In the vector addition two threads near each other, of index
    tid and tid+1, access elements adjacent in the GPU memory.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: GPU由许多轻量级核心组成，所谓的CUDA中的流处理器（SP），它们在物理上以单元的形式组合在一起，即CUDA架构中的流多处理器（SMP）（注意在AMD中，等效的部分称为计算单元，而在Intel
    GPU中它们称为执行单元）。GPU上的工作是通过启动许多线程来完成的，每个线程执行相同内核的一个实例。执行顺序未定义，线程只能在特定条件下交换信息。由于SPs的分组方式，线程也被分组在**块**中。每个**块**被分配给一个SMP，并且不能分割。一个SMP可以同时容纳多个块，然而不同块之间的线程没有通信。除了SPs之外，每个SMP还包含非常快速的内存，在CUDA中被称为共享内存。块中的线程可以读写共享内存，并将其用作用户控制的缓存。例如，一个线程可以写入共享内存中的某个位置，而同一块中的另一个线程可以读取并使用这些数据。为了确保块中的所有线程都完成了写入，必须使用**__syncthreads()**函数来使块中的线程等待直到它们都到达内核中的特定位置。GPU编程模型中的另一个重要方面是块中的线程不是独立执行的。在NVIDIA设备上，块中的线程以大小为32的warps进行物理分组，在AMD设备上以大小为32或64的wavefronts进行分组（取决于设备架构）。Intel设备值得注意的是，warp的大小，称为SIMD宽度，具有高度可配置性，典型的可能值为8、16或32（取决于硬件）。所有全局GPU内存的内存访问都是按warp进行的。当需要某些计算的数据时，warp从GPU内存中加载特定大小的块（64或128字节）。这些操作非常昂贵，具有数百个周期的延迟。这意味着warp中的线程应该与内存中位置接近的数据元素一起工作。在向量加法中，两个相邻的线程，索引为tid和tid+1，访问GPU内存中相邻的元素。
- en: The shared memory can be used to improve performance in two ways. It is possible
    to avoid extra reads from the memory when several threads in the same block need
    the same data (see [stencil](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/stencil)
    code) or it can be used to improve the memory access patterns like in the case
    of matrix transpose.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存可以通过两种方式来提高性能。当同一块中的多个线程需要相同的数据时，可以避免从内存中读取额外的数据（参见[stencil](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/stencil)代码）或者它可以用来改进内存访问模式，例如矩阵转置的情况。
- en: Memory, Execution - In short
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 内存、执行 - 简而言之
- en: GPUs consist of streaming processors (SPs) grouped together in units, such as
    Streaming Multi-Processors (SMPs) in CUDA architecture.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU由分组在一起的流处理器（SPs）组成，例如在CUDA架构中的流多处理器（SMPs）。
- en: Work on GPUs is done by launching threads, with each thread executing an instance
    of the same kernel, and the execution order is not defined.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU上的工作是通过启动线程来完成的，每个线程执行相同内核的一个实例，执行顺序未定义。
- en: Threads are organized into blocks, assigned to an SMP, and cannot be split,
    and there is no communication between threads in different blocks.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程被组织成块，分配给SMP，并且不能被分割，不同块之间的线程没有通信。
- en: Each SMP contains shared memory, which acts as a user-controlled cache for threads
    within a block, allowing efficient data sharing and synchronization.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个SMP包含共享内存，它作为用户控制的缓存，用于块内的线程，从而实现高效的数据共享和同步。
- en: The shared memory can be used to avoid extra memory reads when multiple threads
    in the same block need the same data or to improve memory access patterns, such
    as in matrix transpose operations.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以用来避免在同一个块中的多个线程需要相同数据时的额外内存读取，或者改进内存访问模式，例如在矩阵转置操作中。
- en: Memory accesses from global GPU memory are performed per warp (groups of threads),
    and loading data from GPU memory has high latency.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从全局GPU内存进行的内存访问是按warp（线程组）进行的，从GPU内存加载数据具有高延迟。
- en: To optimize memory access, threads within a warp should work with adjacent elements
    in memory to reduce latency.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了优化内存访问，warp内的线程应该与内存中的相邻元素一起工作以减少延迟。
- en: Proper utilization of shared memory can improve performance by reducing memory
    reads and enhancing memory access patterns.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确利用共享内存可以通过减少内存读取和增强内存访问模式来提高性能。
- en: Matrix Transpose
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵转置
- en: Matrix transpose is a classic example where shared memory can significantly
    improve the performance. The use of shared memory reduces global memory accesses
    and exploits the high bandwidth and low latency of shared memory.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵转置是共享内存可以显著提高性能的经典示例。使用共享内存减少了全局内存访问，并利用了共享内存的高带宽和低延迟。
- en: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
- en: First as a reference we use a simple kernel which copy the data from one array
    to the other.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，作为一个参考，我们使用一个简单的内核，它将数据从一个数组复制到另一个数组。
- en: '[PRE39]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We note that this code does not do any calculations. Each thread reads one element
    and then writes it to another locations. By measuring the execution time of the
    kernel we can compute the effective bandwidth achieve by this kernel. We can measure
    the time using **rocprof** or **cuda/hip events**. On a NVIDIA V100 GPU this code
    achieves 717 GB/s out of the theoretical peak 900 GB/s.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，此代码没有进行任何计算。每个线程读取一个元素，然后将其写入另一个位置。通过测量内核的执行时间，我们可以计算此内核的有效带宽。我们可以使用**rocprof**或**cuda/hip
    events**来测量时间。在NVIDIA V100 GPU上，此代码实现了717 GB/s，理论峰值带宽为900 GB/s。
- en: Now we do the first iteration of the code, a naive transpose. The reads have
    a nice coalesced access pattern, but the writing is now very inefficient.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们执行代码的第一迭代，一个简单的转置。读取具有很好的合并访问模式，但写入现在非常低效。
- en: '[PRE42]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Checking the index in_index we see that two adjacent threads (threadIx.x, threadIdx.x+1)
    access location in memory near each other. However the writes are not. Threads
    access data which in a strided way. Two adjacent threads access data separated
    by height elements. This practically results in 32 memory operations, however
    due to under the hood optimizations the achieved bandwidth is 311 GB/s.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 检查_in_index_索引，我们看到两个相邻的线程（threadIx.x, threadIdx.x+1）访问彼此附近的内存位置。然而，写入操作不是这样的。线程以步进方式访问数据。两个相邻的线程通过高度元素访问数据。这实际上导致了32次内存操作，但由于底层的优化，实现的带宽为311
    GB/s。
- en: We can improve the code by reading the data in a coalesced way, save it in the
    shared memory row by row and then write in the global memory column by column.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以合并的方式读取数据来改进代码，逐行将其保存到共享内存中，然后按列写入全局内存。
- en: '[PRE44]'
  id: totrans-249
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ''
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We define a **tile_dim** constant to determine the size of the shared memory
    tile. The matrix transpose kernel uses a 2D grid of thread blocks, where each
    thread block operates on a tile_dim x tile_dim tile of the input matrix.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个**tile_dim**常量来确定共享内存块的大小。矩阵转置内核使用二维线程块网格，其中每个线程块操作输入矩阵的tile_dim x tile_dim块。
- en: The kernel first loads data from the global memory into the shared memory tile.
    Each thread loads a single element from the input matrix into the shared memory
    tile. Then, a **__syncthreads()** barrier ensures that all threads have finished
    loading data into shared memory before proceeding.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 内核首先将全局内存中的数据加载到共享内存块中。每个线程将输入矩阵的单个元素加载到共享内存块中。然后，一个**__syncthreads()**屏障确保所有线程在继续之前已经将数据加载到共享内存中。
- en: Next, the kernel writes the transposed data from the shared memory tile back
    to the output matrix in global memory. Each thread writes a single element from
    the shared memory tile to the output matrix. By using shared memory, this optimized
    implementation reduces global memory accesses and exploits memory coalescence,
    resulting in improved performance compared to a naive transpose implementation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，内核将共享内存瓷砖中的转置数据写回到全局内存中的输出矩阵。每个线程将共享内存瓷砖中的一个元素写入输出矩阵。通过使用共享内存，这个优化的实现减少了全局内存访问并利用了内存合并，与简单的转置实现相比，性能得到了提升。
- en: This kernel achieved on NVIDIA V100 674 GB/s.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核在NVIDIA V100上达到了674 GB/s。
- en: This is pretty close to the bandwidth achieved by the simple copy kernel, but
    there is one more thing to improve.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎接近简单复制内核所达到的带宽，但还有一点可以改进。
- en: Shared memory is composed of banks. Each banks can service only one request
    at the time. Bank conflicts happen when more than 1 thread in a specific warp
    try to access data in bank. The bank conflicts are resolved by serializing the
    accesses resulting in less performance. In the above example when data is saved
    to the shared memory, each thread in the warp will save an element of the data
    in a different one. Assuming that shared memory has 16 banks after writing each
    bank will contain one column. At the last step when we write from the shared memory
    to the global memory each warp load data from the same bank. A simple way to avoid
    this is by just padding the temporary array.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存由银行组成。每个银行一次只能服务一个请求。当特定战中的一组线程尝试访问银行中的数据时，会发生银行冲突。通过序列化访问来解决银行冲突，导致性能降低。在上面的例子中，当数据保存到共享内存时，战中的每个线程将数据的一个元素保存到不同的银行。假设共享内存有16个银行，在写入后，每个银行将包含一列。在最后一步，当我们从共享内存写入全局内存时，每个战从相同的银行加载数据。避免这种情况的一个简单方法就是通过填充临时数组。
- en: '[PRE46]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: By padding the array the data is slightly shifting it resulting in no bank conflicts.
    The effective bandwidth for this kernel is 697 GB/s.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过填充数组，数据略有偏移，从而避免了银行冲突。这个内核的有效带宽为697 GB/s。
- en: Using sharing memory as a cache - In short
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 将共享内存用作缓存 - 简而言之
- en: Shared memory can significantly improve performance in operations like matrix
    transpose.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以显著提高矩阵转置等操作的性能。
- en: Shared memory reduces global memory accesses and exploits the high bandwidth
    and low latency of shared memory.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存减少了全局内存访问，并利用了共享内存的高带宽和低延迟。
- en: An optimized implementation utilizes shared memory, loads data coalescedly,
    and performs transpose operations.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的实现利用共享内存，合并加载数据，并执行转置操作。
- en: The optimized implementation uses a 2D grid of thread blocks and a shared memory
    tile size determined by a constant.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的实现使用一个二维的线程块网格和一个由常量确定的共享内存瓷砖大小。
- en: The kernel loads data from global memory into the shared memory tile and uses
    a synchronization barrier.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核将从全局内存加载数据到共享内存的瓷砖中，并使用同步屏障。
- en: To avoid bank conflicts in shared memory, padding the temporary array is a simple
    solution.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免共享内存中的银行冲突，对临时数组进行填充是一个简单的解决方案。
- en: Reductions
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少操作
- en: Reductions refer to operations in which the elements of an array are aggregated
    in a single value through operations such as summing, finding the maximum or minimum,
    or performing logical operations.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 减少操作指的是通过求和、查找最大或最小值或执行逻辑操作等方式，将数组元素聚合为单个值的操作。
- en: In the serial approach, the reduction is performed sequentially by iterating
    through the collection of values and accumulating the result step by step. This
    will be enough for small sizes, but for big problems this results in significant
    time spent in this part of an application. On a GPU, this approach is not feasible.
    Using just one thread to do this operation means the rest of the GPU is wasted.
    Doing reduction in parallel is a little tricky. In order for a thread to do work,
    it needs to have some partial result to use. If we launch, for example, a kernel
    performing a simple vector summation, `sum[0]+=a[tid]`, with N threads we notice
    that this would result in undefined behaviour. GPUs have mechanisms to access
    the memory and lock the access for other threads while 1 thread is doing some
    operations to a given data via **atomics**, however this means that the memory
    access gets again to be serialized. There is not much gain. We note that when
    doing reductions the order of the iterations is not important (barring the typical
    non-associative behavior of floating-point operations). Also we can we might have
    to divide our problem in several subsets and do the reduction operation for each
    subset separately. On the GPUs, since the GPU threads are grouped in blocks, the
    size of the subset based on that. Inside the block, threads can cooperate with
    each other, they can share data via the shared memory and can be synchronized
    as well. All threads read the data to be reduced, but now we have significantly
    less partial results to deal with. In general, the size of the block ranges from
    256 to 1024 threads. In case of very large problems, after this procedure if we
    are left too many partial results this step can be repeated.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在串行方法中，归约是通过迭代值集合并逐步累积结果来顺序执行的。这对于小规模问题来说足够了，但对于大问题，这会导致在这个应用程序的这一部分花费大量时间。在GPU上，这种方法是不可行的。仅使用一个线程来完成这个操作意味着GPU的其余部分都被浪费了。并行执行归约有点棘手。为了线程能够工作，它需要有一些部分结果来使用。例如，如果我们启动一个执行简单向量求和的内核，`sum[0]+=a[tid]`，使用N个线程，我们会注意到这将导致未定义的行为。GPU有机制通过**原子操作**访问内存并锁定其他线程对给定数据的访问，然而这又意味着内存访问再次被序列化。这并没有带来太多好处。我们注意到，在执行归约时，迭代顺序并不重要（除了浮点运算的典型非结合行为）。此外，我们可能需要将问题分成几个子集，并对每个子集分别执行归约操作。在GPU上，由于GPU线程被分组在块中，子集的大小基于这一点。在块内部，线程可以相互合作，可以通过共享内存共享数据，也可以进行同步。所有线程读取要归约的数据，但现在我们处理的部分结果显著减少。一般来说，块的大小从256到1024个线程不等。在非常大的问题中，如果在这个过程之后我们留下了太多的部分结果，这一步可以重复进行。
- en: 'At the block level we still have to perform a reduction in an efficient way.
    Doing it serially means that we are not using all GPU cores (roughly 97% of the
    computing capacity is wasted). Doing it naively parallel using **atomics**, but
    on the shared memory is also not a good option. Going back back to the fact the
    reduction operations are commutative and associative we can set each thread to
    “reduce” two elements of the local part of the array. Shared memory can be used
    to store the partial “reductions” as shown below in the code:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在块级别，我们仍然需要以高效的方式执行归约。串行执行意味着我们没有使用所有GPU核心（大约97%的计算能力被浪费了）。使用**原子操作**进行简单的并行归约，但在共享内存上也不是一个好的选择。回到归约操作是交换律和结合律的事实，我们可以让每个线程“归约”数组局部部分的两个元素。共享内存可以用来存储如以下代码所示的部分“归约”：
- en: '[PRE48]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In the kernel we have each GPU performing thread a reduction of two elements
    from the local portion of the array. If we have tpb GPU threads per block, we
    utilize them to store 2xtpb elements in the local shared memory. To ensure synchronization
    until all data is available in the shared memory, we employ the syncthreads()
    function.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，我们让每个GPU执行线程对数组局部部分进行两个元素的归约。如果我们有每个块的tpb个GPU线程，我们利用它们在局部共享内存中存储2xtpb个元素。为了确保同步直到所有数据都可用在共享内存中，我们使用syncthreads()函数。
- en: Next, we instruct each thread to “reduce” the element in the array at threadIdx.x
    with the element at threadIdx.x+tpb. As this operation saves the result back into
    the shared memory, we once again employ syncthreads(). By doing this, we effectively
    halve the number of elements to be reduced.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指示每个线程“归约”数组中threadIdx.x处的元素与threadIdx.x+tpb处的元素。由于这个操作将结果保存回共享内存，我们再次使用syncthreads()。通过这样做，我们有效地将需要归约的元素数量减半。
- en: This procedure can be repeated, but now we only utilize tpb/2 threads. Each
    thread is responsible for “reducing” the element in the array at threadIdx.x with
    the element at threadIdx.x+tpb/2. After this step, we are left with tpb/4 numbers
    to be reduced. We continue applying this procedure until only one number remains.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以重复进行，但现在我们只使用tpb/2个线程。每个线程负责“减少”数组中threadIdx.x位置的元素与threadIdx.x+tpb/2位置的元素。完成这一步后，我们剩下tpb/4个数字需要减少。我们继续应用这个方法，直到只剩下一个数字。
- en: At this point, we can either “reduce” the final number with a global partial
    result using atomic read and write operations, or we can save it into an array
    for further processing.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们既可以使用全局部分结果和原子读写操作来“减少”最终数字，也可以将其保存到数组中以便进一步处理。
- en: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
- en: Schematic representation on the reduction algorithm with 8 GPU threads.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用8个GPU线程的减少算法的示意图。
- en: For a detail analysis of how to optimize reduction operations in CUDA/HIP check
    this presentation [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细了解如何在CUDA/HIP中优化减少操作，请查看这个演示文稿[优化CUDA中的并行减少](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)。
- en: Reductions - In short
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 减少 - 简而言之
- en: Reductions refer to aggregating elements of an array into a single value through
    operations like summing, finding maximum or minimum, or performing logical operations.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少（Reduction）是指通过求和、查找最大值或最小值或执行逻辑运算等操作将数组的元素聚合为单个值。
- en: Performing reductions sequentially in a serial approach is inefficient for large
    problems, while parallel reduction on GPUs offers better performance.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大型问题，按顺序在串行方式中执行减少操作效率低下，而GPU上的并行减少提供了更好的性能。
- en: Parallel reduction on GPUs involves dividing the problem into subsets, performing
    reductions within blocks of threads using shared memory, and repeatedly reducing
    the number of elements (two per GPU thread) until only one remains.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上执行并行减少操作涉及将问题划分为子集，在线程块中使用共享内存执行减少操作，并重复减少元素的数量（每个GPU线程两个），直到只剩下一个元素。
- en: Overlapping Computations and Memory transfer. CUDA/HIP Streams
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算和内存传输的重叠。CUDA/HIP流。
- en: Modern GPUs can overlap independent operations. They can do transfers between
    CPU and GPU and execute kernels in the same time, or they can execute kernels
    concurrently. CUDA/HIP streams are independent execution units, a sequence of
    operations that execute in issue-order on the GPU. The operations issue in different
    streams can be executed concurrently.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU可以重叠独立操作。它们可以在CPU和GPU之间进行传输，同时执行内核，或者它们可以并发执行内核。CUDA/HIP流是独立的执行单元，是一系列在GPU上按发布顺序执行的运算。不同流中的操作可以并发执行。
- en: Consider the previous case of vector addition, which involves copying data from
    CPU to GPU, computations and then copying back the result to GPU. In this way
    nothing can be overlap.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑之前的向量加法案例，它涉及从CPU到GPU的数据拷贝，计算，然后将结果拷贝回GPU。这种方式无法重叠。
- en: We can improve the performance by dividing the problem in smaller independent
    parts. Let’s consider 5 streams and consider the case where copy in one direction
    and computation take the same amount of time.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将问题划分为更小的独立部分来提高性能。让我们考虑5个流，并考虑单向拷贝和计算花费相同时间的案例。
- en: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
- en: After the first and second stream copy data to the GPU, the GPU is practically
    occupied all time. We can see that significant performance improvements can be
    obtained by eliminating the time in which the GPU is idle, waiting for data to
    arrive from the CPU. This very useful for problems where there is often communication
    to the CPU because the GPU memory can not fit all the problem or the application
    runs in a multi-GPU set up and communication is needed often.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个和第二个流将数据拷贝到GPU后，GPU实际上一直处于忙碌状态。我们可以看到，通过消除GPU空闲等待CPU数据到达的时间，可以获得显著的性能提升。这对于经常需要与CPU通信的问题非常有用，因为GPU内存无法容纳整个问题或应用程序在多GPU设置中运行且需要频繁通信。
- en: We can apply this to the vector addition problem above.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此应用于上述的向量加法问题。
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Instead of having one copy to gpu, one execution of the kernel and one copy
    back, we now have several of these calls independent of each other.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有多个这样的调用，它们彼此独立，而不是只有一个拷贝到GPU，一次内核执行和一次拷贝回。
- en: Note that even when streams are not explicitly used it is possible to launch
    all the GPU operations asynchronous and overlap CPU operations (such I/O) and
    GPU operations. In order to learn more about how to improve performance using
    streams check the NVIDIA blog [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使没有显式使用流，也可以异步启动所有 GPU 操作，并重叠 CPU 操作（如 I/O）和 GPU 操作。要了解更多关于如何使用流来提高性能的信息，请查看
    NVIDIA 博客 [如何在 CUDA C/C++ 中重叠数据传输](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)。
- en: Streams - In short
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 流 - 简而言之
- en: CUDA/HIP streams are independent execution contexts on the GPU that allow for
    concurrent execution of operations issued in different streams.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA/HIP 流是 GPU 上的独立执行上下文，允许在不同流中并发执行操作。
- en: Using streams can improve GPU performance by overlapping operations such as
    data transfers between CPU and GPU and kernel executions.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流可以通过重叠操作（如 CPU 和 GPU 之间的数据传输和内核执行）来提高 GPU 性能。
- en: By dividing a problem into smaller independent parts and utilizing multiple
    streams, the GPU can avoid idle time, resulting in significant performance improvements,
    especially for problems with frequent CPU communication or multi-GPU setups.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将问题分解成更小的独立部分并利用多个流，GPU 可以避免空闲时间，从而实现显著的性能提升，尤其是在频繁 CPU 通信或多 GPU 设置的问题上。
- en: Pros and cons of native programming models
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原生编程模型的优缺点
- en: 'There are advantages and limitations to CUDA and HIP:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 和 HIP 有其优势和局限性：
- en: 'CUDA Pros:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 优点：
- en: 'Performance Boost: CUDA is designed for NVIDIA GPUs and delivers excellent
    performance.'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能提升：CUDA 为 NVIDIA GPU 设计，提供卓越的性能。
- en: 'Wide Adoption: CUDA is popular, with many resources and tools available.'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广泛采用：CUDA 流行，有许多资源和工具可用。
- en: 'Mature Ecosystem: NVIDIA provides comprehensive libraries and tools for CUDA
    programming.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成熟的生态系统：NVIDIA 为 CUDA 编程提供全面的库和工具。
- en: 'HIP Pros:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: HIP 优点：
- en: 'Portability: HIP is portable across different GPU architectures.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可移植性：HIP 可跨不同 GPU 架构移植。
- en: 'Open Standards: HIP is based on open standards, making it more accessible.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开放标准：HIP 基于开放标准，使其更具可访问性。
- en: 'Growing Community: The HIP community is growing, providing more resources and
    support.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社区增长：HIP 社区正在增长，提供更多资源和支持。
- en: 'Cons:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Exclusive for GPUs
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专为 GPU 设计
- en: 'Vendor Lock-in: CUDA is exclusive to NVIDIA GPUs, limiting compatibility.'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 供应商锁定：CUDA 仅适用于 NVIDIA GPU，限制了兼容性。
- en: 'Learning Curve: Both CUDA and HIP require learning GPU programming concepts.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习曲线：CUDA 和 HIP 都需要学习 GPU 编程概念。
- en: 'Limited Hardware Support: HIP may face limitations on older or less common
    GPUs.'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件支持有限：HIP 可能会在较老或不太常见的 GPU 上面临限制。
- en: Keypoints
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: CUDA and HIP are two GPU programming models
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA 和 HIP 是两种 GPU 编程模型
- en: Memory optimizations are very important
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存优化非常重要
- en: Asynchronous launching can be used to overlap operations and avoid idle GPU
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步启动可用于重叠操作并避免 GPU 空闲
- en: Fundamentals of GPU programming with CUDA and HIP
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CUDA 和 HIP 的 GPU 编程基础
- en: Unlike some cross-platform portability ecosystems, such as alpaka, Kokkos, OpenCL,
    RAJA, and SYCL, which cater to multiple architectures, CUDA and HIP are solely
    focused on GPUs. They provide extensive libraries, APIs, and compiler toolchains
    that optimize code execution on NVIDIA GPUs (in the case of CUDA) and both NVIDIA
    and AMD GPUs (in the case of HIP). Because they are developed by the device producers,
    these programming models provide high-performance computing capabilities and offer
    advanced features like shared memory, thread synchronization, and memory management
    specific to GPU architectures.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 与一些面向多个架构的跨平台可移植性生态系统（如 alpaka、Kokkos、OpenCL、RAJA 和 SYCL）不同，CUDA 和 HIP 仅专注于
    GPU。它们提供了广泛的库、API 和编译器工具链，用于优化代码在 NVIDIA GPU（在 CUDA 的情况下）和 NVIDIA 及 AMD GPU（在
    HIP 的情况下）上的执行。由于它们是由设备制造商开发的，这些编程模型提供了高性能计算能力，并提供了针对 GPU 架构的共享内存、线程同步和内存管理等高级功能。
- en: CUDA, developed by NVIDIA, has gained significant popularity and is widely used
    for GPU programming. It offers a comprehensive ecosystem that includes not only
    the CUDA programming model but also a vast collection of GPU-accelerated libraries.
    Developers can write CUDA kernels using C++ and seamlessly integrate them into
    their applications to harness the massive parallelism of GPUs.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 由 NVIDIA 开发，在 GPU 编程中获得了显著的人气，并且被广泛使用。它提供了一个全面的生态系统，不仅包括 CUDA 编程模型，还包括大量的
    GPU 加速库。开发者可以使用 C++ 编写 CUDA 内核，并将其无缝集成到他们的应用程序中，以利用 GPU 的巨大并行性。
- en: HIP, on the other hand, is an open-source project that aims to provide a more
    “portable” GPU programming interface. It allows developers to write GPU code in
    a syntax similar to CUDA and provides a translation layer that enables the same
    code to run on both NVIDIA and AMD GPUs. This approach minimizes the effort required
    to port CUDA code to different GPU architectures and provides flexibility for
    developers to target multiple platforms.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，HIP是一个开源项目，旨在提供一个更“可移植”的GPU编程接口。它允许开发者使用类似于CUDA的语法编写GPU代码，并提供一个翻译层，使得相同的代码可以在NVIDIA和AMD
    GPU上运行。这种方法最小化了将CUDA代码移植到不同GPU架构所需的努力，并为开发者提供了针对多个平台的灵活性。
- en: By being closely tied to the GPU hardware, CUDA and HIP provide a level of performance
    optimization that may not be achievable with cross-platform portability ecosystems.
    The libraries and toolchains offered by these programming models are specifically
    designed to exploit the capabilities of the underlying GPU architectures, enabling
    developers to achieve high performance.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CUDA和HIP与GPU硬件紧密相连，因此它们提供了可能无法通过跨平台可移植生态系统实现的性能优化级别。这些编程模型提供的库和工具链专门设计用于利用底层GPU架构的能力，使开发者能够实现高性能。
- en: Developers utilizing CUDA or HIP can tap into an extensive ecosystem of GPU-accelerated
    libraries, covering various domains, including linear algebra, signal processing,
    image processing, machine learning, and more. These libraries are highly optimized
    to take advantage of the parallelism and computational power offered by GPUs,
    allowing developers to accelerate their applications without having to implement
    complex algorithms from scratch.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CUDA或HIP的开发者可以访问一个广泛的GPU加速库生态系统，涵盖包括线性代数、信号处理、图像处理、机器学习等多个领域。这些库高度优化，以充分利用GPU提供的并行性和计算能力，使得开发者无需从头实现复杂算法即可加速他们的应用程序。
- en: As mentioned before, CUDA and HIP are very similar so it makes sense to cover
    both at the same time.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CUDA和HIP非常相似，因此同时介绍两者是有意义的。
- en: Comparison to portable kernel-based models
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于内核的可移植模型比较
- en: In code examples below, we will also show examples in the portable kernel-based
    frameworks Kokkos, SYCL and OpenCL, which will be covered in the next episode.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们还将展示在可移植内核框架Kokkos、SYCL和OpenCL中的示例，这些将在下一集中介绍。
- en: Hello World
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hello World
- en: 'Below we have the most basic example of CUDA and HIP, the “Hello World” program:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们最基础的CUDA和HIP示例，即“Hello World”程序：
- en: '[PRE52]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In both versions, we include the necessary headers: **cuda_runtime.h** and
    **cuda.h** for CUDA, and **hip_runtime.h** for HIP. These headers provide the
    required functionality for GPU programming.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个版本中，我们都包含了必要的头文件：CUDA的**cuda_runtime.h**和**cuda.h**，以及HIP的**hip_runtime.h**。这些头文件提供了GPU编程所需的功能。
- en: To retrieve information about the available devices, we use the functions **<cuda/hip>GetDeviceCount**
    and **<cuda/hip>GetDevice**. These functions allow us to determine the total number
    of GPUs and the index of the currently used device. In the code examples, we default
    to using device 0.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检索有关可用设备的信息，我们使用函数**<cuda/hip>GetDeviceCount**和**<cuda/hip>GetDevice**。这些函数允许我们确定GPU的总数和当前使用设备的索引。在代码示例中，我们默认使用设备0。
- en: 'As an exercise, modify the “Hello World” code to explicitly use a specific
    GPU. Do this by using the **<cuda/hip>SetDevice** function, which allows to set
    the desired GPU device. Note that the device number provided has to be within
    the range of available devices, otherwise, the program may fail to run or produce
    unexpected results. To experiment with different GPUs, modify the code to include
    the following line before retrieving device information:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，修改“Hello World”代码以显式使用特定的GPU。通过使用**<cuda/hip>SetDevice**函数来实现，该函数允许设置所需的GPU设备。请注意，提供的设备号必须在可用设备范围内，否则程序可能无法运行或产生意外的结果。为了实验不同的GPU，在检索设备信息之前修改代码以包含以下行：
- en: '[PRE57]'
  id: totrans-338
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Replace **deviceNumber** with the desired GPU device index. Run the code with
    different device numbers to observe the output (more examples for the “Hello World”
    program are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    subdirectory of this lesson repository).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 将**deviceNumber**替换为所需的GPU设备索引。使用不同的设备号运行代码以观察输出（本课程仓库的[content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)子目录中提供了“Hello
    World”程序的更多示例）。
- en: Vector Addition
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量加法
- en: To demonstrate the fundamental features of CUDA/HIP programming, let’s begin
    with a straightforward task of element-wise vector addition. The code snippet
    below demonstrates how to utilize CUDA and HIP for efficiently executing this
    operation.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 CUDA/HIP 编程的基本特性，让我们从一项简单的任务——逐元素向量加法——开始。下面的代码片段演示了如何利用 CUDA 和 HIP 高效地执行此操作。
- en: '[PRE58]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In this case, the CUDA and HIP codes are equivalent one to one so we will only
    refer to the CUDA version. The CUDA and HIP programming model are host centric
    programming models. The main program is executed on CPU and controls all the operations,
    memory allocations, data transfers between CPU and GPU, and launches the kernels
    to be executed on the GPU. The code starts with defining the GPU kernel function
    called **vector_add** with attribute **___global__**. It takes three input arrays
    A, B, and C along with the array size n. The kernel function contains the actually
    code which is executed on the GPU by multiple threads in parallel.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，CUDA 和 HIP 代码是一对一等效的，所以我们只参考 CUDA 版本。CUDA 和 HIP 编程模型是主机中心编程模型。主程序在 CPU
    上执行并控制所有操作，包括内存分配、CPU 和 GPU 之间的数据传输，以及启动在 GPU 上执行的内核。代码从定义具有 **___global__** 属性的
    GPU 内核函数 **vector_add** 开始。它接受三个输入数组 A、B 和 C 以及数组大小 n。内核函数包含实际上在 GPU 上由多个线程并行执行的代码。
- en: Accelerators in general and GPUs in particular usually have their own dedicated
    memory separate from the system memory (AMD MI300A is one exception, using the
    same memory for both CPU and GPU). When programming for GPUs, there are two sets
    of pointers involved and it’s necessary to manage data movement between the host
    memory and the accelerator memory. Data needs to be explicitly copied from the
    host memory to the accelerator memory before it can be processed by the accelerator.
    Similarly, results or modified data may need to be copied back from the accelerator
    memory to the host memory to make them accessible to the CPU.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，加速器（尤其是 GPU）都有其自己的专用内存，与系统内存分开（AMD MI300A 是一个例外，它使用相同的内存为 CPU 和 GPU）。当为 GPU
    编程时，涉及两组指针，并且需要管理主机内存和加速器内存之间的数据移动。在加速器处理之前，数据需要显式地从主机内存复制到加速器内存。同样，结果或修改后的数据可能需要从加速器内存复制回主机内存，以便使它们对
    CPU 可访问。
- en: The main function of the code initializes the input arrays Ah, Bh on the CPU
    and computes the reference array Cref. It then allocates memory on the GPU for
    the input and output arrays Ad, Bd, and Cd using **cudaMalloc**. Herein, h is
    for the ‘host’ (CPU) and d for the ‘device’ (GPU). The data is transferred from
    the CPU to the GPU using hipMemcpy, and then the GPU kernel is launched using
    the <<<.>>> syntax. All kernels launch are asynchronous. After launch the control
    returns to the main() and the code proceeds to the next instructions.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的主函数在 CPU 上初始化输入数组 Ah 和 Bh，并计算参考数组 Cref。然后使用 **cudaMalloc** 在 GPU 上为输入和输出数组
    Ad、Bd 和 Cd 分配内存。在这里，h 代表“主机”（CPU），d 代表“设备”（GPU）。数据通过 hipMemcpy 从 CPU 转移到 GPU，然后使用
    <<<.>>> 语法启动 GPU 内核。所有内核启动都是异步的。启动后，控制权返回到 main() 函数，代码继续执行下一行指令。
- en: After the kernel execution, the result array Cd is copied back to the CPU using
    **cudaMemcpy**. The code then prints the reference and result arrays, calculates
    the error by comparing the reference and result arrays. Finally, the GPU and CPU
    memory are deallocated using **cudaFree** and **free** functions, respectively.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 内核执行后，结果数组 Cd 使用 **cudaMemcpy** 返回到 CPU。然后代码打印参考和结果数组，通过比较参考和结果数组来计算误差。最后，使用
    **cudaFree** 和 **free** 函数分别释放 GPU 和 CPU 内存。
- en: The host functions **cudaSetDevice**, **cudaMalloc**, **cudaMemcpy**, and **cudaFree**
    are blocking, i.e. the code does not continues to next instructions until the
    operations are completed. However this is not the default behaviour, for many
    operations there are asynchronous equivalents and there are as well many library
    calls return the control to the main() after calling. This allows the developers
    to launch independent operations and overlap them.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数中的 **cudaSetDevice**, **cudaMalloc**, **cudaMemcpy**, 和 **cudaFree** 函数是阻塞的，即代码不会继续执行到下一行指令，直到这些操作完成。然而，这并不是默认行为，对于许多操作都有异步等效操作，并且许多库调用在调用后也会将控制权返回到
    main() 函数。这允许开发者启动独立的操作并将它们重叠。
- en: In short, this code demonstrates how to utilize the CUDA and HIP to perform
    vector addition on a GPU, showcasing the steps involved in allocating memory,
    transferring data between the CPU and GPU, launching a kernel function, and handling
    the results. It serves as a starting point for GPU-accelerated computations using
    CUDA and HIP. More examples for the vector (array) addition program are available
    at [content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，此代码演示了如何利用CUDA和HIP在GPU上执行向量加法，展示了分配内存、在CPU和GPU之间传输数据、启动内核函数和处理结果的步骤。它作为使用CUDA和HIP进行GPU加速计算的起点。有关向量（数组）加法程序的更多示例，请参阅[content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples)。
- en: In order to practice the concepts shown above, edit the skeleton code in the
    repository and the code corresponding to setting the device, memory allocations
    and transfers, and the kernel execution.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习上述概念，请编辑存储库中的骨架代码以及与设置设备、内存分配和传输以及内核执行相对应的代码。
- en: Vector Addition with Unified Memory
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用统一内存进行向量加法
- en: 'For a while already GPUs support unified memory, which allows to use the same
    pointer for both CPU and GPU data. This simplifies developing codes by removing
    the explicit data transfers. The data resides on CPU until it is needed on GPU
    or vice-versa. However the data transfers still happens “under the hood” and the
    developer needs to construct the code to avoid unnecessary transfers. Below one
    can see the modified vector addition codes:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有一段时间GPU支持统一内存，这允许使用相同的指针来处理CPU和GPU数据。这通过消除显式数据传输简化了代码开发。数据驻留在CPU上，直到需要在GPU上使用，反之亦然。然而，数据传输仍然“在幕后”发生，开发者需要构建代码以避免不必要的传输。下面可以看到修改后的向量加法代码：
- en: '[PRE62]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Now the arrays Ah, Bh, Ch, and Cref are using cudaMallocManaged to allocate
    Unified Memory. The **vector_add kernel** is launched by passing these Unified
    Memory pointers directly. After the kernel launch, **cudaDeviceSynchronize** is
    used to wait for the kernel to complete execution. Finally, **cudaFree** is used
    to free the Unified Memory arrays. The Unified Memory allows for transparent data
    migration between CPU and GPU, eliminating the need for explicit data transfers.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数组Ah、Bh、Ch和Cref正在使用cudaMallocManaged来分配统一内存。**vector_add内核**通过直接传递这些统一内存指针来启动。内核启动后，使用**cudaDeviceSynchronize**等待内核完成执行。最后，使用**cudaFree**释放统一内存数组。统一内存允许在CPU和GPU之间透明地迁移数据，消除了显式数据传输的需要。
- en: As an exercise modify the skeleton code for vector addition to use Unified Memory.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，修改向量加法的骨架代码以使用统一内存。
- en: Basics - In short
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 基础 - 简而言之
- en: CUDA is developed by NVIDIA, while HIP is an open-source project (from AMD)
    for multi-platform GPU programming.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA是由NVIDIA开发的，而HIP是AMD的一个开源项目（用于多平台GPU编程）。
- en: CUDA and HIP are GPU-focused programming models for optimized code execution
    on NVIDIA and AMD GPUs.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是针对NVIDIA和AMD GPU上优化代码执行的GPU-focused编程模型。
- en: CUDA and HIP are similar, allowing developers to write GPU code in a syntax
    similar to CUDA and target multiple platforms.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP相似，允许开发者使用与CUDA类似的语法编写GPU代码，并针对多个平台进行目标开发。
- en: CUDA and HIP are programming models focused solely on GPUs
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是专注于GPU的编程模型。
- en: CUDA and HIP offer high-performance computing capabilities and advanced features
    specific to GPU architectures, such as shared memory and memory management.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP提供了高性能计算能力以及针对GPU架构的特定高级功能，如共享内存和内存管理。
- en: They provide highly GPU-accelerated libraries in various domains like linear
    algebra, signal processing, image processing, and machine learning.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们提供了高度GPU加速的库，涵盖线性代数、信号处理、图像处理和机器学习等多个领域。
- en: Programming for GPUs involves managing data movement between host and accelerator
    memory.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为GPU编程编写代码涉及在主机和加速器内存之间管理数据移动。
- en: Unified Memory simplifies data transfers by using the same pointer for CPU and
    GPU data, but code optimization is still necessary.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一内存通过使用相同的指针来简化CPU和GPU数据的数据传输，但代码优化仍然是必要的。
- en: Hello World
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hello World
- en: 'Below we have the most basic example of CUDA and HIP, the “Hello World” program:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们有CUDA和HIP最基础的示例，“Hello World”程序：
- en: '[PRE65]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'In both versions, we include the necessary headers: **cuda_runtime.h** and
    **cuda.h** for CUDA, and **hip_runtime.h** for HIP. These headers provide the
    required functionality for GPU programming.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个版本中，我们包含了必要的头文件：**cuda_runtime.h**和**cuda.h**用于CUDA，以及**hip_runtime.h**用于HIP。这些头文件提供了GPU编程所需的必要功能。
- en: To retrieve information about the available devices, we use the functions **<cuda/hip>GetDeviceCount**
    and **<cuda/hip>GetDevice**. These functions allow us to determine the total number
    of GPUs and the index of the currently used device. In the code examples, we default
    to using device 0.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索有关可用设备的信息，我们使用函数**<cuda/hip>GetDeviceCount**和**<cuda/hip>GetDevice**。这些函数允许我们确定
    GPU 的总数和当前使用设备的索引。在代码示例中，我们默认使用设备 0。
- en: 'As an exercise, modify the “Hello World” code to explicitly use a specific
    GPU. Do this by using the **<cuda/hip>SetDevice** function, which allows to set
    the desired GPU device. Note that the device number provided has to be within
    the range of available devices, otherwise, the program may fail to run or produce
    unexpected results. To experiment with different GPUs, modify the code to include
    the following line before retrieving device information:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，将“Hello World”代码修改为显式使用特定的 GPU。通过使用**<cuda/hip>SetDevice**函数来实现，该函数允许设置所需的
    GPU 设备。请注意，提供的设备号必须在可用设备范围内，否则程序可能无法运行或产生意外的结果。为了实验不同的 GPU，在检索设备信息之前修改代码以包含以下行：
- en: '[PRE70]'
  id: totrans-379
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Replace **deviceNumber** with the desired GPU device index. Run the code with
    different device numbers to observe the output (more examples for the “Hello World”
    program are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    subdirectory of this lesson repository).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 将**deviceNumber**替换为所需的 GPU 设备索引。使用不同的设备号运行代码以观察输出（本课程存储库中[content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)子目录中提供了“Hello
    World”程序的更多示例）。
- en: Vector Addition
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量加法
- en: To demonstrate the fundamental features of CUDA/HIP programming, let’s begin
    with a straightforward task of element-wise vector addition. The code snippet
    below demonstrates how to utilize CUDA and HIP for efficiently executing this
    operation.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 CUDA/HIP 编程的基本特性，让我们从一个简单的元素级向量加法任务开始。下面的代码片段展示了如何利用 CUDA 和 HIP 高效地执行此操作。
- en: '[PRE71]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: In this case, the CUDA and HIP codes are equivalent one to one so we will only
    refer to the CUDA version. The CUDA and HIP programming model are host centric
    programming models. The main program is executed on CPU and controls all the operations,
    memory allocations, data transfers between CPU and GPU, and launches the kernels
    to be executed on the GPU. The code starts with defining the GPU kernel function
    called **vector_add** with attribute **___global__**. It takes three input arrays
    A, B, and C along with the array size n. The kernel function contains the actually
    code which is executed on the GPU by multiple threads in parallel.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，CUDA 和 HIP 代码是逐个对应的，因此我们只参考 CUDA 版本。CUDA 和 HIP 编程模型是主机中心编程模型。主程序在 CPU
    上执行并控制所有操作、内存分配、CPU 和 GPU 之间的数据传输以及启动在 GPU 上执行的内核。代码从定义具有属性**___global__**的 GPU
    内核函数**vector_add**开始。它接受三个输入数组 A、B 和 C 以及数组大小 n。内核函数包含实际执行的代码，这些代码由多个线程并行地在 GPU
    上执行。
- en: Accelerators in general and GPUs in particular usually have their own dedicated
    memory separate from the system memory (AMD MI300A is one exception, using the
    same memory for both CPU and GPU). When programming for GPUs, there are two sets
    of pointers involved and it’s necessary to manage data movement between the host
    memory and the accelerator memory. Data needs to be explicitly copied from the
    host memory to the accelerator memory before it can be processed by the accelerator.
    Similarly, results or modified data may need to be copied back from the accelerator
    memory to the host memory to make them accessible to the CPU.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，加速器（尤其是 GPU）都有自己的专用内存，与系统内存分开（AMD MI300A 是一个例外，它使用相同的内存为 CPU 和 GPU）。当为
    GPU 编程时，涉及两组指针，并且需要在主机内存和加速器内存之间管理数据移动。在加速器处理之前，数据需要显式地从主机内存复制到加速器内存。同样，结果或修改后的数据可能需要从加速器内存复制回主机内存，以便使它们对
    CPU 可访问。
- en: The main function of the code initializes the input arrays Ah, Bh on the CPU
    and computes the reference array Cref. It then allocates memory on the GPU for
    the input and output arrays Ad, Bd, and Cd using **cudaMalloc**. Herein, h is
    for the ‘host’ (CPU) and d for the ‘device’ (GPU). The data is transferred from
    the CPU to the GPU using hipMemcpy, and then the GPU kernel is launched using
    the <<<.>>> syntax. All kernels launch are asynchronous. After launch the control
    returns to the main() and the code proceeds to the next instructions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的主函数在CPU上初始化输入数组 Ah、Bh，并计算参考数组 Cref。然后使用 **cudaMalloc** 在GPU上为输入和输出数组 Ad、Bd
    和 Cd 分配内存。在这里，h 代表‘主机’（CPU），d 代表‘设备’（GPU）。使用 hipMemcpy 将数据从CPU传输到GPU，然后使用 <<<...>>>
    语法启动GPU内核。所有内核启动都是异步的。启动后，控制返回到 main()，代码继续执行下一个指令。
- en: After the kernel execution, the result array Cd is copied back to the CPU using
    **cudaMemcpy**. The code then prints the reference and result arrays, calculates
    the error by comparing the reference and result arrays. Finally, the GPU and CPU
    memory are deallocated using **cudaFree** and **free** functions, respectively.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 内核执行后，结果数组 Cd 使用 **cudaMemcpy** 返回到CPU。然后代码打印参考和结果数组，通过比较参考和结果数组来计算误差。最后，使用
    **cudaFree** 和 **free** 函数分别释放GPU和CPU内存。
- en: The host functions **cudaSetDevice**, **cudaMalloc**, **cudaMemcpy**, and **cudaFree**
    are blocking, i.e. the code does not continues to next instructions until the
    operations are completed. However this is not the default behaviour, for many
    operations there are asynchronous equivalents and there are as well many library
    calls return the control to the main() after calling. This allows the developers
    to launch independent operations and overlap them.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 主机函数 **cudaSetDevice**、**cudaMalloc**、**cudaMemcpy** 和 **cudaFree** 是阻塞的，即代码不会继续执行到下一个指令，直到操作完成。然而，这并不是默认行为，对于许多操作，都有异步等效操作，以及许多库调用在调用后返回控制到
    main()。这允许开发者启动独立操作并将它们重叠。
- en: In short, this code demonstrates how to utilize the CUDA and HIP to perform
    vector addition on a GPU, showcasing the steps involved in allocating memory,
    transferring data between the CPU and GPU, launching a kernel function, and handling
    the results. It serves as a starting point for GPU-accelerated computations using
    CUDA and HIP. More examples for the vector (array) addition program are available
    at [content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，此代码演示了如何利用CUDA和HIP在GPU上执行向量加法，展示了涉及分配内存、在CPU和GPU之间传输数据、启动内核函数和处理结果的步骤。它作为使用CUDA和HIP进行GPU加速计算的起点。有关向量（数组）加法程序的更多示例，请参阅[content/examples](https://github.com/ENCCS/gpu-programming/tree/main/content/examples)。
- en: In order to practice the concepts shown above, edit the skeleton code in the
    repository and the code corresponding to setting the device, memory allocations
    and transfers, and the kernel execution.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习上述概念，请编辑存储库中的骨架代码以及与设置设备、内存分配和传输以及内核执行相关的代码。
- en: Vector Addition with Unified Memory
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用统一内存的向量加法
- en: 'For a while already GPUs support unified memory, which allows to use the same
    pointer for both CPU and GPU data. This simplifies developing codes by removing
    the explicit data transfers. The data resides on CPU until it is needed on GPU
    or vice-versa. However the data transfers still happens “under the hood” and the
    developer needs to construct the code to avoid unnecessary transfers. Below one
    can see the modified vector addition codes:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间以来，GPU已经支持统一内存，这允许使用相同的指针来访问CPU和GPU数据。这通过消除显式的数据传输简化了代码的开发。数据驻留在CPU上，直到需要在GPU上使用，反之亦然。然而，数据传输仍然在“幕后”发生，开发者需要构建代码以避免不必要的传输。下面可以看到修改后的向量加法代码：
- en: '[PRE75]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Now the arrays Ah, Bh, Ch, and Cref are using cudaMallocManaged to allocate
    Unified Memory. The **vector_add kernel** is launched by passing these Unified
    Memory pointers directly. After the kernel launch, **cudaDeviceSynchronize** is
    used to wait for the kernel to complete execution. Finally, **cudaFree** is used
    to free the Unified Memory arrays. The Unified Memory allows for transparent data
    migration between CPU and GPU, eliminating the need for explicit data transfers.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数组 Ah、Bh、Ch 和 Cref 使用 cudaMallocManaged 来分配统一内存。**向量加法内核**通过传递这些统一内存指针直接启动。内核启动后，使用
    **cudaDeviceSynchronize** 等待内核完成执行。最后，使用 **cudaFree** 释放统一内存数组。统一内存允许在CPU和GPU之间透明地迁移数据，消除了显式数据传输的需要。
- en: As an exercise modify the skeleton code for vector addition to use Unified Memory.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，修改向量加法的基本代码以使用统一内存。
- en: Basics - In short
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 基础 - 简而言之
- en: CUDA is developed by NVIDIA, while HIP is an open-source project (from AMD)
    for multi-platform GPU programming.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA由NVIDIA开发，而HIP是AMD的一个开源项目（用于多平台GPU编程）。
- en: CUDA and HIP are GPU-focused programming models for optimized code execution
    on NVIDIA and AMD GPUs.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是针对NVIDIA和AMD GPU上优化代码执行的GPU-focused编程模型。
- en: CUDA and HIP are similar, allowing developers to write GPU code in a syntax
    similar to CUDA and target multiple platforms.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP相似，允许开发者使用类似于CUDA的语法编写GPU代码，并针对多个平台进行目标化。
- en: CUDA and HIP are programming models focused solely on GPUs
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是仅针对GPU的编程模型
- en: CUDA and HIP offer high-performance computing capabilities and advanced features
    specific to GPU architectures, such as shared memory and memory management.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP提供了针对GPU架构的高性能计算能力和高级功能，例如共享内存和内存管理。
- en: They provide highly GPU-accelerated libraries in various domains like linear
    algebra, signal processing, image processing, and machine learning.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们提供了在各个领域（如线性代数、信号处理、图像处理和机器学习）中高度GPU加速的库。
- en: Programming for GPUs involves managing data movement between host and accelerator
    memory.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为GPU编程涉及在主机和加速器内存之间管理数据移动。
- en: Unified Memory simplifies data transfers by using the same pointer for CPU and
    GPU data, but code optimization is still necessary.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一内存通过使用相同的指针来简化CPU和GPU数据之间的数据传输，但代码优化仍然是必要的。
- en: Memory Optimizations
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存优化
- en: Vector addition is a relatively simple, straight forward case. Each thread reads
    data from memory, does an addition and then saves the result. Two adjacent threads
    access memory location in memory close to each other. Also the data is used only
    once. In practice this not the case. Also sometimes the same data is used several
    times resulting in additional memory accesses.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 向量加法是一个相对简单、直接的案例。每个线程从内存中读取数据，进行加法运算，然后将结果保存。两个相邻的线程访问内存中彼此靠近的内存位置。此外，数据只使用一次。在实践中，情况并非如此。有时相同的数据被多次使用，导致额外的内存访问。
- en: Memory optimization is one of the most important type of optimization done to
    efficiently use the GPUs. Before looking how it is done in practice let’s revisit
    some basic concepts about GPUs and execution model.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 内存优化是用于高效使用GPU的最重要类型的优化之一。在探讨实际操作之前，让我们回顾一下关于GPU和执行模型的一些基本概念。
- en: GPUs are comprised many light cores, the so-called Streaming Processors (SP)
    in CUDA, which are physically group together in units, i.e. Streaming Multi-Processors
    (SMP) in CUDA architecture (note that in AMD the equivalent is called Computing
    Units, while in Intel GPUs they are Execution Units). The work is done on GPUs
    by launching many threads each executing an instance of the same kernel. The order
    of execution is not defined, and the threads can only exchange information in
    specific conditions. Because of the way the SPs are grouped the threads are also
    grouped in **blocks**. Each **block** is assigned to an SMP, and can not be split.
    An SMP can have more than block residing at a moment, however there is no communications
    between the threads in different blocks. In addition to the SPs, each SMP contains
    very fast memory which in CUDA is referred to as shared memory. The threads in
    a block can read and write to the shared memory and use it as a user controlled
    cache. One thread can for example write to a location in the shared memory while
    another thread in the same block can read and use that data. In order to be sure
    that all threads in the block completed writing **__syncthreads()** function has
    to be used to make the threads in the block wait until all of them reached the
    specific place in the kernel. Another important aspect in the GPU programming
    model is that the threads in the block are not executed independently. The threads
    in a block are physically grouped in warps of size 32 in NVIDIA devices or wavefronts
    of size 32 or 64 in AMD devices (depending on device architecture). Intel devices
    are notable in that the warp size, called SIMD width, is highly configurable,
    with typical possible values of 8, 16, or 32 (depends on the hardware). All memory
    accesses of the global GPU memory are done per warp. When data is needed for some
    calculations a warp loads from the GPU memory blocks of specific size (64 or 128
    Bytes). These operation is very expensive, it has a latency of hundreds of cycles.
    This means that the threads in a warp should work with elements of the data located
    close in the memory. In the vector addition two threads near each other, of index
    tid and tid+1, access elements adjacent in the GPU memory.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: GPU由许多轻量级核心组成，这些核心在CUDA中被称为流处理器（SP），它们在物理上以单元的形式组合在一起，即CUDA架构中的流多处理器（SMP）。GPU上的工作是通过启动许多线程来完成的，每个线程执行相同内核的一个实例。执行顺序没有定义，线程只能在特定条件下交换信息。由于SP的分组方式，线程也被分组在**块**中。每个**块**被分配给一个SMP，并且不能分割。一个SMP可以同时容纳多个块，然而不同块之间的线程没有通信。除了SP之外，每个SMP还包含非常快速的内存，在CUDA中被称为共享内存。块中的线程可以读写共享内存，并将其用作用户控制的缓存。例如，一个线程可以写入共享内存中的某个位置，而同一块中的另一个线程可以读取并使用这些数据。为了确保块中的所有线程都完成了写入，必须使用**__syncthreads()**函数来使块中的线程等待，直到它们都到达内核中的特定位置。在GPU编程模型中另一个重要方面是，块中的线程不是独立执行的。在NVIDIA设备中，块中的线程以大小为32的warp（线程束）为单位物理分组，而在AMD设备中，以大小为32或64的wavefront（波前）为单位（取决于设备架构）。英特尔设备值得注意的是，warp的大小，称为SIMD宽度，可以高度配置，典型的可能值为8、16或32（取决于硬件）。所有全局GPU内存的内存访问都是按warp进行的。当需要某些计算的数据时，warp从GPU内存中加载特定大小的块（64或128字节）。这些操作非常昂贵，具有数百个周期的延迟。这意味着warp中的线程应该与内存中位置接近的数据元素一起工作。在向量加法中，两个相邻的线程，索引为tid和tid+1，访问GPU内存中相邻的元素。
- en: The shared memory can be used to improve performance in two ways. It is possible
    to avoid extra reads from the memory when several threads in the same block need
    the same data (see [stencil](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/stencil)
    code) or it can be used to improve the memory access patterns like in the case
    of matrix transpose.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存可以通过两种方式来提高性能。当同一块中的多个线程需要相同的数据时，可以避免从内存中进行额外的读取（参见[stencil](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/stencil)代码）或者它可以用来改进内存访问模式，如矩阵转置的情况。
- en: Memory, Execution - In short
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 内存、执行 - 简而言之
- en: GPUs consist of streaming processors (SPs) grouped together in units, such as
    Streaming Multi-Processors (SMPs) in CUDA architecture.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU由流处理器（SPs）组成，这些处理器以单元的形式组合在一起，例如CUDA架构中的流多处理器（SMPs）。
- en: Work on GPUs is done by launching threads, with each thread executing an instance
    of the same kernel, and the execution order is not defined.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU上的工作是通过启动线程来完成的，每个线程执行相同内核的一个实例，执行顺序没有定义。
- en: Threads are organized into blocks, assigned to an SMP, and cannot be split,
    and there is no communication between threads in different blocks.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程被组织成块，分配给一个SMP，并且不能被分割，不同块之间的线程之间没有通信。
- en: Each SMP contains shared memory, which acts as a user-controlled cache for threads
    within a block, allowing efficient data sharing and synchronization.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个SMP包含共享内存，它作为块内线程的用户可控缓存，允许高效的数据共享和同步。
- en: The shared memory can be used to avoid extra memory reads when multiple threads
    in the same block need the same data or to improve memory access patterns, such
    as in matrix transpose operations.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以用来避免当同一块内的多个线程需要相同数据时进行额外的内存读取，或者改进内存访问模式，例如在矩阵转置操作中。
- en: Memory accesses from global GPU memory are performed per warp (groups of threads),
    and loading data from GPU memory has high latency.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从全局GPU内存进行的内存访问是按warp（线程组）进行的，从GPU内存加载数据具有高延迟。
- en: To optimize memory access, threads within a warp should work with adjacent elements
    in memory to reduce latency.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了优化内存访问，warp内的线程应该与内存中的相邻元素一起工作，以减少延迟。
- en: Proper utilization of shared memory can improve performance by reducing memory
    reads and enhancing memory access patterns.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确利用共享内存可以通过减少内存读取和增强内存访问模式来提高性能。
- en: Matrix Transpose
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵转置
- en: Matrix transpose is a classic example where shared memory can significantly
    improve the performance. The use of shared memory reduces global memory accesses
    and exploits the high bandwidth and low latency of shared memory.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵转置是一个共享内存可以显著提高性能的经典例子。使用共享内存减少了全局内存访问，并利用了共享内存的高带宽和低延迟。
- en: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
- en: First as a reference we use a simple kernel which copy the data from one array
    to the other.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用一个简单的内核，将数据从一个数组复制到另一个数组中。
- en: '[PRE78]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: We note that this code does not do any calculations. Each thread reads one element
    and then writes it to another locations. By measuring the execution time of the
    kernel we can compute the effective bandwidth achieve by this kernel. We can measure
    the time using **rocprof** or **cuda/hip events**. On a NVIDIA V100 GPU this code
    achieves 717 GB/s out of the theoretical peak 900 GB/s.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，此代码没有进行任何计算。每个线程读取一个元素，然后将其写入另一个位置。通过测量内核的执行时间，我们可以计算此内核的有效带宽。我们可以使用**rocprof**或**cuda/hip
    events**来测量时间。在NVIDIA V100 GPU上，此代码实现了717 GB/s，理论峰值900 GB/s。
- en: Now we do the first iteration of the code, a naive transpose. The reads have
    a nice coalesced access pattern, but the writing is now very inefficient.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们执行代码的第一迭代，一个简单的转置。读取具有很好的归一化访问模式，但写入现在非常低效。
- en: '[PRE81]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Checking the index in_index we see that two adjacent threads (threadIx.x, threadIdx.x+1)
    access location in memory near each other. However the writes are not. Threads
    access data which in a strided way. Two adjacent threads access data separated
    by height elements. This practically results in 32 memory operations, however
    due to under the hood optimizations the achieved bandwidth is 311 GB/s.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 检查索引_in_index，我们看到两个相邻的线程（threadIx.x，threadIdx.x+1）访问内存中的相邻位置。然而，写入操作不是这样的。线程以步进方式访问数据。两个相邻的线程访问的数据之间相隔height个元素。这实际上导致了32次内存操作，但由于底层的优化，实现的带宽为311
    GB/s。
- en: We can improve the code by reading the data in a coalesced way, save it in the
    shared memory row by row and then write in the global memory column by column.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以归一化的方式读取数据，按行保存到共享内存中，然后按列写入全局内存来改进代码。
- en: '[PRE83]'
  id: totrans-437
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE83]'
- en: ''
  id: totrans-438
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-439
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE84]'
- en: We define a **tile_dim** constant to determine the size of the shared memory
    tile. The matrix transpose kernel uses a 2D grid of thread blocks, where each
    thread block operates on a tile_dim x tile_dim tile of the input matrix.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个**tile_dim**常量来确定共享内存块的大小。矩阵转置内核使用一个二维线程块网格，其中每个线程块操作输入矩阵的tile_dim x tile_dim块。
- en: The kernel first loads data from the global memory into the shared memory tile.
    Each thread loads a single element from the input matrix into the shared memory
    tile. Then, a **__syncthreads()** barrier ensures that all threads have finished
    loading data into shared memory before proceeding.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 内核首先将全局内存中的数据加载到共享内存块中。每个线程将输入矩阵中的一个元素加载到共享内存块中。然后，一个**__syncthreads()**屏障确保所有线程在继续之前已经将数据加载到共享内存中。
- en: Next, the kernel writes the transposed data from the shared memory tile back
    to the output matrix in global memory. Each thread writes a single element from
    the shared memory tile to the output matrix. By using shared memory, this optimized
    implementation reduces global memory accesses and exploits memory coalescence,
    resulting in improved performance compared to a naive transpose implementation.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，内核将共享内存块中的转置数据写回到全局内存中的输出矩阵。每个线程将共享内存块中的一个元素写入输出矩阵。通过使用共享内存，这个优化实现减少了全局内存访问并利用了内存归一化，与原始转置实现相比，性能得到了提升。
- en: This kernel achieved on NVIDIA V100 674 GB/s.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核在NVIDIA V100上实现了674 GB/s。
- en: This is pretty close to the bandwidth achieved by the simple copy kernel, but
    there is one more thing to improve.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 这与简单复制内核实现的带宽非常接近，但还有一点可以改进。
- en: Shared memory is composed of banks. Each banks can service only one request
    at the time. Bank conflicts happen when more than 1 thread in a specific warp
    try to access data in bank. The bank conflicts are resolved by serializing the
    accesses resulting in less performance. In the above example when data is saved
    to the shared memory, each thread in the warp will save an element of the data
    in a different one. Assuming that shared memory has 16 banks after writing each
    bank will contain one column. At the last step when we write from the shared memory
    to the global memory each warp load data from the same bank. A simple way to avoid
    this is by just padding the temporary array.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存由银行组成。每个银行一次只能服务一个请求。当特定战程中的多个线程试图访问银行中的数据时，就会发生银行冲突。通过序列化访问来解决银行冲突，这会导致性能降低。在上面的例子中，当数据保存到共享内存时，战程中的每个线程将数据的一个元素保存到不同的银行。假设共享内存有16个银行，在写入后，每个银行将包含一列。在最后一步，当我们从共享内存写入全局内存时，每个战程将从相同的银行加载数据。避免这种情况的一个简单方法就是只对临时数组进行填充。
- en: '[PRE85]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: By padding the array the data is slightly shifting it resulting in no bank conflicts.
    The effective bandwidth for this kernel is 697 GB/s.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 通过填充数组，数据略有偏移，从而避免了银行冲突。这个内核的有效带宽为697 GB/s。
- en: Using sharing memory as a cache - In short
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 将共享内存用作缓存 - 简而言之
- en: Shared memory can significantly improve performance in operations like matrix
    transpose.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以显著提高矩阵转置等操作的性能。
- en: Shared memory reduces global memory accesses and exploits the high bandwidth
    and low latency of shared memory.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存减少了全局内存访问，并利用了共享内存的高带宽和低延迟。
- en: An optimized implementation utilizes shared memory, loads data coalescedly,
    and performs transpose operations.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个优化的实现利用共享内存，以归一化的方式加载数据，并执行转置操作。
- en: The optimized implementation uses a 2D grid of thread blocks and a shared memory
    tile size determined by a constant.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的实现使用一个二维线程块网格和一个由常量确定的共享内存块大小。
- en: The kernel loads data from global memory into the shared memory tile and uses
    a synchronization barrier.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核将从全局内存加载数据到共享内存块，并使用同步屏障。
- en: To avoid bank conflicts in shared memory, padding the temporary array is a simple
    solution.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免在共享内存中的银行冲突，对临时数组进行填充是一个简单的解决方案。
- en: Reductions
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少操作
- en: Reductions refer to operations in which the elements of an array are aggregated
    in a single value through operations such as summing, finding the maximum or minimum,
    or performing logical operations.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 减少操作指的是通过求和、查找最大或最小值或执行逻辑运算等操作，将数组元素聚合为单个值的操作。
- en: In the serial approach, the reduction is performed sequentially by iterating
    through the collection of values and accumulating the result step by step. This
    will be enough for small sizes, but for big problems this results in significant
    time spent in this part of an application. On a GPU, this approach is not feasible.
    Using just one thread to do this operation means the rest of the GPU is wasted.
    Doing reduction in parallel is a little tricky. In order for a thread to do work,
    it needs to have some partial result to use. If we launch, for example, a kernel
    performing a simple vector summation, `sum[0]+=a[tid]`, with N threads we notice
    that this would result in undefined behaviour. GPUs have mechanisms to access
    the memory and lock the access for other threads while 1 thread is doing some
    operations to a given data via **atomics**, however this means that the memory
    access gets again to be serialized. There is not much gain. We note that when
    doing reductions the order of the iterations is not important (barring the typical
    non-associative behavior of floating-point operations). Also we can we might have
    to divide our problem in several subsets and do the reduction operation for each
    subset separately. On the GPUs, since the GPU threads are grouped in blocks, the
    size of the subset based on that. Inside the block, threads can cooperate with
    each other, they can share data via the shared memory and can be synchronized
    as well. All threads read the data to be reduced, but now we have significantly
    less partial results to deal with. In general, the size of the block ranges from
    256 to 1024 threads. In case of very large problems, after this procedure if we
    are left too many partial results this step can be repeated.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在串行方法中，通过迭代值集合并逐步累积结果来执行减少操作。这对于小规模数据足够，但对于大型问题，这会导致在应用程序的这一部分花费大量时间。在GPU上，这种方法不可行。仅使用一个线程来完成此操作意味着GPU的其余部分都被浪费了。并行执行减少操作有点棘手。为了使线程能够工作，它需要使用一些部分结果。如果我们启动，例如，一个执行简单向量求和的内核，`sum[0]+=a[tid]`，使用N个线程，我们会发现这会导致未定义的行为。GPU有机制通过**原子操作**访问内存并锁定其他线程的访问，然而这又意味着内存访问再次被序列化。这样并没有带来太多收益。我们注意到，在进行减少操作时，迭代顺序并不重要（除了浮点运算的典型非结合行为）。此外，我们可能需要将问题分成几个子集，并对每个子集分别执行减少操作。在GPU上，由于GPU线程被分组在块中，因此子集的大小基于这一点。在块内部，线程可以相互协作，可以通过共享内存共享数据，也可以进行同步。所有线程都读取要减少的数据，但现在我们处理的部分结果显著减少。一般来说，块的大小从256到1024个线程不等。在非常大型问题的案例中，如果在此步骤后留下太多部分结果，则可以重复此步骤。
- en: 'At the block level we still have to perform a reduction in an efficient way.
    Doing it serially means that we are not using all GPU cores (roughly 97% of the
    computing capacity is wasted). Doing it naively parallel using **atomics**, but
    on the shared memory is also not a good option. Going back back to the fact the
    reduction operations are commutative and associative we can set each thread to
    “reduce” two elements of the local part of the array. Shared memory can be used
    to store the partial “reductions” as shown below in the code:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在块级别，我们仍然需要以有效的方式执行减少操作。串行执行意味着我们没有使用所有GPU核心（大约97%的计算能力被浪费了）。在共享内存上使用**原子操作**进行天真地并行操作也不是一个好的选择。回到减少操作是交换律和结合律的事实，我们可以设置每个线程“减少”数组局部部分的两个元素。共享内存可以用来存储如以下代码所示的部分“减少”：
- en: '[PRE87]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: In the kernel we have each GPU performing thread a reduction of two elements
    from the local portion of the array. If we have tpb GPU threads per block, we
    utilize them to store 2xtpb elements in the local shared memory. To ensure synchronization
    until all data is available in the shared memory, we employ the syncthreads()
    function.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，每个GPU执行线程从数组的局部部分减少两个元素。如果我们有每个块tpb个GPU线程，我们利用它们在局部共享内存中存储2xtpb个元素。为了确保同步直到所有数据都可用在共享内存中，我们使用syncthreads()函数。
- en: Next, we instruct each thread to “reduce” the element in the array at threadIdx.x
    with the element at threadIdx.x+tpb. As this operation saves the result back into
    the shared memory, we once again employ syncthreads(). By doing this, we effectively
    halve the number of elements to be reduced.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指示每个线程将数组中threadIdx.x处的元素与threadIdx.x+tpb处的元素“减少”。由于这个操作将结果保存回共享内存，我们再次使用syncthreads()。通过这样做，我们有效地将需要减少的元素数量减半。
- en: This procedure can be repeated, but now we only utilize tpb/2 threads. Each
    thread is responsible for “reducing” the element in the array at threadIdx.x with
    the element at threadIdx.x+tpb/2. After this step, we are left with tpb/4 numbers
    to be reduced. We continue applying this procedure until only one number remains.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以重复，但现在我们只利用 tpb/2 线程。每个线程负责“缩减”数组中位于 threadIdx.x 的元素与位于 threadIdx.x+tpb/2
    的元素。在此步骤之后，我们剩下 tpb/4 个数字需要缩减。我们继续应用此过程，直到只剩下一个数字。
- en: At this point, we can either “reduce” the final number with a global partial
    result using atomic read and write operations, or we can save it into an array
    for further processing.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用全局部分结果和原子读写操作来“缩减”最终数字，或者将其保存到数组中以供进一步处理。
- en: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
- en: Schematic representation on the reduction algorithm with 8 GPU threads.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 8 个 GPU 线程的缩减算法的示意图。
- en: For a detail analysis of how to optimize reduction operations in CUDA/HIP check
    this presentation [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细了解如何在 CUDA/HIP 中优化缩减操作，请查看这个演示文稿 [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
- en: Reductions - In short
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减 - 简而言之
- en: Reductions refer to aggregating elements of an array into a single value through
    operations like summing, finding maximum or minimum, or performing logical operations.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩减是指通过求和、查找最大或最小值或执行逻辑运算等操作将数组的元素聚合为单个值。
- en: Performing reductions sequentially in a serial approach is inefficient for large
    problems, while parallel reduction on GPUs offers better performance.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在串行方法中对缩减进行顺序执行对于大问题来说效率低下，而 GPU 上的并行缩减提供了更好的性能。
- en: Parallel reduction on GPUs involves dividing the problem into subsets, performing
    reductions within blocks of threads using shared memory, and repeatedly reducing
    the number of elements (two per GPU thread) until only one remains.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU 上的并行缩减涉及将问题划分为子集，在线程块中使用共享内存执行缩减，并重复减少元素数量（每个 GPU 线程两个），直到只剩下一个。
- en: Overlapping Computations and Memory transfer. CUDA/HIP Streams
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算和内存传输的重叠。CUDA/HIP 流
- en: Modern GPUs can overlap independent operations. They can do transfers between
    CPU and GPU and execute kernels in the same time, or they can execute kernels
    concurrently. CUDA/HIP streams are independent execution units, a sequence of
    operations that execute in issue-order on the GPU. The operations issue in different
    streams can be executed concurrently.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 GPU 可以重叠独立操作。它们可以在 CPU 和 GPU 之间进行传输，同时执行内核，或者它们可以并发执行内核。CUDA/HIP 流是独立的执行单元，是一系列在
    GPU 上按发布顺序执行的运算。不同流中的操作可以并发执行。
- en: Consider the previous case of vector addition, which involves copying data from
    CPU to GPU, computations and then copying back the result to GPU. In this way
    nothing can be overlap.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑之前的向量加法案例，它涉及从 CPU 到 GPU 的数据复制，计算，然后将结果复制回 GPU。这种方式下，无法重叠。
- en: We can improve the performance by dividing the problem in smaller independent
    parts. Let’s consider 5 streams and consider the case where copy in one direction
    and computation take the same amount of time.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将问题划分为更小的独立部分来提高性能。让我们考虑 5 个流，并考虑复制和计算花费相同时间的案例。
- en: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
- en: After the first and second stream copy data to the GPU, the GPU is practically
    occupied all time. We can see that significant performance improvements can be
    obtained by eliminating the time in which the GPU is idle, waiting for data to
    arrive from the CPU. This very useful for problems where there is often communication
    to the CPU because the GPU memory can not fit all the problem or the application
    runs in a multi-GPU set up and communication is needed often.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次和第二次流将数据复制到 GPU 后，GPU 实际上一直处于忙碌状态。我们可以看到，通过消除 GPU 空闲等待 CPU 数据到达的时间，可以获得显著的性能提升。这对于经常需要与
    CPU 通信的问题非常有用，因为 GPU 内存无法容纳整个问题或应用程序在多 GPU 设置下运行且需要频繁通信。
- en: We can apply this to the vector addition problem above.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此应用于上述的向量加法问题。
- en: '[PRE89]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Instead of having one copy to gpu, one execution of the kernel and one copy
    back, we now have several of these calls independent of each other.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有多个这样的调用，它们彼此独立，而不是只有一个复制到 GPU、一次内核执行和一个复制回的操作。
- en: Note that even when streams are not explicitly used it is possible to launch
    all the GPU operations asynchronous and overlap CPU operations (such I/O) and
    GPU operations. In order to learn more about how to improve performance using
    streams check the NVIDIA blog [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/).
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使没有显式使用流，也可以异步启动所有GPU操作，并重叠CPU操作（如I/O）和GPU操作。要了解更多关于如何使用流来提高性能的信息，请查看NVIDIA博客[如何在CUDA
    C/C++中重叠数据传输](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)。
- en: Streams - In short
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 流 - 简而言之
- en: CUDA/HIP streams are independent execution contexts on the GPU that allow for
    concurrent execution of operations issued in different streams.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA/HIP流是GPU上的独立执行上下文，允许在不同流中并发执行操作。
- en: Using streams can improve GPU performance by overlapping operations such as
    data transfers between CPU and GPU and kernel executions.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流可以提高GPU性能，通过重叠操作，如CPU和GPU之间的数据传输和内核执行。
- en: By dividing a problem into smaller independent parts and utilizing multiple
    streams, the GPU can avoid idle time, resulting in significant performance improvements,
    especially for problems with frequent CPU communication or multi-GPU setups.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将问题分解成更小的独立部分并利用多个流，GPU可以避免空闲时间，从而显著提高性能，尤其是在需要频繁CPU通信或多GPU设置的程序中。
- en: Matrix Transpose
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵转置
- en: Matrix transpose is a classic example where shared memory can significantly
    improve the performance. The use of shared memory reduces global memory accesses
    and exploits the high bandwidth and low latency of shared memory.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵转置是一个经典的例子，其中共享内存可以显著提高性能。使用共享内存减少了全局内存访问，并利用了共享内存的高带宽和低延迟。
- en: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/transpose_img.png](../Images/aaf127844f7b27f2f62a87d4c863ca09.png)'
- en: First as a reference we use a simple kernel which copy the data from one array
    to the other.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，作为一个参考，我们使用一个简单的内核，将数据从一个数组复制到另一个数组。
- en: '[PRE91]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: We note that this code does not do any calculations. Each thread reads one element
    and then writes it to another locations. By measuring the execution time of the
    kernel we can compute the effective bandwidth achieve by this kernel. We can measure
    the time using **rocprof** or **cuda/hip events**. On a NVIDIA V100 GPU this code
    achieves 717 GB/s out of the theoretical peak 900 GB/s.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到这段代码没有进行任何计算。每个线程读取一个元素，然后将其写入另一个位置。通过测量内核的执行时间，我们可以计算出该内核的有效带宽。我们可以使用**rocprof**或**cuda/hip
    events**来测量时间。在NVIDIA V100 GPU上，此代码实现了717 GB/s，理论峰值900 GB/s。
- en: Now we do the first iteration of the code, a naive transpose. The reads have
    a nice coalesced access pattern, but the writing is now very inefficient.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进行代码的第一迭代，一个简单的转置。读取具有很好的归一化访问模式，但写入现在非常低效。
- en: '[PRE94]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Checking the index in_index we see that two adjacent threads (threadIx.x, threadIdx.x+1)
    access location in memory near each other. However the writes are not. Threads
    access data which in a strided way. Two adjacent threads access data separated
    by height elements. This practically results in 32 memory operations, however
    due to under the hood optimizations the achieved bandwidth is 311 GB/s.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 检查索引in_index，我们看到两个相邻的线程（threadIx.x, threadIdx.x+1）访问内存中的相邻位置。然而，写入不是。线程以步进方式访问数据。两个相邻的线程访问的数据相隔height个元素。这实际上导致了32次内存操作，但由于底层的优化，实现的带宽为311
    GB/s。
- en: We can improve the code by reading the data in a coalesced way, save it in the
    shared memory row by row and then write in the global memory column by column.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以归一化的方式读取数据，逐行保存到共享内存中，然后按列写入全局内存来改进代码。
- en: '[PRE96]'
  id: totrans-501
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE96]'
- en: ''
  id: totrans-502
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-503
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE97]'
- en: We define a **tile_dim** constant to determine the size of the shared memory
    tile. The matrix transpose kernel uses a 2D grid of thread blocks, where each
    thread block operates on a tile_dim x tile_dim tile of the input matrix.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个**tile_dim**常量来确定共享内存块的大小。矩阵转置内核使用一个二维线程块网格，其中每个线程块处理输入矩阵的tile_dim x tile_dim块。
- en: The kernel first loads data from the global memory into the shared memory tile.
    Each thread loads a single element from the input matrix into the shared memory
    tile. Then, a **__syncthreads()** barrier ensures that all threads have finished
    loading data into shared memory before proceeding.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 内核首先将全局内存中的数据加载到共享内存块中。每个线程从输入矩阵中加载单个元素到共享内存块中。然后，一个**__syncthreads()**屏障确保所有线程在继续之前已经将数据加载到共享内存中。
- en: Next, the kernel writes the transposed data from the shared memory tile back
    to the output matrix in global memory. Each thread writes a single element from
    the shared memory tile to the output matrix. By using shared memory, this optimized
    implementation reduces global memory accesses and exploits memory coalescence,
    resulting in improved performance compared to a naive transpose implementation.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，内核将共享内存瓷砖中的转置数据写回到全局内存中的输出矩阵。每个线程将共享内存瓷砖中的一个元素写入输出矩阵。通过使用共享内存，这种优化的实现减少了全局内存访问并利用了内存合并，与简单的转置实现相比，性能得到提升。
- en: This kernel achieved on NVIDIA V100 674 GB/s.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 此内核在NVIDIA V100上实现了674 GB/s。
- en: This is pretty close to the bandwidth achieved by the simple copy kernel, but
    there is one more thing to improve.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 这与简单复制内核实现的带宽非常接近，但还有一点可以改进。
- en: Shared memory is composed of banks. Each banks can service only one request
    at the time. Bank conflicts happen when more than 1 thread in a specific warp
    try to access data in bank. The bank conflicts are resolved by serializing the
    accesses resulting in less performance. In the above example when data is saved
    to the shared memory, each thread in the warp will save an element of the data
    in a different one. Assuming that shared memory has 16 banks after writing each
    bank will contain one column. At the last step when we write from the shared memory
    to the global memory each warp load data from the same bank. A simple way to avoid
    this is by just padding the temporary array.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存由银行组成。每个银行一次只能服务一个请求。当特定战中的一个线程尝试访问银行中的数据时，就会发生银行冲突。通过序列化访问来解决银行冲突，导致性能降低。在上面的例子中，当数据保存到共享内存时，战中的每个线程将数据的一个元素保存到不同的银行。假设共享内存有16个银行，在写入后，每个银行将包含一列。在最后一步，当我们从共享内存写入全局内存时，每个战从相同的银行加载数据。避免这种情况的一个简单方法就是填充临时数组。
- en: '[PRE98]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: By padding the array the data is slightly shifting it resulting in no bank conflicts.
    The effective bandwidth for this kernel is 697 GB/s.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 通过填充数组，数据会略微偏移，从而避免了银行冲突。此内核的有效带宽为697 GB/s。
- en: Using sharing memory as a cache - In short
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 将共享内存用作缓存 - 简而言之
- en: Shared memory can significantly improve performance in operations like matrix
    transpose.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以显著提高矩阵转置等操作的性能。
- en: Shared memory reduces global memory accesses and exploits the high bandwidth
    and low latency of shared memory.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存减少了全局内存访问，并利用了共享内存的高带宽和低延迟。
- en: An optimized implementation utilizes shared memory, loads data coalescedly,
    and performs transpose operations.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的实现利用共享内存，合并加载数据，并执行转置操作。
- en: The optimized implementation uses a 2D grid of thread blocks and a shared memory
    tile size determined by a constant.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的实现使用由常量确定的二维线程块网格和共享内存瓷砖大小。
- en: The kernel loads data from global memory into the shared memory tile and uses
    a synchronization barrier.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核将数据从全局内存加载到共享内存瓷砖中，并使用同步屏障。
- en: To avoid bank conflicts in shared memory, padding the temporary array is a simple
    solution.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免共享内存中的银行冲突，填充临时数组是一个简单的解决方案。
- en: Reductions
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少操作
- en: Reductions refer to operations in which the elements of an array are aggregated
    in a single value through operations such as summing, finding the maximum or minimum,
    or performing logical operations.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 减少操作指的是通过求和、查找最大或最小值或执行逻辑运算等操作将数组元素聚合为单个值的操作。
- en: In the serial approach, the reduction is performed sequentially by iterating
    through the collection of values and accumulating the result step by step. This
    will be enough for small sizes, but for big problems this results in significant
    time spent in this part of an application. On a GPU, this approach is not feasible.
    Using just one thread to do this operation means the rest of the GPU is wasted.
    Doing reduction in parallel is a little tricky. In order for a thread to do work,
    it needs to have some partial result to use. If we launch, for example, a kernel
    performing a simple vector summation, `sum[0]+=a[tid]`, with N threads we notice
    that this would result in undefined behaviour. GPUs have mechanisms to access
    the memory and lock the access for other threads while 1 thread is doing some
    operations to a given data via **atomics**, however this means that the memory
    access gets again to be serialized. There is not much gain. We note that when
    doing reductions the order of the iterations is not important (barring the typical
    non-associative behavior of floating-point operations). Also we can we might have
    to divide our problem in several subsets and do the reduction operation for each
    subset separately. On the GPUs, since the GPU threads are grouped in blocks, the
    size of the subset based on that. Inside the block, threads can cooperate with
    each other, they can share data via the shared memory and can be synchronized
    as well. All threads read the data to be reduced, but now we have significantly
    less partial results to deal with. In general, the size of the block ranges from
    256 to 1024 threads. In case of very large problems, after this procedure if we
    are left too many partial results this step can be repeated.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在串行方法中，通过迭代值集合并逐步累积结果来执行减少操作。这对于小规模数据足够，但对于大型问题，这会导致在应用程序的这一部分花费大量时间。在GPU上，这种方法不可行。仅使用一个线程来完成此操作意味着GPU的其余部分都被浪费了。并行执行减少操作有点棘手。为了使线程能够工作，它需要使用一些部分结果。如果我们启动，例如，一个执行简单向量求和的内核，`sum[0]+=a[tid]`，使用N个线程，我们会发现这会导致未定义的行为。GPU有机制通过**原子操作**访问内存并锁定其他线程的访问，然而这又意味着内存访问再次被序列化。这样并没有带来太多收益。我们注意到，在进行减少操作时，迭代顺序并不重要（除了浮点运算的典型非结合行为）。此外，我们可能需要将问题分成几个子集，并对每个子集分别执行减少操作。在GPU上，由于GPU线程被分组在块中，因此子集的大小基于这一点。在块内部，线程可以相互协作，可以通过共享内存共享数据，也可以进行同步。所有线程都读取要减少的数据，但现在我们处理的部分结果显著减少。一般来说，块的大小从256到1024个线程不等。在非常大型问题的案例中，如果在此步骤后留下太多部分结果，则可以重复此步骤。
- en: 'At the block level we still have to perform a reduction in an efficient way.
    Doing it serially means that we are not using all GPU cores (roughly 97% of the
    computing capacity is wasted). Doing it naively parallel using **atomics**, but
    on the shared memory is also not a good option. Going back back to the fact the
    reduction operations are commutative and associative we can set each thread to
    “reduce” two elements of the local part of the array. Shared memory can be used
    to store the partial “reductions” as shown below in the code:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在块级别，我们仍然需要以有效的方式执行减少操作。串行执行意味着我们没有使用所有GPU核心（大约97%的计算能力被浪费了）。在共享内存上使用**原子操作**进行天真地并行操作也不是一个好的选择。回到减少操作是交换律和结合律的事实，我们可以设置每个线程“减少”数组局部部分的两个元素。共享内存可以用来存储如以下代码所示的部分“减少”：
- en: '[PRE100]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: In the kernel we have each GPU performing thread a reduction of two elements
    from the local portion of the array. If we have tpb GPU threads per block, we
    utilize them to store 2xtpb elements in the local shared memory. To ensure synchronization
    until all data is available in the shared memory, we employ the syncthreads()
    function.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，每个GPU执行线程从数组的局部部分减少两个元素。如果我们有每个块tpb个GPU线程，我们利用它们在局部共享内存中存储2xtpb个元素。为了确保同步直到所有数据都可用在共享内存中，我们使用syncthreads()函数。
- en: Next, we instruct each thread to “reduce” the element in the array at threadIdx.x
    with the element at threadIdx.x+tpb. As this operation saves the result back into
    the shared memory, we once again employ syncthreads(). By doing this, we effectively
    halve the number of elements to be reduced.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指示每个线程将数组中threadIdx.x处的元素与threadIdx.x+tpb处的元素“减少”。由于这个操作将结果保存回共享内存，我们再次使用syncthreads()。通过这样做，我们有效地将需要减少的元素数量减半。
- en: This procedure can be repeated, but now we only utilize tpb/2 threads. Each
    thread is responsible for “reducing” the element in the array at threadIdx.x with
    the element at threadIdx.x+tpb/2. After this step, we are left with tpb/4 numbers
    to be reduced. We continue applying this procedure until only one number remains.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以重复，但现在我们只利用tpb/2个线程。每个线程负责“化简”数组中threadIdx.x位置的元素与threadIdx.x+tpb/2位置的元素。在此步骤之后，我们剩下tpb/4个需要化简的数字。我们继续应用此过程，直到只剩下一个数字。
- en: At this point, we can either “reduce” the final number with a global partial
    result using atomic read and write operations, or we can save it into an array
    for further processing.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用全局部分结果通过原子读写操作“化简”最终数字，或者将其保存到数组中以便进一步处理。
- en: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/Reduction.png](../Images/ae3e36ebff0360dc34deab2fec1ff533.png)'
- en: Schematic representation on the reduction algorithm with 8 GPU threads.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 使用8个GPU线程的化简算法的示意图。
- en: For a detail analysis of how to optimize reduction operations in CUDA/HIP check
    this presentation [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细了解如何在CUDA/HIP中优化化简操作，请查看此演示文稿[优化CUDA中的并行化简](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
- en: Reductions - In short
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 化简 - 简而言之
- en: Reductions refer to aggregating elements of an array into a single value through
    operations like summing, finding maximum or minimum, or performing logical operations.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 化简是指通过求和、查找最大值或最小值或执行逻辑运算等操作将数组元素聚合为单个值。
- en: Performing reductions sequentially in a serial approach is inefficient for large
    problems, while parallel reduction on GPUs offers better performance.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在串行方法中对大问题进行顺序化简是不高效的，而在GPU上并行化简则能提供更好的性能。
- en: Parallel reduction on GPUs involves dividing the problem into subsets, performing
    reductions within blocks of threads using shared memory, and repeatedly reducing
    the number of elements (two per GPU thread) until only one remains.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上并行化简包括将问题划分为子集，在线程块中使用共享内存执行化简操作，并反复减少元素数量（每个GPU线程两个）直到只剩下一个。
- en: Overlapping Computations and Memory transfer. CUDA/HIP Streams
  id: totrans-537
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行计算和内存传输。CUDA/HIP流
- en: Modern GPUs can overlap independent operations. They can do transfers between
    CPU and GPU and execute kernels in the same time, or they can execute kernels
    concurrently. CUDA/HIP streams are independent execution units, a sequence of
    operations that execute in issue-order on the GPU. The operations issue in different
    streams can be executed concurrently.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU可以重叠独立操作。它们可以在CPU和GPU之间进行传输并执行内核，或者它们可以并发执行内核。CUDA/HIP流是独立的执行单元，是一系列在GPU上按发布顺序执行的运算。不同流中的操作可以并发执行。
- en: Consider the previous case of vector addition, which involves copying data from
    CPU to GPU, computations and then copying back the result to GPU. In this way
    nothing can be overlap.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑之前的向量加法案例，这涉及到从CPU复制数据到GPU，进行计算然后将结果复制回GPU。这样就不能重叠。
- en: We can improve the performance by dividing the problem in smaller independent
    parts. Let’s consider 5 streams and consider the case where copy in one direction
    and computation take the same amount of time.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将问题划分为更小的独立部分来提高性能。让我们考虑5个流，并考虑复制和计算花费相同时间的案例。
- en: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/StreamsTimeline.png](../Images/7009d1b99d05dfbce47949d3fd27b82d.png)'
- en: After the first and second stream copy data to the GPU, the GPU is practically
    occupied all time. We can see that significant performance improvements can be
    obtained by eliminating the time in which the GPU is idle, waiting for data to
    arrive from the CPU. This very useful for problems where there is often communication
    to the CPU because the GPU memory can not fit all the problem or the application
    runs in a multi-GPU set up and communication is needed often.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次和第二次流将数据复制到GPU后，GPU几乎一直处于忙碌状态。我们可以看到，通过消除GPU空闲等待CPU数据到达的时间，可以获得显著的性能提升。这对于经常需要与CPU通信的问题非常有用，因为GPU内存无法容纳整个问题或应用程序在多GPU设置中运行且需要频繁通信。
- en: We can apply this to the vector addition problem above.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此应用于上述的向量加法问题。
- en: '[PRE102]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Instead of having one copy to gpu, one execution of the kernel and one copy
    back, we now have several of these calls independent of each other.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有多个这些调用，它们彼此独立，而不是只有一个复制到GPU，一个内核执行和一个复制回的操作。
- en: Note that even when streams are not explicitly used it is possible to launch
    all the GPU operations asynchronous and overlap CPU operations (such I/O) and
    GPU operations. In order to learn more about how to improve performance using
    streams check the NVIDIA blog [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使没有明确使用流，也可以异步启动所有GPU操作，并重叠CPU操作（如I/O）和GPU操作。要了解更多关于如何使用流来提高性能的信息，请查看NVIDIA博客[如何在CUDA
    C/C++中重叠数据传输](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)。
- en: Streams - In short
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 流 - 简而言之
- en: CUDA/HIP streams are independent execution contexts on the GPU that allow for
    concurrent execution of operations issued in different streams.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA/HIP流是GPU上的独立执行上下文，允许并发执行不同流中发出的操作。
- en: Using streams can improve GPU performance by overlapping operations such as
    data transfers between CPU and GPU and kernel executions.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流可以通过重叠操作（如CPU和GPU之间的数据传输和内核执行）来提高GPU性能。
- en: By dividing a problem into smaller independent parts and utilizing multiple
    streams, the GPU can avoid idle time, resulting in significant performance improvements,
    especially for problems with frequent CPU communication or multi-GPU setups.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将问题分解成更小的独立部分并利用多个流，GPU可以避免空闲时间，从而实现显著的性能提升，特别是对于频繁与CPU通信或多GPU配置的问题。
- en: Pros and cons of native programming models
  id: totrans-552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原生编程模型的优缺点
- en: 'There are advantages and limitations to CUDA and HIP:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA和HIP都有优点和局限性：
- en: 'CUDA Pros:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA优点：
- en: 'Performance Boost: CUDA is designed for NVIDIA GPUs and delivers excellent
    performance.'
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能提升：CUDA是为NVIDIA GPU设计的，并提供了卓越的性能。
- en: 'Wide Adoption: CUDA is popular, with many resources and tools available.'
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广泛采用：CUDA很受欢迎，有许多资源和工具可用。
- en: 'Mature Ecosystem: NVIDIA provides comprehensive libraries and tools for CUDA
    programming.'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成熟的生态系统：NVIDIA为CUDA编程提供了全面的库和工具。
- en: 'HIP Pros:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: HIP优点：
- en: 'Portability: HIP is portable across different GPU architectures.'
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可移植性：HIP可以在不同的GPU架构之间移植。
- en: 'Open Standards: HIP is based on open standards, making it more accessible.'
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开放标准：HIP基于开放标准，使其更具可访问性。
- en: 'Growing Community: The HIP community is growing, providing more resources and
    support.'
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续增长的社区：HIP社区正在增长，提供更多资源和支持。
- en: 'Cons:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Exclusive for GPUs
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专为GPU设计
- en: 'Vendor Lock-in: CUDA is exclusive to NVIDIA GPUs, limiting compatibility.'
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 供应商锁定：CUDA仅限于NVIDIA GPU，限制了兼容性。
- en: 'Learning Curve: Both CUDA and HIP require learning GPU programming concepts.'
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习曲线：CUDA和HIP都需要学习GPU编程概念。
- en: 'Limited Hardware Support: HIP may face limitations on older or less common
    GPUs.'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件支持有限：HIP可能在较旧或不太常见的GPU上面临限制。
- en: Keypoints
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: CUDA and HIP are two GPU programming models
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA和HIP是两种GPU编程模型
- en: Memory optimizations are very important
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存优化非常重要
- en: Asynchronous launching can be used to overlap operations and avoid idle GPU*
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步启动可用于重叠操作并避免GPU空闲*
