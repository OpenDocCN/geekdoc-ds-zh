- en: 'PrefaceTiming is so very important in technology, as well as in our academic
    and professional careers. We are an extraordinarily lucky generation of programmers
    who have the initial opportunity to capitalize on inexpensive, generally available,
    massively parallel computing hardware. The impact of GPGPU (General-Purpose Graphics
    Processing Units) technology spans all aspects of computation, from the smallest
    cell phones to the largest supercomputers in the world. They are changing the
    commercial application landscape, scientific computing, cloud computing, computer
    visualization, games, and robotics and are even redefining how computer programming
    is taught. Teraflop (trillion floating-point operations per second) computing
    is now within the economic reach of most people around the world. Teenagers, students,
    parents, teachers, professionals, small research organizations, and large corporations
    can easily afford GPGPU hardware and the software development kits (SDKs) are
    free. NVIDIA estimates that more than 300 million of their programmable GPGPU
    devices have already been sold.Programmed in CUDA (Compute Unified Device Architecture),
    those third of a billion NVIDIA GPUs present a tremendous market opportunity for
    commercial applications, and they provide a hardware base with which to redefine
    what is possible for scientific computing. Most importantly, CUDA and massively
    parallel GPGPU hardware is changing how we think about computation. No longer
    limited to performing one or a few operations at a time, CUDA programmers write
    programs that perform many tens of thousands of operations simultaneously!This
    book will teach you how to think in CUDA and harness those tens of thousands of
    threads of execution to achieve orders-of-magnitude increased performance for
    your applications, be they commercial, academic, or scientific. Further, this
    book will explain how to utilize one or more GPGPUs within a single application,
    whether on a single machine or across a cluster of machines. In addition, this
    book will show you how to use CUDA to develop applications that can run on multicore
    processors, making CUDA a viable choice for *all* application development. No
    GPU required!Not concerned with just syntax and API calls, the material in this
    book covers the thought behind the design of CUDA, plus the architectural reasons
    why GPGPU hardware can perform so spectacularly. Various guidelines and caveats
    will be covered so that you can write concise, readable, and maintainable code.
    The focus is on the latest CUDA 4.x release.Working code is provided that can
    be compiled and modified because playing with and adapting code is an essential
    part of the learning process. The examples demonstrate how to get high-performance
    from the Fermi architecture (NVIDIA 20-series) of GPGPUS because the intention
    is not just to get code working but also to show you how to write efficient code.
    Those with older GPGPUs will benefit from this book, as the examples will compile
    and run on all CUDA-enabled GPGPUs. Where appropriate, this book will reference
    text from my extensive *Doctor Dobb''s Journal* series of CUDA tutorials to highlight
    improvements over previous versions of CUDA and to provide insight on how to achieve
    good performance across multiple generations of GPGPU architectures.Teaching materials,
    additional examples, and reader comments are available on the [http://gpucomputing.net](http://gpucomputing.net)
    wiki. Any of the following URLs will access the wiki:■ My name: [http://gpucomputing.net/RobFarber](http://gpucomputing.net/RobFarber).■
    The title of this book as one word: [http://gpucomputing.net/CUDAapplicationdesignanddevelopment](http://gpucomputing.net/CUDAapplicationdesignanddevelopment).■
    The name of my series: [http://gpucomputing.net/supercomputingforthemasses](http://gpucomputing.net/supercomputingforthemasses).Those
    who purchase the book can download the source code for the examples at [http://booksite.mkp.com/9780123884268](http://booksite.mkp.com/9780123884268).To
    accomplish these goals, the book is organized as follows:[Chapter 1](B978012388426800001X.xhtml#B978-0-12-388426-8.00001-X).
    Introduces basic CUDA concepts and the tools needed to build and debug CUDA applications.
    Simple examples are provided that demonstrates both the thrust C++ and C runtime
    APIs. Three simple rules for high-performance GPU programming are introduced.[Chapter
    2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1). Using only techniques
    introduced in [Chapter 1](B978012388426800001X.xhtml#B978-0-12-388426-8.00001-X),
    this chapter provides a complete, general-purpose machine-learning and optimization
    framework that can run 341 times faster than a single core of a conventional processor.
    Core concepts in machine learning and numerical optimization are also covered,
    which will be of interest to those who desire the domain knowledge as well as
    the ability to program GPUs.[Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3).
    Profiling is the focus of this chapter, as it is an essential skill in high-performance
    programming. The CUDA profiling tools are introduced and applied to the real-world
    example from [Chapter 2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1).
    Some surprising bottlenecks in the Thrust API are uncovered. Introductory data-mining
    techniques are discussed and data-mining functors for both Principle Components
    Analysis and Nonlinear Principle Components Analysis are provided, so this chapter
    should be of interest to users as well as programmers.[Chapter 4](B9780123884268000045.xhtml#B978-0-12-388426-8.00004-5).
    The CUDA execution model is the topic of this chapter. Anyone who wishes to get
    peak performance from a GPU must understand the concepts covered in this chapter.
    Examples and profiling output are provided to help understand both what the GPU
    is doing and how to use the existing tools to see what is happening.[Chapter 5](B9780123884268000057.xhtml#B978-0-12-388426-8.00005-7).
    CUDA provides several types of memory on the GPU. Each type of memory is discussed,
    along with the advantages and disadvantages.[Chapter 6](B9780123884268000069.xhtml#B978-0-12-388426-8.00006-9).
    With over three orders-of-magnitude in performance difference between the fastest
    and slowest GPU memory, efficiently using memory on the GPU is the *only* path
    to high performance. This chapter discusses techniques and provides profiler output
    to help you understand and monitor how efficiently your applications use memory.
    A general functor-based example is provided to teach how to write your own generic
    methods like the Thrust API.[Chapter 7](B9780123884268000070.xhtml#B978-0-12-388426-8.00007-0).
    GPUs provide multiple forms of parallelism, including multiple GPUs, asynchronous
    kernel execution, and a Unified Virtual Address (UVA) space. This chapter provides
    examples and profiler output to understand and utilize all forms of GPU parallelism.[Chapter
    8](B9780123884268000082.xhtml#B978-0-12-388426-8.00008-2). CUDA has matured to
    become a viable platform for *all* application development for both GPU and multicore
    processors. Pathways to multiple CUDA backends are discussed, and examples and
    profiler output to effectively run in heterogeneous multi-GPU environments are
    provided. CUDA libraries and how to interface CUDA and GPU computing with other
    high-level languages like Python, Java, R, and FORTRAN are covered.[Chapter 9](B9780123884268000094.xhtml#B978-0-12-388426-8.00009-4).
    With the focus on the use of CUDA to accelerate computational tasks, it is easy
    to forget that GPU technology is also a splendid platform for visualization. This
    chapter discusses primitive restart and how it can dramatically accelerate visualization
    and gaming applications. A complete working example is provided that allows the
    reader to create and fly around in a 3D world. Profiler output is used to demonstrate
    why primitive restart is so fast. The teaching framework from this chapter is
    extended to work with live video streams in [Chapter 12](B9780123884268000124.xhtml#B978-0-12-388426-8.00012-4).[Chapter
    10](B9780123884268000100.xhtml#B978-0-12-388426-8.00010-0). To teach scalability,
    as well as performance, the example from [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)
    is extended to use MPI (Message Passing Interface). A variant of this example
    code has demonstrated near-linear scalability to 500 GPGPUs (with a peak of over
    500,000 single-precision gigaflops) and delivered over one-third petaflop (10^(15)
    floating-point operations per second) using 60,000 x86 processing cores.[Chapter
    11](B9780123884268000112.xhtml#B978-0-12-388426-8.00011-2). No book can cover
    all aspects of the CUDA tidal wave. This is a survey chapter that points the way
    to other projects that provide free working source code for a variety of techniques,
    including Support Vector Machines (SVM), Multi-Dimensional Scaling (MDS), mutual
    information, force-directed graph layout, molecular modeling, and others. Knowledge
    of these projects—and how to interface with other high-level languages, as discussed
    in [Chapter 8](B9780123884268000082.xhtml#B978-0-12-388426-8.00008-2)—will help
    you mature as a CUDA developer.[Chapter 12](B9780123884268000124.xhtml#B978-0-12-388426-8.00012-4).
    A working real-time video streaming example for vision recognition based on the
    visualization framework in [Chapter 9](B9780123884268000094.xhtml#B978-0-12-388426-8.00009-4)
    is provided. All that is needed is an inexpensive webcam or a video file so that
    you too can work with real-time vision recognition. This example was designed
    for teaching, so it is easy to modify. Robotics, augmented reality games, and
    data fusion for heads-up displays are obvious extensions to the working example
    and technology discussion in this chapter.Learning to think about and program
    in CUDA (and GPGPUs) is a wonderful way to have fun and open new opportunities.
    However, performance is the ultimate reason for using GPGPU technology, and as
    one of my university professors used to say, “The proof of the pudding is in the
    tasting.”[Figure 1](#f0010) illustrates the performance of the top 100 applications
    as reported on the NVIDIA CUDA Showcase[¹](#fn0010) as of July 12, 2011\. They
    demonstrate the wide variety of applications that GPGPU technology can accelerate
    by two or more orders of magnitude (100-times) over multi-core processors, as
    reported in the peer-reviewed scientific literature and by commercial entities.
    It is worth taking time to look over these showcased applications, as many of
    them provide freely downloadable source code and libraries.¹[http://developer.nvidia.com/cuda-action-research-apps](http://developer.nvidia.com/cuda-action-research-apps).'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 前言时间在技术领域，以及我们的学术和职业生涯中都非常重要。我们是一代非常幸运的程序员，我们有机会利用价格低廉、普遍可用的、大规模并行计算硬件。GPGPU（通用图形处理单元）技术的影响涵盖了计算的所有方面，从小型手机到世界上最大的超级计算机。它们正在改变商业应用格局、科学计算、云计算、计算机可视化、游戏、机器人和计算机编程的教学方式。每秒万亿浮点运算（Teraflop）的计算现在对世界上大多数人都经济可行。青少年、学生、家长、教师、专业人士、小型研究机构和大型企业都可以轻松负担GPGPU硬件和软件开发工具包（SDK），而且SDK是免费的。NVIDIA估计，他们已经售出了超过3亿台可编程的GPGPU设备。这些十亿分之一的NVIDIA
    GPU用CUDA（计算统一设备架构）编程，为商业应用提供了巨大的市场机会，并为科学计算提供了重新定义可能性的硬件基础。最重要的是，CUDA和大规模并行GPGPU硬件正在改变我们关于计算的看法。不再局限于一次执行一个或几个操作，CUDA程序员编写的是同时执行数万次操作的程序！本书将教会你如何在CUDA中思考，并利用这些数万条执行线程来为你的应用程序实现数量级的性能提升，无论是商业的、学术的还是科学的。此外，本书将解释如何在一个应用程序中利用一个或多个GPGPU，无论是在单台机器上还是在机器集群中。此外，本书还将向你展示如何使用CUDA开发可以在多核处理器上运行的应用程序，使CUDA成为所有应用开发的可行选择！无需GPU！本书不仅关注语法和API调用，还涵盖了CUDA设计的思想以及GPGPU硬件能够如此出色地执行的原因。本书将涵盖各种指南和注意事项，以便你可以编写简洁、可读性和可维护的代码。重点是最新版本的CUDA
    4.x。提供了可编译和修改的示例代码，因为玩耍和适应代码是学习过程中的一个基本部分。这些示例演示了如何从GPGPU的Fermi架构（NVIDIA 20系列）中获得高性能，因为目的不仅仅是让代码工作，还要向你展示如何编写高效的代码。那些拥有较老GPGPU的人也会从这本书中受益，因为示例可以在所有支持CUDA的GPGPU上编译和运行。在适当的情况下，本书将参考我的广泛的《Doctor
    Dobb's Journal》CUDA教程系列中的文本，以突出CUDA先前版本中的改进，并提供如何在多代GPGPU架构之间实现良好性能的见解。教学材料、附加示例和读者评论可在[http://gpucomputing.net](http://gpucomputing.net)维基百科上找到。以下任何URL都可以访问维基百科：■
    我的名字：[http://gpucomputing.net/RobFarber](http://gpucomputing.net/RobFarber)。■
    本书标题作为一个单词：[http://gpucomputing.net/CUDAapplicationdesignanddevelopment](http://gpucomputing.net/CUDAapplicationdesignanddevelopment)。■
    我系列的名字：[http://gpucomputing.net/supercomputingforthemasses](http://gpucomputing.net/supercomputingforthemasses)。购买本书的读者可以在[http://booksite.mkp.com/9780123884268](http://booksite.mkp.com/9780123884268)下载示例代码。为了实现这些目标，本书的组织结构如下：[第1章](B978012388426800001X.xhtml#B978-0-12-388426-8.00001-X)。介绍了基本的CUDA概念和构建和调试CUDA应用程序所需的工具。提供了简单的示例，演示了thrust
    C++和C运行时API。介绍了高性能GPU编程的三个简单规则。[第2章](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1)。仅使用第1章中介绍的技术，本章提供了一个完整、通用的机器学习和优化框架，其速度比传统处理器的单核快341倍。还涵盖了机器学习和数值优化的核心概念，这对于那些希望获得领域知识以及编程GPU能力的人来说将是有趣的。[第3章](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)。本章的重点是性能分析，因为它是高性能编程的一个基本技能。介绍了CUDA性能分析工具，并将其应用于第2章中的真实世界示例。揭示了Thrust
    API中一些令人惊讶的瓶颈。讨论了数据挖掘技术入门，并提供了主成分分析和非线性主成分分析的数据挖掘算子，因此本章对用户和程序员都应感兴趣。[第4章](B9780123884268000045.xhtml#B978-0-12-388426-8.00004-5)。本章的主题是CUDA执行模型。任何希望从GPU中获得最佳性能的人都必须了解本章中涵盖的概念。提供了示例和性能分析输出，以帮助理解GPU正在做什么以及如何使用现有工具查看正在发生的事情。[第5章](B9780123884268000057.xhtml#B978-0-12-388426-8.00005-7)。CUDA在GPU上提供了多种类型的内存。讨论了每种类型的内存，以及其优缺点。[第6章](B9780123884268000069.xhtml#B978-0-12-388426-8.00006-9)。最快的GPU内存和最慢的GPU内存之间性能差异超过三个数量级，因此高效地使用GPU内存是获得高性能的唯一途径。本章讨论了技术和提供了性能分析输出，以帮助您了解和监控您的应用程序如何高效地使用内存。提供了一个基于泛函的通用示例，以教授如何编写自己的类似于Thrust
    API的通用方法。[第7章](B9780123884268000070.xhtml#B978-0-12-388426-8.00007-0)。GPU提供了多种并行形式，包括多个GPU、异步内核执行和统一虚拟地址（UVA）空间。本章提供了示例和性能分析输出，以帮助理解和利用所有形式的GPU并行性。[第8章](B9780123884268000082.xhtml#B978-0-12-388426-8.00008-2)。CUDA已经成熟，成为GPU和多核处理器上所有应用开发的可行平台。讨论了通往多个CUDA后端的途径，并提供了示例和性能分析输出，以有效地在异构多GPU环境中运行。涵盖了CUDA库以及如何将CUDA和GPU计算与Python、Java、R和FORTRAN等高级语言接口。[第9章](B9780123884268000094.xhtml#B978-0-12-388426-8.00009-4)。本章的重点是使用CUDA加速计算任务，很容易忘记GPU技术也是一个出色的可视化平台。本章讨论了原始重启及其如何显著加速可视化和游戏应用程序。提供了一个完整的可工作示例，允许读者创建并飞越一个3D世界。使用性能分析输出来演示为什么原始重启如此之快。本章的教学框架扩展到在[第12章](B9780123884268000124.xhtml#B978-0-12-388426-8.00012-4)中处理实时视频流。[第10章](B9780123884268000100.xhtml#B978-0-12-388426-8.00010-0)。为了教授可扩展性和性能，第3章中的示例扩展到使用MPI（消息传递接口）。这个示例代码的变体已经证明了接近线性的可扩展性，扩展到500个GPGPU（峰值超过500,000个单精度千兆浮点运算），并使用60,000个x86处理核心提供了超过三分之一的Petaflop（每秒10的15次方浮点运算）。[第11章](B9780123884268000112.xhtml#B978-0-12-388426-8.00011-2)。任何一本书都无法涵盖CUDA浪潮的所有方面。这是一章概述，指出了其他提供免费工作源代码的项目，包括支持向量机（SVM）、多维缩放（MDS）、互信息、力导向图布局、分子建模等。了解这些项目以及如何与其他高级语言接口（如第8章中讨论的）将有助于你成为一名成熟的CUDA开发者。[第12章](B9780123884268000124.xhtml#B978-0-12-388426-8.00012-4)。提供了一个基于第9章中可视化框架的实时视频流工作示例，用于基于视觉识别。所有你需要的是一台廉价的网络摄像头或视频文件，这样你也可以处理实时视觉识别。这个示例是为了教学而设计的，因此很容易修改。本章中工作示例和技术讨论的明显扩展包括机器人技术、增强现实游戏和抬头显示器的数据融合。[学习如何在CUDA（和GPGPU）中思考和编程是一种很好的娱乐方式，也是开拓新机会的好方法。然而，性能是使用GPGPU技术的最终原因，正如我的大学教授经常说的，“甜点的证明在于品尝。”[图1](#f0010)展示了截至2011年7月12日NVIDIA
    CUDA展示区报告的前100个应用程序的性能。[¹](#fn0010)它们展示了GPGPU技术如何通过两到三个数量级（100倍）的速度比多核处理器加速各种应用程序，如同行评审的科学文献和商业实体所报告。花时间查看这些展示的应用程序是值得的，因为其中许多提供了免费下载的源代码和库。¹[http://developer.nvidia.com/cuda-action-research-apps](http://developer.nvidia.com/cuda-action-research-apps)。
- en: '| ![B9780123884268000203/pre01-9780123884268.jpg is missing](B9780123884268000203/pre01-9780123884268.jpg)
    |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000203/pre01-9780123884268.jpg is missing](B9780123884268000203/pre01-9780123884268.jpg)
    |'
- en: '| **Figure 1**Top 100 NVIDIA application showcase speedups. |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '| **图1**NVIDIA应用展示加速的前100名。 |'
- en: GPGPU technology is a disruptive technology that has redefined how computation
    occurs. As NVIDIA notes, “from super phones to supercomputers.” This technology
    has arrived during a perfect storm of opportunities, as traditional multicore
    processors can no longer achieve significant speedups through increases in clock
    rate. The only way manufacturers of traditional processors can entice customers
    to upgrade to a new computer is to deliver speedups two to four times faster through
    the parallelism of dual- and quad-core processors. Multicore parallelism is disruptive,
    as it requires that existing software be rewritten to make use of these extra
    cores. Come join the cutting edge of software application development and research
    as the computer and research industries retool to exploit parallel hardware! Learn
    CUDA and join in this wonderful opportunity.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: GPGPU技术是一项颠覆性技术，它重新定义了计算的方式。正如NVIDIA所指出的，“从超级手机到超级计算机”。这项技术在一个完美的时机到来，因为传统的多核处理器已经无法通过提高时钟频率来实现显著的加速。传统处理器的制造商唯一能够吸引客户升级到新电脑的方法，就是通过双核和四核处理器的并行性提供两到四倍的加速。多核并行性是颠覆性的，因为它要求现有的软件被重写，以利用这些额外的核心。随着计算机和研究行业重新调整以利用并行硬件，快来加入软件应用开发和研究的前沿吧！学习CUDA，抓住这个绝佳的机会。
