- en: 8.7\. Online supplementary materials#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.7\. 在线补充材料#
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/supp/roch-mmids-nn-supp.html](https://mmids-textbook.github.io/chap08_nn/supp/roch-mmids-nn-supp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap08_nn/supp/roch-mmids-nn-supp.html](https://mmids-textbook.github.io/chap08_nn/supp/roch-mmids-nn-supp.html)
- en: 8.7.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7.1\. 测验、解答、代码等.[#](#quizzes-solutions-code-etc "链接到本标题")
- en: 8.7.1.1\. Just the code[#](#just-the-code "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.1\. 仅代码[#](#just-the-code "链接到本标题")
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下面可以访问本章代码的交互式Jupyter笔记本（推荐使用Google Colab）。鼓励您对其进行实验。一些建议的计算练习散布其中。笔记本也可以作为幻灯片查看。
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb)
    ([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))'
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[幻灯片](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html)'
- en: 8.7.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.2\. 自我评估测验[#](#self-assessment-quizzes "链接到本标题")
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下链接可以访问自我评估测验的更全面网络版本。
- en: '[Section 8.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.2节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html)'
- en: '[Section 8.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.3节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html)'
- en: '[Section 8.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.4节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html)'
- en: '[Section 8.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.5节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html)'
- en: 8.7.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.3\. 自动测验[#](#auto-quizzes "链接到本标题")
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在此处访问本章的自动生成的测验（推荐使用Google Colab）。
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb))'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动测验](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb)
    ([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb))'
- en: 8.7.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.4\. 奇数编号预热练习的解答[#](#solutions-to-odd-numbered-warm-up-exercises "链接到本标题")
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*(在克劳德、双子星和ChatGPT的帮助下)*'
- en: '**E8.2.1** The vectorization is obtained by stacking the columns of \(A\):
    \(\text{vec}(A) = (2, 0, 1, -1)\).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.1** 通过堆叠矩阵 \(A\) 的列来获得向量化：\(\text{vec}(A) = (2, 0, 1, -1)\)。'
- en: '**E8.2.3**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.3**'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \cdot B & 2 \cdot B \\ -1 \cdot
    B & 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 & -1 & 6 & -2 \\ 2 & 1 & 4 & 2
    \\ -3 & 1 & -6 & 2 \\ -2 & -1 & -4 & -2 \end{pmatrix}. \end{split}\]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \cdot B & 2 \cdot B \\ -1 \cdot
    B & 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 & -1 & 6 & -2 \\ 2 & 1 & 4 & 2
    \\ -3 & 1 & -6 & 2 \\ -2 & -1 & -4 & -2 \end{pmatrix}. \end{split}\]
- en: '**E8.2.5**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.5**'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 & 6 \\ 7 &
    8 \end{pmatrix} & 2 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} \\ 3 \begin{pmatrix}
    5 & 6 \\ 7 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14 & 16 \\ 15 & 18
    & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 & 6 \\ 7 &
    8 \end{pmatrix} & 2 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} \\ 3 \begin{pmatrix}
    5 & 6 \\ 7 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14 & 16 \\ 15 & 18
    & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix}. \end{split}\]
- en: '**E8.2.7** First, compute the Jacobian matrices of \(\mathbf{f}\) and \(\mathbf{g}\):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.7** 首先，计算 \(\mathbf{f}\) 和 \(\mathbf{g}\) 的雅可比矩阵：'
- en: \[\begin{split} J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x & 2y \\ y & x \end{pmatrix},
    \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v & u \\ 1 & 1 \end{pmatrix}. \end{split}\]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x & 2y \\ y & x \end{pmatrix},
    \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v & u \\ 1 & 1 \end{pmatrix}. \end{split}\]
- en: Then, by the Chain Rule,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据链式法则，
- en: \[\begin{split} J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1,
    2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix}
    2 & 5 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 2 & 4 \\ 2 & 1 \end{pmatrix} = \begin{pmatrix}
    14 & 13 \\ 4 & 5 \end{pmatrix}. \end{split}\]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1,
    2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix}
    2 & 5 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 2 & 4 \\ 2 & 1 \end{pmatrix} = \begin{pmatrix}
    14 & 13 \\ 4 & 5 \end{pmatrix}. \end{split}\]
- en: '**E8.2.9** From E8.2.5, we have'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.9** 从 E8.2.5，我们有'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14
    & 16 \\ 15 & 18 & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix} \end{split}\]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14
    & 16 \\ 15 & 18 & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix} \end{split}\]
- en: So,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 &
    18 & 24 \\ 10 & 14 & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 &
    18 & 24 \\ 10 & 14 & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
- en: Now,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，
- en: \[\begin{split} A^T = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \quad B^T
    = \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \end{split}\]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^T = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \quad B^T
    = \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \end{split}\]
- en: So,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split} A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 & 7 \\
    6 & 8 \end{pmatrix} & 3 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \\ 2 \begin{pmatrix}
    5 & 7 \\ 6 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 & 18 & 24 \\ 10 & 14
    & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 & 7 \\
    6 & 8 \end{pmatrix} & 3 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \\ 2 \begin{pmatrix}
    5 & 7 \\ 6 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 & 18 & 24 \\ 10 & 14
    & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
- en: We see that \((A \otimes B)^T = A^T \otimes B^T\), as expected from the properties
    of the Kronecker product.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 \((A \otimes B)^T = A^T \otimes B^T\)，正如克罗内克积的性质所预期的那样。
- en: '**E8.2.11**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.11**'
- en: \[\begin{split} \nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial
    x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix}
    = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}. \end{split}\]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial
    x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix}
    = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}. \end{split}\]
- en: So,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split} \nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
    \end{split}\]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
    \end{split}\]
- en: '**E8.2.13** First, compute the gradient of \(f\) and the Jacobian matrix of
    \(\mathbf{g}\):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.13** 首先，计算 \(f\) 的梯度以及 \(\mathbf{g}\) 的雅可比矩阵：'
- en: \[\begin{split} \nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad
    J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}. \end{split}\]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad
    J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}. \end{split}\]
- en: Then, by the Chain Rule,
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据链式法则，
- en: \[\begin{split} J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T
    \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix}
    4 & 1 \end{pmatrix} \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix} = \begin{pmatrix}
    8 & 4 \end{pmatrix}. \end{split}\]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T
    \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix}
    4 & 1 \end{pmatrix} \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix} = \begin{pmatrix}
    8 & 4 \end{pmatrix}. \end{split}\]
- en: '**E8.2.15** The Jacobian matrix of \(\mathbf{g}\) is'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.15** \(\mathbf{g}\) 的雅可比矩阵为'
- en: \[\begin{split} J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) & 0 & 0 \\ 0
    & f'(y) & 0 \\ 0 & 0 & f'(z) \end{pmatrix} \end{split}\]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) & 0 & 0 \\ 0
    & f'(y) & 0 \\ 0 & 0 & f'(z) \end{pmatrix} \end{split}\]
- en: where \(f'(x) = \cos(x).\) So,
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(f'(x) = \cos(x).\) 因此，
- en: \[\begin{split} J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6})
    = \begin{pmatrix} \cos(\frac{\pi}{2}) & 0 & 0 \\ 0 & \cos(\frac{\pi}{4}) & 0 \\
    0 & 0 & \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & \frac{\sqrt{2}}{2}
    & 0 \\ 0 & 0 & \frac{\sqrt{3}}{2} \end{pmatrix}. \end{split}\]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6})
    = \begin{pmatrix} \cos(\frac{\pi}{2}) & 0 & 0 \\ 0 & \cos(\frac{\pi}{4}) & 0 \\
    0 & 0 & \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & \frac{\sqrt{2}}{2}
    & 0 \\ 0 & 0 & \frac{\sqrt{3}}{2} \end{pmatrix}. \end{split}\]
- en: '**E8.3.1** Each entry of \(AB\) is the dot product of a row of \(A\) and a
    column of \(B\), which takes 2 multiplications and 1 addition. Since \(AB\) has
    4 entries, the total number of operations is \(4 \times 3 = 12\).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.1** \(AB\) 的每个元素是 \(A\) 的某一行与 \(B\) 的某一列的点积，这需要 2 次乘法和 1 次加法。由于 \(AB\)
    有 4 个元素，总的操作次数是 \(4 \times 3 = 12\)。'
- en: '**E8.3.3** We have'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.3** 我们有'
- en: \[ \ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2 \]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2 \]
- en: so the partial derivatives are \(\frac{\partial \ell}{\partial \hat{y}_1} =
    2 \hat{y}_1\) and \(\frac{\partial \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\) and
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偏导数为 \(\frac{\partial \ell}{\partial \hat{y}_1} = 2 \hat{y}_1\) 和 \(\frac{\partial
    \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\)。
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T. \]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T. \]
- en: '**E8.3.5** From E8.3.4, we have \(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0)
    = (-1, -1)\). Then, \(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\) and
    \(f(\mathbf{x}) = \ell(\mathbf{z}_2) = 5\). By the *Chain Rule*,'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.5** 从 E8.3.4，我们得到 \(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0) = (-1,
    -1)\)。然后，\(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\) 和 \(f(\mathbf{x})
    = \ell(\mathbf{z}_2) = 5\)。根据**链式法则**，'
- en: \[\begin{split} \nabla f(\mathbf{x})^T = J_f(\mathbf{x}) = J_{\ell}(\mathbf{z}_1)
    \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) = 2 \mathbf{z}_2^T
    \begin{pmatrix} -1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ -1 & 0
    \end{pmatrix} = (-10, -4) \begin{pmatrix} 1 & 2 \\ -1 & 0 \end{pmatrix} = (6,
    -20). \end{split}\]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(\mathbf{x})^T = J_f(\mathbf{x}) = J_{\ell}(\mathbf{z}_1)
    \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) = 2 \mathbf{z}_2^T
    \begin{pmatrix} -1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ -1 & 0
    \end{pmatrix} = (-10, -4) \begin{pmatrix} 1 & 2 \\ -1 & 0 \end{pmatrix} = (6,
    -20). \end{split}\]
- en: '**E8.3.7** We have'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.7** 我们有'
- en: \[ g_1(\mathbf{z}_1, \mathbf{w}_1) = w_4 z_{1,1} + w_5 z_{1,2} \]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[ g_1(\mathbf{z}_1, \mathbf{w}_1) = w_4 z_{1,1} + w_5 z_{1,2} \]
- en: so, by computing all partial derivatives,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过计算所有偏导数，
- en: \[ J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) = \begin{pmatrix} w_4 & w_5 & z_{1,1}
    & z_{1,2} \end{pmatrix} = \begin{pmatrix} \mathbf{w}_1^T & \mathbf{z}_1^T \end{pmatrix}
    = \begin{pmatrix} W_1 & I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}. \]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) = \begin{pmatrix} w_4 & w_5 & z_{1,1}
    & z_{1,2} \end{pmatrix} = \begin{pmatrix} \mathbf{w}_1^T & \mathbf{z}_1^T \end{pmatrix}
    = \begin{pmatrix} W_1 & I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}. \]
- en: Using the notation in the text, \(A_1 = W_1\) and \(B_1 = \mathbf{z}_1^T = I_{1
    \times 1} \otimes \mathbf{z}_1^T\).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本中的符号，\(A_1 = W_1\) 和 \(B_1 = \mathbf{z}_1^T = I_{1 \times 1} \otimes \mathbf{z}_1^T\).
- en: '**E8.3.9** We have'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.9** 我们有'
- en: \[ f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2 \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2 \]
- en: so, by the *Chain Rule*, the partial derivatives are
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据**链式法则**，偏导数为
- en: \[ \frac{\partial f}{\partial w_0} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))
    (-w_4) \]\[ \frac{\partial f}{\partial w_1} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2
    + w_3)) (w_4) \]\[ \frac{\partial f}{\partial w_2} = 2(w_4 (- w_0 + w_1) + w_5
    (-w_2 + w_3)) (-w_5) \]\[ \frac{\partial f}{\partial w_3} = 2(w_4 (- w_0 + w_1)
    + w_5 (-w_2 + w_3)) (w_5) \]\[ \frac{\partial f}{\partial w_4} = 2(w_4 (- w_0
    + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1) \]\[ \frac{\partial f}{\partial w_5} =
    2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3). \]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f}{\partial w_0} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))
    (-w_4) \]\[ \frac{\partial f}{\partial w_1} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2
    + w_3)) (w_4) \]\[ \frac{\partial f}{\partial w_2} = 2(w_4 (- w_0 + w_1) + w_5
    (-w_2 + w_3)) (-w_5) \]\[ \frac{\partial f}{\partial w_3} = 2(w_4 (- w_0 + w_1)
    + w_5 (-w_2 + w_3)) (w_5) \]\[ \frac{\partial f}{\partial w_4} = 2(w_4 (- w_0
    + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1) \]\[ \frac{\partial f}{\partial w_5} =
    2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3). \]
- en: Moreover, by E8.3.7,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据 E8.3.7，
- en: \[\begin{split} z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1) = W_1 \mathbf{z}_1 = \begin{pmatrix}
    w_4 & w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix} = w_4
    (- w_0 + w_1) + w_5 (-w_2 + w_3). \end{split}\]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1) = W_1 \mathbf{z}_1 = \begin{pmatrix}
    w_4 & w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix} = w_4
    (- w_0 + w_1) + w_5 (-w_2 + w_3). \end{split}\]
- en: By the fundamental recursion and the results in E8.3.3 and E8.3.8,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基本递归和E8.3.3和E8.3.8的结果，
- en: \[\begin{align*} J_f(\mathbf{w}) &= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
    = 2 z_2 \begin{pmatrix} A_1 B_0 & B_1\end{pmatrix}\\ &= 2 (w_4 (- w_0 + w_1) +
    w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3). \end{align*}\]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} J_f(\mathbf{w}) &= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
    = 2 z_2 \begin{pmatrix} A_1 B_0 & B_1\end{pmatrix}\\ &= 2 (w_4 (- w_0 + w_1) +
    w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3). \end{align*}\]
- en: '**E8.4.1** The full gradient descent step is:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.1** 完整的梯度下降步长为：'
- en: \[ \frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1,
    2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}). \]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1,
    2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}). \]
- en: 'The expected SGD step with a batch size of 2 is:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 批大小为2的期望SGD步长为：
- en: \[ \mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] =
    \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
    \]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] =
    \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
    \]
- en: which is equal to the full gradient descent step, as proven in the “Expected
    SGD Step” lemma.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这等于完整的梯度下降步长，正如在“期望SGD步长”引理中证明的那样。
- en: '**E8.4.3**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.3**'
- en: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= \sum_{i=1}^3 p_i \log
    \frac{p_i}{q_i} \\ &= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5
    \log \frac{0.5}{0.5} \\ &\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5
    \cdot 0 \\ &\approx 0.0525. \end{align*}\]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= \sum_{i=1}^3 p_i \log
    \frac{p_i}{q_i} \\ &= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5
    \log \frac{0.5}{0.5} \\ &\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5
    \cdot 0 \\ &\approx 0.0525. \end{align*}\]
- en: '**E8.4.5** The SGD update is given by'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.5** SGD更新由'
- en: \[\begin{align*} w &\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial
    \ell}{\partial w}(w, b; x_i, y_i), \\ b &\leftarrow b - \frac{\alpha}{|B|} \sum_{i
    \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i). \end{align*}\]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} w &\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial
    \ell}{\partial w}(w, b; x_i, y_i), \\ b &\leftarrow b - \frac{\alpha}{|B|} \sum_{i
    \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i). \end{align*}\]
- en: Plugging in the values, we get
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将值代入，我们得到
- en: \[\begin{align*} w &\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 -
    3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\ b &\leftarrow 0 - \frac{0.1}{2} (2(2
    \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3. \end{align*}\]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} w &\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 -
    3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\ b &\leftarrow 0 - \frac{0.1}{2} (2(2
    \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3. \end{align*}\]
- en: '**E8.4.7**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.7**'
- en: \[\begin{align*} \nabla \ell(w; x, y) &= -\frac{y}{\sigma(wx)} \sigma'(wx) x
    + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\ &= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx)
    \\ &= x(\sigma(wx) - y). \end{align*}\]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla \ell(w; x, y) &= -\frac{y}{\sigma(wx)} \sigma'(wx) x
    + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\ &= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx)
    \\ &= x(\sigma(wx) - y). \end{align*}\]
- en: We used the fact that \(\sigma'(z) = \sigma(z)(1-\sigma(z))\).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了\(\sigma'(z) = \sigma(z)(1-\sigma(z))\)的事实。
- en: '**E8.4.9** First, we compute \(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix}
    0 & 0 \\ 0 & 0 \\ 0 & 0 \end{pmatrix} (1, 2) = (0, 0, 0)\). Then, \(\hat{\mathbf{y}}
    = \boldsymbol{\gamma}(\mathbf{z}_1) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\).
    From the text, we have:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.9** 首先，我们计算\(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix} 0 & 0 \\ 0
    & 0 \\ 0 & 0 \end{pmatrix} (1, 2) = (0, 0, 0)\)。然后，\(\hat{\mathbf{y}} = \boldsymbol{\gamma}(\mathbf{z}_1)
    = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)。从文本中，我们有：'
- en: \[\begin{split} \nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y})
    \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3},
    \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3}
    \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}. \end{split}\]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y})
    \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3},
    \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3}
    \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}. \end{split}\]
- en: '**E8.4.11** First, we compute the individual gradients:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.11** 首先，我们计算各个梯度：'
- en: \[\begin{align*} \nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &= (\boldsymbol{\gamma}(W
    \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3})
    \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3}
    \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}, \\ \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)
    &= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3},
    \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} & \frac{2}{3}
    \\ \frac{4}{3} & -\frac{1}{3} \\ \frac{4}{3} & -\frac{1}{3} \end{pmatrix}. \end{align*}\]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &= (\boldsymbol{\gamma}(W
    \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3})
    \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3}
    \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}, \\ \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)
    &= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3},
    \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} & \frac{2}{3}
    \\ \frac{4}{3} & -\frac{1}{3} \\ \frac{4}{3} & -\frac{1}{3} \end{pmatrix}. \end{align*}\]
- en: 'Then, the full gradient is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，完整的梯度为：
- en: \[\begin{split} \frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla
    f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3}
    & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}
    + \begin{pmatrix} -\frac{8}{3} & \frac{2}{3} \\ \frac{4}{3} & -\frac{1}{3} \\
    \frac{4}{3} & -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6}
    & \frac{2}{3} \\ \frac{5}{6} & \frac{1}{6} \\ \frac{1}{3} & -\frac{5}{6} \end{pmatrix}.
    \end{split}\]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla
    f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3}
    & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}
    + \begin{pmatrix} -\frac{8}{3} & \frac{2}{3} \\ \frac{4}{3} & -\frac{1}{3} \\
    \frac{4}{3} & -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6}
    & \frac{2}{3} \\ \frac{5}{6} & \frac{1}{6} \\ \frac{1}{3} & -\frac{5}{6} \end{pmatrix}.
    \end{split}\]
- en: '**E8.4.13** The cross-entropy loss is given by'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.13** 交叉熵损失由以下公式给出'
- en: \[ -\log(0.3) \approx 1.204. \]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: \[ -\log(0.3) \approx 1.204. \]
- en: '**E8.5.1** \(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\) \(\sigma(-1) =
    \frac{1}{1 + e^{1}} \approx 0.27\) \(\sigma(2) = \frac{1}{1 + e^{-2}} \approx
    0.88\)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.1** \(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\) \(\sigma(-1) =
    \frac{1}{1 + e^{1}} \approx 0.27\) \(\sigma(2) = \frac{1}{1 + e^{-2}} \approx
    0.88\)'
- en: '**E8.5.3** \(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx
    (0.73, 0.27, 0.88)\) \(\bsigma''(\mathbf{z}) = (\bsigma''(1), \bsigma''(-1), \bsigma''(2))
    \approx (0.20, 0.20, 0.10)\)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.3** \(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx
    (0.73, 0.27, 0.88)\) \(\bsigma''(\mathbf{z}) = (\bsigma''(1), \bsigma''(-1), \bsigma''(2))
    \approx (0.20, 0.20, 0.10)\)'
- en: '**E8.5.5** \(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\), so \(\sigma(W\mathbf{x})
    \approx (0.27, 0.98)\), \(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x})
    \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 & 0 \\ 0 & 0.02
    \end{pmatrix}\)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.5** \(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\)，因此 \(\sigma(W\mathbf{x})
    \approx (0.27, 0.98)\)，\(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x})
    \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 & 0 \\ 0 & 0.02
    \end{pmatrix}\)'
- en: '**E8.5.7** \(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2})
    = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.7** \(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2})
    = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)'
- en: 8.7.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.5\. 学习成果[#](#learning-outcomes "链接到这个标题")
- en: Define the Jacobian matrix and use it to compute the differential of a vector-valued
    function.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义雅可比矩阵并使用它来计算向量值函数的微分。
- en: State and apply the generalized Chain Rule to compute the Jacobian of a composition
    of functions.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述并应用广义链式法则来计算函数复合的雅可比矩阵。
- en: Perform calculations with the Hadamard and Kronecker products of matrices.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用矩阵的 Hadamard 和 Kronecker 积进行计算。
- en: Describe the purpose of automatic differentiation and its advantages over symbolic
    and numerical differentiation.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述自动微分的用途及其相对于符号和数值微分的优势。
- en: Implement automatic differentiation in PyTorch to compute gradients of vector-valued
    functions.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现自动微分以计算向量值函数的梯度。
- en: Derive the Chain Rule for multi-layer progressive functions and apply it to
    compute gradients.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导多层递进函数的链式法则并将其应用于计算梯度。
- en: Compare and contrast the forward and reverse modes of automatic differentiation
    in terms of computational complexity.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较自动微分的正向和反向模式的计算复杂度。
- en: Define progressive functions and identify their key properties.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义递进函数并识别它们的关键属性。
- en: Derive the fundamental recursion for the Jacobian of a progressive function.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导递进函数雅可比矩阵的基本递归公式。
- en: Implement the backpropagation algorithm to efficiently compute gradients of
    progressive functions.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现反向传播算法以有效地计算渐进函数的梯度。
- en: Analyze the computational complexity of the backpropagation algorithm in terms
    of the number of matrix-vector products.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析反向传播算法的计算复杂度，以矩阵-向量乘法的数量来衡量。
- en: Describe the stochastic gradient descent (SGD) algorithm and explain how it
    differs from standard gradient descent.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述随机梯度下降（SGD）算法，并解释它与标准梯度下降有何不同。
- en: Derive the update rule for stochastic gradient descent from the gradient of
    the loss function.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从损失函数的梯度推导出随机梯度下降的更新规则。
- en: Prove that the expected SGD step is equal to the full gradient descent step.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明预期的 SGD 步长等于完整梯度下降步长。
- en: Evaluate the performance of models trained using stochastic gradient descent
    on real-world datasets.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估在真实世界数据集上使用随机梯度下降训练的模型性能。
- en: Define the multilayer perceptron (MLP) architecture and describe the role of
    affine maps and activation functions in each layer.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义多层感知器（MLP）架构，并描述每个层中仿射映射和激活函数的作用。
- en: Compute the Jacobian of the sigmoid activation function using properties of
    diagonal matrices and Kronecker products.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对角矩阵和克罗内克积的性质计算 sigmoid 激活函数的雅可比矩阵。
- en: Apply the chain rule to calculate the gradient of the loss function with respect
    to the weights in a small MLP example.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个小型 MLP 示例中应用链式法则来计算损失函数相对于权重的梯度。
- en: Generalize the gradient computation for an MLP with an arbitrary number of layers
    using a forward and backward pass.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前向和反向传递泛化具有任意层数的 MLP 的梯度计算。
- en: Implement the training of a neural network using PyTorch.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现神经网络的训练。
- en: \(\aleph\)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \(\aleph\)
- en: 8.7.2\. Additional sections[#](#additional-sections "Link to this heading")
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7.2\. 其他部分[#](#additional-sections "链接到这个标题")
- en: '8.7.2.1\. Another example: linear regression[#](#another-example-linear-regression
    "Link to this heading")'
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2.1\. 另一个例子：线性回归[#](#another-example-linear-regression "链接到这个标题")
- en: We give another concrete example of progressive functions and of the application
    of backpropagration and stochastic gradient descent.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给出了另一个渐进函数和反向传播及随机梯度下降应用的实例。
- en: '**Computing the gradient** While we have motivated the framework introduced
    in the previous section from the point of view of classification, it also immediately
    applies to the regression setting. Both classification and regression are instances
    of supervised learning.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算梯度** 当我们从分类的角度出发，已经激励了上一节中引入的框架，它也立即适用于回归设置。分类和回归都是监督学习的实例。'
- en: We first compute the gradient of a single sample. Here \(\mathbf{x} \in \mathbb{R}^d\)
    again, but \(y\) is a real-valued outcome variable. We revisit the case of linear
    regression where the loss function is
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算单个样本的梯度。这里 \(\mathbf{x} \in \mathbb{R}^d\)，但 \(y\) 是一个实值结果变量。我们回顾线性回归的案例，其中损失函数是
- en: \[ \ell(z) = (z - y)^2 \]
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(z) = (z - y)^2 \]
- en: and the regression function only has input and output layers and no hidden layer
    (that is, \(L=0\)) with
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 并且回归函数只有输入层和输出层，没有隐藏层（即 \(L=0\)），
- en: \[ h(\mathbf{w}) = \sum_{j=1}^d w_{j} x_{j} = \mathbf{x}^T\mathbf{w}, \]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h(\mathbf{w}) = \sum_{j=1}^d w_{j} x_{j} = \mathbf{x}^T\mathbf{w}, \]
- en: where \(\mathbf{w} \in \mathbb{R}^{d}\) are the parameters. Recall that we can
    include a constant term (one that does not depend on the input) by adding a \(1\)
    to the input. To keep the notation simple, we assume that this pre-processing
    has already been performed if desired.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w} \in \mathbb{R}^{d}\) 是参数。回想一下，我们可以通过向输入添加一个常数项（一个不依赖于输入的项）来包含一个常数项。为了简化符号，我们假设如果需要，这个预处理已经完成。
- en: Finally, the objective function for a single sample is
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，单个样本的目标函数是
- en: \[ f(\mathbf{w}) = \ell(h(\mathbf{w})) = \left(\sum_{j=1}^d w_{j} x_{j} - y\right)^2
    = \left(\mathbf{x}^T\mathbf{w} - y\right)^2\. \]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = \ell(h(\mathbf{w})) = \left(\sum_{j=1}^d w_{j} x_{j} - y\right)^2
    = \left(\mathbf{x}^T\mathbf{w} - y\right)^2\. \]
- en: 'Using the notation from the previous subsection, the forward pass in this case
    is:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一小节中的符号，在这种情况下，前向传递如下：
- en: '*Initialization:*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化：*'
- en: \[\mathbf{z}_0 := \mathbf{x}.\]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbf{z}_0 := \mathbf{x}.\]
- en: '*Forward layer loop:*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*前向层循环：*'
- en: \[\begin{align*} \hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0) &= \sum_{j=1}^d
    w_{0,j} z_{0,j} = \mathbf{z}_0^T \mathbf{w}_0 \end{align*}\]\[\begin{align*} \begin{pmatrix}
    A_0 & B_0 \end{pmatrix} := J_{g_0}(\mathbf{z}_0,\mathbf{w}_0) &= ( w_{0,1},\ldots,
    w_{0,d},z_{0,1},\ldots,z_{0,d} )^T = \begin{pmatrix}\mathbf{w}_0^T & \mathbf{z}_0^T\end{pmatrix},
    \end{align*}\]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0) &= \sum_{j=1}^d
    w_{0,j} z_{0,j} = \mathbf{z}_0^T \mathbf{w}_0 \end{align*}\]\[\begin{align*} \begin{pmatrix}
    A_0 & B_0 \end{pmatrix} := J_{g_0}(\mathbf{z}_0,\mathbf{w}_0) &= ( w_{0,1},\ldots,
    w_{0,d},z_{0,1},\ldots,z_{0,d} )^T = \begin{pmatrix}\mathbf{w}_0^T & \mathbf{z}_0^T\end{pmatrix},
    \end{align*}\]
- en: where \(\mathbf{w}_0 := \mathbf{w}\).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w}_0 := \mathbf{w}\)。
- en: '*Loss:*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失：*'
- en: \[\begin{align*} z_2 &:= \ell(z_1) = (z_1 - y)^2\\ p_2 &:= \frac{\mathrm{d}}{\mathrm{d}
    z_1} {\ell}(z_1) = 2 (z_1 - y). \end{align*}\]
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} z_2 &:= \ell(z_1) = (z_1 - y)^2\\ p_2 &:= \frac{\mathrm{d}}{\mathrm{d}
    z_1} {\ell}(z_1) = 2 (z_1 - y). \end{align*}\]
- en: 'The backward pass is:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播过程是：
- en: '*Backward layer loop:*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向层循环：*'
- en: \[\begin{align*} \mathbf{q}_0 := B_0^T p_1 &= 2 (z_1 - y) \, \mathbf{z}_0. \end{align*}\]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{q}_0 := B_0^T p_1 &= 2 (z_1 - y) \, \mathbf{z}_0. \end{align*}\]
- en: '*Output:*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0. \]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0. \]
- en: As we noted before, there is in fact no need to compute \(A_0\) and \(\mathbf{p}_0\).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，实际上没有必要计算 \(A_0\) 和 \(\mathbf{p}_0\)。
- en: '**The `Advertising` dataset and the least-squares solution** We return to the
    `Advertising` dataset.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**`Advertising` 数据集和最小二乘解** 我们回到 `Advertising` 数据集。'
- en: '[PRE0]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|  | Unnamed: 0 | TV | radio | newspaper | sales |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | Unnamed: 0 | TV | radio | newspaper | sales |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 1 | 230.1 | 37.8 | 69.2 | 22.1 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 230.1 | 37.8 | 69.2 | 22.1 |'
- en: '| 1 | 2 | 44.5 | 39.3 | 45.1 | 10.4 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 44.5 | 39.3 | 45.1 | 10.4 |'
- en: '| 2 | 3 | 17.2 | 45.9 | 69.3 | 9.3 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 17.2 | 45.9 | 69.3 | 9.3 |'
- en: '| 3 | 4 | 151.5 | 41.3 | 58.5 | 18.5 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 151.5 | 41.3 | 58.5 | 18.5 |'
- en: '| 4 | 5 | 180.8 | 10.8 | 58.4 | 12.9 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5 | 180.8 | 10.8 | 58.4 | 12.9 |'
- en: '[PRE1]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We first compute the solution using the least-squares approach we detailed previously.
    We use [`numpy.column_stack`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack)
    to add a column of ones to the feature vectors.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用之前详细说明的平方最小二乘法来计算解。我们使用 `numpy.column_stack` 向特征向量添加一列。
- en: '[PRE3]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The MSE is:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）是：
- en: '[PRE5]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Solving the problem using PyTorch** We will be using PyTorch to implement
    the previous method. We first convert the data into PyTorch tensors. We then use
    [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)
    to create the dataset. Finally, [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    provides the utilities to load the data in batches for training. We take mini-batches
    of size `BATCH_SIZE = 64` and we apply a random permutation of the samples on
    every pass (with the option `shuffle=True`).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 PyTorch 解决问题** 我们将使用 PyTorch 来实现前面的方法。我们首先将数据转换为 PyTorch 张量。然后使用 `torch.utils.data.TensorDataset`
    创建数据集。最后，`torch.utils.data.DataLoader` 提供了加载数据的批处理工具。我们取大小为 `BATCH_SIZE = 64`
    的迷你批次，并在每次传递时对样本进行随机排列（使用选项 `shuffle=True`）。'
- en: '[PRE7]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we construct our model. It is simply an affine map from \(\mathbb{R}^3\)
    to \(\mathbb{R}\). Note that there is no need to pre-process the inputs by adding
    \(1\)s. A constant term (or “bias variable”) is automatically added by PyTorch
    (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们构建我们的模型。它只是一个从 \(\mathbb{R}^3\) 到 \(\mathbb{R}\) 的仿射映射。请注意，没有必要通过添加 \(1\)s
    来预处理输入。PyTorch 会自动添加一个常数项（或“偏置变量”）（除非选择选项 [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)）。
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, we are ready to run an optimization method of our choice on the loss
    function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)
    available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)
    for a brief explanation of many common optimizers.) Here we use SGD as the optimizer.
    And the loss function is the MSE. A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好在损失函数上运行我们选择的优化方法，这些方法将在下面指定。有很多 [优化器](https://pytorch.org/docs/stable/optim.html#algorithms)
    可用。（参见这篇 [文章](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)，了解许多常见优化器的简要说明。）这里我们使用
    SGD 作为优化器。损失函数是均方误差（MSE）。快速教程在这里 [这里](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)。
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Choosing the right number of passes (i.e. epochs) through the data requires
    some experimenting. Here \(10^4\) suffices. But in the interest of time, we will
    run it only for \(100\) epochs. As you will see from the results, this is not
    quite enough. On each pass, we compute the output of the current model, use `backward()`
    to obtain the gradient, and then perform a descent update with `step()`. We also
    have to reset the gradients first (otherwise they add up by default).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的数据遍历次数（即epoch数）需要一些实验。这里\(10^4\)就足够了。但为了节省时间，我们只运行了\(100\)个epoch。从结果中你可以看到，这还不够。在每次遍历中，我们计算当前模型的输出，使用`backward()`获取梯度，然后使用`step()`进行下降更新。我们还需要首先重置梯度（否则它们会默认累加）。
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The final parameters and loss are:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最终参数和损失如下：
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE11]</details>'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE11]</details>'
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE13]</details>'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]</details>'
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 8.7.2.2\. Convolutional neural networks[#](#convolutional-neural-networks "Link
    to this heading")
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2.2\. 卷积神经网络[#](#convolutional-neural-networks "链接到这个标题")
- en: 'We return to the Fashion MNIST dataset. One can do even better than we did
    before using a neural network tailored for images, known as [convolutional neural
    networks](https://cs231n.github.io/convolutional-networks/). From [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到Fashion MNIST数据集。使用为图像定制的神经网络，可以做得比我们之前更好，这种神经网络被称为[卷积神经网络](https://cs231n.github.io/convolutional-networks/)。从[Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)：
- en: In deep learning, a convolutional neural network (CNN, or ConvNet) is a class
    of deep neural networks, most commonly applied to analyzing visual imagery. They
    are also known as shift invariant or space invariant artificial neural networks
    (SIANN), based on their shared-weights architecture and translation invariance
    characteristics.
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在深度学习中，卷积神经网络（CNN，或ConvNet）是一类深度神经网络，最常用于分析视觉图像。它们也被称为基于共享权重架构和平移不变性特征的平移不变或空间不变人工神经网络（SIANN）。
- en: More background can be found in this excellent [module](http://cs231n.github.io/convolutional-networks/)
    from Stanford’s [CS231n](http://cs231n.github.io/). Our CNN will be a composition
    of [convolutional layers](http://cs231n.github.io/convolutional-networks/#conv)
    and [pooling layers](http://cs231n.github.io/convolutional-networks/#pool).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 更多背景信息可以在斯坦福的[CS231n](http://cs231n.github.io/)的出色[模块](http://cs231n.github.io/convolutional-networks/)中找到。我们的CNN将是[卷积层](http://cs231n.github.io/convolutional-networks/#conv)和[池化层](http://cs231n.github.io/convolutional-networks/#pool)的组合。
- en: '**CHAT & LEARN** Convolutional neural networks (CNNs) are powerful for image
    classification. Ask your favorite AI chatbot to explain the basic concepts of
    CNNs, including convolutional layers and pooling layers. \(\ddagger\)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 卷积神经网络（CNN）在图像分类方面非常强大。请你的心仪AI聊天机器人解释CNN的基本概念，包括卷积层和池化层。\(\ddagger\)'
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The new model is the following.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型如下。
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We train and test.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行训练和测试。
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note the higher accuracy.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意更高的准确率。
- en: Finally, we try the original MNIST dataset. We use the same CNN.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们尝试原始的MNIST数据集。我们使用相同的CNN。
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note the very high accuracy on this (easy - as it turns out) dataset.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在这个（容易的 - 如你所见）数据集上的非常高的准确率。
- en: 8.7.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7.3\. 额外证明[#](#additional-proofs "链接到这个标题")
- en: '**Proofs of Lagrange Multipliers Conditions** We first prove the *Lagrange
    Multipliers: First-Order Necessary Conditions*. We follow the excellent textbook
    [[Ber](http://www.athenasc.com/nonlinbook.html), Section 4.1]. The proof uses
    the concept of the Jacobian.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**拉格朗日乘数条件证明** 我们首先证明**拉格朗日乘数：一阶必要条件**。我们遵循优秀的教科书 [[Ber](http://www.athenasc.com/nonlinbook.html)，第4.1节]。证明使用了雅可比矩阵的概念。'
- en: '*Proof idea:* We reduce the problem to an unconstrained optimization problem
    by penalizing the constraint in the objective function. We then apply the unconstrained
    *First-Order Necessary Conditions*.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们通过在目标函数中对约束进行惩罚，将问题简化为一个无约束优化问题。然后我们应用无约束的**一阶必要条件**。'
- en: '*Proof:* *(Lagrange Multipliers: First-Order Necessary Conditions)* We reduce
    the problem to an unconstrained optimization problem by penalizing the constraint
    in the objective function. We also add a regularization term to ensure that \(\mathbf{x}^*\)
    is the unique local minimizer in a neighborhood. Specifically, for each non-negative
    integer \(k\), consider the objective function'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* *(拉格朗日乘数法：一阶必要条件)* 我们通过在目标函数中惩罚约束条件将问题简化为一个无约束优化问题。我们还添加了一个正则化项，以确保 \(\mathbf{x}^*\)
    在一个邻域内是唯一的局部最小值。具体来说，对于每个非负整数 \(k\)，考虑目标函数'
- en: \[ F^k(\mathbf{x}) = f(\mathbf{x}) + \frac{k}{2} \|\mathbf{h}(\mathbf{x})\|^2
    + \frac{\alpha}{2} \|\mathbf{x} - \mathbf{x}^*\|^2 \]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F^k(\mathbf{x}) = f(\mathbf{x}) + \frac{k}{2} \|\mathbf{h}(\mathbf{x})\|^2
    + \frac{\alpha}{2} \|\mathbf{x} - \mathbf{x}^*\|^2 \]
- en: for some positive constant \(\alpha > 0\). Note that as \(k\) gets larger, the
    penalty becomes more significant and, therefore, enforcing the constraint becomes
    more desirable. The proof proceeds in several steps.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个正常数 \(\alpha > 0\)。注意，随着 \(k\) 的增大，惩罚变得更为重要，因此强制约束变得更为可取。证明分几个步骤进行。
- en: We first consider a version minimizing \(F^k\) constrained to lie in a neighborhood
    of \(\mathbf{x}^*\). Because \(\mathbf{x}^*\) is a local minimizer of \(f\) subject
    to \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\), there is \(\delta > 0\) such that
    \(f(\mathbf{x}^*)\leq f(\mathbf{x})\) for all feasible \(\mathbf{x}\) within
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑一个版本，该版本在 \(\mathbf{x}^*\) 的邻域内最小化 \(F^k\)，受到约束。因为 \(\mathbf{x}^*\) 是在
    \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\) 的约束下 \(f\) 的局部最小值，存在 \(\delta > 0\)，使得对于所有在
    \(\mathbf{x}^*\) 邻域内的可行 \(\mathbf{x}\)，都有 \(f(\mathbf{x}^*)\leq f(\mathbf{x})\)。
- en: \[ \mathscr{X} = B_{\delta}(\mathbf{x}^*) = \{\mathbf{x}:\|\mathbf{x} - \mathbf{x}^*\|
    \leq \delta\}. \]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathscr{X} = B_{\delta}(\mathbf{x}^*) = \{\mathbf{x}:\|\mathbf{x} - \mathbf{x}^*\|
    \leq \delta\}. \]
- en: '**LEMMA** **(Step I: Solving the Penalized Problem in a Neighborhood of \(\mathbf{x}^*\))**
    For \(k \geq 1\), let \(\mathbf{x}^k\) be a global minimizer of the minimization
    problem'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(步骤 I：在 \(\mathbf{x}^*\) 邻域内解决惩罚问题)** 对于 \(k \geq 1\)，设 \(\mathbf{x}^k\)
    是最小化问题的全局最小值'
- en: \[ \min_{\mathbf{x} \in \mathscr{X}} F^k(\mathbf{x}). \]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathscr{X}} F^k(\mathbf{x}). \]
- en: a) The sequence \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) converges to \(\mathbf{x}^*\).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: a) 序列 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) 收敛到 \(\mathbf{x}^*\)。
- en: b) For \(k\) sufficiently large, \(\mathbf{x}^k\) is a local minimizer of the
    objective function \(F^k\) *without any constraint*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: b) 对于足够大的 \(k\)，\(\mathbf{x}^k\) 是目标函数 \(F^k\) 的局部最小值，*没有任何约束*。
- en: \(\flat\)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* The set \(\mathscr{X}\) is closed and bounded, and \(F^k\) is continuous.
    Hence, the sequence \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) is well-defined by the
    *Extreme Value Theorem*. Let \(\bar{\mathbf{x}}\) be any limit point of \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\).
    We show that \(\bar{\mathbf{x}} = \mathbf{x}^*\). That will imply a). It also
    implied b) since hen, for large enough \(k\), \(\mathbf{x}^k\) must be an interior
    point of \(\mathscr{X}\).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 集合 \(\mathscr{X}\) 是闭集且有界，\(F^k\) 是连续的。因此，根据*极值定理*，序列 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)
    是有定义的。设 \(\bar{\mathbf{x}}\) 是 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) 的任意极限点。我们证明
    \(\bar{\mathbf{x}} = \mathbf{x}^*\)。这将意味着 a)。这也意味着 b)，因为当 \(k\) 足够大时，\(\mathbf{x}^k\)
    必须是 \(\mathscr{X}\) 的内点。'
- en: Let \(-\infty < m \leq M < + \infty\) be the smallest and largest values of
    \(f\) on \(\mathscr{X}\), which exist by the *Extreme Value Theorem*. Then, for
    all \(k\), by definition of \(\mathbf{x}^k\) and the fact that \(\mathbf{x}^*\)
    is feasible
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(-\infty < m \leq M < + \infty\) 是 \(\mathscr{X}\) 上函数 \(f\) 的最小值和最大值，根据*极值定理*存在。那么，对于所有
    \(k\)，根据 \(\mathbf{x}^k\) 的定义以及 \(\mathbf{x}^*\) 是可行解的事实
- en: \[\begin{align*} (*) \qquad f(\mathbf{x}^k) &+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^k)\|^2
    + \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\\ &\leq f(\mathbf{x}^*) +
    \frac{k}{2} \|\mathbf{h}(\mathbf{x}^*)\|^2 + \frac{\alpha}{2} \|\mathbf{x}^* -
    \mathbf{x}^*\|^2 = f(\mathbf{x}^*). \end{align*}\]
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (*) \qquad f(\mathbf{x}^k) &+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^k)\|^2
    + \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\\ &\leq f(\mathbf{x}^*) +
    \frac{k}{2} \|\mathbf{h}(\mathbf{x}^*)\|^2 + \frac{\alpha}{2} \|\mathbf{x}^* -
    \mathbf{x}^*\|^2 = f(\mathbf{x}^*). \end{align*}\]
- en: Rearraning gives
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列给出
- en: \[ \|\mathbf{h}(\mathbf{x}^k)\|^2 \leq \frac{2}{k} \left[f(\mathbf{x}^*) - f(\mathbf{x}^k)
    - \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\right] \leq \frac{2}{k} \left[
    f(\mathbf{x}^*) - m\right]. \]
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{h}(\mathbf{x}^k)\|^2 \leq \frac{2}{k} \left[f(\mathbf{x}^*) - f(\mathbf{x}^k)
    - \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\right] \leq \frac{2}{k} \left[
    f(\mathbf{x}^*) - m\right]. \]
- en: So \(\lim_{k \to \infty} \|\mathbf{h}(\mathbf{x}^k)\|^2 = 0\), which, by the
    continuity of \(\mathbf{h}\) and of the Frobenius norm, implies that \(\|\mathbf{h}(\bar{\mathbf{x}})\|^2
    = 0\), that is, \(\mathbf{h}(\bar{\mathbf{x}}) = \mathbf{0}\). In other words,
    any limit point \(\bar{\mathbf{x}}\) is feasible.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(\lim_{k \to \infty} \|\mathbf{h}(\mathbf{x}^k)\|^2 = 0\)，由于 \(\mathbf{h}\)
    和 Frobenius 范数的连续性，这导致 \(\|\mathbf{h}(\bar{\mathbf{x}})\|^2 = 0\)，即 \(\mathbf{h}(\bar{\mathbf{x}})
    = \mathbf{0}\)。换句话说，任何极限点 \(\bar{\mathbf{x}}\) 都是可行的。
- en: In addition to being feasible, \(\bar{\mathbf{x}} \in \mathscr{X}\) because
    that constraint set is closed. So, by the choice of \(\mathscr{X}\), we have \(f(\mathbf{x}^*)
    \leq f(\bar{\mathbf{x}})\). Furthermore, by \((*)\), we get
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可行之外，\(\bar{\mathbf{x}} \in \mathscr{X}\) 因为那个约束集是闭合的。因此，根据 \(\mathscr{X}\)
    的选择，我们有 \(f(\mathbf{x}^*) \leq f(\bar{\mathbf{x}})\)。此外，根据 \((*)\)，我们得到
- en: \[ f(\mathbf{x}^*) \leq f(\bar{\mathbf{x}}) + \frac{\alpha}{2} \|\bar{\mathbf{x}}
    - \mathbf{x}^*\|^2 \leq f(\mathbf{x}^*). \]
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^*) \leq f(\bar{\mathbf{x}}) + \frac{\alpha}{2} \|\bar{\mathbf{x}}
    - \mathbf{x}^*\|^2 \leq f(\mathbf{x}^*). \]
- en: This is only possible if \(\|\bar{\mathbf{x}} - \mathbf{x}^*\|^2 = 0\) or, put
    differently, \(\bar{\mathbf{x}} = \mathbf{x}^*\). That proves the lemma. \(\square\)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这只有在 \(\|\bar{\mathbf{x}} - \mathbf{x}^*\|^2 = 0\) 或换句话说，\(\bar{\mathbf{x}}
    = \mathbf{x}^*\) 的情况下才可能。这证明了引理。 \(\square\)
- en: '**LEMMA** **(Step II: Applying the Unconstrained Necessary Conditions)** Let
    \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) be the sequence in the previous lemma.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(第二步：应用无约束必要条件)** 设 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) 是前面引理中的序列。'
- en: a) For sufficiently large \(k\), the vectors \(\nabla h_i(\mathbf{x}^k)\), \(i=1,\ldots,\ell\),
    are linearly independent.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对于足够大的 \(k\)，向量 \(\nabla h_i(\mathbf{x}^k)\)，\(i=1,\ldots,\ell\)，是线性无关的。
- en: b) Let \(\mathbf{J}_{\mathbf{h}}(\mathbf{x})\) be the Jacobian matrix of \(\mathbf{h}\),
    that is, the matrix whose rows are the (row) vectors \(\nabla h_i(\mathbf{x})^T\),
    \(i=1,\ldots,\ell\). Then
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: b) 设 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x})\) 为 \(\mathbf{h}\) 的雅可比矩阵，即其行是 \(\nabla
    h_i(\mathbf{x})^T\) 的向量，\(i=1,\ldots,\ell\)。那么
- en: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
- en: where
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[ \blambda^* = - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \, \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*))^{-1}
    \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*) \nabla f(\mathbf{x}^*). \]
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \blambda^* = - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \, \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*))^{-1}
    \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*) \nabla f(\mathbf{x}^*). \]
- en: \(\flat\)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* By the previous lemma, for \(k\) large enough, \(\mathbf{x}^k\) is
    an unconstrained local minimizer of \(F^k\). So by the (unconstrained) *First-Order
    Necessary Conditions*, it holds that'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 根据前面的引理，对于足够大的 \(k\)，\(\mathbf{x}^k\) 是 \(F^k\) 的无约束局部极小值。因此，根据（无约束的）*一阶必要条件*，它成立，'
- en: \[ \nabla F^k(\mathbf{x}^k) = \mathbf{0}. \]
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla F^k(\mathbf{x}^k) = \mathbf{0}. \]
- en: To compute the gradient of \(F^k\) we note that
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 \(F^k\) 的梯度，我们注意到
- en: \[ \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell (h_i(\mathbf{x}))^2. \]
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell (h_i(\mathbf{x}))^2. \]
- en: The partial derivatives are
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数是
- en: \[ \frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell
    \frac{\partial}{\partial x_j} (h_i(\mathbf{x}))^2 = \sum_{i=1}^\ell 2 h_i(\mathbf{x})
    \frac{\partial h_i(\mathbf{x})}{\partial x_j}, \]
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell
    \frac{\partial}{\partial x_j} (h_i(\mathbf{x}))^2 = \sum_{i=1}^\ell 2 h_i(\mathbf{x})
    \frac{\partial h_i(\mathbf{x})}{\partial x_j}, \]
- en: by the *Chain Rule*. So, in vector form,
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *链式法则*。因此，以向量形式，
- en: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
- en: The term \(\|\mathbf{x} - \mathbf{x}^*\|^2\) can be rewritten as the quadratic
    function
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 项 \(\|\mathbf{x} - \mathbf{x}^*\|^2\) 可以重写为二次函数
- en: \[ \|\mathbf{x} - \mathbf{x}^*\|^2 = \frac{1}{2}\mathbf{x}^T (2 I_{d \times
    d}) \mathbf{x} - 2 (\mathbf{x}^*)^T \mathbf{x} + (\mathbf{x}^*)^T \mathbf{x}^*.
    \]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{x} - \mathbf{x}^*\|^2 = \frac{1}{2}\mathbf{x}^T (2 I_{d \times
    d}) \mathbf{x} - 2 (\mathbf{x}^*)^T \mathbf{x} + (\mathbf{x}^*)^T \mathbf{x}^*.
    \]
- en: Using a previous formula with \(P = 2 I_{d \times d}\) (which is symmetric),
    \(\mathbf{q} = -2 \mathbf{x}^*\) and \(r = (\mathbf{x}^*)^T \mathbf{x}^*\), we
    get
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的公式，其中 \(P = 2 I_{d \times d}\)（这是一个对称矩阵），\(\mathbf{q} = -2 \mathbf{x}^*\)
    和 \(r = (\mathbf{x}^*)^T \mathbf{x}^*\)，我们得到
- en: \[ \nabla \|\mathbf{x} - \mathbf{x}^*\|^2 = 2\mathbf{x} -2 \mathbf{x}^*. \]
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla \|\mathbf{x} - \mathbf{x}^*\|^2 = 2\mathbf{x} -2 \mathbf{x}^*. \]
- en: So, putting everything together,
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，把所有东西放在一起，
- en: \[ (**) \qquad \mathbf{0} = \nabla F^k(\mathbf{x}^k) = \nabla f(\mathbf{x}^k)
    + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T (k \mathbf{h}(\mathbf{x}^k)) + \alpha(\mathbf{x}^k
    - \mathbf{x}^*). \]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (**) \qquad \mathbf{0} = \nabla F^k(\mathbf{x}^k) = \nabla f(\mathbf{x}^k)
    + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T (k \mathbf{h}(\mathbf{x}^k)) + \alpha(\mathbf{x}^k
    - \mathbf{x}^*). \]
- en: By the previous lemma, \(\mathbf{x}^k \to \mathbf{x}^*\), \(\nabla f(\mathbf{x}^k)
    \to \nabla f(\mathbf{x}^*)\), and \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\) as \(k \to +\infty\).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的引理，\(\mathbf{x}^k \to \mathbf{x}^*\)，\(\nabla f(\mathbf{x}^k) \to \nabla
    f(\mathbf{x}^*)\)，以及 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)
    当 \(k \to +\infty\)。
- en: So it remains to derive the limit of \(k \mathbf{h}(\mathbf{x}^k)\). By assumption,
    the columns of \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T\) are linearly independent.
    That implies that for any unit vector \(\mathbf{z} \in \mathbb{R}^\ell\)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要推导 \(k \mathbf{h}(\mathbf{x}^k)\) 的极限。根据假设，\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T\)
    的列是线性无关的。这意味着对于任何单位向量 \(\mathbf{z} \in \mathbb{R}^\ell\)
- en: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} = \|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}\|^2 > 0 \]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} = \|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}\|^2 > 0 \]
- en: otherwise we would have \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}
    = \mathbf{0}\), contradicting the linear independence assumption. By the *Extreme
    Value Theorem*, there is \(\beta > 0\) such that
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，我们将有 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} = \mathbf{0}\)，这与线性无关的假设相矛盾。根据**极值定理**，存在
    \(\beta > 0\) 使得
- en: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} \geq \beta \]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} \geq \beta \]
- en: for all unit vectors \(\mathbf{z} \in \mathbb{R}^\ell\). Since \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)
    \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\), it follows from a previous lemma
    that, for \(k\) large enough and any unit vector \(\mathbf{z} \in \mathbb{R}^\ell\),
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有单位向量 \(\mathbf{z} \in \mathbb{R}^\ell\)。由于 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)
    \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)，根据之前的引理，对于足够大的 \(k\) 和任意单位向量 \(\mathbf{z}
    \in \mathbb{R}^\ell\)，
- en: \[ |\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} - \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z}| \leq \| \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    - \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \|_F \leq \frac{\beta}{2}. \]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} - \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z}| \leq \| \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    - \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \|_F \leq \frac{\beta}{2}. \]
- en: That implies
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z} \geq \frac{\beta}{2}, \]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z} \geq \frac{\beta}{2}, \]
- en: so by the same argument as above the columns of \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)
    are linearly independent for \(k\) large enough and \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)
    \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\) is invertible. That proves a).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过与上述相同的论证，当 \(k\) 足够大时，\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\) 的列是线性无关的，并且
    \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)
    是可逆的。这证明了 a）。
- en: Going back to \((**)\), multiplying both sides by \((\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\,
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T)^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\),
    and taking a limit \(k \to +\infty\), we get after rearranging that
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 \((**)\)，将两边乘以 \((\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T)^{-1}
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\)，并取极限 \(k \to +\infty\)，经过重新排列后我们得到
- en: \[ k \mathbf{h}(\mathbf{x}^k) \to - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) ^T
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \nabla f(\mathbf{x}^*) = \blambda^*. \]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: \[ k \mathbf{h}(\mathbf{x}^k) \to - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) ^T
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \nabla f(\mathbf{x}^*) = \blambda^*. \]
- en: Plugging back we get
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 将其代入后得到
- en: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
- en: as claimed. That proves b). \(\square\)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。这证明了 b）。\(\square\)
- en: Combining the lemmas establishes the theorem. \(\square\)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 结合引理可以建立定理。\(\square\)
- en: 'Next, we prove the *Lagrange Multipliers: Second-Order Sufficient Conditions*.
    Again, we follow [[Ber](http://www.athenasc.com/nonlinbook.html), Section 4.2].
    We will need the following lemma. The proof can be skipped.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们证明 *拉格朗日乘数：二阶充分条件*。再次，我们遵循 [[Ber](http://www.athenasc.com/nonlinbook.html)，第
    4.2 节]。我们需要以下引理。证明可以跳过。
- en: '*Proof idea:* We consider a slightly modified problem'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们考虑一个略微修改的问题'
- en: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0} \end{align*}\]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0} \end{align*}\]
- en: which has the same local minimizers. Applying the *Second-Order Sufficient Conditions*
    to the Lagrangian of the modified problem gives the result when \(c\) is chosen
    large enough.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有相同的局部极小值。将 *二阶充分条件* 应用到修改后问题的拉格朗日函数上，当 \(c\) 足够大时，得到结果。
- en: '**LEMMA** Let \(P\) and \(Q\) be symmetric matrices in \(\mathbb{R}^{n \times
    n}\). Assume that \(Q\) is positive semidefinite and that \(P\) is positive definite
    on the null space of \(Q\), that is, \(\mathbf{w}^T P \mathbf{w} > 0\) for all
    \(\mathbf{w} \neq \mathbf{0}\) such that \(\mathbf{w}^T Q \mathbf{w} = \mathbf{0}\).
    Then there is a scalar \(\bar{c} \geq 0\) such that \(P + c Q\) is positive definite
    for all \(c > \bar{c}\). \(\flat\)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** 设 \(P\) 和 \(Q\) 是 \(\mathbb{R}^{n \times n}\) 中的对称矩阵。假设 \(Q\) 是正半定矩阵，且
    \(P\) 在 \(Q\) 的零空间上是正定的，即对于所有 \(\mathbf{w} \neq \mathbf{0}\) 满足 \(\mathbf{w}^T
    Q \mathbf{w} = \mathbf{0}\) 的 \(\mathbf{w}\)，都有 \(\mathbf{w}^T P \mathbf{w} >
    0\)。那么存在一个标量 \(\bar{c} \geq 0\)，使得对于所有 \(c > \bar{c}\)，\(P + c Q\) 是正定的。 \(\flat\)'
- en: '*Proof:* *(lemma)* We argue by contradiction. Suppose there is an non-negative,
    increasing, diverging sequence \(\{c_k\}_{k=1}^{+\infty}\) and a sequence of unit
    vectors \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) such that'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：*(引理)* 我们通过反证法来论证。假设存在一个非负的、递增的、发散的序列 \(\{c_k\}_{k=1}^{+\infty}\) 和一个单位向量序列
    \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)，'
- en: \[ (\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k \leq 0 \]
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k \leq 0 \]
- en: for all \(k\). Because the sequence is bounded, it has a limit point \(\bar{\mathbf{x}}\).
    Assume without loss of generality that \(\mathbf{x}^k \to \bar{\mathbf{x}}\),
    as \(k \to \infty\). Since \(c_k \to +\infty\) and \((\mathbf{x}^k)^T Q \mathbf{x}^k
    \geq 0\) by assumption, we must have \((\bar{\mathbf{x}})^T Q \bar{\mathbf{x}}
    = 0\), otherwise \((\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k\) would diverge.
    Hence, by the assumption in the statement, it must be that \((\bar{\mathbf{x}})^T
    P \bar{\mathbf{x}} > 0\). This contradicts the inequality in the display above.
    \(\square\)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(k\)。因为序列是有界的，所以它有一个极限点 \(\bar{\mathbf{x}}\)。不失一般性，假设 \(\mathbf{x}^k \to
    \bar{\mathbf{x}}\)，当 \(k \to \infty\)。由于 \(c_k \to +\infty\) 并且根据假设 \((\mathbf{x}^k)^T
    Q \mathbf{x}^k \geq 0\)，我们必须有 \((\bar{\mathbf{x}})^T Q \bar{\mathbf{x}} = 0\)，否则
    \((\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k\) 将会发散。因此，根据陈述中的假设，必须有 \((\bar{\mathbf{x}})^T
    P \bar{\mathbf{x}} > 0\)。这与上面的不等式相矛盾。 \(\square\)
- en: '*Proof:* *(Lagrange Multipliers: Second-Order Sufficient Conditions)* We consider
    the modified problem'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：*(拉格朗日乘数：二阶充分条件)* 我们考虑修改后的问题'
- en: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}. \end{align*}\]
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}. \end{align*}\]
- en: It has the same local minimizers as the orginal problem as the additional term
    in the objective is zero for feasible vectors. That extra term will allow us to
    use the previous lemma. For notational convenience, we define
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有与原问题相同的局部极小值，因为对于可行向量，目标函数中的附加项为零。这个额外的项将允许我们使用之前的引理。为了方便起见，我们定义
- en: \[ g_c(\mathbf{x}) = f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2.
    \]
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: \[ g_c(\mathbf{x}) = f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2.
    \]
- en: The Lagrangian of the modified problem is
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后问题的拉格朗日函数是
- en: \[ L_c(\mathbf{x}, \blambda) = g_c(\mathbf{x}) + \mathbf{h}(\mathbf{x})^T \blambda.
    \]
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_c(\mathbf{x}, \blambda) = g_c(\mathbf{x}) + \mathbf{h}(\mathbf{x})^T \blambda.
    \]
- en: We will apply the *Second-Order Sufficient Conditions* to problem of minimizing
    \(L_c\) over \(\mathbf{x}\). We indicate the Hessian with respect to only the
    variables \(\mathbf{x}\) as \(\nabla^2_{\mathbf{x},\mathbf{x}}\).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用 *二阶充分条件* 来最小化 \(L_c\) 关于 \(\mathbf{x}\) 的问题。我们只表示与变量 \(\mathbf{x}\) 相关的Hessian，记为
    \(\nabla^2_{\mathbf{x},\mathbf{x}}\)。
- en: 'Recall from the proof of the *Lagrange Multipliers: First-Order Necessary Conditions*
    that'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下在证明 *拉格朗日乘数：一阶必要条件* 时的内容，
- en: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
- en: To compute the Hessian of that function, we note
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算该函数的Hessian矩阵，我们注意到
- en: \[\begin{align*} \frac{\partial}{\partial x_i}\left( \frac{\partial}{\partial
    x_j} \|\mathbf{h}(\mathbf{x})\|^2\right) &= \frac{\partial}{\partial x_i}\left(
    \sum_{k=1}^\ell 2 h_k(\mathbf{x}) \frac{\partial h_k(\mathbf{x})}{\partial x_j}\right)\\
    &= 2 \sum_{k=1}^\ell\left( \frac{\partial h_k(\mathbf{x})}{\partial x_i} \frac{\partial
    h_k(\mathbf{x})}{\partial x_j} + h_k(\mathbf{x}) \frac{\partial^2 h_k(\mathbf{x})}{\partial
    x_i \partial x_j} \right)\\ &= 2 \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x})
    + \sum_{k=1}^\ell h_k(\mathbf{x}) \, \mathbf{H}_{h_k}(\mathbf{x})\right)_{i,j}.
    \end{align*}\]
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial x_i}\left( \frac{\partial}{\partial
    x_j} \|\mathbf{h}(\mathbf{x})\|^2\right) &= \frac{\partial}{\partial x_i}\left(
    \sum_{k=1}^\ell 2 h_k(\mathbf{x}) \frac{\partial h_k(\mathbf{x})}{\partial x_j}\right)\\
    &= 2 \sum_{k=1}^\ell\left( \frac{\partial h_k(\mathbf{x})}{\partial x_i} \frac{\partial
    h_k(\mathbf{x})}{\partial x_j} + h_k(\mathbf{x}) \frac{\partial^2 h_k(\mathbf{x})}{\partial
    x_i \partial x_j} \right)\\ &= 2 \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x})
    + \sum_{k=1}^\ell h_k(\mathbf{x}) \, \mathbf{H}_{h_k}(\mathbf{x})\right)_{i,j}.
    \end{align*}\]
- en: Hence
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \nabla f(\mathbf{x}^*)
    + c \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{h}(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \blambda^* \]
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \nabla f(\mathbf{x}^*)
    + c \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{h}(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \blambda^* \]
- en: and
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{H}_{f}(\mathbf{x}^*)
    + c \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    + \sum_{k=1}^\ell h_k(\mathbf{x}^*) \, \mathbf{H}_{h_k}(\mathbf{x}^*)\right) +
    \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*). \]
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{H}_{f}(\mathbf{x}^*)
    + c \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    + \sum_{k=1}^\ell h_k(\mathbf{x}^*) \, \mathbf{H}_{h_k}(\mathbf{x}^*)\right) +
    \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*). \]
- en: By the assumptions of the theorem, this simplifies to
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定理的假设，这可以简化为
- en: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{0} \]
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{0} \]
- en: and
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) =\underbrace{\left\{
    \mathbf{H}_{f}(\mathbf{x}^*) + \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*)
    \right\}}_{P} + c \underbrace{\left\{\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \right\}}_{Q}, \]
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) =\underbrace{\left\{
    \mathbf{H}_{f}(\mathbf{x}^*) + \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*)
    \right\}}_{P} + c \underbrace{\left\{\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \right\}}_{Q}, \]
- en: where further \(\mathbf{v}^T P \mathbf{v} > 0\) for any \(\mathbf{v}\) such
    that \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{v} = \mathbf{0}\) (which
    itself implies \(\mathbf{v}^T Q \mathbf{v} = \mathbf{0}\)). Furthermore, \(Q \succeq
    \mathbf{0}\) since
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于任意的 \(\mathbf{v}\) 满足 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{v}
    = \mathbf{0}\)（这本身意味着 \(\mathbf{v}^T Q \mathbf{v} = \mathbf{0}\)），进一步有 \(\mathbf{v}^T
    P \mathbf{v} > 0\)。此外，由于
- en: \[ \mathbf{w}^T Q \mathbf{w} = \mathbf{w}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \mathbf{w} = \left\|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \mathbf{w}\right\|^2 \geq 0 \]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^T Q \mathbf{w} = \mathbf{w}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \mathbf{w} = \left\|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \mathbf{w}\right\|^2 \geq 0 \]
- en: for any \(\mathbf{w} \in \mathbb{R}^d\). The previous lemma allows us to take
    \(c\) large enough that \(\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
    \succ \mathbf{0}\).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意的 \(\mathbf{w} \in \mathbb{R}^d\)。前一个引理允许我们取 \(c\) 足够大，使得 \(\nabla^2_{\mathbf{x},\mathbf{x}}
    L_c(\mathbf{x}^*, \blambda^*) \succ \mathbf{0}\).
- en: As a result, the unconstrained *Second-Order Sufficient Conditions* are satisfied
    at \(\mathbf{x}^*\) for the problem
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于该问题，在 \(\mathbf{x}^*\) 处满足无约束的**二阶充分条件**。
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} L_c(\mathbf{x}, \blambda^*). \]
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} L_c(\mathbf{x}, \blambda^*). \]
- en: That is, there is \(\delta > 0\) such that
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是存在 \(\delta > 0\) 使得
- en: \[ L_c(\mathbf{x}^*, \blambda^*) < L_c(\mathbf{x}, \blambda^*), \qquad \forall
    \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}. \]
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_c(\mathbf{x}^*, \blambda^*) < L_c(\mathbf{x}, \blambda^*), \qquad \forall
    \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}. \]
- en: Restricting this to the feasible vectors of the modified constrained problem
    (i.e., those such that \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\)) implies after
    simplification
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 将其限制在修改后的约束问题的可行向量上（即那些满足 \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\) 的向量），简化后得到
- en: '\[ f(\mathbf{x}^*) < f(\mathbf{x}), \qquad \forall \mathbf{x} \in \{\mathbf{x}
    : \mathbf{h}(\mathbf{x}) = \mathbf{0}\} \cap (B_{\delta}(\mathbf{x}^*) \setminus
    \{\mathbf{x}^*\}). \]'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ f(\mathbf{x}^*) < f(\mathbf{x}), \qquad \forall \mathbf{x} \in \{\mathbf{x}
    : \mathbf{h}(\mathbf{x}) = \mathbf{0}\} \cap (B_{\delta}(\mathbf{x}^*) \setminus
    \{\mathbf{x}^*\}). \]'
- en: Therefore, \(\mathbf{x}^*\) is a strict local minimizer of the modified constrained
    problem (and, in turn, of the original constrained problem). That concludes the
    proof. \(\square\)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(\mathbf{x}^*\) 是修改后的约束问题的严格局部极小值（进而也是原始约束问题的严格局部极小值）。这就完成了证明。\(\square\)
- en: 8.7.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7.1\. 测验、解答、代码等.[#](#quizzes-solutions-code-etc "链接到本标题")
- en: 8.7.1.1\. Just the code[#](#just-the-code "Link to this heading")
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.1\. 仅代码[#](#just-the-code "链接到本标题")
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 下面可以访问本章代码的交互式 Jupyter 笔记本（推荐使用 Google Colab）。鼓励您对其进行尝试。一些建议的计算练习散布其中。笔记本也可作为幻灯片查看。
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb)
    ([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))'
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[幻灯片](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html)'
- en: 8.7.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.2\. 自我评估测验[#](#self-assessment-quizzes "链接到本标题")
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下链接可以获取更全面的自我评估测验的网络版本。
- en: '[Section 8.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.2节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html)'
- en: '[Section 8.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.3节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html)'
- en: '[Section 8.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.4节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html)'
- en: '[Section 8.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.5节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html)'
- en: 8.7.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.3\. 自动测验[#](#auto-quizzes "链接到本标题")
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本章自动生成的测验可以在此访问（推荐使用 Google Colab）。
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb))'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动测验](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb)
    ([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb))'
- en: 8.7.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.4\. 奇数练习题的解答[#](#solutions-to-odd-numbered-warm-up-exercises "链接到本标题")
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*(在 Claude、Gemini 和 ChatGPT 的帮助下)*'
- en: '**E8.2.1** The vectorization is obtained by stacking the columns of \(A\):
    \(\text{vec}(A) = (2, 0, 1, -1)\).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.1** 向量化是通过堆叠 \(A\) 的列获得的：\(\text{vec}(A) = (2, 0, 1, -1)\)。'
- en: '**E8.2.3**'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.3**'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \cdot B & 2 \cdot B \\ -1 \cdot
    B & 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 & -1 & 6 & -2 \\ 2 & 1 & 4 & 2
    \\ -3 & 1 & -6 & 2 \\ -2 & -1 & -4 & -2 \end{pmatrix}. \end{split}\]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \cdot B & 2 \cdot B \\ -1 \cdot
    B & 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 & -1 & 6 & -2 \\ 2 & 1 & 4 & 2
    \\ -3 & 1 & -6 & 2 \\ -2 & -1 & -4 & -2 \end{pmatrix}. \end{split}\]
- en: '**E8.2.5**'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.5**'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 & 6 \\ 7 &
    8 \end{pmatrix} & 2 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} \\ 3 \begin{pmatrix}
    5 & 6 \\ 7 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14 & 16 \\ 15 & 18
    & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 & 6 \\ 7 &
    8 \end{pmatrix} & 2 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} \\ 3 \begin{pmatrix}
    5 & 6 \\ 7 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14 & 16 \\ 15 & 18
    & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix}. \end{split}\]
- en: '**E8.2.7** First, compute the Jacobian matrices of \(\mathbf{f}\) and \(\mathbf{g}\):'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.7** 首先，计算 \(\mathbf{f}\) 和 \(\mathbf{g}\) 的雅可比矩阵：'
- en: \[\begin{split} J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x & 2y \\ y & x \end{pmatrix},
    \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v & u \\ 1 & 1 \end{pmatrix}. \end{split}\]
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x & 2y \\ y & x \end{pmatrix},
    \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v & u \\ 1 & 1 \end{pmatrix}. \end{split}\]
- en: Then, by the Chain Rule,
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据链式法则，
- en: \[\begin{split} J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1,
    2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix}
    2 & 5 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 2 & 4 \\ 2 & 1 \end{pmatrix} = \begin{pmatrix}
    14 & 13 \\ 4 & 5 \end{pmatrix}. \end{split}\]
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1,
    2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix}
    2 & 5 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 2 & 4 \\ 2 & 1 \end{pmatrix} = \begin{pmatrix}
    14 & 13 \\ 4 & 5 \end{pmatrix}. \end{split}\]
- en: '**E8.2.9** From E8.2.5, we have'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.9** 从 E8.2.5，我们有'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14
    & 16 \\ 15 & 18 & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix} \end{split}\]
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14
    & 16 \\ 15 & 18 & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix} \end{split}\]
- en: So,
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 &
    18 & 24 \\ 10 & 14 & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 &
    18 & 24 \\ 10 & 14 & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
- en: Now,
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，
- en: \[\begin{split} A^T = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \quad B^T
    = \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \end{split}\]
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^T = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \quad B^T
    = \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \end{split}\]
- en: So,
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split} A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 & 7 \\
    6 & 8 \end{pmatrix} & 3 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \\ 2 \begin{pmatrix}
    5 & 7 \\ 6 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 & 18 & 24 \\ 10 & 14
    & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 & 7 \\
    6 & 8 \end{pmatrix} & 3 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \\ 2 \begin{pmatrix}
    5 & 7 \\ 6 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 & 18 & 24 \\ 10 & 14
    & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
- en: We see that \((A \otimes B)^T = A^T \otimes B^T\), as expected from the properties
    of the Kronecker product.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 \((A \otimes B)^T = A^T \otimes B^T\)，正如克罗内克积的性质所预期的那样。
- en: '**E8.2.11**'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.11**'
- en: \[\begin{split} \nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial
    x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix}
    = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}. \end{split}\]
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial
    x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix}
    = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}. \end{split}\]
- en: So,
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split} \nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
    \end{split}\]
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
    \end{split}\]
- en: '**E8.2.13** First, compute the gradient of \(f\) and the Jacobian matrix of
    \(\mathbf{g}\):'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.13** 首先，计算 \(f\) 的梯度以及 \(\mathbf{g}\) 的雅可比矩阵：'
- en: \[\begin{split} \nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad
    J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}. \end{split}\]
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad
    J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}. \end{split}\]
- en: Then, by the Chain Rule,
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据链式法则，
- en: \[\begin{split} J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T
    \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix}
    4 & 1 \end{pmatrix} \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix} = \begin{pmatrix}
    8 & 4 \end{pmatrix}. \end{split}\]
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T
    \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix}
    4 & 1 \end{pmatrix} \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix} = \begin{pmatrix}
    8 & 4 \end{pmatrix}. \end{split}\]
- en: '**E8.2.15** The Jacobian matrix of \(\mathbf{g}\) is'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.15** \(\mathbf{g}\) 的雅可比矩阵为'
- en: \[\begin{split} J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) & 0 & 0 \\ 0
    & f'(y) & 0 \\ 0 & 0 & f'(z) \end{pmatrix} \end{split}\]
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) & 0 & 0 \\ 0
    & f'(y) & 0 \\ 0 & 0 & f'(z) \end{pmatrix} \end{split}\]
- en: where \(f'(x) = \cos(x).\) So,
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(f'(x) = \cos(x).\) 因此，
- en: \[\begin{split} J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6})
    = \begin{pmatrix} \cos(\frac{\pi}{2}) & 0 & 0 \\ 0 & \cos(\frac{\pi}{4}) & 0 \\
    0 & 0 & \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & \frac{\sqrt{2}}{2}
    & 0 \\ 0 & 0 & \frac{\sqrt{3}}{2} \end{pmatrix}. \end{split}\]
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6})
    = \begin{pmatrix} \cos(\frac{\pi}{2}) & 0 & 0 \\ 0 & \cos(\frac{\pi}{4}) & 0 \\
    0 & 0 & \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & \frac{\sqrt{2}}{2}
    & 0 \\ 0 & 0 & \frac{\sqrt{3}}{2} \end{pmatrix}. \end{split}\]
- en: '**E8.3.1** Each entry of \(AB\) is the dot product of a row of \(A\) and a
    column of \(B\), which takes 2 multiplications and 1 addition. Since \(AB\) has
    4 entries, the total number of operations is \(4 \times 3 = 12\).'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.1** \(AB\) 的每个元素是 \(A\) 的一个行与 \(B\) 的一个列的点积，这需要 2 次乘法和 1 次加法。由于 \(AB\)
    有 4 个元素，所以总的操作数是 \(4 \times 3 = 12\)。'
- en: '**E8.3.3** We have'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.3** 我们有'
- en: \[ \ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2 \]
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2 \]
- en: so the partial derivatives are \(\frac{\partial \ell}{\partial \hat{y}_1} =
    2 \hat{y}_1\) and \(\frac{\partial \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\) and
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偏导数为 \(\frac{\partial \ell}{\partial \hat{y}_1} = 2 \hat{y}_1\) 和 \(\frac{\partial
    \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\) 和
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T. \]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T. \]
- en: '**E8.3.5** From E8.3.4, we have \(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0)
    = (-1, -1)\). Then, \(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\) and
    \(f(\mathbf{x}) = \ell(\mathbf{z}_2) = 5\). By the *Chain Rule*,'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.5** 从 E8.3.4，我们得到 \(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0) = (-1,
    -1)\)。然后，\(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\) 和 \(f(\mathbf{x})
    = \ell(\mathbf{z}_2) = 5\)。根据 **链式法则**，'
- en: \[\begin{split} \nabla f(\mathbf{x})^T = J_f(\mathbf{x}) = J_{\ell}(\mathbf{z}_1)
    \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) = 2 \mathbf{z}_2^T
    \begin{pmatrix} -1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ -1 & 0
    \end{pmatrix} = (-10, -4) \begin{pmatrix} 1 & 2 \\ -1 & 0 \end{pmatrix} = (6,
    -20). \end{split}\]
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(\mathbf{x})^T = J_f(\mathbf{x}) = J_{\ell}(\mathbf{z}_1)
    \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) = 2 \mathbf{z}_2^T
    \begin{pmatrix} -1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ -1 & 0
    \end{pmatrix} = (-10, -4) \begin{pmatrix} 1 & 2 \\ -1 & 0 \end{pmatrix} = (6,
    -20). \end{split}\]
- en: '**E8.3.7** We have'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.7** 我们有'
- en: \[ g_1(\mathbf{z}_1, \mathbf{w}_1) = w_4 z_{1,1} + w_5 z_{1,2} \]
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: \[ g_1(\mathbf{z}_1, \mathbf{w}_1) = w_4 z_{1,1} + w_5 z_{1,2} \]
- en: so, by computing all partial derivatives,
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过计算所有偏导数，
- en: \[ J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) = \begin{pmatrix} w_4 & w_5 & z_{1,1}
    & z_{1,2} \end{pmatrix} = \begin{pmatrix} \mathbf{w}_1^T & \mathbf{z}_1^T \end{pmatrix}
    = \begin{pmatrix} W_1 & I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}. \]
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) = \begin{pmatrix} w_4 & w_5 & z_{1,1}
    & z_{1,2} \end{pmatrix} = \begin{pmatrix} \mathbf{w}_1^T & \mathbf{z}_1^T \end{pmatrix}
    = \begin{pmatrix} W_1 & I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}. \]
- en: Using the notation in the text, \(A_1 = W_1\) and \(B_1 = \mathbf{z}_1^T = I_{1
    \times 1} \otimes \mathbf{z}_1^T\).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本中的符号，\(A_1 = W_1\) 和 \(B_1 = \mathbf{z}_1^T = I_{1 \times 1} \otimes \mathbf{z}_1^T\).
- en: '**E8.3.9** We have'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.9** 我们有'
- en: \[ f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2 \]
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2 \]
- en: so, by the *Chain Rule*, the partial derivatives are
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据 **链式法则**，偏导数为
- en: \[ \frac{\partial f}{\partial w_0} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))
    (-w_4) \]\[ \frac{\partial f}{\partial w_1} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2
    + w_3)) (w_4) \]\[ \frac{\partial f}{\partial w_2} = 2(w_4 (- w_0 + w_1) + w_5
    (-w_2 + w_3)) (-w_5) \]\[ \frac{\partial f}{\partial w_3} = 2(w_4 (- w_0 + w_1)
    + w_5 (-w_2 + w_3)) (w_5) \]\[ \frac{\partial f}{\partial w_4} = 2(w_4 (- w_0
    + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1) \]\[ \frac{\partial f}{\partial w_5} =
    2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3). \]
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f}{\partial w_0} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))
    (-w_4) \]\[ \frac{\partial f}{\partial w_1} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2
    + w_3)) (w_4) \]\[ \frac{\partial f}{\partial w_2} = 2(w_4 (- w_0 + w_1) + w_5
    (-w_2 + w_3)) (-w_5) \]\[ \frac{\partial f}{\partial w_3} = 2(w_4 (- w_0 + w_1)
    + w_5 (-w_2 + w_3)) (w_5) \]\[ \frac{\partial f}{\partial w_4} = 2(w_4 (- w_0
    + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1) \]\[ \frac{\partial f}{\partial w_5} =
    2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3). \]
- en: Moreover, by E8.3.7,
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据 E8.3.7，
- en: \[\begin{split} z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1) = W_1 \mathbf{z}_1 = \begin{pmatrix}
    w_4 & w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix} = w_4
    (- w_0 + w_1) + w_5 (-w_2 + w_3). \end{split}\]
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1) = W_1 \mathbf{z}_1 = \begin{pmatrix}
    w_4 & w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix} = w_4
    (- w_0 + w_1) + w_5 (-w_2 + w_3). \end{split}\]
- en: By the fundamental recursion and the results in E8.3.3 and E8.3.8,
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基本递归和E8.3.3和E8.3.8中的结果，
- en: \[\begin{align*} J_f(\mathbf{w}) &= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
    = 2 z_2 \begin{pmatrix} A_1 B_0 & B_1\end{pmatrix}\\ &= 2 (w_4 (- w_0 + w_1) +
    w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3). \end{align*}\]
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} J_f(\mathbf{w}) &= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
    = 2 z_2 \begin{pmatrix} A_1 B_0 & B_1\end{pmatrix}\\ &= 2 (w_4 (- w_0 + w_1) +
    w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3). \end{align*}\]
- en: '**E8.4.1** The full gradient descent step is:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.1** 完整的梯度下降步骤是：'
- en: \[ \frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1,
    2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}). \]
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1,
    2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}). \]
- en: 'The expected SGD step with a batch size of 2 is:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 批大小为2的期望SGD步骤是：
- en: \[ \mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] =
    \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
    \]
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] =
    \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
    \]
- en: which is equal to the full gradient descent step, as proven in the “Expected
    SGD Step” lemma.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这与全梯度下降步骤相等，正如在“期望SGD步骤”引理中证明的那样。
- en: '**E8.4.3**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.3**'
- en: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= \sum_{i=1}^3 p_i \log
    \frac{p_i}{q_i} \\ &= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5
    \log \frac{0.5}{0.5} \\ &\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5
    \cdot 0 \\ &\approx 0.0525. \end{align*}\]
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= \sum_{i=1}^3 p_i \log
    \frac{p_i}{q_i} \\ &= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5
    \log \frac{0.5}{0.5} \\ &\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5
    \cdot 0 \\ &\approx 0.0525. \end{align*}\]
- en: '**E8.4.5** The SGD update is given by'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.5** SGD更新由以下给出'
- en: \[\begin{align*} w &\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial
    \ell}{\partial w}(w, b; x_i, y_i), \\ b &\leftarrow b - \frac{\alpha}{|B|} \sum_{i
    \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i). \end{align*}\]
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} w &\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial
    \ell}{\partial w}(w, b; x_i, y_i), \\ b &\leftarrow b - \frac{\alpha}{|B|} \sum_{i
    \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i). \end{align*}\]
- en: Plugging in the values, we get
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 将值代入，我们得到
- en: \[\begin{align*} w &\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 -
    3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\ b &\leftarrow 0 - \frac{0.1}{2} (2(2
    \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3. \end{align*}\]
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} w &\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 -
    3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\ b &\leftarrow 0 - \frac{0.1}{2} (2(2
    \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3. \end{align*}\]
- en: '**E8.4.7**'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.7**'
- en: \[\begin{align*} \nabla \ell(w; x, y) &= -\frac{y}{\sigma(wx)} \sigma'(wx) x
    + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\ &= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx)
    \\ &= x(\sigma(wx) - y). \end{align*}\]
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla \ell(w; x, y) &= -\frac{y}{\sigma(wx)} \sigma'(wx) x
    + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\ &= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx)
    \\ &= x(\sigma(wx) - y). \end{align*}\]
- en: We used the fact that \(\sigma'(z) = \sigma(z)(1-\sigma(z))\).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了这样一个事实：\(\sigma'(z) = \sigma(z)(1-\sigma(z))\).
- en: '**E8.4.9** First, we compute \(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix}
    0 & 0 \\ 0 & 0 \\ 0 & 0 \end{pmatrix} (1, 2) = (0, 0, 0)\). Then, \(\hat{\mathbf{y}}
    = \boldsymbol{\gamma}(\mathbf{z}_1) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\).
    From the text, we have:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.9** 首先，我们计算 \(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix} 0 & 0 \\
    0 & 0 \\ 0 & 0 \end{pmatrix} (1, 2) = (0, 0, 0)\)。然后，\(\hat{\mathbf{y}} = \boldsymbol{\gamma}(\mathbf{z}_1)
    = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)。从文本中，我们有：'
- en: \[\begin{split} \nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y})
    \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3},
    \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3}
    \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}. \end{split}\]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y})
    \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3},
    \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3}
    \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}. \end{split}\]
- en: '**E8.4.11** First, we compute the individual gradients:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.11** 首先，我们计算各个梯度：'
- en: \[\begin{align*} \nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &= (\boldsymbol{\gamma}(W
    \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3})
    \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3}
    \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}, \\ \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)
    &= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3},
    \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} & \frac{2}{3}
    \\ \frac{4}{3} & -\frac{1}{3} \\ \frac{4}{3} & -\frac{1}{3} \end{pmatrix}. \end{align*}\]
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &= (\boldsymbol{\gamma}(W
    \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3})
    \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3}
    \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}, \\ \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)
    &= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3},
    \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} & \frac{2}{3}
    \\ \frac{4}{3} & -\frac{1}{3} \\ \frac{4}{3} & -\frac{1}{3} \end{pmatrix}. \end{align*}\]
- en: 'Then, the full gradient is:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，完整的梯度为：
- en: \[\begin{split} \frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla
    f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3}
    & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}
    + \begin{pmatrix} -\frac{8}{3} & \frac{2}{3} \\ \frac{4}{3} & -\frac{1}{3} \\
    \frac{4}{3} & -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6}
    & \frac{2}{3} \\ \frac{5}{6} & \frac{1}{6} \\ \frac{1}{3} & -\frac{5}{6} \end{pmatrix}.
    \end{split}\]
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla
    f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3}
    & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}
    + \begin{pmatrix} -\frac{8}{3} & \frac{2}{3} \\ \frac{4}{3} & -\frac{1}{3} \\
    \frac{4}{3} & -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6}
    & \frac{2}{3} \\ \frac{5}{6} & \frac{1}{6} \\ \frac{1}{3} & -\frac{5}{6} \end{pmatrix}.
    \end{split}\]
- en: '**E8.4.13** The cross-entropy loss is given by'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.13** 交叉熵损失由'
- en: \[ -\log(0.3) \approx 1.204. \]
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: \[ -\log(0.3) \approx 1.204. \]
- en: '**E8.5.1** \(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\) \(\sigma(-1) =
    \frac{1}{1 + e^{1}} \approx 0.27\) \(\sigma(2) = \frac{1}{1 + e^{-2}} \approx
    0.88\)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.1** \(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\) \(\sigma(-1) =
    \frac{1}{1 + e^{1}} \approx 0.27\) \(\sigma(2) = \frac{1}{1 + e^{-2}} \approx
    0.88\)'
- en: '**E8.5.3** \(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx
    (0.73, 0.27, 0.88)\) \(\bsigma''(\mathbf{z}) = (\bsigma''(1), \bsigma''(-1), \bsigma''(2))
    \approx (0.20, 0.20, 0.10)\)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.3** \(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx
    (0.73, 0.27, 0.88)\) \(\bsigma''(\mathbf{z}) = (\bsigma''(1), \bsigma''(-1), \bsigma''(2))
    \approx (0.20, 0.20, 0.10)\)'
- en: '**E8.5.5** \(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\), so \(\sigma(W\mathbf{x})
    \approx (0.27, 0.98)\), \(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x})
    \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 & 0 \\ 0 & 0.02
    \end{pmatrix}\)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.5** \(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\)，因此 \(\sigma(W\mathbf{x})
    \approx (0.27, 0.98)\)，\(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x})
    \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 & 0 \\ 0 & 0.02
    \end{pmatrix}\)'
- en: '**E8.5.7** \(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2})
    = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.7** \(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2})
    = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)'
- en: 8.7.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.5\. 学习成果[#](#learning-outcomes "链接到这个标题")
- en: Define the Jacobian matrix and use it to compute the differential of a vector-valued
    function.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义雅可比矩阵并使用它来计算向量值函数的微分。
- en: State and apply the generalized Chain Rule to compute the Jacobian of a composition
    of functions.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈述并应用广义链式法则来计算函数复合的雅可比矩阵。
- en: Perform calculations with the Hadamard and Kronecker products of matrices.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用矩阵的Hadamard和Kronecker积进行计算。
- en: Describe the purpose of automatic differentiation and its advantages over symbolic
    and numerical differentiation.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述自动微分的用途及其相对于符号和数值微分的优势。
- en: Implement automatic differentiation in PyTorch to compute gradients of vector-valued
    functions.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中实现自动微分以计算向量值函数的梯度。
- en: Derive the Chain Rule for multi-layer progressive functions and apply it to
    compute gradients.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导多层递进函数的链式法则并将其应用于计算梯度。
- en: Compare and contrast the forward and reverse modes of automatic differentiation
    in terms of computational complexity.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从计算复杂性的角度比较和对比自动微分的前向和反向模式。
- en: Define progressive functions and identify their key properties.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义递进函数并识别它们的关键属性。
- en: Derive the fundamental recursion for the Jacobian of a progressive function.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导递进函数雅可比矩阵的基本递归公式。
- en: Implement the backpropagation algorithm to efficiently compute gradients of
    progressive functions.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现反向传播算法以高效计算渐进函数的梯度。
- en: Analyze the computational complexity of the backpropagation algorithm in terms
    of the number of matrix-vector products.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从矩阵-向量乘数的数量来分析反向传播算法的计算复杂度。
- en: Describe the stochastic gradient descent (SGD) algorithm and explain how it
    differs from standard gradient descent.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述随机梯度下降（SGD）算法，并解释它与标准梯度下降有何不同。
- en: Derive the update rule for stochastic gradient descent from the gradient of
    the loss function.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从损失函数的梯度推导出随机梯度下降的更新规则。
- en: Prove that the expected SGD step is equal to the full gradient descent step.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明期望的SGD步长等于完整梯度下降步长。
- en: Evaluate the performance of models trained using stochastic gradient descent
    on real-world datasets.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在真实世界数据集上评估使用随机梯度下降（SGD）训练的模型性能。
- en: Define the multilayer perceptron (MLP) architecture and describe the role of
    affine maps and activation functions in each layer.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义多层感知器（MLP）的架构，并描述每个层中仿射映射和激活函数的作用。
- en: Compute the Jacobian of the sigmoid activation function using properties of
    diagonal matrices and Kronecker products.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对角矩阵和克罗内克积的性质计算sigmoid激活函数的雅可比矩阵。
- en: Apply the chain rule to calculate the gradient of the loss function with respect
    to the weights in a small MLP example.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个小型MLP示例中应用链式法则来计算损失函数相对于权重的梯度。
- en: Generalize the gradient computation for an MLP with an arbitrary number of layers
    using a forward and backward pass.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正向和反向传播，泛化具有任意层数的多层感知器（MLP）的梯度计算。
- en: Implement the training of a neural network using PyTorch.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch实现神经网络的训练。
- en: \(\aleph\)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: \(\aleph\)
- en: 8.7.1.1\. Just the code[#](#just-the-code "Link to this heading")
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.1\. 仅代码[#](#just-the-code "链接到本标题")
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 下面可以访问包含本章代码的交互式Jupyter笔记本（推荐使用Google Colab）。鼓励您对其进行实验。一些建议的计算练习散布在其中。笔记本也可以作为幻灯片查看。
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb)
    ([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))'
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html)'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[幻灯片](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_nn_notebook_slides.slides.html)'
- en: 8.7.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.2\. 自我评估测验[#](#self-assessment-quizzes "链接到本标题")
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下链接可以获取更全面的自我评估测验的网页版本。
- en: '[Section 8.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html)'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.2节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_2.html)'
- en: '[Section 8.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html)'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.3节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_3.html)'
- en: '[Section 8.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html)'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.4节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_4.html)'
- en: '[Section 8.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html)'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8.5节](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_8_5.html)'
- en: 8.7.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.3\. 自动测验[#](#auto-quizzes "链接到本标题")
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在此处访问本章的自动生成的测验（推荐使用Google Colab）。
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb))'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动测验](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb)
    ([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-nn-autoquiz.ipynb))'
- en: 8.7.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.4\. 奇数编号的预热练习的解答[#](#solutions-to-odd-numbered-warm-up-exercises "链接到这个标题")
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '*(在 Claude、Gemini 和 ChatGPT 的帮助下)*'
- en: '**E8.2.1** The vectorization is obtained by stacking the columns of \(A\):
    \(\text{vec}(A) = (2, 0, 1, -1)\).'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.1** 向量化是通过堆叠 \(A\) 的列得到的：\(\text{vec}(A) = (2, 0, 1, -1)\)。'
- en: '**E8.2.3**'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.3**'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \cdot B & 2 \cdot B \\ -1 \cdot
    B & 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 & -1 & 6 & -2 \\ 2 & 1 & 4 & 2
    \\ -3 & 1 & -6 & 2 \\ -2 & -1 & -4 & -2 \end{pmatrix}. \end{split}\]
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \cdot B & 2 \cdot B \\ -1 \cdot
    B & 0 \cdot B \end{pmatrix} = \begin{pmatrix} 3 & -1 & 6 & -2 \\ 2 & 1 & 4 & 2
    \\ -3 & 1 & -6 & 2 \\ -2 & -1 & -4 & -2 \end{pmatrix}. \end{split}\]
- en: '**E8.2.5**'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.5**'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 & 6 \\ 7 &
    8 \end{pmatrix} & 2 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} \\ 3 \begin{pmatrix}
    5 & 6 \\ 7 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14 & 16 \\ 15 & 18
    & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 1 \begin{pmatrix} 5 & 6 \\ 7 &
    8 \end{pmatrix} & 2 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} \\ 3 \begin{pmatrix}
    5 & 6 \\ 7 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14 & 16 \\ 15 & 18
    & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix}. \end{split}\]
- en: '**E8.2.7** First, compute the Jacobian matrices of \(\mathbf{f}\) and \(\mathbf{g}\):'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.7** 首先，计算 \(\mathbf{f}\) 和 \(\mathbf{g}\) 的雅可比矩阵：'
- en: \[\begin{split} J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x & 2y \\ y & x \end{pmatrix},
    \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v & u \\ 1 & 1 \end{pmatrix}. \end{split}\]
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{f}}(x, y) = \begin{pmatrix} 2x & 2y \\ y & x \end{pmatrix},
    \quad J_{\mathbf{g}}(u, v) = \begin{pmatrix} v & u \\ 1 & 1 \end{pmatrix}. \end{split}\]
- en: Then, by the Chain Rule,
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据链式法则，
- en: \[\begin{split} J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1,
    2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix}
    2 & 5 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 2 & 4 \\ 2 & 1 \end{pmatrix} = \begin{pmatrix}
    14 & 13 \\ 4 & 5 \end{pmatrix}. \end{split}\]
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g} \circ \mathbf{f}}(1, 2) = J_{\mathbf{g}}(\mathbf{f}(1,
    2)) \, J_{\mathbf{f}}(1, 2) = J_{\mathbf{g}}(5, 2) \, J_{\mathbf{f}}(1, 2) = \begin{pmatrix}
    2 & 5 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 2 & 4 \\ 2 & 1 \end{pmatrix} = \begin{pmatrix}
    14 & 13 \\ 4 & 5 \end{pmatrix}. \end{split}\]
- en: '**E8.2.9** From E8.2.5, we have'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.9** 从 E8.2.5，我们有'
- en: \[\begin{split} A \otimes B = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14
    & 16 \\ 15 & 18 & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix} \end{split}\]
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A \otimes B = \begin{pmatrix} 5 & 6 & 10 & 12 \\ 7 & 8 & 14
    & 16 \\ 15 & 18 & 20 & 24 \\ 21 & 24 & 28 & 32 \end{pmatrix} \end{split}\]
- en: So,
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 &
    18 & 24 \\ 10 & 14 & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split}(A \otimes B)^T = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 &
    18 & 24 \\ 10 & 14 & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
- en: Now,
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，
- en: \[\begin{split} A^T = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \quad B^T
    = \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \end{split}\]
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^T = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \quad B^T
    = \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \end{split}\]
- en: So,
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: \[\begin{split} A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 & 7 \\
    6 & 8 \end{pmatrix} & 3 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \\ 2 \begin{pmatrix}
    5 & 7 \\ 6 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 & 18 & 24 \\ 10 & 14
    & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A^T \otimes B^T = \begin{pmatrix} 1 \begin{pmatrix} 5 & 7 \\
    6 & 8 \end{pmatrix} & 3 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix} \\ 2 \begin{pmatrix}
    5 & 7 \\ 6 & 8 \end{pmatrix} & 4 \begin{pmatrix} 5 & 7 \\ 6 & 8 \end{pmatrix}
    \end{pmatrix} = \begin{pmatrix} 5 & 7 & 15 & 21 \\ 6 & 8 & 18 & 24 \\ 10 & 14
    & 20 & 28 \\ 12 & 16 & 24 & 32 \end{pmatrix}. \end{split}\]
- en: We see that \((A \otimes B)^T = A^T \otimes B^T\), as expected from the properties
    of the Kronecker product.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 \((A \otimes B)^T = A^T \otimes B^T\)，正如 Kronecker 积的性质所预期的那样。
- en: '**E8.2.11**'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.11**'
- en: \[\begin{split} \nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial
    x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix}
    = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}. \end{split}\]
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(x, y, z) = \begin{pmatrix} \frac{\partial f}{\partial
    x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{pmatrix}
    = \begin{pmatrix} 2x \\ 2y \\ 2z \end{pmatrix}. \end{split}\]
- en: So,
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: \[\begin{split} \nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
    \end{split}\]
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(1, 2, 3) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix}.
    \end{split}\]
- en: '**E8.2.13** First, compute the gradient of \(f\) and the Jacobian matrix of
    \(\mathbf{g}\):'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.13** 首先，计算 \(f\) 的梯度以及 \(\mathbf{g}\) 的雅可比矩阵：'
- en: \[\begin{split} \nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad
    J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}. \end{split}\]
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(x, y) = \begin{pmatrix} y \\ x \end{pmatrix}, \quad
    J_{\mathbf{g}}(x, y) = \begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}. \end{split}\]
- en: Then, by the Chain Rule,
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据**链式法则**，
- en: \[\begin{split} J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T
    \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix}
    4 & 1 \end{pmatrix} \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix} = \begin{pmatrix}
    8 & 4 \end{pmatrix}. \end{split}\]
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{f \circ \mathbf{g}}(1, 2) = \nabla f(\mathbf{g}(1, 2))^T
    \, J_{\mathbf{g}}(1, 2) = \nabla f(1, 4)^T \, J_{\mathbf{g}}(1, 2) = \begin{pmatrix}
    4 & 1 \end{pmatrix} \begin{pmatrix} 2 & 0 \\ 0 & 4 \end{pmatrix} = \begin{pmatrix}
    8 & 4 \end{pmatrix}. \end{split}\]
- en: '**E8.2.15** The Jacobian matrix of \(\mathbf{g}\) is'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.2.15** \(\mathbf{g}\) 的雅可比矩阵为'
- en: \[\begin{split} J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) & 0 & 0 \\ 0
    & f'(y) & 0 \\ 0 & 0 & f'(z) \end{pmatrix} \end{split}\]
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(x, y, z) = \begin{pmatrix} f'(x) & 0 & 0 \\ 0
    & f'(y) & 0 \\ 0 & 0 & f'(z) \end{pmatrix} \end{split}\]
- en: where \(f'(x) = \cos(x).\) So,
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(f'(x) = \cos(x).\) 因此，
- en: \[\begin{split} J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6})
    = \begin{pmatrix} \cos(\frac{\pi}{2}) & 0 & 0 \\ 0 & \cos(\frac{\pi}{4}) & 0 \\
    0 & 0 & \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & \frac{\sqrt{2}}{2}
    & 0 \\ 0 & 0 & \frac{\sqrt{3}}{2} \end{pmatrix}. \end{split}\]
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathbf{g}}(\frac{\pi}{2}, \frac{\pi}{4}, \frac{\pi}{6})
    = \begin{pmatrix} \cos(\frac{\pi}{2}) & 0 & 0 \\ 0 & \cos(\frac{\pi}{4}) & 0 \\
    0 & 0 & \cos(\frac{\pi}{6}) \end{pmatrix} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & \frac{\sqrt{2}}{2}
    & 0 \\ 0 & 0 & \frac{\sqrt{3}}{2} \end{pmatrix}. \end{split}\]
- en: '**E8.3.1** Each entry of \(AB\) is the dot product of a row of \(A\) and a
    column of \(B\), which takes 2 multiplications and 1 addition. Since \(AB\) has
    4 entries, the total number of operations is \(4 \times 3 = 12\).'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.1** 矩阵 \(AB\) 的每个元素是 \(A\) 的某一行与 \(B\) 的某一列的点积，这需要 2 次乘法和 1 次加法。由于 \(AB\)
    有 4 个元素，所以总的操作次数是 \(4 \times 3 = 12\)。'
- en: '**E8.3.3** We have'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.3** 我们有'
- en: \[ \ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2 \]
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \hat{y}_1^2 + \hat{y}_2^2 \]
- en: so the partial derivatives are \(\frac{\partial \ell}{\partial \hat{y}_1} =
    2 \hat{y}_1\) and \(\frac{\partial \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\) and
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偏导数为 \(\frac{\partial \ell}{\partial \hat{y}_1} = 2 \hat{y}_1\) 和 \(\frac{\partial
    \ell}{\partial \hat{y}_2} = 2 \hat{y}_2\) 和
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T. \]
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\hat{\mathbf{y}}) = 2 \hat{\mathbf{y}}^T. \]
- en: '**E8.3.5** From E8.3.4, we have \(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0)
    = (-1, -1)\). Then, \(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\) and
    \(f(\mathbf{x}) = \ell(\mathbf{z}_2) = 5\). By the *Chain Rule*,'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.5** 从 E8.3.4，我们得到 \(\mathbf{z}_1 = \mathbf{g}_0(\mathbf{z}_0) = (-1,
    -1)\)。然后，\(\mathbf{z}_2 = \mathbf{g}_1(\mathbf{z}_1) = (1, -2)\) 且 \(f(\mathbf{x})
    = \ell(\mathbf{z}_2) = 5\)。根据**链式法则**，'
- en: \[\begin{split} \nabla f(\mathbf{x})^T = J_f(\mathbf{x}) = J_{\ell}(\mathbf{z}_1)
    \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) = 2 \mathbf{z}_2^T
    \begin{pmatrix} -1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ -1 & 0
    \end{pmatrix} = (-10, -4) \begin{pmatrix} 1 & 2 \\ -1 & 0 \end{pmatrix} = (6,
    -20). \end{split}\]
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(\mathbf{x})^T = J_f(\mathbf{x}) = J_{\ell}(\mathbf{z}_1)
    \,J_{\mathbf{g}_1}(\mathbf{z}_1) \,J_{\mathbf{g}_0}(\mathbf{z}_0) = 2 \mathbf{z}_2^T
    \begin{pmatrix} -1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ -1 & 0
    \end{pmatrix} = (-10, -4) \begin{pmatrix} 1 & 2 \\ -1 & 0 \end{pmatrix} = (6,
    -20). \end{split}\]
- en: '**E8.3.7** We have'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.7** 我们有'
- en: \[ g_1(\mathbf{z}_1, \mathbf{w}_1) = w_4 z_{1,1} + w_5 z_{1,2} \]
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: \[ g_1(\mathbf{z}_1, \mathbf{w}_1) = w_4 z_{1,1} + w_5 z_{1,2} \]
- en: so, by computing all partial derivatives,
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过计算所有偏导数，
- en: \[ J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) = \begin{pmatrix} w_4 & w_5 & z_{1,1}
    & z_{1,2} \end{pmatrix} = \begin{pmatrix} \mathbf{w}_1^T & \mathbf{z}_1^T \end{pmatrix}
    = \begin{pmatrix} W_1 & I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}. \]
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{g_1}(\mathbf{z}_1, \mathbf{w}_1) = \begin{pmatrix} w_4 & w_5 & z_{1,1}
    & z_{1,2} \end{pmatrix} = \begin{pmatrix} \mathbf{w}_1^T & \mathbf{z}_1^T \end{pmatrix}
    = \begin{pmatrix} W_1 & I_{1 \times 1} \otimes \mathbf{z}_1^T \end{pmatrix}. \]
- en: Using the notation in the text, \(A_1 = W_1\) and \(B_1 = \mathbf{z}_1^T = I_{1
    \times 1} \otimes \mathbf{z}_1^T\).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本中的符号，\(A_1 = W_1\) 和 \(B_1 = \mathbf{z}_1^T = I_{1 \times 1} \otimes \mathbf{z}_1^T\).
- en: '**E8.3.9** We have'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.3.9** 我们有'
- en: \[ f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2 \]
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = (w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))^2 \]
- en: so, by the *Chain Rule*, the partial derivatives are
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据**链式法则**，偏导数是
- en: \[ \frac{\partial f}{\partial w_0} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))
    (-w_4) \]\[ \frac{\partial f}{\partial w_1} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2
    + w_3)) (w_4) \]\[ \frac{\partial f}{\partial w_2} = 2(w_4 (- w_0 + w_1) + w_5
    (-w_2 + w_3)) (-w_5) \]\[ \frac{\partial f}{\partial w_3} = 2(w_4 (- w_0 + w_1)
    + w_5 (-w_2 + w_3)) (w_5) \]\[ \frac{\partial f}{\partial w_4} = 2(w_4 (- w_0
    + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1) \]\[ \frac{\partial f}{\partial w_5} =
    2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3). \]
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial f}{\partial w_0} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3))
    (-w_4) \]\[ \frac{\partial f}{\partial w_1} = 2(w_4 (- w_0 + w_1) + w_5 (-w_2
    + w_3)) (w_4) \]\[ \frac{\partial f}{\partial w_2} = 2(w_4 (- w_0 + w_1) + w_5
    (-w_2 + w_3)) (-w_5) \]\[ \frac{\partial f}{\partial w_3} = 2(w_4 (- w_0 + w_1)
    + w_5 (-w_2 + w_3)) (w_5) \]\[ \frac{\partial f}{\partial w_4} = 2(w_4 (- w_0
    + w_1) + w_5 (-w_2 + w_3)) (-w_0 + w_1) \]\[ \frac{\partial f}{\partial w_5} =
    2(w_4 (- w_0 + w_1) + w_5 (-w_2 + w_3)) (-w_2 + w_3). \]
- en: Moreover, by E8.3.7,
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据E8.3.7，
- en: \[\begin{split} z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1) = W_1 \mathbf{z}_1 = \begin{pmatrix}
    w_4 & w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix} = w_4
    (- w_0 + w_1) + w_5 (-w_2 + w_3). \end{split}\]
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} z_2 = g_1(\mathbf{z}_1, \mathbf{w}_1) = W_1 \mathbf{z}_1 = \begin{pmatrix}
    w_4 & w_5\end{pmatrix} \begin{pmatrix}- w_0 + w_1\\-w_2 + w_3\end{pmatrix} = w_4
    (- w_0 + w_1) + w_5 (-w_2 + w_3). \end{split}\]
- en: By the fundamental recursion and the results in E8.3.3 and E8.3.8,
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基本递归和E8.3.3以及E8.3.8的结果，
- en: \[\begin{align*} J_f(\mathbf{w}) &= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
    = 2 z_2 \begin{pmatrix} A_1 B_0 & B_1\end{pmatrix}\\ &= 2 (w_4 (- w_0 + w_1) +
    w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3). \end{align*}\]
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} J_f(\mathbf{w}) &= J_{\ell}(h(\mathbf{w})) \,J_{h}(\mathbf{w})
    = 2 z_2 \begin{pmatrix} A_1 B_0 & B_1\end{pmatrix}\\ &= 2 (w_4 (- w_0 + w_1) +
    w_5 (-w_2 + w_3)) (-w_4, w_4, -w_5, w_5, -w_0 + w_1, -w_2 + w_3). \end{align*}\]
- en: '**E8.4.1** The full gradient descent step is:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.1** 完整的梯度下降步骤是：'
- en: \[ \frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1,
    2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}). \]
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{5} \sum_{i=1}^5 \nabla f_{\mathbf{x}_i, y_i}(w) = \frac{1}{5}((1,
    2) + (-1, 1) + (0, -1) + (2, 0) + (1, 1)) = (\frac{3}{5}, \frac{3}{5}). \]
- en: 'The expected SGD step with a batch size of 2 is:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 批大小为2的期望随机梯度下降步骤是：
- en: \[ \mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] =
    \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
    \]
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{E} [\frac{1}{2} \sum_{i\in B} \nabla f_{\mathbf{x}_i, y_i}(w)] =
    \frac{1}{5} \sum_{i=1}^5 \nabla f_{x_i, y_i}(w) = (\frac{3}{5}, \frac{3}{5}),
    \]
- en: which is equal to the full gradient descent step, as proven in the “Expected
    SGD Step” lemma.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这等于完整的梯度下降步骤，如“期望随机梯度下降步骤”引理中证明的。
- en: '**E8.4.3**'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.3**'
- en: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= \sum_{i=1}^3 p_i \log
    \frac{p_i}{q_i} \\ &= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5
    \log \frac{0.5}{0.5} \\ &\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5
    \cdot 0 \\ &\approx 0.0525. \end{align*}\]
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= \sum_{i=1}^3 p_i \log
    \frac{p_i}{q_i} \\ &= 0.2 \log \frac{0.2}{0.1} + 0.3 \log \frac{0.3}{0.4} + 0.5
    \log \frac{0.5}{0.5} \\ &\approx 0.2 \cdot 0.6931 + 0.3 \cdot (-0.2877) + 0.5
    \cdot 0 \\ &\approx 0.0525. \end{align*}\]
- en: '**E8.4.5** The SGD update is given by'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.5** 随机梯度下降的更新由'
- en: \[\begin{align*} w &\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial
    \ell}{\partial w}(w, b; x_i, y_i), \\ b &\leftarrow b - \frac{\alpha}{|B|} \sum_{i
    \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i). \end{align*}\]
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} w &\leftarrow w - \frac{\alpha}{|B|} \sum_{i \in B} \frac{\partial
    \ell}{\partial w}(w, b; x_i, y_i), \\ b &\leftarrow b - \frac{\alpha}{|B|} \sum_{i
    \in B} \frac{\partial \ell}{\partial b}(w, b; x_i, y_i). \end{align*}\]
- en: Plugging in the values, we get
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 将值代入，我们得到
- en: \[\begin{align*} w &\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 -
    3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\ b &\leftarrow 0 - \frac{0.1}{2} (2(2
    \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3. \end{align*}\]
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} w &\leftarrow 1 - \frac{0.1}{2} (2 \cdot 2(2 \cdot 1 + 0 -
    3) + 2 \cdot 1(1 \cdot 1 + 0 - 2)) = 1.3, \\ b &\leftarrow 0 - \frac{0.1}{2} (2(2
    \cdot 1 + 0 - 3) + 2(1 \cdot 1 + 0 - 2)) = 0.3. \end{align*}\]
- en: '**E8.4.7**'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.7**'
- en: \[\begin{align*} \nabla \ell(w; x, y) &= -\frac{y}{\sigma(wx)} \sigma'(wx) x
    + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\ &= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx)
    \\ &= x(\sigma(wx) - y). \end{align*}\]
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla \ell(w; x, y) &= -\frac{y}{\sigma(wx)} \sigma'(wx) x
    + \frac{1-y}{1-\sigma(wx)} \sigma'(wx) x \\ &= -yx(1 - \sigma(wx)) + x(1-y)\sigma(wx)
    \\ &= x(\sigma(wx) - y). \end{align*}\]
- en: We used the fact that \(\sigma'(z) = \sigma(z)(1-\sigma(z))\).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了\(\sigma'(z) = \sigma(z)(1-\sigma(z))\)的事实。
- en: '**E8.4.9** First, we compute \(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix}
    0 & 0 \\ 0 & 0 \\ 0 & 0 \end{pmatrix} (1, 2) = (0, 0, 0)\). Then, \(\hat{\mathbf{y}}
    = \boldsymbol{\gamma}(\mathbf{z}_1) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\).
    From the text, we have:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.9** 首先，我们计算 \(\mathbf{z}_1 = W\mathbf{x} = \begin{pmatrix} 0 & 0 \\
    0 & 0 \\ 0 & 0 \end{pmatrix} (1, 2) = (0, 0, 0)\)。然后，\(\hat{\mathbf{y}} = \boldsymbol{\gamma}(\mathbf{z}_1)
    = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\)。从文本中，我们有：'
- en: \[\begin{split} \nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y})
    \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3},
    \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3}
    \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}. \end{split}\]
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \nabla f(\mathbf{w}) = (\boldsymbol{\gamma}(W\mathbf{x}) - \mathbf{y})
    \otimes \mathbf{x} = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{x} = (\frac{1}{3},
    \frac{1}{3}, -\frac{2}{3}) \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3}
    \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}. \end{split}\]
- en: '**E8.4.11** First, we compute the individual gradients:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.11** 首先，我们计算各个梯度：'
- en: \[\begin{align*} \nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &= (\boldsymbol{\gamma}(W
    \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3})
    \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3}
    \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}, \\ \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)
    &= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3},
    \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} & \frac{2}{3}
    \\ \frac{4}{3} & -\frac{1}{3} \\ \frac{4}{3} & -\frac{1}{3} \end{pmatrix}. \end{align*}\]
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) &= (\boldsymbol{\gamma}(W
    \mathbf{x}_1) - \mathbf{y}_1) \otimes x_1 = (\frac{1}{3}, \frac{1}{3}, -\frac{2}{3})
    \otimes (1, 2) = \begin{pmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3}
    \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}, \\ \nabla f_{\mathbf{x}_2, \mathbf{y}_2}(W)
    &= (\boldsymbol{\gamma}(W\mathbf{x}_2) - \mathbf{y}_2) \otimes x_2 = (-\frac{2}{3},
    \frac{1}{3}, \frac{1}{3}) \otimes (4, -1) = \begin{pmatrix} -\frac{8}{3} & \frac{2}{3}
    \\ \frac{4}{3} & -\frac{1}{3} \\ \frac{4}{3} & -\frac{1}{3} \end{pmatrix}. \end{align*}\]
- en: 'Then, the full gradient is:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，完整的梯度是：
- en: \[\begin{split} \frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla
    f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3}
    & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}
    + \begin{pmatrix} -\frac{8}{3} & \frac{2}{3} \\ \frac{4}{3} & -\frac{1}{3} \\
    \frac{4}{3} & -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6}
    & \frac{2}{3} \\ \frac{5}{6} & \frac{1}{6} \\ \frac{1}{3} & -\frac{5}{6} \end{pmatrix}.
    \end{split}\]
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{1}{2} (\nabla f_{\mathbf{x}_1, \mathbf{y}_1}(W) + \nabla
    f_{\mathbf{x}_2, \mathbf{y}_2}(W)) = \frac{1}{2} \left(\begin{pmatrix} \frac{1}{3}
    & \frac{2}{3} \\ \frac{1}{3} & \frac{2}{3} \\ -\frac{2}{3} & -\frac{4}{3} \end{pmatrix}
    + \begin{pmatrix} -\frac{8}{3} & \frac{2}{3} \\ \frac{4}{3} & -\frac{1}{3} \\
    \frac{4}{3} & -\frac{1}{3} \end{pmatrix}\right) = \begin{pmatrix} -\frac{11}{6}
    & \frac{2}{3} \\ \frac{5}{6} & \frac{1}{6} \\ \frac{1}{3} & -\frac{5}{6} \end{pmatrix}.
    \end{split}\]
- en: '**E8.4.13** The cross-entropy loss is given by'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.4.13** 交叉熵损失由以下公式给出'
- en: \[ -\log(0.3) \approx 1.204. \]
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: \[ -\log(0.3) \approx 1.204. \]
- en: '**E8.5.1** \(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\) \(\sigma(-1) =
    \frac{1}{1 + e^{1}} \approx 0.27\) \(\sigma(2) = \frac{1}{1 + e^{-2}} \approx
    0.88\)'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.1** \(\sigma(1) = \frac{1}{1 + e^{-1}} \approx 0.73\) \(\sigma(-1) =
    \frac{1}{1 + e^{1}} \approx 0.27\) \(\sigma(2) = \frac{1}{1 + e^{-2}} \approx
    0.88\)'
- en: '**E8.5.3** \(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx
    (0.73, 0.27, 0.88)\) \(\bsigma''(\mathbf{z}) = (\bsigma''(1), \bsigma''(-1), \bsigma''(2))
    \approx (0.20, 0.20, 0.10)\)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.3** \(\bsigma(\mathbf{z}) = (\bsigma(1), \bsigma(-1), \bsigma(2)) \approx
    (0.73, 0.27, 0.88)\) \(\bsigma''(\mathbf{z}) = (\bsigma''(1), \bsigma''(-1), \bsigma''(2))
    \approx (0.20, 0.20, 0.10)\)'
- en: '**E8.5.5** \(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\), so \(\sigma(W\mathbf{x})
    \approx (0.27, 0.98)\), \(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x})
    \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 & 0 \\ 0 & 0.02
    \end{pmatrix}\)'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.5** \(W\mathbf{x} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}\)，所以 \(\sigma(W\mathbf{x})
    \approx (0.27, 0.98)\)，\(J_\bsigma(W\mathbf{x}) = \mathrm{diag}(\bsigma(W\mathbf{x})
    \odot (1 - \bsigma(W\mathbf{x}))) \approx \begin{pmatrix} 0.20 & 0 \\ 0 & 0.02
    \end{pmatrix}\)'
- en: '**E8.5.7** \(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2})
    = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '**E8.5.7** \(\nabla H(\mathbf{y}, \mathbf{z}) = (-\frac{y_1}{z_1}, -\frac{y_2}{z_2})
    = (-\frac{0}{0.3}, -\frac{1}{0.7}) \approx (0, -1.43)\)'
- en: 8.7.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1.5\. 学习成果[#](#learning-outcomes "链接到这个标题")
- en: Define the Jacobian matrix and use it to compute the differential of a vector-valued
    function.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义雅可比矩阵并使用它来计算向量值函数的微分。
- en: State and apply the generalized Chain Rule to compute the Jacobian of a composition
    of functions.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态并应用广义链式法则来计算函数复合的雅可比矩阵。
- en: Perform calculations with the Hadamard and Kronecker products of matrices.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行矩阵的Hadamard积和克罗内克积的计算。
- en: Describe the purpose of automatic differentiation and its advantages over symbolic
    and numerical differentiation.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述自动微分的用途及其与符号微分和数值微分相比的优势。
- en: Implement automatic differentiation in PyTorch to compute gradients of vector-valued
    functions.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中实现自动微分以计算向量值函数的梯度。
- en: Derive the Chain Rule for multi-layer progressive functions and apply it to
    compute gradients.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导多层渐进函数的链式法则，并将其应用于计算梯度。
- en: Compare and contrast the forward and reverse modes of automatic differentiation
    in terms of computational complexity.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较自动微分的正向和反向模式的计算复杂度。
- en: Define progressive functions and identify their key properties.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义渐进函数并识别其关键属性。
- en: Derive the fundamental recursion for the Jacobian of a progressive function.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导渐进函数雅可比矩阵的基本递归公式。
- en: Implement the backpropagation algorithm to efficiently compute gradients of
    progressive functions.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现反向传播算法以高效地计算渐进函数的梯度。
- en: Analyze the computational complexity of the backpropagation algorithm in terms
    of the number of matrix-vector products.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析反向传播算法的计算复杂度，从矩阵-向量乘数的数量来考虑。
- en: Describe the stochastic gradient descent (SGD) algorithm and explain how it
    differs from standard gradient descent.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述随机梯度下降（SGD）算法，并解释它与标准梯度下降的不同之处。
- en: Derive the update rule for stochastic gradient descent from the gradient of
    the loss function.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从损失函数的梯度推导出随机梯度下降的更新规则。
- en: Prove that the expected SGD step is equal to the full gradient descent step.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明期望的SGD步骤等于完整梯度下降步骤。
- en: Evaluate the performance of models trained using stochastic gradient descent
    on real-world datasets.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估在真实世界数据集上使用随机梯度下降训练的模型性能。
- en: Define the multilayer perceptron (MLP) architecture and describe the role of
    affine maps and activation functions in each layer.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义多层感知器（MLP）架构，并描述每一层中仿射映射和激活函数的作用。
- en: Compute the Jacobian of the sigmoid activation function using properties of
    diagonal matrices and Kronecker products.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用对角矩阵和克罗内克积的性质计算sigmoid激活函数的雅可比矩阵。
- en: Apply the chain rule to calculate the gradient of the loss function with respect
    to the weights in a small MLP example.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个小型MLP示例中应用链式法则计算损失函数相对于权重的梯度。
- en: Generalize the gradient computation for an MLP with an arbitrary number of layers
    using a forward and backward pass.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将梯度计算推广到具有任意层数的MLP，使用正向和反向传递。
- en: Implement the training of a neural network using PyTorch.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch实现神经网络的训练。
- en: \(\aleph\)
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: \(\aleph\)
- en: 8.7.2\. Additional sections[#](#additional-sections "Link to this heading")
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7.2. Additional sections[#](#additional-sections "链接到这个标题")
- en: '8.7.2.1\. Another example: linear regression[#](#another-example-linear-regression
    "Link to this heading")'
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2.1. 另一个例子：线性回归[#](#another-example-linear-regression "链接到这个标题")
- en: We give another concrete example of progressive functions and of the application
    of backpropagration and stochastic gradient descent.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给出另一个关于渐进函数以及反向传播和随机梯度下降应用的实例。
- en: '**Computing the gradient** While we have motivated the framework introduced
    in the previous section from the point of view of classification, it also immediately
    applies to the regression setting. Both classification and regression are instances
    of supervised learning.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算梯度** 当我们从分类的角度出发，激励了上一节中引入的框架，它也立即适用于回归设置。分类和回归都是监督学习的实例。'
- en: We first compute the gradient of a single sample. Here \(\mathbf{x} \in \mathbb{R}^d\)
    again, but \(y\) is a real-valued outcome variable. We revisit the case of linear
    regression where the loss function is
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算单个样本的梯度。这里 \(\mathbf{x} \in \mathbb{R}^d\)，但 \(y\) 是一个实值结果变量。我们回顾线性回归的案例，其中损失函数是
- en: \[ \ell(z) = (z - y)^2 \]
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(z) = (z - y)^2 \]
- en: and the regression function only has input and output layers and no hidden layer
    (that is, \(L=0\)) with
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 并且回归函数只有输入层和输出层，没有隐藏层（即 \(L=0\)），
- en: \[ h(\mathbf{w}) = \sum_{j=1}^d w_{j} x_{j} = \mathbf{x}^T\mathbf{w}, \]
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h(\mathbf{w}) = \sum_{j=1}^d w_{j} x_{j} = \mathbf{x}^T\mathbf{w}, \]
- en: where \(\mathbf{w} \in \mathbb{R}^{d}\) are the parameters. Recall that we can
    include a constant term (one that does not depend on the input) by adding a \(1\)
    to the input. To keep the notation simple, we assume that this pre-processing
    has already been performed if desired.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w} \in \mathbb{R}^{d}\) 是参数。回想一下，我们可以通过在输入中添加一个常数项（一个不依赖于输入的项）来包含一个
    \(1\)。为了简化符号，我们假设如果需要，这个预处理已经完成。
- en: Finally, the objective function for a single sample is
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，单个样本的目标函数是
- en: \[ f(\mathbf{w}) = \ell(h(\mathbf{w})) = \left(\sum_{j=1}^d w_{j} x_{j} - y\right)^2
    = \left(\mathbf{x}^T\mathbf{w} - y\right)^2\. \]
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = \ell(h(\mathbf{w})) = \left(\sum_{j=1}^d w_{j} x_{j} - y\right)^2
    = \left(\mathbf{x}^T\mathbf{w} - y\right)^2\. \]
- en: 'Using the notation from the previous subsection, the forward pass in this case
    is:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一小节中的符号，这种情况下的前向传播是：
- en: '*Initialization:*'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化：*'
- en: \[\mathbf{z}_0 := \mathbf{x}.\]
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbf{z}_0 := \mathbf{x}.\]
- en: '*Forward layer loop:*'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '*前向层循环：*'
- en: \[\begin{align*} \hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0) &= \sum_{j=1}^d
    w_{0,j} z_{0,j} = \mathbf{z}_0^T \mathbf{w}_0 \end{align*}\]\[\begin{align*} \begin{pmatrix}
    A_0 & B_0 \end{pmatrix} := J_{g_0}(\mathbf{z}_0,\mathbf{w}_0) &= ( w_{0,1},\ldots,
    w_{0,d},z_{0,1},\ldots,z_{0,d} )^T = \begin{pmatrix}\mathbf{w}_0^T & \mathbf{z}_0^T\end{pmatrix},
    \end{align*}\]
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0) &= \sum_{j=1}^d
    w_{0,j} z_{0,j} = \mathbf{z}_0^T \mathbf{w}_0 \end{align*}\]\[\begin{align*} \begin{pmatrix}
    A_0 & B_0 \end{pmatrix} := J_{g_0}(\mathbf{z}_0,\mathbf{w}_0) &= ( w_{0,1},\ldots,
    w_{0,d},z_{0,1},\ldots,z_{0,d} )^T = \begin{pmatrix}\mathbf{w}_0^T & \mathbf{z}_0^T\end{pmatrix},
    \end{align*}\]
- en: where \(\mathbf{w}_0 := \mathbf{w}\).
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w}_0 := \mathbf{w}\)。
- en: '*Loss:*'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失：*'
- en: \[\begin{align*} z_2 &:= \ell(z_1) = (z_1 - y)^2\\ p_2 &:= \frac{\mathrm{d}}{\mathrm{d}
    z_1} {\ell}(z_1) = 2 (z_1 - y). \end{align*}\]
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} z_2 &:= \ell(z_1) = (z_1 - y)^2\\ p_2 &:= \frac{\mathrm{d}}{\mathrm{d}
    z_1} {\ell}(z_1) = 2 (z_1 - y). \end{align*}\]
- en: 'The backward pass is:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是：
- en: '*Backward layer loop:*'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向层循环：*'
- en: \[\begin{align*} \mathbf{q}_0 := B_0^T p_1 &= 2 (z_1 - y) \, \mathbf{z}_0. \end{align*}\]
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{q}_0 := B_0^T p_1 &= 2 (z_1 - y) \, \mathbf{z}_0. \end{align*}\]
- en: '*Output:*'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0. \]
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0. \]
- en: As we noted before, there is in fact no need to compute \(A_0\) and \(\mathbf{p}_0\).
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，实际上没有必要计算 \(A_0\) 和 \(\mathbf{p}_0\)。
- en: '**The `Advertising` dataset and the least-squares solution** We return to the
    `Advertising` dataset.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '**`Advertising` 数据集和最小二乘解** 我们回到 `Advertising` 数据集。'
- en: '[PRE33]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '|  | Unnamed: 0 | TV | radio | newspaper | sales |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|  | Unnamed: 0 | TV | radio | newspaper | sales |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 1 | 230.1 | 37.8 | 69.2 | 22.1 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 230.1 | 37.8 | 69.2 | 22.1 |'
- en: '| 1 | 2 | 44.5 | 39.3 | 45.1 | 10.4 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 44.5 | 39.3 | 45.1 | 10.4 |'
- en: '| 2 | 3 | 17.2 | 45.9 | 69.3 | 9.3 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 17.2 | 45.9 | 69.3 | 9.3 |'
- en: '| 3 | 4 | 151.5 | 41.3 | 58.5 | 18.5 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 151.5 | 41.3 | 58.5 | 18.5 |'
- en: '| 4 | 5 | 180.8 | 10.8 | 58.4 | 12.9 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5 | 180.8 | 10.8 | 58.4 | 12.9 |'
- en: '[PRE34]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We first compute the solution using the least-squares approach we detailed previously.
    We use [`numpy.column_stack`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack)
    to add a column of ones to the feature vectors.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用之前详细说明的最小二乘法来计算解。我们使用 `numpy.column_stack` 在特征向量中添加一列 \(1\)。
- en: '[PRE36]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The MSE is:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）是：
- en: '[PRE38]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Solving the problem using PyTorch** We will be using PyTorch to implement
    the previous method. We first convert the data into PyTorch tensors. We then use
    [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)
    to create the dataset. Finally, [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    provides the utilities to load the data in batches for training. We take mini-batches
    of size `BATCH_SIZE = 64` and we apply a random permutation of the samples on
    every pass (with the option `shuffle=True`).'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 PyTorch 解决问题** 我们将使用 PyTorch 来实现前面的方法。我们首先将数据转换为 PyTorch 张量。然后使用 `torch.utils.data.TensorDataset`
    创建数据集。最后，`torch.utils.data.DataLoader` 提供了加载数据以进行批处理的工具。我们取大小为 `BATCH_SIZE = 64`
    的迷你批次，并在每次传递时对样本进行随机排列（使用选项 `shuffle=True`）。'
- en: '[PRE40]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now we construct our model. It is simply an affine map from \(\mathbb{R}^3\)
    to \(\mathbb{R}\). Note that there is no need to pre-process the inputs by adding
    \(1\)s. A constant term (or “bias variable”) is automatically added by PyTorch
    (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们构建我们的模型。它只是一个从 \(\mathbb{R}^3\) 到 \(\mathbb{R}\) 的仿射映射。请注意，没有必要通过添加 \(1\)
    来预先处理输入。PyTorch（除非选择选项[`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)）会自动添加一个常数项（或“偏置变量”）。
- en: '[PRE41]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Finally, we are ready to run an optimization method of our choice on the loss
    function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)
    available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)
    for a brief explanation of many common optimizers.) Here we use SGD as the optimizer.
    And the loss function is the MSE. A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好在我们的损失函数上运行我们选择的优化方法，这些方法将在下面指定。有许多[优化器](https://pytorch.org/docs/stable/optim.html#algorithms)可供选择。（参见这篇[文章](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)，了解许多常见优化器的简要说明。）这里我们使用SGD作为优化器。损失函数是均方误差（MSE）。快速教程在这里[这里](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)。
- en: '[PRE42]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Choosing the right number of passes (i.e. epochs) through the data requires
    some experimenting. Here \(10^4\) suffices. But in the interest of time, we will
    run it only for \(100\) epochs. As you will see from the results, this is not
    quite enough. On each pass, we compute the output of the current model, use `backward()`
    to obtain the gradient, and then perform a descent update with `step()`. We also
    have to reset the gradients first (otherwise they add up by default).
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的数据遍历次数（即epoch）需要一些实验。这里 \(10^4\) 就足够了。但为了节省时间，我们只运行 \(100\) 个epoch。正如您将从结果中看到的那样，这并不够。在每次遍历中，我们计算当前模型的输出，使用
    `backward()` 获取梯度，然后使用 `step()` 执行下降更新。我们还需要首先重置梯度（否则它们会默认累加）。
- en: '[PRE43]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The final parameters and loss are:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的参数和损失如下：
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE44]</details>'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE44]</details>'
- en: '[PRE45]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE46]</details>'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE46]</details>'
- en: '[PRE47]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 8.7.2.2\. Convolutional neural networks[#](#convolutional-neural-networks "Link
    to this heading")
  id: totrans-581
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2.2\. 卷积神经网络[#](#convolutional-neural-networks "链接到这个标题")
- en: 'We return to the Fashion MNIST dataset. One can do even better than we did
    before using a neural network tailored for images, known as [convolutional neural
    networks](https://cs231n.github.io/convolutional-networks/). From [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network):'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到Fashion MNIST数据集。使用专门为图像设计的神经网络，即[卷积神经网络](https://cs231n.github.io/convolutional-networks/)，可以获得比我们之前更好的结果。根据[Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)：
- en: In deep learning, a convolutional neural network (CNN, or ConvNet) is a class
    of deep neural networks, most commonly applied to analyzing visual imagery. They
    are also known as shift invariant or space invariant artificial neural networks
    (SIANN), based on their shared-weights architecture and translation invariance
    characteristics.
  id: totrans-583
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在深度学习中，卷积神经网络（CNN，或ConvNet）是一类深度神经网络，最常用于分析视觉图像。它们也被称为基于共享权重架构和平移不变性特征的平移不变或空间不变的人工神经网络（SIANN）。
- en: More background can be found in this excellent [module](http://cs231n.github.io/convolutional-networks/)
    from Stanford’s [CS231n](http://cs231n.github.io/). Our CNN will be a composition
    of [convolutional layers](http://cs231n.github.io/convolutional-networks/#conv)
    and [pooling layers](http://cs231n.github.io/convolutional-networks/#pool).
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 更多背景信息可以在斯坦福大学的[CS231n](http://cs231n.github.io/)提供的这个优秀的[模块](http://cs231n.github.io/convolutional-networks/)中找到。我们的卷积神经网络将是由[卷积层](http://cs231n.github.io/convolutional-networks/#conv)和[池化层](http://cs231n.github.io/convolutional-networks/#pool)组成的。
- en: '**CHAT & LEARN** Convolutional neural networks (CNNs) are powerful for image
    classification. Ask your favorite AI chatbot to explain the basic concepts of
    CNNs, including convolutional layers and pooling layers. \(\ddagger\)'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 卷积神经网络（CNNs）在图像分类中非常强大。请你的喜欢的AI聊天机器人解释CNN的基本概念，包括卷积层和池化层。
    \(\ddagger\)'
- en: '[PRE48]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The new model is the following.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 新的模型如下。
- en: '[PRE50]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We train and test.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行训练和测试。
- en: '[PRE51]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note the higher accuracy.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 注意更高的准确性。
- en: Finally, we try the original MNIST dataset. We use the same CNN.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们尝试原始的MNIST数据集。我们使用相同的CNN。
- en: '[PRE58]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-602
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-604
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-606
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Note the very high accuracy on this (easy - as it turns out) dataset.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在这个（容易的 - 如我们所见）数据集上的非常高的准确性。
- en: '8.7.2.1\. Another example: linear regression[#](#another-example-linear-regression
    "Link to this heading")'
  id: totrans-609
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2.1. 另一个例子：线性回归[#](#another-example-linear-regression "链接到这个标题")
- en: We give another concrete example of progressive functions and of the application
    of backpropagration and stochastic gradient descent.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给出了渐进函数和反向传播及随机梯度下降应用的另一个具体例子。
- en: '**Computing the gradient** While we have motivated the framework introduced
    in the previous section from the point of view of classification, it also immediately
    applies to the regression setting. Both classification and regression are instances
    of supervised learning.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算梯度** 虽然我们是从分类的角度来激励上一节中引入的框架，但它也立即适用于回归设置。分类和回归都是监督学习的实例。'
- en: We first compute the gradient of a single sample. Here \(\mathbf{x} \in \mathbb{R}^d\)
    again, but \(y\) is a real-valued outcome variable. We revisit the case of linear
    regression where the loss function is
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算单个样本的梯度。这里 \(\mathbf{x} \in \mathbb{R}^d\) 仍然适用，但 \(y\) 是一个实值结果变量。我们回顾线性回归的案例，其中损失函数是
- en: \[ \ell(z) = (z - y)^2 \]
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(z) = (z - y)^2 \]
- en: and the regression function only has input and output layers and no hidden layer
    (that is, \(L=0\)) with
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 并且回归函数只有输入层和输出层，没有隐藏层（即 \(L=0\)），
- en: \[ h(\mathbf{w}) = \sum_{j=1}^d w_{j} x_{j} = \mathbf{x}^T\mathbf{w}, \]
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h(\mathbf{w}) = \sum_{j=1}^d w_{j} x_{j} = \mathbf{x}^T\mathbf{w}, \]
- en: where \(\mathbf{w} \in \mathbb{R}^{d}\) are the parameters. Recall that we can
    include a constant term (one that does not depend on the input) by adding a \(1\)
    to the input. To keep the notation simple, we assume that this pre-processing
    has already been performed if desired.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w} \in \mathbb{R}^{d}\) 是参数。回想一下，我们可以通过在输入中添加一个常数项（不依赖于输入的项）来包含一个常数项。为了简化符号，我们假设如果需要，已经完成了预处理。
- en: Finally, the objective function for a single sample is
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，单个样本的目标函数是
- en: \[ f(\mathbf{w}) = \ell(h(\mathbf{w})) = \left(\sum_{j=1}^d w_{j} x_{j} - y\right)^2
    = \left(\mathbf{x}^T\mathbf{w} - y\right)^2\. \]
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = \ell(h(\mathbf{w})) = \left(\sum_{j=1}^d w_{j} x_{j} - y\right)^2
    = \left(\mathbf{x}^T\mathbf{w} - y\right)^2\. \]
- en: 'Using the notation from the previous subsection, the forward pass in this case
    is:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一小节中的符号，本例中的前向传播如下：
- en: '*Initialization:*'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化:*'
- en: \[\mathbf{z}_0 := \mathbf{x}.\]
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbf{z}_0 := \mathbf{x}.\]
- en: '*Forward layer loop:*'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '*前向层循环:*'
- en: \[\begin{align*} \hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0) &= \sum_{j=1}^d
    w_{0,j} z_{0,j} = \mathbf{z}_0^T \mathbf{w}_0 \end{align*}\]\[\begin{align*} \begin{pmatrix}
    A_0 & B_0 \end{pmatrix} := J_{g_0}(\mathbf{z}_0,\mathbf{w}_0) &= ( w_{0,1},\ldots,
    w_{0,d},z_{0,1},\ldots,z_{0,d} )^T = \begin{pmatrix}\mathbf{w}_0^T & \mathbf{z}_0^T\end{pmatrix},
    \end{align*}\]
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \hat{y} := z_1 := g_0(\mathbf{z}_0,\mathbf{w}_0) &= \sum_{j=1}^d
    w_{0,j} z_{0,j} = \mathbf{z}_0^T \mathbf{w}_0 \end{align*}\]\[\begin{align*} \begin{pmatrix}
    A_0 & B_0 \end{pmatrix} := J_{g_0}(\mathbf{z}_0,\mathbf{w}_0) &= ( w_{0,1},\ldots,
    w_{0,d},z_{0,1},\ldots,z_{0,d} )^T = \begin{pmatrix}\mathbf{w}_0^T & \mathbf{z}_0^T\end{pmatrix},
    \end{align*}\]
- en: where \(\mathbf{w}_0 := \mathbf{w}\).
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w}_0 := \mathbf{w}\)。
- en: '*Loss:*'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失:*'
- en: \[\begin{align*} z_2 &:= \ell(z_1) = (z_1 - y)^2\\ p_2 &:= \frac{\mathrm{d}}{\mathrm{d}
    z_1} {\ell}(z_1) = 2 (z_1 - y). \end{align*}\]
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} z_2 &:= \ell(z_1) = (z_1 - y)^2\\ p_2 &:= \frac{\mathrm{d}}{\mathrm{d}
    z_1} {\ell}(z_1) = 2 (z_1 - y). \end{align*}\]
- en: 'The backward pass is:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播如下：
- en: '*Backward layer loop:*'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向层循环:*'
- en: \[\begin{align*} \mathbf{q}_0 := B_0^T p_1 &= 2 (z_1 - y) \, \mathbf{z}_0. \end{align*}\]
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{q}_0 := B_0^T p_1 &= 2 (z_1 - y) \, \mathbf{z}_0. \end{align*}\]
- en: '*Output:*'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出:*'
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0. \]
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0. \]
- en: As we noted before, there is in fact no need to compute \(A_0\) and \(\mathbf{p}_0\).
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，实际上没有必要计算 \(A_0\) 和 \(\mathbf{p}_0\)。
- en: '**The `Advertising` dataset and the least-squares solution** We return to the
    `Advertising` dataset.'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '**`Advertising` 数据集和最小二乘解** 我们回到 `Advertising` 数据集。'
- en: '[PRE66]'
  id: totrans-634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '|  | Unnamed: 0 | TV | radio | newspaper | sales |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '|  | 未命名: 0 | 电视 | 收音机 | 报纸 | 销售额 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 1 | 230.1 | 37.8 | 69.2 | 22.1 |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 230.1 | 37.8 | 69.2 | 22.1 |'
- en: '| 1 | 2 | 44.5 | 39.3 | 45.1 | 10.4 |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 44.5 | 39.3 | 45.1 | 10.4 |'
- en: '| 2 | 3 | 17.2 | 45.9 | 69.3 | 9.3 |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 17.2 | 45.9 | 69.3 | 9.3 |'
- en: '| 3 | 4 | 151.5 | 41.3 | 58.5 | 18.5 |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 151.5 | 41.3 | 58.5 | 18.5 |'
- en: '| 4 | 5 | 180.8 | 10.8 | 58.4 | 12.9 |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5 | 180.8 | 10.8 | 58.4 | 12.9 |'
- en: '[PRE67]'
  id: totrans-642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We first compute the solution using the least-squares approach we detailed previously.
    We use [`numpy.column_stack`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack)
    to add a column of ones to the feature vectors.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用我们之前详细说明的平方最小二乘法来计算解决方案。我们使用 `numpy.column_stack` 向特征向量添加一列。
- en: '[PRE69]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The MSE is:'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）是：
- en: '[PRE71]'
  id: totrans-648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-649
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '**Solving the problem using PyTorch** We will be using PyTorch to implement
    the previous method. We first convert the data into PyTorch tensors. We then use
    [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)
    to create the dataset. Finally, [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    provides the utilities to load the data in batches for training. We take mini-batches
    of size `BATCH_SIZE = 64` and we apply a random permutation of the samples on
    every pass (with the option `shuffle=True`).'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用PyTorch解决问题** 我们将使用PyTorch来实现前面的方法。我们首先将数据转换为PyTorch张量。然后使用 `torch.utils.data.TensorDataset`
    创建数据集。最后，`torch.utils.data.DataLoader` 提供了用于批量加载数据的实用工具。我们取大小为 `BATCH_SIZE = 64`
    的迷你批次，并在每一轮中对样本进行随机排列（使用选项 `shuffle=True`）。'
- en: '[PRE73]'
  id: totrans-651
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now we construct our model. It is simply an affine map from \(\mathbb{R}^3\)
    to \(\mathbb{R}\). Note that there is no need to pre-process the inputs by adding
    \(1\)s. A constant term (or “bias variable”) is automatically added by PyTorch
    (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们构建我们的模型。它只是一个从 \(\mathbb{R}^3\) 到 \(\mathbb{R}\) 的仿射映射。请注意，没有必要通过添加 \(1\)
    来预处理输入。PyTorch（除非选择选项 `bias=False`）会自动添加一个常数项（或“偏置变量”）。
- en: '[PRE74]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Finally, we are ready to run an optimization method of our choice on the loss
    function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)
    available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)
    for a brief explanation of many common optimizers.) Here we use SGD as the optimizer.
    And the loss function is the MSE. A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好在我们的损失函数上运行我们选择的优化方法，这些方法将在下面指定。有许多可用的[优化器](https://pytorch.org/docs/stable/optim.html#algorithms)。（参见这篇[文章](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)，了解许多常见优化器的简要说明。）在这里，我们使用SGD作为优化器。损失函数是均方误差（MSE）。快速教程[在这里](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)。
- en: '[PRE75]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Choosing the right number of passes (i.e. epochs) through the data requires
    some experimenting. Here \(10^4\) suffices. But in the interest of time, we will
    run it only for \(100\) epochs. As you will see from the results, this is not
    quite enough. On each pass, we compute the output of the current model, use `backward()`
    to obtain the gradient, and then perform a descent update with `step()`. We also
    have to reset the gradients first (otherwise they add up by default).
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的数据遍历次数（即训练轮数）需要一些实验。这里 \(10^4\) 足够了。但为了节省时间，我们只运行 \(100\) 轮。正如您将从结果中看到的那样，这并不够。在每一轮中，我们计算当前模型的输出，使用
    `backward()` 获取梯度，然后使用 `step()` 执行下降更新。我们还需要首先重置梯度（否则它们会默认累加）。
- en: '[PRE76]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The final parameters and loss are:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 最终参数和损失如下：
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE77]</details>'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE79]</details>'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 8.7.2.2\. Convolutional neural networks[#](#convolutional-neural-networks "Link
    to this heading")
  id: totrans-665
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2.2\. 卷积神经网络[#](#convolutional-neural-networks "链接到这个标题")
- en: 'We return to the Fashion MNIST dataset. One can do even better than we did
    before using a neural network tailored for images, known as [convolutional neural
    networks](https://cs231n.github.io/convolutional-networks/). From [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network):'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到Fashion MNIST数据集。使用专门针对图像的神经网络，即[卷积神经网络](https://cs231n.github.io/convolutional-networks/)，可以做得比我们之前更好。从[Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)：
- en: In deep learning, a convolutional neural network (CNN, or ConvNet) is a class
    of deep neural networks, most commonly applied to analyzing visual imagery. They
    are also known as shift invariant or space invariant artificial neural networks
    (SIANN), based on their shared-weights architecture and translation invariance
    characteristics.
  id: totrans-667
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在深度学习中，卷积神经网络（CNN，或ConvNet）是一类深度神经网络，最常用于分析视觉图像。它们也被称为基于共享权重架构和平移不变性特征的平移不变或空间不变人工神经网络（SIANN）。
- en: More background can be found in this excellent [module](http://cs231n.github.io/convolutional-networks/)
    from Stanford’s [CS231n](http://cs231n.github.io/). Our CNN will be a composition
    of [convolutional layers](http://cs231n.github.io/convolutional-networks/#conv)
    and [pooling layers](http://cs231n.github.io/convolutional-networks/#pool).
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 更多背景信息可以在斯坦福的[CS231n](http://cs231n.github.io/)的这篇优秀的[模块](http://cs231n.github.io/convolutional-networks/)中找到。我们的CNN将是[卷积层](http://cs231n.github.io/convolutional-networks/#conv)和[池化层](http://cs231n.github.io/convolutional-networks/#pool)的组合。
- en: '**CHAT & LEARN** Convolutional neural networks (CNNs) are powerful for image
    classification. Ask your favorite AI chatbot to explain the basic concepts of
    CNNs, including convolutional layers and pooling layers. \(\ddagger\)'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 卷积神经网络（CNNs）在图像分类方面非常强大。请你的AI聊天机器人解释CNN的基本概念，包括卷积层和池化层。\(\ddagger\)'
- en: '[PRE81]'
  id: totrans-670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The new model is the following.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型如下。
- en: '[PRE83]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We train and test.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行训练和测试。
- en: '[PRE84]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-677
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Note the higher accuracy.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 注意更高的准确率。
- en: Finally, we try the original MNIST dataset. We use the same CNN.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们尝试了原始的MNIST数据集。我们使用了相同的卷积神经网络。
- en: '[PRE91]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Note the very high accuracy on this (easy - as it turns out) dataset.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在这个（容易的——结果证明）数据集上的非常高的准确率。
- en: 8.7.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  id: totrans-693
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7.3\. 其他证明[#](#additional-proofs "链接到这个标题")
- en: '**Proofs of Lagrange Multipliers Conditions** We first prove the *Lagrange
    Multipliers: First-Order Necessary Conditions*. We follow the excellent textbook
    [[Ber](http://www.athenasc.com/nonlinbook.html), Section 4.1]. The proof uses
    the concept of the Jacobian.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '**拉格朗日乘数条件证明** 我们首先证明*拉格朗日乘数：一阶必要条件*。我们遵循优秀的教科书[[Ber](http://www.athenasc.com/nonlinbook.html)，第4.1节]。证明使用了雅可比矩阵的概念。'
- en: '*Proof idea:* We reduce the problem to an unconstrained optimization problem
    by penalizing the constraint in the objective function. We then apply the unconstrained
    *First-Order Necessary Conditions*.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* 我们通过惩罚目标函数中的约束条件将问题简化为一个无约束优化问题。然后我们应用无约束的*一阶必要条件*。'
- en: '*Proof:* *(Lagrange Multipliers: First-Order Necessary Conditions)* We reduce
    the problem to an unconstrained optimization problem by penalizing the constraint
    in the objective function. We also add a regularization term to ensure that \(\mathbf{x}^*\)
    is the unique local minimizer in a neighborhood. Specifically, for each non-negative
    integer \(k\), consider the objective function'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(拉格朗日乘数：一阶必要条件)* 我们通过惩罚目标函数中的约束条件将问题简化为一个无约束优化问题。我们还添加了一个正则化项，以确保\(\mathbf{x}^*\)是邻域中唯一的局部最小值。具体来说，对于每个非负整数\(k\)，考虑以下目标函数'
- en: \[ F^k(\mathbf{x}) = f(\mathbf{x}) + \frac{k}{2} \|\mathbf{h}(\mathbf{x})\|^2
    + \frac{\alpha}{2} \|\mathbf{x} - \mathbf{x}^*\|^2 \]
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F^k(\mathbf{x}) = f(\mathbf{x}) + \frac{k}{2} \|\mathbf{h}(\mathbf{x})\|^2
    + \frac{\alpha}{2} \|\mathbf{x} - \mathbf{x}^*\|^2 \]
- en: for some positive constant \(\alpha > 0\). Note that as \(k\) gets larger, the
    penalty becomes more significant and, therefore, enforcing the constraint becomes
    more desirable. The proof proceeds in several steps.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个正常数\(\alpha > 0\)。注意，随着\(k\)的增大，惩罚变得更为显著，因此，强制约束变得更为可取。证明分几个步骤进行。
- en: We first consider a version minimizing \(F^k\) constrained to lie in a neighborhood
    of \(\mathbf{x}^*\). Because \(\mathbf{x}^*\) is a local minimizer of \(f\) subject
    to \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\), there is \(\delta > 0\) such that
    \(f(\mathbf{x}^*)\leq f(\mathbf{x})\) for all feasible \(\mathbf{x}\) within
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑一个版本，它最小化\(F^k\)，同时约束它位于\(\mathbf{x}^*\)的邻域内。因为\(\mathbf{x}^*\)是受\(\mathbf{h}(\mathbf{x})
    = \mathbf{0}\)约束的\(f\)的局部最小值，存在\(\delta > 0\)，使得对于所有在\(\mathbf{x}^*\)邻域内的可行\(\mathbf{x}\)，都有\(f(\mathbf{x}^*)\leq
    f(\mathbf{x})\)。
- en: \[ \mathscr{X} = B_{\delta}(\mathbf{x}^*) = \{\mathbf{x}:\|\mathbf{x} - \mathbf{x}^*\|
    \leq \delta\}. \]
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathscr{X} = B_{\delta}(\mathbf{x}^*) = \{\mathbf{x}:\|\mathbf{x} - \mathbf{x}^*\|
    \leq \delta\}. \]
- en: '**LEMMA** **(Step I: Solving the Penalized Problem in a Neighborhood of \(\mathbf{x}^*\))**
    For \(k \geq 1\), let \(\mathbf{x}^k\) be a global minimizer of the minimization
    problem'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(步骤 I：在 \(\mathbf{x}^*\) 的邻域内求解惩罚问题)** 对于 \(k \geq 1\)，设 \(\mathbf{x}^k\)
    是最小化问题的全局最小值'
- en: \[ \min_{\mathbf{x} \in \mathscr{X}} F^k(\mathbf{x}). \]
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathscr{X}} F^k(\mathbf{x}). \]
- en: a) The sequence \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) converges to \(\mathbf{x}^*\).
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: a) 序列 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) 收敛到 \(\mathbf{x}^*\)。
- en: b) For \(k\) sufficiently large, \(\mathbf{x}^k\) is a local minimizer of the
    objective function \(F^k\) *without any constraint*.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: b) 对于足够大的 \(k\)，\(\mathbf{x}^k\) 是目标函数 \(F^k\) 的局部最小值，没有任何约束。
- en: \(\flat\)
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* The set \(\mathscr{X}\) is closed and bounded, and \(F^k\) is continuous.
    Hence, the sequence \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) is well-defined by the
    *Extreme Value Theorem*. Let \(\bar{\mathbf{x}}\) be any limit point of \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\).
    We show that \(\bar{\mathbf{x}} = \mathbf{x}^*\). That will imply a). It also
    implied b) since hen, for large enough \(k\), \(\mathbf{x}^k\) must be an interior
    point of \(\mathscr{X}\).'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 集合 \(\mathscr{X}\) 是闭合且有界的，且 \(F^k\) 是连续的。因此，根据**极值定理**，序列 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)
    是有定义的。设 \(\bar{\mathbf{x}}\) 是 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) 的任意极限点。我们证明
    \(\bar{\mathbf{x}} = \mathbf{x}^*\)。这将意味着 a)。这也意味着 b)，因为当 \(k\) 足够大时，\(\mathbf{x}^k\)
    必须是 \(\mathscr{X}\) 的内点。'
- en: Let \(-\infty < m \leq M < + \infty\) be the smallest and largest values of
    \(f\) on \(\mathscr{X}\), which exist by the *Extreme Value Theorem*. Then, for
    all \(k\), by definition of \(\mathbf{x}^k\) and the fact that \(\mathbf{x}^*\)
    is feasible
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(-\infty < m \leq M < + \infty\) 是 \(\mathscr{X}\) 上函数 \(f\) 的最小值和最大值，根据**极值定理**存在。那么，对于所有
    \(k\)，根据 \(\mathbf{x}^k\) 的定义以及 \(\mathbf{x}^*\) 是可行解的事实
- en: \[\begin{align*} (*) \qquad f(\mathbf{x}^k) &+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^k)\|^2
    + \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\\ &\leq f(\mathbf{x}^*) +
    \frac{k}{2} \|\mathbf{h}(\mathbf{x}^*)\|^2 + \frac{\alpha}{2} \|\mathbf{x}^* -
    \mathbf{x}^*\|^2 = f(\mathbf{x}^*). \end{align*}\]
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (*) \qquad f(\mathbf{x}^k) &+ \frac{k}{2} \|\mathbf{h}(\mathbf{x}^k)\|^2
    + \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\\ &\leq f(\mathbf{x}^*) +
    \frac{k}{2} \|\mathbf{h}(\mathbf{x}^*)\|^2 + \frac{\alpha}{2} \|\mathbf{x}^* -
    \mathbf{x}^*\|^2 = f(\mathbf{x}^*). \end{align*}\]
- en: Rearraning gives
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列得到
- en: \[ \|\mathbf{h}(\mathbf{x}^k)\|^2 \leq \frac{2}{k} \left[f(\mathbf{x}^*) - f(\mathbf{x}^k)
    - \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\right] \leq \frac{2}{k} \left[
    f(\mathbf{x}^*) - m\right]. \]
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{h}(\mathbf{x}^k)\|^2 \leq \frac{2}{k} \left[f(\mathbf{x}^*) - f(\mathbf{x}^k)
    - \frac{\alpha}{2} \|\mathbf{x}^k - \mathbf{x}^*\|^2\right] \leq \frac{2}{k} \left[
    f(\mathbf{x}^*) - m\right]. \]
- en: So \(\lim_{k \to \infty} \|\mathbf{h}(\mathbf{x}^k)\|^2 = 0\), which, by the
    continuity of \(\mathbf{h}\) and of the Frobenius norm, implies that \(\|\mathbf{h}(\bar{\mathbf{x}})\|^2
    = 0\), that is, \(\mathbf{h}(\bar{\mathbf{x}}) = \mathbf{0}\). In other words,
    any limit point \(\bar{\mathbf{x}}\) is feasible.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(\lim_{k \to \infty} \|\mathbf{h}(\mathbf{x}^k)\|^2 = 0\)，根据 \(\mathbf{h}\)
    和 Frobenius 范数的连续性，这意味着 \(\|\mathbf{h}(\bar{\mathbf{x}})\|^2 = 0\)，即 \(\mathbf{h}(\bar{\mathbf{x}})
    = \mathbf{0}\)。换句话说，任何极限点 \(\bar{\mathbf{x}}\) 都是可行解。
- en: In addition to being feasible, \(\bar{\mathbf{x}} \in \mathscr{X}\) because
    that constraint set is closed. So, by the choice of \(\mathscr{X}\), we have \(f(\mathbf{x}^*)
    \leq f(\bar{\mathbf{x}})\). Furthermore, by \((*)\), we get
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 除了是可行解外，\(\bar{\mathbf{x}} \in \mathscr{X}\) 因为该约束集是闭合的。因此，根据 \(\mathscr{X}\)
    的选择，我们有 \(f(\mathbf{x}^*) \leq f(\bar{\mathbf{x}})\)。此外，根据 \((*)\)，我们得到
- en: \[ f(\mathbf{x}^*) \leq f(\bar{\mathbf{x}}) + \frac{\alpha}{2} \|\bar{\mathbf{x}}
    - \mathbf{x}^*\|^2 \leq f(\mathbf{x}^*). \]
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}^*) \leq f(\bar{\mathbf{x}}) + \frac{\alpha}{2} \|\bar{\mathbf{x}}
    - \mathbf{x}^*\|^2 \leq f(\mathbf{x}^*). \]
- en: This is only possible if \(\|\bar{\mathbf{x}} - \mathbf{x}^*\|^2 = 0\) or, put
    differently, \(\bar{\mathbf{x}} = \mathbf{x}^*\). That proves the lemma. \(\square\)
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 这只有在 \(\|\bar{\mathbf{x}} - \mathbf{x}^*\|^2 = 0\) 或换句话说，\(\bar{\mathbf{x}}
    = \mathbf{x}^*\) 的情况下才可能。这证明了引理。 \(\square\)
- en: '**LEMMA** **(Step II: Applying the Unconstrained Necessary Conditions)** Let
    \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) be the sequence in the previous lemma.'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(步骤 II：应用无约束必要条件)** 设 \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) 是前一个引理中的序列。'
- en: a) For sufficiently large \(k\), the vectors \(\nabla h_i(\mathbf{x}^k)\), \(i=1,\ldots,\ell\),
    are linearly independent.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对于足够大的 \(k\)，向量 \(\nabla h_i(\mathbf{x}^k)\)，\(i=1,\ldots,\ell\)，是线性无关的。
- en: b) Let \(\mathbf{J}_{\mathbf{h}}(\mathbf{x})\) be the Jacobian matrix of \(\mathbf{h}\),
    that is, the matrix whose rows are the (row) vectors \(\nabla h_i(\mathbf{x})^T\),
    \(i=1,\ldots,\ell\). Then
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: b) 设 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x})\) 为 \(\mathbf{h}\) 的雅可比矩阵，即其行向量是
    \(\nabla h_i(\mathbf{x})^T\)，\(i=1,\ldots,\ell\) 的矩阵。然后
- en: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
- en: where
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[ \blambda^* = - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \, \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*))^{-1}
    \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*) \nabla f(\mathbf{x}^*). \]
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \blambda^* = - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \, \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*))^{-1}
    \mathbf{J}_{\mathbf{h}}^T(\mathbf{x}^*) \nabla f(\mathbf{x}^*). \]
- en: \(\flat\)
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* By the previous lemma, for \(k\) large enough, \(\mathbf{x}^k\) is
    an unconstrained local minimizer of \(F^k\). So by the (unconstrained) *First-Order
    Necessary Conditions*, it holds that'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 根据前面的引理，对于足够大的 \(k\)，\(\mathbf{x}^k\) 是 \(F^k\) 的无约束局部极小值。因此，根据（无约束的）*一阶必要条件*，有'
- en: \[ \nabla F^k(\mathbf{x}^k) = \mathbf{0}. \]
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla F^k(\mathbf{x}^k) = \mathbf{0}. \]
- en: To compute the gradient of \(F^k\) we note that
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 \(F^k\) 的梯度，我们注意到
- en: \[ \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell (h_i(\mathbf{x}))^2. \]
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell (h_i(\mathbf{x}))^2. \]
- en: The partial derivatives are
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数是
- en: \[ \frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell
    \frac{\partial}{\partial x_j} (h_i(\mathbf{x}))^2 = \sum_{i=1}^\ell 2 h_i(\mathbf{x})
    \frac{\partial h_i(\mathbf{x})}{\partial x_j}, \]
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial}{\partial x_j} \|\mathbf{h}(\mathbf{x})\|^2 = \sum_{i=1}^\ell
    \frac{\partial}{\partial x_j} (h_i(\mathbf{x}))^2 = \sum_{i=1}^\ell 2 h_i(\mathbf{x})
    \frac{\partial h_i(\mathbf{x})}{\partial x_j}, \]
- en: by the *Chain Rule*. So, in vector form,
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *链式法则*。因此，以向量形式，
- en: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
- en: The term \(\|\mathbf{x} - \mathbf{x}^*\|^2\) can be rewritten as the quadratic
    function
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 项 \(\|\mathbf{x} - \mathbf{x}^*\|^2\) 可以重写为二次函数
- en: \[ \|\mathbf{x} - \mathbf{x}^*\|^2 = \frac{1}{2}\mathbf{x}^T (2 I_{d \times
    d}) \mathbf{x} - 2 (\mathbf{x}^*)^T \mathbf{x} + (\mathbf{x}^*)^T \mathbf{x}^*.
    \]
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{x} - \mathbf{x}^*\|^2 = \frac{1}{2}\mathbf{x}^T (2 I_{d \times
    d}) \mathbf{x} - 2 (\mathbf{x}^*)^T \mathbf{x} + (\mathbf{x}^*)^T \mathbf{x}^*.
    \]
- en: Using a previous formula with \(P = 2 I_{d \times d}\) (which is symmetric),
    \(\mathbf{q} = -2 \mathbf{x}^*\) and \(r = (\mathbf{x}^*)^T \mathbf{x}^*\), we
    get
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的公式，其中 \(P = 2 I_{d \times d}\)（这是一个对称矩阵），\(\mathbf{q} = -2 \mathbf{x}^*\)
    和 \(r = (\mathbf{x}^*)^T \mathbf{x}^*\)，我们得到
- en: \[ \nabla \|\mathbf{x} - \mathbf{x}^*\|^2 = 2\mathbf{x} -2 \mathbf{x}^*. \]
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla \|\mathbf{x} - \mathbf{x}^*\|^2 = 2\mathbf{x} -2 \mathbf{x}^*. \]
- en: So, putting everything together,
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将所有这些放在一起，
- en: \[ (**) \qquad \mathbf{0} = \nabla F^k(\mathbf{x}^k) = \nabla f(\mathbf{x}^k)
    + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T (k \mathbf{h}(\mathbf{x}^k)) + \alpha(\mathbf{x}^k
    - \mathbf{x}^*). \]
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (**) \qquad \mathbf{0} = \nabla F^k(\mathbf{x}^k) = \nabla f(\mathbf{x}^k)
    + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T (k \mathbf{h}(\mathbf{x}^k)) + \alpha(\mathbf{x}^k
    - \mathbf{x}^*). \]
- en: By the previous lemma, \(\mathbf{x}^k \to \mathbf{x}^*\), \(\nabla f(\mathbf{x}^k)
    \to \nabla f(\mathbf{x}^*)\), and \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\) as \(k \to +\infty\).
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的引理，\(\mathbf{x}^k \to \mathbf{x}^*\)，\(\nabla f(\mathbf{x}^k) \to \nabla
    f(\mathbf{x}^*)\)，以及 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)
    当 \(k \to +\infty\)。
- en: So it remains to derive the limit of \(k \mathbf{h}(\mathbf{x}^k)\). By assumption,
    the columns of \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T\) are linearly independent.
    That implies that for any unit vector \(\mathbf{z} \in \mathbb{R}^\ell\)
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，剩下要推导 \(k \mathbf{h}(\mathbf{x}^k)\) 的极限。根据假设，\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T\)
    的列是线性无关的。这意味着对于任何单位向量 \(\mathbf{z} \in \mathbb{R}^\ell\)
- en: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} = \|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}\|^2 > 0 \]
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} = \|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}\|^2 > 0 \]
- en: otherwise we would have \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z}
    = \mathbf{0}\), contradicting the linear independence assumption. By the *Extreme
    Value Theorem*, there is \(\beta > 0\) such that
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，我们会有 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{z} = \mathbf{0}\)，这与线性无关的假设相矛盾。根据
    *极值定理*，存在 \(\beta > 0\) 使得
- en: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} \geq \beta \]
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} \geq \beta \]
- en: for all unit vectors \(\mathbf{z} \in \mathbb{R}^\ell\). Since \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)
    \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\), it follows from a previous lemma
    that, for \(k\) large enough and any unit vector \(\mathbf{z} \in \mathbb{R}^\ell\),
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有单位向量 \(\mathbf{z} \in \mathbb{R}^\ell\)。由于 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)
    \to \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\)，根据之前的引理，对于足够大的 \(k\) 和任意单位向量 \(\mathbf{z}
    \in \mathbb{R}^\ell\)，
- en: \[ |\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} - \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z}| \leq \| \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    - \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \|_F \leq \frac{\beta}{2}. \]
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: \[ |\mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \mathbf{z} - \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z}| \leq \| \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    - \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \|_F \leq \frac{\beta}{2}. \]
- en: That implies
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z} \geq \frac{\beta}{2}, \]
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T
    \mathbf{z} \geq \frac{\beta}{2}, \]
- en: so by the same argument as above the columns of \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)
    are linearly independent for \(k\) large enough and \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)
    \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\) is invertible. That proves a).
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过上述相同的论证，当 \(k\) 足够大时，\(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\) 的列线性无关，且
    \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k) \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T\)
    是可逆的。这证明了 a)。
- en: Going back to \((**)\), multiplying both sides by \((\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\,
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T)^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\),
    and taking a limit \(k \to +\infty\), we get after rearranging that
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 \((**)\)，两边乘以 \((\mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\, \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)^T)^{-1}
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^k)\)，并取极限 \(k \to +\infty\)，经过重新排列后得到
- en: \[ k \mathbf{h}(\mathbf{x}^k) \to - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) ^T
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \nabla f(\mathbf{x}^*) = \blambda^*. \]
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: \[ k \mathbf{h}(\mathbf{x}^k) \to - (\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) ^T
    \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*))^{-1} \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \nabla f(\mathbf{x}^*) = \blambda^*. \]
- en: Plugging back we get
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 将变量代回，我们得到
- en: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \blambda^*
    = \mathbf{0} \]
- en: as claimed. That proves b). \(\square\)
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 如所声称。这证明了 b)。 \(\square\)
- en: Combining the lemmas establishes the theorem. \(\square\)
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 结合引理可以建立定理。 \(\square\)
- en: 'Next, we prove the *Lagrange Multipliers: Second-Order Sufficient Conditions*.
    Again, we follow [[Ber](http://www.athenasc.com/nonlinbook.html), Section 4.2].
    We will need the following lemma. The proof can be skipped.'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们证明**拉格朗日乘数：二阶充分条件**。再次，我们遵循 [[Ber](http://www.athenasc.com/nonlinbook.html),
    第4.2节]。我们需要以下引理。证明可以跳过。
- en: '*Proof idea:* We consider a slightly modified problem'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明思路：** 我们考虑一个略微修改的问题'
- en: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0} \end{align*}\]
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0} \end{align*}\]
- en: which has the same local minimizers. Applying the *Second-Order Sufficient Conditions*
    to the Lagrangian of the modified problem gives the result when \(c\) is chosen
    large enough.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有相同的局部极小值。将**二阶充分条件**应用于修改后问题的拉格朗日函数，当 \(c\) 足够大时给出结果。
- en: '**LEMMA** Let \(P\) and \(Q\) be symmetric matrices in \(\mathbb{R}^{n \times
    n}\). Assume that \(Q\) is positive semidefinite and that \(P\) is positive definite
    on the null space of \(Q\), that is, \(\mathbf{w}^T P \mathbf{w} > 0\) for all
    \(\mathbf{w} \neq \mathbf{0}\) such that \(\mathbf{w}^T Q \mathbf{w} = \mathbf{0}\).
    Then there is a scalar \(\bar{c} \geq 0\) such that \(P + c Q\) is positive definite
    for all \(c > \bar{c}\). \(\flat\)'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** 设 \(P\) 和 \(Q\) 是 \(\mathbb{R}^{n \times n}\) 中的对称矩阵。假设 \(Q\) 是正半定矩阵，且
    \(P\) 在 \(Q\) 的零空间上是正定的，即对于所有 \(\mathbf{w} \neq \mathbf{0}\) 满足 \(\mathbf{w}^T
    Q \mathbf{w} = \mathbf{0}\) 的 \(\mathbf{w}\)，有 \(\mathbf{w}^T P \mathbf{w} > 0\)。那么存在一个标量
    \(\bar{c} \geq 0\)，使得对于所有 \(c > \bar{c}\)，\(P + c Q\) 是正定的。 \(\flat\)'
- en: '*Proof:* *(lemma)* We argue by contradiction. Suppose there is an non-negative,
    increasing, diverging sequence \(\{c_k\}_{k=1}^{+\infty}\) and a sequence of unit
    vectors \(\{\mathbf{x}^k\}_{k=1}^{+\infty}\) such that'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(引理)* 我们通过反证法进行论证。假设存在一个非负的、递增的、发散的序列\(\{c_k\}_{k=1}^{+\infty}\)和一个单位向量序列\(\{\mathbf{x}^k\}_{k=1}^{+\infty}\)，使得'
- en: \[ (\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k \leq 0 \]
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k \leq 0 \]
- en: for all \(k\). Because the sequence is bounded, it has a limit point \(\bar{\mathbf{x}}\).
    Assume without loss of generality that \(\mathbf{x}^k \to \bar{\mathbf{x}}\),
    as \(k \to \infty\). Since \(c_k \to +\infty\) and \((\mathbf{x}^k)^T Q \mathbf{x}^k
    \geq 0\) by assumption, we must have \((\bar{\mathbf{x}})^T Q \bar{\mathbf{x}}
    = 0\), otherwise \((\mathbf{x}^k)^T (P + c_k Q) \mathbf{x}^k\) would diverge.
    Hence, by the assumption in the statement, it must be that \((\bar{\mathbf{x}})^T
    P \bar{\mathbf{x}} > 0\). This contradicts the inequality in the display above.
    \(\square\)
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有\(k\)。因为序列是有界的，所以它有一个极限点\(\bar{\mathbf{x}}\)。假设不失一般性，\(\mathbf{x}^k \to
    \bar{\mathbf{x}}\)，当\(k \to \infty\)。由于\(c_k \to +\infty\)并且根据假设\((\mathbf{x}^k)^T
    Q \mathbf{x}^k \geq 0\)，我们必须有\((\bar{\mathbf{x}})^T Q \bar{\mathbf{x}} = 0\)，否则\((\mathbf{x}^k)^T
    (P + c_k Q) \mathbf{x}^k\)将会发散。因此，根据陈述中的假设，\((\bar{\mathbf{x}})^T P \bar{\mathbf{x}}
    > 0\)。这与上面的不等式相矛盾。\(\square\)
- en: '*Proof:* *(Lagrange Multipliers: Second-Order Sufficient Conditions)* We consider
    the modified problem'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(拉格朗日乘数：二阶充分条件)* 我们考虑修改后的问题'
- en: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}. \end{align*}\]
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\text{min} f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2\\
    &\text{s.t.}\ \mathbf{h}(\mathbf{x}) = \mathbf{0}. \end{align*}\]
- en: It has the same local minimizers as the orginal problem as the additional term
    in the objective is zero for feasible vectors. That extra term will allow us to
    use the previous lemma. For notational convenience, we define
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标函数中的附加项对于可行向量来说是零，因此它具有与原问题相同的局部极小值。这个额外的项将允许我们使用之前的引理。为了方便起见，我们定义
- en: \[ g_c(\mathbf{x}) = f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2.
    \]
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: \[ g_c(\mathbf{x}) = f(\mathbf{x}) + \frac{c}{2} \|\mathbf{h}(\mathbf{x})\|^2.
    \]
- en: The Lagrangian of the modified problem is
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后问题的拉格朗日函数为
- en: \[ L_c(\mathbf{x}, \blambda) = g_c(\mathbf{x}) + \mathbf{h}(\mathbf{x})^T \blambda.
    \]
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_c(\mathbf{x}, \blambda) = g_c(\mathbf{x}) + \mathbf{h}(\mathbf{x})^T \blambda.
    \]
- en: We will apply the *Second-Order Sufficient Conditions* to problem of minimizing
    \(L_c\) over \(\mathbf{x}\). We indicate the Hessian with respect to only the
    variables \(\mathbf{x}\) as \(\nabla^2_{\mathbf{x},\mathbf{x}}\).
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用**二阶充分条件**来求解在\(\mathbf{x}\)上最小化\(L_c\)的问题。我们用\(\nabla^2_{\mathbf{x},\mathbf{x}}\)表示仅关于变量\(\mathbf{x}\)的Hessian矩阵。
- en: 'Recall from the proof of the *Lagrange Multipliers: First-Order Necessary Conditions*
    that'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆从**拉格朗日乘数：一阶必要条件**的证明中，我们知道
- en: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla \|\mathbf{h}(\mathbf{x})\|^2 = 2 \mathbf{J}_{\mathbf{h}}(\mathbf{x})^T
    \mathbf{h}(\mathbf{x}). \]
- en: To compute the Hessian of that function, we note
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算该函数的Hessian矩阵，我们注意到
- en: \[\begin{align*} \frac{\partial}{\partial x_i}\left( \frac{\partial}{\partial
    x_j} \|\mathbf{h}(\mathbf{x})\|^2\right) &= \frac{\partial}{\partial x_i}\left(
    \sum_{k=1}^\ell 2 h_k(\mathbf{x}) \frac{\partial h_k(\mathbf{x})}{\partial x_j}\right)\\
    &= 2 \sum_{k=1}^\ell\left( \frac{\partial h_k(\mathbf{x})}{\partial x_i} \frac{\partial
    h_k(\mathbf{x})}{\partial x_j} + h_k(\mathbf{x}) \frac{\partial^2 h_k(\mathbf{x})}{\partial
    x_i \partial x_j} \right)\\ &= 2 \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x})
    + \sum_{k=1}^\ell h_k(\mathbf{x}) \, \mathbf{H}_{h_k}(\mathbf{x})\right)_{i,j}.
    \end{align*}\]
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial x_i}\left( \frac{\partial}{\partial
    x_j} \|\mathbf{h}(\mathbf{x})\|^2\right) &= \frac{\partial}{\partial x_i}\left(
    \sum_{k=1}^\ell 2 h_k(\mathbf{x}) \frac{\partial h_k(\mathbf{x})}{\partial x_j}\right)\\
    &= 2 \sum_{k=1}^\ell\left( \frac{\partial h_k(\mathbf{x})}{\partial x_i} \frac{\partial
    h_k(\mathbf{x})}{\partial x_j} + h_k(\mathbf{x}) \frac{\partial^2 h_k(\mathbf{x})}{\partial
    x_i \partial x_j} \right)\\ &= 2 \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x})^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x})
    + \sum_{k=1}^\ell h_k(\mathbf{x}) \, \mathbf{H}_{h_k}(\mathbf{x})\right)_{i,j}.
    \end{align*}\]
- en: Hence
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \nabla f(\mathbf{x}^*)
    + c \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{h}(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \blambda^* \]
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \nabla f(\mathbf{x}^*)
    + c \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \mathbf{h}(\mathbf{x}^*) + \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \blambda^* \]
- en: and
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{H}_{f}(\mathbf{x}^*)
    + c \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    + \sum_{k=1}^\ell h_k(\mathbf{x}^*) \, \mathbf{H}_{h_k}(\mathbf{x}^*)\right) +
    \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*). \]
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{H}_{f}(\mathbf{x}^*)
    + c \left(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    + \sum_{k=1}^\ell h_k(\mathbf{x}^*) \, \mathbf{H}_{h_k}(\mathbf{x}^*)\right) +
    \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*). \]
- en: By the assumptions of the theorem, this simplifies to
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定理的假设，这简化为
- en: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{0} \]
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_{\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) = \mathbf{0} \]
- en: and
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) =\underbrace{\left\{
    \mathbf{H}_{f}(\mathbf{x}^*) + \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*)
    \right\}}_{P} + c \underbrace{\left\{\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \right\}}_{Q}, \]
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*) =\underbrace{\left\{
    \mathbf{H}_{f}(\mathbf{x}^*) + \sum_{k=1}^\ell \lambda^*_k \, \mathbf{H}_{h_k}(\mathbf{x}^*)
    \right\}}_{P} + c \underbrace{\left\{\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \right\}}_{Q}, \]
- en: where further \(\mathbf{v}^T P \mathbf{v} > 0\) for any \(\mathbf{v}\) such
    that \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{v} = \mathbf{0}\) (which
    itself implies \(\mathbf{v}^T Q \mathbf{v} = \mathbf{0}\)). Furthermore, \(Q \succeq
    \mathbf{0}\) since
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于任何满足 \(\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \,\mathbf{v} = \mathbf{0}\)（这本身意味着
    \(\mathbf{v}^T Q \mathbf{v} = \mathbf{0}\)）的 \(\mathbf{v}\)，都有 \(\mathbf{v}^T
    P \mathbf{v} > 0\)。此外，由于
- en: \[ \mathbf{w}^T Q \mathbf{w} = \mathbf{w}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \mathbf{w} = \left\|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \mathbf{w}\right\|^2 \geq 0 \]
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^T Q \mathbf{w} = \mathbf{w}^T \mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)^T
    \,\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*) \mathbf{w} = \left\|\mathbf{J}_{\mathbf{h}}(\mathbf{x}^*)
    \mathbf{w}\right\|^2 \geq 0 \]
- en: for any \(\mathbf{w} \in \mathbb{R}^d\). The previous lemma allows us to take
    \(c\) large enough that \(\nabla^2_{\mathbf{x},\mathbf{x}} L_c(\mathbf{x}^*, \blambda^*)
    \succ \mathbf{0}\).
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 \(\mathbf{w} \in \mathbb{R}^d\)。前述引理允许我们取 \(c\) 足够大，使得 \(\nabla^2_{\mathbf{x},\mathbf{x}}
    L_c(\mathbf{x}^*, \blambda^*) \succ \mathbf{0}\)。
- en: As a result, the unconstrained *Second-Order Sufficient Conditions* are satisfied
    at \(\mathbf{x}^*\) for the problem
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于该问题，在 \(\mathbf{x}^*\) 处满足无约束的**二阶充分条件**。
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} L_c(\mathbf{x}, \blambda^*). \]
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} L_c(\mathbf{x}, \blambda^*). \]
- en: That is, there is \(\delta > 0\) such that
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 即存在 \(\delta > 0\) 使得
- en: \[ L_c(\mathbf{x}^*, \blambda^*) < L_c(\mathbf{x}, \blambda^*), \qquad \forall
    \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}. \]
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_c(\mathbf{x}^*, \blambda^*) < L_c(\mathbf{x}, \blambda^*), \qquad \forall
    \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{\mathbf{x}^*\}. \]
- en: Restricting this to the feasible vectors of the modified constrained problem
    (i.e., those such that \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\)) implies after
    simplification
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 将此限制为修改后的约束问题的可行向量（即那些满足 \(\mathbf{h}(\mathbf{x}) = \mathbf{0}\) 的向量）意味着在简化后
- en: '\[ f(\mathbf{x}^*) < f(\mathbf{x}), \qquad \forall \mathbf{x} \in \{\mathbf{x}
    : \mathbf{h}(\mathbf{x}) = \mathbf{0}\} \cap (B_{\delta}(\mathbf{x}^*) \setminus
    \{\mathbf{x}^*\}). \]'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ f(\mathbf{x}^*) < f(\mathbf{x}), \qquad \forall \mathbf{x} \in \{\mathbf{x}
    : \mathbf{h}(\mathbf{x}) = \mathbf{0}\} \cap (B_{\delta}(\mathbf{x}^*) \setminus
    \{\mathbf{x}^*\}). \]'
- en: Therefore, \(\mathbf{x}^*\) is a strict local minimizer of the modified constrained
    problem (and, in turn, of the original constrained problem). That concludes the
    proof. \(\square\)
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(\mathbf{x}^*\) 是修改后的约束问题的严格局部极小值（并且由此，原始约束问题也是如此）。这就完成了证明。 \(\square\)
