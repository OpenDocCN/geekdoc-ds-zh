- en: 'Chapter 5 Classification I: training & predicting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 分类I：训练与预测
- en: 原文：[https://datasciencebook.ca/classification1.html](https://datasciencebook.ca/classification1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://datasciencebook.ca/classification1.html](https://datasciencebook.ca/classification1.html)
- en: 5.1 Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 概述
- en: In previous chapters, we focused solely on descriptive and exploratory data
    analysis questions. This chapter and the next together serve as our first foray
    into answering *predictive* questions about data. In particular, we will focus
    on *classification*, i.e., using one or more variables to predict the value of
    a categorical variable of interest. This chapter will cover the basics of classification,
    how to preprocess data to make it suitable for use in a classifier, and how to
    use our observed data to make predictions. The next chapter will focus on how
    to evaluate how accurate the predictions from our classifier are, as well as how
    to improve our classifier (where possible) to maximize its accuracy.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们专注于描述性和探索性数据分析问题。本章和下一章一起构成了我们首次尝试回答关于数据的**预测**问题。特别是，我们将专注于**分类**，即使用一个或多个变量来预测感兴趣的分类变量的值。本章将涵盖分类的基础知识、如何预处理数据以使其适合用于分类器，以及如何使用我们的观察数据来做出预测。下一章将专注于如何评估分类器预测的准确性，以及如何改进我们的分类器（如果可能）以最大化其准确性。
- en: 5.2 Chapter learning objectives
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 本章学习目标
- en: 'By the end of the chapter, readers will be able to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，读者将能够做到以下几点：
- en: Recognize situations where a classifier would be appropriate for making predictions.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别适合进行预测的分类器的情况。
- en: Describe what a training data set is and how it is used in classification.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述训练数据集是什么以及它在分类中的应用。
- en: Interpret the output of a classifier.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释分类器的输出。
- en: Compute, by hand, the straight-line (Euclidean) distance between points on a
    graph when there are two predictor variables.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有两个预测变量时，手动计算图上点之间的直线（欧几里得）距离。
- en: Explain the K-nearest neighbors classification algorithm.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释K最近邻分类算法。
- en: Perform K-nearest neighbors classification in R using `tidymodels`.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tidymodels`在R中执行K最近邻分类。
- en: Use a `recipe` to center, scale, balance, and impute data as a preprocessing
    step.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`recipe`作为预处理步骤来中心化、缩放、平衡和插补数据。
- en: Combine preprocessing and model training using a `workflow`.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`workflow`结合预处理和模型训练。
- en: 5.3 The classification problem
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 分类问题
- en: In many situations, we want to make predictions based on the current situation
    as well as past experiences. For instance, a doctor may want to diagnose a patient
    as either diseased or healthy based on their symptoms and the doctor’s past experience
    with patients; an email provider might want to tag a given email as “spam” or
    “not spam” based on the email’s text and past email text data; or a credit card
    company may want to predict whether a purchase is fraudulent based on the current
    purchase item, amount, and location as well as past purchases. These tasks are
    all examples of **classification**, i.e., predicting a categorical class (sometimes
    called a *label*) for an observation given its other variables (sometimes called
    *features*).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们希望根据当前情况以及过去的经验进行预测。例如，医生可能希望根据患者的症状和医生过去与患者的经验来诊断患者是患病还是健康；电子邮件提供商可能希望根据电子邮件的文本和过去的电子邮件文本数据来标记给定的电子邮件为“垃圾邮件”或“非垃圾邮件”；或者信用卡公司可能希望根据当前的购买项目、金额和位置以及过去的购买来预测购买是否欺诈。这些任务都是**分类**的例子，即根据观察的其他变量（有时称为**特征**）预测一个分类类别（有时称为**标签**）。
- en: Generally, a classifier assigns an observation without a known class (e.g.,
    a new patient) to a class (e.g., diseased or healthy) on the basis of how similar
    it is to other observations for which we do know the class (e.g., previous patients
    with known diseases and symptoms). These observations with known classes that
    we use as a basis for prediction are called a **training set**; this name comes
    from the fact that we use these data to train, or teach, our classifier. Once
    taught, we can use the classifier to make predictions on new data for which we
    do not know the class.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分类器将一个未知类别的观察（例如，新患者）分配到一个类别（例如，患病或健康），这是基于它与已知类别的其他观察的相似程度（例如，已知疾病和症状的先前患者）。我们用作预测基础的已知类别的观察称为**训练集**；这个名字来源于我们使用这些数据来训练或教授我们的分类器。一旦教授完毕，我们就可以使用分类器对新数据做出预测，这些新数据我们不知道其类别。
- en: There are many possible methods that we could use to predict a categorical class/label
    for an observation. In this book, we will focus on the widely used **K-nearest
    neighbors** algorithm ([Fix and Hodges 1951](#ref-knnfix); [Cover and Hart 1967](#ref-knncover)).
    In your future studies, you might encounter decision trees, support vector machines
    (SVMs), logistic regression, neural networks, and more; see the additional resources
    section at the end of the next chapter for where to begin learning more about
    these other methods. It is also worth mentioning that there are many variations
    on the basic classification problem. For example, we focus on the setting of **binary
    classification** where only two classes are involved (e.g., a diagnosis of either
    healthy or diseased), but you may also run into multiclass classification problems
    with more than two categories (e.g., a diagnosis of healthy, bronchitis, pneumonia,
    or a common cold).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多可能的方法来预测一个观察值的分类类别/标签。在这本书中，我们将重点关注广泛使用的**K最近邻**算法（[Fix 和 Hodges 1951](#ref-knnfix)；[Cover
    和 Hart 1967](#ref-knncover)）。在你的未来研究中，你可能会遇到决策树、支持向量机（SVMs）、逻辑回归、神经网络等；下一章末尾的附加资源部分提供了开始学习这些其他方法的起点。还值得一提的是，基本分类问题有许多变体。例如，我们关注的是**二元分类**的设置，其中只涉及两个类别（例如，健康或疾病的诊断），但你也可能遇到涉及两个以上类别的多类别分类问题（例如，健康、支气管炎、肺炎或普通感冒）。
- en: 5.4 Exploring a data set
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 探索数据集
- en: In this chapter and the next, we will study a data set of [digitized breast
    cancer image features](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29),
    created by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian ([Street,
    Wolberg, and Mangasarian 1993](#ref-streetbreastcancer)). Each row in the data
    set represents an image of a tumor sample, including the diagnosis (benign or
    malignant) and several other measurements (nucleus texture, perimeter, area, and
    more). Diagnosis for each image was conducted by physicians.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，我们将研究由威廉·H·沃尔伯格博士、W·尼克·斯特里特和奥利维·L·曼加萨里安创建的[数字化乳腺癌图像特征](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)数据集，[Street、沃尔伯格和曼加萨里安
    1993](#ref-streetbreastcancer)。数据集中的每一行代表一个肿瘤样本的图像，包括诊断（良性或恶性）和几个其他测量值（细胞纹理、周长、面积等）。每个图像的诊断都是由医生进行的。
- en: 'As with all data analyses, we first need to formulate a precise question that
    we want to answer. Here, the question is *predictive*: can we use the tumor image
    measurements available to us to predict whether a future tumor image (with unknown
    diagnosis) shows a benign or malignant tumor? Answering this question is important
    because traditional, non-data-driven methods for tumor diagnosis are quite subjective
    and dependent upon how skilled and experienced the diagnosing physician is. Furthermore,
    benign tumors are not normally dangerous; the cells stay in the same place, and
    the tumor stops growing before it gets very large. By contrast, in malignant tumors,
    the cells invade the surrounding tissue and spread into nearby organs, where they
    can cause serious damage ([Stanford Health Care 2021](#ref-stanfordhealthcare)).
    Thus, it is important to quickly and accurately diagnose the tumor type to guide
    patient treatment.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有数据分析一样，我们首先需要明确一个我们想要回答的精确问题。在这里，问题是**预测性的**：我们能否使用我们可用的肿瘤图像测量值来预测未来的肿瘤图像（未知诊断）显示的是良性还是恶性肿瘤？回答这个问题很重要，因为传统的、非数据驱动的肿瘤诊断方法相当主观，并且依赖于诊断医生的技术和经验。此外，良性肿瘤通常不会造成危险；细胞停留在同一位置，肿瘤在变得非常大之前就会停止生长。相比之下，在恶性肿瘤中，细胞会侵犯周围组织并扩散到附近的器官，在那里它们可以造成严重损害（[斯坦福健康护理
    2021](#ref-stanfordhealthcare)）。因此，快速准确地诊断肿瘤类型以指导患者治疗非常重要。
- en: 5.4.1 Loading the cancer data
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 加载癌症数据
- en: Our first step is to load, wrangle, and explore the data using visualizations
    in order to better understand the data we are working with. We start by loading
    the `tidyverse` package needed for our analysis.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是使用可视化来加载、整理和探索数据，以便更好地理解我们所处理的数据。我们首先加载分析所需的`tidyverse`包。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this case, the file containing the breast cancer data set is a `.csv` file
    with headers. We’ll use the `read_csv` function with no additional arguments,
    and then inspect its contents:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，包含乳腺癌数据集的文件是一个带有标题的`.csv`文件。我们将使用不带额外参数的`read_csv`函数，然后检查其内容：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 5.4.2 Describing the variables in the cancer data set
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 描述癌症数据集中的变量
- en: 'Breast tumors can be diagnosed by performing a *biopsy*, a process where tissue
    is removed from the body and examined for the presence of disease. Traditionally
    these procedures were quite invasive; modern methods such as fine needle aspiration,
    used to collect the present data set, extract only a small amount of tissue and
    are less invasive. Based on a digital image of each breast tissue sample collected
    for this data set, ten different variables were measured for each cell nucleus
    in the image (items 3–12 of the list of variables below), and then the mean for
    each variable across the nuclei was recorded. As part of the data preparation,
    these values have been *standardized (centered and scaled)*; we will discuss what
    this means and why we do it later in this chapter. Each image additionally was
    given a unique ID and a diagnosis by a physician. Therefore, the total set of
    variables per image in this data set is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌可以通过进行 *活检* 来诊断，这是一个从体内取出组织并检查疾病存在的过程。传统上这些程序相当侵入性；现代方法，如用于收集现有数据集的细针穿刺，只提取少量组织，侵入性较小。基于为该数据集收集的每个乳腺组织样本的数字图像，测量了图像中每个细胞核的十个不同变量（以下变量列表的第3-12项），然后记录了每个变量的核平均值。作为数据准备的一部分，这些值已经被
    *标准化（居中和缩放）*；我们将在本章后面讨论这意味着什么以及为什么这样做。每个图像还由医生给出了一个唯一的ID和诊断。因此，该数据集中每个图像的变量总数为：
- en: 'ID: identification number'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ID：识别号
- en: 'Class: the diagnosis (M = malignant or B = benign)'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类别：诊断（M = 恶性或 B = 良性）
- en: 'Radius: the mean of distances from center to points on the perimeter'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 半径：从中心到轮廓上点的距离的平均值
- en: 'Texture: the standard deviation of gray-scale values'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 纹理：灰度值的标准差
- en: 'Perimeter: the length of the surrounding contour'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 周长：周围轮廓的长度
- en: 'Area: the area inside the contour'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 面积：轮廓内的面积
- en: 'Smoothness: the local variation in radius lengths'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平滑度：半径长度的局部变化
- en: 'Compactness: the ratio of squared perimeter and area'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 紧凑度：周长平方与面积的比率
- en: 'Concavity: severity of concave portions of the contour'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凹度：轮廓凹部的严重程度
- en: 'Concave Points: the number of concave portions of the contour'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凹点：轮廓的凹部数量
- en: 'Symmetry: how similar the nucleus is when mirrored'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对称性：当镜像时核的相似程度
- en: 'Fractal Dimension: a measurement of how “rough” the perimeter is'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分形维度：衡量轮廓“粗糙”程度的一个度量
- en: Below we use `glimpse` to preview the data frame. This function can make it
    easier to inspect the data when we have a lot of columns, as it prints the data
    such that the columns go down the page (instead of across).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们使用 `glimpse` 预览数据框。当有大量列时，这个函数可以让我们更容易地检查数据，因为它以列向下打印数据（而不是横向）。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From the summary of the data above, we can see that `Class` is of type character
    (denoted by `<chr>`). We can use the `distinct` function to see all the unique
    values present in that column. We see that there are two diagnoses: benign, represented
    by “B”, and malignant, represented by “M”.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的数据摘要中，我们可以看到 `Class` 是字符类型（用 `<chr>` 表示）。我们可以使用 `distinct` 函数查看该列中存在的所有唯一值。我们看到有两种诊断：良性，用“B”表示，和恶性，用“M”表示。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Since we will be working with `Class` as a categorical variable, it is a good
    idea to convert it to a factor type using the `as_factor` function. We will also
    improve the readability of our analysis by renaming “M” to “Malignant” and “B”
    to “Benign” using the `fct_recode` method. The `fct_recode` method is used to
    replace the names of factor values with other names. The arguments of `fct_recode`
    are the column that you want to modify, followed any number of arguments of the
    form `"new name" = "old name"` to specify the renaming scheme.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用 `Class` 作为分类变量，所以使用 `as_factor` 函数将其转换为因子类型是个好主意。我们还将通过使用 `fct_recode`
    方法将“M”重命名为“Malignant”和“B”重命名为“Benign”来提高我们分析的可读性。`fct_recode` 方法用于用其他名称替换因子值的名称。`fct_recode`
    的参数是你想要修改的列，后跟任意数量的形式为 `"new name" = "old name"` 的参数，以指定重命名方案。
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s verify that we have successfully converted the `Class` column to a factor
    variable and renamed its values to “Benign” and “Malignant” using the `distinct`
    function once more.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用 `distinct` 函数验证我们已经成功将 `Class` 列转换为因子变量，并将其值重命名为“Benign”和“Malignant”。
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 5.4.3 Exploring the cancer data
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 探索癌症数据
- en: Before we start doing any modeling, let’s explore our data set. Below we use
    the `group_by`, `summarize` and `n` functions to find the number and percentage
    of benign and malignant tumor observations in our data set. The `n` function within
    `summarize`, when paired with `group_by`, counts the number of observations in
    each `Class` group. Then we calculate the percentage in each group by dividing
    by the total number of observations and multiplying by 100\. We have 357 (63%)
    benign and 212 (37%) malignant tumor observations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始任何建模之前，让我们探索我们的数据集。下面我们使用 `group_by`、`summarize` 和 `n` 函数来找出我们数据集中良性肿瘤观察值和恶性肿瘤观察值的数量和百分比。`summarize`
    中的 `n` 函数与 `group_by` 配对时，计算每个 `Class` 组中的观察值数量。然后我们通过除以观察值总数并乘以 100 来计算每个组中的百分比。我们有
    357（63%）个良性肿瘤观察值和 212（37%）个恶性肿瘤观察值。
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, let’s draw a scatter plot to visualize the relationship between the perimeter
    and concavity variables. Rather than use `ggplot's` default palette, we select
    our own colorblind-friendly colors—`"darkorange"` for orange and `"steelblue"`
    for blue—and pass them as the `values` argument to the `scale_color_manual` function.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们绘制一个散点图来可视化周长和凹度变量之间的关系。我们不会使用 `ggplot` 的默认调色板，而是选择我们自己的色盲友好颜色——“darkorange”用于橙色，“steelblue”用于蓝色——并将它们作为
    `values` 参数传递给 `scale_color_manual` 函数。
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Scatter plot of concavity versus perimeter colored by diagnosis label.](../Images/f2fd4127a1e903e2f2275d3f31eb8064.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![根据诊断标签着色的凹度与周长散点图](../Images/f2fd4127a1e903e2f2275d3f31eb8064.png)'
- en: 'Figure 5.1: Scatter plot of concavity versus perimeter colored by diagnosis
    label.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：根据诊断标签着色的凹度与周长散点图。
- en: In Figure [5.1](classification1.html#fig:05-scatter), we can see that malignant
    observations typically fall in the upper right-hand corner of the plot area. By
    contrast, benign observations typically fall in the lower left-hand corner of
    the plot. In other words, benign observations tend to have lower concavity and
    perimeter values, and malignant ones tend to have larger values. Suppose we obtain
    a new observation not in the current data set that has all the variables measured
    *except* the label (i.e., an image without the physician’s diagnosis for the tumor
    class). We could compute the standardized perimeter and concavity values, resulting
    in values of, say, 1 and 1\. Could we use this information to classify that observation
    as benign or malignant? Based on the scatter plot, how might you classify that
    new observation? If the standardized concavity and perimeter values are 1 and
    1 respectively, the point would lie in the middle of the orange cloud of malignant
    points and thus we could probably classify it as malignant. Based on our visualization,
    it seems like it may be possible to make accurate predictions of the `Class` variable
    (i.e., a diagnosis) for tumor images with unknown diagnoses.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [5.1](classification1.html#fig:05-scatter) 中，我们可以看到恶性观察值通常落在图表区域的右上角。相比之下，良性观察值通常落在图表的左下角。换句话说，良性观察值倾向于具有较低的凹度和周长值，而恶性观察值倾向于具有较大的值。假设我们获得了一个新的观察值，它不在当前数据集中，并且测量了所有变量（即没有医生对肿瘤类别的诊断的图像）。我们可以计算标准化的周长和凹度值，结果可能是
    1 和 1\. 我们能否使用这些信息将该观察值分类为良性或恶性？根据散点图，你如何对该新观察值进行分类？如果标准化的凹度和周长值分别是 1 和 1，那么该点将位于恶性点的橙色云团的中间，因此我们可能将其分类为恶性。根据我们的可视化，似乎可以准确预测具有未知诊断的肿瘤图像的
    `Class` 变量（即诊断）。
- en: 5.5 Classification with K-nearest neighbors
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 使用 K 近邻进行分类
- en: In order to actually make predictions for new observations in practice, we will
    need a classification algorithm. In this book, we will use the K-nearest neighbors
    classification algorithm. To predict the label of a new observation (here, classify
    it as either benign or malignant), the K-nearest neighbors classifier generally
    finds the \(K\) “nearest” or “most similar” observations in our training set,
    and then uses their diagnoses to make a prediction for the new observation’s diagnosis.
    \(K\) is a number that we must choose in advance; for now, we will assume that
    someone has chosen \(K\) for us. We will cover how to choose \(K\) ourselves in
    the next chapter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在实际中对新观察值进行预测，我们需要一个分类算法。在这本书中，我们将使用 K 近邻分类算法。为了预测新观察值的标签（在此处将其分类为良性或恶性），K
    近邻分类器通常在训练集中找到 \(K\) 个“最近”或“最相似”的观察值，然后使用它们的诊断来对新观察值的诊断进行预测。\(K\) 是我们必须提前选择的数字；现在，我们假设有人已经为我们选择了
    \(K\)。我们将在下一章中介绍如何自己选择 \(K\)。
- en: To illustrate the concept of K-nearest neighbors classification, we will walk
    through an example. Suppose we have a new observation, with standardized perimeter
    of 2 and standardized concavity of 4, whose diagnosis “Class” is unknown. This
    new observation is depicted by the red, diamond point in Figure [5.2](classification1.html#fig:05-knn-1).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 K 近邻分类的概念，我们将通过一个例子来讲解。假设我们有一个新的观测值，其标准化周长为 2，标准化凹度为 4，其诊断“类别”未知。这个新的观测值在图
    [5.2](classification1.html#fig:05-knn-1) 中用红色、菱形点表示。
- en: '![Scatter plot of concavity versus perimeter with new observation represented
    as a red diamond.](../Images/313e6d6e9723318ee25131e3985f2317.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![凹度与周长之间的散点图，新的观测值用一个红色菱形表示。](../Images/313e6d6e9723318ee25131e3985f2317.png)'
- en: 'Figure 5.2: Scatter plot of concavity versus perimeter with new observation
    represented as a red diamond.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：凹度与周长之间的散点图，新的观测值用一个红色菱形表示。
- en: Figure [5.3](classification1.html#fig:05-knn-2) shows that the nearest point
    to this new observation is **malignant** and located at the coordinates (2.1,
    3.6). The idea here is that if a point is close to another in the scatter plot,
    then the perimeter and concavity values are similar, and so we may expect that
    they would have the same diagnosis.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5.3](classification1.html#fig:05-knn-2) 显示，这个新观测值最近的点是 **恶性**，坐标为 (2.1, 3.6)。这里的想法是，如果一个点在散点图中接近另一个点，那么它们的周长和凹度值相似，因此我们可能期望它们会有相同的诊断。
- en: '![Scatter plot of concavity versus perimeter. The new observation is represented
    as a red diamond with a line to the one nearest neighbor, which has a malignant
    label.](../Images/55e8ccd01bd0feefc89edc47c95b251c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![凹度与周长之间的散点图。新的观测值用一个红色菱形表示，并有一条线指向最近的邻居，该邻居具有恶性的标签。](../Images/55e8ccd01bd0feefc89edc47c95b251c.png)'
- en: 'Figure 5.3: Scatter plot of concavity versus perimeter. The new observation
    is represented as a red diamond with a line to the one nearest neighbor, which
    has a malignant label.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：凹度与周长之间的散点图。新的观测值用一个红色菱形表示，并有一条线指向最近的邻居，该邻居具有恶性的标签。
- en: Suppose we have another new observation with standardized perimeter 0.2 and
    concavity of 3.3\. Looking at the scatter plot in Figure [5.4](classification1.html#fig:05-knn-4),
    how would you classify this red, diamond observation? The nearest neighbor to
    this new point is a **benign** observation at (0.2, 2.7). Does this seem like
    the right prediction to make for this observation? Probably not, if you consider
    the other nearby points.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个新的观测值，其标准化周长为 0.2，凹度为 3.3。查看图 [5.4](classification1.html#fig:05-knn-4)
    中的散点图，你会如何分类这个红色、菱形的观测值？这个新点的最近邻是一个位于 (0.2, 2.7) 的 **良性** 观测值。考虑到其他附近的点，这个预测看起来正确吗？可能不正确。
- en: '![Scatter plot of concavity versus perimeter. The new observation is represented
    as a red diamond with a line to the one nearest neighbor, which has a benign label.](../Images/e1dc5643d50a06aa4d841a06deab916d.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![凹度与周长之间的散点图。新的观测值用一个红色菱形表示，并有一条线指向最近的邻居，该邻居具有良性的标签。](../Images/e1dc5643d50a06aa4d841a06deab916d.png)'
- en: 'Figure 5.4: Scatter plot of concavity versus perimeter. The new observation
    is represented as a red diamond with a line to the one nearest neighbor, which
    has a benign label.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：凹度与周长之间的散点图。新的观测值用一个红色菱形表示，并有一条线指向最近的邻居，该邻居具有良性的标签。
- en: To improve the prediction we can consider several neighboring points, say \(K
    = 3\), that are closest to the new observation to predict its diagnosis class.
    Among those 3 closest points, we use the *majority class* as our prediction for
    the new observation. As shown in Figure [5.5](classification1.html#fig:05-knn-5),
    we see that the diagnoses of 2 of the 3 nearest neighbors to our new observation
    are malignant. Therefore we take majority vote and classify our new red, diamond
    observation as malignant.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高预测的准确性，我们可以考虑几个邻近的点，比如 \(K = 3\)，这些点距离新的观测值最近，以预测其诊断类别。在这三个最近点中，我们使用 *多数类别*
    作为对新观测值的预测。如图 [5.5](classification1.html#fig:05-knn-5) 所示，我们看到我们的新观测值最近的三个邻居中有两个的诊断为恶性。因此，我们进行多数投票，并将新的红色、菱形观测值分类为恶性。
- en: '![Scatter plot of concavity versus perimeter with three nearest neighbors.](../Images/ca7153b3eb882abb0c40235604b080a9.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![凹度与周长之间的散点图，以及三个最近邻点。](../Images/ca7153b3eb882abb0c40235604b080a9.png)'
- en: 'Figure 5.5: Scatter plot of concavity versus perimeter with three nearest neighbors.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：凹度与周长之间的散点图，以及三个最近邻点。
- en: Here we chose the \(K=3\) nearest observations, but there is nothing special
    about \(K=3\). We could have used \(K=4, 5\) or more (though we may want to choose
    an odd number to avoid ties). We will discuss more about choosing \(K\) in the
    next chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择了 \(K=3\) 个最近的观测，但 \(K=3\) 没有什么特殊之处。我们也可以使用 \(K=4, 5\) 或更多（尽管我们可能希望选择一个奇数以避免平局）。我们将在下一章中更详细地讨论如何选择
    \(K\)。
- en: 5.5.1 Distance between points
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 点之间的距离
- en: 'We decide which points are the \(K\) “nearest” to our new observation using
    the *straight-line distance* (we will often just refer to this as *distance*).
    Suppose we have two observations \(a\) and \(b\), each having two predictor variables,
    \(x\) and \(y\). Denote \(a_x\) and \(a_y\) to be the values of variables \(x\)
    and \(y\) for observation \(a\); \(b_x\) and \(b_y\) have similar definitions
    for observation \(b\). Then the straight-line distance between observation \(a\)
    and \(b\) on the x-y plane can be computed using the following formula:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定哪些点是相对于我们的新观测最近的 \(K\) 个，使用的是*直线距离*（我们通常只称之为*距离*）。假设我们有两个观测 \(a\) 和 \(b\)，每个观测都有两个预测变量，\(x\)
    和 \(y\)。用 \(a_x\) 和 \(a_y\) 表示观测 \(a\) 的变量 \(x\) 和 \(y\) 的值；\(b_x\) 和 \(b_y\)
    对于观测 \(b\) 有类似的定义。那么，在 x-y 平面上观测 \(a\) 和 \(b\) 之间的直线距离可以使用以下公式计算：
- en: \[\mathrm{Distance} = \sqrt{(a_x -b_x)^2 + (a_y - b_y)^2}\]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: \[距离 = \sqrt{(a_x -b_x)^2 + (a_y - b_y)^2}\]
- en: 'To find the \(K\) nearest neighbors to our new observation, we compute the
    distance from that new observation to each observation in our training data, and
    select the \(K\) observations corresponding to the \(K\) *smallest* distance values.
    For example, suppose we want to use \(K=5\) neighbors to classify a new observation
    with perimeter of 0 and concavity of 3.5, shown as a red diamond in Figure [5.6](classification1.html#fig:05-multiknn-1).
    Let’s calculate the distances between our new point and each of the observations
    in the training set to find the \(K=5\) neighbors that are nearest to our new
    point. You will see in the `mutate` step below, we compute the straight-line distance
    using the formula above: we square the differences between the two observations’
    perimeter and concavity coordinates, add the squared differences, and then take
    the square root. In order to find the \(K=5\) nearest neighbors, we will use the
    `slice_min` function.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到新观测的 \(K\) 个最近邻，我们计算新观测到训练数据中每个观测的距离，并选择与 \(K\) 个*最小*距离值对应的 \(K\) 个观测。例如，假设我们想使用
    \(K=5\) 个邻居来分类周长为 0 且凸度为 3.5 的新观测，如图 [5.6](classification1.html#fig:05-multiknn-1)
    中的红色菱形所示。让我们计算新点与训练集中每个观测之间的距离，以找到最近的 \(K=5\) 个邻居。你将在下面的 `mutate` 步骤中看到，我们使用上述公式计算直线距离：我们将两个观测的周长和凸度坐标之间的差异平方，然后相加，并取平方根。为了找到
    \(K=5\) 个最近邻，我们将使用 `slice_min` 函数。
- en: '![Scatter plot of concavity versus perimeter with new observation represented
    as a red diamond.](../Images/6a86babb8324f0c2761c7ec68f9bea4b.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![凸度与周长的散点图，新观测用红色菱形表示。](../Images/6a86babb8324f0c2761c7ec68f9bea4b.png)'
- en: 'Figure 5.6: Scatter plot of concavity versus perimeter with new observation
    represented as a red diamond.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：凸度与周长的散点图，新观测用红色菱形表示。
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In Table [5.1](classification1.html#tab:05-multiknn-mathtable) we show in mathematical
    detail how the `mutate` step was used to compute the `dist_from_new` variable
    (the distance to the new observation) for each of the 5 nearest neighbors in the
    training data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [5.1](classification1.html#tab:05-multiknn-mathtable) 中，我们详细展示了如何使用 `mutate`
    步骤来计算训练数据中每个最近邻的 `dist_from_new` 变量（即到新观测的距离）。
- en: 'Table 5.1: Evaluating the distances from the new observation to each of its
    5 nearest neighbors'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1：评估新观测点到其 5 个最近邻的距离
- en: '| Perimeter | Concavity | Distance | Class |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 周长 | 凸度 | 距离 | 类别 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0.24 | 2.65 | \(\sqrt{(0 - 0.24)^2 + (3.5 - 2.65)^2} = 0.88\) | Benign |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 0.24 | 2.65 | \(\sqrt{(0 - 0.24)^2 + (3.5 - 2.65)^2} = 0.88\) | 良性 |'
- en: '| 0.75 | 2.87 | \(\sqrt{(0 - 0.75)^2 + (3.5 - 2.87)^2} = 0.98\) | Malignant
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 0.75 | 2.87 | \(\sqrt{(0 - 0.75)^2 + (3.5 - 2.87)^2} = 0.98\) | 恶性 |'
- en: '| 0.62 | 2.54 | \(\sqrt{(0 - 0.62)^2 + (3.5 - 2.54)^2} = 1.14\) | Malignant
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 0.62 | 2.54 | \(\sqrt{(0 - 0.62)^2 + (3.5 - 2.54)^2} = 1.14\) | 恶性 |'
- en: '| 0.42 | 2.31 | \(\sqrt{(0 - 0.42)^2 + (3.5 - 2.31)^2} = 1.26\) | Malignant
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 0.42 | 2.31 | \(\sqrt{(0 - 0.42)^2 + (3.5 - 2.31)^2} = 1.26\) | 恶性 |'
- en: '| -1.16 | 4.04 | \(\sqrt{(0 - (-1.16))^2 + (3.5 - 4.04)^2} = 1.28\) | Benign
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| -1.16 | 4.04 | \(\sqrt{(0 - (-1.16))^2 + (3.5 - 4.04)^2} = 1.28\) | 良性 |'
- en: The result of this computation shows that 3 of the 5 nearest neighbors to our
    new observation are malignant; since this is the majority, we classify our new
    observation as malignant. These 5 neighbors are circled in Figure [5.7](classification1.html#fig:05-multiknn-3).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算的结果显示，我们新观测值的 5 个最近邻中有 3 个是恶性的；由于这是多数，我们将我们的新观测值分类为恶性。这 5 个邻居在图 [5.7](classification1.html#fig:05-multiknn-3)
    中被圈出。
- en: '![Scatter plot of concavity versus perimeter with 5 nearest neighbors circled.](../Images/e7222050606b1a8b3f4b07d87d34974d.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![凹度与周长的散点图，周围圈出了 5 个最近邻。](../Images/e7222050606b1a8b3f4b07d87d34974d.png)'
- en: 'Figure 5.7: Scatter plot of concavity versus perimeter with 5 nearest neighbors
    circled.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：凹度与周长的散点图，周围圈出了 5 个最近邻。
- en: 5.5.2 More than two explanatory variables
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 多于两个解释变量
- en: Although the above description is directed toward two predictor variables, exactly
    the same K-nearest neighbors algorithm applies when you have a higher number of
    predictor variables. Each predictor variable may give us new information to help
    create our classifier. The only difference is the formula for the distance between
    points. Suppose we have \(m\) predictor variables for two observations \(a\) and
    \(b\), i.e., \(a = (a_{1}, a_{2}, \dots, a_{m})\) and \(b = (b_{1}, b_{2}, \dots,
    b_{m})\).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述描述是针对两个预测变量，但当你有更多预测变量时，完全相同的 K 近邻算法同样适用。每个预测变量都可能提供新的信息来帮助我们创建分类器。唯一的区别是点之间距离的公式。假设我们有两个观测值
    \(a\) 和 \(b\) 的 \(m\) 个预测变量，即 \(a = (a_{1}, a_{2}, \dots, a_{m})\) 和 \(b = (b_{1},
    b_{2}, \dots, b_{m})\)。
- en: The distance formula becomes
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 距离公式变为
- en: \[\mathrm{Distance} = \sqrt{(a_{1} -b_{1})^2 + (a_{2} - b_{2})^2 + \dots + (a_{m}
    - b_{m})^2}.\]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathrm{Distance} = \sqrt{(a_{1} -b_{1})^2 + (a_{2} - b_{2})^2 + \dots + (a_{m}
    - b_{m})^2}.\]
- en: 'This formula still corresponds to a straight-line distance, just in a space
    with more dimensions. Suppose we want to calculate the distance between a new
    observation with a perimeter of 0, concavity of 3.5, and symmetry of 1, and another
    observation with a perimeter, concavity, and symmetry of 0.417, 2.31, and 0.837
    respectively. We have two observations with three predictor variables: perimeter,
    concavity, and symmetry. Previously, when we had two variables, we added up the
    squared difference between each of our (two) variables, and then took the square
    root. Now we will do the same, except for our three variables. We calculate the
    distance as follows'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式仍然对应于直线距离，只是在更高维度的空间中。假设我们想要计算一个周长为 0、凹度为 3.5、对称度为 1 的新观测值与另一个周长、凹度和对称度分别为
    0.417、2.31 和 0.837 的观测值之间的距离。我们有两个具有三个预测变量（周长、凹度和对称度）的观测值。之前，当我们有两个变量时，我们计算了每个（两个）变量之间的平方差的和，然后取平方根。现在我们将做同样的事情，只是对于我们的三个变量。我们计算距离如下
- en: \[\mathrm{Distance} =\sqrt{(0 - 0.417)^2 + (3.5 - 2.31)^2 + (1 - 0.837)^2} =
    1.27.\]
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathrm{Distance} =\sqrt{(0 - 0.417)^2 + (3.5 - 2.31)^2 + (1 - 0.837)^2} =
    1.27.\]
- en: Let’s calculate the distances between our new observation and each of the observations
    in the training set to find the \(K=5\) neighbors when we have these three predictors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算新观测值与训练集中每个观测值之间的距离，以找到当有三个预测变量时 \(K=5\) 的最近邻。
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Based on \(K=5\) nearest neighbors with these three predictors, we would classify
    the new observation as malignant since 4 out of 5 of the nearest neighbors are
    from the malignant class. Figure [5.8](classification1.html#fig:05-more) shows
    what the data look like when we visualize them as a 3-dimensional scatter with
    lines from the new observation to its five nearest neighbors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这三个预测变量的 \(K=5\) 个最近邻，我们将新观测值分类为恶性，因为 5 个最近邻中有 4 个来自恶性类别。图 [5.8](classification1.html#fig:05-more)
    展示了我们将这些数据可视化为三维散点图，并从新观测值到其五个最近邻画线的样子。
- en: 'Figure 5.8: 3D scatter plot of the standardized symmetry, concavity, and perimeter
    variables. Note that in general we recommend against using 3D visualizations;
    here we show the data in 3D only to illustrate what higher dimensions and nearest
    neighbors look like, for learning purposes.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：标准化对称性、凹度和周长变量的 3D 散点图。请注意，通常我们不建议使用 3D 可视化；这里我们仅以 3D 形式展示数据，以说明高维和最近邻的形态，供学习参考。
- en: 5.5.3 Summary of K-nearest neighbors algorithm
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.3 K 近邻算法总结
- en: 'In order to classify a new observation using a K-nearest neighbors classifier,
    we have to do the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 K 近邻分类器对新观测值进行分类，我们必须执行以下操作：
- en: Compute the distance between the new observation and each observation in the
    training set.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新观测值与训练集中每个观测值之间的距离。
- en: Sort the data table in ascending order according to the distances.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据距离对数据表进行升序排序。
- en: Choose the top \(K\) rows of the sorted table.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择排序表中排名前 \(K\) 的行。
- en: Classify the new observation based on a majority vote of the neighbor classes.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据邻近类别的多数投票来对新观测进行分类。
- en: 5.6 K-nearest neighbors with `tidymodels`
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 使用 `tidymodels` 的 K 近邻算法
- en: Coding the K-nearest neighbors algorithm in R ourselves can get complicated,
    especially if we want to handle multiple classes, more than two variables, or
    predict the class for multiple new observations. Thankfully, in R, the K-nearest
    neighbors algorithm is implemented in [the `parsnip` R package](https://parsnip.tidymodels.org/)
    ([Kuhn and Vaughan 2021](#ref-parsnip)) included in `tidymodels`, along with many
    [other models](https://www.tidymodels.org/find/parsnip/) that you will encounter
    in this and future chapters of the book. The `tidymodels` collection provides
    tools to help make and use models, such as classifiers. Using the packages in
    this collection will help keep our code simple, readable and accurate; the less
    we have to code ourselves, the fewer mistakes we will likely make. We start by
    loading `tidymodels`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中自己编写 K 近邻算法的代码可能会变得复杂，尤其是如果我们想要处理多个类别、超过两个变量，或者预测多个新观测的类别时。幸运的是，在 R 中，K
    近邻算法已经在 `tidymodels` 包中实现，包括 [`parsnip` R 包](https://parsnip.tidymodels.org/)
    ([Kuhn and Vaughan 2021](#ref-parsnip))，以及你将在本书的此章节和未来章节中遇到的许多其他模型。`tidymodels`
    集合提供了帮助构建和使用模型（如分类器）的工具。使用这个集合中的包将有助于使我们的代码简单、可读和准确；我们编写的代码越少，我们可能犯的错误就越少。我们首先通过加载
    `tidymodels` 开始。
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s walk through how to use `tidymodels` to perform K-nearest neighbors classification.
    We will use the `cancer` data set from above, with perimeter and concavity as
    predictors and \(K = 5\) neighbors to build our classifier. Then we will use the
    classifier to predict the diagnosis label for a new observation with perimeter
    0, concavity 3.5, and an unknown diagnosis label. Let’s pick out our two desired
    predictor variables and class label and store them as a new data set named `cancer_train`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 `tidymodels` 如何执行 K 近邻分类来一步步说明。我们将使用上面的 `cancer` 数据集，以周长和凹凸性作为预测变量，并使用
    \(K = 5\) 个邻居来构建我们的分类器。然后我们将使用这个分类器来预测一个新观测的周长为 0、凹凸性为 3.5 且诊断标签未知的诊断标签。让我们挑选出我们想要的两个预测变量和类别标签，并将它们存储在一个名为
    `cancer_train` 的新数据集中：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we create a *model specification* for K-nearest neighbors classification
    by calling the `nearest_neighbor` function, specifying that we want to use \(K
    = 5\) neighbors (we will discuss how to choose \(K\) in the next chapter) and
    that each neighboring point should have the same weight when voting (`weight_func
    = "rectangular"`). The `weight_func` argument controls how neighbors vote when
    classifying a new observation; by setting it to `"rectangular"`, each of the \(K\)
    nearest neighbors gets exactly 1 vote as described above. Other choices, which
    weigh each neighbor’s vote differently, can be found on [the `parsnip` website](https://parsnip.tidymodels.org/reference/nearest_neighbor.html).
    In the `set_engine` argument, we specify which package or system will be used
    for training the model. Here `kknn` is the R package we will use for performing
    K-nearest neighbors classification. Finally, we specify that this is a classification
    problem with the `set_mode` function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用 `nearest_neighbor` 函数创建一个 K 近邻分类的 *模型规范*，指定我们想要使用 \(K = 5\) 个邻居（我们将在下一章讨论如何选择
    \(K\)），并且每个邻近点在投票时应该具有相同的权重（`weight_func = "rectangular"`）。`weight_func` 参数控制邻居在分类新观测时如何投票；将其设置为
    `"rectangular"`，则每个最近的 \(K\) 个邻居将获得上述描述中的确切 1 票。其他选择，其中每个邻居的投票权重不同，可以在 [`parsnip`
    网站上找到](https://parsnip.tidymodels.org/reference/nearest_neighbor.html)。在 `set_engine`
    参数中，我们指定将用于训练模型的包或系统。在这里，`kknn` 是我们将用于执行 K 近邻分类的 R 包。最后，我们通过 `set_mode` 函数指定这是一个分类问题。
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In order to fit the model on the breast cancer data, we need to pass the model
    specification and the data set to the `fit` function. We also need to specify
    what variables to use as predictors and what variable to use as the response.
    Below, the `Class ~ Perimeter + Concavity` argument specifies that `Class` is
    the response variable (the one we want to predict), and both `Perimeter` and `Concavity`
    are to be used as the predictors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在乳腺癌数据上拟合模型，我们需要将模型规范和数据集传递给 `fit` 函数。我们还需要指定要使用哪些变量作为预测变量以及要使用哪个变量作为响应变量。下面，`Class
    ~ Perimeter + Concavity` 参数指定 `Class` 是响应变量（我们想要预测的那个），而 `Perimeter` 和 `Concavity`
    都将用作预测变量。
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can also use a convenient shorthand syntax using a period, `Class ~ .`, to
    indicate that we want to use every variable *except* `Class` as a predictor in
    the model. In this particular setup, since `Concavity` and `Perimeter` are the
    only two predictors in the `cancer_train` data frame, `Class ~ Perimeter + Concavity`
    and `Class ~ .` are equivalent. In general, you can choose individual predictors
    using the `+` symbol, or you can specify to use *all* predictors using the `.`
    symbol.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用一个方便的缩写语法，使用点号，`Class ~ .`，来表示我们想要在模型中使用除`Class`之外的所有变量作为预测因子。在这个特定的设置中，由于`Concavity`和`Perimeter`是`cancer_train`数据框中唯一的两个预测因子，`Class
    ~ Perimeter + Concavity`和`Class ~ .`是等价的。一般来说，你可以使用`+`符号选择单个预测因子，或者你可以使用`.`符号指定使用**所有**预测因子。
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here you can see the final trained model summary. It confirms that the computational
    engine used to train the model was `kknn::train.kknn`. It also shows the fraction
    of errors made by the K-nearest neighbors model, but we will ignore this for now
    and discuss it in more detail in the next chapter. Finally, it shows (somewhat
    confusingly) that the “best” weight function was “rectangular” and “best” setting
    of \(K\) was 5; but since we specified these earlier, R is just repeating those
    settings to us here. In the next chapter, we will actually let R find the value
    of \(K\) for us.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到最终的训练模型摘要。它确认用于训练模型的计算引擎是`kknn::train.kknn`。它还显示了K最近邻模型犯错的比率，但我们现在将忽略它，并在下一章中更详细地讨论。最后，它（有些令人困惑地）显示“最佳”权重函数是“矩形”和“最佳”的\(K\)设置是5；但因为我们之前已经指定了这些，所以R只是在这里重复那些设置。在下一章中，我们将实际上让R为我们找到\(K\)的值。
- en: Finally, we make the prediction on the new observation by calling the `predict`
    function, passing both the fit object we just created and the new observation
    itself. As above, when we ran the K-nearest neighbors classification algorithm
    manually, the `knn_fit` object classifies the new observation as malignant. Note
    that the `predict` function outputs a data frame with a single variable named
    `.pred_class`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过调用`predict`函数，传递我们刚刚创建的拟合对象以及新的观察本身，对新观察进行预测。如上所述，当我们手动运行K最近邻分类算法时，`knn_fit`对象将新观察分类为恶性。请注意，`predict`函数输出一个包含单个变量名为`.pred_class`的数据框。
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Is this predicted malignant label the actual class for this observation? Well,
    we don’t know because we do not have this observation’s diagnosis— that is what
    we were trying to predict! The classifier’s prediction is not necessarily correct,
    but in the next chapter, we will learn ways to quantify how accurate we think
    our predictions are.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预测的恶性标签是否是这一观察的实际类别？嗯，我们不知道，因为我们没有这个观察的诊断——这正是我们试图预测的内容！分类器的预测并不一定正确，但在下一章中，我们将学习如何量化我们认为我们的预测有多准确。
- en: 5.7 Data preprocessing with `tidymodels`
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 使用`tidymodels`进行数据预处理
- en: 5.7.1 Centering and scaling
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7.1 中心化和缩放
- en: When using K-nearest neighbors classification, the *scale* of each variable
    (i.e., its size and range of values) matters. Since the classifier predicts classes
    by identifying observations nearest to it, any variables with a large scale will
    have a much larger effect than variables with a small scale. But just because
    a variable has a large scale *doesn’t mean* that it is more important for making
    accurate predictions. For example, suppose you have a data set with two features,
    salary (in dollars) and years of education, and you want to predict the corresponding
    type of job. When we compute the neighbor distances, a difference of $1000 is
    huge compared to a difference of 10 years of education. But for our conceptual
    understanding and answering of the problem, it’s the opposite; 10 years of education
    is huge compared to a difference of $1000 in yearly salary!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用K最近邻分类时，每个变量的**尺度**（即其大小和值域）很重要。由于分类器通过识别最近的观察来预测类别，任何具有大尺度的变量将比具有小尺度的变量有更大的影响。但仅仅因为一个变量具有大尺度**并不意味着**它在做出准确预测时更重要。例如，假设你有一个包含两个特征的数据集，工资（以美元计）和受教育年限，你想预测相应的职业类型。当我们计算邻近距离时，1000美元的差异与10年教育年限的差异相比是巨大的。但对我们概念理解和解决问题的回答来说，情况正好相反；10年的教育年限与年薪1000美元的差异相比是巨大的！
- en: In many other predictive models, the *center* of each variable (e.g., its mean)
    matters as well. For example, if we had a data set with a temperature variable
    measured in degrees Kelvin, and the same data set with temperature measured in
    degrees Celsius, the two variables would differ by a constant shift of 273 (even
    though they contain exactly the same information). Likewise, in our hypothetical
    job classification example, we would likely see that the center of the salary
    variable is in the tens of thousands, while the center of the years of education
    variable is in the single digits. Although this doesn’t affect the K-nearest neighbors
    classification algorithm, this large shift can change the outcome of using many
    other predictive models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多其他预测模型中，每个变量的*中心*（例如，其均值）也很重要。例如，如果我们有一个以开尔文度测量的温度变量，以及同一个数据集以摄氏度测量的温度，这两个变量将相差一个常数偏移量273（尽管它们包含的信息完全相同）。同样，在我们的假设职业分类示例中，我们可能会看到工资变量的中心在数万之间，而教育年数的中心在个位数。尽管这不会影响K最近邻分类算法，但这种大的偏移量可能会改变使用许多其他预测模型的结果。
- en: 'To scale and center our data, we need to find our variables’ *mean* (the average,
    which quantifies the “central” value of a set of numbers) and *standard deviation*
    (a number quantifying how spread out values are). For each observed value of the
    variable, we subtract the mean (i.e., center the variable) and divide by the standard
    deviation (i.e., scale the variable). When we do this, the data is said to be
    *standardized*, and all variables in a data set will have a mean of 0 and a standard
    deviation of 1\. To illustrate the effect that standardization can have on the
    K-nearest neighbors algorithm, we will read in the original, unstandardized Wisconsin
    breast cancer data set; we have been using a standardized version of the data
    set up until now. As before, we will convert the `Class` variable to the factor
    type and rename the values to “Malignant” and “Benign.” To keep things simple,
    we will just use the `Area`, `Smoothness`, and `Class` variables:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缩放和居中我们的数据，我们需要找到变量的*均值*（平均数，它量化了一组数字的“中心”值）和*标准差*（一个量化值分布情况的数字）。对于变量的每个观测值，我们减去均值（即居中变量）并除以标准差（即缩放变量）。当我们这样做时，数据就被说成是*标准化*的，并且数据集中的所有变量都将具有均值为0和标准差为1。为了说明标准化对K最近邻算法的影响，我们将读取原始的、未标准化的威斯康星州乳腺癌数据集；直到现在，我们一直在使用该数据集的标准化版本。和之前一样，我们将`Class`变量转换为因子类型，并将值重命名为“Malignant”和“Benign”。为了简化问题，我们只使用`Area`、`Smoothness`和`Class`变量：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Looking at the unscaled and uncentered data above, you can see that the differences
    between the values for area measurements are much larger than those for smoothness.
    Will this affect predictions? In order to find out, we will create a scatter plot
    of these two predictors (colored by diagnosis) for both the unstandardized data
    we just loaded, and the standardized version of that same data. But first, we
    need to standardize the `unscaled_cancer` data set with `tidymodels`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 观察上面未缩放和未居中的数据，你可以看到面积测量值之间的差异远大于平滑度测量值之间的差异。这会影响预测结果吗？为了找出答案，我们将为这两种预测因子（按诊断结果着色）创建散点图，包括我们刚刚加载的未标准化数据和该数据的标准化版本。但首先，我们需要使用`tidymodels`对`unscaled_cancer`数据集进行标准化。
- en: 'In the `tidymodels` framework, all data preprocessing happens using a `recipe`
    from [the `recipes` R package](https://recipes.tidymodels.org/) ([Kuhn and Wickham
    2021](#ref-recipes)). Here we will initialize a recipe for the `unscaled_cancer`
    data above, specifying that the `Class` variable is the response, and all other
    variables are predictors:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在`tidymodels`框架中，所有数据预处理都使用来自[`recipes` R包](https://recipes.tidymodels.org/)（[Kuhn
    and Wickham 2021](#ref-recipes)）的`recipe`。在这里，我们将初始化一个针对上面提到的`unscaled_cancer`数据集的recipe，指定`Class`变量是响应变量，所有其他变量都是预测变量：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'So far, there is not much in the recipe; just a statement about the number
    of response variables and predictors. Let’s add scaling (`step_scale`) and centering
    (`step_center`) steps for all of the predictors so that they each have a mean
    of 0 and standard deviation of 1. Note that `tidyverse` actually provides `step_normalize`,
    which does both centering and scaling in a single recipe step; in this book we
    will keep `step_scale` and `step_center` separate to emphasize conceptually that
    there are two steps happening. The `prep` function finalizes the recipe by using
    the data (here, `unscaled_cancer`) to compute anything necessary to run the recipe
    (in this case, the column means and standard deviations):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，配方中还没有太多内容；只是关于响应变量和预测变量的数量声明。让我们为所有预测变量添加缩放（`step_scale`）和居中（`step_center`）步骤，以便它们各自具有均值为
    0 和标准差为 1。请注意，`tidyverse` 实际上提供了 `step_normalize`，它可以在单个配方步骤中同时进行居中和缩放；在这本书中，我们将保持
    `step_scale` 和 `step_center` 分开，以强调概念上存在两个步骤。`prep` 函数通过使用数据（在这里，`unscaled_cancer`）来计算运行配方所需的所有必要内容（在这种情况下，列均值和标准差）来最终确定配方：
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can now see that the recipe includes a scaling and centering step for all
    predictor variables. Note that when you add a step to a recipe, you must specify
    what columns to apply the step to. Here we used the `all_predictors()` function
    to specify that each step should be applied to all predictor variables. However,
    there are a number of different arguments one could use here, as well as naming
    particular columns with the same syntax as the `select` function. For example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以看到配方包括对所有预测变量的缩放和居中步骤。请注意，当你向配方添加步骤时，你必须指定要应用步骤的列。在这里，我们使用了 `all_predictors()`
    函数来指定每个步骤应应用于所有预测变量。然而，这里可以使用许多不同的参数，以及使用与 `select` 函数相同的语法命名特定的列。例如：
- en: '`all_nominal()` and `all_numeric()`: specify all categorical or all numeric
    variables'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all_nominal()` 和 `all_numeric()`: 指定所有分类变量或所有数值变量'
- en: '`all_predictors()` and `all_outcomes()`: specify all predictor or all response
    variables'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all_predictors()` 和 `all_outcomes()`: 指定所有预测变量或所有响应变量'
- en: '`Area, Smoothness`: specify both the `Area` and `Smoothness` variable'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`面积, 平滑度`: 指定 `面积` 和 `平滑度` 变量'
- en: '`-Class`: specify everything except the `Class` variable'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-类别`: 指定除 `类别` 变量之外的所有内容'
- en: You can find a full set of all the steps and variable selection functions on
    the [`recipes` reference page](https://recipes.tidymodels.org/reference/index.html).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 `recipes` 参考页面上找到所有步骤和变量选择函数的完整集合（[recipes reference page](https://recipes.tidymodels.org/reference/index.html)）。
- en: At this point, we have calculated the required statistics based on the data
    input into the recipe, but the data are not yet scaled and centered. To actually
    scale and center the data, we need to apply the `bake` function to the unscaled
    data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经根据输入到配方中的数据计算了所需的统计量，但数据尚未进行缩放和居中。要实际缩放和居中数据，我们需要将 `bake` 函数应用于未缩放的数据。
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: It may seem redundant that we had to both `bake` *and* `prep` to scale and center
    the data. However, we do this in two steps so we can specify a different data
    set in the `bake` step if we want. For example, we may want to specify new data
    that were not part of the training set.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须同时进行 `bake` 和 `prep` 来缩放和居中数据，这看起来可能有些多余。然而，我们分两步进行这样做，以便如果我们想的话，可以在 `bake`
    步骤中指定不同的数据集。例如，我们可能想要指定不属于训练集的新数据。
- en: You may wonder why we are doing so much work just to center and scale our variables.
    Can’t we just manually scale and center the `Area` and `Smoothness` variables
    ourselves before building our K-nearest neighbors model? Well, technically *yes*;
    but doing so is error-prone. In particular, we might accidentally forget to apply
    the same centering / scaling when making predictions, or accidentally apply a
    *different* centering / scaling than what we used while training. Proper use of
    a `recipe` helps keep our code simple, readable, and error-free. Furthermore,
    note that using `prep` and `bake` is required only when you want to inspect the
    result of the preprocessing steps yourself. You will see further on in Section
    [5.8](classification1.html#puttingittogetherworkflow) that `tidymodels` provides
    tools to automatically apply `prep` and `bake` as necessary without additional
    coding effort.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道我们为什么要做这么多工作来对变量进行居中和缩放。我们难道不能在构建 K 近邻模型之前手动对 `Area` 和 `Smoothness` 变量进行缩放和居中吗？技术上*是的*；但这样做是容易出错的。特别是，我们可能会在做出预测时意外忘记应用相同的居中和缩放，或者意外应用与我们训练时使用的不同的居中和缩放。正确使用
    `recipe` 有助于保持我们的代码简单、可读且无错误。此外，请注意，只有在你想亲自检查预处理步骤的结果时才需要使用 `prep` 和 `bake`。你将在第
    [5.8](classification1.html#puttingittogetherworkflow) 节中进一步看到，`tidymodels` 提供了工具，可以在无需额外编码努力的情况下自动应用
    `prep` 和 `bake`。
- en: Figure [5.9](classification1.html#fig:05-scaling-plt) shows the two scatter
    plots side-by-side—one for `unscaled_cancer` and one for `scaled_cancer`. Each
    has the same new observation annotated with its \(K=3\) nearest neighbors. In
    the original unstandardized data plot, you can see some odd choices for the three
    nearest neighbors. In particular, the “neighbors” are visually well within the
    cloud of benign observations, and the neighbors are all nearly vertically aligned
    with the new observation (which is why it looks like there is only one black line
    on this plot). Figure [5.10](classification1.html#fig:05-scaling-plt-zoomed) shows
    a close-up of that region on the unstandardized plot. Here the computation of
    nearest neighbors is dominated by the much larger-scale area variable. The plot
    for standardized data on the right in Figure [5.9](classification1.html#fig:05-scaling-plt)
    shows a much more intuitively reasonable selection of nearest neighbors. Thus,
    standardizing the data can change things in an important way when we are using
    predictive algorithms. Standardizing your data should be a part of the preprocessing
    you do before predictive modeling and you should always think carefully about
    your problem domain and whether you need to standardize your data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5.9](classification1.html#fig:05-scaling-plt) 展示了两个并排的散点图——一个用于 `unscaled_cancer`，另一个用于
    `scaled_cancer`。每个图都有用其 \(K=3\) 个最近邻标注的新观测值。在原始未标准化数据图中，你可以看到三个最近邻的选择有些奇怪。特别是，“邻居”在良性观测值的云团中视觉上非常接近，并且邻居都与新观测值几乎垂直对齐（这就是为什么在这个图表上看起来只有一条黑色线的原因）。图
    [5.10](classification1.html#fig:05-scaling-plt-zoomed) 展示了未标准化图上该区域的特写。在这里，最近邻的计算主要由更大的规模变量域主导。图
    [5.9](classification1.html#fig:05-scaling-plt) 中右侧的标准化数据图显示了最近邻选择的更直观合理的选取。因此，当我们使用预测算法时，标准化数据可以以重要方式改变事物。在预测建模之前，你应该将数据标准化作为预处理的一部分，并且你应该始终仔细思考你的问题域以及你是否需要标准化你的数据。
- en: '![Comparison of K = 3 nearest neighbors with unstandardized and standardized
    data.](../Images/0bbb31b01bd86fe4dccea552df73f16f.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![未标准化和标准化数据中 K = 3 个最近邻的比较](../Images/0bbb31b01bd86fe4dccea552df73f16f.png)'
- en: 'Figure 5.9: Comparison of K = 3 nearest neighbors with unstandardized and standardized
    data. ![Close-up of three nearest neighbors for unstandardized data.](../Images/7e5225f9ea2af6b28d24451df49a3733.png)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：未标准化和标准化数据中 K = 3 个最近邻的比较。![未标准化数据中三个最近邻的特写](../Images/7e5225f9ea2af6b28d24451df49a3733.png)
- en: 'Figure 5.10: Close-up of three nearest neighbors for unstandardized data.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：未标准化数据中三个最近邻的特写。
- en: 5.7.2 Balancing
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7.2 平衡
- en: 'Another potential issue in a data set for a classifier is *class imbalance*,
    i.e., when one label is much more common than another. Since classifiers like
    the K-nearest neighbors algorithm use the labels of nearby points to predict the
    label of a new point, if there are many more data points with one label overall,
    the algorithm is more likely to pick that label in general (even if the “pattern”
    of data suggests otherwise). Class imbalance is actually quite a common and important
    problem: from rare disease diagnosis to malicious email detection, there are many
    cases in which the “important” class to identify (presence of disease, malicious
    email) is much rarer than the “unimportant” class (no disease, normal email).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类器的数据集，另一个潜在问题是**类别不平衡**，即当一个标签比另一个标签更常见时。由于像K最近邻算法这样的分类器使用附近点的标签来预测新点的标签，如果整体上具有一个标签的数据点比另一个标签多得多，那么算法更有可能在一般情况下选择那个标签（即使数据“模式”表明否则）。类别不平衡实际上是一个相当常见且重要的问题：从罕见疾病的诊断到恶意电子邮件检测，有许多情况中，“重要”的类别（疾病的存在，恶意电子邮件）比“不重要”的类别（无疾病，正常电子邮件）要少得多。
- en: 'To better illustrate the problem, let’s revisit the scaled breast cancer data,
    `cancer`; except now we will remove many of the observations of malignant tumors,
    simulating what the data would look like if the cancer was rare. We will do this
    by picking only 3 observations from the malignant group, and keeping all of the
    benign observations. We choose these 3 observations using the `slice_head` function,
    which takes two arguments: a data frame-like object, and the number of rows to
    select from the top (`n`). We will use the `bind_rows` function to glue the two
    resulting filtered data frames back together, and name the result `rare_cancer`.
    The new imbalanced data is shown in Figure [5.11](classification1.html#fig:05-unbalanced).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明问题，让我们重新审视缩放后的乳腺癌数据，`cancer`；但现在我们将删除许多恶性肿瘤的观察数据，模拟如果癌症很少见的数据看起来会是什么样子。我们将通过只从恶性组中选择3个观察数据，并保留所有良性观察数据来实现这一点。我们使用`slice_head`函数选择这些3个观察数据，该函数接受两个参数：一个类似数据框的对象，以及从顶部选择行数（`n`）。我们将使用`bind_rows`函数将两个结果过滤后的数据框粘合在一起，并将结果命名为`rare_cancer`。新的不平衡数据在图
    [5.11](classification1.html#fig:05-unbalanced) 中显示。
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![Imbalanced data.](../Images/b6af287e8f6c32ef18325d5a3114446d.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![不平衡数据。](../Images/b6af287e8f6c32ef18325d5a3114446d.png)'
- en: 'Figure 5.11: Imbalanced data.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：不平衡数据。
- en: Suppose we now decided to use \(K = 7\) in K-nearest neighbors classification.
    With only 3 observations of malignant tumors, the classifier will *always predict
    that the tumor is benign, no matter what its concavity and perimeter are!* This
    is because in a majority vote of 7 observations, at most 3 will be malignant (we
    only have 3 total malignant observations), so at least 4 must be benign, and the
    benign vote will always win. For example, Figure [5.12](classification1.html#fig:05-upsample)
    shows what happens for a new tumor observation that is quite close to three observations
    in the training data that were tagged as malignant.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们现在决定在K最近邻分类中使用 \(K = 7\)。由于只有3个恶性肿瘤的观察数据，分类器将**总是预测肿瘤为良性，无论其凹度和周长如何**！这是因为在一个由7个观察数据组成的多数投票中，最多只有3个是恶性的（我们总共只有3个恶性观察数据），所以至少有4个必须是良性的，良性投票总是获胜。例如，图
    [5.12](classification1.html#fig:05-upsample) 展示了对于一个新的肿瘤观察数据，它非常接近训练数据中标记为恶性的三个观察数据。
- en: '![Imbalanced data with 7 nearest neighbors to a new observation highlighted.](../Images/10101c31a0bb61e65f499a8f401f3566.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![不平衡数据，突出显示新观察点附近的7个最近邻。](../Images/10101c31a0bb61e65f499a8f401f3566.png)'
- en: 'Figure 5.12: Imbalanced data with 7 nearest neighbors to a new observation
    highlighted.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：不平衡数据，突出显示新观察点附近的7个最近邻。
- en: Figure [5.13](classification1.html#fig:05-upsample-2) shows what happens if
    we set the background color of each area of the plot to the prediction the K-nearest
    neighbors classifier would make for a new observation at that location. We can
    see that the decision is always “benign,” corresponding to the blue color.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5.13](classification1.html#fig:05-upsample-2) 展示了如果我们将图中每个区域的背景颜色设置为K最近邻分类器对新观察点的预测，会发生什么。我们可以看到决策总是“良性”，对应蓝色。
- en: '![Imbalanced data with background color indicating the decision of the classifier
    and the points represent the labeled data.](../Images/9b168e7fa6d9b2c6170071ac4e50379f.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![不平衡数据，背景颜色表示分类器的决策，点表示标记的数据。](../Images/9b168e7fa6d9b2c6170071ac4e50379f.png)'
- en: 'Figure 5.13: Imbalanced data with background color indicating the decision
    of the classifier and the points represent the labeled data.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：不平衡数据，背景颜色表示分类器的决策，点代表标记的数据。
- en: 'Despite the simplicity of the problem, solving it in a statistically sound
    manner is actually fairly nuanced, and a careful treatment would require a lot
    more detail and mathematics than we will cover in this textbook. For the present
    purposes, it will suffice to rebalance the data by *oversampling* the rare class.
    In other words, we will replicate rare observations multiple times in our data
    set to give them more voting power in the K-nearest neighbors algorithm. In order
    to do this, we will add an oversampling step to the earlier `uc_recipe` recipe
    with the `step_upsample` function from the `themis` R package. We show below how
    to do this, and also use the `group_by` and `summarize` functions to see that
    our classes are now balanced:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管问题简单，但以统计上可靠的方式解决它实际上相当复杂，并且需要比我们在本教材中涵盖的更多细节和数学。就目前目的而言，通过*过采样*稀有类别来重新平衡数据就足够了。换句话说，我们将多次复制数据集中的稀有观测，以在K最近邻算法中赋予它们更多的投票权。为了做到这一点，我们将向早期的`uc_recipe`食谱中添加一个过采样步骤，使用来自`themis`R包的`step_upsample`函数。我们下面将展示如何做到这一点，并使用`group_by`和`summarize`函数来查看我们的类别现在是否平衡：
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now suppose we train our K-nearest neighbors classifier with \(K=7\) on this
    *balanced* data. Figure [5.14](classification1.html#fig:05-upsample-plot) shows
    what happens now when we set the background color of each area of our scatter
    plot to the decision the K-nearest neighbors classifier would make. We can see
    that the decision is more reasonable; when the points are close to those labeled
    malignant, the classifier predicts a malignant tumor, and vice versa when they
    are closer to the benign tumor observations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们使用K=7在这个*平衡*数据上训练我们的K最近邻分类器。图[5.14](classification1.html#fig:05-upsample-plot)显示了现在当我们将散点图每个区域的背景颜色设置为K最近邻分类器将做出的决策时会发生什么。我们可以看到决策更加合理；当点接近标记为恶性的点时，分类器预测为恶性肿瘤，反之亦然，当它们更接近良性肿瘤观测点时。
- en: '![Upsampled data with background color indicating the decision of the classifier.](../Images/25e2d05f94566ae5386f5399610e07a1.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![通过背景颜色表示分类器决策的上采样数据。](../Images/25e2d05f94566ae5386f5399610e07a1.png)'
- en: 'Figure 5.14: Upsampled data with background color indicating the decision of
    the classifier.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14：通过背景颜色表示分类器决策的上采样数据。
- en: 5.7.3 Missing data
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7.3 缺失数据
- en: 'One of the most common issues in real data sets in the wild is *missing data*,
    i.e., observations where the values of some of the variables were not recorded.
    Unfortunately, as common as it is, handling missing data properly is very challenging
    and generally relies on expert knowledge about the data, setting, and how the
    data were collected. One typical challenge with missing data is that missing entries
    can be *informative*: the very fact that an entries were missing is related to
    the values of other variables. For example, survey participants from a marginalized
    group of people may be less likely to respond to certain kinds of questions if
    they fear that answering honestly will come with negative consequences. In that
    case, if we were to simply throw away data with missing entries, we would bias
    the conclusions of the survey by inadvertently removing many members of that group
    of respondents. So ignoring this issue in real problems can easily lead to misleading
    analyses, with detrimental impacts. In this book, we will cover only those techniques
    for dealing with missing entries in situations where missing entries are just
    “randomly missing”, i.e., where the fact that certain entries are missing *isn’t
    related to anything else* about the observation.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在野外真实数据集中最常见的问题之一是*缺失数据*，即某些变量的值未被记录的观测。不幸的是，尽管它很常见，但正确处理缺失数据非常具有挑战性，通常依赖于对数据、设置以及数据收集方式的专家知识。处理缺失数据的一个典型挑战是缺失条目可能包含*信息*：一个条目缺失的事实与其它变量的值相关。例如，来自边缘化群体的调查参与者可能不太可能回答某些类型的提问，如果他们担心诚实地回答会带来负面后果。在这种情况下，如果我们简单地丢弃包含缺失条目的数据，我们就会通过无意中移除许多该群体受访者而使调查的结论产生偏差。因此，在现实问题中忽视这个问题很容易导致误导性的分析，产生有害的影响。在这本书中，我们将仅介绍在缺失条目只是“随机缺失”的情况下处理缺失条目的技术，即缺失条目的事实*与观察的其他任何方面无关*。
- en: 'Let’s load and examine a modified subset of the tumor image data that has a
    few missing entries:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载并检查一个经过修改的肿瘤图像数据子集，该子集中有几个缺失条目：
- en: '[PRE41]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Recall that K-nearest neighbors classification makes predictions by computing
    the straight-line distance to nearby training observations, and hence requires
    access to the values of *all* variables for *all* observations in the training
    data. So how can we perform K-nearest neighbors classification in the presence
    of missing data? Well, since there are not too many observations with missing
    entries, one option is to simply remove those observations prior to building the
    K-nearest neighbors classifier. We can accomplish this by using the `drop_na`
    function from `tidyverse` prior to working with the data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，K-最近邻分类通过计算到附近训练观测值的直线距离来进行预测，因此需要访问训练数据中所有观测值的*所有*变量的值。那么，在存在缺失数据的情况下，我们如何执行K-最近邻分类呢？嗯，由于缺失条目的观测值并不多，一个选项是在构建K-最近邻分类器之前简单地删除这些观测值。我们可以通过在处理数据之前使用`tidyverse`中的`drop_na`函数来完成这项工作。
- en: '[PRE43]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: However, this strategy will not work when many of the rows have missing entries,
    as we may end up throwing away too much data. In this case, another possible approach
    is to *impute* the missing entries, i.e., fill in synthetic values based on the
    other observations in the data set. One reasonable choice is to perform *mean
    imputation*, where missing entries are filled in using the mean of the present
    entries in each variable. To perform mean imputation, we add the `step_impute_mean`
    step to the `tidymodels` preprocessing recipe.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当许多行存在缺失条目时，这种策略将不会奏效，因为我们可能会丢弃过多的数据。在这种情况下，另一种可能的方法是对缺失条目进行*插补*，即根据数据集中其他观测值填充合成值。一个合理的选择是执行*均值插补*，即使用每个变量现有条目的平均值来填充缺失条目。要执行均值插补，我们需要将`step_impute_mean`步骤添加到`tidymodels`预处理配方中。
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: To visualize what mean imputation does, let’s just apply the recipe directly
    to the `missing_cancer` data frame using the `bake` function. The imputation step
    fills in the missing entries with the mean values of their corresponding variables.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化均值插补的效果，让我们直接使用`bake`函数将配方应用于`missing_cancer`数据框。插补步骤使用相应变量的平均值填充缺失条目。
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Many other options for missing data imputation can be found in [the `recipes`
    documentation](https://recipes.tidymodels.org/reference/index.html). However you
    decide to handle missing data in your data analysis, it is always crucial to think
    critically about the setting, how the data were collected, and the question you
    are answering.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在[recipes文档](https://recipes.tidymodels.org/reference/index.html)中可以找到许多其他缺失数据插补的选项。然而，无论你决定如何处理数据分析中的缺失数据，始终都要批判性地思考设置、数据的收集方式以及你正在回答的问题。
- en: 5.8 Putting it together in a `workflow`
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 在`workflow`中组合
- en: 'The `tidymodels` package collection also provides the `workflow`, a way to
    chain together multiple data analysis steps without a lot of otherwise necessary
    code for intermediate steps. To illustrate the whole pipeline, let’s start from
    scratch with the `wdbc_unscaled.csv` data. First we will load the data, create
    a model, and specify a recipe for how the data should be preprocessed:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`tidymodels`包集合还提供了`workflow`，这是一种将多个数据分析步骤链接在一起的方法，而不需要大量其他中间步骤的代码。为了说明整个流程，让我们从`wdbc_unscaled.csv`数据从头开始。首先，我们将加载数据，创建一个模型，并指定一个配方来定义数据应该如何预处理：'
- en: '[PRE49]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Note that each of these steps is exactly the same as earlier, except for one
    major difference: we did not use the `select` function to extract the relevant
    variables from the data frame, and instead simply specified the relevant variables
    to use via the formula `Class ~ Area + Smoothness` (instead of `Class ~ .`) in
    the recipe. You will also notice that we did not call `prep()` on the recipe;
    this is unnecessary when it is placed in a workflow.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些步骤与之前完全相同，只有一个主要区别：我们没有使用`select`函数从数据框中提取相关变量，而是通过配方中的公式`Class ~ Area
    + Smoothness`（而不是`Class ~ .`）简单地指定了要使用的相关变量。你也会注意到我们没有在配方上调用`prep()`；当它放在工作流程中时，这是不必要的。
- en: 'We will now place these steps in a `workflow` using the `add_recipe` and `add_model`
    functions, and finally we will use the `fit` function to run the whole workflow
    on the `unscaled_cancer` data. Note another difference from earlier here: we do
    not include a formula in the `fit` function. This is again because we included
    the formula in the recipe, so there is no need to respecify it:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用`add_recipe`和`add_model`函数将这些步骤放入一个`工作流程`中，最后我们将使用`fit`函数在`unscaled_cancer`数据上运行整个工作流程。注意这里与之前的一个不同点：我们在`fit`函数中不包括公式。这同样是因为我们在配方中已经包含了公式，所以没有必要重新指定它：
- en: '[PRE50]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'As before, the fit object lists the function that trains the model as well
    as the “best” settings for the number of neighbors and weight function (for now,
    these are just the values we chose manually when we created `knn_spec` above).
    But now the fit object also includes information about the overall workflow, including
    the centering and scaling preprocessing steps. In other words, when we use the
    `predict` function with the `knn_fit` object to make a prediction for a new observation,
    it will first apply the same recipe steps to the new observation. As an example,
    we will predict the class label of two new observations: one with `Area = 500`
    and `Smoothness = 0.075`, and one with `Area = 1500` and `Smoothness = 0.1`.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，fit对象列出了训练模型的函数以及“最佳”的邻居数量和权重函数设置（目前，这些只是我们在创建`knn_spec`时手动选择的值）。但现在fit对象还包括有关整体工作流程的信息，包括中心化和缩放预处理步骤。换句话说，当我们使用`predict`函数与`knn_fit`对象对新观察结果进行预测时，它将首先将相同的配方步骤应用于新观察结果。作为一个例子，我们将预测两个新观察结果的类别标签：一个`Area
    = 500`和`Smoothness = 0.075`，另一个`Area = 1500`和`Smoothness = 0.1`。
- en: '[PRE52]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The classifier predicts that the first observation is benign, while the second
    is malignant. Figure [5.15](classification1.html#fig:05-workflow-plot-show) visualizes
    the predictions that this trained K-nearest neighbors model will make on a large
    range of new observations. Although you have seen colored prediction map visualizations
    like this a few times now, we have not included the code to generate them, as
    it is a little bit complicated. For the interested reader who wants a learning
    challenge, we now include it below. The basic idea is to create a grid of synthetic
    new observations using the `expand.grid` function, predict the label of each,
    and visualize the predictions with a colored scatter having a very high transparency
    (low `alpha` value) and large point radius. See if you can figure out what each
    line is doing!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器预测第一个观察结果是良性的，而第二个是恶性的。图[5.15](classification1.html#fig:05-workflow-plot-show)展示了这个训练好的K最近邻模型将在大量新观察结果上做出的预测。尽管你现在已经看到过几次这样的彩色预测地图可视化，但我们没有包括生成它们的代码，因为这有点复杂。对于想要一个学习挑战的读者，我们现在将其包含在下面。基本思路是使用`expand.grid`函数创建一个合成新观察值的网格，预测每个的标签，并使用具有非常高的透明度（低`alpha`值）和大型点半径的彩色散点图来可视化预测。看看你是否能弄清楚每一行的作用！
- en: '**Note:** Understanding this code is not required for the remainder of the
    textbook. It is included for those readers who would like to use similar visualizations
    in their own data analyses.'
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**理解此代码对于本教科书其余部分不是必需的。它包含在此处是为了那些想要在自己的数据分析中使用类似可视化的读者。'
- en: '[PRE54]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![Scatter plot of smoothness versus area where background color indicates the
    decision of the classifier.](../Images/7f5ab8c71dc3cc4be17e13b54391d8ab.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![平滑度与面积散点图，其中背景颜色表示分类器的决策。](../Images/7f5ab8c71dc3cc4be17e13b54391d8ab.png)'
- en: 'Figure 5.15: Scatter plot of smoothness versus area where background color
    indicates the decision of the classifier.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：平滑度与面积散点图，其中背景颜色表示分类器的决策。
- en: 5.9 Exercises
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.9 练习
- en: 'Practice exercises for the material covered in this chapter can be found in
    the accompanying [worksheets repository](https://worksheets.datasciencebook.ca)
    in the “Classification I: training and predicting” row. You can launch an interactive
    version of the worksheet in your browser by clicking the “launch binder” button.
    You can also preview a non-interactive version of the worksheet by clicking “view
    worksheet.” If you instead decide to download the worksheet and run it on your
    own machine, make sure to follow the instructions for computer setup found in
    Chapter [13](setup.html#setup). This will ensure that the automated feedback and
    guidance that the worksheets provide will function as intended.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的练习材料可以在“分类 I：训练和预测”行中的配套[工作表存储库](https://worksheets.datasciencebook.ca)中找到。您可以通过点击“启动绑定器”按钮在浏览器中启动工作表的交互式版本。您还可以通过点击“查看工作表”预览非交互式版本的工作表。如果您决定下载工作表并在自己的机器上运行，请确保遵循第[13](setup.html#setup)章中找到的计算机设置说明。这将确保工作表提供的自动反馈和指导能够按预期工作。
- en: References
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Cover, Thomas, and Peter Hart. 1967\. “Nearest Neighbor Pattern Classification.”
    *IEEE Transactions on Information Theory* 13 (1): 21–27.Fix, Evelyn, and Joseph
    Hodges. 1951\. “Discriminatory Analysis. Nonparametric Discrimination: Consistency
    Properties.” USAF School of Aviation Medicine, Randolph Field, Texas.Kuhn, Max,
    and David Vaughan. 2021\. *parsnip R package*. [https://parsnip.tidymodels.org/](https://parsnip.tidymodels.org/).Kuhn,
    Max, and Hadley Wickham. 2021\. *recipes R package*. [https://recipes.tidymodels.org/](https://recipes.tidymodels.org/).Stanford
    Health Care. 2021\. “What Is Cancer?” [https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html](https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html).Street,
    William Nick, William Wolberg, and Olvi Mangasarian. 1993\. “Nuclear Feature Extraction
    for Breast Tumor Diagnosis.” In *International Symposium on Electronic Imaging:
    Science and Technology*.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cover, Thomas, 和 Peter Hart. 1967. “最近邻模式分类。” *IEEE 信息系统传输* 13 (1): 21–27.
    Fix, Evelyn, 和 Joseph Hodges. 1951. “判别分析。非参数判别：一致性属性。” 美国空军医学院，德克萨斯州兰道夫空军基地。Kuhn,
    Max, 和 David Vaughan. 2021. *parsnip R 包*. [https://parsnip.tidymodels.org/](https://parsnip.tidymodels.org/).
    Kuhn, Max, 和 Hadley Wickham. 2021. *recipes R 包*. [https://recipes.tidymodels.org/](https://recipes.tidymodels.org/).
    Stanford Health Care. 2021. “什么是癌症？” [https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html](https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html).
    Street, William Nick, William Wolberg, 和 Olvi Mangasarian. 1993. “用于乳腺癌诊断的核特征提取。”
    在 *国际电子成像：科学和技术研讨会* 中。'
