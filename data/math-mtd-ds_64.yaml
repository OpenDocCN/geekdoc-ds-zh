- en: '8.3\. Building blocks of AI 1: backpropagation#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.3\. 人工智能构建块 1：反向传播#
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/03_backprop/roch-mmids-nn-backprop.html](https://mmids-textbook.github.io/chap08_nn/03_backprop/roch-mmids-nn-backprop.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap08_nn/03_backprop/roch-mmids-nn-backprop.html](https://mmids-textbook.github.io/chap08_nn/03_backprop/roch-mmids-nn-backprop.html)
- en: 'We develop the basic mathematical foundations of automatic differentiation.
    We restrict ourselves to a special setting: multi-layer progressive functions.
    Many important classifiers take the form of a sequence of compositions where parameters
    are specific to each layer of composition. We show how to systematically apply
    the *Chain Rule* to such functions. We also give a few examples.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发展了自动微分的基本数学基础。我们限制自己在一个特殊的环境中：多层递进函数。许多重要的分类器都采用序列组合的形式，其中参数对每个组合层都是特定的。我们展示了如何系统地应用**链式法则**到这样的函数上。我们还给出了一些例子。
- en: 8.3.1\. Forward v. backward[#](#forward-v-backward "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.1\. 前向与反向[#](#forward-v-backward "链接到本标题")
- en: 'We begin with a fixed-parameter example to illustrate the issues. Suppose \(f
    : \mathbb{R}^d \to \mathbb{R}\) can be expressed as a composition of \(L+1\) vector-valued
    functions \(\bfg_i : \mathbb{R}^{n_i} \to \mathbb{R}^{n_{i+1}}\) and a real-valued
    function \(\ell : \mathbb{R}^{n_{L+1}} \to \mathbb{R}\) as follows'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从一个固定参数的例子开始，以说明问题。假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 可以表示为 \(L+1\) 个向量值函数
    \(\bfg_i : \mathbb{R}^{n_i} \to \mathbb{R}^{n_{i+1}}\) 和一个实值函数 \(\ell : \mathbb{R}^{n_{L+1}}
    \to \mathbb{R}\) 的组合，如下所示'
- en: \[ f(\mathbf{x}) = \ell \circ \bfg_{L} \circ \bfg_{L-1} \circ \cdots \circ \bfg_1
    \circ \bfg_0(\mathbf{x}) = \ell(\bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))).
    \]
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}) = \ell \circ \bfg_{L} \circ \bfg_{L-1} \circ \cdots \circ \bfg_1
    \circ \bfg_0(\mathbf{x}) = \ell(\bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))).
    \]
- en: Here \(n_0 = d\) is the input dimension. We also let \(n_{L+1} = K\) be the
    output dimension. Think of
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 \(n_0 = d\) 是输入维度。我们还让 \(n_{L+1} = K\) 成为输出维度。考虑
- en: \[ h(\mathbf{x}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))
    \]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h(\mathbf{x}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))
    \]
- en: as a prediction function (i.e., a regression or classification function) and
    think of \(\ell\) as a loss function.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预测函数（即回归或分类函数），并将 \(\ell\) 视为损失函数。
- en: 'Observe first that the function \(f\) itself is straighforward to compute recursively
    *starting from the inside* as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先观察函数 \(f\) 本身可以通过以下方式递归地从内部开始计算，这是直截了当的：
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0)\\
    \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1)\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1})\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L})\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}). \end{align*}\]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0)\\
    \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1)\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1})\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L})\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}). \end{align*}\]
- en: Anticipating the setting of neural networks, our main application of interest,
    we refer to \(\mathbf{z}_0 = \mathbf{x}\) as the “input layer”, \(\hat{\mathbf{y}}
    = \mathbf{z}_{L+1} = \bfg_{L}(\mathbf{z}_{L})\) as the “output layer”, and \(\mathbf{z}_{1}
    = \bfg_0(\mathbf{z}_0), \ldots, \mathbf{z}_L = \bfg_{L-1}(\mathbf{z}_{L-1})\)
    as the “hidden layers”. In particular, \(L\) is the number of hidden layers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预见到神经网络的环境，我们感兴趣的主要应用，我们将 \(\mathbf{z}_0 = \mathbf{x}\) 称为“输入层”，\(\hat{\mathbf{y}}
    = \mathbf{z}_{L+1} = \bfg_{L}(\mathbf{z}_{L})\) 称为“输出层”，并将 \(\mathbf{z}_{1} =
    \bfg_0(\mathbf{z}_0), \ldots, \mathbf{z}_L = \bfg_{L-1}(\mathbf{z}_{L-1})\) 称为“隐藏层”。特别是，\(L\)
    是隐藏层的数量。
- en: '**EXAMPLE:** We will use the following running example throughout this subsection.
    We assume that each \(\bfg_i\) is a linear map, that is, \(\bfg_i(\mathbf{z}_i)
    = \mathcal{W}_{i} \mathbf{z}_i\) where \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1}
    \times n_i}\) is a fixed, known matrix. Assume also that \(\ell : \mathbb{R}^K
    \to \mathbb{R}\) is defined as'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 我们将在本小节中始终使用以下运行示例。我们假设每个 \(\bfg_i\) 是一个线性映射，即 \(\bfg_i(\mathbf{z}_i)
    = \mathcal{W}_{i} \mathbf{z}_i\)，其中 \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1}
    \times n_i}\) 是一个固定、已知的矩阵。还假设 \(\ell : \mathbb{R}^K \to \mathbb{R}\) 定义为'
- en: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
- en: for a fixed, known vector \(\mathbf{y} \in \mathbb{R}^{K}\).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个固定的、已知的向量 \(\mathbf{y} \in \mathbb{R}^{K}\)。
- en: Computing \(f\) recursively *starting from the inside* as above gives here
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以递归方式从内部开始计算 \(f\)，如上所示
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{W}_{0}
    \mathbf{z}_0 = \mathcal{W}_{0} \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{W}_{1} \mathbf{z}_1
    = \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{W}_{L-1}
    \mathbf{z}_{L-1} = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}) = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 =
    \frac{1}{2}\left\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x}\right\|^2. \end{align*}\]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{W}_{0}
    \mathbf{z}_0 = \mathcal{W}_{0} \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{W}_{1} \mathbf{z}_1
    = \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{W}_{L-1}
    \mathbf{z}_{L-1} = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}) = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 =
    \frac{1}{2}\left\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x}\right\|^2. \end{align*}\]
- en: In essence, we are comparing an observed outcome \(\mathbf{y}\) to a prediction
    \(\mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\)
    based on input \(\mathbf{x}\).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们是在将观察到的结果 \(\mathbf{y}\) 与基于输入 \(\mathbf{x}\) 的预测 \(\mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\) 进行比较。
- en: In this section, we look into computing the gradient with respect to \(\mathbf{x}\).
    (In reality, we will be more interested in taking the gradient with respect to
    the parameters, i.e., the entries of the matrices \(\mathcal{W}_{0}, \ldots, \mathcal{W}_{L}\),
    a task to which we will come back later in this section. We will also be interested
    in more complex – in particular, non-linear – prediction functions.) \(\lhd\)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨如何计算相对于 \(\mathbf{x}\) 的梯度。（实际上，我们更感兴趣的是计算相对于参数的梯度，即矩阵 \(\mathcal{W}_{0},
    \ldots, \mathcal{W}_{L}\) 的元素，这是一个我们将在本节后面再次讨论的任务。我们还将对更复杂的——特别是非线性的——预测函数感兴趣。）\(\lhd\)
- en: '**NUMERICAL CORNER:** To make things more concrete, we consider a specific
    example. We will use [`torch.linalg.vector_norm`](https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html)
    to compute the Euclidean norm in PyTorch. Suppose \(d=3\), \(L=1\), \(n_1 = 2\),
    and \(K = 2\) with the following choices:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**：为了使问题更具体，我们考虑一个特定的例子。我们将使用 `torch.linalg.vector_norm`（[链接](https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html)）在
    PyTorch 中计算欧几里得范数。假设 \(d=3\), \(L=1\), \(n_1 = 2\), 和 \(K = 2\)，我们有以下选择：'
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: \(\unlhd\)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Forward mode** \(\idx{forward mode}\xdi\) We are ready to apply the *Chain
    Rule*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**正向模式** \(\idx{forward mode}\xdi\) 我们准备应用**链式法则**'
- en: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\mathbf{z}_{L+1}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x})
    \]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\mathbf{z}_{L+1}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x})
    \]
- en: where the \(\mathbf{z}_{i}\)s are as above and we used that \(\mathbf{z}_0 =
    \mathbf{x}\). The matrix product here is well-defined. Indeed, the size of \(J_{g_i}(\mathbf{z}_i)\)
    is \(n_{i+1} \times n_{i}\) (i.e., number of outputs by number of inputs) while
    the size of \(J_{g_{i-1}}(\mathbf{z}_{i-1})\) is \(n_{i} \times n_{i-1}\) – so
    the dimensions are compatible.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{z}_{i}\) 如上所述，我们使用了 \(\mathbf{z}_0 = \mathbf{x}\)。这里的矩阵乘积是定义良好的。实际上，\(J_{g_i}(\mathbf{z}_i)\)
    的大小是 \(n_{i+1} \times n_{i}\)（即输出数量乘以输入数量），而 \(J_{g_{i-1}}(\mathbf{z}_{i-1})\)
    的大小是 \(n_{i} \times n_{i-1}\)——因此维度是兼容的。
- en: 'So it is straighforward to compute \(\nabla f(\mathbf{x})^T\) recursively as
    we did for \(f\) itself. In fact, we can compute both simultaneously. This is
    called the forward mode:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以像对 \(f\) 本身那样递归地计算 \(\nabla f(\mathbf{x})^T\)。实际上，我们可以同时计算这两个。这被称为正向模式：
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0),
    \quad F_0 := J_{\bfg_0}(\mathbf{z}_0)\\ \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1),
    \quad F_1 := J_{\bfg_1}(\mathbf{z}_1)\, F_0\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1}),
    \quad F_{L-1} := J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}), \quad F_{L} := J_{\bfg_{L}}(\mathbf{z}_{L})\,
    F_{L-1}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}}), \quad \nabla f(\mathbf{x})^T
    := J_{\ell}(\hat{\mathbf{y}}) F_L. \end{align*}\]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0),
    \quad F_0 := J_{\bfg_0}(\mathbf{z}_0)\\ \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1),
    \quad F_1 := J_{\bfg_1}(\mathbf{z}_1)\, F_0\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1}),
    \quad F_{L-1} := J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}), \quad F_{L} := J_{\bfg_{L}}(\mathbf{z}_{L})\,
    F_{L-1}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}}), \quad \nabla f(\mathbf{x})^T
    := J_{\ell}(\hat{\mathbf{y}}) F_L. \end{align*}\]
- en: '**EXAMPLE:** **(continued)** We apply this procedure to the running example.
    The Jacobian of the linear map \(\bfg_i(\mathbf{z}_i) = \mathcal{W}_{i} \mathbf{z}_i\)
    is the matrix \(\mathcal{W}_{i}\), as we have seen in a previous example. That
    is, \(J_{\bfg_i}(\mathbf{z}_i) = \mathcal{W}_{i}\) for any \(\mathbf{z}_i\). To
    compute the Jacobian of \(\ell\), we rewrite it as a quadratic function'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** **(继续)** 我们将此过程应用于正在运行的例子。线性映射\(\bfg_i(\mathbf{z}_i) = \mathcal{W}_{i}
    \mathbf{z}_i\)的雅可比矩阵是矩阵\(\mathcal{W}_{i}\)，正如我们之前所看到的。也就是说，对于任何\(\mathbf{z}_i\)，\(J_{\bfg_i}(\mathbf{z}_i)
    = \mathcal{W}_{i}\)。为了计算\(\ell\)的雅可比矩阵，我们将其重写为一个二次函数'
- en: \[\begin{align*} \ell(\hat{\mathbf{y}}) &= \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\
    &= \frac{1}{2} \mathbf{y}^T\mathbf{y} - \frac{1}{2} 2 \mathbf{y}^T\hat{\mathbf{y}}
    + \frac{1}{2}\hat{\mathbf{y}}^T \hat{\mathbf{y}}\\ &= \frac{1}{2} \hat{\mathbf{y}}^T
    I_{n_{L+1} \times n_{L+1}}\hat{\mathbf{y}} + (-\mathbf{y})^T\hat{\mathbf{y}} +
    \frac{1}{2} \mathbf{y}^T\mathbf{y}. \end{align*}\]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \ell(\hat{\mathbf{y}}) &= \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\
    &= \frac{1}{2} \mathbf{y}^T\mathbf{y} - \frac{1}{2} 2 \mathbf{y}^T\hat{\mathbf{y}}
    + \frac{1}{2}\hat{\mathbf{y}}^T \hat{\mathbf{y}}\\ &= \frac{1}{2} \hat{\mathbf{y}}^T
    I_{n_{L+1} \times n_{L+1}}\hat{\mathbf{y}} + (-\mathbf{y})^T\hat{\mathbf{y}} +
    \frac{1}{2} \mathbf{y}^T\mathbf{y}. \end{align*}\]
- en: From a previous example,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前的例子中，
- en: \[ J_\ell(\hat{\mathbf{y}})^T = \nabla \ell(\hat{\mathbf{y}}) = \frac{1}{2}\left[I_{n_{L+1}
    \times n_{L+1}} + I_{n_{L+1} \times n_{L+1}}^T\right]\, \hat{\mathbf{y}} + (-\mathbf{y})
    = \hat{\mathbf{y}} - \mathbf{y}. \]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_\ell(\hat{\mathbf{y}})^T = \nabla \ell(\hat{\mathbf{y}}) = \frac{1}{2}\left[I_{n_{L+1}
    \times n_{L+1}} + I_{n_{L+1} \times n_{L+1}}^T\right]\, \hat{\mathbf{y}} + (-\mathbf{y})
    = \hat{\mathbf{y}} - \mathbf{y}. \]
- en: Putting it all together, we get
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容综合起来，我们得到
- en: \[\begin{align*} F_0 &:= J_{\bfg_0}(\mathbf{z}_0) = \mathcal{W}_{0}\\ F_1 &:=
    J_{\bfg_1}(\mathbf{z}_1)\, F_0 = \mathcal{W}_{1} F_0 = \mathcal{W}_{1} \mathcal{W}_{0}\\
    \vdots\\ F_{L-1} &:= J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2} = \mathcal{W}_{L-1}
    F_{L-2}= \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ F_{L} &:=
    J_{\bfg_{L}}(\mathbf{z}_{L})\, F_{L-1} = \mathcal{W}_{L} F_{L-1} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ \nabla f(\mathbf{x})^T
    &:= J_{\ell}(\hat{\mathbf{y}}) F_L = (\hat{\mathbf{y}} - \mathbf{y})^T F_L = (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\
    &= (\mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}.
    \end{align*}\]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} F_0 &:= J_{\bfg_0}(\mathbf{z}_0) = \mathcal{W}_{0}\\ F_1 &:=
    J_{\bfg_1}(\mathbf{z}_1)\, F_0 = \mathcal{W}_{1} F_0 = \mathcal{W}_{1} \mathcal{W}_{0}\\
    \vdots\\ F_{L-1} &:= J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2} = \mathcal{W}_{L-1}
    F_{L-2}= \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ F_{L} &:=
    J_{\bfg_{L}}(\mathbf{z}_{L})\, F_{L-1} = \mathcal{W}_{L} F_{L-1} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ \nabla f(\mathbf{x})^T
    &:= J_{\ell}(\hat{\mathbf{y}}) F_L = (\hat{\mathbf{y}} - \mathbf{y})^T F_L = (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\
    &= (\mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}.
    \end{align*}\]
- en: \(\lhd\)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**NUMERICAL CORNER:** We return to our concrete example. Using `.T` to convert
    a column vector into a row vector throws an error in PyTorch, as it is meant to
    be used only on 2D tensors. Instead, one can use [`torch.unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html).
    Below, `(z2 - y).unsqueeze(0)` adds a dimension to `z2 - y`, making it a 2D tensor
    with shape \((1, 2)\).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们回到我们的具体例子。在PyTorch中使用`.T`将列向量转换为行向量会引发错误，因为它仅适用于2D张量。相反，可以使用`[`torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)`。下面，`(z2
    - y).unsqueeze(0)`给`z2 - y`添加了一个维度，使其成为一个形状为\((1, 2)\)的2D张量。'
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can check that we get the same outcome using AD.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用AD来验证我们得到相同的结果。
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: \(\unlhd\)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**KNOWLEDGE CHECK:** Obtain that last expression directly by taking the gradient
    of'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查：** 通过对以下表达式求梯度直接得到：'
- en: \[ f(\mathbf{x}) = \frac{1}{2}\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2. \]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}) = \frac{1}{2}\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2. \]
- en: \(\checkmark\)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: '**Reverse mode** \(\idx{reverse mode}\xdi\) What we just described corresponds
    to performing the matrix products in the *Chain Rule* formula'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆模式** \(\idx{reverse mode}\xdi\) 我们刚才描述的对应于在 **链式法则** 公式中执行矩阵乘法'
- en: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\hat{\mathbf{y}}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x}).
    \]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\hat{\mathbf{y}}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x}).
    \]
- en: '*from right to left*. But there is another way to do this – from left to right!
    This is called reverse mode. First we compute \(f\) recursively as we did previously,
    in particular we compute the \(\mathbf{z}_i\)s, then going backwards we compute'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*从右到左*。但还有另一种方法来做这件事——从左到右！这被称为逆模式。首先，我们像之前一样递归地计算 \(f\)，特别是我们计算 \(\mathbf{z}_i\)，然后从后往前计算'
- en: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}})\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L})\\
    &\vdots\\ G_1 &:= G_2 \, J_{\bfg_1}(\mathbf{z}_1)\\ \nabla f(\mathbf{x})^T &:=
    G_1 \, J_{\bfg_0}(\mathbf{z}_0) \end{align*}\]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}})\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L})\\
    &\vdots\\ G_1 &:= G_2 \, J_{\bfg_1}(\mathbf{z}_1)\\ \nabla f(\mathbf{x})^T &:=
    G_1 \, J_{\bfg_0}(\mathbf{z}_0) \end{align*}\]
- en: This may seem like a rather minor modification to our previous algorithm. It
    is not – it is much faster! To see this, we need to count the number of elementary
    operations involved in implementing each mode.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来只是对我们之前算法的微小修改。实际上并非如此——它要快得多！为了看到这一点，我们需要计算实现每个模式所需的基本操作数。
- en: 'First a simple calcultion: How many elementary operations (i.e., additions
    and multiplications) does it take to multiply an \(n \times m\) matrix \(A = (a_{ij})_{i,j}\)
    and an \(m \times \ell\) matrix \(B= (b_{ij})_{i,j}\)? Let’s ask ChatGPT. Here’s
    the answer:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先进行一个简单的计算：将一个 \(n \times m\) 矩阵 \(A = (a_{ij})_{i,j}\) 与一个 \(m \times \ell\)
    矩阵 \(B= (b_{ij})_{i,j}\) 相乘需要多少个基本操作（即加法和乘法）？让我们问问 ChatGPT。这是答案：
- en: “To calculate the number of elementary operations required to multiply an \(n
    \times m\) matrix by an \(m \times \ell\) matrix, consider the computation involved
    for each element of the resulting \(n \times \ell\) matrix.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: “为了计算将一个 \(n \times m\) 矩阵与一个 \(m \times \ell\) 矩阵相乘所需的基本操作数，考虑结果 \(n \times
    \ell\) 矩阵中每个元素的涉及的计算。
- en: 'Each element of the resulting matrix is the dot product of a row from the first
    matrix and a column from the second matrix. If the first matrix is denoted as
    \(A\) and the second matrix as \(B\), then the element at position \(i, j\) in
    the resulting matrix \(C\) is calculated as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵的每个元素是第一个矩阵的某一行与第二个矩阵的某一列的点积。如果第一个矩阵表示为 \(A\)，第二个矩阵表示为 \(B\)，那么结果矩阵 \(C\)
    中位置 \(i, j\) 的元素计算如下：
- en: \[ c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \ldots + a_{im}b_{mj} \]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[ c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \ldots + a_{im}b_{mj} \]
- en: This computation involves \(m\) multiplications and \(m - 1\) additions, making
    a total of \(m + (m - 1) = 2m - 1\) elementary operations for each element.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算涉及 \(m\) 次乘法和 \(m - 1\) 次加法，对于每个元素总共是 \(m + (m - 1) = 2m - 1\) 个基本操作。
- en: 'Since matrix \(C\) has \(n \times \ell\) elements, the total number of elementary
    operations required for the entire matrix \(C\) is:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵 \(C\) 有 \(n \times \ell\) 个元素，整个矩阵 \(C\) 所需的基本操作总数是：
- en: \[ (n \ell)(2m - 1) = 2nm\ell - n\ell \]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (n \ell)(2m - 1) = 2nm\ell - n\ell \]
- en: Thus, multiplying an \(n \times m\) matrix by an \(m \times \ell\) matrix takes
    \(2nm\ell - n\ell\) elementary operations.”
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将一个 \(n \times m\) 矩阵与一个 \(m \times \ell\) 矩阵相乘需要 \(2nm\ell - n\ell\) 个基本操作。”
- en: Returning to computing the gradient, we count the number of elementary operations
    needed for each mode. To simplify the calculation, we assume that \(n_0 = d\)
    and \(n_1 = n_2 = \cdots = n_{L} = m\) and \(n_{L+1} = K\).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 回到计算梯度的过程，我们计算每个模式所需的基本操作数。为了简化计算，我们假设 \(n_0 = d\)，\(n_1 = n_2 = \cdots = n_{L}
    = m\)，\(n_{L+1} = K\)。
- en: '*Forward:* The matrix \(F_0 = J_{\bfg_0}(\mathbf{z}_0)\) has dimensions \(m
    \times d\). The matrix \(F_1\), as a product of \(J_{\bfg_1}(\mathbf{z}_1) \in
    \mathbb{R}^{m \times m}\) and \(F_0 \in \mathbb{R}^{m \times d}\) has dimensions
    \(m \times d\); it therefore takes \(m (2m-1) d\) operations to compute. The same
    holds for \(F_2, \ldots, F_{L-1}\) (check it!). By similar considerations, the
    matrix \(F_L\) has dimensions \(K \times d\) and takes \(K (2m-1) d\) operations
    to compute. Finally, \(\nabla f(\mathbf{x})^T = J_{\ell}(\mathbf{z}_{L+1}) F_L
    \in \mathbb{R}^{1 \times d}\) and takes \((2K-1) d\) operations to compute. Overall
    the number of operations is'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*正向:* 矩阵 \(F_0 = J_{\bfg_0}(\mathbf{z}_0)\) 的维度是 \(m \times d\)。矩阵 \(F_1\)，作为
    \(J_{\bfg_1}(\mathbf{z}_1) \in \mathbb{R}^{m \times m}\) 和 \(F_0 \in \mathbb{R}^{m
    \times d}\) 的乘积，其维度是 \(m \times d\)；因此，计算它需要 \(m (2m-1) d\) 次操作。对于 \(F_2, \ldots,
    F_{L-1}\) 也是如此（检查一下！）！通过类似的考虑，矩阵 \(F_L\) 的维度是 \(K \times d\)，计算它需要 \(K (2m-1)
    d\) 次操作。最后，\(\nabla f(\mathbf{x})^T = J_{\ell}(\mathbf{z}_{L+1}) F_L \in \mathbb{R}^{1
    \times d}\) 并需要 \((2K-1) d\) 次操作。总的来说，操作次数是'
- en: \[ (L-1) m (2m-1) d + K (2m-1) d + (2K-1) d. \]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (L-1) m (2m-1) d + K (2m-1) d + (2K-1) d. \]
- en: This is approximately \(2 L m^2 d\) if we think of \(K\) as a small constant
    and ignore the smaller order terms.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 \(K\) 视为一个小的常数并忽略较小的阶数项，这大约是 \(2 L m^2 d\)。
- en: '*Reverse:* The matrix \(G_{L+1} = J_{\ell}(\mathbf{z}_{L+1})\) has dimensions
    \(1 \times K\). The matrix \(G_{L}\), as a product of \(G_{L+1} \in \mathbb{R}^{1
    \times K}\) and \(J_{g_{L}}(\mathbf{z}_{L}) \in \mathbb{R}^{K \times m}\) has
    dimensions \(1 \times m\); it therefore takes \((2K-1) m\) operations to compute.
    The matrix \(G_{L-1}\), as a product of \(G_{L} \in \mathbb{R}^{1 \times m}\)
    and \(J_{g_{L-1}}(\mathbf{z}_{L-1}) \in \mathbb{R}^{m \times m}\) has dimensions
    \(1 \times m\); it therefore takes \((2m-1) m\) operations to compute. The same
    holds for \(G_{L-2}, \ldots, G_{1}\) (check it!). By similar considerations, \(\nabla
    f(\mathbf{x})^T = G_1 \, J_{\bfg_0}(\mathbf{z}_0) \in \mathbb{R}^{1 \times d}\)
    and takes \((2m-1) d\) operations to compute. Overall the number of operations
    is'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向:* 矩阵 \(G_{L+1} = J_{\ell}(\mathbf{z}_{L+1})\) 的维度是 \(1 \times K\)。矩阵 \(G_{L}\)，作为
    \(G_{L+1} \in \mathbb{R}^{1 \times K}\) 和 \(J_{g_{L}}(\mathbf{z}_{L}) \in \mathbb{R}^{K
    \times m}\) 的乘积，其维度是 \(1 \times m\)；因此，计算它需要 \((2K-1) m\) 次操作。矩阵 \(G_{L-1}\)，作为
    \(G_{L} \in \mathbb{R}^{1 \times m}\) 和 \(J_{g_{L-1}}(\mathbf{z}_{L-1}) \in \mathbb{R}^{m
    \times m}\) 的乘积，其维度是 \(1 \times m\)；因此，计算它需要 \((2m-1) m\) 次操作。对于 \(G_{L-2}, \ldots,
    G_{1}\) 也是如此（检查一下！）！通过类似的考虑，\(\nabla f(\mathbf{x})^T = G_1 \, J_{\bfg_0}(\mathbf{z}_0)
    \in \mathbb{R}^{1 \times d}\) 并需要 \((2m-1) d\) 次操作。总的来说，操作次数是'
- en: \[ (2K-1) m + (L-1) (2m-1) m + (2m-1) d. \]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (2K-1) m + (L-1) (2m-1) m + (2m-1) d. \]
- en: This is approximately \(2 L m^2 + 2 m d\) – which can be much smaller than \(2
    L m^2 d\)! In other words, the reverse mode approach can be much faster. Note
    in particular that all computations in the reverse mode are matrix-vector products
    (or more precisely row vector-matrix products) rather than matrix-matrix products.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约是 \(2 L m^2 + 2 m d\) – 这可以比 \(2 L m^2 d\) 小得多！换句话说，反向模式方法可以快得多。特别注意的是，反向模式中的所有计算都是矩阵-向量乘积（或者更精确地说，是行向量-矩阵乘积），而不是矩阵-矩阵乘积。
- en: '**EXAMPLE:** **(continued)** We apply the reverse mode approach to our previous
    example. We get'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** **(继续)** 我们将反向模式方法应用于先前的例子。我们得到'
- en: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L}) = G_{L+1} \mathcal{W}_{L}
    = (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L} \\ \vdots\\ G_1 &:= G_2 \,
    J_{\bfg_1}(\mathbf{z}_1) = G_2 \mathcal{W}_{1} = [(\hat{\mathbf{y}} - \mathbf{y})^T
    \mathcal{W}_{L} \cdots \mathcal{W}_{2}] \mathcal{W}_{1} \\ \nabla f(\mathbf{x})^T
    &:= G_1 \, J_{\bfg_0}(\mathbf{z}_0) = [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L}
    \cdots \mathcal{W}_{2}\mathcal{W}_{1}] \mathcal{W}_{0}\\ &= (\mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} - \mathbf{y})^T \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}, \end{align*}\]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L}) = G_{L+1} \mathcal{W}_{L}
    = (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L} \\ \vdots\\ G_1 &:= G_2 \,
    J_{\bfg_1}(\mathbf{z}_1) = G_2 \mathcal{W}_{1} = [(\hat{\mathbf{y}} - \mathbf{y})^T
    \mathcal{W}_{L} \cdots \mathcal{W}_{2}] \mathcal{W}_{1} \\ \nabla f(\mathbf{x})^T
    &:= G_1 \, J_{\bfg_0}(\mathbf{z}_0) = [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L}
    \cdots \mathcal{W}_{2}\mathcal{W}_{1}] \mathcal{W}_{0}\\ &= (\mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} - \mathbf{y})^T \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}, \end{align*}\]
- en: which matches our previous calculation. Note that all computations involve multiplying
    a row vector by a matrix. \(\lhd\)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们的先前计算相匹配。注意，所有计算都涉及将行向量乘以矩阵。 \(\lhd\)
- en: '**NUMERICAL CORNER:** We try our specific example.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们尝试我们的特定例子。'
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We indeed obtain the same answer yet again.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实再次得到了相同的答案。
- en: \(\unlhd\)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 'To provide a little more insight in the savings obtained through the reverse
    mode, consider the following simple calculations. Let \(A, B \in \mathbb{R}^{n
    \times n}\) and \(\mathbf{v} \in \mathbb{R}^n\). Suppose we seek to compute \(\mathbf{v}^T
    B A\). By [associativity](https://en.wikipedia.org/wiki/Associative_property)
    of matrix multiplication, there are two ways of doing this: compute \(\mathbf{v}^{T}(BA)\)
    (i.e., first compute \(BA\) then multiply by \(\mathbf{v}^T\); or compute \((\mathbf{v}^T
    B) A\). The first approach requires \(n^2(2n-1) + n(2n-1)\) operations, while
    the second only requires \(2n(2n-1)\). The latter is much smaller since \(2 n^3\)
    (the leading term in the first approach) grows much faster than \(4 n^2\) (the
    leading term in the second approach) when \(n\) is large.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更多关于通过反向模式获得的节省的洞察，考虑以下简单的计算。设 \(A, B \in \mathbb{R}^{n \times n}\) 和 \(\mathbf{v}
    \in \mathbb{R}^n\)。假设我们想要计算 \(\mathbf{v}^T B A\)。通过矩阵乘法的 [结合律](https://en.wikipedia.org/wiki/Associative_property)，有两种方法来做这件事：计算
    \(\mathbf{v}^{T}(BA)\)（即，首先计算 \(BA\) 然后乘以 \(\mathbf{v}^T\)；或者计算 \((\mathbf{v}^T
    B) A\)。第一种方法需要 \(n^2(2n-1) + n(2n-1)\) 次操作，而第二种方法只需要 \(2n(2n-1)\)。后者要小得多，因为当 \(n\)
    很大时，\(2 n^3\)（第一种方法中的主导项）的增长速度比 \(4 n^2\)（第二种方法中的主导项）快得多。
- en: Why is this happening? One way to understand this is to think of the output
    \(\mathbf{v}^T B A\) as a *linear combination of the rows of \(A\)* – a very specific
    linear combination in fact. In the first approach, we compute \(BA\) which gives
    us \(n\) different linear combinations of the rows of \(A\) – none being the one
    we want – and then we compute the desired linear combination by multiplying by
    \(\mathbf{v}^T\). This is wasteful. In the second approach, we immediately compute
    the coefficients of the specific linear combination we seek – \(\mathbf{v}^T B\)
    – and then we compute that linear combination by multiplying to the right by \(A\).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会发生这种情况？理解这一点的其中一种方法是考虑输出 \(\mathbf{v}^T B A\) 是 \(A\) 的行的一个 *线性组合* – 实际上是一个非常具体的线性组合。在第一种方法中，我们计算
    \(BA\)，这给我们 \(n\) 个不同的 \(A\) 的行的线性组合 – 没有一个是我们想要的 – 然后我们通过乘以 \(\mathbf{v}^T\)
    来计算所需的线性组合。这是浪费的。在第二种方法中，我们立即计算我们寻求的特定线性组合的系数 – \(\mathbf{v}^T B\) – 然后通过右乘 \(A\)
    来计算这个线性组合。
- en: While the setting we examined in this subsection is illuminating, it is not
    exactly what we want. In the machine learning context, each “layer” \(\bfg_i\)
    has parameters (in our running example, there were the entries of \(\mathcal{W}_{i}\))
    and we seek to optimize with respect to those parameters. For this, we need the
    gradient with respect to the parameters, not the input \(\mathbf{x}\). In the
    next subsection, we consider a generalization of the current setting, progressive
    functions, which will allow us to do this. The notation gets more complicated,
    but the basic idea remains the same.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本小节中考察的设置具有启发性，但它并不完全是我们想要的。在机器学习环境中，每个“层” \(\bfg_i\) 都有参数（在我们的运行示例中，是
    \(\mathcal{W}_{i}\) 的条目）并且我们希望针对这些参数进行优化。为此，我们需要关于参数的梯度，而不是输入 \(\mathbf{x}\)。在下一个小节中，我们将考虑当前设置的推广，即递进函数，这将使我们能够做到这一点。符号变得更加复杂，但基本思想保持不变。
- en: 8.3.2\. Progressive functions[#](#progressive-functions "Link to this heading")
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.2\. 递进函数[#](#progressive-functions "链接到这个标题")
- en: As mentioned previously, while it may seem natural to define a prediction function
    \(h\) (e.g., a classifier) as a function of the input data \(\mathbf{x}\in \mathbb{R}^{d}\),
    when fitting data we are ultimately interested in thinking of \(h\) as a function
    of the parameters \(\mathbf{w} \in \mathbb{R}^r\) that need to be adjusted – over
    a fixed dataset. Hence, in this section, the input \(\mathbf{x}\) is fixed while
    the vector of parameters \(\mathbf{w}\) is now variable.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，虽然将预测函数 \(h\)（例如，分类器）定义为输入数据 \(\mathbf{x}\in \mathbb{R}^{d}\) 的函数可能看起来很自然，但在拟合数据时，我们最终感兴趣的将
    \(h\) 视为需要调整的参数 \(\mathbf{w} \in \mathbb{R}^r\) 的函数 – 在一个固定的数据集上。因此，在本节中，输入 \(\mathbf{x}\)
    是固定的，而参数向量 \(\mathbf{w}\) 现在是可变的。
- en: '**A first example** We use the example from the previous subsection to illustrate
    the main ideas. That is, suppose \(d=3\), \(L=1\), \(n_1 = 2\), and \(K = 2\).
    Fix a data sample \(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3, \mathbf{y} =
    (y_1, y_2) \in \mathbb{R}^2\). For \(i=0, 1\), we use the notation'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个例子** 我们使用前一小节中的例子来说明主要思想。也就是说，假设 \(d=3\)，\(L=1\)，\(n_1 = 2\)，\(K = 2\)。固定一个数据样本
    \(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\)。对于
    \(i=0, 1\)，我们使用以下记号'
- en: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix}. \end{split}\]
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix}. \end{split}\]
- en: and let
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让
- en: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2
    = \frac{1}{2}(y_1 - \hat{y}_1)^2 + \frac{1}{2}(y_2 - \hat{y}_2)^2. \]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2
    = \frac{1}{2}(y_1 - \hat{y}_1)^2 + \frac{1}{2}(y_2 - \hat{y}_2)^2. \]
- en: 'We change the notation for the “layer” function \(\bfg_i\) to reflect the fact
    that it is now a function of two (concatenated) vectors: the input \(\mathbf{z}_i
    = (z_{i,1},\ldots,z_{i,n_i})\) from the previous layer and a layer-specific set
    of parameters \(\mathbf{w}_i\). That is,'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将“层”函数 \(\bfg_i\) 的记号改为反映它现在是一个两个（连接的）向量的函数：来自前一层的输入 \(\mathbf{z}_i = (z_{i,1},\ldots,z_{i,n_i})\)
    和一个特定层的参数集 \(\mathbf{w}_i\)。也就是说，
- en: \[\begin{split} \bfg_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_{i} \mathbf{z}_i
    = \begin{pmatrix} (\mathbf{w}_i^{(1)})^T\\ (\mathbf{w}_i^{(2)})^T \end{pmatrix}
    \mathbf{z}_i \end{split}\]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bfg_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_{i} \mathbf{z}_i
    = \begin{pmatrix} (\mathbf{w}_i^{(1)})^T\\ (\mathbf{w}_i^{(2)})^T \end{pmatrix}
    \mathbf{z}_i \end{split}\]
- en: with \(\mathbf{w}_i = (\mathbf{w}_i^{(1)}, \mathbf{w}_i^{(2)})\), the concatenation
    of the rows of \(\mathcal{W}_{i}\) (as column vectors). A different way to put
    this is that
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 的梯度，其中 \(\mathbf{w}_i = (\mathbf{w}_i^{(1)}, \mathbf{w}_i^{(2)})\) 是 \(\mathcal{W}_{i}\)
    的行的连接（作为列向量）。另一种说法是
- en: \[ \mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T), \]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T), \]
- en: where we took the transpose to turn the rows into columns. More specifically,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们取转置将行转换为列。更具体地说，
- en: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0} \quad\text{with}\quad
    \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0} \quad\text{with}\quad
    \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]
- en: (i.e., \(\mathbf{w}_0^{(1)} = (w_0, w_1, w_2)\) and \(\mathbf{w}_0^{(2)} = (w_3,
    w_4, w_5)\)) and
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: （即，\(\mathbf{w}_0^{(1)} = (w_0, w_1, w_2)\) 和 \(\mathbf{w}_0^{(2)} = (w_3, w_4,
    w_5)\)）和
- en: \[ \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1} \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9) \]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1} \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9) \]
- en: (i.e., \(\mathbf{w}_1^{(1)} = (w_6, w_7)\) and \(\mathbf{w}_1^{(2)} = (w_8,
    w_9)\)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: （即，\(\mathbf{w}_1^{(1)} = (w_6, w_7)\) 和 \(\mathbf{w}_1^{(2)} = (w_8, w_9)\)）。
- en: We seek to compute the gradient of
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图计算
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= \frac{1}{2} \|\mathbf{y} - \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2\\
    &= \frac{1}{2}\left(y_1 - w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4
    x_2 + w_5 x_3)\right)^2\\ & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1
    x_2 + w_2 x_3) - w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2. \end{align*}\]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= \frac{1}{2} \|\mathbf{y} - \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2\\
    &= \frac{1}{2}\left(y_1 - w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4
    x_2 + w_5 x_3)\right)^2\\ & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1
    x_2 + w_2 x_3) - w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2. \end{align*}\]
- en: by applying the *Chain Rule* backwards, as we justified in the previous subsection
    – but this time we take the gradient with respect to the parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反向应用**链式法则**，正如我们在前一小节中所证明的那样——但这次我们对参数求梯度
- en: \[ \mathbf{w} := (\mathbf{w}_0, \mathbf{w}_1) = (w_0,w_1,\ldots,w_9). \]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w} := (\mathbf{w}_0, \mathbf{w}_1) = (w_0,w_1,\ldots,w_9). \]
- en: 'Notice a key change in the notation: we now accordingly think of \(f\) *as
    a function of \(\mathbf{w}\)*; the role of \(\mathbf{x}\) is implicit.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意记号中的一个关键变化：我们现在相应地认为 \(f\) 是 \(\mathbf{w}\) 的函数；\(\mathbf{x}\) 的作用是隐含的。
- en: On the other hand, it may seem counter-intuitive that we now think of \(\bfg_i\)
    as a function of *both* its own parameters and its inputs from the previous layer
    when we just stated that we only care about the gradient with respect to the former.
    But, as we will see, it turns out that we need the Jacobians with respect to both
    as the input from the previous layer *actually depends on the parameters of the
    previous layers*. For instance, \(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1}
    \mathbf{z}_{1}\) where \(\mathbf{z}_{1} = \bfg_0(\mathbf{z}_0, \mathbf{w}_0) =
    \mathcal{W}_{0} \mathbf{z}_{0}\).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当我们刚刚表示我们只关心相对于前者的梯度时，现在认为 \(\bfg_i\) 是其自身参数和来自前一层的输入的函数，这似乎是反直觉的。但是，正如我们将看到的，结果是我们确实需要两个相对于的雅可比矩阵，因为前一层的输入实际上依赖于前一层的参数。例如，\(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1}\)，其中 \(\mathbf{z}_{1} = \bfg_0(\mathbf{z}_0,
    \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0}\)。
- en: Recall that we have already computed the requisite Jacobians \(J_{\bfg_0}\)
    and \(J_{\bfg_1}\) in a previous example. We have also computed the Jacobian \(J_{\ell}\)
    of \(\ell\). At this point, it is tempting to apply the *Chain Rule* and deduce
    that the gradient of \(f\) is
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们已经在先前的例子中计算了所需的雅可比矩阵 \(J_{\bfg_0}\) 和 \(J_{\bfg_1}\)。我们也计算了 \(\ell\)
    的雅可比矩阵 \(J_{\ell}\)。在这个时候，应用链式法则并推断 \(f\) 的梯度是
- en: \[ J_{\ell}(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)) \,J_{\bfg_1}(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)
    \,J_{\bfg_0}(\mathbf{x}, \mathbf{w}_0). \]
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)) \,J_{\bfg_1}(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)
    \,J_{\bfg_0}(\mathbf{x}, \mathbf{w}_0). \]
- en: But this is not correct. For one, the dimensions do not match! For instance,
    \(J_{\bfg_0} \in \mathbb{R}^{2 \times 9}\) since \(\bfg_0\) has \(2\) outputs
    and \(9\) inputs (i.e., \(z_{0,1}, z_{0,2}, z_{0,3}, w_0, w_1, w_2, w_3, w_4,
    w_5\)) while \(J_{\bfg_1} \in \mathbb{R}^{2 \times 6}\) since \(\bfg_1\) has \(2\)
    outputs and \(6\) inputs (i.e., \(z_{1,1}, z_{1,2}, w_6, w_7, w_8, w_9\)). So
    what went wrong?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是不正确的。首先，维度不匹配！例如，\(J_{\bfg_0} \in \mathbb{R}^{2 \times 9}\)，因为 \(\bfg_0\)
    有 \(2\) 个输出和 \(9\) 个输入（即 \(z_{0,1}, z_{0,2}, z_{0,3}, w_0, w_1, w_2, w_3, w_4,
    w_5\)），而 \(J_{\bfg_1} \in \mathbb{R}^{2 \times 6}\)，因为 \(\bfg_1\) 有 \(2\) 个输出和
    \(6\) 个输入（即 \(z_{1,1}, z_{1,2}, w_6, w_7, w_8, w_9\)）。那么出了什么问题？
- en: The function \(f\) is *not* in fact a straight composition of the functions
    \(\ell\), \(\bfg_1\), and \(\bfg_0\). Indeed the parameters to differentiate with
    respect to are introduced progressively, each layer injecting its own additional
    parameters which are not obtained from the previous layers. Hence we cannot write
    the gradient of \(f\) as a simple product the Jacobians, unlike what happend in
    the previous subsection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 \(f\) 实际上并不是由函数 \(\ell\)、\(\bfg_1\) 和 \(\bfg_0\) 的简单组合。确实，相对于要微分的是逐步引入的参数，每一层都注入自己的附加参数，这些参数不是从前一层获得的。因此，我们不能像前一小节中发生的那样，将
    \(f\) 的梯度写成雅可比矩阵的简单乘积。
- en: But not all is lost. We show below that we can still apply the *Chain Rule*
    step-by-step in a way that accounts for the additional parameters on each layer.
    Taking a hint from the previous subsection, we proceed forward first to compute
    \(f\) and the Jacobians, and then go backwards to compute the gradient \(\nabla
    f\). We use the notation \(\mathbb{A}_{n}[\mathbf{x}]\) and \(\mathbb{B}_{n}[\mathbf{z}]\)
    from the background section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非一切尽失。我们下面将展示，我们仍然可以逐步应用链式法则，同时考虑到每一层的附加参数。借鉴前一小节，我们首先向前计算 \(f\) 和雅可比矩阵，然后向后计算梯度
    \(\nabla f\)。我们使用背景部分中的符号 \(\mathbb{A}_{n}[\mathbf{x}]\) 和 \(\mathbb{B}_{n}[\mathbf{z}]\)。
- en: 'In the forward phase, we compute \(f\) itself and the requisite Jacobians:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向阶段，我们计算 \(f\) 本身和所需的雅可比矩阵：
- en: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0}\\ &= \begin{pmatrix}
    (\mathbf{w}_0^{(1)})^T\mathbf{x}\\ (\mathbf{w}_0^{(2)})^T\mathbf{x}\end{pmatrix}
    = \begin{pmatrix} w_0 x_1 + w_1 x_2 + w_2 x_3\\ w_3 x_1 + w_4 x_2 + w_5 x_3 \end{pmatrix}\\
    &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0]
    & \mathbb{B}_{2}[\mathbf{z}_0] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{0}
    & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix} w_0
    & w_1 & w_2 & x_1 & x_2 & x_3 & 0 & 0 & 0\\ w_3 & w_4 & w_5 & 0 & 0 & 0 & x_1
    & x_2 & x_3 \end{pmatrix} \end{align*}\]\[\begin{align*} &\hat{\mathbf{y}} :=
    \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1}\\
    &= \begin{pmatrix} w_6 z_{1,1} + w_7 z_{1,2}\\ w_8 z_{1,1} + w_9 z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7 (\mathbf{w}_0^{(2)})^T\mathbf{x}\\
    w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} \end{pmatrix}\\
    &= \begin{pmatrix} w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7(w_3 x_1 + w_4 x_2 +
    w_5 x_3)\\ w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)
    \end{pmatrix}\\ &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1]
    & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{1}
    & I_{2\times 2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} w_6
    & w_7 & z_{1,1} & z_{1,2} & 0 & 0\\ w_8 & w_9 & 0 & 0 & z_{1,1} & z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 & w_7 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    & 0 & 0\\ w_8 & w_9 & 0 & 0 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    \end{pmatrix} \end{align*}\]\[\begin{align*} &f(\mathbf{x}) := \ell(\hat{\mathbf{y}})
    = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\ &= \frac{1}{2}\left(y_1 -
    w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2\\
    & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_9(w_3 x_1
    + w_4 x_2 + w_5 x_3)\right)^2\\ &J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7
    (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_1 & w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x}
    + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_2 \end{pmatrix}. \end{align*}\]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0}\\ &= \begin{pmatrix}
    (\mathbf{w}_0^{(1)})^T\mathbf{x}\\ (\mathbf{w}_0^{(2)})^T\mathbf{x}\end{pmatrix}
    = \begin{pmatrix} w_0 x_1 + w_1 x_2 + w_2 x_3\\ w_3 x_1 + w_4 x_2 + w_5 x_3 \end{pmatrix}\\
    &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0]
    & \mathbb{B}_{2}[\mathbf{z}_0] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{0}
    & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix} w_0
    & w_1 & w_2 & x_1 & x_2 & x_3 & 0 & 0 & 0\\ w_3 & w_4 & w_5 & 0 & 0 & 0 & x_1
    & x_2 & x_3 \end{pmatrix} \end{align*}\]\[\begin{align*} &\hat{\mathbf{y}} :=
    \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1}\\
    &= \begin{pmatrix} w_6 z_{1,1} + w_7 z_{1,2}\\ w_8 z_{1,1} + w_9 z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7 (\mathbf{w}_0^{(2)})^T\mathbf{x}\\
    w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} \end{pmatrix}\\
    &= \begin{pmatrix} w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7(w_3 x_1 + w_4 x_2 +
    w_5 x_3)\\ w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)
    \end{pmatrix}\\ &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1]
    & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{1}
    & I_{2\times 2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} w_6
    & w_7 & z_{1,1} & z_{1,2} & 0 & 0\\ w_8 & w_9 & 0 & 0 & z_{1,1} & z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 & w_7 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    & 0 & 0\\ w_8 & w_9 & 0 & 0 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    \end{pmatrix} \end{align*}\]\[\begin{align*} &f(\mathbf{x}) := \ell(\hat{\mathbf{y}})
    = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\ &= \frac{1}{2}\left(y_1 -
    w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2\\
    & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_9(w_3 x_1
    + w_4 x_2 + w_5 x_3)\right)^2\\ &J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7
    (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_1 & w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x}
    + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_2 \end{pmatrix}. \end{align*}\]
- en: We now compute the gradient of \(f\) with respect to \(\mathbf{w}\). We start
    with \(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\). For this step, we think of \(f\)
    as the composition \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\). Here \(\mathbf{z}_1\)
    does not depend on \(\mathbf{w}_1\) and therefore can be considered fixed for
    this calculation. By the *Chain Rule*
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在计算 \(f\) 关于 \(\mathbf{w}\) 的梯度。我们从 \(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\)
    开始。对于这一步，我们将 \(f\) 视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 的复合函数。在这里，\(\mathbf{z}_1\)
    不依赖于 \(\mathbf{w}_1\)，因此可以认为在这个计算中是固定的。根据**链式法则**
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_6} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_6} = \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_6} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_6} = (\hat{y}_1 - y_1) z_{1,1} \end{align*}\]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_6} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_6} = \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_6} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_6} = (\hat{y}_1 - y_1) z_{1,1} \end{align*}\]
- en: where we used the fact that \(g_{1,2}(\mathbf{z}_1, \mathbf{w}_1) = w_8 z_{1,1}
    + w_9 z_{1,2}\) does not depend on \(w_6\) and therefore \(\frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_6} = 0\). Similarly
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了这样一个事实，即 \(g_{1,2}(\mathbf{z}_1, \mathbf{w}_1) = w_8 z_{1,1} + w_9 z_{1,2}\)
    不依赖于 \(w_6\)，因此 \(\frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial
    w_6} = 0\)。同样地
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_7} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_7} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_7} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_7} = (\hat{y}_1 - y_1) z_{1,2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_8} &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_8}
    =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_8} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_8} =
    (\hat{y}_2 - y_2) z_{1,1}\\ \frac{\partial f(\mathbf{w})}{\partial w_9} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_9} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_9} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_9} = (\hat{y}_2 - y_2) z_{1,2}. \end{align*}\]
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_7} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_7} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_7} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_7} = (\hat{y}_1 - y_1) z_{1,2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_8} &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_8}
    =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_8} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_8} =
    (\hat{y}_2 - y_2) z_{1,1}\\ \frac{\partial f(\mathbf{w})}{\partial w_9} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_9} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_9} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_9} = (\hat{y}_2 - y_2) z_{1,2}. \end{align*}\]
- en: In matrix form, this is
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式表示，这是
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{B}_{2}[\mathbf{z}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T (I_{2\times
    2} \otimes \mathbf{z}_1^T)\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) z_{1,1} & (\hat{y}_1 - y_1) z_{1,2} & (\hat{y}_2
    - y_2) z_{1,1} & (\hat{y}_2 - y_2) z_{1,2} \end{pmatrix} \end{align*}\]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{B}_{2}[\mathbf{z}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T (I_{2\times
    2} \otimes \mathbf{z}_1^T)\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) z_{1,1} & (\hat{y}_1 - y_1) z_{1,2} & (\hat{y}_2
    - y_2) z_{1,1} & (\hat{y}_2 - y_2) z_{1,2} \end{pmatrix} \end{align*}\]
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一行我们使用了**克罗内克积的性质（f）**。
- en: To compute the partial derivatives with respect to \(\mathbf{w}_0 = (w_0, w_1,
    \ldots, w_5)\), we first need to compute partial derivatives with respect to \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) since \(f\) depends on \(\mathbf{w}_0\) through it. For
    this calculation, we think again of \(f\) as the composition \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\), but this time our focus is on the variables \(\mathbf{z}_1\).
    We obtain
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算关于 \(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\) 的偏导数，我们首先需要计算关于 \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) 的偏导数，因为 \(f\) 通过它依赖于 \(\mathbf{w}_0\)。对于这个计算，我们再次将 \(f\)
    视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 的复合，但这次我们的重点是变量 \(\mathbf{z}_1\)。我们得到
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,1}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,1}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,1}}\\
    &= (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 \end{align*}\]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,1}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,1}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,1}}\\
    &= (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 \end{align*}\]
- en: and
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,2}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,2}}\\
    &= (\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9. \end{align*}\]
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,2}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,2}}\\
    &= (\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9. \end{align*}\]
- en: In matrix form, this is
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式，这可以表示为
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 & (\hat{y}_1
    - y_1) w_7 + (\hat{y}_2 - y_2) w_9 \end{pmatrix}. \end{align*}\]
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 & (\hat{y}_1
    - y_1) w_7 + (\hat{y}_2 - y_2) w_9 \end{pmatrix}. \end{align*}\]
- en: The vector \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial
    f(\mathbf{w})}{\partial z_{1,2}}\right)\) is called an adjoint.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial f(\mathbf{w})}{\partial
    z_{1,2}}\right)\) 被称为伴随向量。
- en: We now compute the gradient of \(f\) with respect to \(\mathbf{w}_0 = (w_0,
    w_1, \ldots, w_5)\). For this step, we think of \(f\) as the composition of \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\) as a function of \(\mathbf{z}_1\) and \(\bfg_0(\mathbf{z}_0,
    \mathbf{w}_0)\) as a function of \(\mathbf{w}_0\). Here \(\mathbf{z}_0\) does
    not depend on \(\mathbf{w}_0\) and therefore can be considered fixed for this
    calculation. By the *Chain Rule*
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在计算 \(f\) 关于 \(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\) 的梯度。对于这一步，我们将 \(f\)
    视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 作为 \(\mathbf{z}_1\) 的函数和 \(\bfg_0(\mathbf{z}_0,
    \mathbf{w}_0)\) 作为 \(\mathbf{w}_0\) 的函数的复合。在这里，\(\mathbf{z}_0\) 不依赖于 \(\mathbf{w}_0\)，因此可以认为在这个计算中是固定的。根据**链式法则**
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_0} &= \frac{\partial
    \ell(\bfg_1(\bfg_0(\mathbf{z}_0, \mathbf{w}_0), \mathbf{w}_1))}{\partial w_0}\\
    &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}
    \frac{\partial g_{0,1}(\mathbf{z}_0, \mathbf{w}_0)}{\partial w_0} + \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}} \frac{\partial g_{0,2}(\mathbf{z}_0,
    \mathbf{w}_0)}{\partial w_0}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8)
    z_{0,1}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1}, \end{align*}\]
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_0} &= \frac{\partial
    \ell(\bfg_1(\bfg_0(\mathbf{z}_0, \mathbf{w}_0), \mathbf{w}_1))}{\partial w_0}\\
    &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}
    \frac{\partial g_{0,1}(\mathbf{z}_0, \mathbf{w}_0)}{\partial w_0} + \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}} \frac{\partial g_{0,2}(\mathbf{z}_0,
    \mathbf{w}_0)}{\partial w_0}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8)
    z_{0,1}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1}, \end{align*}\]
- en: where we used the fact that \(g_{0,2}(\mathbf{z}_0, \mathbf{w}_0) = w_3 z_{0,1}
    + w_4 z_{0,2} + w_5 z_{0,3}\) does not depend on \(w_0\) and therefore \(\frac{\partial
    g_{0,2}(\mathbf{z}_0, \mathbf{w}_0)}{\partial w_0} = 0\).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(g_{0,2}(\mathbf{z}_0, \mathbf{w}_0) = w_3 z_{0,1} + w_4 z_{0,2} +
    w_5 z_{0,3}\) 不依赖于 \(w_0\) 的这一事实，因此 \(\frac{\partial g_{0,2}(\mathbf{z}_0, \mathbf{w}_0)}{\partial
    w_0} = 0\)。
- en: Similarly (check it!)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地（检查一下！）
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_1} &= ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_2} &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{3}\\ \frac{\partial
    f(\mathbf{w})}{\partial w_3} &= ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9)
    x_{1}\\ \frac{\partial f(\mathbf{w})}{\partial w_4} &= ((\hat{y}_1 - y_1) w_7
    + (\hat{y}_2 - y_2) w_9) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial w_5} &=
    ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9) x_{3}. \end{align*}\]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_1} &= ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_2} &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{3}\\ \frac{\partial
    f(\mathbf{w})}{\partial w_3} &= ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9)
    x_{1}\\ \frac{\partial f(\mathbf{w})}{\partial w_4} &= ((\hat{y}_1 - y_1) w_7
    + (\hat{y}_2 - y_2) w_9) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial w_5} &=
    ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9) x_{3}. \end{align*}\]
- en: In matrix form, this is
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式，这可以表示为
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1] \,\mathbb{B}_{2}[\mathbf{z}_0]\\ &= (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1 (I_{2\times 2} \otimes \mathbf{z}_0^T)\\ &= ((\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1) \otimes \mathbf{x}^T\\ &= \begin{pmatrix} ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1} & \cdots & ((\hat{y}_1 - y_1) w_7 +
    (\hat{y}_2 - y_2) w_9) x_{3} \end{pmatrix} \end{align*}\]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1] \,\mathbb{B}_{2}[\mathbf{z}_0]\\ &= (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1 (I_{2\times 2} \otimes \mathbf{z}_0^T)\\ &= ((\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1) \otimes \mathbf{x}^T\\ &= \begin{pmatrix} ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1} & \cdots & ((\hat{y}_1 - y_1) w_7 +
    (\hat{y}_2 - y_2) w_9) x_{3} \end{pmatrix} \end{align*}\]
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们在最后一行使用了 *克罗内克积的性质（f）*。
- en: To sum up,
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，
- en: \[ \nabla f (\mathbf{w})^T = \begin{pmatrix} (\hat{\mathbf{y}} - \mathbf{y})^T
    \otimes (\mathcal{W}_{0} \mathbf{x})^T & ((\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1)
    \otimes \mathbf{x}^T \end{pmatrix}. \]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f (\mathbf{w})^T = \begin{pmatrix} (\hat{\mathbf{y}} - \mathbf{y})^T
    \otimes (\mathcal{W}_{0} \mathbf{x})^T & ((\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1)
    \otimes \mathbf{x}^T \end{pmatrix}. \]
- en: '**NUMERICAL CORNER:** We return to the concrete example from the previous subsection.
    This time the matrices `W0` and `W1` require partial derivatives.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们回到前一小节的具体例子。这次矩阵 `W0` 和 `W1` 需要计算偏导数。'
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We compute the gradient \(\nabla f(\mathbf{w})\) using AD.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用AD（自动微分）来计算梯度 \(\nabla f(\mathbf{w})\)。
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: These are written in the form of matrix derivatives
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是矩阵导数的形式
- en: \[\begin{split} \frac{\partial f}{\partial \mathcal{W}_0} = \begin{pmatrix}
    \frac{\partial f}{\partial w_0} & \frac{\partial f}{\partial w_1} & \frac{\partial
    f}{\partial w_2} \\ \frac{\partial f}{\partial w_3} & \frac{\partial f}{\partial
    w_4} & \frac{\partial f}{\partial w_5} \end{pmatrix} \quad\text{and}\quad \frac{\partial
    f}{\partial \mathcal{W}_1} = \begin{pmatrix} \frac{\partial f}{\partial w_6} &
    \frac{\partial f}{\partial w_7} \\ \frac{\partial f}{\partial w_8} & \frac{\partial
    f}{\partial w_9} \end{pmatrix}. \end{split}\]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{\partial f}{\partial \mathcal{W}_0} = \begin{pmatrix}
    \frac{\partial f}{\partial w_0} & \frac{\partial f}{\partial w_1} & \frac{\partial
    f}{\partial w_2} \\ \frac{\partial f}{\partial w_3} & \frac{\partial f}{\partial
    w_4} & \frac{\partial f}{\partial w_5} \end{pmatrix} \quad\text{和}\quad \frac{\partial
    f}{\partial \mathcal{W}_1} = \begin{pmatrix} \frac{\partial f}{\partial w_6} &
    \frac{\partial f}{\partial w_7} \\ \frac{\partial f}{\partial w_8} & \frac{\partial
    f}{\partial w_9} \end{pmatrix}. \end{split}\]
- en: We use our formulas to confirm that they match these results. We need the Kronecker
    product, which in PyTorch is implemented as [`torch.kron`](https://pytorch.org/docs/stable/generated/torch.kron.html).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的公式来验证它们是否与这些结果匹配。我们需要克罗内克积，在PyTorch中通过 `torch.kron` 实现[`torch.kron`](https://pytorch.org/docs/stable/generated/torch.kron.html)。
- en: '[PRE35]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Observe that this time these results are written in vectorized form (i.e., obtained
    by concatenating the rows). But they do match with the AD output.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这次这些结果是以向量化的形式（即通过连接行获得）写出的。但它们与 AD 输出相匹配。
- en: \(\unlhd\)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**General setting** \(\idx{progressive function}\xdi\) More generally, we have
    \(L+2\) layers. The input layer is \(\mathbf{z}_0 := \mathbf{x}\), which we refer
    to as layer \(0\). Hidden layer \(i\), \(i=1,\ldots,L\), is defined by a continuously
    differentiable function \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)
    which this time takes *two vector-valued inputs*: a vector \(\mathbf{z}_{i-1}
    \in \mathbb{R}^{n_{i-1}}\) fed from the \((i-1)\)-st layer and a vector \(\mathbf{w}_{i-1}
    \in \mathbb{R}^{r_{i-1}}\) of parameters specific to the \(i\)-th layer'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般设置** \(\idx{渐进函数}\xdi\) 更一般地，我们有 \(L+2\) 层。输入层是 \(\mathbf{z}_0 := \mathbf{x}\)，我们将其称为层
    \(0\)。隐藏层 \(i\)，\(i=1,\ldots,L\)，由一个连续可微的函数 \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1},
    \mathbf{w}_{i-1})\) 定义，这次它接受两个向量值输入：一个来自 \((i-1)\) 层的向量 \(\mathbf{z}_{i-1} \in
    \mathbb{R}^{n_{i-1}}\) 和一个特定于 \(i\) 层的参数向量 \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)'
- en: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
- en: The output \(\mathbf{z}_i\) of \(\bfg_{i-1}\) is a vector in \(\mathbb{R}^{n_{i}}\)
    which is passed to the \((i+1)\)-st layer as input. The output layer is \(\mathbf{z}_{L+1}
    := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\), which we also refer to as layer
    \(L+1\).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bfg_{i-1}\) 的输出 \(\mathbf{z}_i\) 是一个在 \(\mathbb{R}^{n_{i}}\) 中的向量，它作为输入传递给
    \((i+1)\) 层。输出层是 \(\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\)，我们也将它称为层
    \(L+1\)。
- en: For \(i = 1,\ldots,L+1\), let
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = 1,\ldots,L+1\)，令
- en: \[ \overline{\mathbf{w}}^{i-1} = (\mathbf{w}_0,\mathbf{w}_1,\ldots,\mathbf{w}_{i-1})
    \in \mathbb{R}^{r_0 + r_1+\cdots+r_{i-1}} \]
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \overline{\mathbf{w}}^{i-1} = (\mathbf{w}_0,\mathbf{w}_1,\ldots,\mathbf{w}_{i-1})
    \in \mathbb{R}^{r_0 + r_1+\cdots+r_{i-1}} \]
- en: be the concatenation of the parameters from the first \(i\) layers (not including
    the input layer, which does not have parameters) as a vector in \(\mathbb{R}^{r_0+r_1+\cdots+r_{i-1}}\).
    Then the output of layer \(i\) *as a function of the parameters* is the composition
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 是前 \(i\) 层参数的连接（不包括没有参数的输入层）作为一个向量在 \(\mathbb{R}^{r_0+r_1+\cdots+r_{i-1}}\)
    中。然后层 \(i\) 作为参数的输出是组合
- en: \[ \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}) = \bfg_{i-1}(\mathcal{O}_{i-2}(\overline{\mathbf{w}}^{i-2}),
    \mathbf{w}_{i-1}) = \bfg_{i-1}(\bfg_{i-2}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{i-2}), \mathbf{w}_{i-1}) \in \mathbb{R}^{n_{i}}, \]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}) = \bfg_{i-1}(\mathcal{O}_{i-2}(\overline{\mathbf{w}}^{i-2}),
    \mathbf{w}_{i-1}) = \bfg_{i-1}(\bfg_{i-2}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{i-2}), \mathbf{w}_{i-1}) \in \mathbb{R}^{n_{i}}, \]
- en: for \(i = 2, \ldots, L+1\). When \(i=1\), we have simply
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = 2, \ldots, L+1\)。当 \(i=1\) 时，我们有
- en: \[ \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_{0}(\mathbf{x}, \mathbf{w}_0).
    \]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_{0}(\mathbf{x}, \mathbf{w}_0).
    \]
- en: Observe that the function \(\mathcal{O}_{i-1}\) depends implicitly on the input
    \(\mathbf{x}\) – which we do *not* think of as a variable in this setting. To
    simplify the notation, we do not make the dependence on \(\mathbf{x}\) explicit.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到函数 \(\mathcal{O}_{i-1}\) 依赖于输入 \(\mathbf{x}\) —— 在这个设置中我们并不将其视为变量。为了简化符号，我们没有明确表示对
    \(\mathbf{x}\) 的依赖。
- en: Letting \(\mathbf{w} := \overline{\mathbf{w}}^{L}\), the final output is
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 令 \(\mathbf{w} := \overline{\mathbf{w}}^{L}\)，最终的输出是
- en: \[ \bfh(\mathbf{w}) = \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}). \]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfh(\mathbf{w}) = \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}). \]
- en: Expanding out the composition, this can be written alternatively as
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 展开组合，这可以写成另一种形式
- en: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
- en: Again, we do not make the dependence on \(\mathbf{x}\) explicit.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们没有明确表示对 \(\mathbf{x}\) 的依赖。
- en: 'As a final step, we have a loss function \(\ell : \mathbb{R}^{n_{L+1}} \to
    \mathbb{R}\) which takes as input the output of the last layer and measures the
    fit to the given label \(\mathbf{y} \in \Delta_K\). We will see some example below.
    The final function is then'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '作为最后一步，我们有一个损失函数 \(\ell : \mathbb{R}^{n_{L+1}} \to \mathbb{R}\)，它接受最后一层的输出并衡量与给定标签
    \(\mathbf{y} \in \Delta_K\) 的拟合度。我们将在下面看到一些示例。最终的函数是'
- en: \[ f(\mathbf{w}) = \ell(\bfh(\mathbf{w})) \in \mathbb{R}. \]
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = \ell(\bfh(\mathbf{w})) \in \mathbb{R}. \]
- en: We seek to compute the gradient of \(f(\mathbf{w})\) with respect to the parameters
    \(\mathbf{w}\) in order to apply a gradient descent method.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求计算 \(f(\mathbf{w})\) 关于参数 \(\mathbf{w}\) 的梯度，以便应用梯度下降法。
- en: '**EXAMPLE:** **(continued)** We return to the running example from the previous
    subsection. That is, \(\bfg_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_{i} \mathbf{z}_i\)
    where the entries of \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1} \times n_i}\) are
    considered parameters and we let \(\mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T)\).
    Assume also that \(\ell : \mathbb{R}^K \to \mathbb{R}\) is defined as'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（继续）** 我们回到前一小节中的运行示例。也就是说，\(\bfg_i(\mathbf{z}_i, \mathbf{w}_i) =
    \mathcal{W}_{i} \mathbf{z}_i\) 其中 \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1} \times
    n_i}\) 的元素被视为参数，我们令 \(\mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T)\)。还假设 \(\ell
    : \mathbb{R}^K \to \mathbb{R}\) 定义为'
- en: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
- en: for a fixed, known vector \(\mathbf{y} \in \mathbb{R}^{K}\).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个固定的、已知的向量 \(\mathbf{y} \in \mathbb{R}^{K}\)。
- en: Computing \(f\) recursively gives
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 递归计算 \(f\) 给出
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{O}_0(\overline{\mathbf{w}}^0)
    = \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_0 = \mathcal{W}_{0}
    \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{O}_1(\overline{\mathbf{w}}^1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_1 = \mathcal{W}_{1} \mathcal{W}_{0}
    \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{O}_{L-1}(\overline{\mathbf{w}}^{L-1})
    = \bfg_{L-1}(\mathbf{z}_{L-1}, \mathbf{w}_{L-1}) = \mathcal{W}_{L-1} \mathbf{z}_{L-1}
    = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}) = \bfg_{L}(\mathbf{z}_{L},
    \mathbf{w}_{L}) = \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}})
    = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 = \frac{1}{2}\left\|\mathbf{y}
    - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\right\|^2.
    \end{align*}\]
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{O}_0(\overline{\mathbf{w}}^0)
    = \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_0 = \mathcal{W}_{0}
    \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{O}_1(\overline{\mathbf{w}}^1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_1 = \mathcal{W}_{1} \mathcal{W}_{0}
    \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{O}_{L-1}(\overline{\mathbf{w}}^{L-1})
    = \bfg_{L-1}(\mathbf{z}_{L-1}, \mathbf{w}_{L-1}) = \mathcal{W}_{L-1} \mathbf{z}_{L-1}
    = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}) = \bfg_{L}(\mathbf{z}_{L},
    \mathbf{w}_{L}) = \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}})
    = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 = \frac{1}{2}\left\|\mathbf{y}
    - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\right\|^2.
    \end{align*}\]
- en: \(\lhd\)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**Applying the chain rule** Recall that the key insight from the *Chain Rule*
    is that to compute the gradient of a composition such as \(\bfh(\mathbf{w})\)
    – no matter how complex – it suffices to *separately* compute the Jacobians of
    the intervening functions and then take *matrix products*. In this section, we
    compute the necessary Jacobians in the progressive case.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用链式法则** 回想一下，链式法则的关键洞察是，为了计算复合函数如 \(\bfh(\mathbf{w})\) 的梯度——无论其多么复杂——只需要分别计算中间函数的雅可比矩阵，然后进行矩阵乘法。在本节中，我们将计算渐进情况下的必要雅可比矩阵。'
- en: It will be convenient to re-write the basic composition step as
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 将基本复合步骤重新写为将方便
- en: \[ \mathcal{O}_{i}(\overline{\mathbf{w}}^{i}) = \bfg_{i}(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i}) = \bfg_{i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i})) \in \mathbb{R}^{n_{i+1}},
    \]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{O}_{i}(\overline{\mathbf{w}}^{i}) = \bfg_{i}(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i}) = \bfg_{i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i})) \in \mathbb{R}^{n_{i+1}},
    \]
- en: where the input to layer \(i+1\) (both layer-specific parameters and the output
    of the previous layer) is
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其中层 \(i+1\) 的输入（既包括层特定参数也包括前一层输出）是
- en: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i} \right) \in \mathbb{R}^{n_{i} + r_{i}}, \]
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i} \right) \in \mathbb{R}^{n_{i} + r_{i}}, \]
- en: for \(i = 1, \ldots, L\). When \(i=0\), we have simply
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = 1, \ldots, L\)。当 \(i=0\) 时，我们有
- en: \[ \mathcal{I}_{0}(\overline{\mathbf{w}}^{0}) = \left(\mathbf{z}_0, \mathbf{w}_0
    \right) = \left(\mathbf{x}, \mathbf{w}_0 \right). \]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{I}_{0}(\overline{\mathbf{w}}^{0}) = \left(\mathbf{z}_0, \mathbf{w}_0
    \right) = \left(\mathbf{x}, \mathbf{w}_0 \right). \]
- en: Applying the *Chain Rule* we get
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 应用链式法则我们得到
- en: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = J_{\bfg_i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i}))
    \,J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}). \]
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = J_{\bfg_i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i}))
    \,J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}). \]
- en: First, the Jacobian of
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，的雅可比矩阵
- en: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_i \right) \]
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_i \right) \]
- en: has a simple block diagonal structure
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 具有简单的分块对角结构
- en: \[\begin{split} J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1}) & 0 \\ 0 & I_{r_i \times r_i}
    \end{pmatrix} \in \mathbb{R}^{(n_{i} + r_{i})\times(r_0 + \cdots + r_i)} \end{split}\]
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1}) & 0 \\ 0 & I_{r_i \times r_i}
    \end{pmatrix} \in \mathbb{R}^{(n_{i} + r_{i})\times(r_0 + \cdots + r_i)} \end{split}\]
- en: since the first block component of \(\mathcal{I}_{i}\), \(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1})\),
    does not depend on \(\mathbf{w}_i\) whereas the second block component of \(\mathcal{I}_{i}\),
    \(\mathbf{w}_i\), does not depend on \(\overline{\mathbf{w}}^{i-1}\). Observe
    that this is a fairly large matrix whose number of columns in particular grows
    with \(i\). That last formula is for \(i \geq 1\). When \(i=0\) we have \(\mathcal{I}_{0}(\overline{\mathbf{w}}^{0})
    = \left(\mathbf{x}, \mathbf{w}_0\right)\), so that
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathcal{I}_{i}\) 的第一个块组件 \(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1})\)
    不依赖于 \(\mathbf{w}_i\)，而 \(\mathcal{I}_{i}\) 的第二个块组件 \(\mathbf{w}_i\) 不依赖于 \(\overline{\mathbf{w}}^{i-1}\)。观察到一个相当大的矩阵，其列数特别是随着
    \(i\) 的增长而增长。最后一个公式适用于 \(i \geq 1\)。当 \(i=0\) 时，我们有 \(\mathcal{I}_{0}(\overline{\mathbf{w}}^{0})
    = \left(\mathbf{x}, \mathbf{w}_0\right)\)，所以
- en: \[\begin{split} J_{\mathcal{I}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    \mathbf{0}_{d \times r_0} \\ I_{r_0 \times r_0} \end{pmatrix} \in \mathbb{R}^{(d+
    r_0) \times r_0}. \end{split}\]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{I}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    \mathbf{0}_{d \times r_0} \\ I_{r_0 \times r_0} \end{pmatrix} \in \mathbb{R}^{(d+
    r_0) \times r_0}. \end{split}\]
- en: We partition the Jacobian of \(\bfg_i(\mathbf{z}_i, \mathbf{w}_i)\) likewise,
    that is, we divide it into those columns corresponding to partial derivatives
    with respect to \(\mathbf{z}_{i}\) (the corresponding block being denoted by \(A_i\))
    and with respect to \(\mathbf{w}_i\) (the corresponding block being denoted by
    \(B_i\))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样将 \(\bfg_i(\mathbf{z}_i, \mathbf{w}_i)\) 的雅可比矩阵进行划分，即，我们将它划分为对应于对 \(\mathbf{z}_{i}\)
    的偏导数（相应的块用 \(A_i\) 表示）和对应于 \(\mathbf{w}_i\) 的偏导数（相应的块用 \(B_i\) 表示）
- en: \[ J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) = \begin{pmatrix} A_i & B_i \end{pmatrix}
    \in \mathbb{R}^{n_{i+1} \times (n_i + r_i)}, \]
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) = \begin{pmatrix} A_i & B_i \end{pmatrix}
    \in \mathbb{R}^{n_{i+1} \times (n_i + r_i)}, \]
- en: evaluated at \((\mathbf{z}_i, \mathbf{w}_i) = \mathcal{I}_{i}(\overline{\mathbf{w}}^{i})
    = (\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}), \mathbf{w}_i)\). Note that
    \(A_i\) and \(B_i\) depend on the details of the function \(\bfg_i\), which typically
    is fairly simple. We give examples in the next subsections.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 \((\mathbf{z}_i, \mathbf{w}_i) = \mathcal{I}_{i}(\overline{\mathbf{w}}^{i})
    = (\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}), \mathbf{w}_i)\) 处评估。注意，\(A_i\)
    和 \(B_i\) 取决于函数 \(\bfg_i\) 的细节，该函数通常相对简单。我们将在下一节给出示例。
- en: Plugging back above we get
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述内容代入，我们得到
- en: \[\begin{split} J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    A_i & B_i \end{pmatrix} \,\begin{pmatrix} J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & 0 \\ 0 & I_{r_i \times r_i} \end{pmatrix}. \end{split}\]
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    A_i & B_i \end{pmatrix} \,\begin{pmatrix} J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & 0 \\ 0 & I_{r_i \times r_i} \end{pmatrix}. \end{split}\]
- en: This leads to the recursion
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致递归
- en: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix} A_i \, J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & B_i \end{pmatrix} \in \mathbb{R}^{n_{i+1}\times(r_0 + \cdots + r_i)} \]
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix} A_i \, J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & B_i \end{pmatrix} \in \mathbb{R}^{n_{i+1}\times(r_0 + \cdots + r_i)} \]
- en: from which the Jacobian of \(\mathbf{h}(\mathbf{w})\) can be computed. Like
    \(J_{\mathcal{I}_{i}}\), \(J_{\mathcal{O}_{i}}\) is a large matrix. We refer to
    this matrix equation as the *fundamental recursion*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以计算出 \(\mathbf{h}(\mathbf{w})\) 的雅可比矩阵。像 \(J_{\mathcal{I}_{i}}\) 一样，\(J_{\mathcal{O}_{i}}\)
    也是一个大矩阵。我们将这个矩阵方程称为 *基本递归*。
- en: The base case \(i=0\) is
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基本情况 \(i=0\) 是
- en: \[\begin{split} J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    A_0 & B_0 \end{pmatrix}\begin{pmatrix} \mathbf{0}_{d \times r_0} \\ I_{r_0 \times
    r_0} \end{pmatrix} = B_0. \end{split}\]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    A_0 & B_0 \end{pmatrix}\begin{pmatrix} \mathbf{0}_{d \times r_0} \\ I_{r_0 \times
    r_0} \end{pmatrix} = B_0. \end{split}\]
- en: Finally, using the *Chain Rule* again
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，再次使用**链式法则**。
- en: \[\begin{align*} \nabla {f(\mathbf{w})} &= J_{f}(\mathbf{w})^T\\ &= [J_{\ell}(\bfh(\mathbf{w}))
    \,J_{\bfh}(\mathbf{w})]^T\\ &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\mathcal{O}_{L}(\overline{\mathbf{w}}^{L})).
    \end{align*}\]
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f(\mathbf{w})} &= J_{f}(\mathbf{w})^T\\ &= [J_{\ell}(\bfh(\mathbf{w}))
    \,J_{\bfh}(\mathbf{w})]^T\\ &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\mathcal{O}_{L}(\overline{\mathbf{w}}^{L})).
    \end{align*}\]
- en: The matrix \(J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})\) is computed using
    the recursion above, while \(\nabla {\ell}\) depends on the function \(\ell\).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})\) 是使用上述递归方法计算的，而 \(\nabla
    {\ell}\) 取决于函数 \(\ell\)。
- en: '**Backpropagation** \(\idx{backpropagation}\xdi\) We take advantage of the
    fundamental recursion to compute the gradient of \(\bfh\). As we have seen, there
    are two ways of doing this. Applying the recursion directly is one of them, but
    it requires many matrix-matrix products. The first few steps are'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播** \(\idx{backpropagation}\xdi\) 我们利用基本的递归方法来计算 \(\bfh\) 的梯度。正如我们所见，有两种方法可以做到这一点。直接应用递归是其中之一，但它需要许多矩阵-矩阵乘法。最初的几步是'
- en: \[ J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = B_0, \]\[ J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    = \begin{pmatrix} A_1 J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) & B_1 \end{pmatrix}
    \]\[ J_{\mathcal{O}_{2}}(\overline{\mathbf{w}}^{2}) = \begin{pmatrix} A_2 \, J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    & B_2 \end{pmatrix}, \]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = B_0, \]\[ J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    = \begin{pmatrix} A_1 J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) & B_1 \end{pmatrix}
    \]\[ J_{\mathcal{O}_{2}}(\overline{\mathbf{w}}^{2}) = \begin{pmatrix} A_2 \, J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    & B_2 \end{pmatrix}, \]
- en: and so on.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。
- en: Instead, as in the case of differentiating with respect to the input \(\mathbf{x}\),
    one can also run the recursion backwards. The latter approach can be much faster
    because, as we detail next, it involves only matrix-vector products. Start from
    the end, that is, with the equation
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，正如对输入 \(\mathbf{x}\) 求导的情况一样，也可以反向运行递归。后一种方法可能要快得多，因为，正如我们下面将要详细说明的，它只涉及矩阵-向量乘积。从末尾开始，即从以下方程开始：
- en: \[ \nabla {f}(\mathbf{w}) = J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w})).
    \]
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla {f}(\mathbf{w}) = J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w})).
    \]
- en: Note that \(\nabla {\ell}(\bfh(\mathbf{w}))\) is a vector – not a matrix. Then
    expand the matrix \(J_{\bfh}(\mathbf{w})\) using the recursion above
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\nabla {\ell}(\bfh(\mathbf{w}))\) 是一个向量——不是一个矩阵。然后使用上述递归方法扩展矩阵 \(J_{\bfh}(\mathbf{w})\)。
- en: \[\begin{align*} \nabla {f}(\mathbf{w}) &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= \begin{pmatrix} A_L \, J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1}) &
    B_L \end{pmatrix}^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    A_L^T \\ B_L^T \end{pmatrix} \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f}(\mathbf{w}) &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= \begin{pmatrix} A_L \, J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1}) &
    B_L \end{pmatrix}^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    A_L^T \\ B_L^T \end{pmatrix} \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
- en: The key is that both expressions \(A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\)
    and \(B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\) are *matrix-vector products*.
    That pattern persists at the next level of recursion. Note that this supposes
    that we have precomputed \(\bfh(\mathbf{w})\) first.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，两个表达式 \(A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\) 和 \(B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\)
    都是**矩阵-向量乘积**。这种模式在递归的下一个层次上持续存在。请注意，这假设我们首先已经预计算了 \(\bfh(\mathbf{w})\)。
- en: At the next level, we expand the matrix \(J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T\)
    using the fundamental recursion
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个层次，我们使用基本的递归方法扩展矩阵 \(J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T\)。
- en: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))
    \end{pmatrix}\\ &= \begin{pmatrix} \begin{pmatrix} A_{L-1} \, J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})
    & B_{L-1} \end{pmatrix}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    \begin{pmatrix} J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\,A_{L-1}^T
    \\ B_{L-1}^T \end{pmatrix} \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\left\{A_{L-1}^T \left\{ A_L^T
    \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \\ B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}.
    \end{align*}\]
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))
    \end{pmatrix}\\ &= \begin{pmatrix} \begin{pmatrix} A_{L-1} \, J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})
    & B_{L-1} \end{pmatrix}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    \begin{pmatrix} J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\,A_{L-1}^T
    \\ B_{L-1}^T \end{pmatrix} \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\left\{A_{L-1}^T \left\{ A_L^T
    \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \\ B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}.
    \end{align*}\]
- en: Continuing by induction gives an alternative formula for the gradient of \(f\).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过归纳法继续推导给出了 \(f\) 的梯度的另一种公式。
- en: Indeed, the next level gives
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，下一级给出
- en: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-3}}(\overline{\mathbf{w}}^{L-3})\left\{A_{L-2}^T
    \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}\right\}
    \\ B_{L-2}^T \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}
    \\ B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-3}}(\overline{\mathbf{w}}^{L-3})\left\{A_{L-2}^T
    \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}\right\}
    \\ B_{L-2}^T \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}
    \\ B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
- en: and so on. Observe that we do not in fact need to compute the large matrices
    \(J_{\mathcal{O}_{i}}\) – only the sequence of vectors \(B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\),
    \(B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\), \(B_{L-2}^T
    \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\},
    \)etc.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。观察发现，实际上我们并不需要计算大的矩阵 \(J_{\mathcal{O}_{i}}\) ——只需要计算向量序列 \(B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\), \(B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\),
    \(B_{L-2}^T \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\},
    \)等等。
- en: These formulas may seem cumbersome, but they take an intuitive form. Matrix
    \(A_i\) is the submatrix of the Jacobian \(J_{\bfg_i}\) corresponding only to
    the partial derivatives with respect to \(\mathbf{z}_i\), i.e., the input from
    the previous layer. Matrix \(B_i\) is the submatrix of the Jacobian \(J_{\bfg_i}\)
    corresponding only to the partial derivatives with respect to \(\mathbf{w}_i\),
    i.e., the layer-specific parameters. To compute the subvector of \(\nabla f\)
    corresponding to the parameters \(\mathbf{w}_i\) of the \((i+1)\)-th layer, we
    repeatedly differentiate with respect to the inputs of the previous layer (by
    multiplying by the corresponding \(A_j^T\)) starting from the last one, until
    we reach layer \(i+1\) at which point we take partial derivatives with respect
    to the layer-specific parameters (by multiplying by \(B_i^T\)). The process stops
    there since the layers preceding it do not depend on \(\mathbf{w}_i\) and therefore
    its full effect on \(f\) has been accounted for.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些公式可能看起来有些繁琐，但它们具有直观的形式。矩阵 \(A_i\) 是雅可比矩阵 \(J_{\bfg_i}\) 的子矩阵，仅对应于对 \(\mathbf{z}_i\)
    的偏导数，即来自前一层的输入。矩阵 \(B_i\) 是雅可比矩阵 \(J_{\bfg_i}\) 的子矩阵，仅对应于对 \(\mathbf{w}_i\) 的偏导数，即特定层的参数。为了计算与
    \((i+1)\) 层的参数 \(\mathbf{w}_i\) 对应的 \(\nabla f\) 的子向量，我们从最后一个开始，通过乘以相应的 \(A_j^T\)
    对前一层的输入进行反复求导，直到达到 \(i+1\) 层，此时我们通过乘以 \(B_i^T\) 对特定层的参数进行偏导。这个过程在这里停止，因为它之前的层不依赖于
    \(\mathbf{w}_i\)，因此其完全影响 \(f\) 已经被考虑在内。
- en: In other words, we need to compute
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们需要计算
- en: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\]
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w})), \]
- en: and
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbf{q}_{L} := B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})), \]
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_{L} := B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})), \]
- en: then
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L} = A_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\} \]
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L} = A_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\} \]
- en: and
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbf{q}_{L-1} := B_{L-1}^T \mathbf{p}_{L} = B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}, \]
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_{L-1} := B_{L-1}^T \mathbf{p}_{L} = B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}, \]
- en: then
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1} = A_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \]
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1} = A_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \]
- en: and
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbf{q}_{L-2} := B_{L-2}^T \mathbf{p}_{L-1} = B_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \right\}, \]
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_{L-2} := B_{L-2}^T \mathbf{p}_{L-1} = B_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \right\}, \]
- en: and so on. The \(\mathbf{p}_i\)s are referred to as adjoints; they correspond
    to the vectors of partial derivatives of \(f\) with respect to the \(\mathbf{z}_i\)s.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。这些 \(\mathbf{p}_i\) 被称为伴随项；它们对应于 \(f\) 对 \(\mathbf{z}_i\) 的偏导数向量。
- en: There is one more detail to note. The matrices \(A_i, B_i\) depend on the output
    of layer \(i-1\). To compute them, we first proceed forward, that is, we let \(\mathbf{z}_0
    = \mathbf{x}\) then
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个细节是。矩阵 \(A_i, B_i\) 依赖于层 \(i-1\) 的输出。为了计算它们，我们首先进行正向传播，即，我们令 \(\mathbf{z}_0
    = \mathbf{x}\) 然后
- en: \[ \mathbf{z}_1 = \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_0(\mathbf{z}_0,
    \mathbf{w}_0), \]\[ \mathbf{z}_2 = \mathcal{O}_{1}(\overline{\mathbf{w}}^{1})
    = \bfg_1(\mathcal{O}_{0}(\overline{\mathbf{w}}^{0}), \mathbf{w}_1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1), \]
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}_1 = \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_0(\mathbf{z}_0,
    \mathbf{w}_0), \]\[ \mathbf{z}_2 = \mathcal{O}_{1}(\overline{\mathbf{w}}^{1})
    = \bfg_1(\mathcal{O}_{0}(\overline{\mathbf{w}}^{0}), \mathbf{w}_1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1), \]
- en: and so on. In that forward pass, we also compute \(A_i, B_i\) along the way.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。在正向传播过程中，我们也会沿途计算 \(A_i, B_i\)。
- en: We give the full algorithm now, which involves two passes. In the forward pass,
    or forward propagation step, we compute the following.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在给出完整的算法，它涉及两个过程。在正向传播或正向传播步骤中，我们计算以下内容。
- en: '*Initialization:*'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化:*'
- en: \[\mathbf{z}_0 := \mathbf{x}\]
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbf{z}_0 := \mathbf{x}\]
- en: '*Forward layer loop:* For \(i = 0, 1,\ldots,L\),'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*正向层循环:* 对于 \(i = 0, 1,\ldots,L\)，'
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i, \mathbf{w}_i)\\ \begin{pmatrix}
    A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) \end{align*}\]
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i, \mathbf{w}_i)\\ \begin{pmatrix}
    A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) \end{align*}\]
- en: '*Loss:*'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失:*'
- en: \[\begin{align*} z_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1} &:= \nabla
    {\ell}(\mathbf{z}_{L+1}). \end{align*}\]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} z_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1} &:= \nabla
    {\ell}(\mathbf{z}_{L+1}). \end{align*}\]
- en: In the backward pass, or backpropagation step, we compute the following.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播或反向传播步骤中，我们计算以下内容。
- en: '*Backward layer loop:* For \(i = L,\ldots,1, 0\),'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向层循环:* 对于 \(i = L,\ldots,1, 0\)，'
- en: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1}\\ \mathbf{q}_{i}
    &:= B_i^T \mathbf{p}_{i+1} \end{align*}\]
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1}\\ \mathbf{q}_{i}
    &:= B_i^T \mathbf{p}_{i+1} \end{align*}\]
- en: '*Output:*'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出:*'
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_L). \]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_L). \]
- en: Note that we do not in fact need to compute \(A_0\) and \(\mathbf{p}_0\).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，实际上我们不需要计算 \(A_0\) 和 \(\mathbf{p}_0\)。
- en: '**EXAMPLE:** **(continued)** We apply the algorithm to our running example.
    From previous calculations, for \(i = 0, 1,\ldots,L\), the Jacobians are'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** **(继续)** 我们将算法应用于我们的运行示例。从前面的计算中，对于 \(i = 0, 1,\ldots,L\)，雅可比矩阵是'
- en: '\[\begin{align*} J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) &= \begin{pmatrix}
    \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}\\
    &= \begin{pmatrix} \mathcal{W}_i & I_{n_{i+1} \times n_{i+1}} \otimes \mathbf{z}_i^T
    \end{pmatrix}\\ &=: \begin{pmatrix} A_i & B_i \end{pmatrix} \end{align*}\]'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{align*} J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) &= \begin{pmatrix}
    \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}\\
    &= \begin{pmatrix} \mathcal{W}_i & I_{n_{i+1} \times n_{i+1}} \otimes \mathbf{z}_i^T
    \end{pmatrix}\\ &=: \begin{pmatrix} A_i & B_i \end{pmatrix} \end{align*}\]'
- en: and
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}} - \mathbf{y})^T. \]
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}} - \mathbf{y})^T. \]
- en: Using the *Properties of the Kronecker Product*, we obtain
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 利用**克罗内克积的性质**，我们得到
- en: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L} &:= B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) = (I_{n_{L+1} \times n_{L+1}} \otimes \mathbf{z}_L^T)^T
    (\hat{\mathbf{y}} - \mathbf{y}) = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_L\\
    &= (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L}
    = \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*}
    \mathbf{q}_{L-1} &:= B_{L-1}^T \mathbf{p}_{L} = (I_{n_{L} \times n_{L}} \otimes
    \mathbf{z}_{L-1}^T)^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-1}\\ &= \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-2} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1}
    = \mathcal{W}_{L-2}^T \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} -
    \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L-2} &:= B_{L-2}^T \mathbf{p}_{L-1}
    = (I_{n_{L-1} \times n_{L-1}} \otimes \mathbf{z}_{L-2}^T)^T \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-2}\\ &= \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-3} \cdots
    \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} \end{align*}\]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L} &:= B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) = (I_{n_{L+1} \times n_{L+1}} \otimes \mathbf{z}_L^T)^T
    (\hat{\mathbf{y}} - \mathbf{y}) = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_L\\
    &= (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L}
    = \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*}
    \mathbf{q}_{L-1} &:= B_{L-1}^T \mathbf{p}_{L} = (I_{n_{L} \times n_{L}} \otimes
    \mathbf{z}_{L-1}^T)^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-1}\\ &= \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-2} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1}
    = \mathcal{W}_{L-2}^T \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} -
    \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L-2} &:= B_{L-2}^T \mathbf{p}_{L-1}
    = (I_{n_{L-1} \times n_{L-1}} \otimes \mathbf{z}_{L-2}^T)^T \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-2}\\ &= \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-3} \cdots
    \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} \end{align*}\]
- en: and so on. Following the pattern, the last step is
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。按照模式，最后一步是
- en: \[ \mathbf{p}_1 := \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[ \mathbf{q}_0 := B_{0}^T \mathbf{p}_{1} =
    \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}}
    - \mathbf{y}) \otimes \mathbf{x}. \]
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_1 := \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[ \mathbf{q}_0 := B_{0}^T \mathbf{p}_{1} =
    \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}}
    - \mathbf{y}) \otimes \mathbf{x}. \]
- en: These calculations are consistent with the case \(L=1\) that we derived previously
    (check it!). \(\lhd\)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算与我们之前推导的 \(L=1\) 的情况一致（检查一下！）。\(\lhd\)
- en: '**CHAT & LEARN** The efficiency of backpropagation has been key to the success
    of deep learning. Ask your favorite AI chatbot about the history of backpropagation
    and its role in the development of modern deep learning. \(\ddagger\)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 反向传播的效率对深度学习的成功至关重要。向您最喜欢的 AI 聊天机器人询问反向传播的历史及其在现代深度学习发展中的作用。
    \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(在 Claude, Gemini 和 ChatGPT 的帮助下)*'
- en: '**1** In the backpropagation algorithm, what does the ‘forward pass’ compute?'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在反向传播算法中，‘前向传递’计算了什么？'
- en: a) The adjoints \(\mathbf{p}_i\) for each layer \(i\).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: a) 每一层 \(i\) 的伴随 \(\mathbf{p}_i\)。
- en: b) The gradients \(\mathbf{q}_i\) for the parameters of each layer \(i\).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: b) 每一层 \(i\) 参数的梯度 \(\mathbf{q}_i\)。
- en: c) The function values \(\mathbf{z}_i\) and the Jacobians \(A_i, B_i\) for each
    layer \(i\).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: c) 每一层 \(i\) 的函数值 \(\mathbf{z}_i\) 和雅可比矩阵 \(A_i, B_i\)。
- en: d) The final gradient \(\nabla f(\mathbf{w})\) with respect to all parameters.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: d) 关于所有参数的最终梯度 \(\nabla f(\mathbf{w})\)。
- en: '**2** What is the purpose of the ‘backward pass’ in the backpropagation algorithm?'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 反向传播算法中‘反向传递’的目的是什么？'
- en: a) To compute the function values \(\mathbf{z}_i\) for each layer \(i\) from
    the input \(\mathbf{x}\).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: a) 从输入 \(\mathbf{x}\) 计算每一层 \(i\) 的函数值 \(\mathbf{z}_i\)。
- en: b) To compute the Jacobians \(A_i, B_i\) for each layer \(i\) using the fundamental
    recursion.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: b) 使用基本递归计算每一层 \(i\) 的雅可比矩阵 \(A_i, B_i\)。
- en: c) To compute the adjoints \(\mathbf{p}_i\) and the gradients \(\mathbf{q}_i\)
    for each layer \(i\) using the fundamental recursion.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: c) 使用基本递归计算每层 \(i\) 的伴随矩阵 \(\mathbf{p}_i\) 和梯度 \(\mathbf{q}_i\)。
- en: d) To compute the final output \(\ell(\mathbf{z}_{L+1})\) of the progressive
    function.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: d) 为了计算渐进函数的最终输出 \(\ell(\mathbf{z}_{L+1})\)。
- en: '**3** What is the computational complexity of the backpropagation algorithm
    in terms of the number of layers \(L\) and the matrix dimensions \(m\)?'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 后向传播算法在层数 \(L\) 和矩阵维度 \(m\) 方面的计算复杂度是什么？'
- en: a) \(\approx Lm\)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\approx Lm\)
- en: b) \(\approx Lm^2\)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\approx Lm^2\)
- en: c) \(\approx Lm^2d\)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\approx Lm^2d\)
- en: d) \(\approx Lm^3d\)
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\approx Lm^3d\)
- en: '**4** In the context of progressive functions, what is the significance of
    the matrices \(A_i\) and \(B_i\)?'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在渐进函数的背景下，矩阵 \(A_i\) 和 \(B_i\) 的意义是什么？'
- en: a) They represent the Jacobians of the layer functions with respect to the inputs
    and parameters, respectively.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它们分别代表了层函数相对于输入和参数的雅可比矩阵。
- en: b) They are the intermediate values computed during the forward pass.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它们是前向传播过程中计算的中间值。
- en: c) They are the adjoints used in the backpropagation algorithm.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它们是在后向传播算法中使用的伴随矩阵。
- en: d) They are the matrices of parameters for each layer.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它们是每层的参数矩阵。
- en: '**5** In the context of progressive functions, which of the following best
    describes the role of the vector \(\mathbf{w}_i\)?'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在渐进函数的背景下，以下哪个选项最能描述向量 \(\mathbf{w}_i\) 的作用？'
- en: a) The input to the \(i\)-th layer.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: a) 第 \(i\) 层的输入。
- en: b) The output of the \(i\)-th layer.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: b) 第 \(i\) 层的输出。
- en: c) The parameters specific to the \(i\)-th layer.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: c) 第 \(i\) 层特有的参数。
- en: d) The concatenation of parameters from all layers up to \(i\).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: d) 将所有层到 \(i\) 的参数连接起来。
- en: 'Answer for 1: c. Justification: The section presents the forward propagation
    step which computes “the following: Initialization: \(\mathbf{z}_0 := \mathbf{x}\)
    Forward layer loop: For \(i=0,1,\dots,L\), \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i,
    \mathbf{w}_i)\) \((A_i,B_i) := J_{\mathbf{g}_i}(\mathbf{z}_i, \mathbf{w}_i)\)
    Loss: \(\mathbf{z}_{L+2} := \ell(\mathbf{z}_{L+1})\)”'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：c. 理由：本节介绍了前向传播步骤，该步骤计算“以下内容：初始化：\(\mathbf{z}_0 := \mathbf{x}\) 前向层循环：对于
    \(i=0,1,\dots,L\)，\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i)\)
    \((A_i,B_i) := J_{\mathbf{g}_i}(\mathbf{z}_i, \mathbf{w}_i)\) 损失：\(\mathbf{z}_{L+2}
    := \ell(\mathbf{z}_{L+1})\)”
- en: 'Answer for 2: c. Justification: The backward pass is described as follows:
    “Backward layer loop: For \(i=L,\dots,1,0\), \(\mathbf{p}_i := A_i^T \mathbf{p}_{i+1}\)
    \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1}\) Output: \(\nabla f(\mathbf{w}) = (\mathbf{q}_0,
    \mathbf{q}_1, \dots, \mathbf{q}_L)\).”'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：c. 理由：反向传播过程描述如下：“反向层循环：对于 \(i=L,\dots,1,0\)，\(\mathbf{p}_i := A_i^T \mathbf{p}_{i+1}\)
    \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1}\) 输出：\(\nabla f(\mathbf{w}) = (\mathbf{q}_0,
    \mathbf{q}_1, \dots, \mathbf{q}_L)\)。”
- en: 'Answer for 3: b. Justification: The text derives that the number of operations
    in the reverse mode is approximately \(2Lm^2\), stating “This is approximately
    \(2Lm^2\) – which can be much smaller than \(2Lm^2d\)!”'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：b. 理由：文本推导出反向模式的操作数大约是 \(2Lm^2\)，并指出“这大约是 \(2Lm^2\) – 这可以比 \(2Lm^2d\)
    小得多！”
- en: 'Answer for 4: a. Justification: The text defines \(A_i\) and \(B_i\) as the
    blocks of the Jacobian \(J_{\mathbf{g}_i}(\mathbf{z}_i, \mathbf{w}_i)\) corresponding
    to the partial derivatives with respect to \(\mathbf{z}_i\) and \(\mathbf{w}_i\),
    respectively.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：a. 理由：文本将 \(A_i\) 和 \(B_i\) 定义为雅可比矩阵 \(J_{\mathbf{g}_i}(\mathbf{z}_i,
    \mathbf{w}_i)\) 对应于相对于 \(\mathbf{z}_i\) 和 \(\mathbf{w}_i\) 的偏导数的块。
- en: 'Answer for 5: c. Justification: The text explains: “In the machine learning
    context, each “layer” \(\mathbf{g}_i\) has parameters (in our running example,
    there were the entries of \(\mathcal{W}_i\)) and we seek to optimize with respect
    to those parameters.”'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：c. 理由：文本解释道：“在机器学习背景下，每个“层”\(\mathbf{g}_i\)都有参数（在我们的例子中，是\(\mathcal{W}_i\)的条目），我们试图优化这些参数。”
- en: 8.3.1\. Forward v. backward[#](#forward-v-backward "Link to this heading")
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.1\. 前向传播与后向传播[#](#forward-v-backward "链接到本标题")
- en: 'We begin with a fixed-parameter example to illustrate the issues. Suppose \(f
    : \mathbb{R}^d \to \mathbb{R}\) can be expressed as a composition of \(L+1\) vector-valued
    functions \(\bfg_i : \mathbb{R}^{n_i} \to \mathbb{R}^{n_{i+1}}\) and a real-valued
    function \(\ell : \mathbb{R}^{n_{L+1}} \to \mathbb{R}\) as follows'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从一个固定参数的例子开始，以说明问题。假设 \(f : \mathbb{R}^d \to \mathbb{R}\) 可以表示为 \(L+1\) 个向量值函数
    \(\bfg_i : \mathbb{R}^{n_i} \to \mathbb{R}^{n_{i+1}}\) 和一个实值函数 \(\ell : \mathbb{R}^{n_{L+1}}
    \to \mathbb{R}\) 的组合，如下所示'
- en: \[ f(\mathbf{x}) = \ell \circ \bfg_{L} \circ \bfg_{L-1} \circ \cdots \circ \bfg_1
    \circ \bfg_0(\mathbf{x}) = \ell(\bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))).
    \]
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}) = \ell \circ \bfg_{L} \circ \bfg_{L-1} \circ \cdots \circ \bfg_1
    \circ \bfg_0(\mathbf{x}) = \ell(\bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))).
    \]
- en: Here \(n_0 = d\) is the input dimension. We also let \(n_{L+1} = K\) be the
    output dimension. Think of
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 \(n_0 = d\) 是输入维度。我们还让 \(n_{L+1} = K\) 成为输出维度。考虑
- en: \[ h(\mathbf{x}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))
    \]
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h(\mathbf{x}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x}))\cdots))
    \]
- en: as a prediction function (i.e., a regression or classification function) and
    think of \(\ell\) as a loss function.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预测函数（即回归或分类函数），并将 \(\ell\) 视为损失函数。
- en: 'Observe first that the function \(f\) itself is straighforward to compute recursively
    *starting from the inside* as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 首先观察函数 \(f\) 本身可以通过以下方式递归地从内部开始计算：
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0)\\
    \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1)\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1})\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L})\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}). \end{align*}\]
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0)\\
    \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1)\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1})\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L})\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}). \end{align*}\]
- en: Anticipating the setting of neural networks, our main application of interest,
    we refer to \(\mathbf{z}_0 = \mathbf{x}\) as the “input layer”, \(\hat{\mathbf{y}}
    = \mathbf{z}_{L+1} = \bfg_{L}(\mathbf{z}_{L})\) as the “output layer”, and \(\mathbf{z}_{1}
    = \bfg_0(\mathbf{z}_0), \ldots, \mathbf{z}_L = \bfg_{L-1}(\mathbf{z}_{L-1})\)
    as the “hidden layers”. In particular, \(L\) is the number of hidden layers.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 预测神经网络（我们的主要应用兴趣）的设置，我们将 \(\mathbf{z}_0 = \mathbf{x}\) 称为“输入层”，\(\hat{\mathbf{y}}
    = \mathbf{z}_{L+1} = \bfg_{L}(\mathbf{z}_{L})\) 称为“输出层”，以及 \(\mathbf{z}_{1} =
    \bfg_0(\mathbf{z}_0), \ldots, \mathbf{z}_L = \bfg_{L-1}(\mathbf{z}_{L-1})\) 称为“隐藏层”。特别是，\(L\)
    是隐藏层的数量。
- en: '**EXAMPLE:** We will use the following running example throughout this subsection.
    We assume that each \(\bfg_i\) is a linear map, that is, \(\bfg_i(\mathbf{z}_i)
    = \mathcal{W}_{i} \mathbf{z}_i\) where \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1}
    \times n_i}\) is a fixed, known matrix. Assume also that \(\ell : \mathbb{R}^K
    \to \mathbb{R}\) is defined as'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 我们将在本小节中始终使用以下运行示例。我们假设每个 \(\bfg_i\) 是一个线性映射，即 \(\bfg_i(\mathbf{z}_i)
    = \mathcal{W}_{i} \mathbf{z}_i\)，其中 \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1}
    \times n_i}\) 是一个固定、已知的矩阵。还假设 \(\ell : \mathbb{R}^K \to \mathbb{R}\) 被定义为'
- en: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
- en: for a fixed, known vector \(\mathbf{y} \in \mathbb{R}^{K}\).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个固定的、已知的向量 \(\mathbf{y} \in \mathbb{R}^{K}\)。
- en: Computing \(f\) recursively *starting from the inside* as above gives here
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 递归地计算 \(f\)，从内部开始，如上所述，给出以下结果
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{W}_{0}
    \mathbf{z}_0 = \mathcal{W}_{0} \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{W}_{1} \mathbf{z}_1
    = \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{W}_{L-1}
    \mathbf{z}_{L-1} = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}) = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 =
    \frac{1}{2}\left\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x}\right\|^2. \end{align*}\]
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{W}_{0}
    \mathbf{z}_0 = \mathcal{W}_{0} \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{W}_{1} \mathbf{z}_1
    = \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{W}_{L-1}
    \mathbf{z}_{L-1} = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\
    \hat{\mathbf{y}} := \mathbf{z}_{L+1} &:= \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x})
    &:= \ell(\hat{\mathbf{y}}) = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 =
    \frac{1}{2}\left\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x}\right\|^2. \end{align*}\]
- en: In essence, we are comparing an observed outcome \(\mathbf{y}\) to a prediction
    \(\mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\)
    based on input \(\mathbf{x}\).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们是在比较基于输入 \(\mathbf{x}\) 的预测 \(\mathcal{W}_{L} \mathcal{W}_{L-1} \cdots
    \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\) 与观察到的结果 \(\mathbf{y}\)。
- en: In this section, we look into computing the gradient with respect to \(\mathbf{x}\).
    (In reality, we will be more interested in taking the gradient with respect to
    the parameters, i.e., the entries of the matrices \(\mathcal{W}_{0}, \ldots, \mathcal{W}_{L}\),
    a task to which we will come back later in this section. We will also be interested
    in more complex – in particular, non-linear – prediction functions.) \(\lhd\)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何计算相对于 \(\mathbf{x}\) 的梯度。（实际上，我们更感兴趣的是计算相对于参数的梯度，即矩阵 \(\mathcal{W}_{0},
    \ldots, \mathcal{W}_{L}\) 的元素，我们将在本节的后面部分回到这个任务。我们还将对更复杂的——特别是非线性的——预测函数感兴趣。）\(\lhd\)
- en: '**NUMERICAL CORNER:** To make things more concrete, we consider a specific
    example. We will use [`torch.linalg.vector_norm`](https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html)
    to compute the Euclidean norm in PyTorch. Suppose \(d=3\), \(L=1\), \(n_1 = 2\),
    and \(K = 2\) with the following choices:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 为了使事情更加具体，我们考虑一个特定的例子。我们将使用 \[`torch.linalg.vector_norm`](https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html)
    在 PyTorch 中计算欧几里得范数。假设 \(d=3\)，\(L=1\)，\(n_1 = 2\)，和 \(K = 2\)，我们有以下选择：'
- en: '[PRE39]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: \(\unlhd\)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Forward mode** \(\idx{forward mode}\xdi\) We are ready to apply the *Chain
    Rule*'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**正向模式** \(\idx{forward mode}\xdi\) 我们准备应用**链式法则**'
- en: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\mathbf{z}_{L+1}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x})
    \]
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\mathbf{z}_{L+1}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x})
    \]
- en: where the \(\mathbf{z}_{i}\)s are as above and we used that \(\mathbf{z}_0 =
    \mathbf{x}\). The matrix product here is well-defined. Indeed, the size of \(J_{g_i}(\mathbf{z}_i)\)
    is \(n_{i+1} \times n_{i}\) (i.e., number of outputs by number of inputs) while
    the size of \(J_{g_{i-1}}(\mathbf{z}_{i-1})\) is \(n_{i} \times n_{i-1}\) – so
    the dimensions are compatible.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，\(\mathbf{z}_{i}\) 如上所述，我们使用了 \(\mathbf{z}_0 = \mathbf{x}\)。这里的矩阵乘积是有定义的。确实，\(J_{g_i}(\mathbf{z}_i)\)
    的大小是 \(n_{i+1} \times n_{i}\)（即输出数量乘以输入数量），而 \(J_{g_{i-1}}(\mathbf{z}_{i-1})\)
    的大小是 \(n_{i} \times n_{i-1}\)——因此维度是兼容的。
- en: 'So it is straighforward to compute \(\nabla f(\mathbf{x})^T\) recursively as
    we did for \(f\) itself. In fact, we can compute both simultaneously. This is
    called the forward mode:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像对 \(f\) 本身那样，我们可以递归地计算 \(\nabla f(\mathbf{x})^T\)。实际上，我们可以同时计算这两个。这被称为正向模式：
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0),
    \quad F_0 := J_{\bfg_0}(\mathbf{z}_0)\\ \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1),
    \quad F_1 := J_{\bfg_1}(\mathbf{z}_1)\, F_0\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1}),
    \quad F_{L-1} := J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}), \quad F_{L} := J_{\bfg_{L}}(\mathbf{z}_{L})\,
    F_{L-1}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}}), \quad \nabla f(\mathbf{x})^T
    := J_{\ell}(\hat{\mathbf{y}}) F_L. \end{align*}\]
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \bfg_0(\mathbf{z}_0),
    \quad F_0 := J_{\bfg_0}(\mathbf{z}_0)\\ \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1),
    \quad F_1 := J_{\bfg_1}(\mathbf{z}_1)\, F_0\\ \vdots\\ \mathbf{z}_L &:= \bfg_{L-1}(\mathbf{z}_{L-1}),
    \quad F_{L-1} := J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}), \quad F_{L} := J_{\bfg_{L}}(\mathbf{z}_{L})\,
    F_{L-1}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}}), \quad \nabla f(\mathbf{x})^T
    := J_{\ell}(\hat{\mathbf{y}}) F_L. \end{align*}\]
- en: '**EXAMPLE:** **(continued)** We apply this procedure to the running example.
    The Jacobian of the linear map \(\bfg_i(\mathbf{z}_i) = \mathcal{W}_{i} \mathbf{z}_i\)
    is the matrix \(\mathcal{W}_{i}\), as we have seen in a previous example. That
    is, \(J_{\bfg_i}(\mathbf{z}_i) = \mathcal{W}_{i}\) for any \(\mathbf{z}_i\). To
    compute the Jacobian of \(\ell\), we rewrite it as a quadratic function'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（继续）** 我们将此过程应用于运行示例。线性映射 \(\bfg_i(\mathbf{z}_i) = \mathcal{W}_{i}
    \mathbf{z}_i\) 的雅可比矩阵是矩阵 \(\mathcal{W}_{i}\)，正如我们在之前的例子中所看到的。也就是说，对于任何 \(\mathbf{z}_i\)，\(J_{\bfg_i}(\mathbf{z}_i)
    = \mathcal{W}_{i}\)。为了计算 \(\ell\) 的雅可比矩阵，我们将它重写为一个二次函数'
- en: \[\begin{align*} \ell(\hat{\mathbf{y}}) &= \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\
    &= \frac{1}{2} \mathbf{y}^T\mathbf{y} - \frac{1}{2} 2 \mathbf{y}^T\hat{\mathbf{y}}
    + \frac{1}{2}\hat{\mathbf{y}}^T \hat{\mathbf{y}}\\ &= \frac{1}{2} \hat{\mathbf{y}}^T
    I_{n_{L+1} \times n_{L+1}}\hat{\mathbf{y}} + (-\mathbf{y})^T\hat{\mathbf{y}} +
    \frac{1}{2} \mathbf{y}^T\mathbf{y}. \end{align*}\]
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \ell(\hat{\mathbf{y}}) &= \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\
    &= \frac{1}{2} \mathbf{y}^T\mathbf{y} - \frac{1}{2} 2 \mathbf{y}^T\hat{\mathbf{y}}
    + \frac{1}{2}\hat{\mathbf{y}}^T \hat{\mathbf{y}}\\ &= \frac{1}{2} \hat{\mathbf{y}}^T
    I_{n_{L+1} \times n_{L+1}}\hat{\mathbf{y}} + (-\mathbf{y})^T\hat{\mathbf{y}} +
    \frac{1}{2} \mathbf{y}^T\mathbf{y}. \end{align*}\]
- en: From a previous example,
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的例子中，
- en: \[ J_\ell(\hat{\mathbf{y}})^T = \nabla \ell(\hat{\mathbf{y}}) = \frac{1}{2}\left[I_{n_{L+1}
    \times n_{L+1}} + I_{n_{L+1} \times n_{L+1}}^T\right]\, \hat{\mathbf{y}} + (-\mathbf{y})
    = \hat{\mathbf{y}} - \mathbf{y}. \]
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_\ell(\hat{\mathbf{y}})^T = \nabla \ell(\hat{\mathbf{y}}) = \frac{1}{2}\left[I_{n_{L+1}
    \times n_{L+1}} + I_{n_{L+1} \times n_{L+1}}^T\right]\, \hat{\mathbf{y}} + (-\mathbf{y})
    = \hat{\mathbf{y}} - \mathbf{y}. \]
- en: Putting it all together, we get
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们得到
- en: \[\begin{align*} F_0 &:= J_{\bfg_0}(\mathbf{z}_0) = \mathcal{W}_{0}\\ F_1 &:=
    J_{\bfg_1}(\mathbf{z}_1)\, F_0 = \mathcal{W}_{1} F_0 = \mathcal{W}_{1} \mathcal{W}_{0}\\
    \vdots\\ F_{L-1} &:= J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2} = \mathcal{W}_{L-1}
    F_{L-2}= \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ F_{L} &:=
    J_{\bfg_{L}}(\mathbf{z}_{L})\, F_{L-1} = \mathcal{W}_{L} F_{L-1} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ \nabla f(\mathbf{x})^T
    &:= J_{\ell}(\hat{\mathbf{y}}) F_L = (\hat{\mathbf{y}} - \mathbf{y})^T F_L = (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\
    &= (\mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}.
    \end{align*}\]
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} F_0 &:= J_{\bfg_0}(\mathbf{z}_0) = \mathcal{W}_{0}\\ F_1 &:=
    J_{\bfg_1}(\mathbf{z}_1)\, F_0 = \mathcal{W}_{1} F_0 = \mathcal{W}_{1} \mathcal{W}_{0}\\
    \vdots\\ F_{L-1} &:= J_{\bfg_{L-1}}(\mathbf{z}_{L-1})\, F_{L-2} = \mathcal{W}_{L-1}
    F_{L-2}= \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ F_{L} &:=
    J_{\bfg_{L}}(\mathbf{z}_{L})\, F_{L-1} = \mathcal{W}_{L} F_{L-1} = \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\ \nabla f(\mathbf{x})^T
    &:= J_{\ell}(\hat{\mathbf{y}}) F_L = (\hat{\mathbf{y}} - \mathbf{y})^T F_L = (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}\\
    &= (\mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}
    - \mathbf{y})^T \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}.
    \end{align*}\]
- en: \(\lhd\)
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**NUMERICAL CORNER:** We return to our concrete example. Using `.T` to convert
    a column vector into a row vector throws an error in PyTorch, as it is meant to
    be used only on 2D tensors. Instead, one can use [`torch.unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html).
    Below, `(z2 - y).unsqueeze(0)` adds a dimension to `z2 - y`, making it a 2D tensor
    with shape \((1, 2)\).'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落:** 我们回到我们的具体例子。在PyTorch中使用 `.T` 将列向量转换为行向量会引发错误，因为它仅适用于2D张量。相反，可以使用
    `torch.unsqueeze`。下面，`(z2 - y).unsqueeze(0)` 向 `z2 - y` 添加一个维度，使其成为一个形状为 \((1,
    2)\) 的2D张量。'
- en: '[PRE47]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We can check that we get the same outcome using AD.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用AD来验证我们得到相同的结果。
- en: '[PRE53]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: \(\unlhd\)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**KNOWLEDGE CHECK:** Obtain that last expression directly by taking the gradient
    of'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 通过对以下表达式求梯度来直接获得最后一个表达式'
- en: \[ f(\mathbf{x}) = \frac{1}{2}\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2. \]
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}) = \frac{1}{2}\|\mathbf{y} - \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2. \]
- en: \(\checkmark\)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: '**Reverse mode** \(\idx{reverse mode}\xdi\) What we just described corresponds
    to performing the matrix products in the *Chain Rule* formula'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向模式** \(\idx{reverse mode}\xdi\) 我们刚才描述的对应于在 *链式法则* 公式中执行矩阵乘法'
- en: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\hat{\mathbf{y}}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x}).
    \]
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{x})^T = J_{f}(\mathbf{x}) = J_{\ell}(\hat{\mathbf{y}}) J_{\bfg_L}(\mathbf{z}_L)
    J_{\bfg_{L-1}}(\mathbf{z}_{L-1}) \cdots J_{\bfg_1}(\mathbf{z}_1) J_{\bfg_0}(\mathbf{x}).
    \]
- en: '*from right to left*. But there is another way to do this – from left to right!
    This is called reverse mode. First we compute \(f\) recursively as we did previously,
    in particular we compute the \(\mathbf{z}_i\)s, then going backwards we compute'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '*从右到左*。但还有另一种方法来做这件事——从左到右！这被称为反向模式。首先，我们像之前一样递归地计算 \(f\)，特别是我们计算 \(\mathbf{z}_i\)，然后反向计算'
- en: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}})\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L})\\
    &\vdots\\ G_1 &:= G_2 \, J_{\bfg_1}(\mathbf{z}_1)\\ \nabla f(\mathbf{x})^T &:=
    G_1 \, J_{\bfg_0}(\mathbf{z}_0) \end{align*}\]
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}})\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L})\\
    &\vdots\\ G_1 &:= G_2 \, J_{\bfg_1}(\mathbf{z}_1)\\ \nabla f(\mathbf{x})^T &:=
    G_1 \, J_{\bfg_0}(\mathbf{z}_0) \end{align*}\]
- en: This may seem like a rather minor modification to our previous algorithm. It
    is not – it is much faster! To see this, we need to count the number of elementary
    operations involved in implementing each mode.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来是对我们之前算法的微小修改。实际上并非如此——它要快得多！为了看到这一点，我们需要计算实现每种模式所涉及的基本操作的数量。
- en: 'First a simple calcultion: How many elementary operations (i.e., additions
    and multiplications) does it take to multiply an \(n \times m\) matrix \(A = (a_{ij})_{i,j}\)
    and an \(m \times \ell\) matrix \(B= (b_{ij})_{i,j}\)? Let’s ask ChatGPT. Here’s
    the answer:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 首先进行一个简单的计算：要乘以一个 \(n \times m\) 的矩阵 \(A = (a_{ij})_{i,j}\) 和一个 \(m \times \ell\)
    的矩阵 \(B= (b_{ij})_{i,j}\)，需要多少基本运算（即加法和乘法）？让我们问问 ChatGPT。这是答案：
- en: “To calculate the number of elementary operations required to multiply an \(n
    \times m\) matrix by an \(m \times \ell\) matrix, consider the computation involved
    for each element of the resulting \(n \times \ell\) matrix.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: “为了计算将一个 \(n \times m\) 的矩阵乘以一个 \(m \times \ell\) 的矩阵所需的基本运算数，考虑结果 \(n \times
    \ell\) 矩阵每个元素的运算过程。
- en: 'Each element of the resulting matrix is the dot product of a row from the first
    matrix and a column from the second matrix. If the first matrix is denoted as
    \(A\) and the second matrix as \(B\), then the element at position \(i, j\) in
    the resulting matrix \(C\) is calculated as:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵的每个元素是第一个矩阵的一行与第二个矩阵的一列的点积。如果第一个矩阵表示为 \(A\)，第二个矩阵表示为 \(B\)，那么结果矩阵 \(C\)
    中位置 \(i, j\) 的元素计算如下：
- en: \[ c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \ldots + a_{im}b_{mj} \]
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: \[ c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \ldots + a_{im}b_{mj} \]
- en: This computation involves \(m\) multiplications and \(m - 1\) additions, making
    a total of \(m + (m - 1) = 2m - 1\) elementary operations for each element.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算涉及 \(m\) 次乘法和 \(m - 1\) 次加法，因此每个元素总共需要 \(m + (m - 1) = 2m - 1\) 个基本运算。
- en: 'Since matrix \(C\) has \(n \times \ell\) elements, the total number of elementary
    operations required for the entire matrix \(C\) is:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵 \(C\) 有 \(n \times \ell\) 个元素，整个矩阵 \(C\) 所需的基本运算总数为：
- en: \[ (n \ell)(2m - 1) = 2nm\ell - n\ell \]
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (n \ell)(2m - 1) = 2nm\ell - n\ell \]
- en: Thus, multiplying an \(n \times m\) matrix by an \(m \times \ell\) matrix takes
    \(2nm\ell - n\ell\) elementary operations.”
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，乘以一个 \(n \times m\) 的矩阵和一个 \(m \times \ell\) 的矩阵需要 \(2nm\ell - n\ell\) 个基本运算。”
- en: Returning to computing the gradient, we count the number of elementary operations
    needed for each mode. To simplify the calculation, we assume that \(n_0 = d\)
    and \(n_1 = n_2 = \cdots = n_{L} = m\) and \(n_{L+1} = K\).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到计算梯度，我们计算每个模式的所需基本运算数。为了简化计算，我们假设 \(n_0 = d\)，\(n_1 = n_2 = \cdots = n_{L}
    = m\)，\(n_{L+1} = K\)。
- en: '*Forward:* The matrix \(F_0 = J_{\bfg_0}(\mathbf{z}_0)\) has dimensions \(m
    \times d\). The matrix \(F_1\), as a product of \(J_{\bfg_1}(\mathbf{z}_1) \in
    \mathbb{R}^{m \times m}\) and \(F_0 \in \mathbb{R}^{m \times d}\) has dimensions
    \(m \times d\); it therefore takes \(m (2m-1) d\) operations to compute. The same
    holds for \(F_2, \ldots, F_{L-1}\) (check it!). By similar considerations, the
    matrix \(F_L\) has dimensions \(K \times d\) and takes \(K (2m-1) d\) operations
    to compute. Finally, \(\nabla f(\mathbf{x})^T = J_{\ell}(\mathbf{z}_{L+1}) F_L
    \in \mathbb{R}^{1 \times d}\) and takes \((2K-1) d\) operations to compute. Overall
    the number of operations is'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '*正向计算:* 矩阵 \(F_0 = J_{\bfg_0}(\mathbf{z}_0)\) 的维度为 \(m \times d\)。矩阵 \(F_1\)，作为
    \(J_{\bfg_1}(\mathbf{z}_1) \in \mathbb{R}^{m \times m}\) 和 \(F_0 \in \mathbb{R}^{m
    \times d}\) 的乘积，其维度为 \(m \times d\)；因此，计算它需要 \(m (2m-1) d\) 次运算。对于 \(F_2, \ldots,
    F_{L-1}\) 也是如此（检查一下！）通过类似的考虑，矩阵 \(F_L\) 的维度为 \(K \times d\)，计算它需要 \(K (2m-1) d\)
    次运算。最后，\(\nabla f(\mathbf{x})^T = J_{\ell}(\mathbf{z}_{L+1}) F_L \in \mathbb{R}^{1
    \times d}\) 并需要 \((2K-1) d\) 次运算来计算。总体上，运算次数为'
- en: \[ (L-1) m (2m-1) d + K (2m-1) d + (2K-1) d. \]
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (L-1) m (2m-1) d + K (2m-1) d + (2K-1) d. \]
- en: This is approximately \(2 L m^2 d\) if we think of \(K\) as a small constant
    and ignore the smaller order terms.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将 \(K\) 视为一个小的常数并忽略较小的阶数项，这大约是 \(2 L m^2 d\)。
- en: '*Reverse:* The matrix \(G_{L+1} = J_{\ell}(\mathbf{z}_{L+1})\) has dimensions
    \(1 \times K\). The matrix \(G_{L}\), as a product of \(G_{L+1} \in \mathbb{R}^{1
    \times K}\) and \(J_{g_{L}}(\mathbf{z}_{L}) \in \mathbb{R}^{K \times m}\) has
    dimensions \(1 \times m\); it therefore takes \((2K-1) m\) operations to compute.
    The matrix \(G_{L-1}\), as a product of \(G_{L} \in \mathbb{R}^{1 \times m}\)
    and \(J_{g_{L-1}}(\mathbf{z}_{L-1}) \in \mathbb{R}^{m \times m}\) has dimensions
    \(1 \times m\); it therefore takes \((2m-1) m\) operations to compute. The same
    holds for \(G_{L-2}, \ldots, G_{1}\) (check it!). By similar considerations, \(\nabla
    f(\mathbf{x})^T = G_1 \, J_{\bfg_0}(\mathbf{z}_0) \in \mathbb{R}^{1 \times d}\)
    and takes \((2m-1) d\) operations to compute. Overall the number of operations
    is'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '*逆向：* 矩阵 \(G_{L+1} = J_{\ell}(\mathbf{z}_{L+1})\) 的维度为 \(1 \times K\)。矩阵 \(G_{L}\)，作为
    \(G_{L+1} \in \mathbb{R}^{1 \times K}\) 和 \(J_{g_{L}}(\mathbf{z}_{L}) \in \mathbb{R}^{K
    \times m}\) 的乘积，其维度为 \(1 \times m\)；因此需要 \((2K-1) m\) 次操作来计算。矩阵 \(G_{L-1}\)，作为
    \(G_{L} \in \mathbb{R}^{1 \times m}\) 和 \(J_{g_{L-1}}(\mathbf{z}_{L-1}) \in \mathbb{R}^{m
    \times m}\) 的乘积，其维度为 \(1 \times m\)；因此需要 \((2m-1) m\) 次操作来计算。对于 \(G_{L-2}, \ldots,
    G_{1}\) 也是如此（请检查！）通过类似的考虑，\(\nabla f(\mathbf{x})^T = G_1 \, J_{\bfg_0}(\mathbf{z}_0)
    \in \mathbb{R}^{1 \times d}\) 并需要 \((2m-1) d\) 次操作来计算。总体上，操作次数为'
- en: \[ (2K-1) m + (L-1) (2m-1) m + (2m-1) d. \]
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (2K-1) m + (L-1) (2m-1) m + (2m-1) d. \]
- en: This is approximately \(2 L m^2 + 2 m d\) – which can be much smaller than \(2
    L m^2 d\)! In other words, the reverse mode approach can be much faster. Note
    in particular that all computations in the reverse mode are matrix-vector products
    (or more precisely row vector-matrix products) rather than matrix-matrix products.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约是 \(2 L m^2 + 2 m d\) – 这可以比 \(2 L m^2 d\) 小得多！换句话说，逆向模式方法可以快得多。特别是请注意，逆向模式中的所有计算都是矩阵-向量乘法（或者更精确地说，是行向量-矩阵乘法），而不是矩阵-矩阵乘法。
- en: '**EXAMPLE:** **(continued)** We apply the reverse mode approach to our previous
    example. We get'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（继续）** 我们将逆向模式方法应用于之前的示例。我们得到'
- en: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L}) = G_{L+1} \mathcal{W}_{L}
    = (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L} \\ \vdots\\ G_1 &:= G_2 \,
    J_{\bfg_1}(\mathbf{z}_1) = G_2 \mathcal{W}_{1} = [(\hat{\mathbf{y}} - \mathbf{y})^T
    \mathcal{W}_{L} \cdots \mathcal{W}_{2}] \mathcal{W}_{1} \\ \nabla f(\mathbf{x})^T
    &:= G_1 \, J_{\bfg_0}(\mathbf{z}_0) = [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L}
    \cdots \mathcal{W}_{2}\mathcal{W}_{1}] \mathcal{W}_{0}\\ &= (\mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} - \mathbf{y})^T \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}, \end{align*}\]
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} G_{L+1} &:= J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ G_{L} &:= G_{L+1}\,J_{g_{L}}(\mathbf{z}_{L}) = G_{L+1} \mathcal{W}_{L}
    = (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L} \\ \vdots\\ G_1 &:= G_2 \,
    J_{\bfg_1}(\mathbf{z}_1) = G_2 \mathcal{W}_{1} = [(\hat{\mathbf{y}} - \mathbf{y})^T
    \mathcal{W}_{L} \cdots \mathcal{W}_{2}] \mathcal{W}_{1} \\ \nabla f(\mathbf{x})^T
    &:= G_1 \, J_{\bfg_0}(\mathbf{z}_0) = [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{L}
    \cdots \mathcal{W}_{2}\mathcal{W}_{1}] \mathcal{W}_{0}\\ &= (\mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} - \mathbf{y})^T \mathcal{W}_{L}
    \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0}, \end{align*}\]
- en: which matches our previous calculation. Note that all computations involve multiplying
    a row vector by a matrix. \(\lhd\)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们的先前计算相匹配。请注意，所有计算都涉及行向量与矩阵的乘法。 \(\lhd\)
- en: '**NUMERICAL CORNER:** We try our specific example.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角：** 我们尝试我们的特定示例。'
- en: '[PRE55]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We indeed obtain the same answer yet again.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实再次得到了相同的答案。
- en: \(\unlhd\)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 'To provide a little more insight in the savings obtained through the reverse
    mode, consider the following simple calculations. Let \(A, B \in \mathbb{R}^{n
    \times n}\) and \(\mathbf{v} \in \mathbb{R}^n\). Suppose we seek to compute \(\mathbf{v}^T
    B A\). By [associativity](https://en.wikipedia.org/wiki/Associative_property)
    of matrix multiplication, there are two ways of doing this: compute \(\mathbf{v}^{T}(BA)\)
    (i.e., first compute \(BA\) then multiply by \(\mathbf{v}^T\); or compute \((\mathbf{v}^T
    B) A\). The first approach requires \(n^2(2n-1) + n(2n-1)\) operations, while
    the second only requires \(2n(2n-1)\). The latter is much smaller since \(2 n^3\)
    (the leading term in the first approach) grows much faster than \(4 n^2\) (the
    leading term in the second approach) when \(n\) is large.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地了解通过反向模式获得的节省，考虑以下简单的计算。设 \(A, B \in \mathbb{R}^{n \times n}\) 和 \(\mathbf{v}
    \in \mathbb{R}^n\)。假设我们想要计算 \(\mathbf{v}^T B A\)。通过矩阵乘法的 [结合律](https://en.wikipedia.org/wiki/Associative_property)，有两种方法可以做到这一点：计算
    \(\mathbf{v}^{T}(BA)\)（即，首先计算 \(BA\) 然后乘以 \(\mathbf{v}^T\)；或者计算 \((\mathbf{v}^T
    B) A\)。第一种方法需要 \(n^2(2n-1) + n(2n-1)\) 次操作，而第二种方法只需要 \(2n(2n-1)\)。后者要小得多，因为当 \(n\)
    很大时，\(2 n^3\)（第一种方法中的主导项）的增长速度比 \(4 n^2\)（第二种方法中的主导项）快得多。
- en: Why is this happening? One way to understand this is to think of the output
    \(\mathbf{v}^T B A\) as a *linear combination of the rows of \(A\)* – a very specific
    linear combination in fact. In the first approach, we compute \(BA\) which gives
    us \(n\) different linear combinations of the rows of \(A\) – none being the one
    we want – and then we compute the desired linear combination by multiplying by
    \(\mathbf{v}^T\). This is wasteful. In the second approach, we immediately compute
    the coefficients of the specific linear combination we seek – \(\mathbf{v}^T B\)
    – and then we compute that linear combination by multiplying to the right by \(A\).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会发生这种情况？理解这一点的其中一个方法是将输出 \(\mathbf{v}^T B A\) 视为 \(A\) 的行的一个 *线性组合* – 实际上是一个非常具体的线性组合。在第一种方法中，我们计算
    \(BA\)，这给我们 \(n\) 个不同的 \(A\) 的行的线性组合 – 没有一个是我们想要的 – 然后我们通过乘以 \(\mathbf{v}^T\)
    来计算所需的线性组合。这是浪费的。在第二种方法中，我们立即计算我们寻求的特定线性组合的系数 – \(\mathbf{v}^T B\) – 然后通过右乘 \(A\)
    来计算那个线性组合。
- en: While the setting we examined in this subsection is illuminating, it is not
    exactly what we want. In the machine learning context, each “layer” \(\bfg_i\)
    has parameters (in our running example, there were the entries of \(\mathcal{W}_{i}\))
    and we seek to optimize with respect to those parameters. For this, we need the
    gradient with respect to the parameters, not the input \(\mathbf{x}\). In the
    next subsection, we consider a generalization of the current setting, progressive
    functions, which will allow us to do this. The notation gets more complicated,
    but the basic idea remains the same.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本小节中考察的设置很有启发性，但它并不完全符合我们的要求。在机器学习环境中，每个“层” \(\bfg_i\) 都有参数（在我们的例子中，是 \(\mathcal{W}_{i}\)
    的项），我们寻求对这些参数进行优化。为此，我们需要关于参数的梯度，而不是输入 \(\mathbf{x}\) 的梯度。在下一个小节中，我们考虑当前设置的推广，即渐进函数，这将使我们能够做到这一点。符号变得更加复杂，但基本思想保持不变。
- en: 8.3.2\. Progressive functions[#](#progressive-functions "Link to this heading")
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3.2\. 渐进函数[#](#progressive-functions "链接到这个标题")
- en: As mentioned previously, while it may seem natural to define a prediction function
    \(h\) (e.g., a classifier) as a function of the input data \(\mathbf{x}\in \mathbb{R}^{d}\),
    when fitting data we are ultimately interested in thinking of \(h\) as a function
    of the parameters \(\mathbf{w} \in \mathbb{R}^r\) that need to be adjusted – over
    a fixed dataset. Hence, in this section, the input \(\mathbf{x}\) is fixed while
    the vector of parameters \(\mathbf{w}\) is now variable.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，虽然将预测函数 \(h\)（例如，分类器）定义为输入数据 \(\mathbf{x}\in \mathbb{R}^{d}\) 的函数看起来很自然，但在拟合数据时，我们最终感兴趣的是将
    \(h\) 视为需要调整的参数 \(\mathbf{w} \in \mathbb{R}^r\) 的函数 – 在一个固定的数据集上。因此，在本节中，输入 \(\mathbf{x}\)
    是固定的，而参数向量 \(\mathbf{w}\) 现在是可变的。
- en: '**A first example** We use the example from the previous subsection to illustrate
    the main ideas. That is, suppose \(d=3\), \(L=1\), \(n_1 = 2\), and \(K = 2\).
    Fix a data sample \(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3, \mathbf{y} =
    (y_1, y_2) \in \mathbb{R}^2\). For \(i=0, 1\), we use the notation'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个例子** 我们使用前一小节中的例子来说明主要思想。也就是说，假设 \(d=3\)，\(L=1\)，\(n_1 = 2\)，和 \(K = 2\)。固定一个数据样本
    \(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\)。对于
    \(i=0, 1\)，我们使用以下符号'
- en: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix}. \end{split}\]
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix}. \end{split}\]
- en: and let
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 并且令
- en: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2
    = \frac{1}{2}(y_1 - \hat{y}_1)^2 + \frac{1}{2}(y_2 - \hat{y}_2)^2. \]
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2
    = \frac{1}{2}(y_1 - \hat{y}_1)^2 + \frac{1}{2}(y_2 - \hat{y}_2)^2. \]
- en: 'We change the notation for the “layer” function \(\bfg_i\) to reflect the fact
    that it is now a function of two (concatenated) vectors: the input \(\mathbf{z}_i
    = (z_{i,1},\ldots,z_{i,n_i})\) from the previous layer and a layer-specific set
    of parameters \(\mathbf{w}_i\). That is,'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将“层”函数 \(\bfg_i\) 的表示法进行更改，以反映它现在是一个两个（连接的）向量的函数：来自前一层的输入 \(\mathbf{z}_i =
    (z_{i,1},\ldots,z_{i,n_i})\) 和一个特定层的参数集 \(\mathbf{w}_i\)。也就是说，
- en: \[\begin{split} \bfg_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_{i} \mathbf{z}_i
    = \begin{pmatrix} (\mathbf{w}_i^{(1)})^T\\ (\mathbf{w}_i^{(2)})^T \end{pmatrix}
    \mathbf{z}_i \end{split}\]
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bfg_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_{i} \mathbf{z}_i
    = \begin{pmatrix} (\mathbf{w}_i^{(1)})^T\\ (\mathbf{w}_i^{(2)})^T \end{pmatrix}
    \mathbf{z}_i \end{split}\]
- en: with \(\mathbf{w}_i = (\mathbf{w}_i^{(1)}, \mathbf{w}_i^{(2)})\), the concatenation
    of the rows of \(\mathcal{W}_{i}\) (as column vectors). A different way to put
    this is that
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w}_i = (\mathbf{w}_i^{(1)}, \mathbf{w}_i^{(2)})\)，这是 \(\mathcal{W}_{i}\)
    的行（作为列向量）的连接。另一种说法是
- en: \[ \mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T), \]
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T), \]
- en: where we took the transpose to turn the rows into columns. More specifically,
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们取转置是为了将行转换为列。更具体地说，
- en: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0} \quad\text{with}\quad
    \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0} \quad\text{with}\quad
    \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]
- en: (i.e., \(\mathbf{w}_0^{(1)} = (w_0, w_1, w_2)\) and \(\mathbf{w}_0^{(2)} = (w_3,
    w_4, w_5)\)) and
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: （即，\(\mathbf{w}_0^{(1)} = (w_0, w_1, w_2)\) 和 \(\mathbf{w}_0^{(2)} = (w_3, w_4,
    w_5)\)）和
- en: \[ \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1} \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9) \]
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1} \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9) \]
- en: (i.e., \(\mathbf{w}_1^{(1)} = (w_6, w_7)\) and \(\mathbf{w}_1^{(2)} = (w_8,
    w_9)\)).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: （即，\(\mathbf{w}_1^{(1)} = (w_6, w_7)\) 和 \(\mathbf{w}_1^{(2)} = (w_8, w_9)\)）。
- en: We seek to compute the gradient of
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图计算
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= \frac{1}{2} \|\mathbf{y} - \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2\\
    &= \frac{1}{2}\left(y_1 - w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4
    x_2 + w_5 x_3)\right)^2\\ & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1
    x_2 + w_2 x_3) - w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2. \end{align*}\]
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= \frac{1}{2} \|\mathbf{y} - \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\|^2\\
    &= \frac{1}{2}\left(y_1 - w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4
    x_2 + w_5 x_3)\right)^2\\ & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1
    x_2 + w_2 x_3) - w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2. \end{align*}\]
- en: by applying the *Chain Rule* backwards, as we justified in the previous subsection
    – but this time we take the gradient with respect to the parameters
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用反向的**链式法则**，正如我们在前一小节中所证明的那样——但这次我们对参数求梯度
- en: \[ \mathbf{w} := (\mathbf{w}_0, \mathbf{w}_1) = (w_0,w_1,\ldots,w_9). \]
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w} := (\mathbf{w}_0, \mathbf{w}_1) = (w_0,w_1,\ldots,w_9). \]
- en: 'Notice a key change in the notation: we now accordingly think of \(f\) *as
    a function of \(\mathbf{w}\)*; the role of \(\mathbf{x}\) is implicit.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 注意符号的一个关键变化：我们现在相应地认为 \(f\) 是 \(\mathbf{w}\) 的函数；\(\mathbf{x}\) 的作用是隐含的。
- en: On the other hand, it may seem counter-intuitive that we now think of \(\bfg_i\)
    as a function of *both* its own parameters and its inputs from the previous layer
    when we just stated that we only care about the gradient with respect to the former.
    But, as we will see, it turns out that we need the Jacobians with respect to both
    as the input from the previous layer *actually depends on the parameters of the
    previous layers*. For instance, \(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1}
    \mathbf{z}_{1}\) where \(\mathbf{z}_{1} = \bfg_0(\mathbf{z}_0, \mathbf{w}_0) =
    \mathcal{W}_{0} \mathbf{z}_{0}\).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当我们刚刚说我们只关心与前者相关的梯度时，现在将 \(\bfg_i\) 视为既与其自身参数又与前一层输入相关的函数，这似乎有些反直觉。但是，正如我们将要看到的，我们确实需要关于两者的雅可比矩阵，因为前一层的输入实际上依赖于前一层的参数。例如，\(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1}\)，其中 \(\mathbf{z}_{1} = \bfg_0(\mathbf{z}_0,
    \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0}\)。
- en: Recall that we have already computed the requisite Jacobians \(J_{\bfg_0}\)
    and \(J_{\bfg_1}\) in a previous example. We have also computed the Jacobian \(J_{\ell}\)
    of \(\ell\). At this point, it is tempting to apply the *Chain Rule* and deduce
    that the gradient of \(f\) is
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们已经在之前的例子中计算了所需的雅可比矩阵 \(J_{\bfg_0}\) 和 \(J_{\bfg_1}\)。我们还计算了 \(\ell\)
    的雅可比矩阵 \(J_{\ell}\)。在这个时候，应用**链式法则**并推断出 \(f\) 的梯度是
- en: \[ J_{\ell}(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)) \,J_{\bfg_1}(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)
    \,J_{\bfg_0}(\mathbf{x}, \mathbf{w}_0). \]
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)) \,J_{\bfg_1}(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1)
    \,J_{\bfg_0}(\mathbf{x}, \mathbf{w}_0). \]
- en: But this is not correct. For one, the dimensions do not match! For instance,
    \(J_{\bfg_0} \in \mathbb{R}^{2 \times 9}\) since \(\bfg_0\) has \(2\) outputs
    and \(9\) inputs (i.e., \(z_{0,1}, z_{0,2}, z_{0,3}, w_0, w_1, w_2, w_3, w_4,
    w_5\)) while \(J_{\bfg_1} \in \mathbb{R}^{2 \times 6}\) since \(\bfg_1\) has \(2\)
    outputs and \(6\) inputs (i.e., \(z_{1,1}, z_{1,2}, w_6, w_7, w_8, w_9\)). So
    what went wrong?
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不正确。首先，维度不匹配！例如，\(J_{\bfg_0} \in \mathbb{R}^{2 \times 9}\)，因为\(\bfg_0\)有\(2\)个输出和\(9\)个输入（即，\(z_{0,1},
    z_{0,2}, z_{0,3}, w_0, w_1, w_2, w_3, w_4, w_5\)），而\(J_{\bfg_1} \in \mathbb{R}^{2
    \times 6}\)，因为\(\bfg_1\)有\(2\)个输出和\(6\)个输入（即，\(z_{1,1}, z_{1,2}, w_6, w_7, w_8,
    w_9\)）。那么问题出在哪里呢？
- en: The function \(f\) is *not* in fact a straight composition of the functions
    \(\ell\), \(\bfg_1\), and \(\bfg_0\). Indeed the parameters to differentiate with
    respect to are introduced progressively, each layer injecting its own additional
    parameters which are not obtained from the previous layers. Hence we cannot write
    the gradient of \(f\) as a simple product the Jacobians, unlike what happend in
    the previous subsection.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 \(f\) 实际上并不是由函数 \(\ell\)、\(\bfg_1\) 和 \(\bfg_0\) 的简单组合。事实上，我们要对参数进行微分，这些参数是逐步引入的，每一层都注入它自己的附加参数，而这些参数并不是从前一层次获得的。因此，我们不能像前一小节中那样简单地将雅可比矩阵的乘积作为
    \(f\) 的梯度来写。
- en: But not all is lost. We show below that we can still apply the *Chain Rule*
    step-by-step in a way that accounts for the additional parameters on each layer.
    Taking a hint from the previous subsection, we proceed forward first to compute
    \(f\) and the Jacobians, and then go backwards to compute the gradient \(\nabla
    f\). We use the notation \(\mathbb{A}_{n}[\mathbf{x}]\) and \(\mathbb{B}_{n}[\mathbf{z}]\)
    from the background section.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非一切都已失去。我们下面将展示，我们仍然可以逐步应用**链式法则**，同时考虑到每一层的附加参数。借鉴前一小节的方法，我们首先向前计算 \(f\)
    和雅可比矩阵，然后向后计算梯度 \(\nabla f\)。我们使用背景部分中的符号 \(\mathbb{A}_{n}[\mathbf{x}]\) 和 \(\mathbb{B}_{n}[\mathbf{z}]\)。
- en: 'In the forward phase, we compute \(f\) itself and the requisite Jacobians:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向阶段，我们计算 \(f\) 本身和所需的雅可比矩阵：
- en: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0}\\ &= \begin{pmatrix}
    (\mathbf{w}_0^{(1)})^T\mathbf{x}\\ (\mathbf{w}_0^{(2)})^T\mathbf{x}\end{pmatrix}
    = \begin{pmatrix} w_0 x_1 + w_1 x_2 + w_2 x_3\\ w_3 x_1 + w_4 x_2 + w_5 x_3 \end{pmatrix}\\
    &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0]
    & \mathbb{B}_{2}[\mathbf{z}_0] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{0}
    & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix} w_0
    & w_1 & w_2 & x_1 & x_2 & x_3 & 0 & 0 & 0\\ w_3 & w_4 & w_5 & 0 & 0 & 0 & x_1
    & x_2 & x_3 \end{pmatrix} \end{align*}\]\[\begin{align*} &\hat{\mathbf{y}} :=
    \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1}\\
    &= \begin{pmatrix} w_6 z_{1,1} + w_7 z_{1,2}\\ w_8 z_{1,1} + w_9 z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7 (\mathbf{w}_0^{(2)})^T\mathbf{x}\\
    w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} \end{pmatrix}\\
    &= \begin{pmatrix} w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7(w_3 x_1 + w_4 x_2 +
    w_5 x_3)\\ w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)
    \end{pmatrix}\\ &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1]
    & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{1}
    & I_{2\times 2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} w_6
    & w_7 & z_{1,1} & z_{1,2} & 0 & 0\\ w_8 & w_9 & 0 & 0 & z_{1,1} & z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 & w_7 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    & 0 & 0\\ w_8 & w_9 & 0 & 0 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    \end{pmatrix} \end{align*}\]\[\begin{align*} &f(\mathbf{x}) := \ell(\hat{\mathbf{y}})
    = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\ &= \frac{1}{2}\left(y_1 -
    w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2\\
    & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_9(w_3 x_1
    + w_4 x_2 + w_5 x_3)\right)^2\\ &J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7
    (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_1 & w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x}
    + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_2 \end{pmatrix}. \end{align*}\]
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_{0}\\ &= \begin{pmatrix}
    (\mathbf{w}_0^{(1)})^T\mathbf{x}\\ (\mathbf{w}_0^{(2)})^T\mathbf{x}\end{pmatrix}
    = \begin{pmatrix} w_0 x_1 + w_1 x_2 + w_2 x_3\\ w_3 x_1 + w_4 x_2 + w_5 x_3 \end{pmatrix}\\
    &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0]
    & \mathbb{B}_{2}[\mathbf{z}_0] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{0}
    & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix} w_0
    & w_1 & w_2 & x_1 & x_2 & x_3 & 0 & 0 & 0\\ w_3 & w_4 & w_5 & 0 & 0 & 0 & x_1
    & x_2 & x_3 \end{pmatrix} \end{align*}\]\[\begin{align*} &\hat{\mathbf{y}} :=
    \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_{1}\\
    &= \begin{pmatrix} w_6 z_{1,1} + w_7 z_{1,2}\\ w_8 z_{1,1} + w_9 z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7 (\mathbf{w}_0^{(2)})^T\mathbf{x}\\
    w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} \end{pmatrix}\\
    &= \begin{pmatrix} w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7(w_3 x_1 + w_4 x_2 +
    w_5 x_3)\\ w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_9(w_3 x_1 + w_4 x_2 + w_5 x_3)
    \end{pmatrix}\\ &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1]
    & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix} = \begin{pmatrix} \mathcal{W}_{1}
    & I_{2\times 2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} w_6
    & w_7 & z_{1,1} & z_{1,2} & 0 & 0\\ w_8 & w_9 & 0 & 0 & z_{1,1} & z_{1,2} \end{pmatrix}\\
    &= \begin{pmatrix} w_6 & w_7 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    & 0 & 0\\ w_8 & w_9 & 0 & 0 & (\mathbf{w}_0^{(1)})^T\mathbf{x} & (\mathbf{w}_0^{(2)})^T\mathbf{x}
    \end{pmatrix} \end{align*}\]\[\begin{align*} &f(\mathbf{x}) := \ell(\hat{\mathbf{y}})
    = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2\\ &= \frac{1}{2}\left(y_1 -
    w_6(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_7(w_3 x_1 + w_4 x_2 + w_5 x_3)\right)^2\\
    & \qquad + \frac{1}{2}\left(y_2 - w_8(w_0 x_1 + w_1 x_2 + w_2 x_3) - w_9(w_3 x_1
    + w_4 x_2 + w_5 x_3)\right)^2\\ &J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}}
    - \mathbf{y})^T\\ &= \begin{pmatrix} w_6 (\mathbf{w}_0^{(1)})^T\mathbf{x} + w_7
    (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_1 & w_8 (\mathbf{w}_0^{(1)})^T\mathbf{x}
    + w_9 (\mathbf{w}_0^{(2)})^T\mathbf{x} - y_2 \end{pmatrix}. \end{align*}\]
- en: We now compute the gradient of \(f\) with respect to \(\mathbf{w}\). We start
    with \(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\). For this step, we think of \(f\)
    as the composition \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\). Here \(\mathbf{z}_1\)
    does not depend on \(\mathbf{w}_1\) and therefore can be considered fixed for
    this calculation. By the *Chain Rule*
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在计算 \(f\) 关于 \(\mathbf{w}\) 的梯度。我们首先从 \(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\)
    开始。对于这一步，我们将 \(f\) 视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 的复合函数。在这里，\(\mathbf{z}_1\)
    不依赖于 \(\mathbf{w}_1\)，因此可以认为在这个计算中是固定的。根据 **链式法则**
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_6} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_6} = \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_6} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_6} = (\hat{y}_1 - y_1) z_{1,1} \end{align*}\]
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_6} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_6} = \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_6} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_6} = (\hat{y}_1 - y_1) z_{1,1} \end{align*}\]
- en: where we used the fact that \(g_{1,2}(\mathbf{z}_1, \mathbf{w}_1) = w_8 z_{1,1}
    + w_9 z_{1,2}\) does not depend on \(w_6\) and therefore \(\frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_6} = 0\). Similarly
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(g_{1,2}(\mathbf{z}_1, \mathbf{w}_1) = w_8 z_{1,1} + w_9 z_{1,2}\)
    不依赖于 \(w_6\) 的这一事实，因此 \(\frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial
    w_6} = 0\)。同样
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_7} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_7} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_7} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_7} = (\hat{y}_1 - y_1) z_{1,2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_8} &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_8}
    =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_8} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_8} =
    (\hat{y}_2 - y_2) z_{1,1}\\ \frac{\partial f(\mathbf{w})}{\partial w_9} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_9} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_9} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_9} = (\hat{y}_2 - y_2) z_{1,2}. \end{align*}\]
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_7} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_7} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_7} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_7} = (\hat{y}_1 - y_1) z_{1,2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_8} &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_8}
    =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_8} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_8} =
    (\hat{y}_2 - y_2) z_{1,1}\\ \frac{\partial f(\mathbf{w})}{\partial w_9} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial w_9} =\frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1, \mathbf{w}_1)}{\partial w_9} +
    \frac{\partial \ell(\hat{\mathbf{y}})}{\partial \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial w_9} = (\hat{y}_2 - y_2) z_{1,2}. \end{align*}\]
- en: In matrix form, this is
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式表示，这为
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{B}_{2}[\mathbf{z}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T (I_{2\times
    2} \otimes \mathbf{z}_1^T)\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) z_{1,1} & (\hat{y}_1 - y_1) z_{1,2} & (\hat{y}_2
    - y_2) z_{1,1} & (\hat{y}_2 - y_2) z_{1,2} \end{pmatrix} \end{align*}\]
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{B}_{2}[\mathbf{z}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T (I_{2\times
    2} \otimes \mathbf{z}_1^T)\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) z_{1,1} & (\hat{y}_1 - y_1) z_{1,2} & (\hat{y}_2
    - y_2) z_{1,1} & (\hat{y}_2 - y_2) z_{1,2} \end{pmatrix} \end{align*}\]
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们在最后一行使用了**克罗内克积的性质（f）**。
- en: To compute the partial derivatives with respect to \(\mathbf{w}_0 = (w_0, w_1,
    \ldots, w_5)\), we first need to compute partial derivatives with respect to \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) since \(f\) depends on \(\mathbf{w}_0\) through it. For
    this calculation, we think again of \(f\) as the composition \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\), but this time our focus is on the variables \(\mathbf{z}_1\).
    We obtain
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算相对于 \(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\) 的偏导数，我们首先需要计算相对于 \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) 的偏导数，因为 \(f\) 通过它依赖于 \(\mathbf{w}_0\)。为此计算，我们再次将 \(f\)
    视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 的复合，但这次我们的重点是变量 \(\mathbf{z}_1\)。我们得到
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,1}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,1}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,1}}\\
    &= (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 \end{align*}\]
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,1}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,1}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,1}}\\
    &= (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 \end{align*}\]
- en: and
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,2}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,2}}\\
    &= (\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9. \end{align*}\]
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} &= \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}}\\ &= \frac{\partial
    \ell(\hat{\mathbf{y}})}{\partial \hat{y}_1} \frac{\partial g_{1,1}(\mathbf{z}_1,
    \mathbf{w}_1)}{\partial z_{1,2}} + \frac{\partial \ell(\hat{\mathbf{y}})}{\partial
    \hat{y}_2} \frac{\partial g_{1,2}(\mathbf{z}_1, \mathbf{w}_1)}{\partial z_{1,2}}\\
    &= (\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9. \end{align*}\]
- en: In matrix form, this is
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式，这可以表示为
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 & (\hat{y}_1
    - y_1) w_7 + (\hat{y}_2 - y_2) w_9 \end{pmatrix}. \end{align*}\]
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1]\\ &= (\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1\\
    &= \begin{pmatrix} (\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8 & (\hat{y}_1
    - y_1) w_7 + (\hat{y}_2 - y_2) w_9 \end{pmatrix}. \end{align*}\]
- en: The vector \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial
    f(\mathbf{w})}{\partial z_{1,2}}\right)\) is called an adjoint.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial f(\mathbf{w})}{\partial
    z_{1,2}}\right)\) 被称为伴随向量。
- en: We now compute the gradient of \(f\) with respect to \(\mathbf{w}_0 = (w_0,
    w_1, \ldots, w_5)\). For this step, we think of \(f\) as the composition of \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\) as a function of \(\mathbf{z}_1\) and \(\bfg_0(\mathbf{z}_0,
    \mathbf{w}_0)\) as a function of \(\mathbf{w}_0\). Here \(\mathbf{z}_0\) does
    not depend on \(\mathbf{w}_0\) and therefore can be considered fixed for this
    calculation. By the *Chain Rule*
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算 \(f\) 相对于 \(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\) 的梯度。为此步骤，我们将 \(f\)
    视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 作为 \(\mathbf{z}_1\) 的函数，以及 \(\bfg_0(\mathbf{z}_0,
    \mathbf{w}_0)\) 作为 \(\mathbf{w}_0\) 的函数的复合。在这里，\(\mathbf{z}_0\) 不依赖于 \(\mathbf{w}_0\)，因此可以在此计算中视为固定。根据**链式法则**
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_0} &= \frac{\partial
    \ell(\bfg_1(\bfg_0(\mathbf{z}_0, \mathbf{w}_0), \mathbf{w}_1))}{\partial w_0}\\
    &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}
    \frac{\partial g_{0,1}(\mathbf{z}_0, \mathbf{w}_0)}{\partial w_0} + \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}} \frac{\partial g_{0,2}(\mathbf{z}_0,
    \mathbf{w}_0)}{\partial w_0}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8)
    z_{0,1}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1}, \end{align*}\]
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_0} &= \frac{\partial
    \ell(\bfg_1(\bfg_0(\mathbf{z}_0, \mathbf{w}_0), \mathbf{w}_1))}{\partial w_0}\\
    &= \frac{\partial \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,1}}
    \frac{\partial g_{0,1}(\mathbf{z}_0, \mathbf{w}_0)}{\partial w_0} + \frac{\partial
    \ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))}{\partial z_{1,2}} \frac{\partial g_{0,2}(\mathbf{z}_0,
    \mathbf{w}_0)}{\partial w_0}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8)
    z_{0,1}\\ &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1}, \end{align*}\]
- en: where we used the fact that \(g_{0,2}(\mathbf{z}_0, \mathbf{w}_0) = w_3 z_{0,1}
    + w_4 z_{0,2} + w_5 z_{0,3}\) does not depend on \(w_0\) and therefore \(\frac{\partial
    g_{0,2}(\mathbf{z}_0, \mathbf{w}_0)}{\partial w_0} = 0\).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(g_{0,2}(\mathbf{z}_0, \mathbf{w}_0) = w_3 z_{0,1} + w_4 z_{0,2} +
    w_5 z_{0,3}\) 不依赖于 \(w_0\) 的性质，因此 \(\frac{\partial g_{0,2}(\mathbf{z}_0, \mathbf{w}_0)}{\partial
    w_0} = 0\).
- en: Similarly (check it!)
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 同样（检查一下！）
- en: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_1} &= ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_2} &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{3}\\ \frac{\partial
    f(\mathbf{w})}{\partial w_3} &= ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9)
    x_{1}\\ \frac{\partial f(\mathbf{w})}{\partial w_4} &= ((\hat{y}_1 - y_1) w_7
    + (\hat{y}_2 - y_2) w_9) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial w_5} &=
    ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9) x_{3}. \end{align*}\]
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial f(\mathbf{w})}{\partial w_1} &= ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial
    w_2} &= ((\hat{y}_1 - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{3}\\ \frac{\partial
    f(\mathbf{w})}{\partial w_3} &= ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9)
    x_{1}\\ \frac{\partial f(\mathbf{w})}{\partial w_4} &= ((\hat{y}_1 - y_1) w_7
    + (\hat{y}_2 - y_2) w_9) x_{2}\\ \frac{\partial f(\mathbf{w})}{\partial w_5} &=
    ((\hat{y}_1 - y_1) w_7 + (\hat{y}_2 - y_2) w_9) x_{3}. \end{align*}\]
- en: In matrix form, this is
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式表示，这是
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1] \,\mathbb{B}_{2}[\mathbf{z}_0]\\ &= (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1 (I_{2\times 2} \otimes \mathbf{z}_0^T)\\ &= ((\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1) \otimes \mathbf{x}^T\\ &= \begin{pmatrix} ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1} & \cdots & ((\hat{y}_1 - y_1) w_7 +
    (\hat{y}_2 - y_2) w_9) x_{3} \end{pmatrix} \end{align*}\]
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= J_{\ell}(\hat{\mathbf{y}})
    \,\mathbb{A}_{2}[\mathbf{w}_1] \,\mathbb{B}_{2}[\mathbf{z}_0]\\ &= (\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1 (I_{2\times 2} \otimes \mathbf{z}_0^T)\\ &= ((\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_1) \otimes \mathbf{x}^T\\ &= \begin{pmatrix} ((\hat{y}_1
    - y_1) w_6 + (\hat{y}_2 - y_2) w_8) x_{1} & \cdots & ((\hat{y}_1 - y_1) w_7 +
    (\hat{y}_2 - y_2) w_9) x_{3} \end{pmatrix} \end{align*}\]
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们在最后一行使用了克罗内克积的性质（f）。
- en: To sum up,
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，
- en: \[ \nabla f (\mathbf{w})^T = \begin{pmatrix} (\hat{\mathbf{y}} - \mathbf{y})^T
    \otimes (\mathcal{W}_{0} \mathbf{x})^T & ((\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1)
    \otimes \mathbf{x}^T \end{pmatrix}. \]
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f (\mathbf{w})^T = \begin{pmatrix} (\hat{\mathbf{y}} - \mathbf{y})^T
    \otimes (\mathcal{W}_{0} \mathbf{x})^T & ((\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_1)
    \otimes \mathbf{x}^T \end{pmatrix}. \]
- en: '**NUMERICAL CORNER:** We return to the concrete example from the previous subsection.
    This time the matrices `W0` and `W1` require partial derivatives.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们回到前一小节的具体例子。这次矩阵 `W0` 和 `W1` 需要计算偏导数。'
- en: '[PRE61]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We compute the gradient \(\nabla f(\mathbf{w})\) using AD.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用AD计算梯度 \(\nabla f(\mathbf{w})\).
- en: '[PRE69]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: These are written in the form of matrix derivatives
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是以矩阵导数的形式编写的
- en: \[\begin{split} \frac{\partial f}{\partial \mathcal{W}_0} = \begin{pmatrix}
    \frac{\partial f}{\partial w_0} & \frac{\partial f}{\partial w_1} & \frac{\partial
    f}{\partial w_2} \\ \frac{\partial f}{\partial w_3} & \frac{\partial f}{\partial
    w_4} & \frac{\partial f}{\partial w_5} \end{pmatrix} \quad\text{and}\quad \frac{\partial
    f}{\partial \mathcal{W}_1} = \begin{pmatrix} \frac{\partial f}{\partial w_6} &
    \frac{\partial f}{\partial w_7} \\ \frac{\partial f}{\partial w_8} & \frac{\partial
    f}{\partial w_9} \end{pmatrix}. \end{split}\]
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{\partial f}{\partial \mathcal{W}_0} = \begin{pmatrix}
    \frac{\partial f}{\partial w_0} & \frac{\partial f}{\partial w_1} & \frac{\partial
    f}{\partial w_2} \\ \frac{\partial f}{\partial w_3} & \frac{\partial f}{\partial
    w_4} & \frac{\partial f}{\partial w_5} \end{pmatrix} \quad\text{and}\quad \frac{\partial
    f}{\partial \mathcal{W}_1} = \begin{pmatrix} \frac{\partial f}{\partial w_6} &
    \frac{\partial f}{\partial w_7} \\ \frac{\partial f}{\partial w_8} & \frac{\partial
    f}{\partial w_9} \end{pmatrix}. \end{split}\]
- en: We use our formulas to confirm that they match these results. We need the Kronecker
    product, which in PyTorch is implemented as [`torch.kron`](https://pytorch.org/docs/stable/generated/torch.kron.html).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的公式来验证它们是否与这些结果匹配。我们需要克罗内克积，在PyTorch中通过 `torch.kron` 实现[torch.kron](https://pytorch.org/docs/stable/generated/torch.kron.html)。
- en: '[PRE74]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Observe that this time these results are written in vectorized form (i.e., obtained
    by concatenating the rows). But they do match with the AD output.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这次这些结果是以向量化的形式（即通过连接行获得）写出的。但它们与 AD 输出相匹配。
- en: \(\unlhd\)
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**General setting** \(\idx{progressive function}\xdi\) More generally, we have
    \(L+2\) layers. The input layer is \(\mathbf{z}_0 := \mathbf{x}\), which we refer
    to as layer \(0\). Hidden layer \(i\), \(i=1,\ldots,L\), is defined by a continuously
    differentiable function \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)
    which this time takes *two vector-valued inputs*: a vector \(\mathbf{z}_{i-1}
    \in \mathbb{R}^{n_{i-1}}\) fed from the \((i-1)\)-st layer and a vector \(\mathbf{w}_{i-1}
    \in \mathbb{R}^{r_{i-1}}\) of parameters specific to the \(i\)-th layer'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用设置** \(\idx{渐进函数}\xdi\) 更普遍地，我们有 \(L+2\) 层。输入层是 \(\mathbf{z}_0 := \mathbf{x}\)，我们将其称为层
    \(0\)。隐藏层 \(i\)，\(i=1,\ldots,L\)，由一个连续可微的函数 \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1},
    \mathbf{w}_{i-1})\) 定义，这次它接受 *两个向量值输入*：一个来自 \((i-1)\) 层的向量 \(\mathbf{z}_{i-1}
    \in \mathbb{R}^{n_{i-1}}\) 和一个属于 \(i\) 层的参数向量 \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)'
- en: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
- en: The output \(\mathbf{z}_i\) of \(\bfg_{i-1}\) is a vector in \(\mathbb{R}^{n_{i}}\)
    which is passed to the \((i+1)\)-st layer as input. The output layer is \(\mathbf{z}_{L+1}
    := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\), which we also refer to as layer
    \(L+1\).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bfg_{i-1}\) 的输出 \(\mathbf{z}_i\) 是一个 \(\mathbb{R}^{n_{i}}\) 中的向量，它作为输入传递给
    \((i+1)\) 层。输出层是 \(\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\)，我们也将它称为层
    \(L+1\)。
- en: For \(i = 1,\ldots,L+1\), let
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = 1,\ldots,L+1\)，设
- en: \[ \overline{\mathbf{w}}^{i-1} = (\mathbf{w}_0,\mathbf{w}_1,\ldots,\mathbf{w}_{i-1})
    \in \mathbb{R}^{r_0 + r_1+\cdots+r_{i-1}} \]
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \overline{\mathbf{w}}^{i-1} = (\mathbf{w}_0,\mathbf{w}_1,\ldots,\mathbf{w}_{i-1})
    \in \mathbb{R}^{r_0 + r_1+\cdots+r_{i-1}} \]
- en: be the concatenation of the parameters from the first \(i\) layers (not including
    the input layer, which does not have parameters) as a vector in \(\mathbb{R}^{r_0+r_1+\cdots+r_{i-1}}\).
    Then the output of layer \(i\) *as a function of the parameters* is the composition
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 是前 \(i\) 层参数（不包括没有参数的输入层）的连接，作为一个向量在 \(\mathbb{R}^{r_0+r_1+\cdots+r_{i-1}}\)
    中。然后层 \(i\) 的输出 *作为参数的函数* 是组合
- en: \[ \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}) = \bfg_{i-1}(\mathcal{O}_{i-2}(\overline{\mathbf{w}}^{i-2}),
    \mathbf{w}_{i-1}) = \bfg_{i-1}(\bfg_{i-2}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{i-2}), \mathbf{w}_{i-1}) \in \mathbb{R}^{n_{i}}, \]
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}) = \bfg_{i-1}(\mathcal{O}_{i-2}(\overline{\mathbf{w}}^{i-2}),
    \mathbf{w}_{i-1}) = \bfg_{i-1}(\bfg_{i-2}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{i-2}), \mathbf{w}_{i-1}) \in \mathbb{R}^{n_{i}}, \]
- en: for \(i = 2, \ldots, L+1\). When \(i=1\), we have simply
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = 2, \ldots, L+1\)。当 \(i=1\) 时，我们有
- en: \[ \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_{0}(\mathbf{x}, \mathbf{w}_0).
    \]
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_{0}(\mathbf{x}, \mathbf{w}_0).
    \]
- en: Observe that the function \(\mathcal{O}_{i-1}\) depends implicitly on the input
    \(\mathbf{x}\) – which we do *not* think of as a variable in this setting. To
    simplify the notation, we do not make the dependence on \(\mathbf{x}\) explicit.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到函数 \(\mathcal{O}_{i-1}\) 依赖于输入 \(\mathbf{x}\) —— 在这个设置中我们并不将其视为变量。为了简化符号，我们没有明确表示对
    \(\mathbf{x}\) 的依赖。
- en: Letting \(\mathbf{w} := \overline{\mathbf{w}}^{L}\), the final output is
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 令 \(\mathbf{w} := \overline{\mathbf{w}}^{L}\)，最终的输出是
- en: \[ \bfh(\mathbf{w}) = \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}). \]
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfh(\mathbf{w}) = \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}). \]
- en: Expanding out the composition, this can be written alternatively as
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 展开组合，这可以写成另一种形式
- en: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
- en: Again, we do not make the dependence on \(\mathbf{x}\) explicit.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们没有明确表示对 \(\mathbf{x}\) 的依赖。
- en: 'As a final step, we have a loss function \(\ell : \mathbb{R}^{n_{L+1}} \to
    \mathbb{R}\) which takes as input the output of the last layer and measures the
    fit to the given label \(\mathbf{y} \in \Delta_K\). We will see some example below.
    The final function is then'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '作为最后一步，我们有一个损失函数 \(\ell : \mathbb{R}^{n_{L+1}} \to \mathbb{R}\)，它接受最后一层的输出作为输入，并衡量与给定标签
    \(\mathbf{y} \in \Delta_K\) 的拟合度。下面我们将看到一些示例。最终的函数是'
- en: \[ f(\mathbf{w}) = \ell(\bfh(\mathbf{w})) \in \mathbb{R}. \]
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = \ell(\bfh(\mathbf{w})) \in \mathbb{R}. \]
- en: We seek to compute the gradient of \(f(\mathbf{w})\) with respect to the parameters
    \(\mathbf{w}\) in order to apply a gradient descent method.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求计算 \(f(\mathbf{w})\) 关于参数 \(\mathbf{w}\) 的梯度，以便应用梯度下降法。
- en: '**EXAMPLE:** **(continued)** We return to the running example from the previous
    subsection. That is, \(\bfg_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_{i} \mathbf{z}_i\)
    where the entries of \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1} \times n_i}\) are
    considered parameters and we let \(\mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T)\).
    Assume also that \(\ell : \mathbb{R}^K \to \mathbb{R}\) is defined as'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（继续）** 我们回到上一小节中的运行示例。也就是说，\(\bfg_i(\mathbf{z}_i, \mathbf{w}_i) =
    \mathcal{W}_{i} \mathbf{z}_i\)，其中 \(\mathcal{W}_{i} \in \mathbb{R}^{n_{i+1} \times
    n_i}\) 的元素被视为参数，我们令 \(\mathbf{w}_i = \mathrm{vec}(\mathcal{W}_{i}^T)\)。还假设 \(\ell
    : \mathbb{R}^K \to \mathbb{R}\) 被定义为'
- en: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = \frac{1}{2} \|\mathbf{y} - \hat{\mathbf{y}}\|^2,
    \]
- en: for a fixed, known vector \(\mathbf{y} \in \mathbb{R}^{K}\).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个固定的、已知的向量 \(\mathbf{y} \in \mathbb{R}^{K}\)。
- en: Computing \(f\) recursively gives
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 通过递归计算 \(f\) 得到
- en: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{O}_0(\overline{\mathbf{w}}^0)
    = \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_0 = \mathcal{W}_{0}
    \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{O}_1(\overline{\mathbf{w}}^1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_1 = \mathcal{W}_{1} \mathcal{W}_{0}
    \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{O}_{L-1}(\overline{\mathbf{w}}^{L-1})
    = \bfg_{L-1}(\mathbf{z}_{L-1}, \mathbf{w}_{L-1}) = \mathcal{W}_{L-1} \mathbf{z}_{L-1}
    = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}) = \bfg_{L}(\mathbf{z}_{L},
    \mathbf{w}_{L}) = \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}})
    = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 = \frac{1}{2}\left\|\mathbf{y}
    - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\right\|^2.
    \end{align*}\]
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_0 &:= \mathbf{x}\\ \mathbf{z}_1 &:= \mathcal{O}_0(\overline{\mathbf{w}}^0)
    = \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_{0} \mathbf{z}_0 = \mathcal{W}_{0}
    \mathbf{x}\\ \mathbf{z}_2 &:= \mathcal{O}_1(\overline{\mathbf{w}}^1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \mathcal{W}_{1} \mathbf{z}_1 = \mathcal{W}_{1} \mathcal{W}_{0}
    \mathbf{x}\\ \vdots\\ \mathbf{z}_L &:= \mathcal{O}_{L-1}(\overline{\mathbf{w}}^{L-1})
    = \bfg_{L-1}(\mathbf{z}_{L-1}, \mathbf{w}_{L-1}) = \mathcal{W}_{L-1} \mathbf{z}_{L-1}
    = \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ \hat{\mathbf{y}}
    := \mathbf{z}_{L+1} &:= \mathcal{O}_{L}(\overline{\mathbf{w}}^{L}) = \bfg_{L}(\mathbf{z}_{L},
    \mathbf{w}_{L}) = \mathcal{W}_{L} \mathbf{z}_{L} = \mathcal{W}_{L} \mathcal{W}_{L-1}
    \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\\ f(\mathbf{x}) &:= \ell(\hat{\mathbf{y}})
    = \frac{1}{2}\|\mathbf{y} - \hat{\mathbf{y}}\|^2 = \frac{1}{2}\left\|\mathbf{y}
    - \mathcal{W}_{L} \mathcal{W}_{L-1} \cdots \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x}\right\|^2.
    \end{align*}\]
- en: \(\lhd\)
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**Applying the chain rule** Recall that the key insight from the *Chain Rule*
    is that to compute the gradient of a composition such as \(\bfh(\mathbf{w})\)
    – no matter how complex – it suffices to *separately* compute the Jacobians of
    the intervening functions and then take *matrix products*. In this section, we
    compute the necessary Jacobians in the progressive case.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用链式法则** 回想一下，从**链式法则**中得到的关键洞察是，为了计算复合函数如 \(\bfh(\mathbf{w})\) 的梯度——无论其多么复杂——只需要分别计算中间函数的雅可比矩阵，然后取**矩阵乘积**。在本节中，我们计算渐进情况下的必要雅可比矩阵。'
- en: It will be convenient to re-write the basic composition step as
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 将基本复合步骤重新写为将是有用的。
- en: \[ \mathcal{O}_{i}(\overline{\mathbf{w}}^{i}) = \bfg_{i}(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i}) = \bfg_{i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i})) \in \mathbb{R}^{n_{i+1}},
    \]
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{O}_{i}(\overline{\mathbf{w}}^{i}) = \bfg_{i}(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i}) = \bfg_{i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i})) \in \mathbb{R}^{n_{i+1}},
    \]
- en: where the input to layer \(i+1\) (both layer-specific parameters and the output
    of the previous layer) is
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，层 \(i+1\) 的输入（包括层特定参数和前一层输出）是
- en: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i} \right) \in \mathbb{R}^{n_{i} + r_{i}}, \]
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_{i} \right) \in \mathbb{R}^{n_{i} + r_{i}}, \]
- en: for \(i = 1, \ldots, L\). When \(i=0\), we have simply
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = 1, \ldots, L\)。当 \(i=0\) 时，我们有
- en: \[ \mathcal{I}_{0}(\overline{\mathbf{w}}^{0}) = \left(\mathbf{z}_0, \mathbf{w}_0
    \right) = \left(\mathbf{x}, \mathbf{w}_0 \right). \]
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{I}_{0}(\overline{\mathbf{w}}^{0}) = \left(\mathbf{z}_0, \mathbf{w}_0
    \right) = \left(\mathbf{x}, \mathbf{w}_0 \right). \]
- en: Applying the *Chain Rule* we get
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 应用**链式法则**，我们得到
- en: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = J_{\bfg_i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i}))
    \,J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}). \]
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = J_{\bfg_i}(\mathcal{I}_{i}(\overline{\mathbf{w}}^{i}))
    \,J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}). \]
- en: First, the Jacobian of
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，求
- en: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_i \right) \]
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{I}_{i}(\overline{\mathbf{w}}^{i}) = \left( \mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}),
    \mathbf{w}_i \right) \]
- en: has a simple block diagonal structure
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 具有简单的分块对角结构
- en: \[\begin{split} J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1}) & 0 \\ 0 & I_{r_i \times r_i}
    \end{pmatrix} \in \mathbb{R}^{(n_{i} + r_{i})\times(r_0 + \cdots + r_i)} \end{split}\]
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{I}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1}) & 0 \\ 0 & I_{r_i \times r_i}
    \end{pmatrix} \in \mathbb{R}^{(n_{i} + r_{i})\times(r_0 + \cdots + r_i)} \end{split}\]
- en: since the first block component of \(\mathcal{I}_{i}\), \(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1})\),
    does not depend on \(\mathbf{w}_i\) whereas the second block component of \(\mathcal{I}_{i}\),
    \(\mathbf{w}_i\), does not depend on \(\overline{\mathbf{w}}^{i-1}\). Observe
    that this is a fairly large matrix whose number of columns in particular grows
    with \(i\). That last formula is for \(i \geq 1\). When \(i=0\) we have \(\mathcal{I}_{0}(\overline{\mathbf{w}}^{0})
    = \left(\mathbf{x}, \mathbf{w}_0\right)\), so that
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathcal{I}_{i}\) 的第一个块组件 \(\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1})\)
    不依赖于 \(\mathbf{w}_i\)，而 \(\mathcal{I}_{i}\) 的第二个块组件 \(\mathbf{w}_i\) 不依赖于 \(\overline{\mathbf{w}}^{i-1}\)。注意，这是一个相当大的矩阵，其列数特别是随着
    \(i\) 的增加而增长。最后一个公式适用于 \(i \geq 1\)。当 \(i=0\) 时，我们有 \(\mathcal{I}_{0}(\overline{\mathbf{w}}^{0})
    = \left(\mathbf{x}, \mathbf{w}_0\right)\)，因此
- en: \[\begin{split} J_{\mathcal{I}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    \mathbf{0}_{d \times r_0} \\ I_{r_0 \times r_0} \end{pmatrix} \in \mathbb{R}^{(d+
    r_0) \times r_0}. \end{split}\]
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{I}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    \mathbf{0}_{d \times r_0} \\ I_{r_0 \times r_0} \end{pmatrix} \in \mathbb{R}^{(d+
    r_0) \times r_0}. \end{split}\]
- en: We partition the Jacobian of \(\bfg_i(\mathbf{z}_i, \mathbf{w}_i)\) likewise,
    that is, we divide it into those columns corresponding to partial derivatives
    with respect to \(\mathbf{z}_{i}\) (the corresponding block being denoted by \(A_i\))
    and with respect to \(\mathbf{w}_i\) (the corresponding block being denoted by
    \(B_i\))
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样将 \(\bfg_i(\mathbf{z}_i, \mathbf{w}_i)\) 的雅可比矩阵进行分块，即，将其分为对应于对 \(\mathbf{z}_{i}\)
    的偏导数（对应的块用 \(A_i\) 表示）和对应于 \(\mathbf{w}_i\) 的偏导数（对应的块用 \(B_i\) 表示）的列
- en: \[ J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) = \begin{pmatrix} A_i & B_i \end{pmatrix}
    \in \mathbb{R}^{n_{i+1} \times (n_i + r_i)}, \]
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) = \begin{pmatrix} A_i & B_i \end{pmatrix}
    \in \mathbb{R}^{n_{i+1} \times (n_i + r_i)}, \]
- en: evaluated at \((\mathbf{z}_i, \mathbf{w}_i) = \mathcal{I}_{i}(\overline{\mathbf{w}}^{i})
    = (\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}), \mathbf{w}_i)\). Note that
    \(A_i\) and \(B_i\) depend on the details of the function \(\bfg_i\), which typically
    is fairly simple. We give examples in the next subsections.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在 \((\mathbf{z}_i, \mathbf{w}_i) = \mathcal{I}_{i}(\overline{\mathbf{w}}^{i})
    = (\mathcal{O}_{i-1}(\overline{\mathbf{w}}^{i-1}), \mathbf{w}_i)\) 处进行评估。注意，\(A_i\)
    和 \(B_i\) 依赖于函数 \(\bfg_i\) 的细节，通常这个函数相当简单。我们将在下一节给出例子。
- en: Plugging back above we get
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述内容代入，我们得到
- en: \[\begin{split} J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    A_i & B_i \end{pmatrix} \,\begin{pmatrix} J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & 0 \\ 0 & I_{r_i \times r_i} \end{pmatrix}. \end{split}\]
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix}
    A_i & B_i \end{pmatrix} \,\begin{pmatrix} J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & 0 \\ 0 & I_{r_i \times r_i} \end{pmatrix}. \end{split}\]
- en: This leads to the recursion
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致递归
- en: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix} A_i \, J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & B_i \end{pmatrix} \in \mathbb{R}^{n_{i+1}\times(r_0 + \cdots + r_i)} \]
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathcal{O}_{i}}(\overline{\mathbf{w}}^{i}) = \begin{pmatrix} A_i \, J_{\mathcal{O}_{i-1}}(\overline{\mathbf{w}}^{i-1})
    & B_i \end{pmatrix} \in \mathbb{R}^{n_{i+1}\times(r_0 + \cdots + r_i)} \]
- en: from which the Jacobian of \(\mathbf{h}(\mathbf{w})\) can be computed. Like
    \(J_{\mathcal{I}_{i}}\), \(J_{\mathcal{O}_{i}}\) is a large matrix. We refer to
    this matrix equation as the *fundamental recursion*.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以计算出 \(\mathbf{h}(\mathbf{w})\) 的雅可比矩阵。像 \(J_{\mathcal{I}_{i}}\) 一样，\(J_{\mathcal{O}_{i}}\)
    也是一个大矩阵。我们将这个矩阵方程称为 *基本递归*。
- en: The base case \(i=0\) is
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 基本情况 \(i=0\) 是
- en: \[\begin{split} J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    A_0 & B_0 \end{pmatrix}\begin{pmatrix} \mathbf{0}_{d \times r_0} \\ I_{r_0 \times
    r_0} \end{pmatrix} = B_0. \end{split}\]
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = \begin{pmatrix}
    A_0 & B_0 \end{pmatrix}\begin{pmatrix} \mathbf{0}_{d \times r_0} \\ I_{r_0 \times
    r_0} \end{pmatrix} = B_0. \end{split}\]
- en: Finally, using the *Chain Rule* again
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，再次使用**链式法则**
- en: \[\begin{align*} \nabla {f(\mathbf{w})} &= J_{f}(\mathbf{w})^T\\ &= [J_{\ell}(\bfh(\mathbf{w}))
    \,J_{\bfh}(\mathbf{w})]^T\\ &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\mathcal{O}_{L}(\overline{\mathbf{w}}^{L})).
    \end{align*}\]
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f(\mathbf{w})} &= J_{f}(\mathbf{w})^T\\ &= [J_{\ell}(\bfh(\mathbf{w}))
    \,J_{\bfh}(\mathbf{w})]^T\\ &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\mathcal{O}_{L}(\overline{\mathbf{w}}^{L})).
    \end{align*}\]
- en: The matrix \(J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})\) is computed using
    the recursion above, while \(\nabla {\ell}\) depends on the function \(\ell\).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})\) 是通过上述递归计算得到的，而 \(\nabla
    {\ell}\) 取决于函数 \(\ell\)。
- en: '**Backpropagation** \(\idx{backpropagation}\xdi\) We take advantage of the
    fundamental recursion to compute the gradient of \(\bfh\). As we have seen, there
    are two ways of doing this. Applying the recursion directly is one of them, but
    it requires many matrix-matrix products. The first few steps are'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播** \(\idx{backpropagation}\xdi\) 我们利用基本的递归来计算 \(\bfh\) 的梯度。正如我们所见，有两种方法来做这件事。直接应用递归是其中之一，但它需要许多矩阵-矩阵乘法。最初的几步是'
- en: \[ J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = B_0, \]\[ J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    = \begin{pmatrix} A_1 J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) & B_1 \end{pmatrix}
    \]\[ J_{\mathcal{O}_{2}}(\overline{\mathbf{w}}^{2}) = \begin{pmatrix} A_2 \, J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    & B_2 \end{pmatrix}, \]
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) = B_0, \]\[ J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    = \begin{pmatrix} A_1 J_{\mathcal{O}_{0}}(\overline{\mathbf{w}}^{0}) & B_1 \end{pmatrix}
    \]\[ J_{\mathcal{O}_{2}}(\overline{\mathbf{w}}^{2}) = \begin{pmatrix} A_2 \, J_{\mathcal{O}_{1}}(\overline{\mathbf{w}}^{1})
    & B_2 \end{pmatrix}, \]
- en: and so on.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。
- en: Instead, as in the case of differentiating with respect to the input \(\mathbf{x}\),
    one can also run the recursion backwards. The latter approach can be much faster
    because, as we detail next, it involves only matrix-vector products. Start from
    the end, that is, with the equation
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，正如对输入 \(\mathbf{x}\) 求导的情况一样，也可以反向运行递归。后一种方法可能要快得多，因为，正如我们下面将要详细说明的，它只涉及矩阵-向量乘法。从最后一步开始，即从以下方程开始
- en: \[ \nabla {f}(\mathbf{w}) = J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w})).
    \]
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla {f}(\mathbf{w}) = J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w})).
    \]
- en: Note that \(\nabla {\ell}(\bfh(\mathbf{w}))\) is a vector – not a matrix. Then
    expand the matrix \(J_{\bfh}(\mathbf{w})\) using the recursion above
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\nabla {\ell}(\bfh(\mathbf{w}))\) 是一个向量——不是一个矩阵。然后使用上述递归展开矩阵 \(J_{\bfh}(\mathbf{w})\)
- en: \[\begin{align*} \nabla {f}(\mathbf{w}) &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= \begin{pmatrix} A_L \, J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1}) &
    B_L \end{pmatrix}^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    A_L^T \\ B_L^T \end{pmatrix} \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f}(\mathbf{w}) &= J_{\bfh}(\mathbf{w})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= J_{\mathcal{O}_{L}}(\overline{\mathbf{w}}^{L})^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\
    &= \begin{pmatrix} A_L \, J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1}) &
    B_L \end{pmatrix}^T \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    A_L^T \\ B_L^T \end{pmatrix} \,\nabla {\ell}(\bfh(\mathbf{w}))\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
- en: The key is that both expressions \(A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\)
    and \(B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\) are *matrix-vector products*.
    That pattern persists at the next level of recursion. Note that this supposes
    that we have precomputed \(\bfh(\mathbf{w})\) first.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，两个表达式 \(A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\) 和 \(B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\)
    都是**矩阵-向量乘法**。这种模式在递归的下一级也持续存在。注意，这假设我们首先已经预计算了 \(\bfh(\mathbf{w})\)。
- en: At the next level, we expand the matrix \(J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T\)
    using the fundamental recursion
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一级，我们使用基本递归展开矩阵 \(J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T\)。
- en: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))
    \end{pmatrix}\\ &= \begin{pmatrix} \begin{pmatrix} A_{L-1} \, J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})
    & B_{L-1} \end{pmatrix}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    \begin{pmatrix} J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\,A_{L-1}^T
    \\ B_{L-1}^T \end{pmatrix} \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\left\{A_{L-1}^T \left\{ A_L^T
    \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \\ B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}.
    \end{align*}\]
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-1}}(\overline{\mathbf{w}}^{L-1})^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))
    \end{pmatrix}\\ &= \begin{pmatrix} \begin{pmatrix} A_{L-1} \, J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})
    & B_{L-1} \end{pmatrix}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    \begin{pmatrix} J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\,A_{L-1}^T
    \\ B_{L-1}^T \end{pmatrix} \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}
    \\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}\\ &= \begin{pmatrix}
    J_{\mathcal{O}_{L-2}}(\overline{\mathbf{w}}^{L-2})\left\{A_{L-1}^T \left\{ A_L^T
    \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \\ B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) \end{pmatrix}.
    \end{align*}\]
- en: Continuing by induction gives an alternative formula for the gradient of \(f\).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 通过归纳法继续推导给出了 \(f\) 的梯度的另一种公式。
- en: Indeed, the next level gives
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，下一级给出
- en: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-3}}(\overline{\mathbf{w}}^{L-3})\left\{A_{L-2}^T
    \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}\right\}
    \\ B_{L-2}^T \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}
    \\ B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla {f}(\mathbf{w}) &= \begin{pmatrix} J_{\mathcal{O}_{L-3}}(\overline{\mathbf{w}}^{L-3})\left\{A_{L-2}^T
    \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}\right\}
    \\ B_{L-2}^T \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}
    \\ B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\\ B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) \end{pmatrix}. \end{align*}\]
- en: and so on. Observe that we do not in fact need to compute the large matrices
    \(J_{\mathcal{O}_{i}}\) – only the sequence of vectors \(B_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\),
    \(B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\), \(B_{L-2}^T
    \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\},
    \)etc.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。注意，我们实际上不需要计算大的矩阵 \(J_{\mathcal{O}_{i}}\) - 只需要计算向量序列 \(B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\), \(B_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\),
    \(B_{L-2}^T \left\{A_{L-1}^T \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\}\)，等等。
- en: These formulas may seem cumbersome, but they take an intuitive form. Matrix
    \(A_i\) is the submatrix of the Jacobian \(J_{\bfg_i}\) corresponding only to
    the partial derivatives with respect to \(\mathbf{z}_i\), i.e., the input from
    the previous layer. Matrix \(B_i\) is the submatrix of the Jacobian \(J_{\bfg_i}\)
    corresponding only to the partial derivatives with respect to \(\mathbf{w}_i\),
    i.e., the layer-specific parameters. To compute the subvector of \(\nabla f\)
    corresponding to the parameters \(\mathbf{w}_i\) of the \((i+1)\)-th layer, we
    repeatedly differentiate with respect to the inputs of the previous layer (by
    multiplying by the corresponding \(A_j^T\)) starting from the last one, until
    we reach layer \(i+1\) at which point we take partial derivatives with respect
    to the layer-specific parameters (by multiplying by \(B_i^T\)). The process stops
    there since the layers preceding it do not depend on \(\mathbf{w}_i\) and therefore
    its full effect on \(f\) has been accounted for.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 这些公式可能看起来有些繁琐，但它们具有直观的形式。矩阵 \(A_i\) 是雅可比矩阵 \(J_{\bfg_i}\) 的子矩阵，仅对应于对 \(\mathbf{z}_i\)
    的偏导数，即来自前一层的输入。矩阵 \(B_i\) 是雅可比矩阵 \(J_{\bfg_i}\) 的子矩阵，仅对应于对 \(\mathbf{w}_i\) 的偏导数，即特定层的参数。为了计算与第
    \(i+1\) 层的参数 \(\mathbf{w}_i\) 对应的 \(\nabla f\) 的子向量，我们从最后一个开始，通过乘以相应的 \(A_j^T\)
    对前一层的输入进行反复求导，直到达到 \(i+1\) 层，此时我们通过对特定层参数求偏导数（通过乘以 \(B_i^T\)）来停止这个过程。这个过程在这里停止，因为它之前的层不依赖于
    \(\mathbf{w}_i\)，因此其完全影响 \(f\) 已经被考虑在内。
- en: In other words, we need to compute
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们需要计算
- en: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\]
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w})), \]
- en: and
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[ \mathbf{q}_{L} := B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})), \]
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_{L} := B_L^T \,\nabla {\ell}(\bfh(\mathbf{w})), \]
- en: then
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L} = A_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\} \]
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L} = A_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\} \]
- en: and
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[ \mathbf{q}_{L-1} := B_{L-1}^T \mathbf{p}_{L} = B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}, \]
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_{L-1} := B_{L-1}^T \mathbf{p}_{L} = B_{L-1}^T \left\{ A_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w}))\right\}, \]
- en: then
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1} = A_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \]
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1} = A_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\}\right\} \]
- en: and
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[ \mathbf{q}_{L-2} := B_{L-2}^T \mathbf{p}_{L-1} = B_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \right\}, \]
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_{L-2} := B_{L-2}^T \mathbf{p}_{L-1} = B_{L-2}^T \left\{A_{L-1}^T
    \left\{ A_L^T \,\nabla {\ell}(\bfh(\mathbf{w}))\right\} \right\}, \]
- en: and so on. The \(\mathbf{p}_i\)s are referred to as adjoints; they correspond
    to the vectors of partial derivatives of \(f\) with respect to the \(\mathbf{z}_i\)s.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。\(\mathbf{p}_i\) 被称为伴随项；它们对应于 \(f\) 对 \(\mathbf{z}_i\) 的偏导数向量。
- en: There is one more detail to note. The matrices \(A_i, B_i\) depend on the output
    of layer \(i-1\). To compute them, we first proceed forward, that is, we let \(\mathbf{z}_0
    = \mathbf{x}\) then
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个细节是，矩阵 \(A_i, B_i\) 依赖于层 \(i-1\) 的输出。为了计算它们，我们首先进行正向传播，即我们令 \(\mathbf{z}_0
    = \mathbf{x}\) 然后
- en: \[ \mathbf{z}_1 = \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_0(\mathbf{z}_0,
    \mathbf{w}_0), \]\[ \mathbf{z}_2 = \mathcal{O}_{1}(\overline{\mathbf{w}}^{1})
    = \bfg_1(\mathcal{O}_{0}(\overline{\mathbf{w}}^{0}), \mathbf{w}_1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1), \]
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}_1 = \mathcal{O}_{0}(\overline{\mathbf{w}}^{0}) = \bfg_0(\mathbf{z}_0,
    \mathbf{w}_0), \]\[ \mathbf{z}_2 = \mathcal{O}_{1}(\overline{\mathbf{w}}^{1})
    = \bfg_1(\mathcal{O}_{0}(\overline{\mathbf{w}}^{0}), \mathbf{w}_1) = \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1), \]
- en: and so on. In that forward pass, we also compute \(A_i, B_i\) along the way.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。在正向传播过程中，我们还沿途计算 \(A_i, B_i\)。
- en: We give the full algorithm now, which involves two passes. In the forward pass,
    or forward propagation step, we compute the following.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们给出完整的算法，它涉及两个阶段。在正向传播阶段，或正向传播步骤中，我们计算以下内容。
- en: '*Initialization:*'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化:*'
- en: \[\mathbf{z}_0 := \mathbf{x}\]
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbf{z}_0 := \mathbf{x}\]
- en: '*Forward layer loop:* For \(i = 0, 1,\ldots,L\),'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '*正向层循环:* 对于 \(i = 0, 1,\ldots,L\),'
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i, \mathbf{w}_i)\\ \begin{pmatrix}
    A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) \end{align*}\]
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i, \mathbf{w}_i)\\ \begin{pmatrix}
    A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) \end{align*}\]
- en: '*Loss:*'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失:*'
- en: \[\begin{align*} z_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1} &:= \nabla
    {\ell}(\mathbf{z}_{L+1}). \end{align*}\]
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} z_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1} &:= \nabla
    {\ell}(\mathbf{z}_{L+1}). \end{align*}\]
- en: In the backward pass, or backpropagation step, we compute the following.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播阶段，或反向传播步骤中，我们计算以下内容。
- en: '*Backward layer loop:* For \(i = L,\ldots,1, 0\),'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向层循环:* 对于 \(i = L,\ldots,1, 0\)，'
- en: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1}\\ \mathbf{q}_{i}
    &:= B_i^T \mathbf{p}_{i+1} \end{align*}\]
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1}\\ \mathbf{q}_{i}
    &:= B_i^T \mathbf{p}_{i+1} \end{align*}\]
- en: '*Output:*'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出:*'
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_L). \]
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_L). \]
- en: Note that we do not in fact need to compute \(A_0\) and \(\mathbf{p}_0\).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，实际上我们不需要计算 \(A_0\) 和 \(\mathbf{p}_0\)。
- en: '**EXAMPLE:** **(continued)** We apply the algorithm to our running example.
    From previous calculations, for \(i = 0, 1,\ldots,L\), the Jacobians are'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（继续）** 我们将算法应用于我们的运行示例。从前面的计算中，对于 \(i = 0, 1,\ldots,L\)，雅可比矩阵为'
- en: '\[\begin{align*} J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) &= \begin{pmatrix}
    \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}\\
    &= \begin{pmatrix} \mathcal{W}_i & I_{n_{i+1} \times n_{i+1}} \otimes \mathbf{z}_i^T
    \end{pmatrix}\\ &=: \begin{pmatrix} A_i & B_i \end{pmatrix} \end{align*}\]'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{align*} J_{\bfg_i}(\mathbf{z}_i, \mathbf{w}_i) &= \begin{pmatrix}
    \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}\\
    &= \begin{pmatrix} \mathcal{W}_i & I_{n_{i+1} \times n_{i+1}} \otimes \mathbf{z}_i^T
    \end{pmatrix}\\ &=: \begin{pmatrix} A_i & B_i \end{pmatrix} \end{align*}\]'
- en: and
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}} - \mathbf{y})^T. \]
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\hat{\mathbf{y}}) = (\hat{\mathbf{y}} - \mathbf{y})^T. \]
- en: Using the *Properties of the Kronecker Product*, we obtain
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 利用**克罗内克积的性质**，我们得到
- en: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L} &:= B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) = (I_{n_{L+1} \times n_{L+1}} \otimes \mathbf{z}_L^T)^T
    (\hat{\mathbf{y}} - \mathbf{y}) = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_L\\
    &= (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L}
    = \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*}
    \mathbf{q}_{L-1} &:= B_{L-1}^T \mathbf{p}_{L} = (I_{n_{L} \times n_{L}} \otimes
    \mathbf{z}_{L-1}^T)^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-1}\\ &= \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-2} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1}
    = \mathcal{W}_{L-2}^T \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} -
    \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L-2} &:= B_{L-2}^T \mathbf{p}_{L-1}
    = (I_{n_{L-1} \times n_{L-1}} \otimes \mathbf{z}_{L-2}^T)^T \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-2}\\ &= \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-3} \cdots
    \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} \end{align*}\]
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_{L} := A_L^T \,\nabla {\ell}(\bfh(\mathbf{w})) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L} &:= B_L^T \,\nabla
    {\ell}(\bfh(\mathbf{w})) = (I_{n_{L+1} \times n_{L+1}} \otimes \mathbf{z}_L^T)^T
    (\hat{\mathbf{y}} - \mathbf{y}) = (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_L\\
    &= (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-1} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-1} := A_{L-1}^T \mathbf{p}_{L}
    = \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \]\[\begin{align*}
    \mathbf{q}_{L-1} &:= B_{L-1}^T \mathbf{p}_{L} = (I_{n_{L} \times n_{L}} \otimes
    \mathbf{z}_{L-1}^T)^T \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-1}\\ &= \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-2} \cdots \mathcal{W}_{1}
    \mathcal{W}_{0} \mathbf{x} \end{align*}\]\[ \mathbf{p}_{L-2} := A_{L-2}^T \mathbf{p}_{L-1}
    = \mathcal{W}_{L-2}^T \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}} -
    \mathbf{y}) \]\[\begin{align*} \mathbf{q}_{L-2} &:= B_{L-2}^T \mathbf{p}_{L-1}
    = (I_{n_{L-1} \times n_{L-1}} \otimes \mathbf{z}_{L-2}^T)^T \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) = \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathbf{z}_{L-2}\\ &= \mathcal{W}_{L-1}^T
    \mathcal{W}_L^T (\hat{\mathbf{y}} - \mathbf{y}) \otimes \mathcal{W}_{L-3} \cdots
    \mathcal{W}_{1} \mathcal{W}_{0} \mathbf{x} \end{align*}\]
- en: and so on. Following the pattern, the last step is
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。按照模式，最后一步是
- en: \[ \mathbf{p}_1 := \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[ \mathbf{q}_0 := B_{0}^T \mathbf{p}_{1} =
    \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}}
    - \mathbf{y}) \otimes \mathbf{x}. \]
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}_1 := \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T
    (\hat{\mathbf{y}} - \mathbf{y}) \]\[ \mathbf{q}_0 := B_{0}^T \mathbf{p}_{1} =
    \mathcal{W}_{1}^T \cdots \mathcal{W}_{L-1}^T \mathcal{W}_L^T (\hat{\mathbf{y}}
    - \mathbf{y}) \otimes \mathbf{x}. \]
- en: These calculations are consistent with the case \(L=1\) that we derived previously
    (check it!). \(\lhd\)
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算与之前推导的 \(L=1\) 的情况一致（检查一下！）。\(\lhd\)
- en: '**CHAT & LEARN** The efficiency of backpropagation has been key to the success
    of deep learning. Ask your favorite AI chatbot about the history of backpropagation
    and its role in the development of modern deep learning. \(\ddagger\)'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 反向传播的效率是深度学习成功的关键。向你的最爱人工智能聊天机器人询问反向传播的历史及其在现代深度学习发展中的作用。\(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** In the backpropagation algorithm, what does the ‘forward pass’ compute?'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在反向传播算法中，“正向传播”计算了什么？'
- en: a) The adjoints \(\mathbf{p}_i\) for each layer \(i\).
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: a) 每一层 \(i\) 的伴随 \(\mathbf{p}_i\)。
- en: b) The gradients \(\mathbf{q}_i\) for the parameters of each layer \(i\).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: b) 每一层 \(i\) 参数的梯度 \(\mathbf{q}_i\)。
- en: c) The function values \(\mathbf{z}_i\) and the Jacobians \(A_i, B_i\) for each
    layer \(i\).
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: c) 每一层 \(i\) 的函数值 \(\mathbf{z}_i\) 和雅可比矩阵 \(A_i, B_i\)。
- en: d) The final gradient \(\nabla f(\mathbf{w})\) with respect to all parameters.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: d) 关于所有参数的最终梯度 \(\nabla f(\mathbf{w})\)。
- en: '**2** What is the purpose of the ‘backward pass’ in the backpropagation algorithm?'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 反向传播算法中“反向传播”的目的是什么？'
- en: a) To compute the function values \(\mathbf{z}_i\) for each layer \(i\) from
    the input \(\mathbf{x}\).
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: a) 计算每一层 \(i\) 的函数值 \(\mathbf{z}_i\)，从输入 \(\mathbf{x}\) 开始。
- en: b) To compute the Jacobians \(A_i, B_i\) for each layer \(i\) using the fundamental
    recursion.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: b) 使用基本递归计算每一层 \(i\) 的雅可比矩阵 \(A_i, B_i\)。
- en: c) To compute the adjoints \(\mathbf{p}_i\) and the gradients \(\mathbf{q}_i\)
    for each layer \(i\) using the fundamental recursion.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: c) 使用基本递归计算每个层 \(i\) 的伴随 \(\mathbf{p}_i\) 和梯度 \(\mathbf{q}_i\)。
- en: d) To compute the final output \(\ell(\mathbf{z}_{L+1})\) of the progressive
    function.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: d) 为了计算渐进函数的最终输出 \(\ell(\mathbf{z}_{L+1})\)。
- en: '**3** What is the computational complexity of the backpropagation algorithm
    in terms of the number of layers \(L\) and the matrix dimensions \(m\)?'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 从层数 \(L\) 和矩阵维度 \(m\) 的角度来看，反向传播算法的计算复杂度是什么？'
- en: a) \(\approx Lm\)
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\approx Lm\)
- en: b) \(\approx Lm^2\)
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\approx Lm^2\)
- en: c) \(\approx Lm^2d\)
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\approx Lm^2d\)
- en: d) \(\approx Lm^3d\)
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\approx Lm^3d\)
- en: '**4** In the context of progressive functions, what is the significance of
    the matrices \(A_i\) and \(B_i\)?'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在渐进函数的背景下，矩阵 \(A_i\) 和 \(B_i\) 有什么意义？'
- en: a) They represent the Jacobians of the layer functions with respect to the inputs
    and parameters, respectively.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它们分别代表层函数相对于输入和参数的雅可比矩阵。
- en: b) They are the intermediate values computed during the forward pass.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它们是在前向传递过程中计算的中间值。
- en: c) They are the adjoints used in the backpropagation algorithm.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它们是在反向传播算法中使用的伴随。
- en: d) They are the matrices of parameters for each layer.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它们是每层的参数矩阵。
- en: '**5** In the context of progressive functions, which of the following best
    describes the role of the vector \(\mathbf{w}_i\)?'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在渐进函数的背景下，以下哪项最好地描述了向量 \(\mathbf{w}_i\) 的作用？'
- en: a) The input to the \(i\)-th layer.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: a) 第 \(i\) 层的输入。
- en: b) The output of the \(i\)-th layer.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: b) 第 \(i\) 层的输出。
- en: c) The parameters specific to the \(i\)-th layer.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: c) 第 \(i\) 层的特定参数。
- en: d) The concatenation of parameters from all layers up to \(i\).
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有层到 \(i\) 的参数的连接。
- en: 'Answer for 1: c. Justification: The section presents the forward propagation
    step which computes “the following: Initialization: \(\mathbf{z}_0 := \mathbf{x}\)
    Forward layer loop: For \(i=0,1,\dots,L\), \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i,
    \mathbf{w}_i)\) \((A_i,B_i) := J_{\mathbf{g}_i}(\mathbf{z}_i, \mathbf{w}_i)\)
    Loss: \(\mathbf{z}_{L+2} := \ell(\mathbf{z}_{L+1})\)”'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：c. 证明：本节介绍了前向传播步骤，该步骤计算“以下内容：初始化：\(\mathbf{z}_0 := \mathbf{x}\) 前向层循环：对于
    \(i=0,1,\dots,L\)，\(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i)\)
    \((A_i,B_i) := J_{\mathbf{g}_i}(\mathbf{z}_i, \mathbf{w}_i)\) 损失：\(\mathbf{z}_{L+2}
    := \ell(\mathbf{z}_{L+1})\)”
- en: 'Answer for 2: c. Justification: The backward pass is described as follows:
    “Backward layer loop: For \(i=L,\dots,1,0\), \(\mathbf{p}_i := A_i^T \mathbf{p}_{i+1}\)
    \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1}\) Output: \(\nabla f(\mathbf{w}) = (\mathbf{q}_0,
    \mathbf{q}_1, \dots, \mathbf{q}_L)\).”'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：c. 证明：反向传递被描述如下：“反向层循环：对于 \(i=L,\dots,1,0\)，\(\mathbf{p}_i := A_i^T \mathbf{p}_{i+1}\)
    \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1}\) 输出：\(\nabla f(\mathbf{w}) = (\mathbf{q}_0,
    \mathbf{q}_1, \dots, \mathbf{q}_L)\)”。
- en: 'Answer for 3: b. Justification: The text derives that the number of operations
    in the reverse mode is approximately \(2Lm^2\), stating “This is approximately
    \(2Lm^2\) – which can be much smaller than \(2Lm^2d\)!”'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：b. 证明：文本推导出反向模式的操作数大约为 \(2Lm^2\)，声明“这大约是 \(2Lm^2\) – 这可以比 \(2Lm^2d\) 小得多！”
- en: 'Answer for 4: a. Justification: The text defines \(A_i\) and \(B_i\) as the
    blocks of the Jacobian \(J_{\mathbf{g}_i}(\mathbf{z}_i, \mathbf{w}_i)\) corresponding
    to the partial derivatives with respect to \(\mathbf{z}_i\) and \(\mathbf{w}_i\),
    respectively.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：a. 证明：文本将 \(A_i\) 和 \(B_i\) 定义为雅可比矩阵 \(J_{\mathbf{g}_i}(\mathbf{z}_i,
    \mathbf{w}_i)\) 对应于相对于 \(\mathbf{z}_i\) 和 \(\mathbf{w}_i\) 的偏导数的块。
- en: 'Answer for 5: c. Justification: The text explains: “In the machine learning
    context, each “layer” \(\mathbf{g}_i\) has parameters (in our running example,
    there were the entries of \(\mathcal{W}_i\)) and we seek to optimize with respect
    to those parameters.”'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：c. 证明：文本解释说：“在机器学习背景下，每个“层”\(\mathbf{g}_i\)都有参数（在我们的运行示例中，是\(\mathcal{W}_i\)的条目），我们试图优化这些参数。”
