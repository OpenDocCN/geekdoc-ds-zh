- en: '8.4\. Building blocks of AI 2: stochastic gradient descent#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.4\. 人工智能构建块 2：随机梯度下降#
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html](https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html](https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html)
- en: Having shown how to compute the gradient, we can now apply gradient descent
    to fit the data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示了如何计算梯度之后，我们现在可以应用梯度下降来拟合数据。
- en: To get the full gradient, we consider \(n\) samples \((\mathbf{x}_i,y_i)\),
    \(i=1,\ldots,n\). At this point, we make the dependence on \((\mathbf{x}_i, y_i)\)
    explicit. The loss function can be taken as the average of the individual sample
    contributions, so the gradient is obtained by linearity
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得完整的梯度，我们考虑 \(n\) 个样本 \((\mathbf{x}_i,y_i)\)，\(i=1,\ldots,n\)。在此点，我们使 \((\mathbf{x}_i,
    y_i)\) 的依赖关系明确。损失函数可以取为单个样本贡献的平均值，因此梯度通过线性获得
- en: \[ \nabla \left(\frac{1}{n} \sum_{i=1}^n f_{\mathbf{x}_i,y_i}(\mathbf{w})\right)
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}), \]
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla \left(\frac{1}{n} \sum_{i=1}^n f_{\mathbf{x}_i,y_i}(\mathbf{w})\right)
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}), \]
- en: where each term can be computed separately by the procedure above.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个项都可以通过上述程序单独计算。
- en: We can then apply gradient decent. We start from an arbitrary \(\mathbf{w}^{0}\)
    and update as follows
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以应用梯度下降。我们从任意 \(\mathbf{w}^{0}\) 开始并按以下方式更新
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \left(\frac{1}{n} \sum_{i=1}^n
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t})\right). \]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \left(\frac{1}{n} \sum_{i=1}^n
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t})\right). \]
- en: In a large dataset, computing the sum over all samples may be prohibitively
    expensive. We present a popular alternative.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集中，对所有样本求和可能过于昂贵。我们提出了一种流行的替代方案。
- en: 8.4.1\. Algorithm[#](#algorithm "Link to this heading")
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4.1\. 算法[#](#algorithm "链接到本标题")
- en: In [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
    (SGD)\(\idx{stochastic gradient descent}\xdi\), a variant of gradient descent,
    we pick a sample \(I_t\) uniformly at random in \(\{1,\ldots,n\}\) and update
    as follows
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD)\(\idx{随机梯度下降}\xdi\)，梯度下降的一种变体中，我们从
    \(\{1,\ldots,n\}\) 中随机均匀地选择一个样本 \(I_t\) 并按以下方式更新
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
    \]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
    \]
- en: More generally, in the so-called mini-batch version of SGD, we pick instead
    a uniformly random sub-sample \(\mathcal{B}_t \subseteq \{1,\ldots,n\}\) of size
    \(b\) without replacement (i.e., all sub-samples of that size have the same probability
    of being picked)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，在所谓的 mini-batch 版本的 SGD 中，我们选择一个大小为 \(b\) 的均匀随机子样本 \(\mathcal{B}_t \subseteq
    \{1,\ldots,n\}\) 而不是替换（即，该大小所有子样本被选中的概率相同）
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t}
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}). \]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t}
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}). \]
- en: The key observation about the two stochastic updates above is that, in expectation,
    they perform a step of gradient descent. That turns out to be enough and it has
    computational advantages.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于上述两个随机更新的关键观察是，在期望上，它们执行了一步梯度下降。这证明是足够的，并且具有计算优势。
- en: '**LEMMA** Fix a batch size \(1 \leq b \leq n\) and and an arbitrary vector
    of parameters \(\mathbf{w}\). Let \(\mathcal{B} \subseteq \{1,\ldots,n\}\) be
    a uniformly random sub-sample of size \(b\). Then'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** 设定一个批大小 \(1 \leq b \leq n\) 和一个任意的参数向量 \(\mathbf{w}\)。设 \(\mathcal{B}
    \subseteq \{1,\ldots,n\}\) 是大小为 \(b\) 的均匀随机子样本。那么'
- en: \[ \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}). \]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}). \]
- en: \(\flat\)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* Because \(\mathcal{B}\) is picked uniformly at random (without replacement),
    for any sub-sample \(B \subseteq \{1,\ldots,n\}\) of size \(b\) without repeats'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 因为 \(\mathcal{B}\) 是随机均匀选择（不替换），对于任何大小为 \(b\) 的不重复子样本 \(B \subseteq \{1,\ldots,n\}\)'
- en: \[ \mathbb{P}[\mathcal{B} = B] = \frac{1}{\binom{n}{b}}. \]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}[\mathcal{B} = B] = \frac{1}{\binom{n}{b}}. \]
- en: So, summing over all such sub-samples, we obtain
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对所有这样的子样本求和，我们得到
- en: \[\begin{align*} \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla
    f_{\mathbf{x}_i,y_i}(\mathbf{w})\right] &= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B}
    = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{B
    \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i
    \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})
    \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
    \end{align*}\]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla
    f_{\mathbf{x}_i,y_i}(\mathbf{w})\right] &= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B}
    = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{B
    \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i
    \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})
    \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
    \end{align*}\]
- en: Computing the internal sum requires a combinatorial argument. Indeed, \(\sum_{B
    \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\) counts the number of ways that
    \(i\) can be picked in a sub-sample of size \(b\) without repeats. That is \(\binom{n-1}{b-1}\),
    which is the number of ways of picking the remaining \(b-1\) elements of \(B\)
    from the other \(n-1\) possible elements. By definition of the binomial coefficient
    and the properties of factorials,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算内部和需要组合论证。实际上，\(\sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\)
    计算了 \(i\) 在大小为 \(b\) 的子样本中不重复被选择的方式数。那就是 \(\binom{n-1}{b-1}\)，这是从其他 \(n-1\) 个可能元素中选择
    \(B\) 的剩余 \(b-1\) 个元素的方式数。根据二项系数的定义和阶乘的性质，
- en: \[ \frac{\binom{n-1}{b-1}}{b \binom{n}{b}} = \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b
    \frac{n!}{b! (n-b)!}} = \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!} = \frac{1}{n}. \]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\binom{n-1}{b-1}}{b \binom{n}{b}} = \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b
    \frac{n!}{b! (n-b)!}} = \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!} = \frac{1}{n}. \]
- en: Plugging back gives the claim. \(\square\)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将其代入给出结论。 \(\square\)
- en: 'As a first illustration, we return to logistic regression\(\idx{logistic regression}\xdi\).
    Recall that the input data is of the form \(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots,
    n\}\) where \(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in
    \mathbb{R}^d\) are the features and \(b_i \in \{0,1\}\) is the label. As before
    we use a matrix representation: \(A \in \mathbb{R}^{n \times d}\) has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). We
    want to solve the minimization problem'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '作为第一个示例，我们回到逻辑回归\(\idx{logistic regression}\xdi\)。回想一下，输入数据的形式是 \(\{(\boldsymbol{\alpha}_i,
    b_i) : i=1,\ldots, n\}\)，其中 \(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d})
    \in \mathbb{R}^d\) 是特征，\(b_i \in \{0,1\}\) 是标签。和之前一样，我们使用矩阵表示：\(A \in \mathbb{R}^{n
    \times d}\) 的行是 \(\boldsymbol{\alpha}_i^T\)，\(i = 1,\ldots, n\)，而 \(\mathbf{b}
    = (b_1, \ldots, b_n) \in \{0,1\}^n\)。我们想要解决最小化问题'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}) \]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}) \]
- en: where the loss is
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中损失是
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right). \end{align*}\]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right). \end{align*}\]
- en: The gradient was previously computed as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前面已经计算了梯度
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= - \frac{1}{n} \sum_{i=1}^n
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i\\
    &= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \end{align*}\]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= - \frac{1}{n} \sum_{i=1}^n
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i\\
    &= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \end{align*}\]
- en: For the mini-batch version of SGD, we pick a random sub-sample \(\mathcal{B}_t
    \subseteq \{1,\ldots,n\}\) of size \(B\) and take the step
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SGD 的小批量版本，我们选择一个大小为 \(B\) 的随机子样本 \(\mathcal{B}_t \subseteq \{1,\ldots,n\}\)，并采取以下步骤
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t}
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i.
    \]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t}
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i.
    \]
- en: We modify our previous code for logistic regression. The only change is to pick
    a random mini-batch which can be fed to the descent update sub-routine as dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改了之前用于逻辑回归的代码。唯一的改变是选择一个随机的小批量，将其作为数据集输入到下降更新子例程中。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**NUMERICAL CORNER:** We analyze a dataset from [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)],
    which can be downloaded [here](https://web.stanford.edu/~hastie/ElemStatLearn/data.html).
    Quoting [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/), Section 4.4.2]'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们分析了来自 [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)] 的数据集，该数据集可以在此
    [下载](https://web.stanford.edu/~hastie/ElemStatLearn/data.html)。引用 [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)]，第
    4.4.2 节'
- en: The data […] are a subset of the Coronary Risk-Factor Study (CORIS) baseline
    survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw
    et al., 1983). The aim of the study was to establish the intensity of ischemic
    heart disease risk factors in that high-incidence region. The data represent white
    males between 15 and 64, and the response variable is the presence or absence
    of myocardial infarction (MI) at the time of the survey (the overall prevalence
    of MI was 5.1% in this region). There are 160 cases in our data set, and a sample
    of 302 controls. These data are described in more detail in Hastie and Tibshirani
    (1987).
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据 [...] 是南非开普敦西部三个农村地区的冠状动脉风险因素研究（CORIS）基线调查的一个子集（Rousseauw 等人，1983 年）。该研究的目的是确定该高发病率地区缺血性心脏病风险因素的强度。数据代表
    15 至 64 岁的白人男性，应变量是在调查时心肌梗死（MI）的存在或不存在（该地区 MI 的总体患病率为 5.1%）。我们的数据集中有 160 个病例，以及
    302 个对照组。这些数据在 Hastie 和 Tibshirani（1987 年）的书中描述得更详细。
- en: We load the data, which we slightly reformatted and look at a summary.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了数据，我们对数据进行了一些格式上的调整，并查看了一个摘要。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  | sbp | tobacco | ldl | adiposity | typea | obesity | alcohol | age | chd
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | sbp | tobacco | ldl | adiposity | typea | obesity | alcohol | age | chd
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 160.0 | 12.00 | 5.73 | 23.11 | 49.0 | 25.30 | 97.20 | 52.0 | 1.0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 160.0 | 12.00 | 5.73 | 23.11 | 49.0 | 25.30 | 97.20 | 52.0 | 1.0 |'
- en: '| 1 | 144.0 | 0.01 | 4.41 | 28.61 | 55.0 | 28.87 | 2.06 | 63.0 | 1.0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 144.0 | 0.01 | 4.41 | 28.61 | 55.0 | 28.87 | 2.06 | 63.0 | 1.0 |'
- en: '| 2 | 118.0 | 0.08 | 3.48 | 32.28 | 52.0 | 29.14 | 3.81 | 46.0 | 0.0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 118.0 | 0.08 | 3.48 | 32.28 | 52.0 | 29.14 | 3.81 | 46.0 | 0.0 |'
- en: '| 3 | 170.0 | 7.50 | 6.41 | 38.03 | 51.0 | 31.99 | 24.26 | 58.0 | 1.0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 170.0 | 7.50 | 6.41 | 38.03 | 51.0 | 31.99 | 24.26 | 58.0 | 1.0 |'
- en: '| 4 | 134.0 | 13.60 | 3.50 | 27.78 | 60.0 | 25.99 | 57.34 | 49.0 | 1.0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 134.0 | 13.60 | 3.50 | 27.78 | 60.0 | 25.99 | 57.34 | 49.0 | 1.0 |'
- en: Our goal to predict `chd`, which stands for coronary heart disease, based on
    the other variables (which are briefly described [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)).
    We use logistic regression again.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是根据其他变量（这些变量简要描述 [在此](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)）预测
    `chd`，即冠状动脉心脏病。我们再次使用逻辑回归。
- en: We first construct the data matrices. We only use three of the predictors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建数据矩阵。我们只使用了三个预测变量。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We try mini-batch SGD.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了小批量随机梯度下降法（mini-batch SGD）。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The outcome is harder to vizualize. To get a sense of how accurate the result
    is, we compare our predictions to the true labels. By prediction, let us say that
    we mean that we predict label \(1\) whenever \(\sigma(\boldsymbol{\alpha}^T \mathbf{x})
    > 1/2\). We try this on the training set. (A better approach would be to split
    the data into training and testing sets, but we will not do this here.)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果更难以可视化。为了了解结果的准确性，我们将我们的预测与真实标签进行比较。通过预测，我们指的是当 \(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) > 1/2\) 时，我们预测标签 \(1\)。我们在训练集上尝试了这种方法。（更好的方法是将数据分成训练集和测试集，但在这里我们不会这样做。）
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: \(\unlhd\)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '8.4.2\. Example: multinomial logistic regression[#](#example-multinomial-logistic-regression
    "Link to this heading")'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4.2. 示例：多项逻辑回归[#](#example-multinomial-logistic-regression "链接到这个标题")
- en: We give a concrete example of progressive functions and of the application of
    backpropagation and SGD.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给出了渐进函数的一个具体例子，以及反向传播和随机梯度下降的应用。
- en: Recall that a classifier \(h\) takes an input in \(\mathbb{R}^d\) and predicts
    one of \(K\) possible labels. It will be convenient for reasons that will become
    clear below to use [one-hot encoding](https://en.wikipedia.org/wiki/One-hot)\(\idx{one-hot
    encoding}\xdi\) of the labels. That is, we encode label \(i\) as the \(K\)-dimensional
    vector \(\mathbf{e}_i\). Here, as usual, \(\mathbf{e}_i\) the standard basis of
    \(\mathbb{R}^K\), i.e., the vector with a \(1\) in entry \(i\) and a \(0\) elsewhere.
    Furthermore, we allow the output of the classifier to be a probability distribution
    over the labels \(\{1,\ldots,K\}\), that is, a vector in
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，一个分类器 \(h\) 接受 \(\mathbb{R}^d\) 中的输入并预测 \(K\) 个可能标签中的一个。下面将要变得清楚的原因是，使用
    [独热编码](https://en.wikipedia.org/wiki/One-hot)\(\idx{one-hot encoding}\xdi\) 的标签将很方便。也就是说，我们将标签
    \(i\) 编码为 \(K\) 维向量 \(\mathbf{e}_i\)。在这里，像往常一样，\(\mathbf{e}_i\) 是 \(\mathbb{R}^K\)
    的标准基，即一个 \(1\) 在条目 \(i\) 上，其他地方为 \(0\) 的向量。此外，我们允许分类器的输出是标签 \(\{1,\ldots,K\}\)
    的概率分布，即一个向量在
- en: \[ \Delta_K = \left\{ (p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k =
    1 \right\}. \]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \Delta_K = \left\{ (p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k =
    1 \right\}. \]
- en: Observe that \(\mathbf{e}_i\) can itself be thought of as a probability distribution,
    one that assigns probability one to \(i\).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mathbf{e}_i\) 本身可以被视为一个概率分布，它将概率 \(1\) 分配给 \(i\)。
- en: '**Background on multinomial logistic regression** We use [multinomial logistic
    regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\(\idx{multinomial
    logistic regression}\xdi\) to learn a classifier over \(K\) labels. In multinomial
    logistic regression, we once again use an affine function of the input data.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**多项式逻辑回归的背景** 我们使用 [多项式逻辑回归](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\(\idx{multinomial
    logistic regression}\xdi\) 来学习 \(K\) 个标签的分类器。在多项式逻辑回归中，我们再次使用输入数据的仿射函数。'
- en: 'This time, we have \(K\) functions that output a score associated to each label.
    We then transform these scores into a probability distribution over the \(K\)
    labels. There are many ways of doing this. A standard approach is the [softmax
    function](https://en.wikipedia.org/wiki/Softmax_function)\(\idx{softmax}\xdi\)
    \(\bgamma = (\gamma_1,\ldots,\gamma_K)\): for \(\mathbf{z} \in \mathbb{R}^K\)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们有 \(K\) 个函数，每个函数输出与每个标签关联的分数。然后我们将这些分数转换成 \(K\) 个标签的概率分布。有多种方法可以做到这一点。一种标准的方法是
    [softmax 函数](https://en.wikipedia.org/wiki/Softmax_function)\(\idx{softmax}\xdi\)
    \(\bgamma = (\gamma_1,\ldots,\gamma_K)\)：对于 \(\mathbf{z} \in \mathbb{R}^K\)
- en: \[ \gamma_i(\mathbf{z}) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \quad i=1,\ldots,K.
    \]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \gamma_i(\mathbf{z}) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \quad i=1,\ldots,K.
    \]
- en: To explain the name, observe that the larger inputs are mapped to larger probabilities.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释这个名称，注意到较大的输入被映射到较大的概率。
- en: In fact, since a probability distribution must sum to \(1\), it is determined
    by the probabilities assigned to the first \(K-1\) labels. In other words, we
    could drop the score associated to the last label. But the keep the notation simple,
    we will not do this here.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，由于概率分布必须求和为 \(1\)，它由分配给前 \(K-1\) 个标签的概率决定。换句话说，我们可以省略与最后一个标签相关的分数。但为了简化符号，我们在这里不会这样做。
- en: For each \(k\), we have a regression function
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 \(k\)，我们有一个回归函数
- en: \[ \sum_{j=1}^d w^{(k)}_{j} x_{j} = \mathbf{x}_1^T \mathbf{w}^{(k)}, \quad k=1,\ldots,K
    \]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^d w^{(k)}_{j} x_{j} = \mathbf{x}_1^T \mathbf{w}^{(k)}, \quad k=1,\ldots,K
    \]
- en: where \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\) are the parameters
    with \(\mathbf{w}^{(k)} \in \mathbb{R}^{d}\) and \(\mathbf{x} \in \mathbb{R}^d\)
    is the input. A constant term can be included by adding an additional entry \(1\)
    to \(\mathbf{x}\). As we did in the linear regression case, we assume that this
    pre-processing has been performed previously. To simplify the notation, we let
    \(\mathcal{W} \in \mathbb{R}^{K \times d}\) as the matrix with rows \((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\) 是参数，\(\mathbf{w}^{(k)}
    \in \mathbb{R}^{d}\) 且 \(\mathbf{x} \in \mathbb{R}^d\) 是输入。可以通过向 \(\mathbf{x}\)
    添加一个额外的条目 \(1\) 来包含一个常数项。正如我们在线性回归案例中所做的那样，我们假设这种预处理已经完成。为了简化符号，我们让 \(\mathcal{W}
    \in \mathbb{R}^{K \times d}\) 作为具有行 \((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\)
    的矩阵。
- en: The output of the classifier is
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的输出是
- en: \[\begin{align*} \bfh(\mathbf{w}) &= \bgamma\left(\mathcal{W} \mathbf{x}\right),
    \end{align*}\]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \bfh(\mathbf{w}) &= \bgamma\left(\mathcal{W} \mathbf{x}\right),
    \end{align*}\]
- en: for \(i=1,\ldots,K\), where \(\bgamma\) is the softmax function. Note that the
    latter has no associated parameter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i=1,\ldots,K\)，其中 \(\bgamma\) 是 softmax 函数。注意，后者没有关联的参数。
- en: It remains to define a loss function. To quantify the fit, it is natural to
    use a notion of distance between probability measures, here between the output
    \(\mathbf{h}(\mathbf{w}) \in \Delta_K\) and the correct label \(\mathbf{y} \in
    \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq \Delta_K\). There are many such
    measures. In multinomial logistic regression, we use the Kullback-Leibler divergence,
    which we have encountered in the context of maximum likelihood estimation. Recall
    that, for two probability distributions \(\mathbf{p}, \mathbf{q} \in \Delta_K\),
    it is defined as
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下需要定义一个损失函数。为了量化拟合度，自然地使用概率测度之间的距离概念，这里是在输出 \(\mathbf{h}(\mathbf{w}) \in \Delta_K\)
    和正确标签 \(\mathbf{y} \in \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq \Delta_K\)
    之间。存在许多这样的度量。在多项式逻辑回归中，我们使用Kullback-Leibler散度，我们在最大似然估计的上下文中已经遇到过。回想一下，对于两个概率分布
    \(\mathbf{p}, \mathbf{q} \in \Delta_K\)，它定义为
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
- en: where it will suffice to restrict ourselves to the case \(\mathbf{q} > \mathbf{0}\)
    and where we use the convention \(0 \log 0 = 0\) (so that terms with \(p_i = 0\)
    contribute \(0\) to the sum). Notice that \(\mathbf{p} = \mathbf{q}\) implies
    \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) = 0\). We proved previously that \(\mathrm{KL}(\mathbf{p}
    \| \mathbf{q}) \geq 0\), a result known as *Gibbs’ inequality*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们只需限制在 \(\mathbf{q} > \mathbf{0}\) 的情况下，并且我们使用约定 \(0 \log 0 = 0\)（这样 \(p_i
    = 0\) 的项对总和的贡献为 \(0\)）。注意，\(\mathbf{p} = \mathbf{q}\) 意味着 \(\mathrm{KL}(\mathbf{p}
    \| \mathbf{q}) = 0\)。我们之前已经证明了 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)，这是一个被称为
    *吉布斯不等式* 的结果。
- en: Going back to the loss function, we use the identity \(\log\frac{\alpha}{\beta}
    = \log \alpha - \log \beta\) to re-write
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 回到损失函数，我们使用恒等式 \(\log\frac{\alpha}{\beta} = \log \alpha - \log \beta\) 来重新写
- en: \[\begin{align*} \mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w})) &= \sum_{i=1}^K
    y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\ &= \sum_{i=1}^K y_i \log y_i - \sum_{i=1}^K
    y_i \log h_{i}(\mathbf{w}), \end{align*}\]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w})) &= \sum_{i=1}^K
    y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\ &= \sum_{i=1}^K y_i \log y_i - \sum_{i=1}^K
    y_i \log h_{i}(\mathbf{w}), \end{align*}\]
- en: where \(\bfh = (h_{1},\ldots,h_{K})\). Notice that the first term on right-hand
    side does not depend on \(\mathbf{w}\). Hence we can ignore it when optimizing
    \(\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))\). The remaining term is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bfh = (h_{1},\ldots,h_{K})\). 注意到右侧第一个项不依赖于 \(\mathbf{w}\). 因此，当优化 \(\mathrm{KL}(\mathbf{y}
    \| \bfh(\mathbf{w}))\) 时，我们可以忽略它。剩余的项是
- en: \[ H(\mathbf{y}, \bfh(\mathbf{w})) = - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
    \]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H(\mathbf{y}, \bfh(\mathbf{w})) = - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
    \]
- en: We use it to define our loss function. That is, we set
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用它来定义我们的损失函数。也就是说，我们设
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_{i}. \]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_{i}. \]
- en: Finally,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfh(\mathbf{w}))\\ &= H(\mathbf{y},
    \bfh(\mathbf{w}))\\ &= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
    &= - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right). \end{align*}\]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfh(\mathbf{w}))\\ &= H(\mathbf{y},
    \bfh(\mathbf{w}))\\ &= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
    &= - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right). \end{align*}\]
- en: '**Computing the gradient** We apply the forward and backpropagation steps from
    the previous section. We then use the resulting recursions to derive an analytical
    formula for the gradient.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算梯度** 我们应用了上一节中的前向和反向传播步骤。然后，我们使用这些递归关系推导出梯度的解析公式。'
- en: The forward pass starts with the initialization \(\mathbf{z}_0 := \mathbf{x}\).
    The forward layer loop has two steps. Set \(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)
    equal to \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\). First we
    compute
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播从初始化 \(\mathbf{z}_0 := \mathbf{x}\) 开始。前向层循环有两个步骤。将 \(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)
    等于 \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\)。首先我们计算
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0) &:=\begin{pmatrix} A_0 &
    B_0 \end{pmatrix} \end{align*}\]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0) &:=\begin{pmatrix} A_0 &
    B_0 \end{pmatrix} \end{align*}\]
- en: 'where we defined \(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\) as the matrix
    with rows \((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\). We have
    previously computed the Jacobian:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\) 为具有行 \((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\)
    的矩阵。我们之前已经计算了雅可比矩阵：
- en: \[\begin{split} A_0 = \mathbb{A}_{K}[\mathbf{w}_0] = \mathcal{W}_0 = \begin{pmatrix}
    (\mathbf{w}^{(1)}_0)^T\\ \vdots\\ (\mathbf{w}^{(K)}_0)^T \end{pmatrix} \end{split}\]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A_0 = \mathbb{A}_{K}[\mathbf{w}_0] = \mathcal{W}_0 = \begin{pmatrix}
    (\mathbf{w}^{(1)}_0)^T\\ \vdots\\ (\mathbf{w}^{(K)}_0)^T \end{pmatrix} \end{split}\]
- en: and
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ B_0 = \mathbb{B}_{K}[\mathbf{z}_0] = I_{K\times K} \otimes \mathbf{z}_0^T
    = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_0^T & \cdots & \mathbf{e}_{K}\mathbf{z}_0^T
    \end{pmatrix}. \]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B_0 = \mathbb{B}_{K}[\mathbf{z}_0] = I_{K\times K} \otimes \mathbf{z}_0^T
    = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_0^T & \cdots & \mathbf{e}_{K}\mathbf{z}_0^T
    \end{pmatrix}. \]
- en: In the second step of the forward layer loop, we compute
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向层循环的第二步中，我们计算
- en: \[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1) =
    \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1) = J_{\bgamma}(\mathbf{z}_1).
    \end{align*}\]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1) =
    \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1) = J_{\bgamma}(\mathbf{z}_1).
    \end{align*}\]
- en: So we need to compute the Jacobian of \(\bgamma\). We divide this computation
    into two cases. When \(1 \leq i = j \leq K\),
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要计算 \(\bgamma\) 的雅可比矩阵。我们将这个计算分为两种情况。当 \(1 \leq i = j \leq K\) 时，
- en: \[\begin{align*} (A_1)_{ii} &= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)
    - e^{z_{1,i}}\left(e^{z_{1,i}}\right)} {\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
    &= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2, \end{align*}\]
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (A_1)_{ii} &= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)
    - e^{z_{1,i}}\left(e^{z_{1,i}}\right)} {\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
    &= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2, \end{align*}\]
- en: by the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[商规则](https://en.wikipedia.org/wiki/Quotient_rule)。
- en: When \(1 \leq i, j \leq K\) with \(i \neq j\),
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(1 \leq i, j \leq K\) 且 \(i \neq j\) 时，
- en: \[\begin{align*} (A_1)_{ij} &= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)} {\left(\sum_{k=1}^{K}
    e^{z_{1,k}}\right)^2}\\ &= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1). \end{align*}\]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (A_1)_{ij} &= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)} {\left(\sum_{k=1}^{K}
    e^{z_{1,k}}\right)^2}\\ &= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1). \end{align*}\]
- en: In matrix form,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵形式中，
- en: \[ J_{\bgamma}(\mathbf{z}_1) = A_1 = \mathrm{diag}(\bgamma(\mathbf{z}_1)) -
    \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T. \]
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bgamma}(\mathbf{z}_1) = A_1 = \mathrm{diag}(\bgamma(\mathbf{z}_1)) -
    \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T. \]
- en: The Jacobian of the loss function is
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的雅可比矩阵是
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i}
    \right]^T = -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
    = - (\mathbf{y}\oslash\hat{\mathbf{y}})^T, \]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\hat{\mathbf{y}}) = \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i}
    \right]^T = -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
    = - (\mathbf{y}\oslash\hat{\mathbf{y}})^T, \]
- en: where recall that \(\oslash\) is the Hadamard division (i.e., element-wise division).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中记住 \(\oslash\) 是Hadamard除法（即逐元素除法）。
- en: We summarize the whole procedure next.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来总结整个过程。
- en: '*Initialization:*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化：*'
- en: \[\mathbf{z}_0 := \mathbf{x}\]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbf{z}_0 := \mathbf{x}\]
- en: '*Forward layer loop:*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*前向层循环：*'
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ \begin{pmatrix} A_0 & B_0 \end{pmatrix} &:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
    = \begin{pmatrix} \mathbb{A}_{K}[\mathbf{w}_0] & \mathbb{B}_{K}[\mathbf{z}_0]
    \end{pmatrix} \end{align*}\]\[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2
    &:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1)
    = \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
    \end{align*}\]
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ \begin{pmatrix} A_0 & B_0 \end{pmatrix} &:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
    = \begin{pmatrix} \mathbb{A}_{K}[\mathbf{w}_0] & \mathbb{B}_{K}[\mathbf{z}_0]
    \end{pmatrix} \end{align*}\]\[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2
    &:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1)
    = \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
    \end{align*}\]
- en: '*Loss:*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失：*'
- en: \[\begin{align*} z_3 &:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
    \mathbf{p}_2 &:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2) = -\left(\frac{y_1}{z_{2,1}},
    \ldots, \frac{y_K}{z_{2,K}}\right) = - \mathbf{y} \oslash \mathbf{z}_2. \end{align*}\]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} z_3 &:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
    \mathbf{p}_2 &:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2) = -\left(\frac{y_1}{z_{2,1}},
    \ldots, \frac{y_K}{z_{2,K}}\right) = - \mathbf{y} \oslash \mathbf{z}_2. \end{align*}\]
- en: '*Backward layer loop:*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向层循环:*'
- en: \[\begin{align*} \mathbf{p}_{1} &:= A_1^T \mathbf{p}_{2} \end{align*}\]\[\begin{align*}
    \mathbf{q}_{0} &:= B_0^T \mathbf{p}_{1} \end{align*}\]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{1} &:= A_1^T \mathbf{p}_{2} \end{align*}\]\[\begin{align*}
    \mathbf{q}_{0} &:= B_0^T \mathbf{p}_{1} \end{align*}\]
- en: '*Output:*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出:*'
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0, \]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0, \]
- en: where recall that \(\mathbf{w} := \mathbf{w}_0\).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中记住\(\mathbf{w} := \mathbf{w}_0\)。
- en: Explicit formulas can be derived from the previous recursion.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从之前的递归中推导出显式公式。
- en: We first compute \(\mathbf{p}_1\). We use the *Properties of the Hadamard Product*.
    We get
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算\(\mathbf{p}_1\)。我们使用了哈达玛积的性质。我们得到
- en: \[\begin{align*} \mathbf{p}_1 &= A_1^T \mathbf{p}_{2}\\ &= [\mathrm{diag}(\bgamma(\mathbf{z}_1))
    - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T [- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
    &= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))
    + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
    &= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\ &= \bgamma(\mathbf{z}_1)
    - \mathbf{y}, \end{align*}\]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_1 &= A_1^T \mathbf{p}_{2}\\ &= [\mathrm{diag}(\bgamma(\mathbf{z}_1))
    - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T [- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
    &= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))
    + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
    &= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\ &= \bgamma(\mathbf{z}_1)
    - \mathbf{y}, \end{align*}\]
- en: where we used that \(\sum_{k=1}^{K} y_k = 1\).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了\(\sum_{k=1}^{K} y_k = 1\)。
- en: It remains to compute \(\mathbf{q}_0\). We have by parts (e) and (f) of the
    *Properties of the Kronecker Product*
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是计算\(\mathbf{q}_0\)。根据克朗内克积的性质的(e)和(f)部分，我们有
- en: \[\begin{align*} \mathbf{q}_{0} = B_0^T \mathbf{p}_{1} &= (I_{K\times K} \otimes
    \mathbf{z}_0^T)^T (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= ( I_{K\times K} \otimes
    \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= (\bgamma(\mathbf{z}_1)
    - \mathbf{y}) \otimes \mathbf{z}_0. \end{align*}\]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{q}_{0} = B_0^T \mathbf{p}_{1} &= (I_{K\times K} \otimes
    \mathbf{z}_0^T)^T (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= ( I_{K\times K} \otimes
    \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= (\bgamma(\mathbf{z}_1)
    - \mathbf{y}) \otimes \mathbf{z}_0. \end{align*}\]
- en: Finally, replacing \(\mathbf{z}_0 = \mathbf{x}\) and \(\mathbf{z}_1 = \mathcal{W}
    \mathbf{x}\), the gradient is
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将\(\mathbf{z}_0 = \mathbf{x}\)和\(\mathbf{z}_1 = \mathcal{W} \mathbf{x}\)代入，梯度为
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0 = (\bgamma\left(\mathcal{W} \mathbf{x}\right)
    - \mathbf{y}) \otimes \mathbf{x}. \]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0 = (\bgamma\left(\mathcal{W} \mathbf{x}\right)
    - \mathbf{y}) \otimes \mathbf{x}. \]
- en: It can be shown that the objective function \(f(\mathbf{w})\) is convex in \(\mathbf{w}\).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明目标函数\(f(\mathbf{w})\)在\(\mathbf{w}\)上是凸的。
- en: '**NUMERICAL CORNER:** We will use the Fashion-MNIST dataset. This example is
    inspired by [these](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)
    [tutorials](https://www.tensorflow.org/tutorials/keras/classification). We first
    check for the availability of GPUs and load the data.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落:** 我们将使用Fashion-MNIST数据集。这个例子受到了[这些](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)
    [教程](https://www.tensorflow.org/tutorials/keras/classification)的启发。我们首先检查GPU的可用性并加载数据。'
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We used [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader),
    which provides utilities to load the data in batches for training. We took mini-batches
    of size `BATCH_SIZE = 32` and we apply a random permutation of the samples on
    every pass over the training data (with the option `shuffle=True`). The function
    [`torch.manual_seed()`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html)
    is used to set the global seed for PyTorch operations (e.g., weight initialization).
    The shuffling in `DataLoader` uses its own separate random number generator, which
    we initialize with [`torch.Generator()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator)
    and [`manual_seed()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed).
    (You can tell from the fact that `seed=42` that Claude explained that one to me…)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了[`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)，它提供了加载数据进行训练的实用工具。我们使用了大小为`BATCH_SIZE
    = 32`的小批量，并在每次遍历训练数据时对样本进行随机排列（使用选项`shuffle=True`）。函数[`torch.manual_seed()`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html)用于设置PyTorch操作的全球种子（例如，权重初始化）。`DataLoader`中的洗牌使用它自己的独立随机数生成器，我们使用[`torch.Generator()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator)和[`manual_seed()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed)进行初始化。（你可以从`seed=42`这个事实中看出，克劳德向我解释了这一点……）
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain the lines:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 请您向您喜欢的AI聊天机器人询问以下内容：'
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: \(\ddagger\)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \(\ddagger\)
- en: 'We implement multinomial logistic regression to learn a classifier for the
    Fashion-MNIST data. In PyTorch, composition of functions can be achieved with
    [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).
    Our model is:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现多项式逻辑回归来学习Fashion-MNIST数据的分类器。在PyTorch中，可以使用 `torch.nn.Sequential` 实现函数的组合。我们的模型是：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)
    layer turns each input image into a vector of size \(784\) (where \(784 = 28^2\)
    is the number of pixels in each image). After the flattening, we have an affine
    map from \(\mathbb{R}^{784}\) to \(\mathbb{R}^{10}\). Note that there is no need
    to pre-process the inputs by adding \(1\)s. A constant term (or “bias variable”)
    is automatically added by PyTorch (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).
    The final output is \(10\)-dimensional.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Flatten` 层将每个输入图像转换为大小为 \(784\) 的向量（其中 \(784 = 28^2\) 是每张图像中的像素数）。在展平之后，我们有一个从
    \(\mathbb{R}^{784}\) 到 \(\mathbb{R}^{10}\) 的仿射映射。请注意，不需要通过添加 \(1\) 来预先处理输入。PyTorch会自动添加一个常数项（或“偏置变量”），除非选择选项
    `bias=False`（见[这里](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)）。最终输出是
    \(10\) 维的。'
- en: Finally, we are ready to run an optimization method of our choice on the loss
    function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)
    available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)
    for a brief explanation of many common optimizers.) Here we use SGD as the optimizer.
    A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).
    The loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy),
    as implemented by [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),
    which first takes the softmax and expects the labels to be the actual class labels
    rather than their one-hot encoding.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好在我们的损失函数上运行我们选择的优化方法，这些方法将在下面指定。有许多[优化器](https://pytorch.org/docs/stable/optim.html#algorithms)可供选择。（参见[这篇帖子](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)，了解许多常见优化器的简要说明。）这里我们使用SGD作为优化器。快速教程[在这里](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)。损失函数是[交叉熵](https://en.wikipedia.org/wiki/Cross_entropy)，由
    `torch.nn.CrossEntropyLoss` 实现，它首先执行softmax操作，并期望标签是实际的类别标签而不是它们的one-hot编码。
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We implement special functions for training.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了特殊的训练函数。
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: An epoch is one training iteration where all samples are iterated once (in a
    randomly shuffled order). In the interest of time, we train for 10 epochs only.
    But it does better if you train it longer (try it!). On each pass, we compute
    the output of the current model, use `backward()` to obtain the gradient, and
    then perform a descent update with `step()`. We also have to reset the gradients
    first (otherwise they add up by default).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一个epoch是一次训练迭代，其中所有样本都会被迭代一次（以随机打乱顺序）。为了节省时间，我们只训练了10个epochs。但如果你训练得更久，效果会更好（试试看！）在每次遍历中，我们计算当前模型的输出，使用
    `backward()` 获取梯度，然后使用 `step()` 执行下降更新。我们还需要首先重置梯度（否则它们会默认累加）。
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Because of the issue of [overfitting](https://en.wikipedia.org/wiki/Overfitting),
    we use the *test* images to assess the performance of the final classifier.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[过拟合](https://en.wikipedia.org/wiki/Overfitting)的问题，我们使用测试图像来评估最终分类器的性能。
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To make a prediction, we take a [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    of the output of our model. Recall that it is implicitly included in `torch.nn.CrossEntropyLoss`,
    but is not actually part of `model`. (Note that the softmax itself has no parameter.)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行预测，我们对模型的输出执行 `torch.nn.functional.softmax`。回想一下，它隐式地包含在 `torch.nn.CrossEntropyLoss`
    中，但不是 `model` 的实际部分。（注意，softmax本身没有参数。）
- en: As an illustration, we do this for each test image. We use [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html)
    to concatenate a sequence of tensors into a single tensor.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 作为说明，我们对每个测试图像都这样做。我们使用 `torch.cat` 将一系列张量连接成一个单独的张量。
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The result for the first test image is shown below. To make a prediction, we
    choose the label with the highest probability.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张测试图像的结果如下。为了做出预测，我们选择概率最高的标签。
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The truth is:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 真实情况是：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Above, `next(iter(test_loader))` loads the first batch of test images. (See
    [here](https://docs.python.org/3/tutorial/classes.html#iterators) for background
    on iterators in Python.)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的`next(iter(test_loader))`加载了第一个测试图像批次。（有关Python中迭代器的背景信息，请参阅[这里](https://docs.python.org/3/tutorial/classes.html#iterators)。）
- en: \(\unlhd\)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** In stochastic gradient descent (SGD), how is the gradient estimated at
    each iteration?'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在随机梯度下降（SGD）中，每个迭代中如何估计梯度？'
- en: a) By computing the gradient over the entire dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: a) 通过在整个数据集上计算梯度。
- en: b) By using the gradient from the previous iteration.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: b) 使用上一迭代的梯度。
- en: c) By randomly selecting a subset of sample and computing their gradient.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: c) 通过随机选择样本子集并计算它们的梯度。
- en: d) By averaging the gradients of all samples in the dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: d) 通过平均数据集中所有样本的梯度。
- en: '**2** What is the key advantage of using mini-batch SGD over standard SGD?'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 使用小批量SGD而不是标准SGD的关键优势是什么？'
- en: a) It guarantees faster convergence to the optimal solution.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它保证了更快地收敛到最优解。
- en: b) It reduces the variance of the gradient estimate at each iteration.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它在每个迭代中减少了梯度估计的方差。
- en: c) It eliminates the need for computing gradients altogether.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它消除了计算梯度的需要。
- en: d) It increases the computational cost per iteration.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它增加了每个迭代的计算成本。
- en: '**3** Which of the following statements is true about the update step in stochastic
    gradient descent?'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 关于随机梯度下降中的更新步骤，以下哪个陈述是正确的？'
- en: a) It is always equal to the full gradient descent update.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它始终等于完整梯度下降更新。
- en: b) It is always in the opposite direction of the full gradient descent update.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它始终与完整梯度下降更新的方向相反。
- en: c) It is, on average, equal to the full gradient descent update.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: c) 平均而言，它等于完整梯度下降更新。
- en: d) It has no relationship to the full gradient descent update.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它与完整梯度下降更新没有关系。
- en: '**4** In multinomial logistic regression, what is the role of the softmax function
    \(\boldsymbol{\gamma}\)?'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在多项式逻辑回归中，softmax函数\(\boldsymbol{\gamma}\)的作用是什么？'
- en: a) To compute the gradient of the loss function.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: a) 计算损失函数的梯度。
- en: b) To normalize the input features.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: b) 用于归一化输入特征。
- en: c) To transform scores into a probability distribution over labels.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: c) 将分数转换成标签的概率分布。
- en: d) To update the model parameters during gradient descent.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: d) 在梯度下降过程中更新模型参数。
- en: '**5** What is the Kullback-Leibler (KL) divergence used for in multinomial
    logistic regression?'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在多项式逻辑回归中，Kullback-Leibler（KL）散度有什么用途？'
- en: a) To measure the distance between the predicted probabilities and the true
    labels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: a) 测量预测概率与真实标签之间的距离。
- en: b) To normalize the input features.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: b) 用于归一化输入特征。
- en: c) To update the model parameters during gradient descent.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: c) 在梯度下降过程中更新模型参数。
- en: d) To compute the gradient of the loss function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: d) 计算损失函数的梯度。
- en: 'Answer for 1: c. Justification: The text states that in SGD, “we pick a sample
    uniformly at random in \(\{1, ..., n\}\) and update as follows \(\mathbf{w}^{t+1}
    = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)”'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 1的答案：c. 理由：文本指出在SGD中，“我们在\(\{1, ..., n\}\)中随机选择一个样本，并按以下方式更新：\(\mathbf{w}^{t+1}
    = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)”
- en: 'Answer for 2: b. Justification: The text implies that mini-batch SGD reduces
    the variance of the gradient estimate compared to standard SGD, which only uses
    a single sample.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 2的答案：b. 理由：文本暗示与标准SGD相比，小批量SGD减少了梯度估计的方差，而标准SGD只使用单个样本。
- en: 'Answer for 3: c. Justification: The text proves a lemma stating that “in expectation,
    they [stochastic updates] perform a step of gradient descent.”'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 3的答案：c. 理由：文本证明了一个引理，表明“在期望中，它们[随机更新]执行梯度下降的一步。”
- en: 'Answer for 4: c. Justification: The text defines the softmax function and states
    that it is used to “transform these scores into a probability distribution over
    the labels.”'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 4的答案：c. 理由：文本定义了softmax函数，并指出它被用来“将这些分数转换成标签的概率分布。”
- en: 'Answer for 5: a. Justification: The text introduces the KL divergence as a
    “notion of distance between probability measures” and uses it to define the loss
    function in multinomial logistic regression.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 5号问题的答案：a. 证明：文本将KL散度引入为“概率测度之间的距离概念”，并使用它来定义多项式逻辑回归中的损失函数。
- en: 8.4.1\. Algorithm[#](#algorithm "Link to this heading")
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4.1\. 算法[#](#algorithm "链接到这个标题")
- en: In [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
    (SGD)\(\idx{stochastic gradient descent}\xdi\), a variant of gradient descent,
    we pick a sample \(I_t\) uniformly at random in \(\{1,\ldots,n\}\) and update
    as follows
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)（SGD）中，梯度下降的一个变体，我们在
    \(\{1,\ldots,n\}\) 中随机均匀地选择一个样本 \(I_t\)，并按以下方式更新
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
    \]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
    \]
- en: More generally, in the so-called mini-batch version of SGD, we pick instead
    a uniformly random sub-sample \(\mathcal{B}_t \subseteq \{1,\ldots,n\}\) of size
    \(b\) without replacement (i.e., all sub-samples of that size have the same probability
    of being picked)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，在所谓的SGD的小批量版本中，我们选择一个大小为 \(b\) 的均匀随机子样本 \(\mathcal{B}_t \subseteq \{1,\ldots,n\}\)
    而不是替换（即，所有该大小的子样本被选中的概率相同）
- en: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t}
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}). \]
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t}
    \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}). \]
- en: The key observation about the two stochastic updates above is that, in expectation,
    they perform a step of gradient descent. That turns out to be enough and it has
    computational advantages.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 关于上述两个随机更新的关键观察是，在期望上，它们执行了一步梯度下降。这证明是足够的，并且具有计算上的优势。
- en: '**LEMMA** Fix a batch size \(1 \leq b \leq n\) and and an arbitrary vector
    of parameters \(\mathbf{w}\). Let \(\mathcal{B} \subseteq \{1,\ldots,n\}\) be
    a uniformly random sub-sample of size \(b\). Then'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** 设定一个批大小 \(1 \leq b \leq n\) 和一个任意的参数向量 \(\mathbf{w}\)。令 \(\mathcal{B}
    \subseteq \{1,\ldots,n\}\) 是大小为 \(b\) 的均匀随机子样本。那么'
- en: \[ \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}). \]
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
    = \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}). \]
- en: \(\flat\)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* Because \(\mathcal{B}\) is picked uniformly at random (without replacement),
    for any sub-sample \(B \subseteq \{1,\ldots,n\}\) of size \(b\) without repeats'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 因为 \(\mathcal{B}\) 是随机均匀选择（不替换），对于任何大小为 \(b\) 的不重复子样本 \(B \subseteq \{1,\ldots,n\}\)'
- en: \[ \mathbb{P}[\mathcal{B} = B] = \frac{1}{\binom{n}{b}}. \]
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}[\mathcal{B} = B] = \frac{1}{\binom{n}{b}}. \]
- en: So, summing over all such sub-samples, we obtain
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对所有这样的子样本求和，我们得到
- en: \[\begin{align*} \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla
    f_{\mathbf{x}_i,y_i}(\mathbf{w})\right] &= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B}
    = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{B
    \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i
    \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})
    \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
    \end{align*}\]
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla
    f_{\mathbf{x}_i,y_i}(\mathbf{w})\right] &= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B}
    = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{B
    \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i
    \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\ &= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})
    \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
    \end{align*}\]
- en: Computing the internal sum requires a combinatorial argument. Indeed, \(\sum_{B
    \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\) counts the number of ways that
    \(i\) can be picked in a sub-sample of size \(b\) without repeats. That is \(\binom{n-1}{b-1}\),
    which is the number of ways of picking the remaining \(b-1\) elements of \(B\)
    from the other \(n-1\) possible elements. By definition of the binomial coefficient
    and the properties of factorials,
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 计算内部和需要组合论证。实际上，\(\sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\)
    计算了 \(i\) 在大小为 \(b\) 的子样本中不重复被选择的方式数。这就是 \(\binom{n-1}{b-1}\)，这是从其他 \(n-1\) 个可能元素中选择
    \(B\) 的剩余 \(b-1\) 个元素的方式数。根据二项式系数的定义和阶乘的性质，
- en: \[ \frac{\binom{n-1}{b-1}}{b \binom{n}{b}} = \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b
    \frac{n!}{b! (n-b)!}} = \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!} = \frac{1}{n}. \]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\binom{n-1}{b-1}}{b \binom{n}{b}} = \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b
    \frac{n!}{b! (n-b)!}} = \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!} = \frac{1}{n}. \]
- en: Plugging back gives the claim. \(\square\)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 将其代入给出结论。 \(\square\)
- en: 'As a first illustration, we return to logistic regression\(\idx{logistic regression}\xdi\).
    Recall that the input data is of the form \(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots,
    n\}\) where \(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in
    \mathbb{R}^d\) are the features and \(b_i \in \{0,1\}\) is the label. As before
    we use a matrix representation: \(A \in \mathbb{R}^{n \times d}\) has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). We
    want to solve the minimization problem'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '作为第一个示例，我们回到逻辑回归\(\idx{logistic regression}\xdi\)。回想一下，输入数据的形式为\(\{(\boldsymbol{\alpha}_i,
    b_i) : i=1,\ldots, n\}\)，其中\(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d})
    \in \mathbb{R}^d\)是特征，\(b_i \in \{0,1\}\)是标签。和之前一样，我们使用矩阵表示：\(A \in \mathbb{R}^{n
    \times d}\)的行是\(\boldsymbol{\alpha}_i^T\)，\(i = 1,\ldots, n\)，\(\mathbf{b} = (b_1,
    \ldots, b_n) \in \{0,1\}^n\)。我们想要解决最小化问题'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}) \]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}) \]
- en: where the loss is
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中损失为
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right). \end{align*}\]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right). \end{align*}\]
- en: The gradient was previously computed as
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 前向梯度之前被计算为
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= - \frac{1}{n} \sum_{i=1}^n
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i\\
    &= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \end{align*}\]
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= - \frac{1}{n} \sum_{i=1}^n
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i\\
    &= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \end{align*}\]
- en: For the mini-batch version of SGD, we pick a random sub-sample \(\mathcal{B}_t
    \subseteq \{1,\ldots,n\}\) of size \(B\) and take the step
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小批量版本的随机梯度下降（SGD），我们随机选择一个大小为\(B\)的子样本\(\mathcal{B}_t \subseteq \{1,\ldots,n\}\)，并采取以下步骤
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t}
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i.
    \]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t}
    ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i.
    \]
- en: We modify our previous code for logistic regression. The only change is to pick
    a random mini-batch which can be fed to the descent update sub-routine as dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改了我们之前的逻辑回归代码。唯一的改变是随机选择一个迷你批量，将其作为数据集输入到下降更新子例程中。
- en: '[PRE28]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**NUMERICAL CORNER:** We analyze a dataset from [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)],
    which can be downloaded [here](https://web.stanford.edu/~hastie/ElemStatLearn/data.html).
    Quoting [[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/), Section 4.4.2]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们分析了来自[[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)]的数据集，该数据集可以在此[下载](https://web.stanford.edu/~hastie/ElemStatLearn/data.html)。引用[[ESL](https://web.stanford.edu/~hastie/ElemStatLearn/)]，第4.4.2节'
- en: The data […] are a subset of the Coronary Risk-Factor Study (CORIS) baseline
    survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw
    et al., 1983). The aim of the study was to establish the intensity of ischemic
    heart disease risk factors in that high-incidence region. The data represent white
    males between 15 and 64, and the response variable is the presence or absence
    of myocardial infarction (MI) at the time of the survey (the overall prevalence
    of MI was 5.1% in this region). There are 160 cases in our data set, and a sample
    of 302 controls. These data are described in more detail in Hastie and Tibshirani
    (1987).
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据 [...] 是南非开普敦西部三个农村地区的冠状动脉风险因素研究（CORIS）基线调查的一个子集（Rousseauw等，1983年）。该研究旨在确定该高发病率地区缺血性心脏病风险因素的强度。数据代表15至64岁的白人男性，响应变量是在调查时心肌梗死（MI）的存在或不存在（该地区MI的整体患病率为5.1%）。我们的数据集中有160个病例，302个对照组样本。这些数据在Hastie和Tibshirani（1987年）中描述得更详细。
- en: We load the data, which we slightly reformatted and look at a summary.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载数据，我们稍作格式化并查看摘要。
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|  | sbp | tobacco | ldl | adiposity | typea | obesity | alcohol | age | chd
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | sbp | tobacco | ldl | adiposity | typea | obesity | alcohol | age | chd
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 160.0 | 12.00 | 5.73 | 23.11 | 49.0 | 25.30 | 97.20 | 52.0 | 1.0 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 160.0 | 12.00 | 5.73 | 23.11 | 49.0 | 25.30 | 97.20 | 52.0 | 1.0 |'
- en: '| 1 | 144.0 | 0.01 | 4.41 | 28.61 | 55.0 | 28.87 | 2.06 | 63.0 | 1.0 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 144.0 | 0.01 | 4.41 | 28.61 | 55.0 | 28.87 | 2.06 | 63.0 | 1.0 |'
- en: '| 2 | 118.0 | 0.08 | 3.48 | 32.28 | 52.0 | 29.14 | 3.81 | 46.0 | 0.0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 118.0 | 0.08 | 3.48 | 32.28 | 52.0 | 29.14 | 3.81 | 46.0 | 0.0 |'
- en: '| 3 | 170.0 | 7.50 | 6.41 | 38.03 | 51.0 | 31.99 | 24.26 | 58.0 | 1.0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 170.0 | 7.50 | 6.41 | 38.03 | 51.0 | 31.99 | 24.26 | 58.0 | 1.0 |'
- en: '| 4 | 134.0 | 13.60 | 3.50 | 27.78 | 60.0 | 25.99 | 57.34 | 49.0 | 1.0 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 134.0 | 13.60 | 3.50 | 27.78 | 60.0 | 25.99 | 57.34 | 49.0 | 1.0 |'
- en: Our goal to predict `chd`, which stands for coronary heart disease, based on
    the other variables (which are briefly described [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)).
    We use logistic regression again.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是根据其他变量（简要描述见[此处](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt)）预测
    `chd`，即冠心病。我们再次使用逻辑回归。
- en: We first construct the data matrices. We only use three of the predictors.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建数据矩阵。我们只使用三个预测变量。
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We try mini-batch SGD.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试使用小批量随机梯度下降法。
- en: '[PRE33]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The outcome is harder to vizualize. To get a sense of how accurate the result
    is, we compare our predictions to the true labels. By prediction, let us say that
    we mean that we predict label \(1\) whenever \(\sigma(\boldsymbol{\alpha}^T \mathbf{x})
    > 1/2\). We try this on the training set. (A better approach would be to split
    the data into training and testing sets, but we will not do this here.)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 结果更难以可视化。为了了解结果的准确性，我们将我们的预测与真实标签进行比较。让我们假设，当我们说预测时，我们指的是当 \(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) > 1/2\) 时预测标签 \(1\)。我们尝试在训练集上这样做。（更好的方法是将数据分成训练集和测试集，但在这里我们不会这样做。）
- en: '[PRE35]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: \(\unlhd\)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '8.4.2\. Example: multinomial logistic regression[#](#example-multinomial-logistic-regression
    "Link to this heading")'
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4.2\. 示例：多元逻辑回归[#](#example-multinomial-logistic-regression "链接到本标题")
- en: We give a concrete example of progressive functions and of the application of
    backpropagation and SGD.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给出一个关于渐进函数以及反向传播和随机梯度下降法应用的实例。
- en: Recall that a classifier \(h\) takes an input in \(\mathbb{R}^d\) and predicts
    one of \(K\) possible labels. It will be convenient for reasons that will become
    clear below to use [one-hot encoding](https://en.wikipedia.org/wiki/One-hot)\(\idx{one-hot
    encoding}\xdi\) of the labels. That is, we encode label \(i\) as the \(K\)-dimensional
    vector \(\mathbf{e}_i\). Here, as usual, \(\mathbf{e}_i\) the standard basis of
    \(\mathbb{R}^K\), i.e., the vector with a \(1\) in entry \(i\) and a \(0\) elsewhere.
    Furthermore, we allow the output of the classifier to be a probability distribution
    over the labels \(\{1,\ldots,K\}\), that is, a vector in
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，分类器 \(h\) 接受 \(\mathbb{R}^d\) 中的输入并预测 \(K\) 个可能标签中的一个。为了下面的原因，使用 [独热编码](https://en.wikipedia.org/wiki/One-hot)\(\idx{one-hot
    encoding}\xdi\) 的标签将很方便。也就是说，我们将标签 \(i\) 编码为 \(K\) 维向量 \(\mathbf{e}_i\)。在这里，像往常一样，\(\mathbf{e}_i\)
    是 \(\mathbb{R}^K\) 的标准基，即在第 \(i\) 个位置有 \(1\)，其他位置为 \(0\) 的向量。此外，我们允许分类器的输出是标签
    \(\{1,\ldots,K\}\) 的概率分布，即一个向量在
- en: \[ \Delta_K = \left\{ (p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k =
    1 \right\}. \]
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \Delta_K = \left\{ (p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k =
    1 \right\}. \]
- en: Observe that \(\mathbf{e}_i\) can itself be thought of as a probability distribution,
    one that assigns probability one to \(i\).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mathbf{e}_i\) 本身也可以被视为一个概率分布，它将概率 \(1\) 分配给 \(i\)。
- en: '**Background on multinomial logistic regression** We use [multinomial logistic
    regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\(\idx{multinomial
    logistic regression}\xdi\) to learn a classifier over \(K\) labels. In multinomial
    logistic regression, we once again use an affine function of the input data.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**多元逻辑回归背景** 我们使用 [多元逻辑回归](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\(\idx{multinomial
    logistic regression}\xdi\) 来学习 \(K\) 个标签的分类器。在多元逻辑回归中，我们再次使用输入数据的仿射函数。'
- en: 'This time, we have \(K\) functions that output a score associated to each label.
    We then transform these scores into a probability distribution over the \(K\)
    labels. There are many ways of doing this. A standard approach is the [softmax
    function](https://en.wikipedia.org/wiki/Softmax_function)\(\idx{softmax}\xdi\)
    \(\bgamma = (\gamma_1,\ldots,\gamma_K)\): for \(\mathbf{z} \in \mathbb{R}^K\)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们有 \(K\) 个函数，每个函数输出与每个标签相关的分数。然后我们将这些分数转换成 \(K\) 个标签上的概率分布。有多种方法可以做到这一点。一种标准的方法是
    [softmax 函数](https://en.wikipedia.org/wiki/Softmax_function)\(\idx{softmax}\xdi\)
    \(\bgamma = (\gamma_1,\ldots,\gamma_K)\)：对于 \(\mathbf{z} \in \mathbb{R}^K\)
- en: \[ \gamma_i(\mathbf{z}) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \quad i=1,\ldots,K.
    \]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \gamma_i(\mathbf{z}) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \quad i=1,\ldots,K.
    \]
- en: To explain the name, observe that the larger inputs are mapped to larger probabilities.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释名称，观察较大的输入被映射到较大的概率。
- en: In fact, since a probability distribution must sum to \(1\), it is determined
    by the probabilities assigned to the first \(K-1\) labels. In other words, we
    could drop the score associated to the last label. But the keep the notation simple,
    we will not do this here.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，由于概率分布必须求和为 \(1\)，它由分配给前 \(K-1\) 个标签的概率决定。换句话说，我们可以省略与最后一个标签相关的分数。但为了简化符号，我们在这里不会这样做。
- en: For each \(k\), we have a regression function
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 \(k\)，我们有一个回归函数
- en: \[ \sum_{j=1}^d w^{(k)}_{j} x_{j} = \mathbf{x}_1^T \mathbf{w}^{(k)}, \quad k=1,\ldots,K
    \]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{j=1}^d w^{(k)}_{j} x_{j} = \mathbf{x}_1^T \mathbf{w}^{(k)}, \quad k=1,\ldots,K
    \]
- en: where \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\) are the parameters
    with \(\mathbf{w}^{(k)} \in \mathbb{R}^{d}\) and \(\mathbf{x} \in \mathbb{R}^d\)
    is the input. A constant term can be included by adding an additional entry \(1\)
    to \(\mathbf{x}\). As we did in the linear regression case, we assume that this
    pre-processing has been performed previously. To simplify the notation, we let
    \(\mathcal{W} \in \mathbb{R}^{K \times d}\) as the matrix with rows \((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\) 是参数，其中 \(\mathbf{w}^{(k)}
    \in \mathbb{R}^{d}\) 且 \(\mathbf{x} \in \mathbb{R}^d\) 是输入。可以通过向 \(\mathbf{x}\)
    添加一个额外的条目 \(1\) 来包含一个常数项。正如我们在线性回归案例中所做的那样，我们假设这种预处理已经完成。为了简化符号，我们让 \(\mathcal{W}
    \in \mathbb{R}^{K \times d}\) 作为具有行 \((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\)
    的矩阵。
- en: The output of the classifier is
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的输出是
- en: \[\begin{align*} \bfh(\mathbf{w}) &= \bgamma\left(\mathcal{W} \mathbf{x}\right),
    \end{align*}\]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \bfh(\mathbf{w}) &= \bgamma\left(\mathcal{W} \mathbf{x}\right),
    \end{align*}\]
- en: for \(i=1,\ldots,K\), where \(\bgamma\) is the softmax function. Note that the
    latter has no associated parameter.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i=1,\ldots,K\)，其中 \(\bgamma\) 是 softmax 函数。注意，后者没有关联的参数。
- en: It remains to define a loss function. To quantify the fit, it is natural to
    use a notion of distance between probability measures, here between the output
    \(\mathbf{h}(\mathbf{w}) \in \Delta_K\) and the correct label \(\mathbf{y} \in
    \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq \Delta_K\). There are many such
    measures. In multinomial logistic regression, we use the Kullback-Leibler divergence,
    which we have encountered in the context of maximum likelihood estimation. Recall
    that, for two probability distributions \(\mathbf{p}, \mathbf{q} \in \Delta_K\),
    it is defined as
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的任务是定义一个损失函数。为了量化拟合度，自然地使用概率测度之间的距离概念，这里是在输出 \(\mathbf{h}(\mathbf{w}) \in
    \Delta_K\) 和正确标签 \(\mathbf{y} \in \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq
    \Delta_K\) 之间的距离。存在许多这样的度量。在多项式逻辑回归中，我们使用 Kullback-Leibler 散度，这在最大似然估计的上下文中已经遇到过。回想一下，对于两个概率分布
    \(\mathbf{p}, \mathbf{q} \in \Delta_K\)，它被定义为
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
- en: where it will suffice to restrict ourselves to the case \(\mathbf{q} > \mathbf{0}\)
    and where we use the convention \(0 \log 0 = 0\) (so that terms with \(p_i = 0\)
    contribute \(0\) to the sum). Notice that \(\mathbf{p} = \mathbf{q}\) implies
    \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) = 0\). We proved previously that \(\mathrm{KL}(\mathbf{p}
    \| \mathbf{q}) \geq 0\), a result known as *Gibbs’ inequality*.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，我们只需将注意力限制在 \(\mathbf{q} > \mathbf{0}\) 的情况，并且使用约定 \(0 \log 0 = 0\)（这样 \(p_i
    = 0\) 的项对总和的贡献为 \(0\)）。注意，\(\mathbf{p} = \mathbf{q}\) 意味着 \(\mathrm{KL}(\mathbf{p}
    \| \mathbf{q}) = 0\)。我们之前已经证明了 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)，这是一个被称为
    *吉布斯不等式* 的结果。
- en: Going back to the loss function, we use the identity \(\log\frac{\alpha}{\beta}
    = \log \alpha - \log \beta\) to re-write
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 回到损失函数，我们使用恒等式 \(\log\frac{\alpha}{\beta} = \log \alpha - \log \beta\) 来重新写
- en: \[\begin{align*} \mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w})) &= \sum_{i=1}^K
    y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\ &= \sum_{i=1}^K y_i \log y_i - \sum_{i=1}^K
    y_i \log h_{i}(\mathbf{w}), \end{align*}\]
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w})) &= \sum_{i=1}^K
    y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\ &= \sum_{i=1}^K y_i \log y_i - \sum_{i=1}^K
    y_i \log h_{i}(\mathbf{w}), \end{align*}\]
- en: where \(\bfh = (h_{1},\ldots,h_{K})\). Notice that the first term on right-hand
    side does not depend on \(\mathbf{w}\). Hence we can ignore it when optimizing
    \(\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))\). The remaining term is
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bfh = (h_{1},\ldots,h_{K})\)。注意，右侧的第一个项不依赖于 \(\mathbf{w}\)。因此，在优化 \(\mathrm{KL}(\mathbf{y}
    \| \bfh(\mathbf{w}))\) 时可以忽略它。剩余的项是
- en: \[ H(\mathbf{y}, \bfh(\mathbf{w})) = - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
    \]
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H(\mathbf{y}, \bfh(\mathbf{w})) = - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
    \]
- en: We use it to define our loss function. That is, we set
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用它来定义我们的损失函数。也就是说，我们设置
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_{i}. \]
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_{i}. \]
- en: Finally,
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfh(\mathbf{w}))\\ &= H(\mathbf{y},
    \bfh(\mathbf{w}))\\ &= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
    &= - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right). \end{align*}\]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfh(\mathbf{w}))\\ &= H(\mathbf{y},
    \bfh(\mathbf{w}))\\ &= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
    &= - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right). \end{align*}\]
- en: '**Computing the gradient** We apply the forward and backpropagation steps from
    the previous section. We then use the resulting recursions to derive an analytical
    formula for the gradient.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算梯度** 我们应用了上一节中的前向和反向传播步骤。然后我们使用这些递归关系推导出梯度的解析公式。'
- en: The forward pass starts with the initialization \(\mathbf{z}_0 := \mathbf{x}\).
    The forward layer loop has two steps. Set \(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)
    equal to \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\). First we
    compute
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播从初始化 \(\mathbf{z}_0 := \mathbf{x}\) 开始。前向层循环有两个步骤。将 \(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)
    等于 \(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\)。首先我们计算
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0) &:=\begin{pmatrix} A_0 &
    B_0 \end{pmatrix} \end{align*}\]
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0) &:=\begin{pmatrix} A_0 &
    B_0 \end{pmatrix} \end{align*}\]
- en: 'where we defined \(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\) as the matrix
    with rows \((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\). We have
    previously computed the Jacobian:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\) 为具有行 \((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\)
    的矩阵。我们之前已经计算了雅可比矩阵：
- en: \[\begin{split} A_0 = \mathbb{A}_{K}[\mathbf{w}_0] = \mathcal{W}_0 = \begin{pmatrix}
    (\mathbf{w}^{(1)}_0)^T\\ \vdots\\ (\mathbf{w}^{(K)}_0)^T \end{pmatrix} \end{split}\]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A_0 = \mathbb{A}_{K}[\mathbf{w}_0] = \mathcal{W}_0 = \begin{pmatrix}
    (\mathbf{w}^{(1)}_0)^T\\ \vdots\\ (\mathbf{w}^{(K)}_0)^T \end{pmatrix} \end{split}\]
- en: and
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ B_0 = \mathbb{B}_{K}[\mathbf{z}_0] = I_{K\times K} \otimes \mathbf{z}_0^T
    = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_0^T & \cdots & \mathbf{e}_{K}\mathbf{z}_0^T
    \end{pmatrix}. \]
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B_0 = \mathbb{B}_{K}[\mathbf{z}_0] = I_{K\times K} \otimes \mathbf{z}_0^T
    = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_0^T & \cdots & \mathbf{e}_{K}\mathbf{z}_0^T
    \end{pmatrix}. \]
- en: In the second step of the forward layer loop, we compute
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向层循环的第二步中，我们计算
- en: \[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1) =
    \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1) = J_{\bgamma}(\mathbf{z}_1).
    \end{align*}\]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2 &:= \bfg_1(\mathbf{z}_1) =
    \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1) = J_{\bgamma}(\mathbf{z}_1).
    \end{align*}\]
- en: So we need to compute the Jacobian of \(\bgamma\). We divide this computation
    into two cases. When \(1 \leq i = j \leq K\),
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要计算 \(\bgamma\) 的雅可比矩阵。我们将这个计算分为两种情况。当 \(1 \leq i = j \leq K\)，
- en: \[\begin{align*} (A_1)_{ii} &= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)
    - e^{z_{1,i}}\left(e^{z_{1,i}}\right)} {\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
    &= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2, \end{align*}\]
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (A_1)_{ii} &= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)
    - e^{z_{1,i}}\left(e^{z_{1,i}}\right)} {\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
    &= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2, \end{align*}\]
- en: by the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[商规则](https://en.wikipedia.org/wiki/Quotient_rule)。
- en: When \(1 \leq i, j \leq K\) with \(i \neq j\),
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(1 \leq i, j \leq K\) 且 \(i \neq j\) 时，
- en: \[\begin{align*} (A_1)_{ij} &= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)} {\left(\sum_{k=1}^{K}
    e^{z_{1,k}}\right)^2}\\ &= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1). \end{align*}\]
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} (A_1)_{ij} &= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1)
    \right]\\ &= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K}
    e^{z_{1,k}}} \right]\\ &= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)} {\left(\sum_{k=1}^{K}
    e^{z_{1,k}}\right)^2}\\ &= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1). \end{align*}\]
- en: In matrix form,
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式，
- en: \[ J_{\bgamma}(\mathbf{z}_1) = A_1 = \mathrm{diag}(\bgamma(\mathbf{z}_1)) -
    \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T. \]
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bgamma}(\mathbf{z}_1) = A_1 = \mathrm{diag}(\bgamma(\mathbf{z}_1)) -
    \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T. \]
- en: The Jacobian of the loss function is
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的雅可比矩阵是
- en: \[ J_{\ell}(\hat{\mathbf{y}}) = \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i}
    \right]^T = -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
    = - (\mathbf{y}\oslash\hat{\mathbf{y}})^T, \]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\ell}(\hat{\mathbf{y}}) = \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i}
    \right]^T = -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
    = - (\mathbf{y}\oslash\hat{\mathbf{y}})^T, \]
- en: where recall that \(\oslash\) is the Hadamard division (i.e., element-wise division).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中回忆起 \(\oslash\) 是哈达玛除法（即逐元素除法）。
- en: We summarize the whole procedure next.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来总结整个流程。
- en: '*Initialization:*'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化：*'
- en: \[\mathbf{z}_0 := \mathbf{x}\]
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbf{z}_0 := \mathbf{x}\]
- en: '*Forward layer loop:*'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*正向层循环：*'
- en: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ \begin{pmatrix} A_0 & B_0 \end{pmatrix} &:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
    = \begin{pmatrix} \mathbb{A}_{K}[\mathbf{w}_0] & \mathbb{B}_{K}[\mathbf{z}_0]
    \end{pmatrix} \end{align*}\]\[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2
    &:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1)
    = \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
    \end{align*}\]
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{1} &:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \mathcal{W}_0
    \mathbf{z}_0\\ \begin{pmatrix} A_0 & B_0 \end{pmatrix} &:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
    = \begin{pmatrix} \mathbb{A}_{K}[\mathbf{w}_0] & \mathbb{B}_{K}[\mathbf{z}_0]
    \end{pmatrix} \end{pmatrix}\]\[\begin{align*} \hat{\mathbf{y}} := \mathbf{z}_2
    &:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\ A_1 &:= J_{\bfg_1}(\mathbf{z}_1)
    = \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
    \end{align*}\]
- en: '*Loss:*'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失：*'
- en: \[\begin{align*} z_3 &:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
    \mathbf{p}_2 &:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2) = -\left(\frac{y_1}{z_{2,1}},
    \ldots, \frac{y_K}{z_{2,K}}\right) = - \mathbf{y} \oslash \mathbf{z}_2. \end{align*}\]
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} z_3 &:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
    \mathbf{p}_2 &:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2) = -\left(\frac{y_1}{z_{2,1}},
    \ldots, \frac{y_K}{z_{2,K}}\right) = - \mathbf{y} \oslash \mathbf{z}_2. \end{align*}\]
- en: '*Backward layer loop:*'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向层循环：*'
- en: \[\begin{align*} \mathbf{p}_{1} &:= A_1^T \mathbf{p}_{2} \end{align*}\]\[\begin{align*}
    \mathbf{q}_{0} &:= B_0^T \mathbf{p}_{1} \end{align*}\]
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{1} &:= A_1^T \mathbf{p}_{2} \end{align*}\]\[\begin{align*}
    \mathbf{q}_{0} &:= B_0^T \mathbf{p}_{1} \end{align*}\]
- en: '*Output:*'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0, \]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0, \]
- en: where recall that \(\mathbf{w} := \mathbf{w}_0\).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 其中回忆起 \(\mathbf{w} := \mathbf{w}_0\).
- en: Explicit formulas can be derived from the previous recursion.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从之前的递归中推导出显式公式。
- en: We first compute \(\mathbf{p}_1\). We use the *Properties of the Hadamard Product*.
    We get
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算 \(\mathbf{p}_1\)。我们使用哈达玛积的性质。我们得到
- en: \[\begin{align*} \mathbf{p}_1 &= A_1^T \mathbf{p}_{2}\\ &= [\mathrm{diag}(\bgamma(\mathbf{z}_1))
    - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T [- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
    &= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))
    + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
    &= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\ &= \bgamma(\mathbf{z}_1)
    - \mathbf{y}, \end{align*}\]
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_1 &= A_1^T \mathbf{p}_{2}\\ &= [\mathrm{diag}(\bgamma(\mathbf{z}_1))
    - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T [- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
    &= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))
    + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
    &= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\ &= \bgamma(\mathbf{z}_1)
    - \mathbf{y}, \end{align*}\]
- en: where we used that \(\sum_{k=1}^{K} y_k = 1\).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 \(\sum_{k=1}^{K} y_k = 1\) 的性质。
- en: It remains to compute \(\mathbf{q}_0\). We have by parts (e) and (f) of the
    *Properties of the Kronecker Product*
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的任务是计算 \(\mathbf{q}_0\)。根据克朗内克积的性质（部分 e 和 f），我们有
- en: \[\begin{align*} \mathbf{q}_{0} = B_0^T \mathbf{p}_{1} &= (I_{K\times K} \otimes
    \mathbf{z}_0^T)^T (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= ( I_{K\times K} \otimes
    \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= (\bgamma(\mathbf{z}_1)
    - \mathbf{y}) \otimes \mathbf{z}_0. \end{align*}\]
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{q}_{0} = B_0^T \mathbf{p}_{1} &= (I_{K\times K} \otimes
    \mathbf{z}_0^T)^T (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= ( I_{K\times K} \otimes
    \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\ &= (\bgamma(\mathbf{z}_1)
    - \mathbf{y}) \otimes \mathbf{z}_0. \end{align*}\]
- en: Finally, replacing \(\mathbf{z}_0 = \mathbf{x}\) and \(\mathbf{z}_1 = \mathcal{W}
    \mathbf{x}\), the gradient is
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将 \(\mathbf{z}_0 = \mathbf{x}\) 和 \(\mathbf{z}_1 = \mathcal{W} \mathbf{x}\)
    代入，梯度为
- en: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0 = (\bgamma\left(\mathcal{W} \mathbf{x}\right)
    - \mathbf{y}) \otimes \mathbf{x}. \]
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = \mathbf{q}_0 = (\bgamma\left(\mathcal{W} \mathbf{x}\right)
    - \mathbf{y}) \otimes \mathbf{x}. \]
- en: It can be shown that the objective function \(f(\mathbf{w})\) is convex in \(\mathbf{w}\).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明目标函数 \(f(\mathbf{w})\) 在 \(\mathbf{w}\) 上是凸的。
- en: '**NUMERICAL CORNER:** We will use the Fashion-MNIST dataset. This example is
    inspired by [these](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)
    [tutorials](https://www.tensorflow.org/tutorials/keras/classification). We first
    check for the availability of GPUs and load the data.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**NUMERICAL CORNER:** 我们将使用Fashion-MNIST数据集。这个例子受到了[这些](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)
    [教程](https://www.tensorflow.org/tutorials/keras/classification)的启发。我们首先检查GPU的可用性并加载数据。'
- en: '[PRE38]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We used [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader),
    which provides utilities to load the data in batches for training. We took mini-batches
    of size `BATCH_SIZE = 32` and we apply a random permutation of the samples on
    every pass over the training data (with the option `shuffle=True`). The function
    [`torch.manual_seed()`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html)
    is used to set the global seed for PyTorch operations (e.g., weight initialization).
    The shuffling in `DataLoader` uses its own separate random number generator, which
    we initialize with [`torch.Generator()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator)
    and [`manual_seed()`](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed).
    (You can tell from the fact that `seed=42` that Claude explained that one to me…)
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 `torch.utils.data.DataLoader`，它提供了加载数据的批量训练工具。我们使用了大小为 `BATCH_SIZE = 32`
    的迷你批次，并在每次遍历训练数据时对样本进行随机排列（使用选项 `shuffle=True`）。函数 `torch.manual_seed()` 用于设置PyTorch操作的全球种子（例如，权重初始化）。`DataLoader`
    中的洗牌使用其自己的独立随机数生成器，我们使用 `torch.Generator()` 和 `manual_seed()` 进行初始化。（您可以从 `seed=42`
    这一事实中看出，Claude向我解释了这一点…）
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain the lines:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 请您最喜欢的AI聊天机器人解释以下行：'
- en: '[PRE41]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: \(\ddagger\)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \(\ddagger\)
- en: 'We implement multinomial logistic regression to learn a classifier for the
    Fashion-MNIST data. In PyTorch, composition of functions can be achieved with
    [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).
    Our model is:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了多项式逻辑回归来学习Fashion-MNIST数据的分类器。在PyTorch中，可以使用 `torch.nn.Sequential` 实现函数的组合。我们的模型是：
- en: '[PRE42]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)
    layer turns each input image into a vector of size \(784\) (where \(784 = 28^2\)
    is the number of pixels in each image). After the flattening, we have an affine
    map from \(\mathbb{R}^{784}\) to \(\mathbb{R}^{10}\). Note that there is no need
    to pre-process the inputs by adding \(1\)s. A constant term (or “bias variable”)
    is automatically added by PyTorch (unless one chooses the option [`bias=False`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).
    The final output is \(10\)-dimensional.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Flatten` 层将每个输入图像转换为大小为 \(784\) 的向量（其中 \(784 = 28^2\) 是每个图像中的像素数）。在展平之后，我们有一个从
    \(\mathbb{R}^{784}\) 到 \(\mathbb{R}^{10}\) 的仿射映射。请注意，不需要通过添加 \(1\) 来预处理输入。PyTorch会自动添加一个常数项（或“偏置变量”），除非选择选项
    `bias=False`。最终的输出是 \(10\) 维的。'
- en: Finally, we are ready to run an optimization method of our choice on the loss
    function, which are specified next. There are many [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms)
    available. (See this [post](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)
    for a brief explanation of many common optimizers.) Here we use SGD as the optimizer.
    A quick tutorial is [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).
    The loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy),
    as implemented by [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),
    which first takes the softmax and expects the labels to be the actual class labels
    rather than their one-hot encoding.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好在损失函数上运行我们选择的优化方法，这些方法将在下面指定。有许多[优化器](https://pytorch.org/docs/stable/optim.html#algorithms)可供选择。（有关许多常见优化器的简要解释，请参阅这篇[文章](https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc)。）这里我们使用SGD作为优化器。快速教程[在这里](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)。损失函数是[交叉熵](https://en.wikipedia.org/wiki/Cross_entropy)，由`torch.nn.CrossEntropyLoss`实现，它首先进行softmax，并期望标签是实际的类别标签而不是它们的one-hot编码。
- en: '[PRE43]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We implement special functions for training.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了用于训练的特殊函数。
- en: '[PRE44]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: An epoch is one training iteration where all samples are iterated once (in a
    randomly shuffled order). In the interest of time, we train for 10 epochs only.
    But it does better if you train it longer (try it!). On each pass, we compute
    the output of the current model, use `backward()` to obtain the gradient, and
    then perform a descent update with `step()`. We also have to reset the gradients
    first (otherwise they add up by default).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一个epoch是一次训练迭代，其中所有样本都迭代一次（以随机打乱顺序）。为了节省时间，我们只训练了10个epochs。但如果你训练得更久，效果会更好（试试看！）在每次遍历中，我们计算当前模型的输出，使用`backward()`获取梯度，然后使用`step()`执行下降更新。我们还需要首先重置梯度（否则它们会默认累加）。
- en: '[PRE45]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Because of the issue of [overfitting](https://en.wikipedia.org/wiki/Overfitting),
    we use the *test* images to assess the performance of the final classifier.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[过拟合](https://en.wikipedia.org/wiki/Overfitting)的问题，我们使用*测试*图像来评估最终分类器的性能。
- en: '[PRE46]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: To make a prediction, we take a [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    of the output of our model. Recall that it is implicitly included in `torch.nn.CrossEntropyLoss`,
    but is not actually part of `model`. (Note that the softmax itself has no parameter.)
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出预测，我们对模型输出进行`torch.nn.functional.softmax`计算。回想一下，它在`torch.nn.CrossEntropyLoss`中隐式包含，但不是`model`的实际部分。（注意，softmax本身没有参数。）
- en: As an illustration, we do this for each test image. We use [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html)
    to concatenate a sequence of tensors into a single tensor.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 作为说明，我们对每个测试图像都这样做。我们使用`torch.cat`将一系列张量连接成一个单一的张量。
- en: '[PRE49]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The result for the first test image is shown below. To make a prediction, we
    choose the label with the highest probability.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张测试图像的结果如下。为了做出预测，我们选择概率最高的标签。
- en: '[PRE50]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The truth is:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是：
- en: '[PRE54]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Above, `next(iter(test_loader))` loads the first batch of test images. (See
    [here](https://docs.python.org/3/tutorial/classes.html#iterators) for background
    on iterators in Python.)
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，`next(iter(test_loader))`加载了第一批测试图像。（有关Python中迭代器的背景信息，请参阅[这里](https://docs.python.org/3/tutorial/classes.html#iterators)。）
- en: \(\unlhd\)
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude，Gemini和ChatGPT协助)*'
- en: '**1** In stochastic gradient descent (SGD), how is the gradient estimated at
    each iteration?'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在随机梯度下降（SGD）中，每个迭代中是如何估计梯度的？'
- en: a) By computing the gradient over the entire dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: a) 通过在整个数据集上计算梯度。
- en: b) By using the gradient from the previous iteration.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: b) 通过使用前一次迭代的梯度。
- en: c) By randomly selecting a subset of sample and computing their gradient.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: c) 通过随机选择样本子集并计算它们的梯度。
- en: d) By averaging the gradients of all samples in the dataset.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: d) 通过平均数据集中所有样本的梯度。
- en: '**2** What is the key advantage of using mini-batch SGD over standard SGD?'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 使用小批量随机梯度下降（mini-batch SGD）相对于标准随机梯度下降（SGD）的关键优势是什么？'
- en: a) It guarantees faster convergence to the optimal solution.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它保证了更快地收敛到最优解。
- en: b) It reduces the variance of the gradient estimate at each iteration.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它减少了每个迭代中梯度估计的方差。
- en: c) It eliminates the need for computing gradients altogether.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: c) 完全消除了计算梯度的需要。
- en: d) It increases the computational cost per iteration.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它增加了每迭代的计算成本。
- en: '**3** Which of the following statements is true about the update step in stochastic
    gradient descent?'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 关于随机梯度下降的更新步骤，以下哪个陈述是正确的？'
- en: a) It is always equal to the full gradient descent update.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它始终等于完整梯度下降的更新。
- en: b) It is always in the opposite direction of the full gradient descent update.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它始终与完整梯度下降的更新方向相反。
- en: c) It is, on average, equal to the full gradient descent update.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它平均上等于完整梯度下降的更新。
- en: d) It has no relationship to the full gradient descent update.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它与完整梯度下降的更新没有关系。
- en: '**4** In multinomial logistic regression, what is the role of the softmax function
    \(\boldsymbol{\gamma}\)?'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在多项式逻辑回归中，softmax 函数 \(\boldsymbol{\gamma}\) 的作用是什么？'
- en: a) To compute the gradient of the loss function.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: a) 用于计算损失函数的梯度。
- en: b) To normalize the input features.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: b) 用于归一化输入特征。
- en: c) To transform scores into a probability distribution over labels.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: c) 将分数转换成标签的概率分布。
- en: d) To update the model parameters during gradient descent.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: d) 在梯度下降过程中更新模型参数。
- en: '**5** What is the Kullback-Leibler (KL) divergence used for in multinomial
    logistic regression?'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** Kullback-Leibler (KL) 散度在多项式逻辑回归中用于什么？'
- en: a) To measure the distance between the predicted probabilities and the true
    labels.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: a) 用于衡量预测概率与真实标签之间的距离。
- en: b) To normalize the input features.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: b) 用于归一化输入特征。
- en: c) To update the model parameters during gradient descent.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: c) 在梯度下降过程中更新模型参数。
- en: d) To compute the gradient of the loss function.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: d) 用于计算损失函数的梯度。
- en: 'Answer for 1: c. Justification: The text states that in SGD, “we pick a sample
    uniformly at random in \(\{1, ..., n\}\) and update as follows \(\mathbf{w}^{t+1}
    = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)”'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：c. 理由：文本中提到在随机梯度下降（SGD）中，“我们在 \(\{1, ..., n\}\) 中随机均匀地选择一个样本，并按以下方式更新：\(\mathbf{w}^{t+1}
    = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)””
- en: 'Answer for 2: b. Justification: The text implies that mini-batch SGD reduces
    the variance of the gradient estimate compared to standard SGD, which only uses
    a single sample.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：b. 理由：文本暗示，与仅使用单个样本的标准 SGD 相比，小批量 SGD 减少了梯度估计的方差。
- en: 'Answer for 3: c. Justification: The text proves a lemma stating that “in expectation,
    they [stochastic updates] perform a step of gradient descent.”'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：c. 理由：文本证明了一个引理，表明“在期望上，它们[随机更新]执行梯度下降的一步。”
- en: 'Answer for 4: c. Justification: The text defines the softmax function and states
    that it is used to “transform these scores into a probability distribution over
    the labels.”'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：c. 理由：文本定义了 softmax 函数，并指出它被用来“将这些分数转换成标签的概率分布。”
- en: 'Answer for 5: a. Justification: The text introduces the KL divergence as a
    “notion of distance between probability measures” and uses it to define the loss
    function in multinomial logistic regression.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：a. 理由：文本将 KL 散度介绍为“概率测度之间的距离概念”，并使用它来定义多项式逻辑回归中的损失函数。
