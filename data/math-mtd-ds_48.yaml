- en: '6.4\. Modeling more complex dependencies 2: marginalizing out an unobserved
    variable#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.4\. 建模更复杂的依赖关系 2：边缘化未观察到的变量#
- en: 原文：[https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html](https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html](https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html)
- en: 'In this section, we move on to the second technique for constructing joint
    distributions from simpler building blocks: marginalizing out an unobserved random
    variable.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们继续介绍从更简单的构建块构建联合分布的第二种技术：边缘化未观察到的随机变量。
- en: 6.4.1\. Mixtures[#](#mixtures "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4.1\. 混合[#](#mixtures "链接到这个标题")
- en: Mixtures\(\idx{mixture}\xdi\) are a natural way to define probability distributions.
    The basic idea is to consider a pair of random vectors \((\bX,\bY)\) and assume
    that \(\bY\) is unobserved. The effect on the observed vector \(\bX\) is that
    \(\bY\) is marginalized out. Indeed, by the law of total probability, for any
    \(\bx \in \S_\bX\)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 混合\(\idx{mixture}\xdi\)是定义概率分布的一种自然方式。基本思想是考虑一对随机向量 \((\bX,\bY)\) 并假设 \(\bY\)
    是未观察到的。对观察到的向量 \(\bX\) 的影响是 \(\bY\) 被边缘化。事实上，根据全概率定律，对于任何 \(\bx \in \S_\bX\)
- en: \[\begin{align*} p_\bX(\bx) &= \P[\bX = \bx]\\ &= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by]
    \,\P[\bY=\by]\\ &= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by) \end{align*}\]
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_\bX(\bx) &= \P[\bX = \bx]\\ &= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by]
    \,\P[\bY=\by]\\ &= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by) \end{align*}\]
- en: 'where we used that the events \(\{\bY=\by\}\), \(\by \in \S_\bY\), form a partition
    of the probability space. We interpret this equation as defining \(p_\bX(\bx)\)
    as a convex combination – a mixture – of the distributions \(p_{\bX|\bY}(\bx|\by)\),
    \(\by \in \S_\bY\), with mixing weights \(p_\bY(\by)\). In general, we need to
    specify the full conditional probability distribution (CPD): \(p_{\bX|\bY}(\bx|\by),
    \forall \bx \in \S_{\bX}, \by \in \S_\bY\). But assuming that the mixing weights
    and/or CPD come from parametric families can help reduce the complexity of the
    model.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了事件 \(\{\bY=\by\}\)，其中 \(\by \in \S_\bY\)，构成了概率空间的一个划分。我们解释这个方程为定义 \(p_\bX(\bx)\)
    为 \(p_{\bX|\bY}(\bx|\by)\)，其中 \(\by \in \S_\bY\)，的凸组合——一种混合——具有混合权重 \(p_\bY(\by)\)。一般来说，我们需要指定完整的条件概率分布（CPD）：\(p_{\bX|\bY}(\bx|\by),
    \forall \bx \in \S_{\bX}, \by \in \S_\bY\)。但假设混合权重和/或CPD来自参数族可以帮助降低模型的复杂性。
- en: That can be represented in a digraph with a directed edge from a vertex for
    \(\mathbf{Y}\) to a vertex for \(\mathbf{X}\). Further, we let the vertex for
    \(\mathbf{X}\) be shaded to indicate that it is observed, while the vertex for
    \(\mathbf{Y}\) is not shaded to indicate that it is not. Mathematically, that
    corresponds to applying the law of total probability as we did previously.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用一个有向图表示，其中从 \(\mathbf{Y}\) 的顶点到 \(\mathbf{X}\) 的顶点有一条有向边。进一步，我们让 \(\mathbf{X}\)
    的顶点被阴影覆盖以表示它是被观察到的，而 \(\mathbf{Y}\) 的顶点没有被阴影覆盖以表示它没有被观察到。数学上，这对应于我们之前应用的全概率定律。
- en: '![A mixture](../Images/d7c8f87d42f669398ac57f92c0a43388.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![混合](../Images/d7c8f87d42f669398ac57f92c0a43388.png)'
- en: In the parametric context, this gives rise to a fruitful approach to expanding
    distribution families. Suppose \(\{p_{\btheta}:\btheta \in \Theta\}\) is a parametric
    family of distributions. Let \(K \geq 2\), \(\btheta_1, \ldots, \btheta_K \in
    \Theta\) and \(\bpi = (\pi_1,\ldots,\pi_K) \in \Delta_K\). Suppose \(Y \sim \mathrm{Cat}(\bpi)\)
    and that the conditional distributions satisfy
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数背景下，这产生了一种丰富的方法来扩展分布族。假设 \(\{p_{\btheta}:\btheta \in \Theta\}\) 是一个参数分布族。设
    \(K \geq 2\)，\(\btheta_1, \ldots, \btheta_K \in \Theta\) 和 \(\bpi = (\pi_1,\ldots,\pi_K)
    \in \Delta_K\)。假设 \(Y \sim \mathrm{Cat}(\bpi)\) 并且条件分布满足
- en: \[ p_{\bX|Y}(\bx|i) = p_{\btheta_i}(\bx). \]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\bX|Y}(\bx|i) = p_{\btheta_i}(\bx). \]
- en: We write this as \(\bX|\{Y=i\} \sim p_{\btheta_i}\). Then we obtain the mixture
    model
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其表示为 \(\bX|\{Y=i\} \sim p_{\btheta_i}\)。然后我们获得混合模型
- en: \[ p_{\bX}(\bx) = \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i) = \sum_{i=1}^K \pi_i
    p_{\btheta_i}(\bx). \]
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\bX}(\bx) = \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i) = \sum_{i=1}^K \pi_i
    p_{\btheta_i}(\bx). \]
- en: '**EXAMPLE:** **(Mixture of Multinomials)** Let \(n, m , K \geq 1\), \(\bpi
    \in \Delta_K\) and, for \(i=1,\ldots,K\), \(\mathbf{p}_i = (p_{i1},\ldots,p_{im})
    \in \Delta_m\). Suppose that \(Y \sim \mathrm{Cat}(\bpi)\) and that the conditional
    distributions are'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(多项式混合)** 设 \(n, m , K \geq 1\)，\(\bpi \in \Delta_K\)，并且对于 \(i=1,\ldots,K\)，\(\mathbf{p}_i
    = (p_{i1},\ldots,p_{im}) \in \Delta_m\)。假设 \(Y \sim \mathrm{Cat}(\bpi)\) 并且条件分布是'
- en: \[ \bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i). \]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i). \]
- en: Then \(\bX\) is a mixture of multinomials. Its distribution is then
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 \(\bX\) 是多项式混合。其分布如下
- en: \[ p_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m
    p_{ij}^{x_j}. \]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m
    p_{ij}^{x_j}. \]
- en: \(\lhd\)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: Next is an important continuous example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个重要的连续示例。
- en: '**EXAMPLE:** **(Gaussian mixture model)** \(\idx{Gaussian mixture model}\xdi\)
    For \(i=1,\ldots,K\), let \(\bmu_i\) and \(\bSigma_i\) be the mean and covariance
    matrix of a multivariate Gaussian. Let \(\bpi \in \Delta_K\). A Gaussian Mixture
    Model (GMM) is obtained as follows: take \(Y \sim \mathrm{Cat}(\bpi)\) and'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（高斯混合模型）** \(\idx{Gaussian mixture model}\xdi\) 对于 \(i=1,\ldots,K\)，设
    \(\bmu_i\) 和 \(\bSigma_i\) 为多元高斯的均值和协方差矩阵。设 \(\bpi \in \Delta_K\)。高斯混合模型（GMM）的获得如下：取
    \(Y \sim \mathrm{Cat}(\bpi)\) 和'
- en: \[ \bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i). \]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i). \]
- en: Its probability density function (PDF) takes the form
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其概率密度函数（PDF）的形式如下
- en: \[ f_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
    \]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
    \]
- en: \(\lhd\)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**NUMERICAL CORNER:** We plot the density for means \(\bmu_1 = (-2,-2)\) and
    \(\bmu_2 = (2,2)\) and covariance matrices'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们绘制了均值 \(\bmu_1 = (-2,-2)\) 和 \(\bmu_2 = (2,2)\) 以及协方差矩阵的密度'
- en: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{and} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1
    \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{和} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2
    \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
- en: where \(\sigma_1 = 1.5\), \(\sigma_2 = 0.5\) and \(\rho = -0.75\). The mixing
    weights are \(\pi_1 = 0.25\) and \(\pi_2 = 0.75\).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_1 = 1.5\)，\(\sigma_2 = 0.5\) 和 \(\rho = -0.75\)。混合权重为 \(\pi_1 =
    0.25\) 和 \(\pi_2 = 0.75\)。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源
    隐藏代码单元格源</summary>
- en: '[PRE1]</details> ![../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png](../Images/810f47a1332c13f0595b65b28e44ab63.png)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</details> ![../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png](../Images/810f47a1332c13f0595b65b28e44ab63.png)'
- en: \(\unlhd\)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: In NumPy, as we have seen before, the module [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html)
    also provides a way to sample from mixture models by using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，正如我们之前所看到的，模块 `numpy.random` 也提供了一种通过使用 `numpy.random.Generator.choice`
    来从混合模型中采样的方法。
- en: For instance, we consider mixtures of multivariate Gaussians. We chage the notation
    slightly to track Python’s indexing. For \(i=0,1\), we have a mean \(\bmu_i \in
    \mathbb{R}^d\) and a positive definite covariance matrix \(\bSigma_i \in \mathbb{R}^{d
    \times d}\). We also have mixture weights \(\phi_0, \phi_1 \in (0,1)\) such that
    \(\phi_0 + \phi_1 = 1\). Suppose we want to generate a total of \(n\) samples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们考虑多元高斯的混合。我们稍微改变一下符号以跟踪 Python 的索引。对于 \(i=0,1\)，我们有一个均值 \(\bmu_i \in \mathbb{R}^d\)
    和一个正定协方差矩阵 \(\bSigma_i \in \mathbb{R}^{d \times d}\)。我们还有一个混合权重 \(\phi_0, \phi_1
    \in (0,1)\)，使得 \(\phi_0 + \phi_1 = 1\)。假设我们想要生成总共 \(n\) 个样本。
- en: 'For each sample \(j=1,\ldots, n\), independently from everything else:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个样本 \(j=1,\ldots, n\)，独立于其他所有事物：
- en: We first pick a component \(i \in \{0,1\}\) at random according to the mixture
    weights, that is, \(i=0\) is chosen with probability \(\phi_0\) and \(i=1\) is
    chosen with probability \(\phi_1\).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先随机选择一个组件 \(i \in \{0,1\}\)，根据混合权重，即 \(i=0\) 以概率 \(\phi_0\) 被选中，\(i=1\) 以概率
    \(\phi_1\) 被选中。
- en: We generate a sample \(\bX_j = (X_{j,1},\ldots,X_{j,d})\) according to a multivariate
    Gaussian with mean \(\bmu_i\) and covariance \(\bSigma_i\).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据均值 \(\bmu_i\) 和协方差 \(\bSigma_i\) 生成一个样本 \(\bX_j = (X_{j,1},\ldots,X_{j,d})\)，它服从一个多元高斯分布。
- en: This is straightforward to implement by using again [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html)
    to choose the component of each sample and [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html)
    to generate multivariate Gaussians. For convenience, we will stack the means and
    covariances into one array with a new dimension. So, for instance, the covariance
    matrices will now be in a 3d-array, that is, an array with \(3\) indices. The
    first index corresponds to the component (here \(0\) or \(1\)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过再次使用 `numpy.random.Generator.choice` 来选择每个样本的成分，并使用 `numpy.random.Generator.multivariate_normal`
    来生成多元高斯分布来实现。为了方便，我们将均值和协方差矩阵堆叠到一个具有新维度的数组中。因此，例如，协方差矩阵现在将是一个三维数组，即具有 \(3\) 个索引的数组。第一个索引对应于成分（这里
    \(0\) 或 \(1\))。
- en: '**Figure:** Three matrices ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：三个矩阵([来源](https://www.tensorflow.org/guide/tensor#basics))'
- en: '![Three matrices](../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![三个矩阵](../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png)'
- en: \(\bowtie\)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**Figure:** Three matrices stacked into a 3d-array ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：三个矩阵堆叠成一个三维数组([来源](https://www.tensorflow.org/guide/tensor#basics))'
- en: '![Three matrices stacked into a tensor](../Images/a61cc745a3c58639bf7340b2af821416.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![将三个矩阵堆叠成一个张量](../Images/a61cc745a3c58639bf7340b2af821416.png)'
- en: \(\bowtie\)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: The code is the following. It returns an `d` by `n` array `X`, where each row
    is a sample from a 2-component Gaussian mixture.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下。它返回一个 `d` 行 `n` 列的数组 `X`，其中每一行是从一个两成分高斯混合分布中抽取的样本。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**NUMERICAL CORNER:** Let us try it with following parameters. We first define
    the covariance matrices and show what happens when they are stacked into a 3d
    array (as is done within `gmm2`).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 让我们尝试以下参数。我们首先定义协方差矩阵，并展示当它们被堆叠到三维数组中时会发生什么（正如在 `gmm2` 中所做的那样）。'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then we define the rest of the parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义其余的参数。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png](../Images/22ec3b142976b6b20a0185e9e997fea9.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png](../Images/22ec3b142976b6b20a0185e9e997fea9.png)'
- en: \(\unlhd\)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '6.4.2\. Example: Mixtures of multivariate Bernoullis and the EM algorithm[#](#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm
    "Link to this heading")'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4.2. 示例：多元伯努利混合和EM算法[#](#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm
    "链接到这个标题")
- en: Let \(\mathcal{C} = \{1, \ldots, K\}\) be a collection of classes. Let \(C\)
    be a random variable taking values in \(\mathcal{C}\) and, for \(m=1, \ldots,
    M\), let \(X_i\) take values in \(\{0,1\}\). Define \(\pi_k = \P[C = k]\) and
    \(p_{k,m} = \P[X_m = 1|C = k]\) for \(m = 1,\ldots, M\). We denote by \(\bX =
    (X_1, \ldots, X_M)\) the corresponding vector of \(X_i\)’s and assume that the
    entries are conditionally independent given \(C\).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 令 \(\mathcal{C} = \{1, \ldots, K\}\) 为一个类别的集合。令 \(C\) 为一个取值在 \(\mathcal{C}\)
    中的随机变量，对于 \(m=1, \ldots, M\)，令 \(X_i\) 取值在 \(\{0,1\}\) 中。定义 \(\pi_k = \P[C = k]\)
    和 \(p_{k,m} = \P[X_m = 1|C = k]\) 对于 \(m = 1,\ldots, M\)。我们用 \(\bX = (X_1, \ldots,
    X_M)\) 表示相应的 \(X_i\) 的向量，并假设在 \(C\) 条件下这些条目是条件独立的。
- en: However, we assume this time that \(C\) itself is *not observed*. So the resulting
    joint distribution is the mixture
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们这次假设 \(C\) 本身是*未观察到的*。因此，得到的联合分布是混合分布
- en: \[\begin{align*} \P[\bX = \bx] &= \sum_{k=1}^K \P[C = k, \bX = \bx]\\ &= \sum_{k=1}^K
    \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}. \end{align*}\]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \P[\bX = \bx] &= \sum_{k=1}^K \P[C = k, \bX = \bx]\\ &= \sum_{k=1}^K
    \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}. \end{align*}\]
- en: Graphically, this is the same are the Naive Bayes model, except that \(C\) is
    not observed and therefore is not shaded.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，这与朴素贝叶斯模型相同，只是 \(C\) 未被观察到，因此没有被阴影覆盖。
- en: '![Mixture of multivariate Bernoullis](../Images/c51352d9d6ab19d2f9e73f898c25b648.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![多元伯努利混合](../Images/c51352d9d6ab19d2f9e73f898c25b648.png)'
- en: This type of model is useful in particular for clustering tasks, where the \(c_k\)s
    can be thought of as different clusters. Similarly to what we did in the previous
    section, our goal is to infer the parameters from samples and then predict the
    class of an old or new sample given its features. The main – substantial – difference
    is that the true labels of the samples are not observed. As we will see, that
    complicates the task considerably.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的模型在聚类任务中特别有用，其中 \(c_k\)s 可以被视为不同的簇。类似于我们在上一节中所做的，我们的目标是根据样本推断参数，然后根据其特征预测旧样本或新样本的类别。主要的
    - 重要的 - 差异是样本的真实标签没有被观察到。正如我们将看到的，这大大增加了任务的复杂性。
- en: '**Model fitting** We first fit the model from training data \(\{\bx_i\}_{i=1}^n\).
    Recall that the corresponding class labels \(c_i\)s are not observed. In this
    type of model, they are referred to as hidden or latent variables and we will
    come back to their inference below.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型拟合** 我们首先从训练数据 \(\{\bx_i\}_{i=1}^n\) 中拟合模型。回想一下，相应的类标签 \(c_i\)s 是未观察到的。在这种类型的模型中，它们被称为隐藏或潜在变量，我们将在下面回到它们的推理。'
- en: We would like to use maximum likelihood estimation, that is, maximize the probability
    of observing the data
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用最大似然估计，即最大化观察数据的概率
- en: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\}) = \prod_{i=1}^n \left( \sum_{k=1}^K
    \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right). \]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\}) = \prod_{i=1}^n \left( \sum_{k=1}^K
    \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right). \]
- en: As usual, we assume that the samples are independent and identically distributed.
    Consider the negative log-likelihood (NLL)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们假设样本是独立同分布的。考虑负对数似然（NLL）
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
- en: Already, we see that things are potentially more difficult than they were in
    the supervised (or fully observed) case. The NLL does not decompose into a sum
    of terms depending on different sets of parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 已经，我们看到事情可能比在监督（或完全观察）的情况下更复杂。NLL 不能分解为依赖于不同参数集的项之和。
- en: At this point, one could fall back on the field of optimization and use a gradient-based
    method to minimize the NLL. Indeed that is an option, although note that one must
    be careful to account for the constrained nature of the problem (i.e., the parameters
    sum to one). There is a vast array of constrained optimization techniques suited
    for this task.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，一个人可以退回到优化领域，并使用基于梯度的方法来最小化NLL。的确，这是一个选择，但请注意，必须小心地考虑问题的约束性质（即参数之和为1）。有大量的约束优化技术适合这项任务。
- en: Instead a more popular approach in this context, the EM algorithm, is based
    on the general principle of majorization-minimization, which we have encountered
    implicitly in the \(k\)-means algorithm and the convergence proof of gradient
    descent in the smooth case. We detail this important principle in the next subsection
    before returning to model fitting in mixtures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是在这个上下文中更受欢迎的方法，即EM算法，它基于主成分最小化的通用原则，我们在 \(k\)-means 算法和光滑情况下的梯度下降收敛证明中隐式地遇到了这个原则。在回到混合模型拟合之前，我们将在下一小节中详细阐述这个重要的原则。
- en: '**Majorization-minimization** \(\idx{majorization-minimization}\xdi\) Here
    is a deceptively simple, yet powerful observation. Suppose we want to minimize
    a function \(f : \mathbb{R}^d \to \mathbb{R}\). Finding a local minimum of \(f\)
    may not be easy. But imagine that for each \(\mathbf{x} \in \mathbb{R}^d\) we
    have a surrogate function \(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\) that
    (1) dominates \(f\) in the following sense'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分最小化** \(\idx{majorization-minimization}\xdi\) 这是一个看似简单却强大的观察。假设我们想要最小化一个函数
    \(f : \mathbb{R}^d \to \mathbb{R}\)。找到 \(f\) 的局部最小值可能并不容易。但想象一下，对于每个 \(\mathbf{x}
    \in \mathbb{R}^d\)，我们都有一个代理函数 \(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\)，它（1）以下意义上支配
    \(f\)'
- en: \[ U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in
    \mathbb{R}^d \]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in
    \mathbb{R}^d \]
- en: and (2) equals \(f\) at \(\mathbf{x}\)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 并且（2）在 \(\mathbf{x}\) 处等于 \(f\)
- en: \[ U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}). \]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}). \]
- en: We say that \(U_\mathbf{x}\) majorizes \(f\) at \(\mathbf{x}\). Then we prove
    in the next lemma that \(U_\mathbf{x}\) can be used to make progress towards minimizing
    \(f\), that is, find a point \(\mathbf{x}'\) such that \(f(\mathbf{x}') \leq f(\mathbf{x})\).
    If in addition \(U_\mathbf{x}\) is easier to minimize than \(f\) itself, say because
    an explicit minimum can be computed, then this observation naturally leads to
    an iterative algorithm.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说 \(U_\mathbf{x}\) 在 \(\mathbf{x}\) 处主元化 \(f\)。然后我们在下一个引理中证明 \(U_\mathbf{x}\)
    可以用来向最小化 \(f\) 的方向前进，也就是说，找到一个点 \(\mathbf{x}'\) 使得 \(f(\mathbf{x}') \leq f(\mathbf{x})\)。如果
    \(U_\mathbf{x}\) 比本身更容易最小化，比如说可以计算出一个显式的最小值，那么这个观察自然导致了一个迭代算法。
- en: '![A majorizing function (with help from ChatGPT; inspired by Source)](../Images/c05a4e1add7058b646f07a53f61281a2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![一个主元函数（得益于 ChatGPT；受来源启发）](../Images/c05a4e1add7058b646f07a53f61281a2.png)'
- en: '**LEMMA** **(Majorization-Minimization)** \(\idx{majorization-minimization
    lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) and suppose \(U_{\mathbf{x}}\)
    majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\) be a global minimum of
    \(U_\mathbf{x}\). Then'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(主元-最小化)** \(\idx{主元-最小化引理}\xdi\) 设 \(f : \mathbb{R}^d \to \mathbb{R}\)
    并假设 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处主元化 \(f\)。设 \(\mathbf{x}''\) 是 \(U_\mathbf{x}\)
    的全局最小值。那么'
- en: \[ f(\mathbf{x}') \leq f(\mathbf{x}). \]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}') \leq f(\mathbf{x}). \]
- en: \(\flat\)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* Indeed'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 确实'
- en: \[ f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x})
    = f(\mathbf{x}), \]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x})
    = f(\mathbf{x}), \]
- en: where the first inequality follows from the domination property of \(U_\mathbf{x}\),
    the second inequality follows from the fact that \(\mathbf{x}'\) is a global minimum
    of \(U_\mathbf{x}\) and the equality follows from the fact that \(U_{\mathbf{x}}\)
    equals \(f\) at \(\mathbf{x}\). \(\square\)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一个不等式来自于 \(U_\mathbf{x}\) 的支配性质，第二个不等式来自于 \(\mathbf{x}'\) 是 \(U_\mathbf{x}\)
    的全局最小值，等式来自于 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处等于 \(f\)。 \(\square\)
- en: We have already encountered this idea.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遇到过这个想法。
- en: '**EXAMPLE:** **(Minimizing a smooth function)** Let \(f : \mathbb{R}^d \to
    \mathbb{R}\) be \(L\)-smooth. By the *Quadratic Bound for Smooth Functions*, for
    all \(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\) it holds that'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例** **(最小化光滑函数)** 设 \(f : \mathbb{R}^d \to \mathbb{R}\) 是 \(L\)-光滑的。根据光滑函数的**二次界**，对于所有
    \(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\)，都有'
- en: \[ f(\mathbf{z}) \leq U_{\mathbf{x}}(\mathbf{z}) := f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{z}
    - \mathbf{x}) + \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2. \]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{z}) \leq U_{\mathbf{x}}(\mathbf{z}) := f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{z}
    - \mathbf{x}) + \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2. \]
- en: By showing that \(U_{\mathbf{x}}\) is minimized at \(\mathbf{z} = \mathbf{x}
    - (1/L)\nabla f(\mathbf{x})\), we previously obtained the descent guarantee
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过证明 \(U_{\mathbf{x}}\) 在 \(\mathbf{z} = \mathbf{x} - (1/L)\nabla f(\mathbf{x})\)
    处达到最小值，我们之前得到了下降保证
- en: \[ f(\mathbf{x} - (1/L)\nabla f(\mathbf{x})) \leq f(\mathbf{x}) - \frac{1}{2
    L} \|\nabla f(\mathbf{x})\|^2 \]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x} - (1/L)\nabla f(\mathbf{x})) \leq f(\mathbf{x}) - \frac{1}{2
    L} \|\nabla f(\mathbf{x})\|^2 \]
- en: for gradient descent, which played a central role in the analysis of its convergence\(\idx{convergence
    analysis}\xdi\). \(\lhd\)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降，这在其收敛性分析\(\idx{收敛性分析}\xdi\)中扮演了核心角色\(\lhd\)。 \(\lhd\)
- en: '**EXAMPLE:** **(\(k\)-means)** \(\idx{Lloyd''s algorithm}\xdi\) Let \(\mathbf{x}_1,\ldots,\mathbf{x}_n\)
    be \(n\) vectors in \(\mathbb{R}^d\). One way to formulate the \(k\)-means clustering
    problem is as the minimization of'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例** **(\(k\)-均值)** \(\idx{Lloyd 的算法}\xdi\) 设 \(\mathbf{x}_1,\ldots,\mathbf{x}_n\)
    是 \(\mathbb{R}^d\) 中的 \(n\) 个向量。一种将 \(k\)-均值聚类问题表述为最小化的方法是'
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 \]
- en: over the centers \(\bmu_1,\ldots,\bmu_K\), where recall that \([K] = \{1,\ldots,K\}\).
    For fixed \(\bmu_1,\ldots,\bmu_K\) and \(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\),
    define
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在中心 \(\bmu_1,\ldots,\bmu_K\) 上，记住 \([K] = \{1,\ldots,K\}\)。对于固定的 \(\bmu_1,\ldots,\bmu_K\)
    和 \(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\)，定义
- en: \[ c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in
    [K]\right\}, \quad i=1,\ldots,n \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in
    [K]\right\}, \quad i=1,\ldots,n \]
- en: and
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \|\mathbf{x}_i
    - \blambda_{c_\mathbf{m}(i)}\|^2 \]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \|\mathbf{x}_i
    - \blambda_{c_\mathbf{m}(i)}\|^2 \]
- en: for \(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\). That is, we fix the optimal
    cluster assignments under \(\bmu_1,\ldots,\bmu_K\) and then vary the centers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\)。也就是说，我们固定了在 \(\bmu_1,\ldots,\bmu_K\)
    下的最优聚类分配，然后改变中心。
- en: We claim that \(U_\mathbf{m}\) is majorizing \(f\) at \(\bmu_1,\ldots,\bmu_K\).
    Indeed
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声称 \(U_\mathbf{m}\) 在 \(\bmu_1,\ldots,\bmu_K\) 处是 \(f\) 的主导函数。实际上
- en: \[ f(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i
    - \blambda_j\|^2 \leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
    = U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) \]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i
    - \blambda_j\|^2 \leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
    = U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) \]
- en: and
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 = \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2 = U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
    \]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 = \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2 = U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
    \]
- en: Moreover \(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\) is easy to minimize.
    We showed previously that the optimal representatives are
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，\(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\) 很容易最小化。我们之前已经证明，最优代表是
- en: \[ \boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i \]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i \]
- en: 'where \(C_j = \{i : c_\mathbf{m}(i) = j\}\).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 \(C_j = \{i : c_\mathbf{m}(i) = j\}\).'
- en: The *Majorization-Minimization Lemma* implies that
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*主次最小化引理* 意味着'
- en: \[ f(\bmu_1',\ldots,\bmu_K') \leq f(\bmu_1,\ldots,\bmu_K). \]
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\bmu_1',\ldots,\bmu_K') \leq f(\bmu_1,\ldots,\bmu_K). \]
- en: This argument is equivalent to our previous analysis of the \(k\)-means algorithm.
    \(\lhd\)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个论点等同于我们之前对 \(k\)-means 算法的分析。\(\lhd\)
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model assumes a fixed
    number of clusters. Ask your favorite AI chatbot to discuss Bayesian nonparametric
    extensions of this model, such as the Dirichlet process mixture model, which can
    automatically infer the number of clusters from the data. \(\ddagger\)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 多变量伯努利混合模型假设有固定数量的簇。请向您喜欢的 AI 聊天机器人询问该模型的贝叶斯非参数扩展，例如狄利克雷过程混合模型，它可以自动从数据中推断簇的数量。\(\ddagger\)'
- en: '**EM algorithm** The [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\(\idx{EM
    algorithm}\xdi\) is an instantiation of the majorization-minimization principle
    that applies widely to parameter estimation of mixtures. Here we focus on the
    mixture of multivariate Bernoullis.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**EM 算法** [期望最大化 (EM) 算法](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\(\idx{EM
    algorithm}\xdi\) 是一个应用广泛的原理实例，该原理适用于混合参数估计。在这里，我们专注于多变量伯努利混合。'
- en: Recall that the objective to be minimized is
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，要最小化的目标是
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
- en: To simplify the notation and highlight the general idea, we let \(\btheta =
    (\bpi, \{\bp_k\})\), denote by \(\Theta\) the set of allowed values for \(\btheta\),
    and use \(\P_{\btheta}\) to indicate that probabilities are computed under the
    parameters \(\btheta\). We also return to the description of the model in terms
    of the unobserved latent variables \(\{C_i\}\). That is, we write the NLL as
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号并突出一般思想，我们令 \(\btheta = (\bpi, \{\bp_k\})\)，用 \(\Theta\) 表示 \(\btheta\)
    的允许值集合，并使用 \(\P_{\btheta}\) 表示在参数 \(\btheta\) 下计算概率。我们还将回到用未观察到的潜在变量 \(\{C_i\}\)
    来描述模型。也就是说，我们写出 NLL 为
- en: \[\begin{align*} L_n(\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i
    = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\ &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right). \end{align*}\]
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i
    = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\ &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right). \end{align*}\]
- en: To derive a majorizing function, we use the convexity of the negative logarithm.
    Indeed
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推导出一个主导函数，我们使用了负对数的凸性。实际上
- en: \[ \frac{\partial}{\partial z}[- \log z] = - \frac{1}{z} \quad \text{and} \quad
    \frac{\partial^2}{\partial^2 z}[- \log z] = \frac{1}{z^2} > 0, \quad \forall z
    > 0. \]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial}{\partial z}[- \log z] = - \frac{1}{z} \quad \text{and} \quad
    \frac{\partial^2}{\partial^2 z}[- \log z] = \frac{1}{z^2} > 0, \quad \forall z
    > 0. \]
- en: The first step of the construction is not obvious – it just works. For each
    \(i=1,\ldots,n\), we let \(r_{k,i}^{\btheta}\), \(k=1,\ldots,K\), be a strictly
    positive probability distribution over \([K]\). In other words, it defines a convex
    combination for every \(i\). Then we use convexity to obtain the upper bound
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 构造的第一步并不明显 – 它只是有效。对于每个 \(i=1,\ldots,n\)，我们让 \(r_{k,i}^{\btheta}\)，\(k=1,\ldots,K\)，成为
    \([K]\) 上的严格正概率分布。换句话说，它为每个 \(i\) 定义了一个凸组合。然后我们利用凸性得到上界
- en: \[\begin{align*} L_n(\tilde\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K
    r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right), \end{align*}\]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\tilde\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K
    r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right), \end{align*}\]
- en: which holds for any \(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于任何 \(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\) 都成立。
- en: We choose
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择
- en: \[ r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i] \]
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i] \]
- en: (which for the time being we assume is strictly positive) and we denote the
    right-hand side of the inequality by \(Q_{n}(\tilde\btheta|\btheta)\) (as a function
    of \(\tilde\btheta\)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: （我们暂时假设它是严格正的）并且我们用 \(Q_{n}(\tilde\btheta|\btheta)\) 表示不等式的右侧（作为 \(\tilde\btheta\)
    的函数）。
- en: We make two observations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做出两个观察。
- en: '1- *Dominating property*: For any \(\tilde\btheta \in \Theta\), the inequality
    above implies immediately that \(L_n(\tilde\btheta) \leq Q_n(\tilde\btheta|\btheta)\).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '1- *支配性质*: 对于任何 \(\tilde\btheta \in \Theta\)，上述不等式立即意味着 \(L_n(\tilde\btheta)
    \leq Q_n(\tilde\btheta|\btheta)\)。'
- en: '2- *Equality at \(\btheta\)*: At \(\tilde\btheta = \btheta\),'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '2- *在 \(\btheta\) 处的等价性*: 在 \(\tilde\btheta = \btheta\)，'
- en: \[\begin{align*} Q_n(\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i
    = k | \bX_i = \bx_i] \P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\btheta}[\bX_i = \bx_i]\\
    &= - \sum_{i=1}^n \log \P_{\btheta}[\bX_i = \bx_i]\\ &= L_n(\btheta). \end{align*}\]
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} Q_n(\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i
    = k | \bX_i = \bx_i] \P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\btheta}[\bX_i = \bx_i]\\
    &= - \sum_{i=1}^n \log \P_{\btheta}[\bX_i = \bx_i]\\ &= L_n(\btheta). \end{align*}\]
- en: The two properties above show that \(Q_n(\tilde\btheta|\btheta)\), as a function
    of \(\tilde\btheta\), majorizes \(L_n\) at \(\btheta\).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个性质表明，\(Q_n(\tilde\btheta|\btheta)\)，作为 \(\tilde\btheta\) 的函数，在 \(\btheta\)
    处主次于 \(L_n\)。
- en: '**LEMMA** **(EM Guarantee)** \(\idx{EM guarantee}\xdi\) Let \(\btheta^*\) be
    a global minimizer of \(Q_n(\tilde\btheta|\btheta)\) as a function of \(\tilde\btheta\),
    provided it exists. Then'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(EM保证)** \(\idx{EM guarantee}\xdi\) 设 \(\btheta^*\) 是 \(Q_n(\tilde\btheta|\btheta)\)
    作为 \(\tilde\btheta\) 的函数的全局最小化者，前提是它存在。那么'
- en: \[ L_n(\btheta^*) \leq L_n(\btheta). \]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\btheta^*) \leq L_n(\btheta). \]
- en: \(\flat\)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* The result follows directly from the *Majorization-Minimization Lemma*.
    \(\square\)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 该结果直接来自 *主次最小化引理*。 \(\square\)'
- en: What have we gained from this? As we mentioned before, using the *Majorization-Minimization
    Lemma* makes sense if \(Q_n\) is easier to minimize than \(L_n\) itself. Let us
    see why that is the case here.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中得到了什么？正如我们之前提到的，如果 \(Q_n\) 比本身更容易最小化，那么使用 *主次最小化引理* 是有意义的。让我们看看为什么在这里是这样的。
- en: '*E Step:* The function \(Q_n\) naturally decomposes into two terms'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*E 步:* 函数 \(Q_n\) 自然分解为两个部分'
- en: \[\begin{align*} Q_n(\tilde\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k] + \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
    \end{align*}\]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} Q_n(\tilde\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k] + \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
    \end{align*}\]
- en: Because \(r_{k,i}^{\btheta}\) depends on \(\btheta\) *but not \(\tilde\btheta\)*,
    the second term is irrelevant to the opimization with respect to \(\tilde\btheta\).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(r_{k,i}^{\btheta}\) 依赖于 \(\btheta\) 但不依赖于 \(\tilde\btheta\)，所以第二个项与关于 \(\tilde\btheta\)
    的优化无关。
- en: The first term above can be written as
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述第一个项可以写成
- en: \[\begin{align*} & - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k}
    \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
    &= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M
    [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta})
    \log (1-\tilde{p}_{k,m})], \end{align*}\]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} & - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k}
    \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
    &= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M
    [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta})
    \log (1-\tilde{p}_{k,m})], \end{align*}\]
- en: where we defined, for \(k=1,\ldots,K\),
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了 \(k=1,\ldots,K\) 的情况，
- en: \[ \eta_{k,m}^{\btheta} = \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta} \quad \text{and}
    \quad \eta_k^{\btheta} = \sum_{i=1}^n r_{k,i}^{\btheta}. \]
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \eta_{k,m}^{\btheta} = \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta} \quad \text{和}
    \quad \eta_k^{\btheta} = \sum_{i=1}^n r_{k,i}^{\btheta}. \]
- en: 'Here comes the key observation: this last expression is essentially the same
    as the NLL for the fully observed Naive Bayes model, except that the terms \(\mathbf{1}_{\{c_i
    = k\}}\) are replaced by \(r_{k,i}^{\btheta}\). If \(\btheta\) is our current
    estimate of the parameters, then the quantity \(r_{k,i}^{\btheta} = \P_{\btheta}[C_i
    = k|\bX_i = \bx_i]\) is our estimate – under the current parameter \(\btheta\)
    – of the probability that the sample \(\bx_i\) comes from cluster \(k\). We have
    previously computed \(r_{k,i}^{\btheta}\) for prediction under the Naive Bayes
    model. We showed there that'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键观察：这个最后的表达式本质上与完全观察的朴素贝叶斯模型的NLL相同，只是将项 \(\mathbf{1}_{\{c_i = k\}}\) 替换为
    \(r_{k,i}^{\btheta}\)。如果 \(\btheta\) 是我们当前参数的估计，那么量 \(r_{k,i}^{\btheta} = \P_{\btheta}[C_i
    = k|\bX_i = \bx_i]\) 是我们在当前参数 \(\btheta\) 下对样本 \(\bx_i\) 来自簇 \(k\) 的概率的估计。我们之前已经计算了
    \(r_{k,i}^{\btheta}\) 以在朴素贝叶斯模型下进行预测。我们展示了在那里，
- en: \[ r_{k,i}^{\btheta} = \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
    \]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^{\btheta} = \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
    \]
- en: 'which in this new context is referred to as the responsibility that cluster
    \(k\) takes for data point \(i\). So we can interpret the expression above as
    follows: the variables \(\mathbf{1}_{\{c_i = k\}}\) are not observed here, but
    we have estimated their conditional probability distribution given the observed
    data \(\{\bx_i\}\), and we are taking an expectation with respect to that distribution
    instead.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的上下文中被称为簇 \(k\) 对数据点 \(i\) 的责任。因此，我们可以这样解释上述表达式：变量 \(\mathbf{1}_{\{c_i =
    k\}}\) 在这里没有观察到，但我们已经根据观察到的数据 \(\{\bx_i\}\) 估计了它们的条件概率分布，并且我们是在这个分布上取期望。
- en: The “E” in E Step (and EM) stands for “expectation”, which refers to using a
    surrogate function that is essentially an expected NLL.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: E 步骤（以及EM）中的“E”代表“期望”，这指的是使用一个本质上为期望NLL的代理函数。
- en: '*M Step:* In any case, from a practical point of view, minimizing \(Q_n(\tilde\btheta|\btheta)\)
    over \(\tilde\btheta\) turns out to be a variant of fitting a Naive Bayes model
    – and the upshot to all this is that there is a straightforward formula for that!
    Recall that this happens because, the NLL in the Naive Bayes model decomposes:
    it naturally breaks up into terms that depend on separate sets of parameters,
    each of which can be optimized with a closed-form expression. The same happens
    with \(Q_n\) as should be clear from the derivation.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*M 步骤:* 从实际的角度来看，在 \(\tilde\btheta\) 上最小化 \(Q_n(\tilde\btheta|\btheta)\) 实际上是一种拟合朴素贝叶斯模型的变体——而且所有这一切的最终结果是有一个简单的公式！回想一下，这是因为朴素贝叶斯模型中的NLL分解：它自然地分解成依赖于不同参数集的项，每个参数集都可以用闭式表达式进行优化。从推导中可以看出，\(Q_n\)
    也是如此。'
- en: Adapting our previous calculations for fitting a Naive Bayes model, we get that
    \(Q_n(\tilde\btheta|\btheta)\) is minimized at
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整我们之前的计算以拟合朴素贝叶斯模型时，我们得到 \(Q_n(\tilde\btheta|\btheta)\) 在以下情况下最小化：
- en: \[ \pi_k^* = \frac{\eta_k^{\btheta}}{n} \quad \text{and} \quad p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
    \quad \forall k \in [K], m \in [M]. \]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \pi_k^* = \frac{\eta_k^{\btheta}}{n} \quad \text{和} \quad p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
    \quad \forall k \in [K], m \in [M]. \]
- en: We used the fact that
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了以下事实
- en: \[\begin{align*} \sum_{k=1}^K \eta_k^{\btheta} &= \sum_{k=1}^K \sum_{i=1}^n
    r_{k,i}^{\btheta}\\ &= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i =
    \bx_i]\\ &= \sum_{i=1}^n 1\\ &= n, \end{align*}\]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{k=1}^K \eta_k^{\btheta} &= \sum_{k=1}^K \sum_{i=1}^n
    r_{k,i}^{\btheta}\\ &= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i =
    \bx_i]\\ &= \sum_{i=1}^n 1\\ &= n, \end{align*}\]
- en: since the conditional probability \(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\) adds
    up to one when summed over \(k\).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因为条件概率 \(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\) 在 \(k\) 上求和时总和为1。
- en: The “M” in M Step (and EM) stands for maximization, which here turns into minimization
    because of the use of the NLL.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: M步（和EM）中的“M”代表最大化，在这里由于使用了NLL，它变成了最小化。
- en: To summarize, the EM algorithm works as follows in this case. Assume we have
    data points \(\{\bx_i\}_{i=1}^n\), that we have fixed \(K\) and that we have some
    initial parameter estimate \(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\) with
    strictly positive \(\pi_k^0\)s and \(p_{k,m}^0\)s. For \(t = 0,1,\ldots, T-1\)
    we compute for all \(i \in [n]\), \(k \in [K]\), and \(m \in [M]\)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在这种情况下，EM算法的工作原理如下。假设我们有一些数据点 \(\{\bx_i\}_{i=1}^n\)，我们固定了 \(K\)，并且我们有一些初始参数估计
    \(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\)，其中 \(\pi_k^0\)s 和 \(p_{k,m}^0\)s
    都是严格正的。对于 \(t = 0,1,\ldots, T-1\)，我们计算所有 \(i \in [n]\)，\(k \in [K]\)，和 \(m \in
    [M]\) 的值
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \quad \text{(E Step)} \]\[ \eta_{k,m}^t = \sum_{i=1}^n x_{i,m} r_{k,i}^t \quad
    \text{and} \quad \eta_k^t = \sum_{i=1}^n r_{k,i}^t, \]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \quad \text{(E步)} \]\[ \eta_{k,m}^t = \sum_{i=1}^n x_{i,m} r_{k,i}^t \quad \text{和}
    \quad \eta_k^t = \sum_{i=1}^n r_{k,i}^t, \]
- en: and
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \pi_k^{t+1} = \frac{\eta_k^t}{n} \quad \text{and} \quad p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
    \quad \text{(M Step)} \]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \pi_k^{t+1} = \frac{\eta_k^t}{n} \quad \text{和} \quad p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
    \quad \text{(M步)} \]
- en: Provided \(\sum_{i=1}^n x_{i,m} > 0\) for all \(m\), the \(\eta_{k,m}^t\)s and
    \(\eta_k^t\)s remain positive for all \(t\) and the algorithm is well-defined.
    The *EM Guarantee* stipulates that the NLL cannot deteriorate, although note that
    it does not guarantee convergence to a global minimum.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设对于所有 \(m\)，\(\sum_{i=1}^n x_{i,m} > 0\)，则 \(\eta_{k,m}^t\)s 和 \(\eta_k^t\)s
    对于所有 \(t\) 都保持正值，并且算法是良好定义的。*EM保证*规定NLL不能恶化，尽管请注意，它并不保证收敛到全局最小值。
- en: We implement the EM algorithm for mixtures of multivariate Bernoullis. For this
    purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility
    of using Laplace smoothing.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了多元伯努利混合的EM算法。为此，我们调整了我们之前的朴素贝叶斯程序。我们还允许使用拉普拉斯平滑。
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We implement the E and M Step next.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来实现E步和M步。
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**NUMERICAL CORNER:** We test the algorithm on a very simple dataset.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们在一个非常简单的数据集上测试了该算法。'
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We compute the probability that the vector \((0, 0, 1)\) is in each cluster.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算向量 \((0, 0, 1)\) 在每个簇中的概率。
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**CHAT & LEARN** The EM algorithm can sometimes get stuck in local optima.
    Ask your favorite AI chatbot to discuss strategies for initializing the EM algorithm
    to avoid this issue, such as using multiple random restarts or using the k-means
    algorithm for initialization. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习**: EM算法有时会陷入局部最优。请向您的首选AI聊天机器人询问初始化EM算法以避免此问题的策略，例如使用多次随机重启或使用k-means算法进行初始化。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 6.4.3\. Clustering handwritten digits[#](#clustering-handwritten-digits "Link
    to this heading")
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4.3\. 手写数字聚类[#](#clustering-handwritten-digits "链接到这个标题")
- en: To give a more involved example, we use the MNIST dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给出一个更复杂的例子，我们使用了MNIST数据集。
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database) again:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 再次引用[Wikipedia](https://en.wikipedia.org/wiki/MNIST_database)：
- en: The MNIST database (Modified National Institute of Standards and Technology
    database) is a large database of handwritten digits that is commonly used for
    training various image processing systems. The database is also widely used for
    training and testing in the field of machine learning. It was created by “re-mixing”
    the samples from NIST’s original datasets. The creators felt that since NIST’s
    training dataset was taken from American Census Bureau employees, while the testing
    dataset was taken from American high school students, it was not well-suited for
    machine learning experiments. Furthermore, the black and white images from NIST
    were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which
    introduced grayscale levels. The MNIST database contains 60,000 training images
    and 10,000 testing images. Half of the training set and half of the test set were
    taken from NIST’s training dataset, while the other half of the training set and
    the other half of the test set were taken from NIST’s testing dataset.
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MNIST数据库（修改后的国家标准与技术研究院数据库）是一个包含大量手写数字的大型数据库，常用于训练各种图像处理系统。该数据库在机器学习领域的训练和测试中也得到了广泛应用。它是通过“重新混合”NIST原始数据集的样本创建的。创建者认为，由于NIST的训练数据集是从美国人口普查局员工那里获得的，而测试数据集是从美国高中生那里获得的，因此它不适合机器学习实验。此外，NIST的黑白图像被归一化以适应28x28像素的边界框，并进行了抗锯齿处理，这引入了灰度级别。MNIST数据库包含60,000张训练图像和10,000张测试图像。训练集和测试集各有一半来自NIST的训练数据集，另一半来自NIST的测试数据集。
- en: '**Figure:** MNIST sample images ([Source](https://commons.wikimedia.org/wiki/File:MnistExamples.png))'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：** MNIST样本图像 ([来源](https://commons.wikimedia.org/wiki/File:MnistExamples.png))'
- en: '![MNIST sample images](../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST样本图像](../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png)'
- en: \(\bowtie\)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**NUMERICAL CORNER:** We load it from PyTorch. The data can be accessed with
    [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html).
    The [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html)
    below removes the color dimension in the image, which is grayscale. The [`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    converts the PyTorch tensors into NumPy arrays. See [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    for details on the data loading. We will say more about PyTorch in a later chapter.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角：** 我们从PyTorch中加载它。数据可以通过 `torchvision.datasets.MNIST` ([链接](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html))
    访问。下面的 `squeeze()` ([链接](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html))
    从图像中移除了颜色维度，使其变为灰度图。`numpy()` ([链接](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html))
    将PyTorch张量转换为NumPy数组。有关数据加载的详细信息，请参阅 `torch.utils.data.DataLoader` ([链接](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader))。我们将在下一章中详细介绍PyTorch。'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE16]</details>'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE16]</details>'
- en: We turn the grayscale images into a black-and-white images by rounding the pixels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过四舍五入像素将灰度图像转换为黑白图像。
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are two common ways to write a \(2\). Let’s see if a mixture of multivariate
    Bernoullis can find them. We extract the images labelled \(2\).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 写 \(2\) 有两种常见方式。让我们看看多元伯努利混合能否找到它们。我们提取了标记为 \(2\) 的图像。
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The first image is the following.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图像如下。
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE19]</details> ![../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png](../Images/e8eea00af5868fb166365334b2072575.png)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE19]</details> ![../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png](../Images/e8eea00af5868fb166365334b2072575.png)'
- en: Next, we transform the images into vectors.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将图像转换为向量。
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We run the algorithm with \(2\) clusters.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 \(2\) 个簇运行算法。
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Uh-oh. Something went wrong. We encountered a numerical issue, underflow, which
    we discussed briefly previously. To confirm this, we run the code again but ask
    Python to warn us about it using [`numpy.seterr`](https://numpy.org/doc/stable/reference/generated/numpy.seterr.html).
    (By default, warnings are turned off in the book, but they can be reactivated
    using [`warnings.resetwarnings`](https://docs.python.org/3/library/warnings.html#warnings.resetwarnings).)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。出错了。我们遇到了一个数值问题，下溢，我们之前简要讨论过。为了确认这一点，我们再次运行代码，但要求 Python 使用 `numpy.seterr`（https://numpy.org/doc/stable/reference/generated/numpy.seterr.html）来警告我们。（默认情况下，警告是关闭的，但可以使用
    `warnings.resetwarnings`（https://docs.python.org/3/library/warnings.html#warnings.resetwarnings）重新激活。）
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: \(\unlhd\)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: When we compute the responsibilities
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算责任
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \]
- en: we first compute the negative logarithm of each term in the numerator as we
    did in the Naive Bayes case. But then we apply the function \(e^{-x}\), because
    this time we are not simply computing an optimal score. When all scores are high,
    this last step may result in underflow, that is, produces numbers so small that
    they get rounded down to zero by NumPy. Then the ratio defining `r_k` is not well-defined.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算分子中每个项的负对数，就像在朴素贝叶斯案例中做的那样。但这次我们应用函数 \(e^{-x}\)，因为这次我们不仅仅是在计算一个最优分数。当所有分数都很高时，这一步可能会导致下溢，也就是说，产生非常小的数字，它们会被
    NumPy 四舍五入到零。然后定义 `r_k` 的比率就不明确了。
- en: To deal with this, we introduce a technique called the log-sum-exp trick\(\idx{log-sum-exp
    trick}\xdi\) (with some help from ChatGPT). Consider the computation of a function
    of \(\mathbf{a} = (a_1, \ldots, a_n)\) of the form
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这个问题，我们引入了一种称为对数和指数技巧（log-sum-exp trick）的技术（在ChatGPT的一些帮助下）。考虑一个函数 \(\mathbf{a}
    = (a_1, \ldots, a_n)\) 的计算，其形式如下
- en: \[ h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right). \]
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right). \]
- en: When the \(a_i\) values are large positive numbers, the terms \(e^{-a_i}\) can
    be so small that they underflow to zero. To counter this, the log-sum-exp trick
    involves a shift to bring these terms into a more favorable numerical range.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(a_i\) 的值是很大的正数时，项 \(e^{-a_i}\) 可以非常小，以至于下溢到零。为了解决这个问题，对数和指数技巧涉及一个移位，将这两个项带入一个更有利的数值范围。
- en: 'It proceeds as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 它的进行过程如下：
- en: Identify the minimum value \(M\) among the \(a_i\)s
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别 \(a_i\) 中的最小值 \(M\)
- en: \[ M = \min\{a_1, a_2, \ldots, a_n\}. \]
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \[ M = \min\{a_1, a_2, \ldots, a_n\}. \]
- en: Subtract \(M\) from each \(a_i\) before exponentiation
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在指数之前从每个 \(a_i\) 中减去 \(M\)
- en: \[ \log \left( \sum_{i=1}^{n} e^{-a_i} \right) = \log \left( e^{-M} \sum_{i=1}^{n}
    e^{- (a_i - M)} \right). \]
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \[ \log \left( \sum_{i=1}^{n} e^{-a_i} \right) = \log \left( e^{-M} \sum_{i=1}^{n}
    e^{- (a_i - M)} \right). \]
- en: Rewrite using log properties
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对数性质重写
- en: \[ = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right). \]
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \[ = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right). \]
- en: 'Why does this help with underflow? By subtracting \(M\), the smallest value
    in the set, from each \(a_i\): (i) the largest term in \(\{e^{-(a_i - M)} : i
    = 1,\ldots,n\}\) becomes \(e^0 = 1\); and (ii) all other terms are between 0 and
    1, as they are exponentiations of nonpositive numbers. This manipulation avoids
    terms underflowing to zero because even very large values, when shifted by \(M\),
    are less likely to hit the underflow threshold.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么这有助于避免下溢？通过从每个 \(a_i\) 中减去 \(M\)，即集合中的最小值：（i）\(\{e^{-(a_i - M)} : i = 1,\ldots,n\}\)
    中的最大项变为 \(e^0 = 1\)；并且（ii）所有其他项都在 0 和 1 之间，因为它们是非正数的指数。这种操作避免了项下溢到零，因为即使是非常大的值，当通过
    \(M\) 移位时，也不太可能达到下溢阈值。'
- en: Here is an example. Imagine you have \(\mathbf{a} = (1000, 1001, 1002)\).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子。假设你有一个 \(\mathbf{a} = (1000, 1001, 1002)\)。
- en: 'Direct computation: \(e^{-1000}\), \(e^{-1001}\), and \(e^{-1002}\) might all
    underflow to zero.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接计算：\(e^{-1000}\)，\(e^{-1001}\)，和 \(e^{-1002}\) 可能都会下溢到零。
- en: 'With the log-sum-exp trick: Subtract \(M = 1000\), leading to \(e^{0}\), \(e^{-1}\),
    and \(e^{-2}\), all meaningful, non-zero results that accurately contribute to
    the sum.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对数和指数技巧：减去 \(M = 1000\)，得到 \(e^{0}\)，\(e^{-1}\)，和 \(e^{-2}\)，所有这些都有意义、非零的结果，准确地贡献到总和。
- en: We implement in NumPy.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 NumPy 中实现。
- en: '[PRE25]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**NUMERICAL CORNER:** We try it on a simple example.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们在一个简单的例子上尝试它。'
- en: '[PRE26]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We first attempt a direct computation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先尝试直接计算。
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Predictly, we get an underflow error and a useless output.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 预计会得到下溢错误和无用的输出。
- en: Next, we try the log-sum-exp trick.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们尝试log-sum-exp技巧。
- en: '[PRE30]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This time we get an output which seems reasonable, something slightly larger
    than \(-1000\) as expected (Why?).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们得到了一个看似合理的输出，略大于预期的 \(-1000\)（为什么？）。
- en: \(\unlhd\)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: After this long – but important! – parenthesis, we return to the EM algorithm.
    We modify it by implementing the log-sum-exp trick in the subroutine `responsibility`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个长——但很重要！——的括号之后，我们回到EM算法。我们通过在子程序`responsibility`中实现log-sum-exp技巧来修改它。
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**NUMERICAL CORNER:** We go back to the MNIST example with only the 2s.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们回到只有2s的MNIST示例。'
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here are the parameters for one cluster.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个簇的参数。
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png](../Images/4e3661e0a1a8051341921aba94079c7b.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png](../Images/4e3661e0a1a8051341921aba94079c7b.png)'
- en: Here is the other one.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个。
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png](../Images/372533fd8fad14df01f1cf71749a0141.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png](../Images/372533fd8fad14df01f1cf71749a0141.png)'
- en: Now that the model is trained, we compute the probability that an example image
    is in each cluster. We use the first image in the dataset that we plotted earlier.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练好，我们计算每个簇中示例图像的概率。我们使用之前绘制的数据集的第一张图像。
- en: '[PRE36]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: It indeed identifies the second cluster as significantly more likely.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实识别出第二个簇更有可能。
- en: '**TRY IT!** In the MNIST example, as we have seen, the probabilities involved
    are extremely small and the responsibilities are close to \(0\) or \(1\). Implement
    a variant of EM, called hard EM, which replaces responsibilities with the one-hot
    encoding of the largest responsibility. Test it on the MNIST example again. ([Open
    In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试一下！** 在MNIST示例中，正如我们所看到的，涉及的概率非常小，责任值接近 \(0\) 或 \(1\)。实现一个EM的变体，称为硬EM，用最大的责任值的一热编码替换责任值。再次在MNIST示例上测试它。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
- en: \(\unlhd\)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model is a simple example
    of a latent variable model. Ask your favorite AI chatbot to discuss more complex
    latent variable models, such as the variational autoencoder or the Gaussian process
    latent variable model, and their applications in unsupervised learning. \(\ddagger\)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习** 多元伯努利混合模型是潜在变量模型的一个简单例子。请你的AI聊天机器人讨论更复杂的潜在变量模型，如变分自编码器或高斯过程潜在变量模型，以及它们在无监督学习中的应用。
    \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(有Claude、Gemini和ChatGPT的帮助)*'
- en: '**1** In the mixture of multivariate Bernoullis model, the joint distribution
    is given by:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在多元伯努利混合模型中，联合分布由以下给出：'
- en: a) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X}
    = \mathbf{x}]\)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X}
    = \mathbf{x}]\)
- en: b) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
- en: c) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
- en: d) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C =
    k, \mathbf{X} = \mathbf{x}]\)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C =
    k, \mathbf{X} = \mathbf{x}]\)
- en: '**2** The majorization-minimization principle states that:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 主导-最小化原理表明：'
- en: a) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: a) 如果 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处主导 \(f\)，那么 \(U_{\mathbf{x}}\) 的全局最小值
    \(\mathbf{x}'\) 满足 \(f(\mathbf{x}') \geq f(\mathbf{x})\)。
- en: b) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: b) 如果 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处主导 \(f\)，那么 \(U_{\mathbf{x}}\) 的全局最小值
    \(\mathbf{x}'\) 满足 \(f(\mathbf{x}') \leq f(\mathbf{x})\)。
- en: c) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: c) 如果 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处次主导 \(f\)，那么 \(U_{\mathbf{x}}\) 的全局最小值
    \(\mathbf{x}'\) 满足 \(f(\mathbf{x}') \geq f(\mathbf{x})\)。
- en: d) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: d) 如果\(U_{\mathbf{x}}\)在\(\mathbf{x}\)处次支配\(f\)，那么\(U_{\mathbf{x}}\)的全局最小值\(\mathbf{x}'\)满足\(f(\mathbf{x}')
    \leq f(\mathbf{x})\)。
- en: '**3** In the EM algorithm for mixtures of multivariate Bernoullis, the M-step
    involves:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 在多变量伯努利混合的EM算法中，M步骤包括：'
- en: a) Updating the parameters \(\pi_k\) and \(p_{k,m}\)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: a) 更新参数\(\pi_k\)和\(p_{k,m}\)
- en: b) Computing the responsibilities \(r_{k,i}^t\)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: b) 计算责任\(r_{k,i}^t\)
- en: c) Minimizing the negative log-likelihood
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: c) 最小化负对数似然
- en: d) Applying the log-sum-exp trick
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: d) 应用对数和指数技巧
- en: '**4** The mixture of multivariate Bernoullis model is represented by the following
    graphical model:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 多变量伯努利混合模型由以下图模型表示：'
- en: a)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: a)
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: b)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: b)
- en: '[PRE39]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: c)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: c)
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: d)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: d)
- en: '[PRE41]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '**5** In the context of clustering, what is the interpretation of the responsibilities
    computed in the E-step of the EM algorithm?'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在聚类的背景下，EM算法E步骤中计算的责任有什么解释？'
- en: a) They represent the distance of each data point to the cluster centers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: a) 他们表示每个数据点到簇中心的距离。
- en: b) They indicate the probability of each data point belonging to each cluster.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它们表示每个数据点属于每个簇的概率。
- en: c) They determine the optimal number of clusters.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: c) 他们确定最佳簇数量。
- en: d) They are used to initialize the cluster centers in the M-step.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它们用于M步骤中初始化簇中心。
- en: 'Answer for 1: b. Justification: The text states, “\(\mathbb{P}[\mathbf{X} =
    \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K
    \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\).”'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 1号的答案：b. 理由：文本指出，“\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C
    = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C
    = k] \mathbb{P}[C = k]\)。”
- en: 'Answer for 2: b. Justification: “Let \(f: \mathbb{R}^d \to \mathbb{R}\) and
    suppose \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\)
    be a global minimizer of \(U_{\mathbf{x}}(\mathbf{z})\) as a function of \(\mathbf{z}\),
    provided it exists. Then \(f(\mathbf{x}'') \leq f(\mathbf{x})\).”'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '2号的答案：b. 理由：“设\(f: \mathbb{R}^d \to \mathbb{R}\)并且假设\(U_{\mathbf{x}}\)在\(\mathbf{x}\)处主支配\(f\)。设\(\mathbf{x}''\)是\(U_{\mathbf{x}}(\mathbf{z})\)作为\(\mathbf{z}\)的函数的全局最小值，如果存在的话。那么\(f(\mathbf{x}'')
    \leq f(\mathbf{x})\)。”'
- en: 'Answer for 3: a. Justification: In the summary of the EM algorithm, the M-step
    is described as updating the parameters: “\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\)
    and \(p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}\),” which require the responsibilities.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 3号的答案：a. 理由：在EM算法的总结中，M步骤被描述为更新参数：“\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\)和\(p_{k,m}^{t+1}
    = \frac{\eta_{k,m}^t}{\eta_k^t}\)，”这需要责任值。
- en: 'Answer for 4: b. Justification: The text states, “Mathematically, that corresponds
    to applying the law of total probability as we did previously. Further, we let
    the vertex for \(X\) be shaded to indicate that it is observed, while the vertex
    for \(Y\) is not shaded to indicate that it is not.”'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 4号的答案：b. 理由：文本指出，“数学上，这相当于应用我们之前使用的全概率定律。进一步，我们让\(X\)的顶点被阴影覆盖以表示它是已观察到的，而\(Y\)的顶点没有被阴影覆盖以表示它没有被观察到。”
- en: 'Answer for 5: b. Justification: The text refers to responsibilities as “our
    estimate – under the current parameter – of the probability that the sample comes
    from cluster \(k\).”'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 5号的答案：b. 理由：文本将责任定义为“在当前参数下，样本来自簇\(k\)的概率的估计。”
- en: 6.4.1\. Mixtures[#](#mixtures "Link to this heading")
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4.1\. 混合\[#](#mixtures "链接到这个标题")
- en: Mixtures\(\idx{mixture}\xdi\) are a natural way to define probability distributions.
    The basic idea is to consider a pair of random vectors \((\bX,\bY)\) and assume
    that \(\bY\) is unobserved. The effect on the observed vector \(\bX\) is that
    \(\bY\) is marginalized out. Indeed, by the law of total probability, for any
    \(\bx \in \S_\bX\)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 混合\(\idx{mixture}\xdi\)是定义概率分布的自然方式。基本思想是考虑一对随机向量\((\bX,\bY)\)并假设\(\bY\)是未观察到的。对观察向量\(\bX\)的影响是\(\bY\)被边缘化。实际上，根据全概率定律，对于任何\(\bx
    \in \S_\bX\)
- en: \[\begin{align*} p_\bX(\bx) &= \P[\bX = \bx]\\ &= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by]
    \,\P[\bY=\by]\\ &= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by) \end{align*}\]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_\bX(\bx) &= \P[\bX = \bx]\\ &= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by]
    \,\P[\bY=\by]\\ &= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by) \end{align*}\]
- en: 'where we used that the events \(\{\bY=\by\}\), \(\by \in \S_\bY\), form a partition
    of the probability space. We interpret this equation as defining \(p_\bX(\bx)\)
    as a convex combination – a mixture – of the distributions \(p_{\bX|\bY}(\bx|\by)\),
    \(\by \in \S_\bY\), with mixing weights \(p_\bY(\by)\). In general, we need to
    specify the full conditional probability distribution (CPD): \(p_{\bX|\bY}(\bx|\by),
    \forall \bx \in \S_{\bX}, \by \in \S_\bY\). But assuming that the mixing weights
    and/or CPD come from parametric families can help reduce the complexity of the
    model.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了事件 \(\{\bY=\by\}\)，\(\by \in \S_\bY\)，构成了概率空间的划分。我们解释这个方程为定义 \(p_\bX(\bx)\)
    为 \(p_{\bX|\bY}(\bx|\by)\)，\(\by \in \S_\bY\) 的凸组合——混合——，混合权重为 \(p_\bY(\by)\)。一般来说，我们需要指定完整的条件概率分布（CPD）：\(p_{\bX|\bY}(\bx|\by),
    \forall \bx \in \S_{\bX}, \by \in \S_\bY\)。但假设混合权重和/或CPD来自参数化族可以帮助减少模型的复杂性。
- en: That can be represented in a digraph with a directed edge from a vertex for
    \(\mathbf{Y}\) to a vertex for \(\mathbf{X}\). Further, we let the vertex for
    \(\mathbf{X}\) be shaded to indicate that it is observed, while the vertex for
    \(\mathbf{Y}\) is not shaded to indicate that it is not. Mathematically, that
    corresponds to applying the law of total probability as we did previously.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以用一个有向图表示，其中从 \(\mathbf{Y}\) 的顶点到 \(\mathbf{X}\) 的顶点有一条有向边。进一步地，我们让 \(\mathbf{X}\)
    的顶点被阴影覆盖以表示它被观察，而 \(\mathbf{Y}\) 的顶点没有被阴影覆盖以表示它没有被观察。从数学上来说，这对应于应用我们之前使用的全概率定律。
- en: '![A mixture](../Images/d7c8f87d42f669398ac57f92c0a43388.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![混合](../Images/d7c8f87d42f669398ac57f92c0a43388.png)'
- en: In the parametric context, this gives rise to a fruitful approach to expanding
    distribution families. Suppose \(\{p_{\btheta}:\btheta \in \Theta\}\) is a parametric
    family of distributions. Let \(K \geq 2\), \(\btheta_1, \ldots, \btheta_K \in
    \Theta\) and \(\bpi = (\pi_1,\ldots,\pi_K) \in \Delta_K\). Suppose \(Y \sim \mathrm{Cat}(\bpi)\)
    and that the conditional distributions satisfy
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数化背景下，这产生了一种丰富的方法来扩展分布族。假设 \(\{p_{\btheta}:\btheta \in \Theta\}\) 是一个参数化的分布族。令
    \(K \geq 2\)，\(\btheta_1, \ldots, \btheta_K \in \Theta\) 和 \(\bpi = (\pi_1,\ldots,\pi_K)
    \in \Delta_K\)。假设 \(Y \sim \mathrm{Cat}(\bpi)\) 并且条件分布满足
- en: \[ p_{\bX|Y}(\bx|i) = p_{\btheta_i}(\bx). \]
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\bX|Y}(\bx|i) = p_{\btheta_i}(\bx). \]
- en: We write this as \(\bX|\{Y=i\} \sim p_{\btheta_i}\). Then we obtain the mixture
    model
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其写作 \(\bX|\{Y=i\} \sim p_{\btheta_i}\)。然后我们得到混合模型
- en: \[ p_{\bX}(\bx) = \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i) = \sum_{i=1}^K \pi_i
    p_{\btheta_i}(\bx). \]
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\bX}(\bx) = \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i) = \sum_{i=1}^K \pi_i
    p_{\btheta_i}(\bx). \]
- en: '**EXAMPLE:** **(Mixture of Multinomials)** Let \(n, m , K \geq 1\), \(\bpi
    \in \Delta_K\) and, for \(i=1,\ldots,K\), \(\mathbf{p}_i = (p_{i1},\ldots,p_{im})
    \in \Delta_m\). Suppose that \(Y \sim \mathrm{Cat}(\bpi)\) and that the conditional
    distributions are'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(多项式混合)** 设 \(n, m , K \geq 1\)，\(\bpi \in \Delta_K\)，对于 \(i=1,\ldots,K\)，\(\mathbf{p}_i
    = (p_{i1},\ldots,p_{im}) \in \Delta_m\)。假设 \(Y \sim \mathrm{Cat}(\bpi)\) 并且条件分布是'
- en: \[ \bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i). \]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i). \]
- en: Then \(\bX\) is a mixture of multinomials. Its distribution is then
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 \(\bX\) 是多项式的混合。它的分布是
- en: \[ p_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m
    p_{ij}^{x_j}. \]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m
    p_{ij}^{x_j}. \]
- en: \(\lhd\)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: Next is an important continuous example.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个重要的连续示例。
- en: '**EXAMPLE:** **(Gaussian mixture model)** \(\idx{Gaussian mixture model}\xdi\)
    For \(i=1,\ldots,K\), let \(\bmu_i\) and \(\bSigma_i\) be the mean and covariance
    matrix of a multivariate Gaussian. Let \(\bpi \in \Delta_K\). A Gaussian Mixture
    Model (GMM) is obtained as follows: take \(Y \sim \mathrm{Cat}(\bpi)\) and'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(高斯混合模型)** \(\idx{高斯混合模型}\xdi\) 对于 \(i=1,\ldots,K\)，设 \(\bmu_i\)
    和 \(\bSigma_i\) 是多元高斯的均值和协方差矩阵。设 \(\bpi \in \Delta_K\)。高斯混合模型（GMM）的获得如下：取 \(Y
    \sim \mathrm{Cat}(\bpi)\) 和'
- en: \[ \bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i). \]
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i). \]
- en: Its probability density function (PDF) takes the form
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其概率密度函数（PDF）的形式是
- en: \[ f_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
    \]
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_\bX(\bx) = \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
    \]
- en: \(\lhd\)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**NUMERICAL CORNER:** We plot the density for means \(\bmu_1 = (-2,-2)\) and
    \(\bmu_2 = (2,2)\) and covariance matrices'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们绘制了均值 \(\bmu_1 = (-2,-2)\) 和 \(\bmu_2 = (2,2)\) 以及协方差矩阵的密度图'
- en: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{and} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1
    \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{和} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2
    \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
- en: where \(\sigma_1 = 1.5\), \(\sigma_2 = 0.5\) and \(\rho = -0.75\). The mixing
    weights are \(\pi_1 = 0.25\) and \(\pi_2 = 0.75\).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_1 = 1.5\)，\(\sigma_2 = 0.5\) 和 \(\rho = -0.75\)。混合权重为 \(\pi_1 =
    0.25\) 和 \(\pi_2 = 0.75\)。
- en: '[PRE42]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源
    隐藏代码单元格源</summary>
- en: '[PRE43]</details> ![../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png](../Images/810f47a1332c13f0595b65b28e44ab63.png)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE43]</details> ![../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png](../Images/810f47a1332c13f0595b65b28e44ab63.png)'
- en: \(\unlhd\)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: In NumPy, as we have seen before, the module [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html)
    also provides a way to sample from mixture models by using [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，正如我们之前所看到的，模块 `numpy.random` 也提供了一种通过使用 `numpy.random.Generator.choice`
    来从混合模型中采样的方法。
- en: For instance, we consider mixtures of multivariate Gaussians. We chage the notation
    slightly to track Python’s indexing. For \(i=0,1\), we have a mean \(\bmu_i \in
    \mathbb{R}^d\) and a positive definite covariance matrix \(\bSigma_i \in \mathbb{R}^{d
    \times d}\). We also have mixture weights \(\phi_0, \phi_1 \in (0,1)\) such that
    \(\phi_0 + \phi_1 = 1\). Suppose we want to generate a total of \(n\) samples.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们考虑多元高斯混合。我们稍微改变一下符号以跟踪 Python 的索引。对于 \(i=0,1\)，我们有一个均值 \(\bmu_i \in \mathbb{R}^d\)
    和一个正定协方差矩阵 \(\bSigma_i \in \mathbb{R}^{d \times d}\)。我们还有一个混合权重 \(\phi_0, \phi_1
    \in (0,1)\)，使得 \(\phi_0 + \phi_1 = 1\)。假设我们想要生成总共 \(n\) 个样本。
- en: 'For each sample \(j=1,\ldots, n\), independently from everything else:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个样本 \(j=1,\ldots, n\)，独立于其他所有内容：
- en: We first pick a component \(i \in \{0,1\}\) at random according to the mixture
    weights, that is, \(i=0\) is chosen with probability \(\phi_0\) and \(i=1\) is
    chosen with probability \(\phi_1\).
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先随机选择一个分量 \(i \in \{0,1\}\)，根据混合权重，即 \(i=0\) 以概率 \(\phi_0\) 被选中，\(i=1\) 以概率
    \(\phi_1\) 被选中。
- en: We generate a sample \(\bX_j = (X_{j,1},\ldots,X_{j,d})\) according to a multivariate
    Gaussian with mean \(\bmu_i\) and covariance \(\bSigma_i\).
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据均值 \(\bmu_i\) 和协方差 \(\bSigma_i\) 生成一个样本 \(\bX_j = (X_{j,1},\ldots,X_{j,d})\)，这是一个多元高斯分布。
- en: This is straightforward to implement by using again [`numpy.random.Generator.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html)
    to choose the component of each sample and [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html)
    to generate multivariate Gaussians. For convenience, we will stack the means and
    covariances into one array with a new dimension. So, for instance, the covariance
    matrices will now be in a 3d-array, that is, an array with \(3\) indices. The
    first index corresponds to the component (here \(0\) or \(1\)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过再次使用 `numpy.random.Generator.choice` 来选择每个样本的分量，并使用 `numpy.random.Generator.multivariate_normal`
    来生成多元高斯分布来实现。为了方便，我们将均值和协方差堆叠到一个具有新维度的数组中。因此，例如，协方差矩阵现在将位于一个三维数组中，即具有 \(3\) 个索引的数组。第一个索引对应于分量（这里
    \(0\) 或 \(1\)）。
- en: '**Figure:** Three matrices ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** 三个矩阵 ([来源](https://www.tensorflow.org/guide/tensor#basics))'
- en: '![Three matrices](../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![三个矩阵](../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png)'
- en: \(\bowtie\)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**Figure:** Three matrices stacked into a 3d-array ([Source](https://www.tensorflow.org/guide/tensor#basics))'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** 将三个矩阵堆叠成一个三维数组 ([来源](https://www.tensorflow.org/guide/tensor#basics))'
- en: '![Three matrices stacked into a tensor](../Images/a61cc745a3c58639bf7340b2af821416.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![三个矩阵堆叠成张量](../Images/a61cc745a3c58639bf7340b2af821416.png)'
- en: \(\bowtie\)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: The code is the following. It returns an `d` by `n` array `X`, where each row
    is a sample from a 2-component Gaussian mixture.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码如下。它返回一个 `d` 行 `n` 列的数组 `X`，其中每一行是从一个双分量高斯混合模型中采样的样本。
- en: '[PRE44]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**NUMERICAL CORNER:** Let us try it with following parameters. We first define
    the covariance matrices and show what happens when they are stacked into a 3d
    array (as is done within `gmm2`).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 让我们用以下参数尝试一下。我们首先定义协方差矩阵，并展示当它们堆叠成一个三维数组（正如在`gmm2`中那样）时会发生什么。'
- en: '[PRE45]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Then we define the rest of the parameters.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义其余的参数。
- en: '[PRE49]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png](../Images/22ec3b142976b6b20a0185e9e997fea9.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png](../Images/22ec3b142976b6b20a0185e9e997fea9.png)'
- en: \(\unlhd\)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '6.4.2\. Example: Mixtures of multivariate Bernoullis and the EM algorithm[#](#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm
    "Link to this heading")'
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4.2. 示例：多元伯努利混合与EM算法[#](#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm
    "链接到本标题")
- en: Let \(\mathcal{C} = \{1, \ldots, K\}\) be a collection of classes. Let \(C\)
    be a random variable taking values in \(\mathcal{C}\) and, for \(m=1, \ldots,
    M\), let \(X_i\) take values in \(\{0,1\}\). Define \(\pi_k = \P[C = k]\) and
    \(p_{k,m} = \P[X_m = 1|C = k]\) for \(m = 1,\ldots, M\). We denote by \(\bX =
    (X_1, \ldots, X_M)\) the corresponding vector of \(X_i\)’s and assume that the
    entries are conditionally independent given \(C\).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 令 \(\mathcal{C} = \{1, \ldots, K\}\) 为一个类别的集合。令 \(C\) 为一个取值在 \(\mathcal{C}\)
    中的随机变量，对于 \(m=1, \ldots, M\)，令 \(X_i\) 取值在 \(\{0,1\}\) 中。定义 \(\pi_k = \P[C = k]\)
    和 \(p_{k,m} = \P[X_m = 1|C = k]\) 对于 \(m = 1,\ldots, M\)。我们用 \(\bX = (X_1, \ldots,
    X_M)\) 表示相应的 \(X_i\) 的向量，并假设在 \(C\) 给定的情况下，这些条目是条件独立的。
- en: However, we assume this time that \(C\) itself is *not observed*. So the resulting
    joint distribution is the mixture
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这次我们假设 \(C\) 本身是*未观察到的*。因此，得到的联合分布是混合分布
- en: \[\begin{align*} \P[\bX = \bx] &= \sum_{k=1}^K \P[C = k, \bX = \bx]\\ &= \sum_{k=1}^K
    \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}. \end{align*}\]
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \P[\bX = \bx] &= \sum_{k=1}^K \P[C = k, \bX = \bx]\\ &= \sum_{k=1}^K
    \P[\bX = \bx|C=k] \,\P[C=k]\\ &= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m}
    (1-p_{k,m})^{1-x_m}. \end{align*}\]
- en: Graphically, this is the same are the Naive Bayes model, except that \(C\) is
    not observed and therefore is not shaded.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，这与朴素贝叶斯模型相同，只是 \(C\) 未被观察，因此没有被阴影覆盖。
- en: '![Mixture of multivariate Bernoullis](../Images/c51352d9d6ab19d2f9e73f898c25b648.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![多元伯努利混合](../Images/c51352d9d6ab19d2f9e73f898c25b648.png)'
- en: This type of model is useful in particular for clustering tasks, where the \(c_k\)s
    can be thought of as different clusters. Similarly to what we did in the previous
    section, our goal is to infer the parameters from samples and then predict the
    class of an old or new sample given its features. The main – substantial – difference
    is that the true labels of the samples are not observed. As we will see, that
    complicates the task considerably.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的模型在聚类任务中特别有用，其中 \(c_k\) 可以被视为不同的簇。类似于我们在上一节中所做的，我们的目标是根据样本推断参数，然后根据其特征预测旧样本或新样本的类别。主要的——实质性的——区别是样本的真实标签没有被观察到。正如我们将看到的，这大大增加了任务的复杂性。
- en: '**Model fitting** We first fit the model from training data \(\{\bx_i\}_{i=1}^n\).
    Recall that the corresponding class labels \(c_i\)s are not observed. In this
    type of model, they are referred to as hidden or latent variables and we will
    come back to their inference below.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型拟合**: 我们首先从训练数据 \(\{\bx_i\}_{i=1}^n\) 中拟合模型。回想一下，相应的类别标签 \(c_i\)s 是未观察到的。在这种类型的模型中，它们被称为隐藏或潜在变量，我们将在下面回到它们的推断。'
- en: We would like to use maximum likelihood estimation, that is, maximize the probability
    of observing the data
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用最大似然估计，即最大化观察数据的概率
- en: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\}) = \prod_{i=1}^n \left( \sum_{k=1}^K
    \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right). \]
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\}) = \prod_{i=1}^n \left( \sum_{k=1}^K
    \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right). \]
- en: As usual, we assume that the samples are independent and identically distributed.
    Consider the negative log-likelihood (NLL)
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们假设样本是独立同分布的。考虑负对数似然（NLL）
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
- en: Already, we see that things are potentially more difficult than they were in
    the supervised (or fully observed) case. The NLL does not decompose into a sum
    of terms depending on different sets of parameters.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 已经，我们看到事情可能比监督（或完全观察）的情况更复杂。NLL 不能分解为依赖于不同参数集的项之和。
- en: At this point, one could fall back on the field of optimization and use a gradient-based
    method to minimize the NLL. Indeed that is an option, although note that one must
    be careful to account for the constrained nature of the problem (i.e., the parameters
    sum to one). There is a vast array of constrained optimization techniques suited
    for this task.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以退回到优化领域，并使用基于梯度的方法来最小化负对数似然（NLL）。确实，这是一个选择，但请注意，必须小心地考虑到问题的约束性质（即参数之和为1）。有许多适合此任务的约束优化技术。
- en: Instead a more popular approach in this context, the EM algorithm, is based
    on the general principle of majorization-minimization, which we have encountered
    implicitly in the \(k\)-means algorithm and the convergence proof of gradient
    descent in the smooth case. We detail this important principle in the next subsection
    before returning to model fitting in mixtures.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 而在这个背景下，更常见的方法是EM算法，它基于最大化和最小化的一般原理，我们在k-means算法和光滑情况下的梯度下降收敛证明中隐式地遇到过。在下一小节中，我们将详细阐述这个重要的原理，然后再回到混合模型拟合。
- en: '**Majorization-minimization** \(\idx{majorization-minimization}\xdi\) Here
    is a deceptively simple, yet powerful observation. Suppose we want to minimize
    a function \(f : \mathbb{R}^d \to \mathbb{R}\). Finding a local minimum of \(f\)
    may not be easy. But imagine that for each \(\mathbf{x} \in \mathbb{R}^d\) we
    have a surrogate function \(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\) that
    (1) dominates \(f\) in the following sense'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大化和最小化** \(\idx{majorization-minimization}\xdi\) 这里有一个看似简单但强大的观察。假设我们想要最小化一个函数
    \(f : \mathbb{R}^d \to \mathbb{R}\)。找到 \(f\) 的局部最小值可能不容易。但想象一下，对于每个 \(\mathbf{x}
    \in \mathbb{R}^d\)，我们都有一个代理函数 \(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\)，它（1）以下意义上支配
    \(f\)'
- en: \[ U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in
    \mathbb{R}^d \]
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in
    \mathbb{R}^d \]
- en: and (2) equals \(f\) at \(\mathbf{x}\)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 并且（2）在 \(\mathbf{x}\) 处等于 \(f\)
- en: \[ U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}). \]
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}). \]
- en: We say that \(U_\mathbf{x}\) majorizes \(f\) at \(\mathbf{x}\). Then we prove
    in the next lemma that \(U_\mathbf{x}\) can be used to make progress towards minimizing
    \(f\), that is, find a point \(\mathbf{x}'\) such that \(f(\mathbf{x}') \leq f(\mathbf{x})\).
    If in addition \(U_\mathbf{x}\) is easier to minimize than \(f\) itself, say because
    an explicit minimum can be computed, then this observation naturally leads to
    an iterative algorithm.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说 \(U_\mathbf{x}\) 在 \(\mathbf{x}\) 处对 \(f\) 进行最大化。然后我们在下一个引理中证明 \(U_\mathbf{x}\)
    可以用来朝着最小化 \(f\) 做出进展，也就是说，找到一个点 \(\mathbf{x}'\) 使得 \(f(\mathbf{x}') \leq f(\mathbf{x})\)。如果此外
    \(U_\mathbf{x}\) 比本身更容易最小化，比如说，因为可以计算出一个显式的最小值，那么这个观察自然导致了一个迭代算法。
- en: '![A majorizing function (with help from ChatGPT; inspired by Source)](../Images/c05a4e1add7058b646f07a53f61281a2.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![一个最大化的函数（得益于ChatGPT的帮助；受来源启发）](../Images/c05a4e1add7058b646f07a53f61281a2.png)'
- en: '**LEMMA** **(Majorization-Minimization)** \(\idx{majorization-minimization
    lemma}\xdi\) Let \(f : \mathbb{R}^d \to \mathbb{R}\) and suppose \(U_{\mathbf{x}}\)
    majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\) be a global minimum of
    \(U_\mathbf{x}\). Then'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(最大化和最小化)** \(\idx{majorization-minimization lemma}\xdi\) 设 \(f :
    \mathbb{R}^d \to \mathbb{R}\)，并假设 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处对 \(f\)
    进行最大化。设 \(\mathbf{x}''\) 是 \(U_\mathbf{x}\) 的全局最小值。那么'
- en: \[ f(\mathbf{x}') \leq f(\mathbf{x}). \]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}') \leq f(\mathbf{x}). \]
- en: \(\flat\)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* Indeed'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 确实'
- en: \[ f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x})
    = f(\mathbf{x}), \]
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x})
    = f(\mathbf{x}), \]
- en: where the first inequality follows from the domination property of \(U_\mathbf{x}\),
    the second inequality follows from the fact that \(\mathbf{x}'\) is a global minimum
    of \(U_\mathbf{x}\) and the equality follows from the fact that \(U_{\mathbf{x}}\)
    equals \(f\) at \(\mathbf{x}\). \(\square\)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一个不等式来自 \(U_\mathbf{x}\) 的支配性质，第二个不等式来自 \(\mathbf{x}'\) 是 \(U_\mathbf{x}\)
    的全局最小值的事实，等式来自 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处等于 \(f\) 的事实。 \(\square\)
- en: We have already encountered this idea.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遇到过这个想法。
- en: '**EXAMPLE:** **(Minimizing a smooth function)** Let \(f : \mathbb{R}^d \to
    \mathbb{R}\) be \(L\)-smooth. By the *Quadratic Bound for Smooth Functions*, for
    all \(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\) it holds that'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（最小化光滑函数）** 设 \(f : \mathbb{R}^d \to \mathbb{R}\) 为 \(L\)-光滑。根据 *光滑函数的二次界*，对于所有
    \(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\)，都有'
- en: \[ f(\mathbf{z}) \leq U_{\mathbf{x}}(\mathbf{z}) := f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{z}
    - \mathbf{x}) + \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2. \]
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{z}) \leq U_{\mathbf{x}}(\mathbf{z}) := f(\mathbf{x}) + \nabla f(\mathbf{x})^T(\mathbf{z}
    - \mathbf{x}) + \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2. \]
- en: By showing that \(U_{\mathbf{x}}\) is minimized at \(\mathbf{z} = \mathbf{x}
    - (1/L)\nabla f(\mathbf{x})\), we previously obtained the descent guarantee
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 通过证明 \(U_{\mathbf{x}}\) 在 \(\mathbf{z} = \mathbf{x} - (1/L)\nabla f(\mathbf{x})\)
    处达到最小值，我们之前获得了下降保证
- en: \[ f(\mathbf{x} - (1/L)\nabla f(\mathbf{x})) \leq f(\mathbf{x}) - \frac{1}{2
    L} \|\nabla f(\mathbf{x})\|^2 \]
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{x} - (1/L)\nabla f(\mathbf{x})) \leq f(\mathbf{x}) - \frac{1}{2
    L} \|\nabla f(\mathbf{x})\|^2 \]
- en: for gradient descent, which played a central role in the analysis of its convergence\(\idx{convergence
    analysis}\xdi\). \(\lhd\)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降，它在其收敛性分析（\(\idx{收敛性分析}\xdi\)）中扮演了核心角色。\(\lhd\)
- en: '**EXAMPLE:** **(\(k\)-means)** \(\idx{Lloyd''s algorithm}\xdi\) Let \(\mathbf{x}_1,\ldots,\mathbf{x}_n\)
    be \(n\) vectors in \(\mathbb{R}^d\). One way to formulate the \(k\)-means clustering
    problem is as the minimization of'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（\(k\)-means）** \(\idx{Lloyd''s algorithm}\xdi\) 设 \(\mathbf{x}_1,\ldots,\mathbf{x}_n\)
    是 \(\mathbb{R}^d\) 中的 \(n\) 个向量。一种将 \(k\)-means 聚类问题表述为最小化的方法是'
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 \]
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 \]
- en: over the centers \(\bmu_1,\ldots,\bmu_K\), where recall that \([K] = \{1,\ldots,K\}\).
    For fixed \(\bmu_1,\ldots,\bmu_K\) and \(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\),
    define
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在中心 \(\bmu_1,\ldots,\bmu_K\) 上，记住 \([K] = \{1,\ldots,K\}\)。对于固定的 \(\bmu_1,\ldots,\bmu_K\)
    和 \(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\)，定义
- en: \[ c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in
    [K]\right\}, \quad i=1,\ldots,n \]
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: \[ c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in
    [K]\right\}, \quad i=1,\ldots,n \]
- en: and
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \|\mathbf{x}_i
    - \blambda_{c_\mathbf{m}(i)}\|^2 \]
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \|\mathbf{x}_i
    - \blambda_{c_\mathbf{m}(i)}\|^2 \]
- en: for \(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\). That is, we fix the optimal
    cluster assignments under \(\bmu_1,\ldots,\bmu_K\) and then vary the centers.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\)。也就是说，我们固定了在 \(\bmu_1,\ldots,\bmu_K\)
    下的最优簇分配，然后改变中心。
- en: We claim that \(U_\mathbf{m}\) is majorizing \(f\) at \(\bmu_1,\ldots,\bmu_K\).
    Indeed
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声称 \(U_\mathbf{m}\) 在 \(\bmu_1,\ldots,\bmu_K\) 处是 \(f\) 的主函数。确实
- en: \[ f(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i
    - \blambda_j\|^2 \leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
    = U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) \]
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\blambda_1,\ldots,\blambda_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i
    - \blambda_j\|^2 \leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
    = U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) \]
- en: and
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 = \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2 = U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
    \]
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\bmu_1,\ldots,\bmu_K) = \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i -
    \bmu_j\|^2 = \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2 = U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
    \]
- en: Moreover \(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\) is easy to minimize.
    We showed previously that the optimal representatives are
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，\(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\) 很容易最小化。我们之前已经证明了最优代表是
- en: \[ \boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i \]
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i \]
- en: 'where \(C_j = \{i : c_\mathbf{m}(i) = j\}\).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 \(C_j = \{i : c_\mathbf{m}(i) = j\}\).'
- en: The *Majorization-Minimization Lemma* implies that
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '*主函数-最小化引理* 意味着'
- en: \[ f(\bmu_1',\ldots,\bmu_K') \leq f(\bmu_1,\ldots,\bmu_K). \]
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\bmu_1',\ldots,\bmu_K') \leq f(\bmu_1,\ldots,\bmu_K). \]
- en: This argument is equivalent to our previous analysis of the \(k\)-means algorithm.
    \(\lhd\)
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这个论点等价于我们之前对 \(k\)-means 算法的分析。\(\lhd\)
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model assumes a fixed
    number of clusters. Ask your favorite AI chatbot to discuss Bayesian nonparametric
    extensions of this model, such as the Dirichlet process mixture model, which can
    automatically infer the number of clusters from the data. \(\ddagger\)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习** 多变量伯努利混合模型假设有固定的簇数。请向您的首选AI聊天机器人询问该模型的贝叶斯非参数扩展，例如狄利克雷过程混合模型，它可以自动从数据中推断簇数。\(\ddagger\)'
- en: '**EM algorithm** The [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\(\idx{EM
    algorithm}\xdi\) is an instantiation of the majorization-minimization principle
    that applies widely to parameter estimation of mixtures. Here we focus on the
    mixture of multivariate Bernoullis.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '**EM算法** [期望最大化（EM）算法](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\(\idx{EM
    algorithm}\xdi\) 是主次最小化原理的一个实例，该原理广泛应用于混合参数估计。在这里，我们关注多元伯努利混合。'
- en: Recall that the objective to be minimized is
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，要最小化的目标是
- en: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\bpi, \{\bp_k\}; \{\bx_i\}) &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
    \end{align*}\]
- en: To simplify the notation and highlight the general idea, we let \(\btheta =
    (\bpi, \{\bp_k\})\), denote by \(\Theta\) the set of allowed values for \(\btheta\),
    and use \(\P_{\btheta}\) to indicate that probabilities are computed under the
    parameters \(\btheta\). We also return to the description of the model in terms
    of the unobserved latent variables \(\{C_i\}\). That is, we write the NLL as
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号并突出一般思想，我们让 \(\btheta = (\bpi, \{\bp_k\})\)，用 \(\Theta\) 表示 \(\btheta\)
    的允许值集合，并使用 \(\P_{\btheta}\) 表示在参数 \(\btheta\) 下计算概率。我们还将回到用未观察到的潜在变量 \(\{C_i\}\)
    来描述模型。也就是说，我们写出NLL为
- en: \[\begin{align*} L_n(\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i
    = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\ &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right). \end{align*}\]
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i
    = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\ &= - \sum_{i=1}^n \log \left(
    \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right). \end{align*}\]
- en: To derive a majorizing function, we use the convexity of the negative logarithm.
    Indeed
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推导一个主函数，我们使用负对数的凸性。确实
- en: \[ \frac{\partial}{\partial z}[- \log z] = - \frac{1}{z} \quad \text{and} \quad
    \frac{\partial^2}{\partial^2 z}[- \log z] = \frac{1}{z^2} > 0, \quad \forall z
    > 0. \]
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial}{\partial z}[- \log z] = - \frac{1}{z} \quad \text{和} \quad
    \frac{\partial^2}{\partial^2 z}[- \log z] = \frac{1}{z^2} > 0, \quad \forall z
    > 0. \]
- en: The first step of the construction is not obvious – it just works. For each
    \(i=1,\ldots,n\), we let \(r_{k,i}^{\btheta}\), \(k=1,\ldots,K\), be a strictly
    positive probability distribution over \([K]\). In other words, it defines a convex
    combination for every \(i\). Then we use convexity to obtain the upper bound
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 构造的第一步并不明显——它只是有效。对于每个 \(i=1,\ldots,n\)，我们让 \(r_{k,i}^{\btheta}\)，\(k=1,\ldots,K\)，是
    \([K]\) 上一个严格正的概率分布。换句话说，它为每个 \(i\) 定义了一个凸组合。然后我们使用凸性来获得上界
- en: \[\begin{align*} L_n(\tilde\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K
    r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right), \end{align*}\]
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\tilde\btheta) &= - \sum_{i=1}^n \log \left( \sum_{k=1}^K
    r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right), \end{align*}\]
- en: which holds for any \(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这对任何 \(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\) 都成立。
- en: We choose
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择
- en: \[ r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i] \]
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i] \]
- en: (which for the time being we assume is strictly positive) and we denote the
    right-hand side of the inequality by \(Q_{n}(\tilde\btheta|\btheta)\) (as a function
    of \(\tilde\btheta\)).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: （我们暂时假设它是严格正的）并将不等式的右边表示为 \(Q_{n}(\tilde\btheta|\btheta)\)（作为 \(\tilde\btheta\)
    的函数）。
- en: We make two observations.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个观察。
- en: '1- *Dominating property*: For any \(\tilde\btheta \in \Theta\), the inequality
    above implies immediately that \(L_n(\tilde\btheta) \leq Q_n(\tilde\btheta|\btheta)\).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '1- *支配性质*: 对于任何 \(\tilde\btheta \in \Theta\)，上述不等式立即意味着 \(L_n(\tilde\btheta)
    \leq Q_n(\tilde\btheta|\btheta)\)。'
- en: '2- *Equality at \(\btheta\)*: At \(\tilde\btheta = \btheta\),'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '2- *在 \(\btheta\) 上的等价性*: 在 \(\tilde\btheta = \btheta\)，'
- en: \[\begin{align*} Q_n(\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i
    = k | \bX_i = \bx_i] \P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\btheta}[\bX_i = \bx_i]\\
    &= - \sum_{i=1}^n \log \P_{\btheta}[\bX_i = \bx_i]\\ &= L_n(\btheta). \end{align*}\]
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} Q_n(\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i
    = k | \bX_i = \bx_i] \P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
    &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\btheta}[\bX_i = \bx_i]\\
    &= - \sum_{i=1}^n \log \P_{\btheta}[\bX_i = \bx_i]\\ &= L_n(\btheta). \end{align*}\]
- en: The two properties above show that \(Q_n(\tilde\btheta|\btheta)\), as a function
    of \(\tilde\btheta\), majorizes \(L_n\) at \(\btheta\).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个性质表明，\(Q_n(\tilde\btheta|\btheta)\) 作为 \(\tilde\btheta\) 的函数，在 \(\btheta\)
    处对 \(L_n\) 进行了主优。
- en: '**LEMMA** **(EM Guarantee)** \(\idx{EM guarantee}\xdi\) Let \(\btheta^*\) be
    a global minimizer of \(Q_n(\tilde\btheta|\btheta)\) as a function of \(\tilde\btheta\),
    provided it exists. Then'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(EM 保证)** \(\idx{EM guarantee}\xdi\) 设 \(\btheta^*\) 是 \(Q_n(\tilde\btheta|\btheta)\)
    作为 \(\tilde\btheta\) 的函数的全局最小化者，前提是它存在。那么'
- en: \[ L_n(\btheta^*) \leq L_n(\btheta). \]
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\btheta^*) \leq L_n(\btheta). \]
- en: \(\flat\)
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* The result follows directly from the *Majorization-Minimization Lemma*.
    \(\square\)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 结果直接来自 *Majorization-Minimization Lemma*. \(\square\)'
- en: What have we gained from this? As we mentioned before, using the *Majorization-Minimization
    Lemma* makes sense if \(Q_n\) is easier to minimize than \(L_n\) itself. Let us
    see why that is the case here.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中学到了什么？正如我们之前提到的，如果 \(Q_n\) 比本身更容易最小化 \(L_n\)，那么使用 *Majorization-Minimization
    Lemma* 是有意义的。让我们看看为什么在这里是这样的情况。
- en: '*E Step:* The function \(Q_n\) naturally decomposes into two terms'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '*E Step:* 函数 \(Q_n\) 自然分解为两个部分'
- en: \[\begin{align*} Q_n(\tilde\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k] + \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
    \end{align*}\]
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} Q_n(\tilde\btheta|\btheta) &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta}
    \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}}
    \right)\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k] + \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
    \end{align*}\]
- en: Because \(r_{k,i}^{\btheta}\) depends on \(\btheta\) *but not \(\tilde\btheta\)*,
    the second term is irrelevant to the opimization with respect to \(\tilde\btheta\).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(r_{k,i}^{\btheta}\) 依赖于 \(\btheta\) *但不依赖于 \(\tilde\btheta\)*，第二个部分与关于
    \(\tilde\btheta\) 的优化无关。
- en: The first term above can be written as
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 上述第一个部分可以写成
- en: \[\begin{align*} & - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k}
    \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
    &= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M
    [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta})
    \log (1-\tilde{p}_{k,m})], \end{align*}\]
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} & - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i
    = \bx_i, C_i = k]\\ &= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k}
    \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
    &= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M
    [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta})
    \log (1-\tilde{p}_{k,m})], \end{align*}\]
- en: where we defined, for \(k=1,\ldots,K\),
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义，对于 \(k=1,\ldots,K\)，
- en: \[ \eta_{k,m}^{\btheta} = \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta} \quad \text{and}
    \quad \eta_k^{\btheta} = \sum_{i=1}^n r_{k,i}^{\btheta}. \]
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \eta_{k,m}^{\btheta} = \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta} \quad \text{and}
    \quad \eta_k^{\btheta} = \sum_{i=1}^n r_{k,i}^{\btheta}. \]
- en: 'Here comes the key observation: this last expression is essentially the same
    as the NLL for the fully observed Naive Bayes model, except that the terms \(\mathbf{1}_{\{c_i
    = k\}}\) are replaced by \(r_{k,i}^{\btheta}\). If \(\btheta\) is our current
    estimate of the parameters, then the quantity \(r_{k,i}^{\btheta} = \P_{\btheta}[C_i
    = k|\bX_i = \bx_i]\) is our estimate – under the current parameter \(\btheta\)
    – of the probability that the sample \(\bx_i\) comes from cluster \(k\). We have
    previously computed \(r_{k,i}^{\btheta}\) for prediction under the Naive Bayes
    model. We showed there that'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键观察：这个最后的表达式本质上与完全观测的朴素贝叶斯模型的NLL相同，只是将项 \(\mathbf{1}_{\{c_i = k\}}\) 替换为
    \(r_{k,i}^{\btheta}\)。如果 \(\btheta\) 是我们当前参数的估计，那么量 \(r_{k,i}^{\btheta} = \P_{\btheta}[C_i
    = k|\bX_i = \bx_i]\) 是我们在当前参数 \(\btheta\) 下对样本 \(\bx_i\) 来自簇 \(k\) 的概率的估计。我们之前已经计算了在朴素贝叶斯模型下的
    \(r_{k,i}^{\btheta}\) 用于预测。我们展示了
- en: \[ r_{k,i}^{\btheta} = \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
    \]
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^{\btheta} = \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
    \]
- en: 'which in this new context is referred to as the responsibility that cluster
    \(k\) takes for data point \(i\). So we can interpret the expression above as
    follows: the variables \(\mathbf{1}_{\{c_i = k\}}\) are not observed here, but
    we have estimated their conditional probability distribution given the observed
    data \(\{\bx_i\}\), and we are taking an expectation with respect to that distribution
    instead.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新背景下被称为簇 \(k\) 对数据点 \(i\) 的责任。因此，我们可以这样解释上面的表达式：变量 \(\mathbf{1}_{\{c_i =
    k\}}\) 在这里没有观察到，但我们已经根据观察到的数据 \(\{\bx_i\}\) 估计了它们的条件概率分布，并且我们是在那个分布上取期望。
- en: The “E” in E Step (and EM) stands for “expectation”, which refers to using a
    surrogate function that is essentially an expected NLL.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: E Step（以及 EM）中的“E”代表“期望”，这指的是使用一个本质上为期望NLL的代理函数。
- en: '*M Step:* In any case, from a practical point of view, minimizing \(Q_n(\tilde\btheta|\btheta)\)
    over \(\tilde\btheta\) turns out to be a variant of fitting a Naive Bayes model
    – and the upshot to all this is that there is a straightforward formula for that!
    Recall that this happens because, the NLL in the Naive Bayes model decomposes:
    it naturally breaks up into terms that depend on separate sets of parameters,
    each of which can be optimized with a closed-form expression. The same happens
    with \(Q_n\) as should be clear from the derivation.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '*M Step:* 从实际的角度来看，在 \(\tilde\btheta\) 上最小化 \(Q_n(\tilde\btheta|\btheta)\)
    实际上是一种拟合朴素贝叶斯模型的变体——所有这一切的最终结果是有一个直接的公式！回想一下，这是因为朴素贝叶斯模型中的NLL分解：它自然地分解成依赖于不同参数集的项，每个都可以用闭式表达式进行优化。同样，\(Q_n\)
    也是如此，这应该从推导中清楚可见。'
- en: Adapting our previous calculations for fitting a Naive Bayes model, we get that
    \(Q_n(\tilde\btheta|\btheta)\) is minimized at
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 适应我们之前用于拟合朴素贝叶斯模型的计算，我们得到 \(Q_n(\tilde\btheta|\btheta)\) 在
- en: \[ \pi_k^* = \frac{\eta_k^{\btheta}}{n} \quad \text{and} \quad p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
    \quad \forall k \in [K], m \in [M]. \]
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \pi_k^* = \frac{\eta_k^{\btheta}}{n} \quad \text{和} \quad p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
    \quad \forall k \in [K], m \in [M]. \]
- en: We used the fact that
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了以下事实
- en: \[\begin{align*} \sum_{k=1}^K \eta_k^{\btheta} &= \sum_{k=1}^K \sum_{i=1}^n
    r_{k,i}^{\btheta}\\ &= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i =
    \bx_i]\\ &= \sum_{i=1}^n 1\\ &= n, \end{align*}\]
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \sum_{k=1}^K \eta_k^{\btheta} &= \sum_{k=1}^K \sum_{i=1}^n
    r_{k,i}^{\btheta}\\ &= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i =
    \bx_i]\\ &= \sum_{i=1}^n 1\\ &= n, \end{align*}\]
- en: since the conditional probability \(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\) adds
    up to one when summed over \(k\).
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 因为条件概率 \(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\) 在 \(k\) 上求和时总和为1。
- en: The “M” in M Step (and EM) stands for maximization, which here turns into minimization
    because of the use of the NLL.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: M Step（以及 EM）中的“M”代表最大化，在这里由于使用了负对数似然（NLL），它变成了最小化。
- en: To summarize, the EM algorithm works as follows in this case. Assume we have
    data points \(\{\bx_i\}_{i=1}^n\), that we have fixed \(K\) and that we have some
    initial parameter estimate \(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\) with
    strictly positive \(\pi_k^0\)s and \(p_{k,m}^0\)s. For \(t = 0,1,\ldots, T-1\)
    we compute for all \(i \in [n]\), \(k \in [K]\), and \(m \in [M]\)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在这种情况下，EM算法的工作方式如下。假设我们有一些数据点 \(\{\bx_i\}_{i=1}^n\)，我们固定了 \(K\)，并且我们有一些初始参数估计
    \(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\)，其中 \(\pi_k^0\)s 和 \(p_{k,m}^0\)s
    是严格正的。对于 \(t = 0,1,\ldots, T-1\)，我们对于所有 \(i \in [n]\)，\(k \in [K]\)，和 \(m \in
    [M]\) 计算
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \quad \text{(E Step)} \]\[ \eta_{k,m}^t = \sum_{i=1}^n x_{i,m} r_{k,i}^t \quad
    \text{and} \quad \eta_k^t = \sum_{i=1}^n r_{k,i}^t, \]
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \quad \text{(E 步)} \]\[ \eta_{k,m}^t = \sum_{i=1}^n x_{i,m} r_{k,i}^t \quad \text{和}
    \quad \eta_k^t = \sum_{i=1}^n r_{k,i}^t, \]
- en: and
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \pi_k^{t+1} = \frac{\eta_k^t}{n} \quad \text{and} \quad p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
    \quad \text{(M Step)} \]
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \pi_k^{t+1} = \frac{\eta_k^t}{n} \quad \text{和} \quad p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
    \quad \text{(M 步)} \]
- en: Provided \(\sum_{i=1}^n x_{i,m} > 0\) for all \(m\), the \(\eta_{k,m}^t\)s and
    \(\eta_k^t\)s remain positive for all \(t\) and the algorithm is well-defined.
    The *EM Guarantee* stipulates that the NLL cannot deteriorate, although note that
    it does not guarantee convergence to a global minimum.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有 \(m\) 上，如果 \(\sum_{i=1}^n x_{i,m} > 0\)，则 \(\eta_{k,m}^t\) 和 \(\eta_k^t\)
    在所有 \(t\) 上保持正值，算法是良好定义的。*EM保证*规定NLL不能恶化，尽管请注意，它并不保证收敛到全局最小值。
- en: We implement the EM algorithm for mixtures of multivariate Bernoullis. For this
    purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility
    of using Laplace smoothing.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了多元伯努利混合的EM算法。为此，我们调整了之前的朴素贝叶斯程序。我们还允许使用拉普拉斯平滑的可能性。
- en: '[PRE50]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We implement the E and M Step next.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来实现E步和M步。
- en: '[PRE51]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**NUMERICAL CORNER:** We test the algorithm on a very simple dataset.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们在一个非常简单的数据集上测试了该算法。'
- en: '[PRE52]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We compute the probability that the vector \((0, 0, 1)\) is in each cluster.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算向量 \((0, 0, 1)\) 在每个聚类中的概率。
- en: '[PRE56]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '**CHAT & LEARN** The EM algorithm can sometimes get stuck in local optima.
    Ask your favorite AI chatbot to discuss strategies for initializing the EM algorithm
    to avoid this issue, such as using multiple random restarts or using the k-means
    algorithm for initialization. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** EM算法有时会陷入局部最优。请向您的首选AI聊天机器人询问初始化EM算法以避免此问题的策略，例如使用多次随机重启或使用k-means算法进行初始化。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 6.4.3\. Clustering handwritten digits[#](#clustering-handwritten-digits "Link
    to this heading")
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4.3\. 手写数字聚类[#](#clustering-handwritten-digits "链接到本标题")
- en: To give a more involved example, we use the MNIST dataset.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给出一个更复杂的例子，我们使用MNIST数据集。
- en: 'Quoting [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database) again:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 再次引用 [维基百科](https://en.wikipedia.org/wiki/MNIST_database)：
- en: The MNIST database (Modified National Institute of Standards and Technology
    database) is a large database of handwritten digits that is commonly used for
    training various image processing systems. The database is also widely used for
    training and testing in the field of machine learning. It was created by “re-mixing”
    the samples from NIST’s original datasets. The creators felt that since NIST’s
    training dataset was taken from American Census Bureau employees, while the testing
    dataset was taken from American high school students, it was not well-suited for
    machine learning experiments. Furthermore, the black and white images from NIST
    were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which
    introduced grayscale levels. The MNIST database contains 60,000 training images
    and 10,000 testing images. Half of the training set and half of the test set were
    taken from NIST’s training dataset, while the other half of the training set and
    the other half of the test set were taken from NIST’s testing dataset.
  id: totrans-439
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MNIST 数据库（修改后的国家标准与技术研究院数据库）是一个常用的手写数字大型数据库，常用于训练各种图像处理系统。该数据库在机器学习领域的训练和测试中也得到了广泛应用。该数据库是通过“重新混合”NIST原始数据集的样本创建的。创建者认为，由于NIST的训练数据集是从美国人口普查局员工那里获得的，而测试数据集是从美国高中生那里获得的，因此它不适合机器学习实验。此外，NIST的黑白图像被归一化以适应28x28像素的边界框，并进行了抗锯齿处理，这引入了灰度级别。MNIST数据库包含60,000张训练图像和10,000张测试图像。训练集和测试集各有一半来自NIST的训练数据集，另一半来自NIST的测试数据集。
- en: '**Figure:** MNIST sample images ([Source](https://commons.wikimedia.org/wiki/File:MnistExamples.png))'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** MNIST样本图像 ([来源](https://commons.wikimedia.org/wiki/File:MnistExamples.png))'
- en: '![MNIST sample images](../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST样本图像](../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png)'
- en: \(\bowtie\)
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: '**NUMERICAL CORNER:** We load it from PyTorch. The data can be accessed with
    [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html).
    The [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html)
    below removes the color dimension in the image, which is grayscale. The [`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    converts the PyTorch tensors into NumPy arrays. See [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    for details on the data loading. We will say more about PyTorch in a later chapter.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们从PyTorch中加载它。数据可以通过 `torchvision.datasets.MNIST` 访问。[`squeeze()`](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html)
    以下从图像中移除了颜色维度，该图像是灰度的。[`numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)
    将PyTorch张量转换为NumPy数组。有关数据加载的详细信息，请参阅[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)。我们将在下一章中更多地介绍PyTorch。'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源
    隐藏代码单元格源</summary>
- en: '[PRE58]</details>'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE58]</details>'
- en: We turn the grayscale images into a black-and-white images by rounding the pixels.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过四舍五入像素来将灰度图像转换为黑白图像。
- en: '[PRE59]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: There are two common ways to write a \(2\). Let’s see if a mixture of multivariate
    Bernoullis can find them. We extract the images labelled \(2\).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 写 \(2\) 有两种常见的方式。让我们看看多元伯努利混合能否找到它们。我们提取标记为 \(2\) 的图像。
- en: '[PRE60]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The first image is the following.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图像如下。
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源
    隐藏代码单元格源</summary>
- en: '[PRE61]</details> ![../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png](../Images/e8eea00af5868fb166365334b2072575.png)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE61]</details> ![../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png](../Images/e8eea00af5868fb166365334b2072575.png)'
- en: Next, we transform the images into vectors.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将图像转换为向量。
- en: '[PRE62]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We run the algorithm with \(2\) clusters.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 \(2\) 个聚类运行算法。
- en: '[PRE63]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Uh-oh. Something went wrong. We encountered a numerical issue, underflow, which
    we discussed briefly previously. To confirm this, we run the code again but ask
    Python to warn us about it using [`numpy.seterr`](https://numpy.org/doc/stable/reference/generated/numpy.seterr.html).
    (By default, warnings are turned off in the book, but they can be reactivated
    using [`warnings.resetwarnings`](https://docs.python.org/3/library/warnings.html#warnings.resetwarnings).)
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。出问题了。我们遇到了一个数值问题，下溢，我们之前简要讨论过。为了确认这一点，我们再次运行代码，但要求Python使用 `numpy.seterr`
    [警告我们](https://numpy.org/doc/stable/reference/generated/numpy.seterr.html)。（默认情况下，警告是关闭的，但可以使用
    `warnings.resetwarnings` [重新激活](https://docs.python.org/3/library/warnings.html#warnings.resetwarnings)。）
- en: '[PRE65]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: \(\unlhd\)
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: When we compute the responsibilities
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算责任
- en: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \]
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r_{k,i}^t = \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
    {\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
    \]
- en: we first compute the negative logarithm of each term in the numerator as we
    did in the Naive Bayes case. But then we apply the function \(e^{-x}\), because
    this time we are not simply computing an optimal score. When all scores are high,
    this last step may result in underflow, that is, produces numbers so small that
    they get rounded down to zero by NumPy. Then the ratio defining `r_k` is not well-defined.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算分子中每个项的负对数，就像在朴素贝叶斯案例中做的那样。但这次我们应用函数 \(e^{-x}\)，因为这次我们不仅仅是在计算一个最优分数。当所有分数都很高时，这一步可能会导致下溢，也就是说，产生非常小的数字，它们会被NumPy四舍五入到零。然后定义
    `r_k` 的比率就不再明确了。
- en: To deal with this, we introduce a technique called the log-sum-exp trick\(\idx{log-sum-exp
    trick}\xdi\) (with some help from ChatGPT). Consider the computation of a function
    of \(\mathbf{a} = (a_1, \ldots, a_n)\) of the form
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这个问题，我们引入了一种称为对数和指数技巧的技术（在ChatGPT的一些帮助下）。考虑一个形如 \(\mathbf{a} = (a_1, \ldots,
    a_n)\) 的函数 \(\mathbf{a}\) 的计算
- en: \[ h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right). \]
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: \[ h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right). \]
- en: When the \(a_i\) values are large positive numbers, the terms \(e^{-a_i}\) can
    be so small that they underflow to zero. To counter this, the log-sum-exp trick
    involves a shift to bring these terms into a more favorable numerical range.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(a_i\) 的值是大的正数时，项 \(e^{-a_i}\) 可以非常小，以至于它们下溢到零。为了解决这个问题，log-sum-exp 技巧涉及一个移位，将这些项带入一个更有利的数值范围。
- en: 'It proceeds as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 它按以下方式进行：
- en: Identify the minimum value \(M\) among the \(a_i\)s
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 \(a_i\) 中识别最小值 \(M\)
- en: \[ M = \min\{a_1, a_2, \ldots, a_n\}. \]
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \[ M = \min\{a_1, a_2, \ldots, a_n\}. \]
- en: Subtract \(M\) from each \(a_i\) before exponentiation
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在指数运算之前从每个 \(a_i\) 中减去 \(M\)
- en: \[ \log \left( \sum_{i=1}^{n} e^{-a_i} \right) = \log \left( e^{-M} \sum_{i=1}^{n}
    e^{- (a_i - M)} \right). \]
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \[ \log \left( \sum_{i=1}^{n} e^{-a_i} \right) = \log \left( e^{-M} \sum_{i=1}^{n}
    e^{- (a_i - M)} \right). \]
- en: Rewrite using log properties
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对数性质重写
- en: \[ = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right). \]
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \[ = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right). \]
- en: 'Why does this help with underflow? By subtracting \(M\), the smallest value
    in the set, from each \(a_i\): (i) the largest term in \(\{e^{-(a_i - M)} : i
    = 1,\ldots,n\}\) becomes \(e^0 = 1\); and (ii) all other terms are between 0 and
    1, as they are exponentiations of nonpositive numbers. This manipulation avoids
    terms underflowing to zero because even very large values, when shifted by \(M\),
    are less likely to hit the underflow threshold.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么这有助于解决下溢问题？通过从每个 \(a_i\) 中减去集合中的最小值 \(M\)：（i）集合 \(\{e^{-(a_i - M)} : i =
    1,\ldots,n\}\) 中的最大项变为 \(e^0 = 1\)；（ii）所有其他项都在 0 和 1 之间，因为它们是非正数的指数。这种操作避免了项下溢到零，因为即使是非常大的值，当通过
    \(M\) 移位时，也不太可能达到下溢阈值。'
- en: Here is an example. Imagine you have \(\mathbf{a} = (1000, 1001, 1002)\).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子。想象一下，你有一个 \(\mathbf{a} = (1000, 1001, 1002)\)。
- en: 'Direct computation: \(e^{-1000}\), \(e^{-1001}\), and \(e^{-1002}\) might all
    underflow to zero.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接计算：\(e^{-1000}\)，\(e^{-1001}\)，和 \(e^{-1002}\) 可能都会下溢到零。
- en: 'With the log-sum-exp trick: Subtract \(M = 1000\), leading to \(e^{0}\), \(e^{-1}\),
    and \(e^{-2}\), all meaningful, non-zero results that accurately contribute to
    the sum.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 log-sum-exp 技巧：减去 \(M = 1000\)，得到 \(e^{0}\)，\(e^{-1}\)，和 \(e^{-2}\)，所有有意义的非零结果，准确地贡献到总和。
- en: We implement in NumPy.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 NumPy 中实现它。
- en: '[PRE67]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '**NUMERICAL CORNER:** We try it on a simple example.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们在一个简单的例子上尝试它。'
- en: '[PRE68]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We first attempt a direct computation.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先尝试直接计算。
- en: '[PRE69]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Predictly, we get an underflow error and a useless output.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 预计我们会得到一个下溢错误和一个无用的输出。
- en: Next, we try the log-sum-exp trick.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们尝试 log-sum-exp 技巧。
- en: '[PRE72]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This time we get an output which seems reasonable, something slightly larger
    than \(-1000\) as expected (Why?).
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们得到了一个看似合理的输出，略大于预期的 \(-1000\)（为什么？）。
- en: \(\unlhd\)
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: After this long – but important! – parenthesis, we return to the EM algorithm.
    We modify it by implementing the log-sum-exp trick in the subroutine `responsibility`.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个长但重要的括号之后，我们回到 EM 算法。我们通过在子程序 `responsibility` 中实现 log-sum-exp 技巧来修改它。
- en: '[PRE74]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '**NUMERICAL CORNER:** We go back to the MNIST example with only the 2s.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们回到只有 2 的 MNIST 例子。'
- en: '[PRE75]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Here are the parameters for one cluster.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个簇的参数。
- en: '[PRE76]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png](../Images/4e3661e0a1a8051341921aba94079c7b.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png](../Images/4e3661e0a1a8051341921aba94079c7b.png)'
- en: Here is the other one.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个。
- en: '[PRE77]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '![../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png](../Images/372533fd8fad14df01f1cf71749a0141.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png](../Images/372533fd8fad14df01f1cf71749a0141.png)'
- en: Now that the model is trained, we compute the probability that an example image
    is in each cluster. We use the first image in the dataset that we plotted earlier.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练好，我们计算一个示例图像属于每个簇的概率。我们使用我们之前绘制的数据集中的第一个图像。
- en: '[PRE78]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: It indeed identifies the second cluster as significantly more likely.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实识别出第二个簇更有可能。
- en: '**TRY IT!** In the MNIST example, as we have seen, the probabilities involved
    are extremely small and the responsibilities are close to \(0\) or \(1\). Implement
    a variant of EM, called hard EM, which replaces responsibilities with the one-hot
    encoding of the largest responsibility. Test it on the MNIST example again. ([Open
    In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '**TRY IT!** 在MNIST示例中，正如我们所见，涉及的概率非常小，责任接近 \(0\) 或 \(1\)。实现EM的一个变体，称为硬EM，它用最大的责任的一热编码替换责任。再次在MNIST示例上测试它。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
- en: \(\unlhd\)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** The mixture of multivariate Bernoullis model is a simple example
    of a latent variable model. Ask your favorite AI chatbot to discuss more complex
    latent variable models, such as the variational autoencoder or the Gaussian process
    latent variable model, and their applications in unsupervised learning. \(\ddagger\)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 多元伯努利混合模型是潜在变量模型的一个简单例子。请你的喜欢的AI聊天机器人讨论更复杂的潜在变量模型，例如变分自编码器或高斯过程潜在变量模型，以及它们在无监督学习中的应用。
    \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** In the mixture of multivariate Bernoullis model, the joint distribution
    is given by:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在多元伯努利混合模型中，联合分布由以下给出：'
- en: a) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X}
    = \mathbf{x}]\)
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X}
    = \mathbf{x}]\)
- en: b) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
- en: c) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X}
    = \mathbf{x}|C = k] \mathbb{P}[C = k]\)
- en: d) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C =
    k, \mathbf{X} = \mathbf{x}]\)
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C =
    k, \mathbf{X} = \mathbf{x}]\)
- en: '**2** The majorization-minimization principle states that:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 主优-次优原理表明：'
- en: a) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: a) 如果 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处对 \(f\) 进行主优，那么 \(U_{\mathbf{x}}\)
    的全局最小值 \(\mathbf{x}'\) 满足 \(f(\mathbf{x}') \geq f(\mathbf{x})\).
- en: b) If \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: b) 如果 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处对 \(f\) 进行主优，那么 \(U_{\mathbf{x}}\)
    的全局最小值 \(\mathbf{x}'\) 满足 \(f(\mathbf{x}') \leq f(\mathbf{x})\).
- en: c) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \geq f(\mathbf{x})\).
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: c) 如果 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处对 \(f\) 进行次优，那么 \(U_{\mathbf{x}}\)
    的全局最小值 \(\mathbf{x}'\) 满足 \(f(\mathbf{x}') \geq f(\mathbf{x})\).
- en: d) If \(U_{\mathbf{x}}\) minorizes \(f\) at \(\mathbf{x}\), then a global minimum
    \(\mathbf{x}'\) of \(U_{\mathbf{x}}\) satisfies \(f(\mathbf{x}') \leq f(\mathbf{x})\).
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: d) 如果 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\) 处对 \(f\) 进行次优，那么 \(U_{\mathbf{x}}\)
    的全局最小值 \(\mathbf{x}'\) 满足 \(f(\mathbf{x}') \leq f(\mathbf{x})\).
- en: '**3** In the EM algorithm for mixtures of multivariate Bernoullis, the M-step
    involves:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 在多元伯努利混合的EM算法中，M步骤包括：'
- en: a) Updating the parameters \(\pi_k\) and \(p_{k,m}\)
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: a) 更新参数 \(\pi_k\) 和 \(p_{k,m}\)
- en: b) Computing the responsibilities \(r_{k,i}^t\)
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: b) 计算责任 \(r_{k,i}^t\)
- en: c) Minimizing the negative log-likelihood
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: c) 最小化负对数似然
- en: d) Applying the log-sum-exp trick
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: d) 应用log-sum-exp技巧
- en: '**4** The mixture of multivariate Bernoullis model is represented by the following
    graphical model:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 多元伯努利混合模型由以下图模型表示：'
- en: a)
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: a)
- en: '[PRE80]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: b)
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: b)
- en: '[PRE81]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: c)
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: c)
- en: '[PRE82]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: d)
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: d)
- en: '[PRE83]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '**5** In the context of clustering, what is the interpretation of the responsibilities
    computed in the E-step of the EM algorithm?'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在聚类的背景下，EM算法E步骤中计算的责任有什么解释？'
- en: a) They represent the distance of each data point to the cluster centers.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: a) 他们表示每个数据点到聚类中心的距离。
- en: b) They indicate the probability of each data point belonging to each cluster.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它们表示每个数据点属于每个聚类的概率。
- en: c) They determine the optimal number of clusters.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: c) 他们确定最佳聚类数量。
- en: d) They are used to initialize the cluster centers in the M-step.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它们用于在M步骤中初始化聚类中心。
- en: 'Answer for 1: b. Justification: The text states, “\(\mathbb{P}[\mathbf{X} =
    \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K
    \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\).”'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 答案1：b. 理由：文本中提到，“\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C
    = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C
    = k] \mathbb{P}[C = k]\).”
- en: 'Answer for 2: b. Justification: “Let \(f: \mathbb{R}^d \to \mathbb{R}\) and
    suppose \(U_{\mathbf{x}}\) majorizes \(f\) at \(\mathbf{x}\). Let \(\mathbf{x}''\)
    be a global minimizer of \(U_{\mathbf{x}}(\mathbf{z})\) as a function of \(\mathbf{z}\),
    provided it exists. Then \(f(\mathbf{x}'') \leq f(\mathbf{x})\).”'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '答案2：b. 理由：“设 \(f: \mathbb{R}^d \to \mathbb{R}\) 并假设 \(U_{\mathbf{x}}\) 在 \(\mathbf{x}\)
    处主支配 \(f\)。设 \(\mathbf{x}''\) 是 \(U_{\mathbf{x}}(\mathbf{z})\) 作为 \(\mathbf{z}\)
    的函数的全局最小值，如果存在的话。那么 \(f(\mathbf{x}'') \leq f(\mathbf{x})\)。”'
- en: 'Answer for 3: a. Justification: In the summary of the EM algorithm, the M-step
    is described as updating the parameters: “\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\)
    and \(p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}\),” which require the responsibilities.'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 答案3：a. 理由：在 EM 算法的总结中，M 步被描述为更新参数：“\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\) 和 \(p_{k,m}^{t+1}
    = \frac{\eta_{k,m}^t}{\eta_k^t}\)，”这需要责任值。
- en: 'Answer for 4: b. Justification: The text states, “Mathematically, that corresponds
    to applying the law of total probability as we did previously. Further, we let
    the vertex for \(X\) be shaded to indicate that it is observed, while the vertex
    for \(Y\) is not shaded to indicate that it is not.”'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：b. 理由：从数学上讲，这相当于应用我们之前使用的全概率定律。进一步，我们让 \(X\) 的顶点被阴影覆盖以表示它被观察，而 \(Y\) 的顶点没有被阴影覆盖以表示它没有被观察。”
- en: 'Answer for 5: b. Justification: The text refers to responsibilities as “our
    estimate – under the current parameter – of the probability that the sample comes
    from cluster \(k\).”'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：b. 理由：文本中提到的责任是“在当前参数下，我们对样本来自簇 \(k\) 的概率的估计。”
