- en: 2.4\. QR decomposition and Householder transformations#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.4\. QR 分解和 Householder 变换#
- en: 原文：[https://mmids-textbook.github.io/chap02_ls/04_qr/roch-mmids-ls-qr.html](https://mmids-textbook.github.io/chap02_ls/04_qr/roch-mmids-ls-qr.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap02_ls/04_qr/roch-mmids-ls-qr.html](https://mmids-textbook.github.io/chap02_ls/04_qr/roch-mmids-ls-qr.html)
- en: 'We have some business left over from previous sections: constructing orthonormal
    bases. We go over the Gram-Schimidt algorithm below. Through a matrix factorization
    perspective, we give an alternative way to solve the linear least squares problem.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一些来自前几节遗留的业务：构造正交归一基。下面我们将介绍 Gram-Schmidt 算法。通过矩阵分解的视角，我们给出了解决线性最小二乘问题的另一种方法。
- en: 2.4.1\. Matrix form of Gram-Schmidt[#](#matrix-form-of-gram-schmidt "Link to
    this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4.1\. Gram-Schmidt 算法的矩阵形式[#](#matrix-form-of-gram-schmidt "链接到本标题")
- en: In this subsection, we prove the *Gram-Schmidt Theorem* and introduce a fruitful
    matrix perspective.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们证明 *Gram-Schmidt 定理* 并介绍一个有成效的矩阵视角。
- en: '**Gram-Schmidt algorithm** Let \(\mathbf{a}_1,\ldots,\mathbf{a}_m\) be linearly
    independent. We use the Gram-Schmidt algorithm\(\idx{Gram-Schmidt algorithm}\xdi\)
    to obtain an orthonormal basis of \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\).
    The process takes advantage of the properties of the orthogonal projection derived
    above. In essence we add the vectors \(\mathbf{a}_i\) one by one, but only after
    taking out their orthogonal projection on the previously included vectors. The
    outcome spans the same subspace and the *Orthogonal Projection Theorem* ensures
    orthogonality.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gram-Schmidt 算法** 设 \(\mathbf{a}_1,\ldots,\mathbf{a}_m\) 是线性无关的。我们使用 Gram-Schmidt
    算法\(\idx{Gram-Schmidt algorithm}\xdi\)来获得 \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\)
    的正交归一基。这个过程利用了上面推导出的正交投影的性质。本质上，我们逐个添加向量 \(\mathbf{a}_i\)，但在添加之前先取出它们在先前包含的向量上的正交投影。结果生成的空间与之前相同，*正交投影定理*
    确保了正交性。'
- en: '*Proof idea:* *(Gram-Schmidt)* Suppose first that \(m=1\). In that case, all
    that needs to be done is to divide \(\mathbf{a}_1\) by its norm to obtain a unit
    vector whose span is the same as \(\mathbf{a}_1\), that is, we set \(\mathbf{q}_1
    = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* *(Gram-Schmidt)* 首先假设 \(m=1\)。在这种情况下，需要做的只是将 \(\mathbf{a}_1\) 除以其范数，以获得一个单位向量，其生成的空间与
    \(\mathbf{a}_1\) 相同，即我们设 \(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)。'
- en: 'Suppose now that \(m=2\). We first let \(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)
    as in the previous case. Then we subtract from \(\mathbf{a}_2\) its projection
    on \(\mathbf{q}_1\), that is, we set \(\mathbf{v}_2 = \mathbf{a}_2 - \langle \mathbf{q}_1,
    \mathbf{a}_2 \rangle \,\mathbf{q}_1\). It is easily checked that \(\mathbf{v}_2\)
    is orthogonal to \(\mathbf{q}_1\) (see the proof of the *Orthogonal Projection
    Theorem* for a similar calculation). Moreover, because \(\mathbf{a}_2\) is a linear
    combination of \(\mathbf{q}_1\) and \(\mathbf{v}_2\), we have \(\mathrm{span}(\mathbf{q}_1,\mathbf{v}_2)
    = \mathrm{span}(\mathbf{a}_1,\mathbf{a}_2)\). It remains to divide by the norm
    of the resulting vector: \(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|}\).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现在 \(m=2\)。我们首先让 \(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)，就像前一种情况一样。然后我们从
    \(\mathbf{a}_2\) 中减去其在 \(\mathbf{q}_1\) 上的投影，即我们设 \(\mathbf{v}_2 = \mathbf{a}_2
    - \langle \mathbf{q}_1, \mathbf{a}_2 \rangle \,\mathbf{q}_1\)。很容易验证 \(\mathbf{v}_2\)
    与 \(\mathbf{q}_1\) 正交（参见 *正交投影定理* 的证明中类似的计算）。此外，因为 \(\mathbf{a}_2\) 是 \(\mathbf{q}_1\)
    和 \(\mathbf{v}_2\) 的线性组合，所以我们有 \(\mathrm{span}(\mathbf{q}_1,\mathbf{v}_2) = \mathrm{span}(\mathbf{a}_1,\mathbf{a}_2)\)。剩下的是除以结果向量的范数：\(\mathbf{q}_2
    = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|}\)。
- en: For general \(m\), we proceed similarly but project onto the subspace spanned
    by the previously added vectors at each step.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的 \(m\)，我们按类似的方式进行，但在每一步都投影到之前添加的向量的子空间上。
- en: '![Gram–Schmidt process (with help from ChatGPT; inspired by Source)](../Images/357bfa05be44b654c5858a352b39044e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Gram–Schmidt 过程（得益于 ChatGPT；受来源启发）](../Images/357bfa05be44b654c5858a352b39044e.png)'
- en: '*Proof:* *(Gram-Schmidt)* The first step of the induction is described above.
    Then the general inductive step is the following. Assume that we have constructed
    orthonormal vectors \(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}\) such that'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(Gram-Schmidt)* 归纳的第一步已在上面描述。然后一般的归纳步骤如下。假设我们已经构造了正交归一向量 \(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}\)，使得'
- en: \[ U_{j-1} := \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}).
    \]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_{j-1} := \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}).
    \]
- en: '*Constructing \(\mathbf{q}_j\):* By the *Properties of Orthonormal Lists*,
    \(\{\mathbf{q}\}_{i=1}^{j-1}\) is an independent list and therefore forms an orthonormal
    basis for \(U_{j-1}\). So we can compute the orthogonal projection of \(\mathbf{a}_j\)
    on \(U_{j-1}\) as'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*构造 \(\mathbf{q}_j\):* 根据正交归一列表的性质，\(\{\mathbf{q}\}_{i=1}^{j-1}\) 是一个独立的列表，因此它形成了
    \(U_{j-1}\) 的正交归一基。因此，我们可以计算 \(\mathbf{a}_j\) 在 \(U_{j-1}\) 上的正交投影。'
- en: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i,
    \]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i,
    \]
- en: where we defined \(r_{ij} = \langle \mathbf{q}_i , \mathbf{a}_j\rangle\). And
    we set
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(r_{ij} = \langle \mathbf{q}_i , \mathbf{a}_j\rangle\)。并且我们设定
- en: \[ \mathbf{v}_j = \mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \mathbf{a}_j
    - \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i \quad \text{and} \quad \mathbf{q}_j =
    \frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}. \]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_j = \mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \mathbf{a}_j
    - \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i \quad \text{and} \quad \mathbf{q}_j =
    \frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}. \]
- en: 'The last step is possible because:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步之所以可能，是因为：
- en: '**LEMMA** \(\|\mathbf{v}_j\| > 0\). \(\flat\)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** \(\|\mathbf{v}_j\| > 0\). \(\flat\)'
- en: '*Proof:* Indeed otherwise \(\mathbf{a}_j\) would be equal to its projection
    \(\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)
    which would contradict linear independence of the \(\mathbf{a}_k\)’s. \(\square\)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 确实，否则 \(\mathbf{a}_j\) 将等于其投影 \(\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in
    \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)，这将与 \(\mathbf{a}_k\) 的线性无关性相矛盾。\(\square\)'
- en: The vector \(\mathbf{q}_j\) is of unit norm by construction. It is also orthogonal
    to \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})\) by the definition of
    \(\mathbf{v}_j\) and the *Orthogonal Projection Theorem*. So \(\mathbf{q}_1,\ldots,\mathbf{q}_j\)
    form an orthonormal list.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 \(\mathbf{q}_j\) 通过构造具有单位范数。根据 \(\mathbf{v}_j\) 的定义和正交投影定理，它也垂直于 \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})\)。因此，\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)
    形成一个正交归一列表。
- en: '*Pushing the induction through:* It remains to prove that \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)
    = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\). Because by induction \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})
    = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\), all we have to prove
    are the following two claims.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过归纳法推进：* 剩下的工作是要证明 \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\)。因为通过归纳
    \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)，我们只需要证明以下两个命题。'
- en: '**LEMMA** \(\mathbf{q}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\).
    \(\flat\)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** \(\mathbf{q}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\).
    \(\flat\)'
- en: '*Proof:* By construction,'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 通过构造，'
- en: \[ \mathbf{q}_j = \frac{1}{\|\mathbf{v}_j\|} \left\{\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\right\}
    = \frac{1}{\|\mathbf{v}_j\|} \mathbf{a}_j + \frac{1}{\|\mathbf{v}_j\|} \mathrm{proj}_{U_{j-1}}\mathbf{a}_j.
    \]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_j = \frac{1}{\|\mathbf{v}_j\|} \left\{\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\right\}
    = \frac{1}{\|\mathbf{v}_j\|} \mathbf{a}_j + \frac{1}{\|\mathbf{v}_j\|} \mathrm{proj}_{U_{j-1}}\mathbf{a}_j.
    \]
- en: By definition of the orthogonal projection,
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据正交投影的定义，
- en: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in U_{j-1}= \mathrm{span} (\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})
    \subseteq \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j}). \]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in U_{j-1}= \mathrm{span} (\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})
    \subseteq \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j}). \]
- en: Hence we have written \(\mathbf{q}_j\) as a linear combination of vectors in
    \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\). That proves the claim.
    \(\square\)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将 \(\mathbf{q}_j\) 写成了 \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)
    中向量的线性组合。这证明了命题。\(\square\)
- en: '**LEMMA** \(\mathbf{a}_j \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)\).
    \(\flat\)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** \(\mathbf{a}_j \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)\).
    \(\flat\)'
- en: '*Proof:* Unrolling the calculations above, \(\mathbf{a}_j\) can be re-written
    as the following linear combination of \(\mathbf{q}_1,\ldots,\mathbf{q}_j\)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 展开上述计算，\(\mathbf{a}_j\) 可以重新表示为以下 \(\mathbf{q}_1,\ldots,\mathbf{q}_j\)
    的线性组合'
- en: \[\begin{align*} \mathbf{a}_j &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \mathbf{v}_j\\
    &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{v}_j\| \mathbf{q}_j\\ &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j
    + \|\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\| \mathbf{q}_j\\ &= \sum_{i=1}^{j-1}
    r_{ij} \,\mathbf{q}_i + \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    \,\mathbf{q}_j\\ &= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \end{align*}\]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{a}_j &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \mathbf{v}_j\\
    &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{v}_j\| \mathbf{q}_j\\ &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j
    + \|\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\| \mathbf{q}_j\\ &= \sum_{i=1}^{j-1}
    r_{ij} \,\mathbf{q}_i + \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    \,\mathbf{q}_j\\ &= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \end{align*}\]
- en: where we defined \(r_{jj} = \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    = \|\mathbf{v}_j\|\). \(\square\)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(r_{jj} = \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    = \|\mathbf{v}_j\|\)。 \(\square\)
- en: Hence \(\mathbf{q}_1,\ldots,\mathbf{q}_j\) forms an orthonormal list with \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\).
    So induction goes through. That concludes the proof of the theorem. \(\square\)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 \(\mathbf{q}_1,\ldots,\mathbf{q}_j\) 形成一个与 \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)
    正交的列表。因此，归纳成立。这就完成了定理的证明。 \(\square\)
- en: We implement the Gram-Schmidt algorithm in Python. For reasons that will become
    clear in the next subsection, we output both the \(\mathbf{q}_j\)’s and \(r_{ij}\)’s,
    each in matrix form. Here we use [`numpy.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)
    to compute inner products.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现了 Gram-Schmidt 算法。由于原因将在下一小节中变得清晰，我们以矩阵形式输出 \(\mathbf{q}_j\) 和
    \(r_{ij}\)，每个都是矩阵形式。在这里，我们使用 `numpy.dot` 来计算内积。
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**NUMERICAL CORNER:** Let’s try a simple example.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角** 让我们尝试一个简单的例子。'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: \(\unlhd\)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Matrix form** Let \(\mathbf{a}_1,\ldots,\mathbf{a}_m \in \mathbb{R}^n\) be
    linearly independent. Above, we presented the Gram-Schmidt algorithm to obtain
    an orthonormal basis of \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\). We
    revisit it in matrix form.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵形式** 设 \(\mathbf{a}_1,\ldots,\mathbf{a}_m \in \mathbb{R}^n\) 是线性无关的。在上面，我们介绍了
    Gram-Schmidt 算法来获得 \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\) 的正交基。我们以矩阵形式重新审视它。'
- en: Let
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad Q = \begin{pmatrix} | & & | \\
    \mathbf{q}_1 & \ldots & \mathbf{q}_m \\ | & & | \end{pmatrix}. \end{split}\]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{和} \quad Q = \begin{pmatrix} | & & | \\ \mathbf{q}_1
    & \ldots & \mathbf{q}_m \\ | & & | \end{pmatrix}. \end{split}\]
- en: Recalling that, for all \(j\),
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，对于所有 \(j\)，
- en: \[ \mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \]
- en: the output of the Gram-Schmidt algorithm can be written in the following compact
    form, known as a [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition)\(\idx{QR
    decomposition}\xdi\),
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Gram-Schmidt 算法的输出可以写成以下紧凑形式，称为 [QR 分解](https://en.wikipedia.org/wiki/QR_decomposition)\(\idx{QR
    decomposition}\xdi\)，
- en: \[ A = QR \]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = QR \]
- en: where column \(i\) of the \(m \times m\) matrix \(R\) contains the coefficients
    of the linear combination of the \(\mathbf{q}_j\)’s that produce \(\mathbf{a}_i\).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(R\) 的第 \(i\) 列包含 \(\mathbf{q}_j\) 的线性组合系数，该组合产生 \(\mathbf{a}_i\)。
- en: 'By the proof of the *Gram-Schmidt Theorem*, \(\mathbf{a}_i \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_i)\).
    So column \(i\) of \(R\) has only zeros below the diagonal. Hence \(R\) has a
    special structure we have previously encountered: it is upper triangular. The
    proof also established that the diagonal elements of \(R\) are strictly positive.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 **Gram-Schmidt 定理** 的证明，\(\mathbf{a}_i \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_i)\)。因此，\(R\)
    的第 \(i\) 列在对角线以下只有零。因此，\(R\) 具有我们之前遇到过的特殊结构：它是上三角矩阵。证明还确立了 \(R\) 的对角线元素是严格正的。
- en: '**DEFINITION** **(Triangular matrix)** \(\idx{triangular matrix}\xdi\) A matrix
    \(R = (r_{ij})_{i,j} \in \mathbb{R}^{n \times m}\) is upper-triangular if all
    entries below the diagonal are zero, that is, if \(i > j\) implies \(r_{ij} =
    0\). Similarly, a lower-triangular matrix has zeros above the diagonal. \(\natural\)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **（三角矩阵）** \(\idx{triangular matrix}\xdi\) 矩阵 \(R = (r_{ij})_{i,j} \in
    \mathbb{R}^{n \times m}\) 是上三角矩阵，如果对角线以下的全部项都是零，即如果 \(i > j\) 则 \(r_{ij} = 0\)。类似地，下三角矩阵的对角线上方为零。
    \(\natural\)'
- en: An upper-triangular matrix looks like this
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上三角矩阵看起来是这样的
- en: \[\begin{split} R = \begin{bmatrix} r_{1,1} & r_{1,2} & r_{1,3} & \ldots & r_{1,n}
    \\ 0 & r_{2,2} & r_{2,3} & \ldots & r_{2,n} \\ & 0 & \ddots & \ddots & \vdots
    \\ & & \ddots & \ddots & r_{n-1,n} \\ 0 & & & 0 & r_{n,n} \end{bmatrix}. \end{split}\]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} R = \begin{bmatrix} r_{1,1} & r_{1,2} & r_{1,3} & \ldots & r_{1,n}
    \\ 0 & r_{2,2} & r_{2,3} & \ldots & r_{2,n} \\ & 0 & \ddots & \ddots & \vdots
    \\ & & \ddots & \ddots & r_{n-1,n} \\ 0 & & & 0 & r_{n,n} \end{bmatrix}. \end{split}\]
- en: '**Remarks:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**备注：**'
- en: a) If the input vectors \(\mathbf{a}_1,\ldots,\mathbf{a}_m\) are not linearly
    independent (in which case we say that the matrix \(A\) is rank-deficient), the
    Gram-Schmidt algorithm will fail. Indeed, at some point we will have that \(\mathbf{a}_j
    \in U_{j-1}\) and the normalization of \(\mathbf{v}_j\) will not be possible.
    In that case, one can instead use a technique called [column pivoting](https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting),
    which we will not describe.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: a) 如果输入向量 \(\mathbf{a}_1,\ldots,\mathbf{a}_m\) 不是线性无关的（在这种情况下，我们说矩阵 \(A\) 是秩亏的），则克莱姆-施密特算法将失败。确实，在某个时刻我们将有
    \(\mathbf{a}_j \in U_{j-1}\)，并且 \(\mathbf{v}_j\) 的归一化将不可能。在这种情况下，可以使用一种称为[列主元](https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting)的技术，我们将在后面描述。
- en: b) The QR decomposition we have derived here is technically called a reduced
    QR decomposition. In a full QR decomposition\(\idx{full QR decomposition}\xdi\),
    the matrix \(Q\) is square and orthogonal. In other words, the columns of such
    a \(Q\) form an orthonormal basis of the full space \(\mathbb{R}^n\). Let \(A
    = Q_1 R_1\) be a reduced QR decomposition, as obtained through the Gram-Schmidt
    algorithm. Then the columns of \(Q_1\) form an orthonormal basis of \(\mathrm{col}(A)\)
    and can be completed into an orthonormal basis of \(\mathbb{R}^n\) by adding further
    vectors \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\). Let \(Q_2\) be the matrix
    with columns \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\). Then a full QR decomposition
    of \(A\) is
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: b) 我们在这里推导出的 QR 分解在技术上称为简化 QR 分解。在完全 QR 分解\(\idx{full QR decomposition}\xdi\)
    中，矩阵 \(Q\) 是方阵且正交。换句话说，这样的 \(Q\) 的列构成了 \(\mathbb{R}^n\) 的完整空间的正交基。设 \(A = Q_1
    R_1\) 是通过克莱姆-施密特算法获得的简化 QR 分解。那么 \(Q_1\) 的列构成了 \(\mathrm{col}(A)\) 的正交基，并且可以通过添加更多的向量
    \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\) 来完成 \(\mathbb{R}^n\) 的正交基。设 \(Q_2\)
    是具有列 \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\) 的矩阵。那么 \(A\) 的完全 QR 分解是
- en: \[\begin{split} Q = \begin{pmatrix} Q_1 & Q_2 \end{pmatrix} \qquad R = \begin{pmatrix}
    R_1\\ \mathbf{0}_{(n-m)\times m} \end{pmatrix} \end{split}\]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} Q = \begin{pmatrix} Q_1 & Q_2 \end{pmatrix} \qquad R = \begin{pmatrix}
    R_1\\ \mathbf{0}_{(n-m)\times m} \end{pmatrix} \end{split}\]
- en: where \(\mathbf{0}_{(n-m)\times m}\) is the all-zero matrix of size \((n-m)\times
    m\). A numerical method for computing a full QR decomposition is presented in
    a later subsection.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{0}_{(n-m)\times m}\) 是大小为 \((n-m)\times m\) 的全零矩阵。在后面的子节中，将介绍计算完全
    QR 分解的数值方法。
- en: c) The Gram-Schmidt algorithm is appealing geometrically, but it is known to
    have numerical issues. Other methods exist for computing QR decompositions with
    better numerical properties. We discuss such a method in a later subsection. (See
    that same subsection for an example where the \(\mathbf{q}_j\)’s produced by Gram-Schmidt
    are far from orthogonal.)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: c) 克莱姆-施密特算法在几何上很有吸引力，但众所周知，它存在数值问题。存在其他方法可以计算具有更好数值特性的 QR 分解。我们将在后面的子节中讨论这样的方法。（参见该子节中关于由克莱姆-施密特算法产生的
    \(\mathbf{q}_j\) 远非正交的例子。）
- en: 2.4.2\. Least squares via QR[#](#least-squares-via-qr "Link to this heading")
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4.2\. 通过 QR 进行最小二乘法[#](#least-squares-via-qr "链接到本标题")
- en: Let \(A \in \mathbb{R}^{n\times m}\) be an \(n\times m\) matrix with linearly
    independent columns and let \(\mathbf{b} \in \mathbb{R}^n\) be a vector. Recall
    that a solution \(\mathbf{x}^*\) to the linear least squares problem
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(A \in \mathbb{R}^{n\times m}\) 是一个 \(n\times m\) 的矩阵，其列线性无关，并且设 \(\mathbf{b}
    \in \mathbb{R}^n\) 是一个向量。回忆一下，线性最小二乘问题的解 \(\mathbf{x}^*\) 是
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2 \]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2 \]
- en: satisfies the normal equations
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 满足正则方程
- en: \[ A^T A \mathbf{x}^* = A^T \mathbf{b}. \]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{x}^* = A^T \mathbf{b}. \]
- en: '**Solving the normal equations** In a first linear algebra course, one learns
    how to solve linear systems such as the normal equations. For this task a common
    approach is Gaussian elimination, or row reduction. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_elimination):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**求解正则方程** 在第一门线性代数课程中，人们学习如何求解如正则方程之类的线性系统。为此，一个常见的方法是高斯消元法或行简化。引用 [维基百科](https://en.wikipedia.org/wiki/Gaussian_elimination)：'
- en: To perform row reduction on a matrix, one uses a sequence of elementary row
    operations to modify the matrix until the lower left-hand corner of the matrix
    is filled with zeros, as much as possible. […] Once all of the leading coefficients
    (the leftmost nonzero entry in each row) are 1, and every column containing a
    leading coefficient has zeros elsewhere, the matrix is said to be in reduced row
    echelon form. […] The process of row reduction […] can be divided into two parts.
    The first part (sometimes called forward elimination) reduces a given system to
    row echelon form, from which one can tell whether there are no solutions, a unique
    solution, or infinitely many solutions. The second part (sometimes called back
    substitution) continues to use row operations until the solution is found; in
    other words, it puts the matrix into reduced row echelon form.
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要对一个矩阵进行行简化，可以使用一系列初等行操作来修改矩阵，直到尽可能多地填充矩阵的左下角为零。 [...] 一旦所有主系数（每行的最左边的非零项）都是1，并且包含主系数的每一列在其他地方都是零，则该矩阵被称为简化行阶梯形。
    [...] 行简化 [...] 可以分为两部分。第一部分（有时称为前向消元）将给定的系统简化为行阶梯形，从其中可以判断是否存在无解、唯一解或无限多解。第二部分（有时称为回代）继续使用行操作直到找到解；换句话说，它将矩阵置于简化行阶梯形。
- en: '**Figure:** An example of Gaussian elimination ([Source](https://en.wikipedia.org/wiki/Gaussian_elimination))'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：高斯消元法的示例 ([来源](https://en.wikipedia.org/wiki/Gaussian_elimination))'
- en: '![Gaussian elimination](../Images/dc18b77fc722fe6abe5812c927848a72.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![高斯消元法](../Images/dc18b77fc722fe6abe5812c927848a72.png)'
- en: \(\bowtie\)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: We will not go over Gaussian elimination here. In this subsection, we develop
    an alternative approach to solving the normal equations using the QR decomposition.
    We will need one component of Gaussian elimination, back substitution\(\idx{back
    substitution}\xdi\). It is based on the observation that triangular systems of
    equations are straightforward to solve. We start with an example.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会介绍高斯消元法。在本小节中，我们将通过QR分解开发求解正则方程的另一种方法。我们需要高斯消元法的一个组成部分，回代\(\idx{back
    substitution}\xdi\)。它基于观察，三角方程组是容易解决的。我们从例子开始。
- en: '**EXAMPLE:** Here is a concrete example of back substitution. Consider the
    system \(R \mathbf{x} = \mathbf{b}\) with'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**以下是一个回代的具体例子。考虑方程组 \(R \mathbf{x} = \mathbf{b}\) 与'
- en: \[\begin{split} R = \begin{pmatrix} 2 & -1 & 2\\ 0 & 1 & 1\\ 0 & 0 & 2 \end{pmatrix}
    \qquad \mathbf{b} = \begin{pmatrix} 0\\ -2\\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} R = \begin{pmatrix} 2 & -1 & 2\\ 0 & 1 & 1\\ 0 & 0 & 2 \end{pmatrix}
    \qquad \mathbf{b} = \begin{pmatrix} 0\\ -2\\ 0 \end{pmatrix}. \end{split}\]
- en: That corresponds to the linear equations
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这对应于线性方程
- en: \[\begin{align*} &2 x_1 - x_2 + 2x_3 = 0\\ &x_2 + x_3 = -2\\ &2 x_3 = 0 \end{align*}\]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &2 x_1 - x_2 + 2x_3 = 0\\ &x_2 + x_3 = -2\\ &2 x_3 = 0 \end{align*}\]
- en: The third equation gives \(x_3 = 0/2 = 0\). Plugging into the second one, we
    get \(x_2 = -2 - x_3 = -2\). Plugging into the first one, we finally have \(x_1
    = (x_2 - 2 x_3)/2 = -1\). So the solution is \(\mathbf{x} = (-1,-2,0)\). \(\lhd\)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个方程给出 \(x_3 = 0/2 = 0\)。将其代入第二个方程，我们得到 \(x_2 = -2 - x_3 = -2\)。将其代入第一个方程，我们最终得到
    \(x_1 = (x_2 - 2 x_3)/2 = -1\)。因此，解为 \(\mathbf{x} = (-1,-2,0)\)。 \(\lhd\)
- en: In general, solving a triangular system of equations works as follows. Let \(R
    = (r_{i,j})_{i,j} \in \mathbb{R}^{m \times m}\) be upper-triangular and let \(\mathbf{b}
    \in \mathbb{R}^m\) be the left-hand vector, i.e., we want to solve the system
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解三角方程组的工作方式如下。设 \(R = (r_{i,j})_{i,j} \in \mathbb{R}^{m \times m}\) 为上三角矩阵，设
    \(\mathbf{b} \in \mathbb{R}^m\) 为左侧向量，即我们想要解的方程组
- en: \[ R \mathbf{x} = \mathbf{b}. \]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: \[ R \mathbf{x} = \mathbf{b}. \]
- en: Starting from the last row of the system, \(r_{m,m} x_m = b_m\) or \(x_m = b_m/r_{m,m}\),
    assuming that \(r_{m,m} \neq 0\). Moving to the second-to-last row, \(r_{m-1,m-1}
    x_{m-1} + r_{m-1,m} x_m = b_{m-1}\) or \(x_{m-1} = (b_{m-1} - r_{m-1,m} x_m)/r_{m-1,m-1}\),
    assuming that \(r_{m-1,m-1} \neq 0\). And so on. This procedure is known as [back
    substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统的最后一行开始，\(r_{m,m} x_m = b_m\) 或 \(x_m = b_m/r_{m,m}\)，假设 \(r_{m,m} \neq 0\)。移动到倒数第二行，\(r_{m-1,m-1}
    x_{m-1} + r_{m-1,m} x_m = b_{m-1}\) 或 \(x_{m-1} = (b_{m-1} - r_{m-1,m} x_m)/r_{m-1,m-1}\)，假设
    \(r_{m-1,m-1} \neq 0\)。依此类推。这个过程被称为[回代](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution)。
- en: Analogously, in the lower triangular case \(L \in \mathbb{R}^{m \times m}\),
    we have [forward substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution)\(\idx{forward
    substitution}\xdi\). These procedures implicitly define an inverse for \(R\) and
    \(L\) *when the diagonal elements are all non-zero*. We will not write them down
    explicitly here.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在下三角情况 \(L \in \mathbb{R}^{m \times m}\) 中，我们有 [前向替换](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution)\(\idx{forward
    substitution}\xdi\)。这些过程隐式地定义了 \(R\) 和 \(L\) 的逆（当对角元素全不为零时）。我们在这里不会明确写出。
- en: We implement back substitution in Python. In our naive implementation, we assume
    that the diagonal entries are not zero, which will suffice for our purposes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现了回代。在我们的原始实现中，我们假设对角元素不为零，这对我们的目的来说足够了。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Forward substitution is implemented similarly.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前向替换以类似的方式实现。
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Using QR** We show how to solve the normal equations via the QR decomposition.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 QR** 我们展示了如何通过 QR 分解求解正则方程。'
- en: 1- Construct an orthonormal basis of \(\mathrm{col}(A)\) through a QR decomposition
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 通过 QR 分解构建 \(\mathrm{col}(A)\) 的一个正交基
- en: \[ A = QR. \]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = QR. \]
- en: 2- Form the orthogonal projection matrix
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 通过 QR 分解形成一个 \(\mathrm{col}(A)\) 的正交基
- en: \[ P = Q Q^T. \]
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P = Q Q^T. \]
- en: 3- Apply the projection to \(\mathbf{b}\) and observe that, by the proof of
    the *Normal Equations*, \(\mathbf{x}^*\) satisfies
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 3- 将投影应用于 \(\mathbf{b}\)，并观察到，根据 *正则方程* 的证明，\(\mathbf{x}^*\) 满足
- en: \[ A \mathbf{x}^* = Q Q^T \mathbf{b}. \]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{x}^* = Q Q^T \mathbf{b}. \]
- en: 4- Plug in the QR decomposition for \(A\) to get
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 4- 将 \(A\) 的 QR 分解代入以获得
- en: \[ QR \mathbf{x}^* = Q Q^T \mathbf{b}. \]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: \[ QR \mathbf{x}^* = Q Q^T \mathbf{b}. \]
- en: 5- Multiply both sides by \(Q^T\) and use \(Q^T Q = I_{m \times m}\)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 5- 两边乘以 \(Q^T\) 并使用 \(Q^T Q = I_{m \times m}\)
- en: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
- en: 6- Solving this system for \(\mathbf{x}^*\) is straightforward because \(R\)
    is upper triangular via back substitution.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 6- 解这个系统以获得 \(\mathbf{x}^*\) 是直接的，因为通过回代 \(R\) 是上三角矩阵。
- en: '**THEOREM** **(Least Squares via QR)** \(\idx{least squares via QR}\xdi\) Let
    \(A \in \mathbb{R}^{n\times m}\) be an \(n\times m\) matrix with linearly independent
    columns, let \(\mathbf{b} \in \mathbb{R}^n\) be a vector, and let \(A = QR\) be
    a QR decomposition of \(A\). The solution to the linear least-squares problem'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(通过 QR 的最小二乘法)** \(\idx{least squares via QR}\xdi\) 设 \(A \in \mathbb{R}^{n\times
    m}\) 是一个 \(n\times m\) 矩阵，其列线性无关，设 \(\mathbf{b} \in \mathbb{R}^n\) 是一个向量，设 \(A
    = QR\) 是 \(A\) 的 QR 分解。线性最小二乘问题的解'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2. \]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2. \]
- en: satisfies
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 满足
- en: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
- en: \(\sharp\)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Note that, in reality, we do not need to form the matrix \(Q Q^T\).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在现实中，我们不需要形成矩阵 \(Q Q^T\)。
- en: We implement the QR approach to least squares.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了最小二乘法的 QR 方法。
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**NUMERICAL CORNER:** We return to our simple overdetermined system example.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们回到我们简单的超定系统示例。'
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: \(\unlhd\)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 2.4.3\. Householder transformations[#](#householder-transformations "Link to
    this heading")
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4.3\. Householder 变换[#](#householder-transformations "链接到这个标题")
- en: 'While the Gram-Schmidt algorithm gives a natural way to compute a (reduced)
    QR decomposition, there are many other numerical algorithms for this purpose.
    Some have better numerical behavior, specifically in terms of how they handle
    roundoff error. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Round-off_error):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然Gram-Schmidt算法给出了计算（简化）QR分解的自然方法，但还有许多其他用于此目的的数值算法。其中一些具有更好的数值行为，特别是在处理舍入误差方面。引用
    [维基百科](https://en.wikipedia.org/wiki/Round-off_error):'
- en: A roundoff error, also called rounding error, is the difference between the
    result produced by a given algorithm using exact arithmetic and the result produced
    by the same algorithm using finite-precision, rounded arithmetic. Rounding errors
    are due to inexactness in the representation of real numbers and the arithmetic
    operations done with them. […] When a sequence of calculations with an input involving
    roundoff error are made, errors may accumulate, sometimes dominating the calculation.
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 舍入误差，也称为舍入误差，是使用精确算术和有限精度舍入算术产生的相同算法的结果之间的差异。舍入误差是由于实数表示的不精确性和使用它们的算术运算造成的。
    [...] 当进行涉及舍入误差的输入的连续计算时，错误可能会累积，有时会主导计算。
- en: We will not prove this here, but the following method based on Householder reflections
    is numerically more [stable](https://en.wikipedia.org/wiki/Numerical_stability#Stability_in_numerical_linear_algebra).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不证明这一点，但以下基于豪斯霍尔德反射的方法在数值上更[稳定](https://en.wikipedia.org/wiki/Numerical_stability#Stability_in_numerical_linear_algebra)。
- en: Recall that a square matrix \(Q \in \mathbb{R}^{m\times m}\) is orthogonal if
    \(Q^T Q = Q Q^T = I_{m \times m}\). In words, the matrix inverse of \(Q\) is its
    transpose. This is equivalent to the columns of \(Q\) forming an orthonormal basis
    of \(\mathbb{R}^m\) (why?).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，一个方阵 \(Q \in \mathbb{R}^{m\times m}\) 是正交的，如果 \(Q^T Q = Q Q^T = I_{m \times
    m}\)。换句话说，\(Q\) 的矩阵逆是其转置。这等价于 \(Q\) 的列形成 \(\mathbb{R}^m\) 的一个正交基（为什么？）。
- en: It can be shown that the product of two orthogonal matrices \(Q_1\) and \(Q_2\)
    is also orthogonal. (Try it!)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明两个正交矩阵 \(Q_1\) 和 \(Q_2\) 的乘积也是正交的。（试一试！）
- en: 'An important property of orthogonal matrices is that they preserve inner products:
    if \(Q \in \mathbb{R}^{m\times m}\) is orthogonal, then for any \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^m\)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正交矩阵的一个重要性质是它们保持内积：如果 \(Q \in \mathbb{R}^{m\times m}\) 是正交的，那么对于任何 \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^m\)
- en: \[ \langle Q \mathbf{x}, Q \mathbf{y} \rangle = (Q \mathbf{x})^T Q \mathbf{y}
    = \mathbf{x}^T Q^T Q \mathbf{y} = \mathbf{x}^T \mathbf{y} = \langle \mathbf{x},
    \mathbf{y} \rangle. \]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle Q \mathbf{x}, Q \mathbf{y} \rangle = (Q \mathbf{x})^T Q \mathbf{y}
    = \mathbf{x}^T Q^T Q \mathbf{y} = \mathbf{x}^T \mathbf{y} = \langle \mathbf{x},
    \mathbf{y} \rangle. \]
- en: In particular, orthogonal matrices preserve norms and angles.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，正交矩阵保持范数和角度。
- en: '**Reflections** One such family of transformations are reflections.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**反射** 一类这样的变换是反射。'
- en: '**DEFINITION** **(Hyperplane)** \(\idx{hyperplane}\xdi\) A hyperplane \(W\)
    is a linear subspace of \(\mathbb{R}^m\) of dimension \(m-1\). \(\natural\)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(超平面)** \(\idx{hyperplane}\xdi\) 超平面 \(W\) 是 \(\mathbb{R}^m\) 的一个维度为
    \(m-1\) 的线性子空间。\(\natural\)'
- en: '**DEFINITION** **(Householder Reflection)** \(\idx{Householder reflection}\xdi\)
    Let \(\mathbf{z} \in \mathbb{R}^m\) be a unit vector and let \(W\) be the hyperplane
    orthogonal to it. The reflection across \(W\) is given by'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(豪斯霍尔德反射)** \(\idx{Householder reflection}\xdi\) 设 \(\mathbf{z} \in
    \mathbb{R}^m\) 为一个单位向量，设 \(W\) 为与它正交的超平面。\(W\) 上的反射由以下公式给出'
- en: \[ H = I_{m \times m} - 2 \mathbf{z} \mathbf{z}^T. \]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H = I_{m \times m} - 2 \mathbf{z} \mathbf{z}^T. \]
- en: This is referred to as a Householder reflection. \(\natural\)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为豪斯霍尔德反射。\(\natural\)
- en: In words, we subtract twice the projection onto \(\mathbf{z}\), as depicted
    below.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们减去两次在 \(\mathbf{z}\) 上的投影，如图所示。
- en: '![Householder reflection. (with the help of Claude; inspired by (Source))](../Images/7c283ce7dd76917f07050963bca3ee2b.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![豪斯霍尔德反射。 (在克劳德的帮助下；受 (来源) 启发)](../Images/7c283ce7dd76917f07050963bca3ee2b.png)'
- en: '**LEMMA** Let \(H = I_{m\times m} - 2\mathbf{z}\mathbf{z}^T\) be a Householder
    reflection. Then \(H\) is an orthogonal matrix. \(\flat\)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** 设 \(H = I_{m\times m} - 2\mathbf{z}\mathbf{z}^T\) 为一个豪斯霍尔德反射。那么 \(H\)
    是一个正交矩阵。\(\flat\)'
- en: '*Proof:* We check the definition:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们检查定义：'
- en: \[\begin{align*} H^T H &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)^T (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T) (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T
    + 4 \mathbf{z}\mathbf{z}^T\mathbf{z}\mathbf{z}^T\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T
    - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T \end{align*}\]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} H^T H &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)^T (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T) (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T
    + 4 \mathbf{z}\mathbf{z}^T\mathbf{z}\mathbf{z}^T\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T
    - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T \end{align*}\]
- en: which is equal to \(I_{m\times m}\). The calculation for \(H H^T\) is the same.\(\square\)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这等于 \(I_{m\times m}\)。\(H H^T\) 的计算是相同的。\(\square\)
- en: '**QR decomposition by introducing zeros** We return to QR decompositions. One
    way to construct a (full) QR decomposition of a matrix \(A \in \mathbb{R}^{n \times
    m}\) is to find a sequence of orthogonal matrices \(H_1, \ldots, H_m\) that triangularize
    \(A\):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过引入零的 QR 分解** 我们回到 QR 分解。构造矩阵 \(A \in \mathbb{R}^{n \times m}\) 的 (满) QR
    分解的一种方法是通过找到一个正交矩阵序列 \(H_1, \ldots, H_m\)，使得 \(A\) 三角化：'
- en: \[ H_m \cdots H_2 H_1 A = R \]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_m \cdots H_2 H_1 A = R \]
- en: for an upper-triangular matrix \(R\). Indeed, by the properties of orthogonal
    matrices, we then have
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个上三角矩阵 \(R\)。实际上，根据正交矩阵的性质，我们随后有
- en: \[ A = H_1^T H_2^T \cdots H_m^T H_m \cdots H_2 H_1 A = H_1^T H_2^T \cdots H_m^T
    R \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = H_1^T H_2^T \cdots H_m^T H_m \cdots H_2 H_1 A = H_1^T H_2^T \cdots H_m^T
    R \]
- en: 'where \(Q = H_1^T H_2^T \cdots H_m^T\) is itself orthogonal as a product of
    orthogonal matrices. So to proceed we need to identify orthogonal matrices that
    have the effect of introducing zeros below the diagonal, as illustrated below:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(Q = H_1^T H_2^T \cdots H_m^T\) 本身作为正交矩阵的乘积也是正交的。因此，为了继续进行，我们需要识别出那些能够引入对角线下方零的正交矩阵，如图所示：
- en: \[\begin{split} H_2 H_1 A = \begin{pmatrix} \times & \times & \times & \times
    & \times\\ 0 & \times & \times & \times & \times\\ 0 & 0 & \times & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ 0 & 0 & \boxed{\times} & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ \end{pmatrix}. \end{split}\]
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_2 H_1 A = \begin{pmatrix} \times & \times & \times & \times
    & \times\\ 0 & \times & \times & \times & \times\\ 0 & 0 & \times & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ 0 & 0 & \boxed{\times} & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ \end{pmatrix}. \end{split}\]
- en: It turns out that a well-chosen Householder reflection does the trick. Let \(\mathbf{y}_1\)
    be the first column of \(A\) and take
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，一个精心选择的Householder反射可以完成这项工作。设 \(\mathbf{y}_1\) 为 \(A\) 的第一列，并取
- en: \[ \mathbf{z}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1}{\|\|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|} \quad \text{and} \quad H_1 = I_{n\times
    n} - 2\mathbf{z}_1\mathbf{z}_1^T \]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1}{\|\|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|} \quad \text{和} \quad H_1 = I_{n\times n}
    - 2\mathbf{z}_1\mathbf{z}_1^T \]
- en: where \(\mathbf{e}_1^{(n)}\) is the first vector in the canonical basis of \(\mathbb{R}^n\).
    As depicted below, this choice sends \(\mathbf{y}_1\) to
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{e}_1^{(n)}\) 是 \(\mathbb{R}^n\) 的标准基中的第一个向量。如图所示，这个选择将 \(\mathbf{y}_1\)
    映射到
- en: \[\begin{split} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)} = \begin{pmatrix} \|\mathbf{y}_1\|\\
    0 \\ \vdots \\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)} = \begin{pmatrix} \|\mathbf{y}_1\|\\
    0 \\ \vdots \\ 0 \end{pmatrix}. \end{split}\]
- en: (It is clear that if \(H_1 \mathbf{y}_1\) is proportional to \(\mathbf{e}_1^{(n)}\),
    than it can only be \(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\) or \(-\|\mathbf{y}_1\|
    \mathbf{e}_1^{(n)}\). Prove it!)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: （显然，如果 \(H_1 \mathbf{y}_1\) 与 \(\mathbf{e}_1^{(n)}\) 成比例，那么它只能是 \(\|\mathbf{y}_1\|
    \mathbf{e}_1^{(n)}\) 或 \(-\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)。证明它！）
- en: '![Introducing zeros by Householder reflection (with the help of Claude; inspired
    by (Source))](../Images/19bf9b4f1f98bc25a2715d91da61fc5b.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![通过Householder反射引入零（在Claude的帮助下；受（来源）的启发）](../Images/19bf9b4f1f98bc25a2715d91da61fc5b.png)'
- en: '**LEMMA** **(Householder)** \(\idx{Householder lemma}\xdi\) Let \(\mathbf{y}_1\),
    \(\mathbf{z}_1\) and \(H_1\) be as above. Then'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(Householder)** \(\idx{Householder lemma}\xdi\) 设 \(\mathbf{y}_1\),
    \(\mathbf{z}_1\) 和 \(H_1\) 如上所述。那么'
- en: \[ H_1 \mathbf{y}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}. \]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_1 \mathbf{y}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}. \]
- en: \(\flat\)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* The proof by picture is in the figure above.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 证明过程如图所示。'
- en: '*Proof:* Note that'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 注意到'
- en: \[\begin{align*} \|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2
    &= (\|\mathbf{y}_1\| - y_{1,1})^2 + \sum_{j=2}^n y_{1,j}^2\\ &= \|\mathbf{y}_1\|^2
    -2 \|\mathbf{y}_1\| y_{1,1} + y_{1,1}^2 + \sum_{j=2}^n y_{1,j}^2\\ &= 2(\|\mathbf{y}_1\|^2
    - \|\mathbf{y}_1\| y_{1,1}) \end{align*}\]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2
    &= (\|\mathbf{y}_1\| - y_{1,1})^2 + \sum_{j=2}^n y_{1,j}^2\\ &= \|\mathbf{y}_1\|^2
    -2 \|\mathbf{y}_1\| y_{1,1} + y_{1,1}^2 + \sum_{j=2}^n y_{1,j}^2\\ &= 2(\|\mathbf{y}_1\|^2
    - \|\mathbf{y}_1\| y_{1,1}) \end{align*}\]
- en: and
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: \[\begin{align*} 2 \mathbf{z}_1 \mathbf{z}_1^T \mathbf{y}_1 &= 2 \mathbf{z}_1
    \frac{\|\mathbf{y}_1\| \,(\mathbf{e}_1^{(n)})^T \mathbf{y}_1 - \mathbf{y}_1^T
    \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}\\ &= 2
    \frac{\|\mathbf{y}_1\| y_{1,1} - \|\mathbf{y}_1\|^2}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}
    - \mathbf{y}_1\|^2} (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)\\ &=
    - (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) \end{align*}\]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} 2 \mathbf{z}_1 \mathbf{z}_1^T \mathbf{y}_1 &= 2 \mathbf{z}_1
    \frac{\|\mathbf{y}_1\| \,(\mathbf{e}_1^{(n)})^T \mathbf{y}_1 - \mathbf{y}_1^T
    \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}\\ &= 2
    \frac{\|\mathbf{y}_1\| y_{1,1} - \|\mathbf{y}_1\|^2}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}
    - \mathbf{y}_1\|^2} (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)\\ &=
    - (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) \end{align*}\]
- en: where we used the previous equation. Hence
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了之前的方程。因此
- en: \[ H_1 \mathbf{y}_1 = (I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T) \,\mathbf{y}_1
    = \mathbf{y}_1 + (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) = \|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)}. \]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_1 \mathbf{y}_1 = (I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T) \,\mathbf{y}_1
    = \mathbf{y}_1 + (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) = \|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)}. \]
- en: That establishes the claim. \(\square\)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这就证明了命题。 \(\square\)
- en: The upshot is that multiplying \(A\) by \(H_1\) introduces zeros below the diagonal
    in the first column. To see this, recall that one interpretation of the matrix-matrix
    product is that each column of the second matrix gets multiplied by the first
    one. By the *Householder Lemma*, applying \(H_1\) to \(A\) gives
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，将 \(A\) 乘以 \(H_1\) 在第一列的下方引入了零。为了看到这一点，回忆矩阵-矩阵乘积的一种解释是，第二个矩阵的每一列都乘以第一个矩阵。根据
    *Householder 引理*，将 \(H_1\) 应用到 \(A\) 上给出
- en: \[ H_1 A = \begin{pmatrix} H_1 \mathbf{y}_1 & H_1 A_{\cdot,2} & \cdots & H_1
    A_{\cdot,m} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}
    & H_1 A_{\cdot,2} & \cdots & H_1 A_{\cdot,m} \end{pmatrix} \]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_1 A = \begin{pmatrix} H_1 \mathbf{y}_1 & H_1 A_{\cdot,2} & \cdots & H_1
    A_{\cdot,m} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}
    & H_1 A_{\cdot,2} & \cdots & H_1 A_{\cdot,m} \end{pmatrix} \]
- en: So the first column is now proportional to \(\mathbf{e}_1\), which has zeros
    in all but the first element. (What should we do if \(\mathbf{y}_1\) is already
    equal to \(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)?)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一列现在与 \(\mathbf{e}_1\) 成比例，除了第一个元素外，其余元素都是零。（如果 \(\mathbf{y}_1\) 已经等于 \(\|\mathbf{y}_1\|
    \mathbf{e}_1^{(n)}\)，我们应该怎么办？）
- en: It turns that there is another choice of Householder reflection. Indeed, it
    can be shown that
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，还有另一种 Householder 反射的选择。实际上，可以证明
- en: \[ \tilde{\mathbf{z}}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1}{\|
    \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1\|} \quad \text{and} \quad
    \tilde{H}_1 = I_{n\times n} - 2\tilde{\mathbf{z}}_1 \tilde{\mathbf{z}}_1^T \]
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \tilde{\mathbf{z}}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1}{\|
    \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1\|} \quad \text{和} \quad \tilde{H}_1
    = I_{n\times n} - 2\tilde{\mathbf{z}}_1 \tilde{\mathbf{z}}_1^T \]
- en: are such that \(\tilde{H}_1 \mathbf{y}_1 = - \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}\)
    (try it!).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 是这样的，使得 \(\tilde{H}_1 \mathbf{y}_1 = - \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}\)（试试看！）。
- en: '**Putting everything together** We have shown how to introduce zeros below
    the diagonal in the first column of a matrix. To introduce zeros in the second
    column below the diagonal we use a block matrix. Recall that if \(A_{ij} \in \mathbb{R}^{n_i
    \times m_j}\) and \(B_{ij} \in \mathbb{R}^{m_i \times p_j}\) for \(i,j = 1, 2\),
    then we have the following formula'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**综合以上内容** 我们已经展示了如何在矩阵的第一列下方引入零。为了在第二列下方引入零，我们使用分块矩阵。回想一下，如果 \(A_{ij} \in
    \mathbb{R}^{n_i \times m_j}\) 和 \(B_{ij} \in \mathbb{R}^{m_i \times p_j}\) 对于
    \(i,j = 1, 2\)，那么我们有以下公式'
- en: \[\begin{split} \begin{pmatrix} A_{11} & A_{12}\\ A_{21} & A_{22} \end{pmatrix}
    \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix} = \begin{pmatrix}
    A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\ A_{21} B_{11}
    + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22} \end{pmatrix}. \end{split}\]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{pmatrix} A_{11} & A_{12}\\ A_{21} & A_{22} \end{pmatrix}
    \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix} = \begin{pmatrix}
    A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\ A_{21} B_{11}
    + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22} \end{pmatrix}. \end{split}\]
- en: Now consider the following block matrix
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下分块矩阵
- en: \[\begin{split} H_2 = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2 \end{pmatrix}
    \end{split}\]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_2 = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2 \end{pmatrix}
    \end{split}\]
- en: where \(F_2\) is the following Householder reflection. Write the second column
    of \(H_1 A\) as \((y^{(2)}, \mathbf{y}_2)\). That is, \(\mathbf{y}_2\) are the
    entries \(2,\ldots, n\) of that column. Define
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(F_2\) 是以下 Householder 反射。将 \(H_1 A\) 的第二列写成 \((y^{(2)}, \mathbf{y}_2)\)。也就是说，\(\mathbf{y}_2\)
    是该列的 \(2,\ldots, n\) 个条目。定义
- en: \[ F_2 = I_{(n-1) \times (n-1)} - 2 \mathbf{z}_2 \mathbf{z}_2^T \quad \text{with}
    \quad \mathbf{z}_2 = \frac{\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2}{\|\|\mathbf{y}_2\|
    \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2\|} \]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F_2 = I_{(n-1) \times (n-1)} - 2 \mathbf{z}_2 \mathbf{z}_2^T \quad \text{其中}
    \quad \mathbf{z}_2 = \frac{\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2}{\|\|\mathbf{y}_2\|
    \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2\|} \]
- en: where now \(\mathbf{e}_1^{(n-1)} \in \mathbb{R}^{n-1}\). By the *Householder
    Lemma*, we have \(F_2 \mathbf{y}_2 = \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}\).
    It can be shown that \(\mathbf{y}_2 \neq \mathbf{0}\) when the columns of \(A\)
    are linearly independent. (Try it!)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中现在 \(\mathbf{e}_1^{(n-1)} \in \mathbb{R}^{n-1}\)。根据 *Householder 引理*，我们有 \(F_2
    \mathbf{y}_2 = \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}\)。可以证明当 \(A\) 的列线性无关时，\(\mathbf{y}_2
    \neq \mathbf{0}\)。（试试看！）
- en: Applying \(H_2\) to \(H_1 A\) preserves the first row and column, and introduces
    zeros under the diagonal in the second column. To see this, first re-write \(H_1
    A\) in block form
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将 \(H_2\) 应用到 \(H_1 A\) 上保持第一行和第一列，并在第二列下方引入零。为了看到这一点，首先将 \(H_1 A\) 重写为分块形式
- en: \[\begin{split} H_1 A = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\
    \mathbf{0} & G_2 \end{pmatrix} \end{split}\]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_1 A = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\
    \mathbf{0} & G_2 \end{pmatrix} \end{split}\]
- en: 'where we used our previous observation about the first column of \(H_1 A\)
    and where \(\mathbf{g}_2 \in \mathbb{R}^{m-1}\), \(G_2 \in \mathbb{R}^{(n-1)\times
    (m-1)}\). One important point to note: the first column of \(G_2\) is equal to
    \(\mathbf{y}_2\). Now multiply by \(H_2\) to get'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了关于 \(H_1 A\) 的第一列的先前观察，并且 \(\mathbf{g}_2 \in \mathbb{R}^{m-1}\)，\(G_2
    \in \mathbb{R}^{(n-1)\times (m-1)}\)。一个需要注意的重要点是：\(G_2\) 的第一列等于 \(\mathbf{y}_2\)。现在乘以
    \(H_2\) 来得到
- en: \[\begin{split} H_2 H_1 A = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2
    \end{pmatrix} \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & G_2 \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & F_2 G_2 \end{pmatrix}. \end{split}\]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_2 H_1 A = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2
    \end{pmatrix} \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & G_2 \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & F_2 G_2 \end{pmatrix}. \end{split}\]
- en: Computing the block \(F_2 G_2\) column by column we get
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过逐列计算 \(F_2 G_2\) 我们得到
- en: \[ F_2 G_2 = \begin{pmatrix} F_2 \mathbf{y}_2 & F_2 (G_2)_{\cdot,2} & \cdots
    & F_2 (G_2)_{\cdot,m-1} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}
    & F_2 (G_2)_{\cdot,2} & \cdots & F_2 (G_2)_{\cdot,m-1}, \end{pmatrix} \]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F_2 G_2 = \begin{pmatrix} F_2 \mathbf{y}_2 & F_2 (G_2)_{\cdot,2} & \cdots
    & F_2 (G_2)_{\cdot,m-1} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}
    & F_2 (G_2)_{\cdot,2} & \cdots & F_2 (G_2)_{\cdot,m-1}, \end{pmatrix} \]
- en: where \((G_2)_{\cdot,j}\) is the \(j\)-th column of \(G_2\). So the second column
    of \(H_2 H_1 A\) has zeros in all but the first two elements.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: \((G_2)_{\cdot,j}\) 是 \(G_2\) 的第 \(j\) 列。因此，\(H_2 H_1 A\) 的第二列除了前两个元素外，其余都是零。
- en: And so on. At Step \(k\), we split the \(k\)-th column of \(H_{k-1} \cdots H_1
    A\) into its first \(k-1\) and last \(n-k+1\) entries \((\mathbf{y}^{(k)}, \mathbf{y}_k)\)
    and form the matrix
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。在第 \(k\) 步，我们将 \(H_{k-1} \cdots H_1 A\) 的第 \(k\) 列分为前 \(k-1\) 个和最后 \(n-k+1\)
    个元素 \((\mathbf{y}^{(k)}, \mathbf{y}_k)\)，并形成矩阵
- en: \[\begin{split} H_k = \begin{pmatrix} I_{(k-1)\times (k-1)} & \mathbf{0} \\
    \mathbf{0} & F_k \end{pmatrix} \end{split}\]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_k = \begin{pmatrix} I_{(k-1)\times (k-1)} & \mathbf{0} \\
    \mathbf{0} & F_k \end{pmatrix} \end{split}\]
- en: where
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[ F_k = I_{(n-k+1) \times (n-k+1)} - 2 \mathbf{z}_k \mathbf{z}_k^T \quad \text{with}
    \quad \mathbf{z}_k = \frac{\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k}{\|\|\mathbf{y}_k\|
    \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k\|}. \]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F_k = I_{(n-k+1) \times (n-k+1)} - 2 \mathbf{z}_k \mathbf{z}_k^T \quad \text{其中}
    \quad \mathbf{z}_k = \frac{\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k}{\|\|\mathbf{y}_k\|
    \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k\|}. \]
- en: This time the first \(k-1\) rows and columns are preserved, while zeros are
    introduced under the diagonal of the \(k\)-th column. We omit the details (try
    it!).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这次保留了前 \(k-1\) 行和列，同时在第 \(k\) 列的对角线下方引入了零。我们省略了细节（试试看！）。
- en: We implement the procedure above in Python. We will need the following function.
    For \(\alpha \in \mathbb{R}\), let the sign of \(\alpha\) be
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现上述过程。我们需要以下函数。对于 \(\alpha \in \mathbb{R}\)，令 \(\alpha\) 的符号为
- en: \[\begin{split} \mathrm{sign}(\alpha) = \begin{cases} 1 & \text{if $\alpha >
    0$}\\ 0 & \text{if $\alpha = 0$}\\ -1 & \text{if $\alpha < 0$} \end{cases} \end{split}\]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathrm{sign}(\alpha) = \begin{cases} 1 & \text{如果} \alpha >
    0\\ 0 & \text{如果} \alpha = 0\\ -1 & \text{如果} \alpha < 0 \end{cases} \end{split}\]
- en: In Python, this is done using the function [`numpy.sign`](https://numpy.org/doc/stable/reference/generated/numpy.sign.html).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，这是通过使用函数 `numpy.sign` 来实现的。
- en: The following function constructs the upper triangular matrix \(R\) by iteratively
    modifying the relevant block of \(A\). On the other hand, computing the matrix
    \(Q\) actually requires extra computational work that is often not needed. We
    saw that, in the context of the least-squares problem, we really only need to
    compute \(Q^T \mathbf{b}\) for some input vector \(\mathbf{b}\). This can be done
    at the same time that \(R\) is constructed, as follows. The key point to note
    is that \(Q^T \mathbf{b} = H_m \cdots H_1 \mathbf{b}\).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数通过迭代修改 \(A\) 的相关块来构建上三角矩阵 \(R\)。另一方面，计算矩阵 \(Q\) 实际上需要额外的计算工作，而这通常是不必要的。我们注意到，在最小二乘问题的背景下，我们实际上只需要计算某些输入向量
    \(\mathbf{b}\) 的 \(Q^T \mathbf{b}\)。这可以在构建 \(R\) 的同时完成，如下所示。需要注意的是，\(Q^T \mathbf{b}
    = H_m \cdots H_1 \mathbf{b}\)。
- en: 'In our implementation of `householder`, we use both reflections defined above.
    We will not prove this here, but the particular choice made has good numerical
    properties. Quoting [TB, Lecture 10] (where \(H^+\) refers to the hyperplane used
    for the reflection when \(z=1\)):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现`householder`时，我们使用上面定义的两种反射。我们在这里不进行证明，但所做出的特定选择具有良好的数值特性。引用[TB, 第10讲]（其中
    \(H^+\) 指的是当 \(z=1\) 时用于反射的超平面）：
- en: Mathematically, either choice of sign is satisfactory. However, this is a case
    where numerical stability – insensitivity to rounding errors – dictates that one
    choice should be taken rather than the other. For numerical stability, it is desirable
    to reflect \(\mathbf{x}\) to the vector \(z \|\mathbf{x}\| \mathbf{e}_1\) that
    is not too close to \(\mathbf{x}\) itself. […] Suppose that [in the figure above]
    the angle between \(H^+\) and the \(\mathbf{e}_1\) axis is very small. Then the
    vector \(\mathbf{v} = \|\mathbf{x}\| \mathbf{e}_1 - \mathbf{x}\) is much smaller
    than \(\mathbf{x}\) or \(\|\mathbf{x}\| \mathbf{e}_1\). Thus the calculation of
    \(\mathbf{v}\) represents a subtraction of nearby quantities and will tend to
    suffer from cancellation errors.
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从数学上讲，两种符号的选择都是令人满意的。然而，这是一个数值稳定性——对舍入误差不敏感——决定应该选择一个而不是另一个的情况。为了数值稳定性，最好将 \(\mathbf{x}\)
    反射到 \(z \|\mathbf{x}\| \mathbf{e}_1\) 这个向量上，这个向量不太接近 \(\mathbf{x}\) 本身。 [...]
    假设在上面的图中，\(H^+\) 和 \(\mathbf{e}_1\) 轴之间的角度非常小。那么向量 \(\mathbf{v} = \|\mathbf{x}\|
    \mathbf{e}_1 - \mathbf{x}\) 比较小，比 \(\mathbf{x}\) 或 \(\|\mathbf{x}\| \mathbf{e}_1\)
    小得多。因此，\(\mathbf{v}\) 的计算代表了附近量的减法，并且可能会受到舍入误差的影响。
- en: We use [`numpy.outer`](https://numpy.org/doc/stable/reference/generated/numpy.outer.html)
    to compute \(\mathbf{z} \mathbf{z}^T\), which is referred to as an outer product.
    See [here](https://numpy.org/doc/stable/reference/generated/numpy.copy.html) for
    an explanation of `numpy.copy`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用[`numpy.outer`](https://numpy.org/doc/stable/reference/generated/numpy.outer.html)来计算
    \(\mathbf{z} \mathbf{z}^T\)，这被称为外积。有关`numpy.copy`的解释，请参阅[这里](https://numpy.org/doc/stable/reference/generated/numpy.copy.html)。
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**NUMERICAL CORNER:** We return to our overdetermined system example.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们回到我们的超定系统示例。'
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: One advantage of the Householder approach is that it produces a matrix \(Q\)
    with very good orthogonality, i.e., \(Q^T Q \approx I\). We give a quick example
    below comparing Gram-Schmidt and Householder. (The choice of matrix \(A\) will
    become clearer when we discuss the singular value decomposition in a later chapter.)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Householder方法的一个优点是它产生一个具有非常好的正交性的矩阵 \(Q\)，即 \(Q^T Q \approx I\)。以下是一个快速示例，比较了Gram-Schmidt和Householder方法。（矩阵
    \(A\) 的选择在我们讨论后续章节中的奇异值分解时会变得清晰。）
- en: '[PRE15]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see above, the \(Q\) and \(R\) factors produced by the Gram-Schmidt
    algorithm do have the property that \(QR \approx A\). However, \(Q\) is far from
    orthogonal. (Recall that `LA.norm` computes the Frobenius norm introduced previously.)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，Gram-Schmidt算法产生的 \(Q\) 和 \(R\) 因子确实具有 \(QR \approx A\) 的性质。然而，\(Q\) 远非正交。（回想一下，`LA.norm`
    计算了之前引入的Frobenius范数。）
- en: On the other hand, Householder reflections perform much better in that respect
    as we show next. Here we use the implementation of Householder transformations
    in [`numpy.linalg.qr`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.qr.html).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Householder反射在这方面表现更好，我们将在下面展示。这里我们使用`numpy.linalg.qr`中的Householder变换实现。
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: \(\unlhd\)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(有Claude、Gemini和ChatGPT的帮助)*'
- en: '**1** Which of the following matrices is upper triangular?'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 以下哪个矩阵是上三角矩阵？'
- en: a) \(\begin{pmatrix} 1 & 2 \\ 0 & 3 \end{pmatrix}\)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\begin{pmatrix} 1 & 2 \\ 0 & 3 \end{pmatrix}\)
- en: b) \(\begin{pmatrix} 1 & 0 \\ 2 & 3 \end{pmatrix}\)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\begin{pmatrix} 1 & 0 \\ 2 & 3 \end{pmatrix}\)
- en: c) \(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\)
- en: d) \(\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\)
- en: '**2** What is the output of the Gram-Schmidt algorithm when applied to a set
    of linearly independent vectors?'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 当将Gram-Schmidt算法应用于一组线性无关的向量时，其输出是什么？'
- en: a) An orthogonal basis for the subspace spanned by the vectors.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: a) 向量所张成的子空间的一个正交基。
- en: b) An orthonormal basis for the subspace spanned by the vectors.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: b) 向量所张成的子空间的一个正交基。
- en: c) A set of linearly dependent vectors.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: c) 一组线性相关的向量。
- en: d) A single vector that is orthogonal to all the input vectors.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: d) 一个与所有输入向量都正交的单个向量。
- en: '**3** What is the dimension of a hyperplane in \(\mathbb{R}^m\)?'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 在 \(\mathbb{R}^m\) 中超平面的维度是多少？'
- en: a) \(m\)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(m\)
- en: b) \(m - 1\)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(m - 1\)
- en: c) \(m - 2\)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(m - 2\)
- en: d) 1
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: d) 1
- en: '**4** Which of the following is true about a Householder reflection?'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 关于Householder反射，以下哪项是正确的？'
- en: a) It is a reflection across a hyperplane orthogonal to a unit vector.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它是围绕一个与单位向量正交的超平面的反射。
- en: b) It is a reflection across a unit vector.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它是围绕一个单位向量的反射。
- en: c) It is a rotation around a hyperplane orthogonal to a unit vector.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它是围绕一个与单位向量正交的超平面的旋转。
- en: d) It is a rotation around a unit vector.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它是围绕一个单位向量的旋转。
- en: '**5** How can a sequence of Householder reflections be used to compute a QR
    decomposition of a matrix \(A\)?'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 如何使用一系列Householder反射来计算矩阵\(A\)的QR分解？'
- en: a) By iteratively introducing zeros above the diagonal of \(A\).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: a) 通过在\(A\)的上三角线上迭代引入零。
- en: b) By iteratively introducing zeros below the diagonal of \(A\).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: b) 通过在\(A\)的下三角线上迭代引入零。
- en: c) By iteratively introducing zeros on the diagonal of \(A\).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: c) 通过在\(A\)的对角线上迭代引入零。
- en: d) By iteratively introducing zeros in the entire matrix \(A\).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: d) 通过在矩阵\(A\)的整个矩阵中迭代引入零。
- en: 'Answer for 1: a. Justification: The text defines an upper triangular matrix
    as one with all entries below the diagonal equal to zero.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 答案1：a. 证明：文本定义上三角矩阵为所有对角线下方元素都为零的矩阵。
- en: 'Answer for 2: b. Justification: The text states that the Gram-Schmidt algorithm
    “produces an orthonormal basis of \(\mathrm{span}(a_1, \dots, a_m)\).”'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 答案2：b. 证明：文本中提到“Gram-Schmidt算法“产生\(\mathrm{span}(a_1, \dots, a_m)\)的正交基”。”
- en: 'Answer for 3: b. Justification: The text defines a hyperplane as a linear subspace
    of \(\mathbb{R}^m\) of dimension \(m - 1\).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 答案3：b. 证明：文本定义超平面为\(\mathbb{R}^m\)的维度为\(m - 1\)的线性子空间。
- en: 'Answer for 4: a. Justification: The text defines a Householder reflection as
    follows: “Let \(\mathbf{z} \in \mathbb{R}^m\) be a unit vector and let \(W\) be
    the hyperplane orthogonal to it. The reflection across \(W\) is given by \(H =
    I_{m \times m} - 2\mathbf{z}\mathbf{z}^T\).”'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：a. 证明：文本如下定义Householder反射：“设\(\mathbf{z} \in \mathbb{R}^m\)为一个单位向量，设\(W\)为与它正交的超平面。\(W\)上的反射由\(H
    = I_{m \times m} - 2\mathbf{z}\mathbf{z}^T\)给出。”
- en: 'Answer for 5: b. Justification: The text states, “One way to construct a (full)
    QR decomposition of a matrix \(A \in \mathbb{R}^{n \times m}\) is to find a sequence
    of orthogonal matrices \(H_1,\ldots,H_m\) that triangularize \(A\): \(H_m \cdots
    H_2H_1A = R\) for an upper-triangular matrix \(R\).”'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：b. 证明：文本中提到，“构造矩阵\(A \in \mathbb{R}^{n \times m}\)的（满）QR分解的一种方法是要找到一个正交矩阵序列\(H_1,\ldots,H_m\)，使得\(A\)三角化：\(H_m
    \cdots H_2H_1A = R\)，其中\(R\)是一个上三角矩阵。”
- en: 2.4.1\. Matrix form of Gram-Schmidt[#](#matrix-form-of-gram-schmidt "Link to
    this heading")
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4.1\. Gram-Schmidt算法的矩阵形式[#](#matrix-form-of-gram-schmidt "链接到这个标题")
- en: In this subsection, we prove the *Gram-Schmidt Theorem* and introduce a fruitful
    matrix perspective.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们证明Gram-Schmidt定理并介绍一个有成效的矩阵视角。
- en: '**Gram-Schmidt algorithm** Let \(\mathbf{a}_1,\ldots,\mathbf{a}_m\) be linearly
    independent. We use the Gram-Schmidt algorithm\(\idx{Gram-Schmidt algorithm}\xdi\)
    to obtain an orthonormal basis of \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\).
    The process takes advantage of the properties of the orthogonal projection derived
    above. In essence we add the vectors \(\mathbf{a}_i\) one by one, but only after
    taking out their orthogonal projection on the previously included vectors. The
    outcome spans the same subspace and the *Orthogonal Projection Theorem* ensures
    orthogonality.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gram-Schmidt算法** 设\(\mathbf{a}_1,\ldots,\mathbf{a}_m\)是线性无关的。我们使用Gram-Schmidt算法\(\idx{Gram-Schmidt
    algorithm}\xdi\)来获得\(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\)的正交基。这个过程利用了上面推导出的正交投影的性质。本质上，我们逐个添加向量\(\mathbf{a}_i\)，但是在添加之前，先从之前包含的向量中取出它们的正交投影。结果生成的空间与之前相同，并且正交投影定理确保了正交性。'
- en: '*Proof idea:* *(Gram-Schmidt)* Suppose first that \(m=1\). In that case, all
    that needs to be done is to divide \(\mathbf{a}_1\) by its norm to obtain a unit
    vector whose span is the same as \(\mathbf{a}_1\), that is, we set \(\mathbf{q}_1
    = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明思路：** *(Gram-Schmidt)* 首先假设\(m=1\)。在这种情况下，需要做的只是将\(\mathbf{a}_1\)除以其范数以获得一个单位向量，其生成的空间与\(\mathbf{a}_1\)相同，即我们设\(\mathbf{q}_1
    = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)。'
- en: 'Suppose now that \(m=2\). We first let \(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)
    as in the previous case. Then we subtract from \(\mathbf{a}_2\) its projection
    on \(\mathbf{q}_1\), that is, we set \(\mathbf{v}_2 = \mathbf{a}_2 - \langle \mathbf{q}_1,
    \mathbf{a}_2 \rangle \,\mathbf{q}_1\). It is easily checked that \(\mathbf{v}_2\)
    is orthogonal to \(\mathbf{q}_1\) (see the proof of the *Orthogonal Projection
    Theorem* for a similar calculation). Moreover, because \(\mathbf{a}_2\) is a linear
    combination of \(\mathbf{q}_1\) and \(\mathbf{v}_2\), we have \(\mathrm{span}(\mathbf{q}_1,\mathbf{v}_2)
    = \mathrm{span}(\mathbf{a}_1,\mathbf{a}_2)\). It remains to divide by the norm
    of the resulting vector: \(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|}\).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现在 \(m=2\)。我们首先让 \(\mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)
    如前所述。然后我们从 \(\mathbf{a}_2\) 中减去其在 \(\mathbf{q}_1\) 上的投影，即我们设定 \(\mathbf{v}_2 =
    \mathbf{a}_2 - \langle \mathbf{q}_1, \mathbf{a}_2 \rangle \,\mathbf{q}_1\)。很容易验证
    \(\mathbf{v}_2\) 与 \(\mathbf{q}_1\) 正交（参见正交投影定理的证明中的类似计算）。此外，因为 \(\mathbf{a}_2\)
    是 \(\mathbf{q}_1\) 和 \(\mathbf{v}_2\) 的线性组合，所以我们有 \(\mathrm{span}(\mathbf{q}_1,\mathbf{v}_2)
    = \mathrm{span}(\mathbf{a}_1,\mathbf{a}_2)\)。剩下的是除以结果向量的范数：\(\mathbf{q}_2 = \frac{\mathbf{v}_2}{\|\mathbf{v}_2\|}\)。
- en: For general \(m\), we proceed similarly but project onto the subspace spanned
    by the previously added vectors at each step.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的 \(m\)，我们采取类似的步骤，但在每一步都投影到由之前添加的向量张成的子空间。
- en: '![Gram–Schmidt process (with help from ChatGPT; inspired by Source)](../Images/357bfa05be44b654c5858a352b39044e.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![Gram–Schmidt过程（得益于ChatGPT的帮助；受来源启发）](../Images/357bfa05be44b654c5858a352b39044e.png)'
- en: '*Proof:* *(Gram-Schmidt)* The first step of the induction is described above.
    Then the general inductive step is the following. Assume that we have constructed
    orthonormal vectors \(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}\) such that'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* *(Gram-Schmidt)* 归纳的第一步已在上面描述。然后一般的归纳步骤如下。假设我们已经构造了正交向量 \(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}\)，使得'
- en: \[ U_{j-1} := \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}).
    \]
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \[ U_{j-1} := \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1}).
    \]
- en: '*Constructing \(\mathbf{q}_j\):* By the *Properties of Orthonormal Lists*,
    \(\{\mathbf{q}\}_{i=1}^{j-1}\) is an independent list and therefore forms an orthonormal
    basis for \(U_{j-1}\). So we can compute the orthogonal projection of \(\mathbf{a}_j\)
    on \(U_{j-1}\) as'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*构造 \(\mathbf{q}_j\):* 根据 *正交列表的性质*，\(\{\mathbf{q}\}_{i=1}^{j-1}\) 是一个独立的列表，因此形成
    \(U_{j-1}\) 的正交基。因此，我们可以计算 \(\mathbf{a}_j\) 在 \(U_{j-1}\) 上的正交投影：'
- en: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i,
    \]
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i,
    \]
- en: where we defined \(r_{ij} = \langle \mathbf{q}_i , \mathbf{a}_j\rangle\). And
    we set
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(r_{ij} = \langle \mathbf{q}_i , \mathbf{a}_j\rangle\)。并且我们设定
- en: \[ \mathbf{v}_j = \mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \mathbf{a}_j
    - \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i \quad \text{and} \quad \mathbf{q}_j =
    \frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}. \]
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{v}_j = \mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j = \mathbf{a}_j
    - \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i \quad \text{和} \quad \mathbf{q}_j = \frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}.
    \]
- en: 'The last step is possible because:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步之所以可能，是因为：
- en: '**LEMMA** \(\|\mathbf{v}_j\| > 0\). \(\flat\)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** \(\|\mathbf{v}_j\| > 0\). \(\flat\)'
- en: '*Proof:* Indeed otherwise \(\mathbf{a}_j\) would be equal to its projection
    \(\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)
    which would contradict linear independence of the \(\mathbf{a}_k\)’s. \(\square\)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 确实，否则 \(\mathbf{a}_j\) 将等于其投影 \(\mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in
    \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)，这将与 \(\mathbf{a}_k\) 的线性无关性相矛盾。\(\square\)'
- en: The vector \(\mathbf{q}_j\) is of unit norm by construction. It is also orthogonal
    to \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})\) by the definition of
    \(\mathbf{v}_j\) and the *Orthogonal Projection Theorem*. So \(\mathbf{q}_1,\ldots,\mathbf{q}_j\)
    form an orthonormal list.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 \(\mathbf{q}_j\) 的范数为单位，这是通过构造得到的。根据 \(\mathbf{v}_j\) 的定义和正交投影定理，它也正交于 \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})\)。因此，\(\mathbf{q}_1,\ldots,\mathbf{q}_j\)
    形成一个正交列表。
- en: '*Pushing the induction through:* It remains to prove that \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)
    = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\). Because by induction \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1})
    = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\), all we have to prove
    are the following two claims.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过归纳推进：* 剩下要证明的是 \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\)。因为通过归纳
    \(\mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_{j-1}) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})\)，我们只需要证明以下两个命题。'
- en: '**LEMMA** \(\mathbf{q}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\).
    \(\flat\)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** \(\mathbf{q}_j \in \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_j)\).
    \(\flat\)'
- en: '*Proof:* By construction,'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 通过构造，'
- en: \[ \mathbf{q}_j = \frac{1}{\|\mathbf{v}_j\|} \left\{\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\right\}
    = \frac{1}{\|\mathbf{v}_j\|} \mathbf{a}_j + \frac{1}{\|\mathbf{v}_j\|} \mathrm{proj}_{U_{j-1}}\mathbf{a}_j.
    \]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{q}_j = \frac{1}{\|\mathbf{v}_j\|} \left\{\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\right\}
    = \frac{1}{\|\mathbf{v}_j\|} \mathbf{a}_j + \frac{1}{\|\mathbf{v}_j\|} \mathrm{proj}_{U_{j-1}}\mathbf{a}_j.
    \]
- en: By definition of the orthogonal projection,
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 根据正交投影的定义，
- en: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in U_{j-1}= \mathrm{span} (\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})
    \subseteq \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j}). \]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{proj}_{U_{j-1}}\mathbf{a}_j \in U_{j-1}= \mathrm{span} (\mathbf{a}_1,\ldots,\mathbf{a}_{j-1})
    \subseteq \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j}). \]
- en: Hence we have written \(\mathbf{q}_j\) as a linear combination of vectors in
    \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\). That proves the claim.
    \(\square\)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将 \(\mathbf{q}_j\) 写成了 \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)
    中向量的线性组合。这就证明了该命题。 \(\square\)
- en: '**LEMMA** \(\mathbf{a}_j \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)\).
    \(\flat\)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** \(\mathbf{a}_j \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_j)\).
    \(\flat\)'
- en: '*Proof:* Unrolling the calculations above, \(\mathbf{a}_j\) can be re-written
    as the following linear combination of \(\mathbf{q}_1,\ldots,\mathbf{q}_j\)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 展开上述计算，\(\mathbf{a}_j\) 可以重新写成以下 \(\mathbf{q}_1,\ldots,\mathbf{q}_j\)
    的线性组合'
- en: \[\begin{align*} \mathbf{a}_j &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \mathbf{v}_j\\
    &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{v}_j\| \mathbf{q}_j\\ &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j
    + \|\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\| \mathbf{q}_j\\ &= \sum_{i=1}^{j-1}
    r_{ij} \,\mathbf{q}_i + \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    \,\mathbf{q}_j\\ &= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \end{align*}\]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{a}_j &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \mathbf{v}_j\\
    &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j + \|\mathbf{v}_j\| \mathbf{q}_j\\ &= \mathrm{proj}_{U_{j-1}}\mathbf{a}_j
    + \|\mathbf{a}_j - \mathrm{proj}_{U_{j-1}}\mathbf{a}_j\| \mathbf{q}_j\\ &= \sum_{i=1}^{j-1}
    r_{ij} \,\mathbf{q}_i + \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    \,\mathbf{q}_j\\ &= \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \end{align*}\]
- en: where we defined \(r_{jj} = \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    = \|\mathbf{v}_j\|\). \(\square\)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义了 \(r_{jj} = \left\|\mathbf{a}_j - \sum_{i=1}^{j-1} r_{ij}\,\mathbf{q}_i\right\|
    = \|\mathbf{v}_j\|\). \(\square\)
- en: Hence \(\mathbf{q}_1,\ldots,\mathbf{q}_j\) forms an orthonormal list with \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\).
    So induction goes through. That concludes the proof of the theorem. \(\square\)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(\mathbf{q}_1,\ldots,\mathbf{q}_j\) 形成了一个与 \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_{j})\)
    正交的列表。因此，归纳成立。这就完成了定理的证明。 \(\square\)
- en: We implement the Gram-Schmidt algorithm in Python. For reasons that will become
    clear in the next subsection, we output both the \(\mathbf{q}_j\)’s and \(r_{ij}\)’s,
    each in matrix form. Here we use [`numpy.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)
    to compute inner products.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现 Gram-Schmidt 算法。由于原因将在下一小节中变得清晰，我们以矩阵形式输出 \(\mathbf{q}_j\) 和
    \(r_{ij}\)，每个都是矩阵形式。在这里，我们使用 `numpy.dot` 来计算内积。
- en: '[PRE19]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**NUMERICAL CORNER:** Let’s try a simple example.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角：* 让我们尝试一个简单的例子。'
- en: '[PRE20]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: \(\unlhd\)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**Matrix form** Let \(\mathbf{a}_1,\ldots,\mathbf{a}_m \in \mathbb{R}^n\) be
    linearly independent. Above, we presented the Gram-Schmidt algorithm to obtain
    an orthonormal basis of \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\). We
    revisit it in matrix form.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵形式** 设 \(\mathbf{a}_1,\ldots,\mathbf{a}_m \in \mathbb{R}^n\) 是线性无关的。在上面，我们介绍了
    Gram-Schmidt 算法来获得 \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\) 的正交基。我们以矩阵形式重新审视它。'
- en: Let
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{and} \quad Q = \begin{pmatrix} | & & | \\
    \mathbf{q}_1 & \ldots & \mathbf{q}_m \\ | & & | \end{pmatrix}. \end{split}\]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} \quad \text{和} \quad Q = \begin{pmatrix} | & & | \\ \mathbf{q}_1
    & \ldots & \mathbf{q}_m \\ | & & | \end{pmatrix}. \end{split}\]
- en: Recalling that, for all \(j\),
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，对于所有的 \(j\)，
- en: \[ \mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \]
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{a}_j = \sum_{i=1}^{j-1} r_{ij} \,\mathbf{q}_i + r_{jj} \,\mathbf{q}_j,
    \]
- en: the output of the Gram-Schmidt algorithm can be written in the following compact
    form, known as a [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition)\(\idx{QR
    decomposition}\xdi\),
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Gram-Schmidt 算法的输出可以写成以下紧凑形式，称为 [QR 分解](https://en.wikipedia.org/wiki/QR_decomposition)\(\idx{QR
    decomposition}\xdi\)。
- en: \[ A = QR \]
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = QR \]
- en: where column \(i\) of the \(m \times m\) matrix \(R\) contains the coefficients
    of the linear combination of the \(\mathbf{q}_j\)’s that produce \(\mathbf{a}_i\).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(m \times m\) 矩阵 \(R\) 的第 \(i\) 列包含产生 \(\mathbf{a}_i\) 的 \(\mathbf{q}_j\)
    的线性组合的系数。
- en: 'By the proof of the *Gram-Schmidt Theorem*, \(\mathbf{a}_i \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_i)\).
    So column \(i\) of \(R\) has only zeros below the diagonal. Hence \(R\) has a
    special structure we have previously encountered: it is upper triangular. The
    proof also established that the diagonal elements of \(R\) are strictly positive.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *Gram-Schmidt 定理* 的证明，\(\mathbf{a}_i \in \mathrm{span}(\mathbf{q}_1,\ldots,\mathbf{q}_i)\)。因此
    \(R\) 的第 \(i\) 列对角线以下只有零。因此 \(R\) 具有我们之前遇到过的特殊结构：它是上三角矩阵。证明还确立了 \(R\) 的对角线元素是严格正的。
- en: '**DEFINITION** **(Triangular matrix)** \(\idx{triangular matrix}\xdi\) A matrix
    \(R = (r_{ij})_{i,j} \in \mathbb{R}^{n \times m}\) is upper-triangular if all
    entries below the diagonal are zero, that is, if \(i > j\) implies \(r_{ij} =
    0\). Similarly, a lower-triangular matrix has zeros above the diagonal. \(\natural\)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(三角矩阵)** \(\idx{triangular matrix}\xdi\) 一个矩阵 \(R = (r_{ij})_{i,j}
    \in \mathbb{R}^{n \times m}\) 是上三角矩阵，如果其对角线以下的全部元素都是零，即如果 \(i > j\) 则 \(r_{ij}
    = 0\)。同样，下三角矩阵的对角线上方元素为零。 \(\natural\)'
- en: An upper-triangular matrix looks like this
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一个上三角矩阵看起来是这样的
- en: \[\begin{split} R = \begin{bmatrix} r_{1,1} & r_{1,2} & r_{1,3} & \ldots & r_{1,n}
    \\ 0 & r_{2,2} & r_{2,3} & \ldots & r_{2,n} \\ & 0 & \ddots & \ddots & \vdots
    \\ & & \ddots & \ddots & r_{n-1,n} \\ 0 & & & 0 & r_{n,n} \end{bmatrix}. \end{split}\]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} R = \begin{bmatrix} r_{1,1} & r_{1,2} & r_{1,3} & \ldots & r_{1,n}
    \\ 0 & r_{2,2} & r_{2,3} & \ldots & r_{2,n} \\ & 0 & \ddots & \ddots & \vdots
    \\ & & \ddots & \ddots & r_{n-1,n} \\ 0 & & & 0 & r_{n,n} \end{bmatrix}. \end{split}\]
- en: '**Remarks:**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**备注：**'
- en: a) If the input vectors \(\mathbf{a}_1,\ldots,\mathbf{a}_m\) are not linearly
    independent (in which case we say that the matrix \(A\) is rank-deficient), the
    Gram-Schmidt algorithm will fail. Indeed, at some point we will have that \(\mathbf{a}_j
    \in U_{j-1}\) and the normalization of \(\mathbf{v}_j\) will not be possible.
    In that case, one can instead use a technique called [column pivoting](https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting),
    which we will not describe.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: a) 如果输入向量 \(\mathbf{a}_1,\ldots,\mathbf{a}_m\) 不是线性无关的（在这种情况下我们说矩阵 \(A\) 是秩不足的），Gram-Schmidt
    算法将失败。实际上，在某个时刻我们将有 \(\mathbf{a}_j \in U_{j-1}\) 并且 \(\mathbf{v}_j\) 的归一化将不可能。在这种情况下，可以使用一种称为[列主元](https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting)的技术，我们将在后面不进行描述。
- en: b) The QR decomposition we have derived here is technically called a reduced
    QR decomposition. In a full QR decomposition\(\idx{full QR decomposition}\xdi\),
    the matrix \(Q\) is square and orthogonal. In other words, the columns of such
    a \(Q\) form an orthonormal basis of the full space \(\mathbb{R}^n\). Let \(A
    = Q_1 R_1\) be a reduced QR decomposition, as obtained through the Gram-Schmidt
    algorithm. Then the columns of \(Q_1\) form an orthonormal basis of \(\mathrm{col}(A)\)
    and can be completed into an orthonormal basis of \(\mathbb{R}^n\) by adding further
    vectors \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\). Let \(Q_2\) be the matrix
    with columns \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\). Then a full QR decomposition
    of \(A\) is
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: b) 我们在这里推导出的 QR 分解在技术上称为简化 QR 分解。在完全 QR 分解\(\idx{full QR decomposition}\xdi\)中，矩阵
    \(Q\) 是方阵且正交。换句话说，这样的 \(Q\) 的列构成了 \(\mathbb{R}^n\) 的完整空间的正交基。设 \(A = Q_1 R_1\)
    是通过 Gram-Schmidt 算法获得的简化 QR 分解。那么 \(Q_1\) 的列构成了 \(\mathrm{col}(A)\) 的正交基，并且可以通过添加更多的向量
    \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\) 来完成 \(\mathbb{R}^n\) 的正交基。设 \(Q_2\)
    是列向量为 \(\mathbf{q}_{m+1},\ldots,\mathbf{q}_{n}\) 的矩阵。那么 \(A\) 的完全 QR 分解是
- en: \[\begin{split} Q = \begin{pmatrix} Q_1 & Q_2 \end{pmatrix} \qquad R = \begin{pmatrix}
    R_1\\ \mathbf{0}_{(n-m)\times m} \end{pmatrix} \end{split}\]
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} Q = \begin{pmatrix} Q_1 & Q_2 \end{pmatrix} \qquad R = \begin{pmatrix}
    R_1\\ \mathbf{0}_{(n-m)\times m} \end{pmatrix} \end{split}\]
- en: where \(\mathbf{0}_{(n-m)\times m}\) is the all-zero matrix of size \((n-m)\times
    m\). A numerical method for computing a full QR decomposition is presented in
    a later subsection.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{0}_{(n-m)\times m}\) 是大小为 \((n-m)\times m\) 的全零矩阵。计算完全 QR 分解的数值方法将在后面的子节中介绍。
- en: c) The Gram-Schmidt algorithm is appealing geometrically, but it is known to
    have numerical issues. Other methods exist for computing QR decompositions with
    better numerical properties. We discuss such a method in a later subsection. (See
    that same subsection for an example where the \(\mathbf{q}_j\)’s produced by Gram-Schmidt
    are far from orthogonal.)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: c) 格拉姆-施密特算法在几何上很有吸引力，但众所周知，它存在数值问题。存在其他方法可以计算具有更好数值特性的 QR 分解。我们将在后面的子节中讨论这样的方法。（参见该子节中，由格拉姆-施密特产生的
    \(\mathbf{q}_j\) 远非正交的示例。）
- en: 2.4.2\. Least squares via QR[#](#least-squares-via-qr "Link to this heading")
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4.2\. 通过 QR 进行最小二乘法[#](#least-squares-via-qr "链接到本标题")
- en: Let \(A \in \mathbb{R}^{n\times m}\) be an \(n\times m\) matrix with linearly
    independent columns and let \(\mathbf{b} \in \mathbb{R}^n\) be a vector. Recall
    that a solution \(\mathbf{x}^*\) to the linear least squares problem
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(A \in \mathbb{R}^{n\times m}\) 为一个 \(n\times m\) 的矩阵，其列线性无关，且设 \(\mathbf{b}
    \in \mathbb{R}^n\) 为一个向量。回忆一下，线性最小二乘问题的解 \(\mathbf{x}^*\) 满足
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2 \]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2 \]
- en: satisfies the normal equations
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 满足正则方程
- en: \[ A^T A \mathbf{x}^* = A^T \mathbf{b}. \]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^T A \mathbf{x}^* = A^T \mathbf{b}. \]
- en: '**Solving the normal equations** In a first linear algebra course, one learns
    how to solve linear systems such as the normal equations. For this task a common
    approach is Gaussian elimination, or row reduction. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_elimination):'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**求解正则方程** 在第一门线性代数课程中，人们学习如何求解线性系统，如正则方程。为此，一个常见的方法是高斯消元法或行简化。引用 [维基百科](https://en.wikipedia.org/wiki/Gaussian_elimination)：'
- en: To perform row reduction on a matrix, one uses a sequence of elementary row
    operations to modify the matrix until the lower left-hand corner of the matrix
    is filled with zeros, as much as possible. […] Once all of the leading coefficients
    (the leftmost nonzero entry in each row) are 1, and every column containing a
    leading coefficient has zeros elsewhere, the matrix is said to be in reduced row
    echelon form. […] The process of row reduction […] can be divided into two parts.
    The first part (sometimes called forward elimination) reduces a given system to
    row echelon form, from which one can tell whether there are no solutions, a unique
    solution, or infinitely many solutions. The second part (sometimes called back
    substitution) continues to use row operations until the solution is found; in
    other words, it puts the matrix into reduced row echelon form.
  id: totrans-289
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要对一个矩阵进行行简化，可以使用一系列初等行操作来修改矩阵，直到矩阵的左下角尽可能多地填充零。 [...] 一旦所有主系数（每行的最左边的非零项）都是
    1，并且包含主系数的每一列在其他地方都有零，则该矩阵被称为简化行阶梯形。 [...] 行简化的过程 [...] 可以分为两部分。第一部分（有时称为前向消元）将给定的系统简化为行阶梯形，从其中可以判断是否存在无解、唯一解或无限多解。第二部分（有时称为回代）继续使用行操作直到找到解；换句话说，它将矩阵置于简化行阶梯形。
- en: '**Figure:** An example of Gaussian elimination ([Source](https://en.wikipedia.org/wiki/Gaussian_elimination))'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：高斯消元法的示例 ([来源](https://en.wikipedia.org/wiki/Gaussian_elimination))'
- en: '![Gaussian elimination](../Images/dc18b77fc722fe6abe5812c927848a72.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![高斯消元](../Images/dc18b77fc722fe6abe5812c927848a72.png)'
- en: \(\bowtie\)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: We will not go over Gaussian elimination here. In this subsection, we develop
    an alternative approach to solving the normal equations using the QR decomposition.
    We will need one component of Gaussian elimination, back substitution\(\idx{back
    substitution}\xdi\). It is based on the observation that triangular systems of
    equations are straightforward to solve. We start with an example.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会介绍高斯消元法。在本节中，我们将开发一种使用 QR 分解求解正则方程的替代方法。我们需要高斯消元法的一个组成部分，回代\(\idx{回代}\xdi\)。它基于这样的观察：三角方程组是容易求解的。我们从例子开始。
- en: '**EXAMPLE:** Here is a concrete example of back substitution. Consider the
    system \(R \mathbf{x} = \mathbf{b}\) with'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 这里有一个回代的具体示例。考虑系统 \(R \mathbf{x} = \mathbf{b}\)，其中'
- en: \[\begin{split} R = \begin{pmatrix} 2 & -1 & 2\\ 0 & 1 & 1\\ 0 & 0 & 2 \end{pmatrix}
    \qquad \mathbf{b} = \begin{pmatrix} 0\\ -2\\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} R = \begin{pmatrix} 2 & -1 & 2\\ 0 & 1 & 1\\ 0 & 0 & 2 \end{pmatrix}
    \qquad \mathbf{b} = \begin{pmatrix} 0\\ -2\\ 0 \end{pmatrix}. \end{split}\]
- en: That corresponds to the linear equations
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这对应于线性方程
- en: \[\begin{align*} &2 x_1 - x_2 + 2x_3 = 0\\ &x_2 + x_3 = -2\\ &2 x_3 = 0 \end{align*}\]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &2 x_1 - x_2 + 2x_3 = 0\\ &x_2 + x_3 = -2\\ &2 x_3 = 0 \end{align*}\]
- en: The third equation gives \(x_3 = 0/2 = 0\). Plugging into the second one, we
    get \(x_2 = -2 - x_3 = -2\). Plugging into the first one, we finally have \(x_1
    = (x_2 - 2 x_3)/2 = -1\). So the solution is \(\mathbf{x} = (-1,-2,0)\). \(\lhd\)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个方程给出 \(x_3 = 0/2 = 0\)。将其代入第二个方程，我们得到 \(x_2 = -2 - x_3 = -2\)。将其代入第一个方程，我们最终得到
    \(x_1 = (x_2 - 2 x_3)/2 = -1\)。所以解是 \(\mathbf{x} = (-1,-2,0)\)。\(\lhd\)
- en: In general, solving a triangular system of equations works as follows. Let \(R
    = (r_{i,j})_{i,j} \in \mathbb{R}^{m \times m}\) be upper-triangular and let \(\mathbf{b}
    \in \mathbb{R}^m\) be the left-hand vector, i.e., we want to solve the system
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解三角方程组的工作方式如下。设 \(R = (r_{i,j})_{i,j} \in \mathbb{R}^{m \times m}\) 是上三角矩阵，设
    \(\mathbf{b} \in \mathbb{R}^m\) 是左端向量，即我们想要解这个系统
- en: \[ R \mathbf{x} = \mathbf{b}. \]
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: \[ R \mathbf{x} = \mathbf{b}. \]
- en: Starting from the last row of the system, \(r_{m,m} x_m = b_m\) or \(x_m = b_m/r_{m,m}\),
    assuming that \(r_{m,m} \neq 0\). Moving to the second-to-last row, \(r_{m-1,m-1}
    x_{m-1} + r_{m-1,m} x_m = b_{m-1}\) or \(x_{m-1} = (b_{m-1} - r_{m-1,m} x_m)/r_{m-1,m-1}\),
    assuming that \(r_{m-1,m-1} \neq 0\). And so on. This procedure is known as [back
    substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统的最后一行开始，\(r_{m,m} x_m = b_m\) 或 \(x_m = b_m/r_{m,m}\)，假设 \(r_{m,m} \neq 0\)。移动到倒数第二行，\(r_{m-1,m-1}
    x_{m-1} + r_{m-1,m} x_m = b_{m-1}\) 或 \(x_{m-1} = (b_{m-1} - r_{m-1,m} x_m)/r_{m-1,m-1}\)，假设
    \(r_{m-1,m-1} \neq 0\)。依此类推。这个过程被称为[回代](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution)。
- en: Analogously, in the lower triangular case \(L \in \mathbb{R}^{m \times m}\),
    we have [forward substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution)\(\idx{forward
    substitution}\xdi\). These procedures implicitly define an inverse for \(R\) and
    \(L\) *when the diagonal elements are all non-zero*. We will not write them down
    explicitly here.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在下三角情况 \(L \in \mathbb{R}^{m \times m}\) 中，我们有 [前向代数](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution)\(\idx{forward
    substitution}\xdi\)。这些过程隐式地定义了 \(R\) 和 \(L\) 的逆（当对角线元素全部非零时）。我们在这里不会明确写出它们。
- en: We implement back substitution in Python. In our naive implementation, we assume
    that the diagonal entries are not zero, which will suffice for our purposes.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现了回代。在我们的原始实现中，我们假设对角线元素不为零，这对于我们的目的来说是足够的。
- en: '[PRE26]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Forward substitution is implemented similarly.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 前向代数也是类似实现的。
- en: '[PRE27]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Using QR** We show how to solve the normal equations via the QR decomposition.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 QR** 我们展示了如何通过 QR 分解求解正则方程。'
- en: 1- Construct an orthonormal basis of \(\mathrm{col}(A)\) through a QR decomposition
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 通过 QR 分解构造 \(\mathrm{col}(A)\) 的正交基
- en: \[ A = QR. \]
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = QR. \]
- en: 2- Form the orthogonal projection matrix
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 构造正交投影矩阵
- en: \[ P = Q Q^T. \]
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P = Q Q^T. \]
- en: 3- Apply the projection to \(\mathbf{b}\) and observe that, by the proof of
    the *Normal Equations*, \(\mathbf{x}^*\) satisfies
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 3- 将投影应用于 \(\mathbf{b}\) 并观察，根据正则方程的证明，\(\mathbf{x}^*\) 满足
- en: \[ A \mathbf{x}^* = Q Q^T \mathbf{b}. \]
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A \mathbf{x}^* = Q Q^T \mathbf{b}. \]
- en: 4- Plug in the QR decomposition for \(A\) to get
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 4- 将 \(A\) 的 QR 分解代入以得到
- en: \[ QR \mathbf{x}^* = Q Q^T \mathbf{b}. \]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \[ QR \mathbf{x}^* = Q Q^T \mathbf{b}. \]
- en: 5- Multiply both sides by \(Q^T\) and use \(Q^T Q = I_{m \times m}\)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 5- 两边乘以 \(Q^T\) 并使用 \(Q^T Q = I_{m \times m}\)
- en: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
- en: 6- Solving this system for \(\mathbf{x}^*\) is straightforward because \(R\)
    is upper triangular via back substitution.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 6- 由于 \(R\) 通过回代是上三角的，解这个系统对于 \(\mathbf{x}^*\) 来说是直接的。
- en: '**THEOREM** **(Least Squares via QR)** \(\idx{least squares via QR}\xdi\) Let
    \(A \in \mathbb{R}^{n\times m}\) be an \(n\times m\) matrix with linearly independent
    columns, let \(\mathbf{b} \in \mathbb{R}^n\) be a vector, and let \(A = QR\) be
    a QR decomposition of \(A\). The solution to the linear least-squares problem'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(通过 QR 的最小二乘法)** \(\idx{least squares via QR}\xdi\) 设 \(A \in \mathbb{R}^{n\times
    m}\) 是一个 \(n\times m\) 矩阵，其列线性无关，设 \(\mathbf{b} \in \mathbb{R}^n\) 是一个向量，设 \(A
    = QR\) 是 \(A\) 的 QR 分解。线性最小二乘问题的解'
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2. \]
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2. \]
- en: satisfies
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 满足
- en: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: \[ R \mathbf{x}^* = Q^T \mathbf{b}. \]
- en: \(\sharp\)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: Note that, in reality, we do not need to form the matrix \(Q Q^T\).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，实际上我们不需要形成矩阵 \(Q Q^T\)。
- en: We implement the QR approach to least squares.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了最小二乘法的 QR 方法。
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**NUMERICAL CORNER:** We return to our simple overdetermined system example.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们回到我们的简单超定系统示例。'
- en: '[PRE29]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: \(\unlhd\)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 2.4.3\. Householder transformations[#](#householder-transformations "Link to
    this heading")
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4.3\. Householder变换[#](#householder-transformations "链接到这个标题")
- en: 'While the Gram-Schmidt algorithm gives a natural way to compute a (reduced)
    QR decomposition, there are many other numerical algorithms for this purpose.
    Some have better numerical behavior, specifically in terms of how they handle
    roundoff error. Quoting [Wikipedia](https://en.wikipedia.org/wiki/Round-off_error):'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Gram-Schmidt算法提供了一种自然的方式来计算（简化的）QR分解，但还有许多其他数值算法用于此目的。其中一些在处理舍入误差方面表现更好。引用[Wikipedia](https://en.wikipedia.org/wiki/Round-off_error)：
- en: A roundoff error, also called rounding error, is the difference between the
    result produced by a given algorithm using exact arithmetic and the result produced
    by the same algorithm using finite-precision, rounded arithmetic. Rounding errors
    are due to inexactness in the representation of real numbers and the arithmetic
    operations done with them. […] When a sequence of calculations with an input involving
    roundoff error are made, errors may accumulate, sometimes dominating the calculation.
  id: totrans-333
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 舍入误差，也称为舍入误差，是使用精确算术和有限精度、舍入算术产生的相同算法的结果之间的差异。舍入误差是由于实数表示的不精确性和与之相关的算术运算造成的。
    [...] 当进行涉及舍入误差的输入的计算序列时，错误可能会累积，有时会主导计算。
- en: We will not prove this here, but the following method based on Householder reflections
    is numerically more [stable](https://en.wikipedia.org/wiki/Numerical_stability#Stability_in_numerical_linear_algebra).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不证明这一点，但以下基于Householder反射的方法在数值上更[稳定](https://en.wikipedia.org/wiki/Numerical_stability#Stability_in_numerical_linear_algebra)。
- en: Recall that a square matrix \(Q \in \mathbb{R}^{m\times m}\) is orthogonal if
    \(Q^T Q = Q Q^T = I_{m \times m}\). In words, the matrix inverse of \(Q\) is its
    transpose. This is equivalent to the columns of \(Q\) forming an orthonormal basis
    of \(\mathbb{R}^m\) (why?).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，一个方阵 \(Q \in \mathbb{R}^{m\times m}\) 是正交的，如果 \(Q^T Q = Q Q^T = I_{m \times
    m}\)。换句话说，\(Q\) 的矩阵逆就是它的转置。这等价于 \(Q\) 的列构成 \(\mathbb{R}^m\) 的一个正交基（为什么？）。
- en: It can be shown that the product of two orthogonal matrices \(Q_1\) and \(Q_2\)
    is also orthogonal. (Try it!)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明两个正交矩阵 \(Q_1\) 和 \(Q_2\) 的乘积也是正交的。（试试看！）
- en: 'An important property of orthogonal matrices is that they preserve inner products:
    if \(Q \in \mathbb{R}^{m\times m}\) is orthogonal, then for any \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^m\)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 正交矩阵的一个重要性质是它们保持内积：如果 \(Q \in \mathbb{R}^{m\times m}\) 是正交的，那么对于任何 \(\mathbf{x},
    \mathbf{y} \in \mathbb{R}^m\)
- en: \[ \langle Q \mathbf{x}, Q \mathbf{y} \rangle = (Q \mathbf{x})^T Q \mathbf{y}
    = \mathbf{x}^T Q^T Q \mathbf{y} = \mathbf{x}^T \mathbf{y} = \langle \mathbf{x},
    \mathbf{y} \rangle. \]
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \langle Q \mathbf{x}, Q \mathbf{y} \rangle = (Q \mathbf{x})^T Q \mathbf{y}
    = \mathbf{x}^T Q^T Q \mathbf{y} = \mathbf{x}^T \mathbf{y} = \langle \mathbf{x},
    \mathbf{y} \rangle. \]
- en: In particular, orthogonal matrices preserve norms and angles.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，正交矩阵保持范数和角度。
- en: '**Reflections** One such family of transformations are reflections.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '**反射** 其中一类变换是反射。'
- en: '**DEFINITION** **(Hyperplane)** \(\idx{hyperplane}\xdi\) A hyperplane \(W\)
    is a linear subspace of \(\mathbb{R}^m\) of dimension \(m-1\). \(\natural\)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(超平面)** \(\idx{hyperplane}\xdi\) 超平面 \(W\) 是 \(\mathbb{R}^m\) 的一个维度为
    \(m-1\) 的线性子空间。\(\natural\)'
- en: '**DEFINITION** **(Householder Reflection)** \(\idx{Householder reflection}\xdi\)
    Let \(\mathbf{z} \in \mathbb{R}^m\) be a unit vector and let \(W\) be the hyperplane
    orthogonal to it. The reflection across \(W\) is given by'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(Householder反射)** \(\idx{Householder reflection}\xdi\) 设 \(\mathbf{z}
    \in \mathbb{R}^m\) 为一个单位向量，设 \(W\) 为与它正交的超平面。通过 \(W\) 的反射由以下给出'
- en: \[ H = I_{m \times m} - 2 \mathbf{z} \mathbf{z}^T. \]
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H = I_{m \times m} - 2 \mathbf{z} \mathbf{z}^T. \]
- en: This is referred to as a Householder reflection. \(\natural\)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为Householder反射。\(\natural\)
- en: In words, we subtract twice the projection onto \(\mathbf{z}\), as depicted
    below.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们减去两次在 \(\mathbf{z}\) 上的投影，如下所示。
- en: '![Householder reflection. (with the help of Claude; inspired by (Source))](../Images/7c283ce7dd76917f07050963bca3ee2b.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![Householder反射. (在Claude的帮助下；受(SOURCE)的启发)](../Images/7c283ce7dd76917f07050963bca3ee2b.png)'
- en: '**LEMMA** Let \(H = I_{m\times m} - 2\mathbf{z}\mathbf{z}^T\) be a Householder
    reflection. Then \(H\) is an orthogonal matrix. \(\flat\)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** 设 \(H = I_{m\times m} - 2\mathbf{z}\mathbf{z}^T\) 为一个Householder反射。那么
    \(H\) 是一个正交矩阵。\(\flat\)'
- en: '*Proof:* We check the definition:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 我们检查定义：'
- en: \[\begin{align*} H^T H &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)^T (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T) (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T
    + 4 \mathbf{z}\mathbf{z}^T\mathbf{z}\mathbf{z}^T\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T
    - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T \end{align*}\]
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} H^T H &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T)^T (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= (I_{m\times m} - 2\mathbf{z}\mathbf{z}^T) (I_{m\times
    m} - 2\mathbf{z}\mathbf{z}^T)\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T - 2\mathbf{z}\mathbf{z}^T
    + 4 \mathbf{z}\mathbf{z}^T\mathbf{z}\mathbf{z}^T\\ &= I_{m\times m} - 2\mathbf{z}\mathbf{z}^T
    - 2\mathbf{z}\mathbf{z}^T + 4 \mathbf{z}\mathbf{z}^T \end{align*}\]
- en: which is equal to \(I_{m\times m}\). The calculation for \(H H^T\) is the same.\(\square\)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这等于 \(I_{m\times m}\)。计算 \(H H^T\) 的过程是相同的。\(\square\)
- en: '**QR decomposition by introducing zeros** We return to QR decompositions. One
    way to construct a (full) QR decomposition of a matrix \(A \in \mathbb{R}^{n \times
    m}\) is to find a sequence of orthogonal matrices \(H_1, \ldots, H_m\) that triangularize
    \(A\):'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过引入零进行 QR 分解** 我们回到 QR 分解。构造矩阵 \(A \in \mathbb{R}^{n \times m}\) 的 (满) QR
    分解的一种方法是找到一个正交矩阵序列 \(H_1, \ldots, H_m\)，使得 \(A\) 三角化：'
- en: \[ H_m \cdots H_2 H_1 A = R \]
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_m \cdots H_2 H_1 A = R \]
- en: for an upper-triangular matrix \(R\). Indeed, by the properties of orthogonal
    matrices, we then have
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上三角矩阵 \(R\)。事实上，根据正交矩阵的性质，我们随后有
- en: \[ A = H_1^T H_2^T \cdots H_m^T H_m \cdots H_2 H_1 A = H_1^T H_2^T \cdots H_m^T
    R \]
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = H_1^T H_2^T \cdots H_m^T H_m \cdots H_2 H_1 A = H_1^T H_2^T \cdots H_m^T
    R \]
- en: 'where \(Q = H_1^T H_2^T \cdots H_m^T\) is itself orthogonal as a product of
    orthogonal matrices. So to proceed we need to identify orthogonal matrices that
    have the effect of introducing zeros below the diagonal, as illustrated below:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(Q = H_1^T H_2^T \cdots H_m^T\) 本身作为正交矩阵的乘积也是正交的。因此，为了继续进行，我们需要识别出具有在主对角线下方引入零的效果的正交矩阵，如图所示：
- en: \[\begin{split} H_2 H_1 A = \begin{pmatrix} \times & \times & \times & \times
    & \times\\ 0 & \times & \times & \times & \times\\ 0 & 0 & \times & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ 0 & 0 & \boxed{\times} & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ \end{pmatrix}. \end{split}\]
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_2 H_1 A = \begin{pmatrix} \times & \times & \times & \times
    & \times\\ 0 & \times & \times & \times & \times\\ 0 & 0 & \times & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ 0 & 0 & \boxed{\times} & \times & \times\\
    0 & 0 & \boxed{\times} & \times & \times\\ \end{pmatrix}. \end{split}\]
- en: It turns out that a well-chosen Householder reflection does the trick. Let \(\mathbf{y}_1\)
    be the first column of \(A\) and take
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，一个精心选择的豪斯霍尔德反射可以做到这一点。设 \(\mathbf{y}_1\) 为 \(A\) 的第一列，并取
- en: \[ \mathbf{z}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1}{\|\|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|} \quad \text{and} \quad H_1 = I_{n\times
    n} - 2\mathbf{z}_1\mathbf{z}_1^T \]
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1}{\|\|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|} \quad \text{and} \quad H_1 = I_{n\times
    n} - 2\mathbf{z}_1\mathbf{z}_1^T \]
- en: where \(\mathbf{e}_1^{(n)}\) is the first vector in the canonical basis of \(\mathbb{R}^n\).
    As depicted below, this choice sends \(\mathbf{y}_1\) to
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{e}_1^{(n)}\) 是 \(\mathbb{R}^n\) 的标准基中的第一个向量。如图所示，这个选择将 \(\mathbf{y}_1\)
    映射到
- en: \[\begin{split} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)} = \begin{pmatrix} \|\mathbf{y}_1\|\\
    0 \\ \vdots \\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)} = \begin{pmatrix} \|\mathbf{y}_1\|\\
    0 \\ \vdots \\ 0 \end{pmatrix}. \end{split}\]
- en: (It is clear that if \(H_1 \mathbf{y}_1\) is proportional to \(\mathbf{e}_1^{(n)}\),
    than it can only be \(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\) or \(-\|\mathbf{y}_1\|
    \mathbf{e}_1^{(n)}\). Prove it!)
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: (显然，如果 \(H_1 \mathbf{y}_1\) 与 \(\mathbf{e}_1^{(n)}\) 成比例，那么它只能是 \(\|\mathbf{y}_1\|
    \mathbf{e}_1^{(n)}\) 或 \(-\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)。证明它！)
- en: '![Introducing zeros by Householder reflection (with the help of Claude; inspired
    by (Source))](../Images/19bf9b4f1f98bc25a2715d91da61fc5b.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![通过豪斯霍尔德反射引入零（在克劳德的帮助下；受（来源）启发）](../Images/19bf9b4f1f98bc25a2715d91da61fc5b.png)'
- en: '**LEMMA** **(Householder)** \(\idx{Householder lemma}\xdi\) Let \(\mathbf{y}_1\),
    \(\mathbf{z}_1\) and \(H_1\) be as above. Then'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(豪斯霍尔德)** \(\idx{Householder lemma}\xdi\) 设 \(\mathbf{y}_1\), \(\mathbf{z}_1\)
    和 \(H_1\) 如上所述。那么'
- en: \[ H_1 \mathbf{y}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}. \]
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_1 \mathbf{y}_1 = \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}. \]
- en: \(\flat\)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof idea:* The proof by picture is in the figure above.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* 证明的图示见上图。'
- en: '*Proof:* Note that'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 注意到'
- en: \[\begin{align*} \|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2
    &= (\|\mathbf{y}_1\| - y_{1,1})^2 + \sum_{j=2}^n y_{1,j}^2\\ &= \|\mathbf{y}_1\|^2
    -2 \|\mathbf{y}_1\| y_{1,1} + y_{1,1}^2 + \sum_{j=2}^n y_{1,j}^2\\ &= 2(\|\mathbf{y}_1\|^2
    - \|\mathbf{y}_1\| y_{1,1}) \end{align*}\]
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|^2
    &= (\|\mathbf{y}_1\| - y_{1,1})^2 + \sum_{j=2}^n y_{1,j}^2\\ &= \|\mathbf{y}_1\|^2
    -2 \|\mathbf{y}_1\| y_{1,1} + y_{1,1}^2 + \sum_{j=2}^n y_{1,j}^2\\ &= 2(\|\mathbf{y}_1\|^2
    - \|\mathbf{y}_1\| y_{1,1}) \end{align*}\]
- en: and
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} 2 \mathbf{z}_1 \mathbf{z}_1^T \mathbf{y}_1 &= 2 \mathbf{z}_1
    \frac{\|\mathbf{y}_1\| \,(\mathbf{e}_1^{(n)})^T \mathbf{y}_1 - \mathbf{y}_1^T
    \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}\\ &= 2
    \frac{\|\mathbf{y}_1\| y_{1,1} - \|\mathbf{y}_1\|^2}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}
    - \mathbf{y}_1\|^2} (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)\\ &=
    - (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) \end{align*}\]
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} 2 \mathbf{z}_1 \mathbf{z}_1^T \mathbf{y}_1 &= 2 \mathbf{z}_1
    \frac{\|\mathbf{y}_1\| \,(\mathbf{e}_1^{(n)})^T \mathbf{y}_1 - \mathbf{y}_1^T
    \mathbf{y}_1}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1\|}\\ &= 2
    \frac{\|\mathbf{y}_1\| y_{1,1} - \|\mathbf{y}_1\|^2}{\|\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}
    - \mathbf{y}_1\|^2} (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1)\\ &=
    - (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) \end{align*}\]
- en: where we used the previous equation. Hence
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了之前的方程。因此
- en: \[ H_1 \mathbf{y}_1 = (I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T) \,\mathbf{y}_1
    = \mathbf{y}_1 + (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) = \|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)}. \]
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_1 \mathbf{y}_1 = (I_{n\times n} - 2\mathbf{z}_1\mathbf{z}_1^T) \,\mathbf{y}_1
    = \mathbf{y}_1 + (\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} - \mathbf{y}_1) = \|\mathbf{y}_1\|
    \,\mathbf{e}_1^{(n)}. \]
- en: That establishes the claim. \(\square\)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这就证明了该命题。 \(\square\)
- en: The upshot is that multiplying \(A\) by \(H_1\) introduces zeros below the diagonal
    in the first column. To see this, recall that one interpretation of the matrix-matrix
    product is that each column of the second matrix gets multiplied by the first
    one. By the *Householder Lemma*, applying \(H_1\) to \(A\) gives
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，将 \(A\) 乘以 \(H_1\) 在第一列下方引入了零。为了看到这一点，回想一下矩阵-矩阵乘积的一种解释是，第二个矩阵的每一列都乘以第一个矩阵。根据**豪斯霍尔德引理**，将
    \(H_1\) 应用到 \(A\) 上给出
- en: \[ H_1 A = \begin{pmatrix} H_1 \mathbf{y}_1 & H_1 A_{\cdot,2} & \cdots & H_1
    A_{\cdot,m} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}
    & H_1 A_{\cdot,2} & \cdots & H_1 A_{\cdot,m} \end{pmatrix} \]
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H_1 A = \begin{pmatrix} H_1 \mathbf{y}_1 & H_1 A_{\cdot,2} & \cdots & H_1
    A_{\cdot,m} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| \mathbf{e}_1^{(n)}
    & H_1 A_{\cdot,2} & \cdots & H_1 A_{\cdot,m} \end{pmatrix} \]
- en: So the first column is now proportional to \(\mathbf{e}_1\), which has zeros
    in all but the first element. (What should we do if \(\mathbf{y}_1\) is already
    equal to \(\|\mathbf{y}_1\| \mathbf{e}_1^{(n)}\)?)
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一列现在与 \(\mathbf{e}_1\) 成正比，除了第一个元素外，其余元素都是零。（如果 \(\mathbf{y}_1\) 已经等于 \(\|\mathbf{y}_1\|
    \mathbf{e}_1^{(n)}\)，我们应该怎么办？）
- en: It turns that there is another choice of Householder reflection. Indeed, it
    can be shown that
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，还有另一种豪斯霍尔德反射的选择。实际上，可以证明
- en: \[ \tilde{\mathbf{z}}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1}{\|
    \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1\|} \quad \text{and} \quad
    \tilde{H}_1 = I_{n\times n} - 2\tilde{\mathbf{z}}_1 \tilde{\mathbf{z}}_1^T \]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \tilde{\mathbf{z}}_1 = \frac{\|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1}{\|
    \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)} + \mathbf{y}_1\|} \quad \text{和} \quad \tilde{H}_1
    = I_{n\times n} - 2\tilde{\mathbf{z}}_1 \tilde{\mathbf{z}}_1^T \]
- en: are such that \(\tilde{H}_1 \mathbf{y}_1 = - \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}\)
    (try it!).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: \(\tilde{H}_1 \mathbf{y}_1 = - \|\mathbf{y}_1\| \,\mathbf{e}_1^{(n)}\)（试试看！）
- en: '**Putting everything together** We have shown how to introduce zeros below
    the diagonal in the first column of a matrix. To introduce zeros in the second
    column below the diagonal we use a block matrix. Recall that if \(A_{ij} \in \mathbb{R}^{n_i
    \times m_j}\) and \(B_{ij} \in \mathbb{R}^{m_i \times p_j}\) for \(i,j = 1, 2\),
    then we have the following formula'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**将所有内容整合在一起** 我们已经展示了如何在矩阵的第一列下方引入零。要在第二列下方引入零，我们使用一个分块矩阵。回想一下，如果 \(A_{ij}
    \in \mathbb{R}^{n_i \times m_j}\) 且 \(B_{ij} \in \mathbb{R}^{m_i \times p_j}\)
    对于 \(i,j = 1, 2\)，那么我们有以下公式'
- en: \[\begin{split} \begin{pmatrix} A_{11} & A_{12}\\ A_{21} & A_{22} \end{pmatrix}
    \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix} = \begin{pmatrix}
    A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\ A_{21} B_{11}
    + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22} \end{pmatrix}. \end{split}\]
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{pmatrix} A_{11} & A_{12}\\ A_{21} & A_{22} \end{pmatrix}
    \begin{pmatrix} B_{11} & B_{12}\\ B_{21} & B_{22} \end{pmatrix} = \begin{pmatrix}
    A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\ A_{21} B_{11}
    + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22} \end{pmatrix}. \end{split}\]
- en: Now consider the following block matrix
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下分块矩阵
- en: \[\begin{split} H_2 = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2 \end{pmatrix}
    \end{split}\]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_2 = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2 \end{pmatrix}
    \end{split}\]
- en: where \(F_2\) is the following Householder reflection. Write the second column
    of \(H_1 A\) as \((y^{(2)}, \mathbf{y}_2)\). That is, \(\mathbf{y}_2\) are the
    entries \(2,\ldots, n\) of that column. Define
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(F_2\) 是以下豪斯霍尔德反射。将 \(H_1 A\) 的第二列写成 \((y^{(2)}, \mathbf{y}_2)\)。也就是说，\(\mathbf{y}_2\)
    是该列的第 \(2,\ldots, n\) 个条目。定义
- en: \[ F_2 = I_{(n-1) \times (n-1)} - 2 \mathbf{z}_2 \mathbf{z}_2^T \quad \text{with}
    \quad \mathbf{z}_2 = \frac{\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2}{\|\|\mathbf{y}_2\|
    \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2\|} \]
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F_2 = I_{(n-1) \times (n-1)} - 2 \mathbf{z}_2 \mathbf{z}_2^T \quad \text{with}
    \quad \mathbf{z}_2 = \frac{\|\mathbf{y}_2\| \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2}{\|\|\mathbf{y}_2\|
    \,\mathbf{e}_1^{(n-1)} - \mathbf{y}_2\|} \]
- en: where now \(\mathbf{e}_1^{(n-1)} \in \mathbb{R}^{n-1}\). By the *Householder
    Lemma*, we have \(F_2 \mathbf{y}_2 = \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}\).
    It can be shown that \(\mathbf{y}_2 \neq \mathbf{0}\) when the columns of \(A\)
    are linearly independent. (Try it!)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 其中现在 \(\mathbf{e}_1^{(n-1)} \in \mathbb{R}^{n-1}\)。根据 *豪斯霍尔德引理*，我们有 \(F_2 \mathbf{y}_2
    = \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}\)。可以证明，当 \(A\) 的列线性无关时，\(\mathbf{y}_2
    \neq \mathbf{0}\)。（试试看！）
- en: Applying \(H_2\) to \(H_1 A\) preserves the first row and column, and introduces
    zeros under the diagonal in the second column. To see this, first re-write \(H_1
    A\) in block form
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 \(H_2\) 到 \(H_1 A\) 保持第一行和列，并在第二列的下方引入零。为了看到这一点，首先将 \(H_1 A\) 重写为分块形式
- en: \[\begin{split} H_1 A = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\
    \mathbf{0} & G_2 \end{pmatrix} \end{split}\]
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_1 A = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\
    \mathbf{0} & G_2 \end{pmatrix} \end{split}\]
- en: 'where we used our previous observation about the first column of \(H_1 A\)
    and where \(\mathbf{g}_2 \in \mathbb{R}^{m-1}\), \(G_2 \in \mathbb{R}^{(n-1)\times
    (m-1)}\). One important point to note: the first column of \(G_2\) is equal to
    \(\mathbf{y}_2\). Now multiply by \(H_2\) to get'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了我们之前关于 \(H_1 A\) 的第一列的观察，并且 \(\mathbf{g}_2 \in \mathbb{R}^{m-1}\)，\(G_2
    \in \mathbb{R}^{(n-1)\times (m-1)}\)。一个需要注意的重要点是：\(G_2\) 的第一列等于 \(\mathbf{y}_2\)。现在乘以
    \(H_2\) 得到
- en: \[\begin{split} H_2 H_1 A = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2
    \end{pmatrix} \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & G_2 \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & F_2 G_2 \end{pmatrix}. \end{split}\]
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_2 H_1 A = \begin{pmatrix} 1 & \mathbf{0} \\ \mathbf{0} & F_2
    \end{pmatrix} \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & G_2 \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_1\| & \mathbf{g}_2^T \\ \mathbf{0}
    & F_2 G_2 \end{pmatrix}. \end{split}\]
- en: Computing the block \(F_2 G_2\) column by column we get
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 按列计算分块 \(F_2 G_2\) 我们得到
- en: \[ F_2 G_2 = \begin{pmatrix} F_2 \mathbf{y}_2 & F_2 (G_2)_{\cdot,2} & \cdots
    & F_2 (G_2)_{\cdot,m-1} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}
    & F_2 (G_2)_{\cdot,2} & \cdots & F_2 (G_2)_{\cdot,m-1}, \end{pmatrix} \]
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F_2 G_2 = \begin{pmatrix} F_2 \mathbf{y}_2 & F_2 (G_2)_{\cdot,2} & \cdots
    & F_2 (G_2)_{\cdot,m-1} \end{pmatrix} = \begin{pmatrix} \|\mathbf{y}_2\| \mathbf{e}_1^{(n-1)}
    & F_2 (G_2)_{\cdot,2} & \cdots & F_2 (G_2)_{\cdot,m-1}, \end{pmatrix} \]
- en: where \((G_2)_{\cdot,j}\) is the \(j\)-th column of \(G_2\). So the second column
    of \(H_2 H_1 A\) has zeros in all but the first two elements.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \((G_2)_{\cdot,j}\) 是 \(G_2\) 的第 \(j\) 列。因此，\(H_2 H_1 A\) 的第二列除了前两个元素外都是零。
- en: And so on. At Step \(k\), we split the \(k\)-th column of \(H_{k-1} \cdots H_1
    A\) into its first \(k-1\) and last \(n-k+1\) entries \((\mathbf{y}^{(k)}, \mathbf{y}_k)\)
    and form the matrix
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。在第 \(k\) 步，我们将 \(H_{k-1} \cdots H_1 A\) 的第 \(k\) 列拆分为其前 \(k-1\) 个和最后 \(n-k+1\)
    个条目 \((\mathbf{y}^{(k)}, \mathbf{y}_k)\)，并形成矩阵
- en: \[\begin{split} H_k = \begin{pmatrix} I_{(k-1)\times (k-1)} & \mathbf{0} \\
    \mathbf{0} & F_k \end{pmatrix} \end{split}\]
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} H_k = \begin{pmatrix} I_{(k-1)\times (k-1)} & \mathbf{0} \\
    \mathbf{0} & F_k \end{pmatrix} \end{split}\]
- en: where
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[ F_k = I_{(n-k+1) \times (n-k+1)} - 2 \mathbf{z}_k \mathbf{z}_k^T \quad \text{with}
    \quad \mathbf{z}_k = \frac{\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k}{\|\|\mathbf{y}_k\|
    \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k\|}. \]
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: \[ F_k = I_{(n-k+1) \times (n-k+1)} - 2 \mathbf{z}_k \mathbf{z}_k^T \quad \text{with}
    \quad \mathbf{z}_k = \frac{\|\mathbf{y}_k\| \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k}{\|\|\mathbf{y}_k\|
    \,\mathbf{e}_1^{(n-k+1)} - \mathbf{y}_k\|}. \]
- en: This time the first \(k-1\) rows and columns are preserved, while zeros are
    introduced under the diagonal of the \(k\)-th column. We omit the details (try
    it!).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这次保留了前 \(k-1\) 行和列，而在第 \(k\) 列的下方引入了零。我们省略了细节（试试看！）
- en: We implement the procedure above in Python. We will need the following function.
    For \(\alpha \in \mathbb{R}\), let the sign of \(\alpha\) be
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现上述过程。我们需要以下函数。对于 \(\alpha \in \mathbb{R}\)，让 \(\alpha\) 的符号为
- en: \[\begin{split} \mathrm{sign}(\alpha) = \begin{cases} 1 & \text{if $\alpha >
    0$}\\ 0 & \text{if $\alpha = 0$}\\ -1 & \text{if $\alpha < 0$} \end{cases} \end{split}\]
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathrm{sign}(\alpha) = \begin{cases} 1 & \text{if $\alpha >
    0$}\\ 0 & \text{if $\alpha = 0$}\\ -1 & \text{if $\alpha < 0$} \end{cases} \end{split}\]
- en: In Python, this is done using the function [`numpy.sign`](https://numpy.org/doc/stable/reference/generated/numpy.sign.html).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，这是通过使用函数 `numpy.sign` ([numpy.sign](https://numpy.org/doc/stable/reference/generated/numpy.sign.html))
    来实现的。
- en: The following function constructs the upper triangular matrix \(R\) by iteratively
    modifying the relevant block of \(A\). On the other hand, computing the matrix
    \(Q\) actually requires extra computational work that is often not needed. We
    saw that, in the context of the least-squares problem, we really only need to
    compute \(Q^T \mathbf{b}\) for some input vector \(\mathbf{b}\). This can be done
    at the same time that \(R\) is constructed, as follows. The key point to note
    is that \(Q^T \mathbf{b} = H_m \cdots H_1 \mathbf{b}\).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数通过迭代修改 \(A\) 的相关块来构建上三角矩阵 \(R\)。另一方面，计算矩阵 \(Q\) 实际上需要额外的计算工作，这通常是不必要的。我们注意到，在最小二乘问题的背景下，我们实际上只需要为某些输入向量
    \(\mathbf{b}\) 计算 \(Q^T \mathbf{b}\)。这可以在构建 \(R\) 的同时完成，如下所示。需要注意的是，\(Q^T \mathbf{b}
    = H_m \cdots H_1 \mathbf{b}\)。
- en: 'In our implementation of `householder`, we use both reflections defined above.
    We will not prove this here, but the particular choice made has good numerical
    properties. Quoting [TB, Lecture 10] (where \(H^+\) refers to the hyperplane used
    for the reflection when \(z=1\)):'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现 `householder` 的过程中，我们使用了上面定义的两个反射。我们在此不进行证明，但所做出的特定选择具有良好的数值特性。引用 [TB,
    Lecture 10]（其中 \(H^+\) 指的是当 \(z=1\) 时用于反射的超平面）：
- en: Mathematically, either choice of sign is satisfactory. However, this is a case
    where numerical stability – insensitivity to rounding errors – dictates that one
    choice should be taken rather than the other. For numerical stability, it is desirable
    to reflect \(\mathbf{x}\) to the vector \(z \|\mathbf{x}\| \mathbf{e}_1\) that
    is not too close to \(\mathbf{x}\) itself. […] Suppose that [in the figure above]
    the angle between \(H^+\) and the \(\mathbf{e}_1\) axis is very small. Then the
    vector \(\mathbf{v} = \|\mathbf{x}\| \mathbf{e}_1 - \mathbf{x}\) is much smaller
    than \(\mathbf{x}\) or \(\|\mathbf{x}\| \mathbf{e}_1\). Thus the calculation of
    \(\mathbf{v}\) represents a subtraction of nearby quantities and will tend to
    suffer from cancellation errors.
  id: totrans-404
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从数学上讲，两种符号的选择都是令人满意的。然而，这是一个数值稳定性——对舍入误差不敏感——决定应该选择一种而不是另一种的情况。为了数值稳定性，最好将 \(\mathbf{x}\)
    反射到 \(z \|\mathbf{x}\| \mathbf{e}_1\) 这个向量上，这个向量不太接近 \(\mathbf{x}\) 本身。 [...]
    假设在上面的图中，\(H^+\) 和 \(\mathbf{e}_1\) 轴之间的角度非常小。那么向量 \(\mathbf{v} = \|\mathbf{x}\|
    \mathbf{e}_1 - \mathbf{x}\) 比较小，比 \(\mathbf{x}\) 或 \(\|\mathbf{x}\| \mathbf{e}_1\)
    都要小得多。因此，\(\mathbf{v}\) 的计算代表了附近量的减法，并且可能会受到舍入误差的影响。
- en: We use [`numpy.outer`](https://numpy.org/doc/stable/reference/generated/numpy.outer.html)
    to compute \(\mathbf{z} \mathbf{z}^T\), which is referred to as an outer product.
    See [here](https://numpy.org/doc/stable/reference/generated/numpy.copy.html) for
    an explanation of `numpy.copy`.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `numpy.outer` ([numpy.outer](https://numpy.org/doc/stable/reference/generated/numpy.outer.html))
    来计算 \(\mathbf{z} \mathbf{z}^T\)，这被称为外积。有关 `numpy.copy` 的解释，请参阅 [这里](https://numpy.org/doc/stable/reference/generated/numpy.copy.html)。
- en: '[PRE31]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**NUMERICAL CORNER:** We return to our overdetermined system example.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们回到我们的超定系统示例。'
- en: '[PRE32]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: One advantage of the Householder approach is that it produces a matrix \(Q\)
    with very good orthogonality, i.e., \(Q^T Q \approx I\). We give a quick example
    below comparing Gram-Schmidt and Householder. (The choice of matrix \(A\) will
    become clearer when we discuss the singular value decomposition in a later chapter.)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: Householder 方法的一个优点是它产生了一个具有非常好的正交性的矩阵 \(Q\)，即 \(Q^T Q \approx I\)。下面我们给出一个快速示例，比较
    Gram-Schmidt 和 Householder。 (当我们讨论后续章节中的奇异值分解时，矩阵 \(A\) 的选择将变得更加清晰。)
- en: '[PRE34]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As you can see above, the \(Q\) and \(R\) factors produced by the Gram-Schmidt
    algorithm do have the property that \(QR \approx A\). However, \(Q\) is far from
    orthogonal. (Recall that `LA.norm` computes the Frobenius norm introduced previously.)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，Gram-Schmidt 算法产生的 \(Q\) 和 \(R\) 因子确实具有 \(QR \approx A\) 的性质。然而，\(Q\)
    远非正交的。 (回想一下，`LA.norm` 计算了之前引入的 Frobenius 范数。)
- en: On the other hand, Householder reflections perform much better in that respect
    as we show next. Here we use the implementation of Householder transformations
    in [`numpy.linalg.qr`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.qr.html).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，正如我们接下来要展示的，豪斯霍尔德反射在这方面表现得更好。这里我们使用 `numpy.linalg.qr` 中的豪斯霍尔德变换实现。
- en: '[PRE36]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: \(\unlhd\)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(有克莱德、双子星和 ChatGPT 的帮助)*'
- en: '**1** Which of the following matrices is upper triangular?'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 以下哪个矩阵是上三角矩阵？'
- en: a) \(\begin{pmatrix} 1 & 2 \\ 0 & 3 \end{pmatrix}\)
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\begin{pmatrix} 1 & 2 \\ 0 & 3 \end{pmatrix}\)
- en: b) \(\begin{pmatrix} 1 & 0 \\ 2 & 3 \end{pmatrix}\)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\begin{pmatrix} 1 & 0 \\ 2 & 3 \end{pmatrix}\)
- en: c) \(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\)
- en: d) \(\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\)
- en: '**2** What is the output of the Gram-Schmidt algorithm when applied to a set
    of linearly independent vectors?'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 将 Gram-Schmidt 算法应用于一组线性无关向量时，其输出是什么？'
- en: a) An orthogonal basis for the subspace spanned by the vectors.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: a) 由这些向量张成的子空间的正交基。
- en: b) An orthonormal basis for the subspace spanned by the vectors.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: b) 由这些向量张成的子空间的正交基。
- en: c) A set of linearly dependent vectors.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: c) 一组线性相关向量。
- en: d) A single vector that is orthogonal to all the input vectors.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: d) 一个与所有输入向量都正交的单个向量。
- en: '**3** What is the dimension of a hyperplane in \(\mathbb{R}^m\)?'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** \(\mathbb{R}^m\) 中超平面的维度是多少？'
- en: a) \(m\)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(m\)
- en: b) \(m - 1\)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(m - 1\)
- en: c) \(m - 2\)
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(m - 2\)
- en: d) 1
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: d) 1
- en: '**4** Which of the following is true about a Householder reflection?'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 关于豪斯霍尔德反射，以下哪个说法是正确的？'
- en: a) It is a reflection across a hyperplane orthogonal to a unit vector.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它是一个单位向量正交的超平面上的反射。
- en: b) It is a reflection across a unit vector.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它是一个单位向量上的反射。
- en: c) It is a rotation around a hyperplane orthogonal to a unit vector.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它是一个围绕单位向量正交的超平面的旋转。
- en: d) It is a rotation around a unit vector.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它是一个围绕单位向量的旋转。
- en: '**5** How can a sequence of Householder reflections be used to compute a QR
    decomposition of a matrix \(A\)?'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 如何使用一系列豪斯霍尔德反射来计算矩阵 \(A\) 的 QR 分解？'
- en: a) By iteratively introducing zeros above the diagonal of \(A\).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: a) 通过在 \(A\) 的对角线上方迭代引入零。
- en: b) By iteratively introducing zeros below the diagonal of \(A\).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: b) 通过在 \(A\) 的对角线下方迭代引入零。
- en: c) By iteratively introducing zeros on the diagonal of \(A\).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: c) 通过在 \(A\) 的对角线上迭代引入零。
- en: d) By iteratively introducing zeros in the entire matrix \(A\).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: d) 通过在矩阵 \(A\) 的整个矩阵中迭代引入零。
- en: 'Answer for 1: a. Justification: The text defines an upper triangular matrix
    as one with all entries below the diagonal equal to zero.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：a. 理由：文本定义上三角矩阵为对角线下方的所有元素都等于零的矩阵。
- en: 'Answer for 2: b. Justification: The text states that the Gram-Schmidt algorithm
    “produces an orthonormal basis of \(\mathrm{span}(a_1, \dots, a_m)\).”'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：b. 理由：文本指出，Gram-Schmidt 算法“生成 \(\mathrm{span}(a_1, \dots, a_m)\) 的正交基。”
- en: 'Answer for 3: b. Justification: The text defines a hyperplane as a linear subspace
    of \(\mathbb{R}^m\) of dimension \(m - 1\).'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：b. 理由：文本定义超平面为 \(\mathbb{R}^m\) 的一个维度为 \(m - 1\) 的线性子空间。
- en: 'Answer for 4: a. Justification: The text defines a Householder reflection as
    follows: “Let \(\mathbf{z} \in \mathbb{R}^m\) be a unit vector and let \(W\) be
    the hyperplane orthogonal to it. The reflection across \(W\) is given by \(H =
    I_{m \times m} - 2\mathbf{z}\mathbf{z}^T\).”'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：a. 理由：文本如下定义豪斯霍尔德反射：“设 \(\mathbf{z} \in \mathbb{R}^m\) 是一个单位向量，设 \(W\)
    是与它正交的超平面。\(W\) 上的反射由 \(H = I_{m \times m} - 2\mathbf{z}\mathbf{z}^T\) 给出。”
- en: 'Answer for 5: b. Justification: The text states, “One way to construct a (full)
    QR decomposition of a matrix \(A \in \mathbb{R}^{n \times m}\) is to find a sequence
    of orthogonal matrices \(H_1,\ldots,H_m\) that triangularize \(A\): \(H_m \cdots
    H_2H_1A = R\) for an upper-triangular matrix \(R\).”'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：b. 理由：文本指出，“构造矩阵 \(A \in \mathbb{R}^{n \times m}\) 的（满）QR 分解的一种方法是通过找到一个正交矩阵序列
    \(H_1,\ldots,H_m\)，使得 \(A\) 被三角化：\(H_m \cdots H_2H_1A = R\)，其中 \(R\) 是一个上三角矩阵。”
