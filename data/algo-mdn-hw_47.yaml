- en: External Sorting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部排序
- en: 原文：[https://en.algorithmica.org/hpc/external-memory/sorting/](https://en.algorithmica.org/hpc/external-memory/sorting/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/external-memory/sorting/](https://en.algorithmica.org/hpc/external-memory/sorting/)
- en: Now, let’s try to design some actually useful algorithms for the new [external
    memory model](../model). Our goal in this section is to slowly build up more complex
    things and eventually get to *external sorting* and its interesting applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试为新的 [外部存储模型](../model) 设计一些真正有用的算法。本节的目标是逐步构建更复杂的东西，并最终达到 *外部排序* 及其有趣的应用。
- en: The algorithm will be based on the standard merge sorting algorithm, so we need
    to derive its main primitive first.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法将基于标准的归并排序算法，因此我们首先需要推导出其主要原语。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#merge)Merge'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#merge)归并'
- en: '**Problem.** Given two sorted arrays $a$ and $b$ of lengths $N$ and $M$, produce
    a single sorted array $c$ of length $N + M$ containing all of their elements.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题。** 给定两个长度为 $N$ 和 $M$ 的已排序数组 $a$ 和 $b$，生成一个包含所有元素的长度为 $N + M$ 的单个已排序数组
    $c$。'
- en: 'The standard two-pointer technique for merging sorted arrays looks like this:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 合并排序数组的标准双指针技术看起来是这样的：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In terms of memory operations, we just linearly read all elements of $a$ and
    $b$ and linearly write all elements of $c$. Since these reads and writes can be
    buffered, it works in $SCAN(N+M)$ I/O operations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存操作方面，我们只是线性地读取 $a$ 和 $b$ 的所有元素，并线性地写入 $c$ 的所有元素。由于这些读取和写入可以缓冲，它可以在 $SCAN(N+M)$
    I/O 操作中工作。
- en: So far the examples have been simple, and their analysis doesn’t differ too
    much from the RAM model, except that we divide the final answer by the block size
    $B$. But here is a case where this is not so.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的例子都很简单，它们的分析并没有太多区别于 RAM 模型，除了我们需要将最终答案除以块大小 $B$。但这里有一个情况，这并不如此。
- en: '**$k$-way merging.** Consider the modification of this algorithm where we need
    to merge not just two arrays, but $k$ arrays of total size $N$ — by likewise looking
    at $k$ values, choosing the minimum between them, writing it into $c$, and incrementing
    one of the iterators.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**$k$-路归并。** 考虑这种算法的修改，我们需要合并的不仅仅是两个数组，而是总大小为 $N$ 的 $k$ 个数组——通过类似地查看 $k$ 个值，选择它们之间的最小值，将其写入
    $c$，并递增一个迭代器。'
- en: In the standard RAM model, the asymptotic complexity would be multiplied $k$,
    since we would need to perform $O(k)$ comparisons to fill each next element. But
    in the external memory model, since everything we do in-memory doesn’t cost us
    anything, its asymptotic complexity would not change as long as we can fit $(k+1)$
    full blocks in memory, that is, if $k = O(\frac{M}{B})$.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的 RAM 模型中，渐近复杂度会乘以 $k$，因为我们需要执行 $O(k)$ 次比较来填充每个下一个元素。但在外部存储模型中，由于我们在内存中做的所有事情都不需要我们付出任何代价，只要我们可以在内存中容纳
    $(k+1)$ 个完整的块，即如果 $k = O(\frac{M}{B})$，其渐近复杂度就不会改变。
- en: Remember [the $M \gg B$ assumption](../model) when we introduced the computational
    model? If we have $M \geq B^{1+ε}$ for $\epsilon > 0$, then we can fit any sub-polynomial
    number of blocks in memory, certainly including $O(\frac{M}{B})$. This condition
    is called *tall cache assumption*, and it is usually required in many other external
    memory algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们介绍计算模型时提到的 [$$M \gg B$$ 假设](../model) 吗？如果我们有 $M \geq B^{1+ε}$ 对于 $\epsilon
    > 0$，那么我们可以将任何次多项式数量的块放入内存中，当然包括 $O(\frac{M}{B})$。这个条件被称为 *高缓存假设*，它在许多其他外部存储算法中通常都是必需的。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#merge-sorting)Merge
    Sorting'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#merge-sorting)归并排序'
- en: 'The “normal” complexity of the standard mergesort algorithm is $O(N \log_2
    N)$: on each of its $O(\log_2 N)$ “layers,” the algorithms need to go through
    all $N$ elements in total and merge them in linear time.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 标准归并排序算法的正常复杂度是 $O(N \log_2 N)$：在其 $O(\log_2 N)$ 的“层”中，算法需要遍历总共 $N$ 个元素并将它们合并成线性时间。
- en: In the external memory model, when we read a block of size $M$, we can sort
    its elements “for free,” since they are already in memory. This way we can split
    the arrays into $O(\frac{N}{M})$ blocks of consecutive elements and sort them
    separately as the base step, and only then merge them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在外部存储模型中，当我们读取一个大小为 $M$ 的块时，我们可以“免费”地对它的元素进行排序，因为它们已经在内存中了。这样我们可以将数组分成 $O(\frac{N}{M})$
    个连续元素的块，并分别对它们进行排序作为基本步骤，然后才合并它们。
- en: '![](../Images/4434bb57f07289c2f870ab1d0b2b9e4c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4434bb57f07289c2f870ab1d0b2b9e4c.png)'
- en: This effectively means that, in terms of I/O operations, the first $O(\log M)$
    layers of mergesort are free, and there are only $O(\log_2 \frac{N}{M})$ non-zero-cost
    layers, each mergeable in $O(\frac{N}{B})$ IOPS in total. This brings total I/O
    complexity to
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上意味着，在I/O操作方面，归并排序的前$O(\log M)$层是免费的，并且只有$O(\log_2 \frac{N}{M})$非零成本的层，每层可以在$O(\frac{N}{B})$
    IOPS下合并。这使总的I/O复杂度达到
- en: $$ O\left(\frac{N}{B} \log_2 \frac{N}{M}\right) $$
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: $$ O\left(\frac{N}{B} \log_2 \frac{N}{M}\right) $$
- en: This is quite fast. If we have 1GB of memory and 10GB of data, this essentially
    means that we need a little bit more than 3 times the effort than just reading
    the data to sort it. Interestingly enough, we can do better.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当快。如果我们有1GB的内存和10GB的数据，这实际上意味着我们只需要比读取数据排序多出一点点的努力。有趣的是，我们可以做得更好。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#k-way-mergesort)$k$-way
    Mergesort'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#k-way-mergesort)$k$-路归并排序'
- en: Half of a page ago we have learned that in the external memory model, we can
    merge $k$ arrays just as easily as two arrays — at the cost of reading them. Why
    don’t we apply this fact here?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一页的一半，我们了解到在外部内存模型中，我们可以像合并两个数组一样轻松地合并$k$个数组——代价是读取它们。为什么我们在这里不应用这个事实呢？
- en: Let’s sort each block of size $M$ in-memory just as we did before, but during
    each merge stage, we will split sorted blocks not just in pairs to be merged,
    but take as many blocks we can fit into our memory during a $k$-way merge. This
    way the height of the merge tree would be greatly reduced, while each layer would
    still be done in $O(\frac{N}{B})$ IOPS.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像之前一样，在内存中对大小为$M$的每个块进行排序，但在每个合并阶段，我们将排序好的块分成要合并的成对块，同时在一个$k$路合并过程中，尽可能多地取我们内存中能容纳的块。这样，合并树的高度将大大降低，而每一层仍然会在$O(\frac{N}{B})$
    IOPS下完成。
- en: How many sorted arrays can we merge at once? Exactly $k = \frac{M}{B}$, since
    we need memory for one block for each array. Since the total number of layers
    will be reduced to $\log_{\frac{M}{B}} \frac{N}{M}$, the total complexity will
    be reduced to
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以一次性合并多少个排序数组？正好是$k = \frac{M}{B}$，因为我们需要为每个数组保留一个块的内存。由于总的层数将减少到$\log_{\frac{M}{B}}
    \frac{N}{M}$，总的复杂度将减少到
- en: $$ SORT(N) \stackrel{\text{def}}{=} O\left(\frac{N}{B} \log_{\frac{M}{B}} \frac{N}{M}
    \right) $$
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: $$ SORT(N) \stackrel{\text{def}}{=} O\left(\frac{N}{B} \log_{\frac{M}{B}} \frac{N}{M}
    \right) $$
- en: Note that, in our example, we have 10GB of data, 1GB of memory, and the block
    size is around 1MB for HDD. This makes $\frac{M}{B} = 1000$ and $\frac{N}{M} =
    10$, and so the logarithm is less than one (namely, $\log_{1000} 10 = \frac{1}{3}$).
    Of course, we can’t sort an array faster than reading it, so this analysis applies
    to the cases when we have a very large dataset, small memory, and/or large block
    sizes, which rarely happens in real life these days.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们的例子中，我们有10GB的数据，1GB的内存，对于硬盘来说，块大小大约是1MB。这使得$\frac{M}{B} = 1000$和$\frac{N}{M}
    = 10$，因此对数小于1（即，$\log_{1000} 10 = \frac{1}{3}$）。当然，我们无法比读取数据更快地排序数组，因此这种分析适用于我们拥有非常大的数据集、较小的内存和/或较大的块大小的情况，这在当今现实生活中很少发生。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#practical-implementation)Practical
    Implementation'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#practical-implementation)实际实现'
- en: 'Under more realistic constraints, instead of using $\log_{\frac{M}{B}} \frac{N}{M}$
    layers, we can use just two: one for sorting data in blocks of $M$ elements, and
    another one for merging all of them at once. This way, from the I/O operations
    perspective, we just loop around our dataset twice. And with a gigabyte of RAM
    and a block size of 1MB, this way can sort arrays up to a terabyte in size.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在更现实的约束条件下，我们不需要使用$\log_{\frac{M}{B}} \frac{N}{M}$层，而只需要两层：一层用于对$M$个元素的块进行排序，另一层用于一次性合并所有这些块。这样，从I/O操作的角度来看，我们只需对数据集进行两次循环。如果拥有1GB的RAM和1MB的块大小，这种方法可以排序高达1TB大小的数组。
- en: 'Here is how the first phase looks in C++. This program opens a multi-gigabyte
    binary file with unsorted integers, reads it in blocks of 256MB, sorts them in
    memory, and then writes them back in files named `part-000.bin`, `part-001.bin`,
    `part-002.bin`, and so on:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是第一阶段在C++中的样子。这个程序打开一个包含未排序整数的多吉字节二进制文件，以256MB的块读取它，在内存中对它们进行排序，然后以`part-000.bin`、`part-001.bin`、`part-002.bin`等命名的文件将它们写回：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'What is left now is to merge them together. The bandwidth of modern HDDs can
    be quite high, and there may be a lot of parts to merge, so the I/O efficiency
    of this stage is not our only concern: we also need a faster way to merge $k$
    arrays than by finding minima with $O(k)$ comparisons. We can do that in $O(\log
    k)$ time per element if we maintain a min-heap for these $k$ elements, in a manner
    almost identical to heapsort.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的是将它们合并在一起。现代硬盘的带宽可以相当高，可能有很多部分需要合并，因此这一阶段的I/O效率并不是我们唯一关心的问题：我们还需要一种比通过
    $O(k)$ 次比较找到最小值更快的方式来合并 $k$ 个数组。如果我们为这些 $k$ 个元素维护一个最小堆，就可以在几乎与堆排序相同的方式中以 $O(\log
    k)$ 每个元素的时间复杂度完成。
- en: 'Here is how to implement it. First, we are going to need a heap (`priority_queue`
    in C++):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何实现它的方法。首先，我们需要一个堆（C++中的 `priority_queue`）：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we need to allocate and fill the buffers:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要分配和填充缓冲区：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we just need to pop elements from the heap into the result file until it
    is empty, carefully writing and reading elements in batches:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要从堆中弹出元素到结果文件，直到它为空，小心地在批量中写入和读取元素：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This implementation is not particularly effective or safe-looking (well, this
    is basically plain C), but is a good educational example of how to work with low-level
    memory APIs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现并不特别有效或看起来安全（好吧，这基本上是纯C），但它是一个很好的教育示例，说明了如何与低级内存API一起工作。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#joining)Joining'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/sorting/#joining) 合并'
- en: Sorting is mainly used not by itself, but as an intermediate step for other
    operations. One important real-world use case of external sorting is joining (as
    in “SQL join”), used in databases and other data processing applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 排序主要用于不是本身，而是作为其他操作的中间步骤。外部排序的一个重要实际用例是合并（如在“SQL join”中），用于数据库和其他数据处理应用程序。
- en: '**Problem.** Given two lists of tuples $(x_i, a_{x_i})$ and $(y_i, b_{y_i})$,
    output a list $(k, a_{x_k}, b_{y_k})$ such that $x_k = y_k$'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题。**给定两个元组列表 $(x_i, a_{x_i})$ 和 $(y_i, b_{y_i})$，输出一个列表 $(k, a_{x_k}, b_{y_k})$
    使得 $x_k = y_k$'
- en: The optimal solution would be to sort the two lists and then use the standard
    two-pointer technique to merge them. The I/O complexity here would be the same
    as sorting, and just $O(\frac{N}{B})$ if the arrays are already sorted. This is
    why most data processing applications (databases, MapReduce systems) like to keep
    their tables at least partially sorted.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳解决方案是对两个列表进行排序，然后使用标准的双指针技术合并它们。这里的I/O复杂度将与排序相同，如果数组已经排序，则为 $O(\frac{N}{B})$。这就是为什么大多数数据处理应用程序（数据库、MapReduce系统）喜欢至少部分保持它们的表排序。
- en: '**Other approaches.** Note that this analysis is only applicable in the external
    memory setting — that is, if you don’t have the memory to read the entire dataset.
    In the real world, alternative methods may be faster.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他方法。**请注意，此分析仅适用于外部内存设置——也就是说，如果你没有足够的内存来读取整个数据集。在现实世界中，其他方法可能更快。'
- en: 'The simplest of them is probably *hash join*, which goes something like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最简单的是可能是 *哈希连接*，其过程如下：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In external memory, joining two lists with a hash table would be unfeasible,
    as it would involve doing $O(M)$ block reads, even though only one element is
    used in each of them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在外部内存中，使用哈希表合并两个列表是不切实际的，因为它将涉及进行 $O(M)$ 块读取，尽管每个列表中只使用了一个元素。
- en: Another method is to use alternative sorting algorithms such as radix sort.
    In particular, radix sort would work in $O(\frac{N}{B} \cdot w)$ block reads if
    enough memory is available to maintain buffers for all possible keys, and it could
    be faster in the case of small keys and large datasets. [← External Memory Model](https://en.algorithmica.org/hpc/external-memory/model/)[List
    Ranking →](https://en.algorithmica.org/hpc/external-memory/list-ranking/)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用替代的排序算法，例如基数排序。特别是，如果足够内存来维护所有可能的键的缓冲区，基数排序在 $O(\frac{N}{B} \cdot w)$
    块读取中工作，并且在键较小且数据集较大的情况下可能会更快。[← 外部内存模型](https://en.algorithmica.org/hpc/external-memory/model/)[列表排序
    →](https://en.algorithmica.org/hpc/external-memory/list-ranking/)
