- en: 4\. Singular value decomposition#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.奇异值分解#
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/00_intro/roch-mmids-svd-intro.html](https://mmids-textbook.github.io/chap04_svd/00_intro/roch-mmids-svd-intro.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap04_svd/00_intro/roch-mmids-svd-intro.html](https://mmids-textbook.github.io/chap04_svd/00_intro/roch-mmids-svd-intro.html)
- en: In this chapter, we introduce the singular value decomposition (SVD), a fundamental
    concept in linear algebra. It is related to the spectral decomposition, but exists
    for any matrix including rectangular ones. The SVD has many applications in data
    science, including principle components analysis, low-rank approximation, pseudoinverses,
    and more. Here is a more detailed overview of the main sections of the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了奇异值分解（SVD），这是线性代数中的一个基本概念。它与谱分解相关，但存在于任何矩阵中，包括矩形矩阵。奇异值分解在数据科学中有许多应用，包括主成分分析、低秩逼近、伪逆等。以下是本章主要部分的更详细概述。
- en: '*“Background: review of matrix rank and spectral decomposition”* This section
    covers two key linear algebra concepts: matrix rank and the spectral theorem.
    The rank of a matrix is defined as the dimension of its column or row space. Properties
    of the rank are discussed, including the Rank-Nullity Theorem relating the rank
    and the dimension of the null space. The spectral theorem states that a real symmetric
    matrix has an orthonormal basis of eigenvectors with real eigenvalues, allowing
    for a spectral decomposition of the matrix. The section also characterizes positive
    semidefinite and positive definite matrices in terms of their eigenvalues.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*“背景：矩阵秩和谱分解的回顾”* 这一部分涵盖了两个关键的线性代数概念：矩阵秩和谱定理。矩阵的秩定义为它的列空间或行空间的维度。讨论了秩的性质，包括与零空间维度相关的秩-零度定理。谱定理表明，一个实对称矩阵有一个正交归一的特征向量基，具有实特征值，从而可以对矩阵进行谱分解。该部分还根据特征值来描述半正定矩阵和正定矩阵。'
- en: '*“Approximating subspaces and the SVD”* The section introduces the SVD as a
    matrix factorization that can be used to find the best low-dimensional approximating
    subspace to a set of data points. The section shows that this problem can be solved
    greedily by finding the best one-dimensional subspace, then the best one-dimensional
    subspace orthogonal to the first, and so on. It then formally defines the SVD
    and proves its existence for any matrix. It also discusses the relationship between
    the SVD and the spectral decomposition of a matrix. The section also discusses
    some important properties and relations satisfied by the SVD.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*“逼近子空间和奇异值分解（SVD）”* 这一部分介绍了奇异值分解（SVD）作为一种矩阵分解方法，可用于找到一组数据点的最佳低维逼近子空间。该部分展示了通过寻找最佳一维子空间，然后是第一个子空间正交的最佳一维子空间，以此类推，可以贪婪地解决这个问题。然后它正式定义了奇异值分解并证明了对于任何矩阵其存在性。它还讨论了奇异值分解与矩阵的谱分解之间的关系。该部分还讨论了奇异值分解满足的一些重要性质和关系。'
- en: '*“Power iteration”* The section discusses the power iteration method for computing
    the (truncated) SVD of a matrix. The key lemma for power iteration states in essence
    that repeated multiplication of \(A^T A\) with a random vector will converge to
    the top right singular vector \(\mathbf{v}_1\) of \(A\). The section also covers
    how to compute the corresponding singular value and left singular vector using
    the converged \(\mathbf{v}_1\).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*“幂迭代”* 这一部分讨论了用于计算矩阵（截断）奇异值分解的幂迭代方法。幂迭代的关键引理本质上表明，重复乘以 \(A^T A\) 与一个随机向量将收敛到
    \(A\) 的右上奇异向量 \(\mathbf{v}_1\)。该部分还涵盖了如何使用收敛的 \(\mathbf{v}_1\) 来计算相应的奇异值和左奇异向量。'
- en: '*“Application: principal components analysis”* This section discusses principal
    components analysis (PCA) as a dimensionality reduction technique. It explains
    that PCA finds linear combinations of features, called principal components, that
    capture the maximum variance in the data. The first principal component \(t_{i1}
    = \sum_{j=1}^p \phi_{j1} x_{ij}\) is obtained by solving \(\max \left\{ \frac{1}{n-1}
    \|X\boldsymbol{\phi}_1\|^2 : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\), where \(X\)
    is the centered data matrix. Subsequent principal components are obtained by imposing
    additional constraints, in particular uncorrelatedness with previous components.
    The section establishes a connection between PCA and the SVD.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*“应用：主成分分析”* 本节讨论主成分分析（PCA）作为一种降维技术。它解释说PCA找到特征的一组线性组合，称为主成分，这些主成分能够捕捉数据中的最大方差。第一个主成分
    \(t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}\) 通过求解 \(\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|^2
    : \|\boldsymbol{\phi}_1\|^2 = 1\right\}\) 得到，其中 \(X\) 是中心化的数据矩阵。后续的主成分通过施加额外的约束得到，特别是与先前组件的不相关性。本节建立了PCA与奇异值分解（SVD）之间的联系。'
- en: '*“Further applications of the SVD: low-rank approximations and ridge regression”*
    This section first defines matrix norms, in particular the induced 2-norm, and
    relates them to the singular values. The section then considers low-rank matrix
    approximations, highlighting the Eckart-Young theorem, which states that the best
    low-rank approximation of a matrix in both Frobenius and induced 2-norms is achieved
    by truncating the SVD. Finally, the section discusses ridge regression, a regularization
    technique that addresses multicollinearity in overdetermined systems by balancing
    data fitting with minimizing the solution’s norm.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*“奇异值分解的进一步应用：低秩逼近和岭回归”* 本节首先定义了矩阵范数，特别是诱导的2-范数，并将它们与奇异值联系起来。然后，本节考虑了低秩矩阵逼近，强调了Eckart-Young定理，该定理指出，在Frobenius范数和诱导的2-范数下，矩阵的最佳低秩逼近是通过截断SVD实现的。最后，本节讨论了岭回归，这是一种正则化技术，通过平衡数据拟合与最小化解的范数来解决过定系统中的多重共线性问题。'
