- en: Memory Bandwidth
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存带宽
- en: 原文：[https://en.algorithmica.org/hpc/cpu-cache/bandwidth/](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/cpu-cache/bandwidth/](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/)
- en: 'On the data path between the CPU registers and the RAM, there is a hierarchy
    of *caches* that exist to speed up access to frequently used data: the layers
    closer to the processor are faster but also smaller in size. The word “faster”
    here applies to two closely related but separate timings:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 寄存器和 RAM 之间的数据路径上，存在一个用于加速访问常用数据的缓存层次结构：靠近处理器的层级更快，但尺寸也更小。这里的“更快”适用于两个密切相关但不同的计时：
- en: The delay between the moment when a read or a write is initiated and when the
    data arrives (*latency*).
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从读取或写入开始到数据到达之间的延迟（*延迟*）。
- en: The number of memory operations that can be processed per unit of time (*bandwidth*).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每单位时间内可以处理的内存操作数（*带宽*）。
- en: For many algorithms, memory bandwidth is the most important characteristic of
    the cache system. And at the same time, it is also the easiest to measure, so
    we are going to start with it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多算法，内存带宽是缓存系统最重要的特性。同时，它也是最容易被测量的，因此我们将从这里开始。
- en: 'For our experiment, we create an array and iterate over it $K$ times, incrementing
    its values:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们创建一个数组并对其迭代 $K$ 次，增加其值：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Changing $N$ and adjusting $K$ so that the total number of array cells accessed
    remains roughly constant and expressing the total time in “operations per second,”
    we get a graph like this:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 改变 $N$ 并调整 $K$，使得访问的总数组单元格数大致保持不变，并以“每秒操作数”来表示总时间，我们得到如下所示的图表：
- en: '![](../Images/b704b1ff058f9b947bc80e669006da78.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b704b1ff058f9b947bc80e669006da78.png)'
- en: Dotted vertical lines are cache layer sizes
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 点状垂直线表示缓存层的大小
- en: 'You can clearly see the cache sizes on this graph:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地看到这个图表上的缓存大小：
- en: When the whole array fits into the lowest layer of cache, the program is bottlenecked
    by the CPU rather than the L1 cache bandwidth. As the array becomes larger, the
    overhead associated with the first iterations of the loop becomes smaller, and
    the performance gets closer to its theoretical maximum of 16 GFLOPS.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当整个数组适合缓存的最底层时，程序的性能瓶颈是由 CPU 而不是 L1 缓存带宽造成的。随着数组变大，与循环第一次迭代相关的开销变得更小，性能更接近其理论最大值
    16 GFLOPS。
- en: 'But then the performance drops: first to 12-13 GFLOPS when it exceeds the L1
    cache, and then gradually to about 2 GFLOPS when it can no longer fit in the L3
    cache.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但随后性能下降：首先当超过 L1 缓存时降至 12-13 GFLOPS，然后逐渐降至大约 2 GFLOPS，当它无法再适合 L3 缓存时。
- en: This situation is typical for many lightweight loops.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况对于许多轻量级循环来说是典型的。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#frequency-scaling)Frequency
    Scaling'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#frequency-scaling)频率缩放'
- en: 'All CPU cache layers are placed on the same microchip as the processor, so
    the bandwidth, latency, and all its other characteristics scale with the clock
    frequency. The RAM, on the other side, lives on its own fixed clock, and its characteristics
    remain constant. We can observe this by re-running the same benchmarking with
    turbo boost on:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 CPU 缓存层都放置在处理器相同的微芯片上，因此带宽、延迟以及所有其他特性都随着时钟频率而缩放。另一方面，RAM 则生活在其自己的固定时钟上，其特性保持不变。我们可以通过重新运行带有超频的相同基准测试来观察这一点：
- en: '![](../Images/df8453e0bd574b085bccd579d6d178f2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df8453e0bd574b085bccd579d6d178f2.png)'
- en: This detail comes into play when comparing algorithm implementations. When the
    working dataset fits in the cache, the relative performance of the two implementations
    may be different depending on the CPU clock rate because the RAM remains unaffected
    by it (while everything else does not).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较算法实现时，这个细节就会发挥作用。当工作数据集适合缓存时，两种实现的相对性能可能因 CPU 时钟率的不同而不同，因为 RAM 不受其影响（而其他一切都会受到影响）。
- en: For this reason, it is [advised](/hpc/profiling/noise) to keep the clock rate
    fixed, and as the turbo boost isn’t stable enough, we run most of the benchmarks
    in this book at plain 2GHz.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，建议[保持时钟频率固定](/hpc/profiling/noise)，由于超频不够稳定，我们在这本书的大多数基准测试中都使用普通的 2GHz。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#directional-access)Directional
    Access'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#directional-access)方向访问'
- en: 'This incrementing loop needs to perform both reads and writes during its execution:
    on each iteration, we fetch a value, increment it, and then write it back. In
    many applications, we only need to do one of them, so let’s try to measure unidirectional
    bandwidth.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个递增循环在其执行过程中需要执行读取和写入：在每次迭代中，我们获取一个值，增加它，然后将其写回。在许多应用中，我们只需要做其中之一，所以让我们尝试测量单向带宽。
- en: 'Calculating the sum of an array only requires memory reads:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数组的和只需要内存读取：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And zeroing an array (or filling it with any other constant value) only requires
    memory writes:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 而清零数组（或用任何其他常量值填充它）只需要内存写入：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Both loops are trivially [vectorized](/hpc/simd) by the compiler, and the second
    one is actually replaced with a `memset`, so the CPU is also not the bottleneck
    here (except when the array fits into the L1 cache).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个循环都可以由编译器轻易[向量化](/hpc/simd)，第二个实际上被替换为`memset`，所以CPU也不是这里的瓶颈（除非数组适合L1缓存）。
- en: '![](../Images/7387e237a0c0c4a7b5759e210932988a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7387e237a0c0c4a7b5759e210932988a.png)'
- en: 'The reason why unidirectional and bidirectional memory accesses would perform
    differently is that they share the cache and memory buses and other CPU facilities.
    In the case of RAM, this causes a twofold difference in performance between the
    pure read and simultaneous read and write scenarios because the memory controller
    has to switch between the modes on the one-way memory bus, thus halving the bandwidth.
    The performance drop is less severe for the L2 cache: the bottleneck here is not
    the cache bus, so the incrementing loop loses by only ~15%.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 单向和双向内存访问之所以会表现不同，是因为它们共享缓存和内存总线以及其他CPU设施。在RAM的情况下，这导致纯读取和同时读取和写入场景之间性能差异翻倍，因为内存控制器必须在单向内存总线上在两种模式之间切换，从而减半带宽。性能下降对于L2缓存来说不那么严重：这里的瓶颈不是缓存总线，所以递增循环只损失了大约15%。
- en: There is one interesting anomaly on the graph, namely that the write-only loop
    performs the same as the read-and-write one when the array hits the L3 cache and
    the RAM. This is because the CPU moves the data to the highest level of cache
    on each access, whether it is a read or a write — which is typically a good optimization,
    as in many use cases we will be needing it soon. When reading data, this isn’t
    a problem, as the data travels through the cache hierarchy anyway, but when writing,
    this causes another implicit read to be dispatched right after a write — thus
    requiring twice the bus bandwidth.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图表上有一个有趣的异常，即当数组达到L3缓存和RAM时，只写循环的表现与读写循环相同。这是因为CPU在每次访问时都会将数据移动到最高级别的缓存，无论是读取还是写入——这通常是一个很好的优化，因为在许多用例中我们很快就会需要它。当读取数据时，这并不是问题，因为数据无论如何都会通过缓存层次结构，但在写入时，这会导致在写入后立即调度另一个隐式读取——因此需要两倍的总线带宽。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#bypassing-the-cache)Bypassing
    the Cache'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#bypassing-the-cache)绕过缓存'
- en: We can prevent the CPU from prefetching the data that we just have written by
    using *non-temporal* memory accesses. To do this, we need to re-implement the
    zeroing loop more directly without relying on compiler vectorization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用*非临时*内存访问来防止CPU预取我们刚刚写入的数据。为此，我们需要更直接地重新实现清零循环，而不依赖于编译器向量化。
- en: 'Ignoring a few special cases, what `memset` and auto-vectorized assignment
    loops do under the hood is they just [move](/hpc/simd/moving) 32-byte blocks of
    data with [SIMD instructions](/hpc/simd):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略一些特殊情况，`memset`和自动向量化赋值循环在底层所做的只是使用[SIMD指令](/hpc/simd)移动32字节数据块：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can replace the usual vector store intrinsic with a *non-temporal* one:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个*非临时*的替换通常的向量存储内建函数：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Non-temporal memory reads or writes are a way to tell the CPU that we won’t
    be needing the data that we have just accessed in the future, so there is no need
    to read the data back after a write.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 非临时内存读取或写入是一种告诉CPU我们将来不需要我们刚刚访问的数据的方法，因此在写入后不需要读取数据。
- en: '![](../Images/cc99a8910190218123f19011de90575f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc99a8910190218123f19011de90575f.png)'
- en: On the one hand, if the array is small enough to fit into the cache, and we
    actually access it some short time after, this has a negative effect because we
    have to read entirely it from the RAM (or, in this case, we have to *write* it
    into the RAM instead of using a locally cached version). And on the other, this
    prevents read-backs and lets us use the memory bus more efficiently.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，如果数组足够小，可以放入缓存，并且我们在短时间内实际访问它，这会有负面影响，因为我们必须从RAM（或在这种情况下，我们必须将数据写入RAM而不是使用本地缓存版本）中完全读取它。另一方面，这防止了读取回写，并使我们能够更有效地使用内存总线。
- en: 'In fact, the performance increase in the case of the RAM is even more than
    2x and faster than the read-only benchmark. This happens because:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在RAM的情况下，性能提升甚至超过2倍，并且比只读基准测试更快。这是因为：
- en: the memory controller doesn’t have to switch the bus between read and write
    modes this way;
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储控制器不必这样在读取和写入模式之间切换总线；
- en: the instruction sequence becomes simpler, allowing for more pending memory instructions;
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令序列变得更简单，允许更多的挂起内存指令；
- en: and, most importantly, the memory controller can simply “fire and forget” non-temporal
    write requests — while for reads, it needs to remember what to do with the data
    once it arrives (similar to connection handles in networking software).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并且，最重要的是，存储控制器可以简单地“发射并忘记”非临时写入请求——而对于读取，它需要记住数据到达后要做什么（类似于网络软件中的连接句柄）。
- en: 'Theoretically, both requests should use the same bandwidth: a read request
    sends an address and gets data, and a non-temporal write request sends an address
    *with* data and gets nothing. Not accounting for the direction, we transmit the
    same data, but the read cycle will be longer because it needs to wait for the
    data to be fetched. Since [there is a practical limit](../mlp) on how many concurrent
    requests the memory system can handle, this difference in read/write cycle latency
    also results in the difference in their bandwidth.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这两个请求应该使用相同的带宽：一个读取请求发送地址并获取数据，一个非临时写入请求发送地址和数据，并获取无数据。不考虑方向，我们传输相同的数据，但读取周期会更长，因为它需要等待数据被检索。由于内存系统可以处理的并发请求数量存在实际限制[1]，这种读写周期延迟的差异也导致了它们带宽的差异。
- en: Also, for these reasons, a single CPU core usually [can’t fully saturate the
    memory bandwidth](../sharing).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于这些原因，单个CPU核心通常[无法完全利用内存带宽](../sharing)。
- en: 'The same technique generalizes to `memcpy`: it also just moves 32-byte blocks
    with SIMD load/store instructions, and it can be similarly made non-temporal,
    increasing the throughput twofold for large arrays. There is also a non-temporal
    load instruction (`_mm256_stream_load_si256`) for when you want to *read* without
    polluting cache (e.g., when you don’t need the original array after a `memcpy`,
    but will need some data that you had accessed before calling it). [← ../RAM &
    CPU Caches](https://en.algorithmica.org/hpc/cpu-cache/)[Memory Latency →](https://en.algorithmica.org/hpc/cpu-cache/latency/)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的技术也适用于`memcpy`：它也仅使用SIMD加载/存储指令移动32字节块，并且可以类似地将其设置为非临时，对于大型数组，可以将吞吐量提高两倍。还有一个非临时加载指令（`_mm256_stream_load_si256`），当你想要*读取*而不污染缓存时使用（例如，当你不需要在调用`memcpy`之后的原始数组，但需要之前访问的一些数据时）。[←
    ../RAM & CPU Caches](https://en.algorithmica.org/hpc/cpu-cache/)[Memory Latency
    →](https://en.algorithmica.org/hpc/cpu-cache/latency/)
