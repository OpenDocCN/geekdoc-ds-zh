- en: Introduction to GPU programming models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU编程模型简介
- en: 原文：[https://enccs.github.io/gpu-programming/5-intro-to-gpu-prog-models/](https://enccs.github.io/gpu-programming/5-intro-to-gpu-prog-models/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[原文链接](https://enccs.github.io/gpu-programming/5-intro-to-gpu-prog-models/)'
- en: '*[GPU programming: why, when and how?](../)* **   Introduction to GPU programming
    models'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*[GPU编程：为什么、何时以及如何？](../)* **   GPU编程模型简介'
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/5-intro-to-gpu-prog-models.rst)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在GitHub上编辑](https://github.com/ENCCS/gpu-programming/blob/main/content/5-intro-to-gpu-prog-models.rst)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Questions
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 问题
- en: What are the key differences between different GPU programming approaches?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同GPU编程方法之间的关键区别是什么？
- en: How should I choose which framework to use for my project?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该如何选择适合我项目的框架？
- en: Objectives
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Understand the basic ideas in different GPU programming frameworks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解不同GPU编程框架的基本思想
- en: Perform a quick cost-benefit analysis in the context of own code projects
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自己的代码项目中进行快速的成本效益分析
- en: Instructor note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 教师备注
- en: 20 min teaching
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20分钟教学
- en: 10 min discussion
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10分钟讨论
- en: There are different ways to use GPUs for computations. In the best case, when
    the code has already been written, one only needs to set the parameters and initial
    configuration in order to get started. In some other cases the problem is posed
    in such a way that a third-party library can be used to solve the most intensive
    part of the code (for example, this is increasingly the case with machine-learning
    workflows in Python). However, these cases are stil quite limited; in general,
    some additional programming might be needed. There are many GPU programming software
    environments and APIs available, which can be broadly grouped into **directive-based
    models**, **non-portable kernel-based models**, and **portable kernel-based models**,
    as well as high-level frameworks and libraries (including attempts at language-level
    support).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU进行计算有不同的方法。在最佳情况下，当代码已经编写完成时，只需要设置参数和初始配置就可以开始。在某些其他情况下，问题被提出的方式使得可以使用第三方库来解决代码中最密集的部分（例如，这在Python中的机器学习工作流程中越来越常见）。然而，这些情况仍然相当有限；一般来说，可能需要额外的编程。有许多GPU编程软件环境和API可用，可以大致分为**指令集模型**、**不可移植内核模型**和**可移植内核模型**，以及高级框架和库（包括语言级别支持的尝试）。
- en: Standard C++/Fortran
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准C++/Fortran
- en: Programs written in standard C++ and Fortran languages can now take advantage
    of NVIDIA GPUs without depending on any external library. This is possible thanks
    to the [NVIDIA SDK](https://developer.nvidia.com/hpc-sdk) suite of compilers that
    translates and optimizes the code for running on GPUs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准C++和Fortran语言编写的程序现在可以利用NVIDIA GPU，而不需要依赖任何外部库。这要归功于[NVIDIA SDK](https://developer.nvidia.com/hpc-sdk)套件的编译器，它可以将代码翻译并优化以在GPU上运行。
- en: '[Here](https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/)
    is the series of articles on acceleration with standard language parallelism.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于使用标准语言并行性加速的文章系列](https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/)'
- en: Guidelines for writing C++ code can be found [here](https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/),
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++代码编写的指南可以在[这里](https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/)找到，
- en: while those for Fortran code can be found [here](https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortran代码的相关信息可以在[这里](https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/)找到。
- en: The performance of these two approaches is promising, as can be seen in the
    examples provided in those guidelines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的表现很有希望，正如那些指南中提供的示例所示。
- en: Directive-based programming
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令集编程
- en: A fast and cheap way is to use **directive based** approaches. In this case
    the existing *serial* code is annotated with *hints* which indicate to the compiler
    which loops and regions should be executed on the GPU. In the absence of the API
    the directives are treated as comments and the code will just be executed as a
    usual serial code. This approach is focused on productivity and easy usage (but
    to the possible detriment of performance), and allows employing accelerators with
    minimal programming effort by adding parallelism to existing code without the
    need to write accelerator-specific code. There are two common ways to program
    using directives, namely **OpenACC** and **OpenMP**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一种快速且经济的方法是使用**基于指令**的方法。在这种情况下，现有的*串行*代码被添加了*提示*，这些提示指示编译器哪些循环和区域应该在GPU上执行。如果没有API，这些指令被视为注释，代码将像通常的串行代码一样执行。这种方法侧重于生产力和易用性（但可能损害性能），通过向现有代码添加并行性，无需编写特定于加速器的代码，就可以以最小的编程努力使用加速器。使用指令编程有两种常见方法，即**OpenACC**和**OpenMP**。
- en: OpenACC
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenACC
- en: '[OpenACC](https://www.openacc.org/) is developed by a consortium formed in
    2010 with the goal of developing a standard, portable, and scalable programming
    model for accelerators, including GPUs. Members of the OpenACC consortium include
    GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers,
    universities, and software companies. Until recently it was supporting only NVIDIA
    GPUs, but now there is effort to support more devices and architectures.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenACC](https://www.openacc.org/)是由2010年成立的一个联盟开发的，旨在为加速器（包括GPU）开发一个标准、可移植和可扩展的编程模型。OpenACC联盟的成员包括GPU供应商，如NVIDIA和AMD，以及领先的超级计算中心、大学和软件公司。直到最近，它只支持NVIDIA
    GPU，但现在有努力支持更多设备和架构。'
- en: OpenMP
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenMP
- en: '[OpenMP](https://www.openmp.org/) started as a multi-platform, shared-memory
    parallel programming API for multi-core CPUs and relatively recently has added
    support for GPU offloading. OpenMP aims to support various types of GPUs, which
    is done through the parent compiler.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenMP](https://www.openmp.org/)最初是一个多平台、共享内存并行编程API，用于多核CPU，最近也增加了对GPU卸载的支持。OpenMP旨在支持各种类型的GPU，这是通过父编译器实现的。'
- en: The directive based approaches work with C/C++ and FORTRAN codes, while some
    third party extensions are available for other languages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于指令的方法与C/C++和FORTRAN代码一起工作，同时为其他语言提供了第三方扩展。
- en: Non-portable kernel-based models (native programming models)
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不可移植的基于内核的模型（原生编程模型）
- en: When doing direct GPU programming the developer has a large level of control
    by writing low-level code that directly communicates with the GPU and its hardware.
    Theoretically direct GPU programming methods provide the ability to write low-level,
    GPU-accelerated code that can provide significant performance improvements over
    CPU-only code. However, they also require a deeper understanding of the GPU architecture
    and its capabilities, as well as the specific programming method being used.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行直接GPU编程时，开发者可以通过编写直接与GPU及其硬件通信的低级代码来获得很高的控制水平。理论上，直接GPU编程方法提供了编写低级、GPU加速的代码的能力，这可以在CPU-only代码上提供显著的性能提升。然而，它们也要求对GPU架构及其功能有更深入的了解，以及正在使用的特定编程方法。
- en: CUDA
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CUDA
- en: '[CUDA](https://developer.nvidia.com/cuda-toolkit) is a parallel computing platform
    and API developed by NVIDIA. It is historically the first mainstream GPU programming
    framework. It allows developers to write C-like code that is executed on the GPU.
    CUDA provides a set of libraries and tools for low-level GPU programming and provides
    a performance boost for demanding computationally-intensive applications. While
    there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDA](https://developer.nvidia.com/cuda-toolkit)是由NVIDIA开发的一个并行计算平台和API。它历史上是第一个主流GPU编程框架。它允许开发者编写在GPU上执行的类似C的代码。CUDA提供了一套用于低级GPU编程的库和工具，并为计算密集型应用程序提供了性能提升。虽然有一个广泛的生态系统，但CUDA仅限于NVIDIA硬件。'
- en: HIP
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HIP
- en: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html) (Heterogeneous
    Interface for Portability) is an API developed by AMD that provides a low-level
    interface for GPU programming. HIP is designed to provide a single source code
    that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming
    model and provides an almost identical programming interface to CUDA.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html)（异构接口可移植性）是由AMD开发的一个API，它为GPU编程提供了一个低级接口。HIP旨在提供一个可以在NVIDIA和AMD
    GPU上使用的单一源代码。它基于CUDA编程模型，并提供与CUDA几乎相同的编程接口。'
- en: Multiple examples of CUDA/HIP code are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    directory of this repository.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在此存储库的[content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)目录中提供了多个CUDA/HIP代码示例。
- en: Portable kernel-based models (cross-platform portability ecosystems)
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于可移植内核的模型（跨平台可移植性生态系统）
- en: Cross-platform portability ecosystems typically provide a higher-level abstraction
    layer which enables a convenient and portable programming model for GPU programming.
    They can help reduce the time and effort required to maintain and deploy GPU-accelerated
    applications. The goal of these ecosystems is to achieve performance portability
    with a single-source application. In C++, the most notable cross-platform portability
    ecosystems are [SYCL](https://www.khronos.org/sycl/), [OpenCL](https://www.khronos.org/opencl/)
    (C and C++ APIs), and [Kokkos](https://github.com/kokkos/kokkos); others include
    [alpaka](https://alpaka.readthedocs.io/) and [RAJA](https://github.com/LLNL/RAJA).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 跨平台可移植性生态系统通常提供更高层次的抽象层，这使得GPU编程具有方便且可移植的编程模型。它们可以帮助减少维护和部署GPU加速应用程序所需的时间和精力。这些生态系统的目标是实现单源应用程序的性能可移植性。在C++中，最著名的跨平台可移植性生态系统包括[SYCL](https://www.khronos.org/sycl/)、[OpenCL](https://www.khronos.org/opencl/)（C和C++
    API）和[Kokkos](https://github.com/kokkos/kokkos)；其他还包括[alpaka](https://alpaka.readthedocs.io/)和[RAJA](https://github.com/LLNL/RAJA)。
- en: OpenCL
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenCL
- en: '[OpenCL](https://www.khronos.org/opencl/) (Open Computing Language) is a cross-platform,
    open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs.
    It supports a wide range of hardware from multiple vendors. OpenCL provides a
    low-level programming interface for GPU programming and enables developers to
    write programs that can be executed on a variety of platforms. Unlike programming
    models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model.
    Recent versions of the OpenCL standard added C++ support for both API and the
    kernel code, but the C-based interface is still more widely used. The OpenCL Working
    Group doesn’t provide any frameworks of its own. Instead, vendors who produce
    OpenCL-compliant devices release frameworks as part of their software development
    kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD.
    In both cases, the development kits are free and contain the libraries and tools
    that make it possible to build OpenCL applications.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenCL](https://www.khronos.org/opencl/)（开放计算语言）是一个跨平台的开放标准API，用于在CPU、GPU和FPGA上执行通用并行计算。它支持来自多个供应商的广泛硬件。OpenCL为GPU编程提供了一个低级编程接口，并允许开发者编写可在各种平台上执行的程序。与CUDA、HIP、Kokkos和SYCL等编程模型不同，OpenCL使用单独的源模型。OpenCL标准的最新版本增加了对API和内核代码的C++支持，但基于C的接口仍然更广泛地被使用。OpenCL工作组不提供任何自己的框架。相反，生产OpenCL兼容设备的供应商将其框架作为其软件开发工具包（SDK）的一部分发布。最受欢迎的两个OpenCL
    SDK分别由NVIDIA和AMD发布。在两种情况下，开发套件都是免费的，并包含构建OpenCL应用程序所需的库和工具。'
- en: Kokkos
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kokkos
- en: '[Kokkos](https://github.com/kokkos/kokkos) is an open-source performance portable
    programming model for heterogeneous parallel computing that has been mainly developed
    at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming
    model for developing efficient and scalable parallel applications that run on
    many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists
    of several components, such as the Kokkos core library, which provides parallel
    execution and memory abstraction, the Kokkos kernel library, which provides math
    kernels for linear algebra and graph algorithms, and the Kokkos tools library,
    which provides profiling and debugging tools. Kokkos components integrate well
    with other software libraries and technologies, such as MPI and OpenMP. Furthermore,
    the project collaborates with other projects, in order to provide interoperability
    and standardization for portable C++ programming.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kokkos](https://github.com/kokkos/kokkos) 是一个开源的性能可移植并行计算编程模型，主要在桑迪亚国家实验室开发。它是一个基于
    C++ 的生态系统，为在多核架构（如 CPU、GPU 和 FPGA）上运行的高效和可扩展的并行应用程序提供编程模型。Kokkos 生态系统包括几个组件，例如提供并行执行和内存抽象的
    Kokkos 核心库，提供线性代数和图算法数学内核的 Kokkos 内核库，以及提供性能分析和调试工具的 Kokkos 工具库。Kokkos 组件与其他软件库和技术（如
    MPI 和 OpenMP）集成良好。此外，该项目与其他项目合作，以提供可移植 C++ 编程的互操作性和标准化。'
- en: alpaka
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: alpaka
- en: '[alpaka](https://alpaka.readthedocs.io/) (Abstraction Library for Parallel
    Kernel Acceleration) is an open-source C++ header-only library that aims to provide
    performance portability across heterogeneous accelerator architectures by abstracting
    the underlying levels of parallelism. The library is platform-independent and
    supports the concurrent and cooperative use of multiple devices, including host
    CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[alpaka](https://alpaka.readthedocs.io/)（并行内核加速抽象库）是一个开源的仅包含头文件的 C++ 库，旨在通过抽象底层并行级别，在异构加速器架构之间提供性能可移植性。该库是平台无关的，并支持包括主机
    CPU（x86、ARM、RISC-V）和来自不同供应商的 GPU（NVIDIA、AMD 和 Intel）在内的多个设备的并发和协作使用。'
- en: A key advantage of alpaka is that it requires only a single implementation of
    a user kernel, expressed as a function object with a standardized interface. This
    eliminates the need to write specialized code for different backends. The library
    provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP,
    and serial execution, that can be selected based on the target device. Moreover,
    multiple accelerator backends can even be combined to target different vendor
    hardware within a single application.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: alpaka 的一个关键优势是它只需要一个用户内核的单个实现，该实现以具有标准化接口的函数对象的形式表达。这消除了为不同后端编写专用代码的需要。该库提供各种加速器后端，包括
    CUDA、HIP、SYCL、OpenMP 和串行执行，可以根据目标设备进行选择。此外，甚至可以将多个加速器后端组合起来，以在单个应用程序中针对不同的供应商硬件。
- en: SYCL
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SYCL
- en: '[SYCL](https://www.khronos.org/sycl/) is a royalty-free, open-standard C++
    programming model for multi-device programming. It provides a high-level, single-source
    programming model for heterogeneous systems, including GPUs. Originally SYCL was
    developed on top of OpenCL; however, it is no more limited to just that. It can
    be implemented on top of other low-level heterogeneous computing APIs, such as
    CUDA or HIP, enabling developers to write programs that can be executed on a variety
    of platforms. Note that while SYCL is relatively high-level model, the developers
    are still required to write GPU kernels explicitly.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[SYCL](https://www.khronos.org/sycl/) 是一个免费的、开放标准的 C++ 多设备编程编程模型。它为包括 GPU 在内的异构系统提供了一种高级、单源编程模型。最初，SYCL
    是在 OpenCL 的基础上开发的；然而，它不再仅限于这一点。它可以在其他低级异构计算 API（如 CUDA 或 HIP）之上实现，使开发者能够编写可以在各种平台上执行的程序。请注意，虽然
    SYCL 是一个相对高级的模型，但开发者仍然需要明确编写 GPU 内核。'
- en: While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only
    a standard, for which several implementations exist. For GPU programming, [Intel
    oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)
    (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with [Codeplay oneAPI
    plugins](https://codeplay.com/solutions/oneapi/)) and [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)
    (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs,
    with experimental Intel GPU support available in combination with Intel oneAPI
    DPC++) are the most widely used. Other implementations of note are [triSYCL](https://github.com/triSYCL/triSYCL)
    and [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 alpaka、Kokkos 和 RAJA 指的是特定的项目，但 SYCL 本身只是一个标准，存在多个实现。对于 GPU 编程，[Intel oneAPI
    DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)（原生支持
    Intel GPU，以及通过 [Codeplay oneAPI 插件](https://codeplay.com/solutions/oneapi/) 支持的
    NVIDIA 和 AMD GPU）和 [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)（之前也称为
    hipSYCL 或 Open SYCL，支持 NVIDIA 和 AMD GPU，与 Intel oneAPI DPC++ 结合使用时提供实验性的 Intel
    GPU 支持）是最广泛使用的。其他值得注意的实现包括 [triSYCL](https://github.com/triSYCL/triSYCL) 和 [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/)。
- en: High-level language support
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级语言支持
- en: Python
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python
- en: Python offers support for GPU programming through multiple abstraction levels.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Python 通过多个抽象级别提供对 GPU 编程的支持。
- en: '**CUDA Python, HIP Python and PyCUDA**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA Python, HIP Python 和 PyCUDA**'
- en: These projects are, respectively, [NVIDIA-](https://developer.nvidia.com/cuda-python),
    [AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/) and [community-supported](https://documen.tician.de/pycuda/)
    wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use
    these approaches directly, in most cases knowledge of CUDA or HIP programming
    is needed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目分别是 [NVIDIA-](https://developer.nvidia.com/cuda-python)、[AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/)
    和 [社区支持](https://documen.tician.de/pycuda/) 的包装器，提供了对低级 CUDA 和 HIP API 的 Python
    绑定。要直接使用这些方法，在大多数情况下需要了解 CUDA 或 HIP 编程。
- en: CUDA Python also aims to support higher-level toolkits and libraries, such as
    **CuPy** and **Numba**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA Python 也旨在支持更高级的工具包和库，例如 **CuPy** 和 **Numba**。
- en: '**CuPy**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**CuPy**'
- en: '[CuPy](https://cupy.dev/) is a GPU-based data array library compatible with
    NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it
    easy for developers to transition to GPU computing. Code written with NumPy can
    often be adapted to use CuPy with minimal modifications; in most straightforward
    cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’
    in their Python code.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[CuPy](https://cupy.dev/) 是一个基于 GPU 的数据数组库，与 NumPy/SciPy 兼容。它提供了一个与 NumPy 和
    SciPy 非常相似的接口，使得开发者可以轻松过渡到 GPU 计算。使用 NumPy 编写的代码通常可以经过最小修改后适应 CuPy；在大多数直接情况下，人们可能只需在他们的
    Python 代码中将‘numpy’和‘scipy’替换为‘cupy’和‘cupyx.scipy’即可。'
- en: '**Numba**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Numba**'
- en: '[Numba](https://numba.pydata.org/) is an open-source JIT compiler that translates
    a subset of Python and NumPy code into optimized machine code. Numba supports
    CUDA-capable GPUs and is able to generate code for them using several different
    syntax variants. In 2021, upstream support for [AMD (ROCm) support](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)
    was discontinued. However, as of 2025, AMD has added downstream support for the
    Numba API through the [Numba HIP package](https://github.com/ROCm/numba-hip).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[Numba](https://numba.pydata.org/) 是一个开源的 JIT 编译器，它将 Python 和 NumPy 代码的一个子集转换为优化的机器代码。Numba
    支持 CUDA 兼容的 GPU，并能够使用几种不同的语法变体为它们生成代码。2021 年，对 [AMD (ROCm) 支持](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)的上游支持已被终止。然而，截至
    2025 年，AMD 通过 [Numba HIP 包](https://github.com/ROCm/numba-hip)添加了对 Numba API 的下游支持。'
- en: Julia
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Julia
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 通过以下针对所有三个主要供应商的 GPU 的包，提供了对 GPU 编程的一流支持：
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA.jl](https://cuda.juliagpu.org/stable/) 用于 NVIDIA GPU'
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) 用于 AMD GPU'
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) 用于 Intel GPU'
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) 用于 Apple M 系列GPU'
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance. Their respective
    APIs are however completely analogous and translation between libraries is straightforward.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA.jl`是最成熟的，`AMDGPU.jl`稍微落后但仍然适用于通用用途，而`oneAPI.jl`和`Metal.jl`功能齐全但可能存在错误，缺少一些功能，并提供次优性能。然而，它们的相应API是完全相似的，库之间的转换简单直接。'
- en: All packages offer both high-level abstractions that require very little programming
    effort and a lower level approach for writing kernels for fine-grained control.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所有包都提供了高级抽象，这需要很少的编程工作，同时也提供了一种低级方法来编写内核，以实现细粒度的控制。
- en: In short
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: '**Directive-based programming:**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于指令的编程：**'
- en: Existing serial code is annotated with directives to indicate which parts should
    be executed on the GPU.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的串行代码通过指令标注了哪些部分应该在GPU上执行。
- en: OpenACC and OpenMP are common directive-based programming models.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC和OpenMP是常见的基于指令的编程模型。
- en: Productivity and easy usage are prioritized over performance.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑生产力和易用性，而不是性能。
- en: Minimum programming effort is required to add parallelism to existing code.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现有代码中添加并行性需要最少的编程工作。
- en: '**Non-portable kernel-based models:**'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可移植的基于内核的模型：**'
- en: Low-level code is written to directly communicate with the GPU.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低级代码被编写来直接与GPU通信。
- en: CUDA is NVIDIA’s parallel computing platform and API for GPU programming.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA是NVIDIA的并行计算平台和GPU编程的API。
- en: HIP is an API developed by AMD that provides a similar programming interface
    to CUDA for both NVIDIA and AMD GPUs.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HIP是由AMD开发的一个API，为NVIDIA和AMD GPU提供了与CUDA相似的编程接口。
- en: Deeper understanding of GPU architecture and programming methods is needed.
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要更深入地了解GPU架构和编程方法。
- en: '**Portable kernel-based models:**'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植的基于内核的模型：**'
- en: Higher-level abstractions for GPU programming that provide portability.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为GPU编程提供可移植性的高级抽象。
- en: Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例子包括OpenCL、Kokkos、alpaka、RAJA和SYCL。
- en: Aim to achieve performance portability with a single-source application.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 力求通过单源应用程序实现性能可移植性。
- en: Can run on various GPUs and platforms, reducing the effort required to maintain
    and deploy GPU-accelerated applications.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在各种GPU和平台上运行，减少了维护和部署GPU加速应用程序所需的努力。
- en: '**High-level language support:**'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级语言支持：**'
- en: C++ and Fortran feature initiatives to support GPUs through language-standard
    parallelism.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++和Fortran有支持GPU的语言标准并行的计划。
- en: Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库如PyCUDA、CuPy和Numba提供了GPU编程能力。
- en: Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU
    programming.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia有CUDA.jl、AMDGPU.jl、oneAPI.jl和Metal.jl等包用于GPU编程。
- en: These approaches provide high-level abstraction and interfaces for GPU programming
    in the respective languages.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法为各自语言中的GPU编程提供了高级抽象和接口。
- en: Summary
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Each of these GPU programming environments has its own strengths and weaknesses,
    and the best choice for a given project will depend on a range of factors, including:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些GPU编程环境各自都有其优势和劣势，对于特定项目而言，最佳选择将取决于一系列因素，包括：
- en: the hardware platforms being targeted,
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对的目标硬件平台，
- en: the type of computation being performed, and
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在执行的计算类型，以及
- en: the developer’s experience and preferences.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者的经验和偏好。
- en: '**High-level and productivity-focused APIs** provide a simplified programming
    model and maximize code portability, while **low-level and performance-focused
    APIs** provide a high level of control over the GPU’s hardware but also require
    more coding effort and expertise.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**高级和生产力导向的API**提供了一个简化的编程模型，并最大化代码的可移植性，而**低级和性能导向的API**提供了对GPU硬件的高级别控制，但也需要更多的编码工作和专业知识。'
- en: Exercises
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Discussion
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论
- en: Which GPU programming frameworks have you already used previously, if any?
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你以前是否使用过任何GPU编程框架？
- en: What did you find most challenging? What was most useful?
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你认为最具有挑战性的是什么？什么最有用？
- en: Let us know in the main room or via comments in HackMD document.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请在主房间或通过HackMD文档中的评论告诉我们。
- en: Keypoints
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: GPU programming approaches can be split into 1) directive-based, 2) non-portable
    kernel-based, 3) portable kernel-based, and 4) high-level language support.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU编程方法可以分为1）基于指令的，2）不可移植的基于内核的，3）可移植的基于内核的，和4）高级语言支持。
- en: There are multiple frameworks/languages available for each approach, each with
    pros and cons. [Previous](../4-gpu-concepts/ "GPU programming concepts") [Next](../6-directive-based-models/
    "Directive-based models")
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种方法都有多个框架/语言可供选择，每个都有其优缺点。[上一页](../4-gpu-concepts/ "GPU编程概念") [下一页](../6-directive-based-models/
    "指令集模型")
- en: '* * *'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: © Copyright 2023-2024, The contributors.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: © 版权所有 2023-2024，贡献者。
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Read the Docs](https://readthedocs.org)提供的[主题](https://github.com/readthedocs/sphinx_rtd_theme)和[Sphinx](https://www.sphinx-doc.org/)构建
- en: What are the key differences between different GPU programming approaches?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的GPU编程方法之间有哪些关键区别？
- en: How should I choose which framework to use for my project?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该如何选择适合我项目的框架？
- en: Objectives
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Understand the basic ideas in different GPU programming frameworks
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解不同GPU编程框架的基本思想
- en: Perform a quick cost-benefit analysis in the context of own code projects
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自己的代码项目背景下进行快速的成本效益分析
- en: Instructor note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 讲师备注
- en: 20 min teaching
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20分钟教学
- en: 10 min discussion
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10分钟讨论
- en: There are different ways to use GPUs for computations. In the best case, when
    the code has already been written, one only needs to set the parameters and initial
    configuration in order to get started. In some other cases the problem is posed
    in such a way that a third-party library can be used to solve the most intensive
    part of the code (for example, this is increasingly the case with machine-learning
    workflows in Python). However, these cases are stil quite limited; in general,
    some additional programming might be needed. There are many GPU programming software
    environments and APIs available, which can be broadly grouped into **directive-based
    models**, **non-portable kernel-based models**, and **portable kernel-based models**,
    as well as high-level frameworks and libraries (including attempts at language-level
    support).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU进行计算有不同的方法。在最佳情况下，当代码已经编写完成时，只需设置参数和初始配置即可开始。在某些其他情况下，问题被提出的方式使得可以使用第三方库来解决代码中最密集的部分（例如，这在Python中的机器学习工作流程中越来越常见）。然而，这些情况仍然相当有限；通常，可能需要额外的编程。有许多GPU编程软件环境和API可用，可以大致分为**指令集模型**、**非可移植内核模型**和**可移植内核模型**，以及高级框架和库（包括语言级别的支持尝试）。
- en: Standard C++/Fortran
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准C++/Fortran
- en: Programs written in standard C++ and Fortran languages can now take advantage
    of NVIDIA GPUs without depending on any external library. This is possible thanks
    to the [NVIDIA SDK](https://developer.nvidia.com/hpc-sdk) suite of compilers that
    translates and optimizes the code for running on GPUs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准C++和Fortran语言编写的程序现在可以利用NVIDIA GPU，而无需依赖任何外部库。这要归功于[NVIDIA SDK](https://developer.nvidia.com/hpc-sdk)编译器套件，它可以将代码翻译并优化以在GPU上运行。
- en: '[Here](https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/)
    is the series of articles on acceleration with standard language parallelism.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[这里](https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/)是关于使用标准语言并行性加速的文章系列。'
- en: Guidelines for writing C++ code can be found [here](https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/),
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在这里找到编写C++代码的指南[这里](https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/),
- en: while those for Fortran code can be found [here](https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而Fortran代码的指南可以在[这里](https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/)找到。
- en: The performance of these two approaches is promising, as can be seen in the
    examples provided in those guidelines.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的表现很有希望，正如那些指南中提供的示例所示。
- en: Directive-based programming
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令集编程
- en: A fast and cheap way is to use **directive based** approaches. In this case
    the existing *serial* code is annotated with *hints* which indicate to the compiler
    which loops and regions should be executed on the GPU. In the absence of the API
    the directives are treated as comments and the code will just be executed as a
    usual serial code. This approach is focused on productivity and easy usage (but
    to the possible detriment of performance), and allows employing accelerators with
    minimal programming effort by adding parallelism to existing code without the
    need to write accelerator-specific code. There are two common ways to program
    using directives, namely **OpenACC** and **OpenMP**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一种快速且经济的方法是使用**基于指令**的方法。在这种情况下，现有的*串行*代码被添加了*提示*，这些提示指示编译器哪些循环和区域应该在GPU上执行。如果没有API，指令将被视为注释，代码将像通常的串行代码一样执行。这种方法侧重于生产力和易用性（但可能损害性能），并且通过在不编写特定加速器代码的情况下将并行性添加到现有代码中，以最小的编程努力使用加速器。使用指令编程有两种常见方法，即**OpenACC**和**OpenMP**。
- en: OpenACC
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenACC
- en: '[OpenACC](https://www.openacc.org/) is developed by a consortium formed in
    2010 with the goal of developing a standard, portable, and scalable programming
    model for accelerators, including GPUs. Members of the OpenACC consortium include
    GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers,
    universities, and software companies. Until recently it was supporting only NVIDIA
    GPUs, but now there is effort to support more devices and architectures.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenACC](https://www.openacc.org/) 是由2010年成立的一个联盟开发的，该联盟的目的是开发一个标准、可移植和可扩展的加速器编程模型，包括GPU。OpenACC联盟的成员包括GPU供应商，如NVIDIA和AMD，以及领先的超级计算中心、大学和软件公司。直到最近，它只支持NVIDIA
    GPU，但现在有努力支持更多设备和架构。'
- en: OpenMP
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenMP
- en: '[OpenMP](https://www.openmp.org/) started as a multi-platform, shared-memory
    parallel programming API for multi-core CPUs and relatively recently has added
    support for GPU offloading. OpenMP aims to support various types of GPUs, which
    is done through the parent compiler.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenMP](https://www.openmp.org/) 最初是一个多平台、共享内存并行编程API，用于多核CPU，并且相对较近地增加了对GPU卸载的支持。OpenMP旨在支持各种类型的GPU，这是通过父编译器实现的。'
- en: The directive based approaches work with C/C++ and FORTRAN codes, while some
    third party extensions are available for other languages.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基于指令的方法与C/C++和FORTRAN代码一起工作，同时还有一些第三方扩展可用于其他语言。
- en: Non-portable kernel-based models (native programming models)
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非可移植的基于内核的模型（原生编程模型）
- en: When doing direct GPU programming the developer has a large level of control
    by writing low-level code that directly communicates with the GPU and its hardware.
    Theoretically direct GPU programming methods provide the ability to write low-level,
    GPU-accelerated code that can provide significant performance improvements over
    CPU-only code. However, they also require a deeper understanding of the GPU architecture
    and its capabilities, as well as the specific programming method being used.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行直接GPU编程时，开发者通过编写与GPU及其硬件直接通信的低级代码，拥有很高的控制水平。理论上，直接GPU编程方法能够提供编写低级、GPU加速的代码的能力，这可以在性能上显著优于仅使用CPU的代码。然而，这也要求开发者对GPU架构及其功能有更深入的理解，以及所使用的特定编程方法。
- en: CUDA
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CUDA
- en: '[CUDA](https://developer.nvidia.com/cuda-toolkit) is a parallel computing platform
    and API developed by NVIDIA. It is historically the first mainstream GPU programming
    framework. It allows developers to write C-like code that is executed on the GPU.
    CUDA provides a set of libraries and tools for low-level GPU programming and provides
    a performance boost for demanding computationally-intensive applications. While
    there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDA](https://developer.nvidia.com/cuda-toolkit) 是由NVIDIA开发的一个并行计算平台和API。它历史上是第一个主流GPU编程框架。它允许开发者编写在GPU上执行的类似C的代码。CUDA提供了一套用于低级GPU编程的库和工具，并为计算密集型应用提供了性能提升。尽管存在一个广泛的生态系统，CUDA仍然仅限于NVIDIA硬件。'
- en: HIP
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HIP
- en: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html) (Heterogeneous
    Interface for Portability) is an API developed by AMD that provides a low-level
    interface for GPU programming. HIP is designed to provide a single source code
    that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming
    model and provides an almost identical programming interface to CUDA.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html)（异构接口可移植性）是由AMD开发的一个API，它为GPU编程提供了一个低级接口。HIP旨在提供一个可以在NVIDIA和AMD
    GPU上使用的单一源代码。它基于CUDA编程模型，并提供与CUDA几乎相同的编程接口。'
- en: Multiple examples of CUDA/HIP code are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    directory of this repository.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在此存储库的[content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)目录中提供了多个CUDA/HIP代码示例。
- en: Portable kernel-based models (cross-platform portability ecosystems)
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可移植的基于内核的模型（跨平台可移植性生态系统）
- en: Cross-platform portability ecosystems typically provide a higher-level abstraction
    layer which enables a convenient and portable programming model for GPU programming.
    They can help reduce the time and effort required to maintain and deploy GPU-accelerated
    applications. The goal of these ecosystems is to achieve performance portability
    with a single-source application. In C++, the most notable cross-platform portability
    ecosystems are [SYCL](https://www.khronos.org/sycl/), [OpenCL](https://www.khronos.org/opencl/)
    (C and C++ APIs), and [Kokkos](https://github.com/kokkos/kokkos); others include
    [alpaka](https://alpaka.readthedocs.io/) and [RAJA](https://github.com/LLNL/RAJA).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 跨平台可移植性生态系统通常提供更高层次的抽象层，这使得GPU编程的编程模型既方便又可移植。它们可以帮助减少维护和部署GPU加速应用程序所需的时间和精力。这些生态系统的目标是实现单源应用程序的性能可移植性。在C++中，最著名的跨平台可移植性生态系统是[SYCL](https://www.khronos.org/sycl/)，[OpenCL](https://www.khronos.org/opencl/)（C和C++
    API）和[Kokkos](https://github.com/kokkos/kokkos)；其他还包括[alpaka](https://alpaka.readthedocs.io/)和[RAJA](https://github.com/LLNL/RAJA)。
- en: OpenCL
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenCL
- en: '[OpenCL](https://www.khronos.org/opencl/) (Open Computing Language) is a cross-platform,
    open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs.
    It supports a wide range of hardware from multiple vendors. OpenCL provides a
    low-level programming interface for GPU programming and enables developers to
    write programs that can be executed on a variety of platforms. Unlike programming
    models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model.
    Recent versions of the OpenCL standard added C++ support for both API and the
    kernel code, but the C-based interface is still more widely used. The OpenCL Working
    Group doesn’t provide any frameworks of its own. Instead, vendors who produce
    OpenCL-compliant devices release frameworks as part of their software development
    kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD.
    In both cases, the development kits are free and contain the libraries and tools
    that make it possible to build OpenCL applications.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenCL](https://www.khronos.org/opencl/)（开放计算语言）是一个跨平台的开放标准API，用于在CPU、GPU和FPGA上执行通用并行计算。它支持来自多个供应商的广泛硬件。OpenCL为GPU编程提供了一个低级编程接口，并允许开发者编写可以在各种平台上执行的程序。与CUDA、HIP、Kokkos和SYCL等编程模型不同，OpenCL使用单独的源模型。OpenCL标准的最新版本增加了对API和内核代码的C++支持，但基于C的接口仍然更广泛地使用。OpenCL工作组不提供自己的框架。相反，生产OpenCL兼容设备的供应商将其作为软件开发套件（SDK）的一部分发布框架。最受欢迎的两个OpenCL
    SDK是由NVIDIA和AMD发布的。在这两种情况下，开发套件都是免费的，并包含构建OpenCL应用程序所需的库和工具。'
- en: Kokkos
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kokkos
- en: '[Kokkos](https://github.com/kokkos/kokkos) is an open-source performance portable
    programming model for heterogeneous parallel computing that has been mainly developed
    at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming
    model for developing efficient and scalable parallel applications that run on
    many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists
    of several components, such as the Kokkos core library, which provides parallel
    execution and memory abstraction, the Kokkos kernel library, which provides math
    kernels for linear algebra and graph algorithms, and the Kokkos tools library,
    which provides profiling and debugging tools. Kokkos components integrate well
    with other software libraries and technologies, such as MPI and OpenMP. Furthermore,
    the project collaborates with other projects, in order to provide interoperability
    and standardization for portable C++ programming.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kokkos](https://github.com/kokkos/kokkos) 是一个开源的性能可移植并行计算编程模型，主要在桑迪亚国家实验室开发。它是一个基于
    C++ 的生态系统，为在多核架构（如 CPU、GPU 和 FPGA）上运行的高效和可扩展的并行应用程序提供编程模型。Kokkos 生态系统包括几个组件，例如提供并行执行和内存抽象的
    Kokkos 核心库，提供线性代数和图算法数学内核的 Kokkos 内核库，以及提供性能分析和调试工具的 Kokkos 工具库。Kokkos 组件与 MPI
    和 OpenMP 等其他软件库和技术很好地集成。此外，该项目与其他项目合作，以提供可移植 C++ 编程的互操作性和标准化。'
- en: alpaka
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: alpaka
- en: '[alpaka](https://alpaka.readthedocs.io/) (Abstraction Library for Parallel
    Kernel Acceleration) is an open-source C++ header-only library that aims to provide
    performance portability across heterogeneous accelerator architectures by abstracting
    the underlying levels of parallelism. The library is platform-independent and
    supports the concurrent and cooperative use of multiple devices, including host
    CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[alpaka](https://alpaka.readthedocs.io/)（并行内核加速抽象库）是一个开源的仅包含头文件的 C++ 库，旨在通过抽象底层并行级别，在异构加速器架构之间提供性能可移植性。该库是平台无关的，并支持包括主机
    CPU（x86、ARM、RISC-V）和来自不同供应商的 GPU（NVIDIA、AMD 和 Intel）在内的多个设备的并发和协作使用。'
- en: A key advantage of alpaka is that it requires only a single implementation of
    a user kernel, expressed as a function object with a standardized interface. This
    eliminates the need to write specialized code for different backends. The library
    provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP,
    and serial execution, that can be selected based on the target device. Moreover,
    multiple accelerator backends can even be combined to target different vendor
    hardware within a single application.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: alpaka 的一个关键优势是它只需要用户内核的单个实现，该实现以具有标准化接口的函数对象的形式表达。这消除了为不同后端编写专用代码的需要。该库提供各种加速器后端，包括
    CUDA、HIP、SYCL、OpenMP 和串行执行，可以根据目标设备进行选择。此外，甚至可以将多个加速器后端组合起来，以在单个应用程序中针对不同的供应商硬件。
- en: SYCL
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SYCL
- en: '[SYCL](https://www.khronos.org/sycl/) is a royalty-free, open-standard C++
    programming model for multi-device programming. It provides a high-level, single-source
    programming model for heterogeneous systems, including GPUs. Originally SYCL was
    developed on top of OpenCL; however, it is no more limited to just that. It can
    be implemented on top of other low-level heterogeneous computing APIs, such as
    CUDA or HIP, enabling developers to write programs that can be executed on a variety
    of platforms. Note that while SYCL is relatively high-level model, the developers
    are still required to write GPU kernels explicitly.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[SYCL](https://www.khronos.org/sycl/) 是一个免费的、开放的 C++ 多设备编程标准编程模型。它为包括 GPU 在内的异构系统提供高级、单源编程模型。最初
    SYCL 是在 OpenCL 的基础上开发的；然而，它不再仅限于这一点。它可以在其他低级异构计算 API（如 CUDA 或 HIP）之上实现，使开发者能够编写可在各种平台上执行的程序。请注意，虽然
    SYCL 是一个相对高级的模型，但开发者仍然需要明确编写 GPU 内核。'
- en: While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only
    a standard, for which several implementations exist. For GPU programming, [Intel
    oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)
    (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with [Codeplay oneAPI
    plugins](https://codeplay.com/solutions/oneapi/)) and [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)
    (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs,
    with experimental Intel GPU support available in combination with Intel oneAPI
    DPC++) are the most widely used. Other implementations of note are [triSYCL](https://github.com/triSYCL/triSYCL)
    and [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 alpaka、Kokkos 和 RAJA 指的是特定的项目，但 SYCL 本身只是一个标准，目前存在多个实现。对于 GPU 编程，[Intel oneAPI
    DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)（原生支持
    Intel GPU，以及通过 [Codeplay oneAPI 插件](https://codeplay.com/solutions/oneapi/) 支持的
    NVIDIA 和 AMD GPU）和 [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)（之前也称为
    hipSYCL 或 Open SYCL，支持 NVIDIA 和 AMD GPU，与 Intel oneAPI DPC++ 结合使用时提供实验性的 Intel
    GPU 支持）是最广泛使用的。其他值得注意的实现包括 [triSYCL](https://github.com/triSYCL/triSYCL) 和 [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/)。
- en: High-level language support
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级语言支持
- en: Python
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python
- en: Python offers support for GPU programming through multiple abstraction levels.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Python 通过多个抽象级别提供对 GPU 编程的支持。
- en: '**CUDA Python, HIP Python and PyCUDA**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA Python、HIP Python 和 PyCUDA**'
- en: These projects are, respectively, [NVIDIA-](https://developer.nvidia.com/cuda-python),
    [AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/) and [community-supported](https://documen.tician.de/pycuda/)
    wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use
    these approaches directly, in most cases knowledge of CUDA or HIP programming
    is needed.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目分别是 [NVIDIA-](https://developer.nvidia.com/cuda-python)、[AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/)
    和 [社区支持的](https://documen.tician.de/pycuda/)包装器，为低级 CUDA 和 HIP API 提供了 Python
    绑定。要直接使用这些方法，在大多数情况下需要了解 CUDA 或 HIP 编程。
- en: CUDA Python also aims to support higher-level toolkits and libraries, such as
    **CuPy** and **Numba**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA Python 也旨在支持高级工具包和库，例如 **CuPy** 和 **Numba**。
- en: '**CuPy**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**CuPy**'
- en: '[CuPy](https://cupy.dev/) is a GPU-based data array library compatible with
    NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it
    easy for developers to transition to GPU computing. Code written with NumPy can
    often be adapted to use CuPy with minimal modifications; in most straightforward
    cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’
    in their Python code.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[CuPy](https://cupy.dev/) 是一个基于 GPU 的数据数组库，与 NumPy/SciPy 兼容。它提供了一个与 NumPy 和
    SciPy 非常相似的接口，使得开发者可以轻松过渡到 GPU 计算。使用 NumPy 编写的代码通常可以经过最小修改后适应 CuPy；在大多数直接情况下，人们可能只需在他们的
    Python 代码中将 ''numpy'' 和 ''scipy'' 替换为 ''cupy'' 和 ''cupyx.scipy''。'
- en: '**Numba**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**Numba**'
- en: '[Numba](https://numba.pydata.org/) is an open-source JIT compiler that translates
    a subset of Python and NumPy code into optimized machine code. Numba supports
    CUDA-capable GPUs and is able to generate code for them using several different
    syntax variants. In 2021, upstream support for [AMD (ROCm) support](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)
    was discontinued. However, as of 2025, AMD has added downstream support for the
    Numba API through the [Numba HIP package](https://github.com/ROCm/numba-hip).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[Numba](https://numba.pydata.org/) 是一个开源的 JIT 编译器，它将 Python 和 NumPy 代码的一个子集转换为优化的机器代码。Numba
    支持 CUDA 兼容的 GPU，并能够使用几种不同的语法变体为它们生成代码。2021 年，对 [AMD (ROCm) 支持](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)的上游支持已被终止。然而，截至
    2025 年，AMD 通过 [Numba HIP 包](https://github.com/ROCm/numba-hip)为 Numba API 添加了下游支持。'
- en: Julia
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Julia
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 通过以下针对所有三个主要供应商的 GPU 的包提供了一等 GPU 编程支持：
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA.jl](https://cuda.juliagpu.org/stable/) 用于 NVIDIA GPU'
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) 用于 AMD GPU'
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) 用于 Intel GPU'
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) 用于 Apple M 系列GPU'
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance. Their respective
    APIs are however completely analogous and translation between libraries is straightforward.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA.jl`是最成熟的，`AMDGPU.jl`稍微落后但仍然可以通用，而`oneAPI.jl`和`Metal.jl`功能齐全但可能存在错误，缺少一些功能，并提供次优性能。然而，它们的相应API是完全类似的，库之间的转换简单直接。'
- en: All packages offer both high-level abstractions that require very little programming
    effort and a lower level approach for writing kernels for fine-grained control.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 所有包都提供了高级抽象，需要极少的编程努力，以及用于编写内核的较低级别方法，以实现细粒度控制。
- en: In short
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: '**Directive-based programming:**'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于指令的编程：**'
- en: Existing serial code is annotated with directives to indicate which parts should
    be executed on the GPU.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的串行代码通过指令进行注释，以指示应在GPU上执行的部分。
- en: OpenACC and OpenMP are common directive-based programming models.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC和OpenMP是常见的基于指令的编程模型。
- en: Productivity and easy usage are prioritized over performance.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑生产力和易用性，而不是性能。
- en: Minimum programming effort is required to add parallelism to existing code.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将并行性添加到现有代码中所需的编程工作量最小。
- en: '**Non-portable kernel-based models:**'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可移植的基于内核的模型：**'
- en: Low-level code is written to directly communicate with the GPU.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低级代码是直接与GPU通信编写的。
- en: CUDA is NVIDIA’s parallel computing platform and API for GPU programming.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA是NVIDIA的并行计算平台和GPU编程API。
- en: HIP is an API developed by AMD that provides a similar programming interface
    to CUDA for both NVIDIA and AMD GPUs.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HIP是AMD开发的一个API，为NVIDIA和AMD的GPU提供了与CUDA类似的编程接口。
- en: Deeper understanding of GPU architecture and programming methods is needed.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要更深入地了解GPU架构和编程方法。
- en: '**Portable kernel-based models:**'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植的基于内核的模型：**'
- en: Higher-level abstractions for GPU programming that provide portability.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为GPU编程提供可移植性的高级抽象。
- en: Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括OpenCL、Kokkos、alpaka、RAJA和SYCL。
- en: Aim to achieve performance portability with a single-source application.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旨在通过单个源应用程序实现性能的可移植性。
- en: Can run on various GPUs and platforms, reducing the effort required to maintain
    and deploy GPU-accelerated applications.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在多种GPU和平台上运行，从而减少维护和部署GPU加速应用程序所需的努力。
- en: '**High-level language support:**'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级语言支持：**'
- en: C++ and Fortran feature initiatives to support GPUs through language-standard
    parallelism.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++和Fortran有支持GPU的语言标准并行的计划。
- en: Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库如PyCUDA、CuPy和Numba提供了GPU编程能力。
- en: Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU
    programming.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia有CUDA.jl、AMDGPU.jl、oneAPI.jl和Metal.jl等用于GPU编程的包。
- en: These approaches provide high-level abstraction and interfaces for GPU programming
    in the respective languages.
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法为各自语言中的GPU编程提供了高级抽象和接口。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Each of these GPU programming environments has its own strengths and weaknesses,
    and the best choice for a given project will depend on a range of factors, including:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 每个GPU编程环境都有其自身的优点和缺点，对于特定项目而言，最佳选择将取决于一系列因素，包括：
- en: the hardware platforms being targeted,
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标硬件平台，
- en: the type of computation being performed, and
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行的计算类型，以及
- en: the developer’s experience and preferences.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者的经验和偏好。
- en: '**High-level and productivity-focused APIs** provide a simplified programming
    model and maximize code portability, while **low-level and performance-focused
    APIs** provide a high level of control over the GPU’s hardware but also require
    more coding effort and expertise.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**高级且注重生产力的API**提供了一个简化的编程模型并最大化代码的可移植性，而**低级且注重性能的API**则提供了对GPU硬件的高级别控制，但也需要更多的编码努力和专业知识。'
- en: Exercises
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Discussion
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论
- en: Which GPU programming frameworks have you already used previously, if any?
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您之前是否已经使用过任何GPU编程框架？
- en: What did you find most challenging? What was most useful?
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您认为哪部分最具挑战性？哪部分最有用？
- en: Let us know in the main room or via comments in HackMD document.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请在主房间或通过HackMD文档中的评论告知我们。
- en: Keypoints
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: GPU programming approaches can be split into 1) directive-based, 2) non-portable
    kernel-based, 3) portable kernel-based, and 4) high-level language support.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU编程方法可以分为1）基于指令的，2）不可移植的基于内核的，3）可移植的基于内核的，和4）高级语言支持。
- en: There are multiple frameworks/languages available for each approach, each with
    pros and cons.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种方法都有多个框架/语言可供选择，各有优缺点。
- en: Standard C++/Fortran
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准C++/Fortran
- en: Programs written in standard C++ and Fortran languages can now take advantage
    of NVIDIA GPUs without depending on any external library. This is possible thanks
    to the [NVIDIA SDK](https://developer.nvidia.com/hpc-sdk) suite of compilers that
    translates and optimizes the code for running on GPUs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准C++和Fortran语言编写的程序现在可以利用NVIDIA GPU，而不需要依赖任何外部库。这是由于[NVIDIA SDK](https://developer.nvidia.com/hpc-sdk)编译器套件，它可以将代码翻译并优化以在GPU上运行。
- en: '[Here](https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/)
    is the series of articles on acceleration with standard language parallelism.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[这里](https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/)是关于使用标准语言并行化加速的文章系列。'
- en: Guidelines for writing C++ code can be found [here](https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/),
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在[这里](https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/)找到编写C++代码的指南，
- en: while those for Fortran code can be found [here](https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而Fortran代码的指南可以在[这里](https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/)找到。
- en: The performance of these two approaches is promising, as can be seen in the
    examples provided in those guidelines.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的表现很有希望，如那些指南中提供的示例所示。
- en: Directive-based programming
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令基于编程
- en: A fast and cheap way is to use **directive based** approaches. In this case
    the existing *serial* code is annotated with *hints* which indicate to the compiler
    which loops and regions should be executed on the GPU. In the absence of the API
    the directives are treated as comments and the code will just be executed as a
    usual serial code. This approach is focused on productivity and easy usage (but
    to the possible detriment of performance), and allows employing accelerators with
    minimal programming effort by adding parallelism to existing code without the
    need to write accelerator-specific code. There are two common ways to program
    using directives, namely **OpenACC** and **OpenMP**.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一种快速且经济的方法是使用**指令基于**的方法。在这种情况下，现有的**串行**代码被添加了**提示**，指示编译器哪些循环和区域应该在GPU上执行。如果没有API，指令将被视为注释，代码将像通常的串行代码一样执行。这种方法侧重于生产力和易用性（但可能损害性能），通过向现有代码添加并行性而不需要编写特定于加速器的代码，可以以最小的编程工作量使用加速器。使用指令编程有两种常见方法，即**OpenACC**和**OpenMP**。
- en: OpenACC
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenACC
- en: '[OpenACC](https://www.openacc.org/) is developed by a consortium formed in
    2010 with the goal of developing a standard, portable, and scalable programming
    model for accelerators, including GPUs. Members of the OpenACC consortium include
    GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers,
    universities, and software companies. Until recently it was supporting only NVIDIA
    GPUs, but now there is effort to support more devices and architectures.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenACC](https://www.openacc.org/)是由2010年成立的一个联盟开发的，旨在为加速器（包括GPU）开发一个标准、可移植和可扩展的编程模型。OpenACC联盟的成员包括GPU供应商，如NVIDIA和AMD，以及领先的超级计算中心、大学和软件公司。直到最近，它只支持NVIDIA
    GPU，但现在有努力支持更多设备和架构。'
- en: OpenMP
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenMP
- en: '[OpenMP](https://www.openmp.org/) started as a multi-platform, shared-memory
    parallel programming API for multi-core CPUs and relatively recently has added
    support for GPU offloading. OpenMP aims to support various types of GPUs, which
    is done through the parent compiler.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenMP](https://www.openmp.org/)最初是一个多平台、共享内存并行编程API，用于多核CPU，最近也增加了对GPU卸载的支持。OpenMP旨在支持各种类型的GPU，这是通过父编译器实现的。'
- en: The directive based approaches work with C/C++ and FORTRAN codes, while some
    third party extensions are available for other languages.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 指令基于的方法与C/C++和FORTRAN代码一起工作，而一些第三方扩展可用于其他语言。
- en: OpenACC
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenACC
- en: '[OpenACC](https://www.openacc.org/) is developed by a consortium formed in
    2010 with the goal of developing a standard, portable, and scalable programming
    model for accelerators, including GPUs. Members of the OpenACC consortium include
    GPU vendors, such as NVIDIA and AMD, as well as leading supercomputing centers,
    universities, and software companies. Until recently it was supporting only NVIDIA
    GPUs, but now there is effort to support more devices and architectures.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenACC](https://www.openacc.org/)是由2010年成立的一个联盟开发的，该联盟的目标是开发一个标准、可移植和可扩展的编程模型，用于加速器，包括GPU。OpenACC联盟的成员包括GPU供应商，如NVIDIA和AMD，以及领先的超级计算中心、大学和软件公司。直到最近，它只支持NVIDIA
    GPU，但现在正在努力支持更多设备和架构。'
- en: OpenMP
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenMP
- en: '[OpenMP](https://www.openmp.org/) started as a multi-platform, shared-memory
    parallel programming API for multi-core CPUs and relatively recently has added
    support for GPU offloading. OpenMP aims to support various types of GPUs, which
    is done through the parent compiler.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenMP](https://www.openmp.org/)最初是一个针对多平台、共享内存并行编程API，用于多核CPU，并且最近增加了对GPU卸载的支持。OpenMP旨在支持各种类型的GPU，这是通过父编译器实现的。'
- en: The directive based approaches work with C/C++ and FORTRAN codes, while some
    third party extensions are available for other languages.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 基于指令的方法适用于C/C++和FORTRAN代码，同时还有一些第三方扩展适用于其他语言。
- en: Non-portable kernel-based models (native programming models)
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不便携的基于内核的模型（原生编程模型）
- en: When doing direct GPU programming the developer has a large level of control
    by writing low-level code that directly communicates with the GPU and its hardware.
    Theoretically direct GPU programming methods provide the ability to write low-level,
    GPU-accelerated code that can provide significant performance improvements over
    CPU-only code. However, they also require a deeper understanding of the GPU architecture
    and its capabilities, as well as the specific programming method being used.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行直接GPU编程时，开发者可以通过编写直接与GPU及其硬件通信的低级代码来获得很高的控制权。理论上，直接GPU编程方法提供了编写低级、GPU加速代码的能力，这可以在性能上显著优于仅使用CPU的代码。然而，这也需要更深入地了解GPU架构及其功能，以及所使用的特定编程方法。
- en: CUDA
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CUDA
- en: '[CUDA](https://developer.nvidia.com/cuda-toolkit) is a parallel computing platform
    and API developed by NVIDIA. It is historically the first mainstream GPU programming
    framework. It allows developers to write C-like code that is executed on the GPU.
    CUDA provides a set of libraries and tools for low-level GPU programming and provides
    a performance boost for demanding computationally-intensive applications. While
    there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDA](https://developer.nvidia.com/cuda-toolkit)是由NVIDIA开发的一个并行计算平台和API。它历史上是第一个主流GPU编程框架。它允许开发者编写在GPU上执行的类似C的代码。CUDA提供了一套用于低级GPU编程的库和工具，并为计算密集型应用提供了性能提升。虽然有一个广泛的生态系统，但CUDA仅限于NVIDIA硬件。'
- en: HIP
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HIP
- en: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html) (Heterogeneous
    Interface for Portability) is an API developed by AMD that provides a low-level
    interface for GPU programming. HIP is designed to provide a single source code
    that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming
    model and provides an almost identical programming interface to CUDA.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html)（异构接口用于可移植性）是由AMD开发的一个API，它为GPU编程提供了一个低级接口。HIP旨在提供可以在NVIDIA和AMD
    GPU上使用的单一源代码。它基于CUDA编程模型，并提供与CUDA几乎相同的编程接口。'
- en: Multiple examples of CUDA/HIP code are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    directory of this repository.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在此存储库的[content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)目录中提供了多个CUDA/HIP代码示例。
- en: CUDA
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CUDA
- en: '[CUDA](https://developer.nvidia.com/cuda-toolkit) is a parallel computing platform
    and API developed by NVIDIA. It is historically the first mainstream GPU programming
    framework. It allows developers to write C-like code that is executed on the GPU.
    CUDA provides a set of libraries and tools for low-level GPU programming and provides
    a performance boost for demanding computationally-intensive applications. While
    there is an extensive ecosystem, CUDA is restricted to NVIDIA hardware.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDA](https://developer.nvidia.com/cuda-toolkit) 是由 NVIDIA 开发的并行计算平台和 API。它在历史上是第一个主流的
    GPU 编程框架。它允许开发者编写类似于 C 的代码，这些代码在 GPU 上执行。CUDA 为低级 GPU 编程提供了一套库和工具，并为计算密集型应用程序提供了性能提升。虽然有一个广泛的生态系统，但
    CUDA 仅限于 NVIDIA 硬件。'
- en: HIP
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HIP
- en: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html) (Heterogeneous
    Interface for Portability) is an API developed by AMD that provides a low-level
    interface for GPU programming. HIP is designed to provide a single source code
    that can be used on both NVIDIA and AMD GPUs. It is based on the CUDA programming
    model and provides an almost identical programming interface to CUDA.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html)（异构接口可移植性）是由
    AMD 开发的一个 API，它为 GPU 编程提供了一个低级接口。HIP 设计用于提供可以在 NVIDIA 和 AMD GPU 上使用的单一源代码。它基于
    CUDA 编程模型，并提供与 CUDA 几乎相同的编程接口。'
- en: Multiple examples of CUDA/HIP code are available in the [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    directory of this repository.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在此存储库的 [content/examples/cuda-hip](https://github.com/ENCCS/gpu-programming/tree/main/content/examples/cuda-hip)
    目录中提供了多个 CUDA/HIP 代码示例。
- en: Portable kernel-based models (cross-platform portability ecosystems)
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可移植的基于内核的模型（跨平台可移植性生态系统）
- en: Cross-platform portability ecosystems typically provide a higher-level abstraction
    layer which enables a convenient and portable programming model for GPU programming.
    They can help reduce the time and effort required to maintain and deploy GPU-accelerated
    applications. The goal of these ecosystems is to achieve performance portability
    with a single-source application. In C++, the most notable cross-platform portability
    ecosystems are [SYCL](https://www.khronos.org/sycl/), [OpenCL](https://www.khronos.org/opencl/)
    (C and C++ APIs), and [Kokkos](https://github.com/kokkos/kokkos); others include
    [alpaka](https://alpaka.readthedocs.io/) and [RAJA](https://github.com/LLNL/RAJA).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 跨平台可移植性生态系统通常提供更高层次的抽象层，这使得 GPU 编程具有方便且可移植的编程模型。它们可以帮助减少维护和部署 GPU 加速应用程序所需的时间和精力。这些生态系统的目标是实现单源应用程序的性能可移植性。在
    C++ 中，最著名的跨平台可移植性生态系统是 [SYCL](https://www.khronos.org/sycl/)、[OpenCL](https://www.khronos.org/opencl/)（C
    和 C++ API）和 [Kokkos](https://github.com/kokkos/kokkos)；其他还包括 [alpaka](https://alpaka.readthedocs.io/)
    和 [RAJA](https://github.com/LLNL/RAJA)。
- en: OpenCL
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenCL
- en: '[OpenCL](https://www.khronos.org/opencl/) (Open Computing Language) is a cross-platform,
    open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs.
    It supports a wide range of hardware from multiple vendors. OpenCL provides a
    low-level programming interface for GPU programming and enables developers to
    write programs that can be executed on a variety of platforms. Unlike programming
    models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model.
    Recent versions of the OpenCL standard added C++ support for both API and the
    kernel code, but the C-based interface is still more widely used. The OpenCL Working
    Group doesn’t provide any frameworks of its own. Instead, vendors who produce
    OpenCL-compliant devices release frameworks as part of their software development
    kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD.
    In both cases, the development kits are free and contain the libraries and tools
    that make it possible to build OpenCL applications.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenCL](https://www.khronos.org/opencl/)（开放计算语言）是一个跨平台的开放标准 API，用于在 CPU、GPU
    和 FPGA 上进行通用并行计算。它支持来自多个供应商的广泛硬件。OpenCL 为 GPU 编程提供了一个低级编程接口，并允许开发者编写可以在各种平台上执行的程序。与
    CUDA、HIP、Kokkos 和 SYCL 等编程模型不同，OpenCL 使用单独的源模型。OpenCL 标准的最新版本增加了对 API 和内核代码的 C++
    支持，但基于 C 的接口仍然更广泛地使用。OpenCL 工作组不提供任何自己的框架。相反，生产 OpenCL 兼容设备的供应商作为其软件开发套件（SDK）的一部分发布框架。最受欢迎的两个
    OpenCL SDK 分别由 NVIDIA 和 AMD 发布。在两种情况下，开发套件都是免费的，并包含构建 OpenCL 应用程序所需的库和工具。'
- en: Kokkos
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kokkos
- en: '[Kokkos](https://github.com/kokkos/kokkos) is an open-source performance portable
    programming model for heterogeneous parallel computing that has been mainly developed
    at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming
    model for developing efficient and scalable parallel applications that run on
    many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists
    of several components, such as the Kokkos core library, which provides parallel
    execution and memory abstraction, the Kokkos kernel library, which provides math
    kernels for linear algebra and graph algorithms, and the Kokkos tools library,
    which provides profiling and debugging tools. Kokkos components integrate well
    with other software libraries and technologies, such as MPI and OpenMP. Furthermore,
    the project collaborates with other projects, in order to provide interoperability
    and standardization for portable C++ programming.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kokkos](https://github.com/kokkos/kokkos) 是一个开源的性能可移植编程模型，主要用于异构并行计算，主要在桑迪亚国家实验室开发。它是一个基于C++的生态系统，为在多核架构（如CPU、GPU和FPGA）上运行的高效和可扩展的并行应用程序提供编程模型。Kokkos生态系统包括几个组件，如Kokkos核心库，它提供并行执行和内存抽象；Kokkos内核库，它提供线性代数和图算法的数学内核；以及Kokkos工具库，它提供分析和调试工具。Kokkos组件与其他软件库和技术（如MPI和OpenMP）集成良好。此外，该项目与其他项目合作，以提供可移植C++编程的互操作性和标准化。'
- en: alpaka
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: alpaka
- en: '[alpaka](https://alpaka.readthedocs.io/) (Abstraction Library for Parallel
    Kernel Acceleration) is an open-source C++ header-only library that aims to provide
    performance portability across heterogeneous accelerator architectures by abstracting
    the underlying levels of parallelism. The library is platform-independent and
    supports the concurrent and cooperative use of multiple devices, including host
    CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[alpaka](https://alpaka.readthedocs.io/)（并行内核加速抽象库）是一个开源的仅包含头文件的C++库，旨在通过抽象并行性的底层级别，实现跨异构加速器架构的性能可移植性。该库是平台无关的，并支持多个设备的并发和协作使用，包括主机CPU（x86、ARM、RISC-V）和来自不同供应商的GPU（NVIDIA、AMD和Intel）。'
- en: A key advantage of alpaka is that it requires only a single implementation of
    a user kernel, expressed as a function object with a standardized interface. This
    eliminates the need to write specialized code for different backends. The library
    provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP,
    and serial execution, that can be selected based on the target device. Moreover,
    multiple accelerator backends can even be combined to target different vendor
    hardware within a single application.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: alpaka的一个关键优势是它只需要一个用户内核的单个实现，该实现以具有标准化接口的函数对象的形式表达。这消除了为不同后端编写专用代码的需要。该库提供各种加速器后端，包括CUDA、HIP、SYCL、OpenMP和串行执行，可以根据目标设备进行选择。此外，甚至可以将多个加速器后端组合起来，以在单个应用程序中针对不同供应商的硬件。
- en: SYCL
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SYCL
- en: '[SYCL](https://www.khronos.org/sycl/) is a royalty-free, open-standard C++
    programming model for multi-device programming. It provides a high-level, single-source
    programming model for heterogeneous systems, including GPUs. Originally SYCL was
    developed on top of OpenCL; however, it is no more limited to just that. It can
    be implemented on top of other low-level heterogeneous computing APIs, such as
    CUDA or HIP, enabling developers to write programs that can be executed on a variety
    of platforms. Note that while SYCL is relatively high-level model, the developers
    are still required to write GPU kernels explicitly.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[SYCL](https://www.khronos.org/sycl/) 是一个免版税的开放标准C++编程模型，用于多设备编程。它为包括GPU在内的异构系统提供了一种高级、单源编程模型。最初，SYCL是在OpenCL之上开发的；然而，它不再仅限于这一点。它可以在其他低级异构计算API之上实现，例如CUDA或HIP，使开发者能够编写可在各种平台上执行的程序。请注意，虽然SYCL是一个相对高级的模型，但开发者仍然需要明确编写GPU内核。'
- en: While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only
    a standard, for which several implementations exist. For GPU programming, [Intel
    oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)
    (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with [Codeplay oneAPI
    plugins](https://codeplay.com/solutions/oneapi/)) and [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)
    (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs,
    with experimental Intel GPU support available in combination with Intel oneAPI
    DPC++) are the most widely used. Other implementations of note are [triSYCL](https://github.com/triSYCL/triSYCL)
    and [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 alpaka、Kokkos和RAJA指的是特定的项目，但SYCL本身只是一个标准，存在多个实现。对于GPU编程，[Intel oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)（原生支持Intel
    GPU，以及通过[Codeplay oneAPI插件](https://codeplay.com/solutions/oneapi/)支持NVIDIA和AMD
    GPU）和[AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)（之前也称为hipSYCL或Open
    SYCL，支持NVIDIA和AMD GPU，与Intel oneAPI DPC++结合使用时提供实验性的Intel GPU支持）是最广泛使用的。其他值得注意的实现包括[triSYCL](https://github.com/triSYCL/triSYCL)和[ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/)。
- en: OpenCL
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenCL
- en: '[OpenCL](https://www.khronos.org/opencl/) (Open Computing Language) is a cross-platform,
    open-standard API for general-purpose parallel computing on CPUs, GPUs and FPGAs.
    It supports a wide range of hardware from multiple vendors. OpenCL provides a
    low-level programming interface for GPU programming and enables developers to
    write programs that can be executed on a variety of platforms. Unlike programming
    models such as CUDA, HIP, Kokkos, and SYCL, OpenCL uses a separate-source model.
    Recent versions of the OpenCL standard added C++ support for both API and the
    kernel code, but the C-based interface is still more widely used. The OpenCL Working
    Group doesn’t provide any frameworks of its own. Instead, vendors who produce
    OpenCL-compliant devices release frameworks as part of their software development
    kits (SDKs). The two most popular OpenCL SDKs are released by NVIDIA and AMD.
    In both cases, the development kits are free and contain the libraries and tools
    that make it possible to build OpenCL applications.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenCL](https://www.khronos.org/opencl/)（开放计算语言）是一个跨平台的、开放标准的API，用于在CPU、GPU和FPGA上执行通用并行计算。它支持来自多个供应商的广泛硬件。OpenCL为GPU编程提供了一个低级编程接口，并允许开发者编写可在各种平台上执行的程序。与CUDA、HIP、Kokkos和SYCL等编程模型不同，OpenCL使用单独的源模型。OpenCL标准的最新版本增加了对API和内核代码的C++支持，但基于C的接口仍然更广泛地被使用。OpenCL工作组不提供任何自己的框架。相反，生产OpenCL兼容设备的供应商将其框架作为其软件开发工具包（SDK）的一部分发布。最受欢迎的两个OpenCL
    SDK是由NVIDIA和AMD发布的。在两种情况下，开发套件都是免费的，并包含构建OpenCL应用程序所需的库和工具。'
- en: Kokkos
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kokkos
- en: '[Kokkos](https://github.com/kokkos/kokkos) is an open-source performance portable
    programming model for heterogeneous parallel computing that has been mainly developed
    at Sandia National Laboratories. It is a C++-based ecosystem that provides a programming
    model for developing efficient and scalable parallel applications that run on
    many-core architectures such as CPUs, GPUs, and FPGAs. The Kokkos ecosystem consists
    of several components, such as the Kokkos core library, which provides parallel
    execution and memory abstraction, the Kokkos kernel library, which provides math
    kernels for linear algebra and graph algorithms, and the Kokkos tools library,
    which provides profiling and debugging tools. Kokkos components integrate well
    with other software libraries and technologies, such as MPI and OpenMP. Furthermore,
    the project collaborates with other projects, in order to provide interoperability
    and standardization for portable C++ programming.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kokkos](https://github.com/kokkos/kokkos)是一个开源的性能可移植编程模型，用于异构并行计算，主要在桑迪亚国家实验室开发。它是一个基于C++的生态系统，为在CPU、GPU和FPGA等多核架构上运行的高效和可扩展的并行应用程序提供编程模型。Kokkos生态系统包括几个组件，例如Kokkos核心库，它提供并行执行和内存抽象；Kokkos内核库，它提供线性代数和图算法的数学内核；以及Kokkos工具库，它提供分析和调试工具。Kokkos组件与其他软件库和技术（如MPI和OpenMP）集成良好。此外，该项目与其他项目合作，以提供可移植C++编程的互操作性和标准化。'
- en: alpaka
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: alpaka
- en: '[alpaka](https://alpaka.readthedocs.io/) (Abstraction Library for Parallel
    Kernel Acceleration) is an open-source C++ header-only library that aims to provide
    performance portability across heterogeneous accelerator architectures by abstracting
    the underlying levels of parallelism. The library is platform-independent and
    supports the concurrent and cooperative use of multiple devices, including host
    CPUs (x86, ARM, RISC-V) and GPUs from different vendors (NVIDIA, AMD, and Intel).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[alpaka](https://alpaka.readthedocs.io/)（并行内核加速抽象库）是一个开源的仅包含头文件的 C++ 库，旨在通过抽象并行性的底层级别，实现跨异构加速器架构的性能可移植性。该库是平台无关的，并支持包括主机
    CPU（x86、ARM、RISC-V）和来自不同供应商的 GPU（NVIDIA、AMD 和 Intel）在内的多个设备的并发和协作使用。'
- en: A key advantage of alpaka is that it requires only a single implementation of
    a user kernel, expressed as a function object with a standardized interface. This
    eliminates the need to write specialized code for different backends. The library
    provides a variety of accelerator backends, including CUDA, HIP, SYCL, OpenMP,
    and serial execution, that can be selected based on the target device. Moreover,
    multiple accelerator backends can even be combined to target different vendor
    hardware within a single application.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: alpaka 的一个关键优势是它只需要一个用户内核的单个实现，该实现以具有标准化接口的函数对象的形式表达。这消除了为不同后端编写专用代码的需要。该库提供各种加速器后端，包括
    CUDA、HIP、SYCL、OpenMP 和串行执行，可以根据目标设备进行选择。此外，甚至可以将多个加速器后端组合起来，以在单个应用程序中针对不同供应商的硬件。
- en: SYCL
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SYCL
- en: '[SYCL](https://www.khronos.org/sycl/) is a royalty-free, open-standard C++
    programming model for multi-device programming. It provides a high-level, single-source
    programming model for heterogeneous systems, including GPUs. Originally SYCL was
    developed on top of OpenCL; however, it is no more limited to just that. It can
    be implemented on top of other low-level heterogeneous computing APIs, such as
    CUDA or HIP, enabling developers to write programs that can be executed on a variety
    of platforms. Note that while SYCL is relatively high-level model, the developers
    are still required to write GPU kernels explicitly.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[SYCL](https://www.khronos.org/sycl/) 是一个免费的、开放的 C++ 多设备编程编程模型标准。它为包括 GPU 在内的异构系统提供了一种高级、单源编程模型。最初，SYCL
    是在 OpenCL 的基础上开发的；然而，它不再仅限于这一点。它可以在其他低级异构计算 API 上实现，如 CUDA 或 HIP，使开发者能够编写可在各种平台上执行的程序。请注意，虽然
    SYCL 是一个相对高级的模型，但开发者仍然需要显式编写 GPU 内核。'
- en: While alpaka, Kokkos, and RAJA refer to specific projects, SYCL itself is only
    a standard, for which several implementations exist. For GPU programming, [Intel
    oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)
    (supporting Intel GPUs natively, and NVIDIA and AMD GPUs with [Codeplay oneAPI
    plugins](https://codeplay.com/solutions/oneapi/)) and [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)
    (previously also known as hipSYCL or Open SYCL, supporting NVIDIA and AMD GPUs,
    with experimental Intel GPU support available in combination with Intel oneAPI
    DPC++) are the most widely used. Other implementations of note are [triSYCL](https://github.com/triSYCL/triSYCL)
    and [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 alpaka、Kokkos 和 RAJA 指的是特定的项目，但 SYCL 本身只是一个标准，目前存在多个实现。对于 GPU 编程，[Intel oneAPI
    DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)（原生支持
    Intel GPU，以及通过 [Codeplay oneAPI 插件](https://codeplay.com/solutions/oneapi/) 支持的
    NVIDIA 和 AMD GPU）和 [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)（之前也称为
    hipSYCL 或 Open SYCL，支持 NVIDIA 和 AMD GPU，与 Intel oneAPI DPC++ 结合使用时提供实验性的 Intel
    GPU 支持）是最广泛使用的。其他值得注意的实现包括 [triSYCL](https://github.com/triSYCL/triSYCL) 和 [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/)。
- en: High-level language support
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级语言支持
- en: Python
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python
- en: Python offers support for GPU programming through multiple abstraction levels.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Python 通过多个抽象级别提供对 GPU 编程的支持。
- en: '**CUDA Python, HIP Python and PyCUDA**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA Python，HIP Python 和 PyCUDA**'
- en: These projects are, respectively, [NVIDIA-](https://developer.nvidia.com/cuda-python),
    [AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/) and [community-supported](https://documen.tician.de/pycuda/)
    wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use
    these approaches directly, in most cases knowledge of CUDA or HIP programming
    is needed.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目分别是分别为 [NVIDIA-](https://developer.nvidia.com/cuda-python)、[AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/)
    和 [社区支持的](https://documen.tician.de/pycuda/)包装器，提供对低级 CUDA 和 HIP API 的 Python
    绑定。要直接使用这些方法，在大多数情况下需要了解 CUDA 或 HIP 编程。
- en: CUDA Python also aims to support higher-level toolkits and libraries, such as
    **CuPy** and **Numba**.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA Python 也旨在支持更高层次的工具包和库，例如 **CuPy** 和 **Numba**。
- en: '**CuPy**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**CuPy**'
- en: '[CuPy](https://cupy.dev/) is a GPU-based data array library compatible with
    NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it
    easy for developers to transition to GPU computing. Code written with NumPy can
    often be adapted to use CuPy with minimal modifications; in most straightforward
    cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’
    in their Python code.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[CuPy](https://cupy.dev/) 是一个与NumPy/SciPy兼容的GPU数据数组库。它提供了与NumPy和SciPy高度相似的用户界面，使得开发者可以轻松过渡到GPU计算。使用NumPy编写的代码通常可以经过最小修改后适应CuPy；在大多数直接的情况下，人们可能只需在Python代码中将‘numpy’和‘scipy’替换为‘cupy’和‘cupyx.scipy’。'
- en: '**Numba**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**Numba**'
- en: '[Numba](https://numba.pydata.org/) is an open-source JIT compiler that translates
    a subset of Python and NumPy code into optimized machine code. Numba supports
    CUDA-capable GPUs and is able to generate code for them using several different
    syntax variants. In 2021, upstream support for [AMD (ROCm) support](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)
    was discontinued. However, as of 2025, AMD has added downstream support for the
    Numba API through the [Numba HIP package](https://github.com/ROCm/numba-hip).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[Numba](https://numba.pydata.org/) 是一个开源的JIT编译器，它将Python和NumPy代码的一个子集转换为优化的机器代码。Numba支持具有CUDA功能的GPU，并能够使用几种不同的语法变体为它们生成代码。2021年，对
    [AMD (ROCm) 支持](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)
    的上游支持已被终止。然而，截至2025年，AMD已通过 [Numba HIP包](https://github.com/ROCm/numba-hip) 为Numba
    API添加了下游支持。'
- en: Julia
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Julia
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Julia通过以下针对所有三个主要供应商的GPU的包，为GPU编程提供了一级支持：
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA.jl](https://cuda.juliagpu.org/stable/) 用于NVIDIA GPU'
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) 用于AMD GPU'
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) 用于Intel GPU'
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) 用于Apple M系列GPU'
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance. Their respective
    APIs are however completely analogous and translation between libraries is straightforward.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA.jl` 是最成熟的，`AMDGPU.jl` 稍微落后但仍然适用于通用用途，而 `oneAPI.jl` 和 `Metal.jl` 虽然功能齐全但可能包含错误，缺少一些功能，并提供次优性能。然而，它们的相应API是完全类似的，库之间的转换简单直接。'
- en: All packages offer both high-level abstractions that require very little programming
    effort and a lower level approach for writing kernels for fine-grained control.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 所有包都提供高级抽象，需要非常少的编程工作，以及一个较低级别的编写内核的方法，以实现细粒度控制。
- en: In short
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: '**Directive-based programming:**'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于指令的编程：**'
- en: Existing serial code is annotated with directives to indicate which parts should
    be executed on the GPU.
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的串行代码通过指令进行注释，以指示哪些部分应该在GPU上执行。
- en: OpenACC and OpenMP are common directive-based programming models.
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC和OpenMP是常见的基于指令的编程模型。
- en: Productivity and easy usage are prioritized over performance.
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑生产力和易用性，而不是性能。
- en: Minimum programming effort is required to add parallelism to existing code.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现有代码中添加并行性需要最少的编程工作。
- en: '**Non-portable kernel-based models:**'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可移植的基于内核的模型：**'
- en: Low-level code is written to directly communicate with the GPU.
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低级代码编写用于直接与GPU通信。
- en: CUDA is NVIDIA’s parallel computing platform and API for GPU programming.
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA是NVIDIA的并行计算平台和GPU编程的API。
- en: HIP is an API developed by AMD that provides a similar programming interface
    to CUDA for both NVIDIA and AMD GPUs.
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HIP是AMD开发的一个API，为NVIDIA和AMD GPU提供与CUDA类似的编程接口。
- en: Deeper understanding of GPU architecture and programming methods is needed.
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要更深入地理解GPU架构和编程方法。
- en: '**Portable kernel-based models:**'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植的基于内核的模型：**'
- en: Higher-level abstractions for GPU programming that provide portability.
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供更高层次抽象的GPU编程，以实现可移植性。
- en: Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括OpenCL、Kokkos、alpaka、RAJA和SYCL等示例。
- en: Aim to achieve performance portability with a single-source application.
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 力求通过单个源应用程序实现性能可移植性。
- en: Can run on various GPUs and platforms, reducing the effort required to maintain
    and deploy GPU-accelerated applications.
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在多种 GPU 和平台上运行，减少了维护和部署 GPU 加速应用程序所需的努力。
- en: '**High-level language support:**'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级语言支持：**'
- en: C++ and Fortran feature initiatives to support GPUs through language-standard
    parallelism.
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++ 和 Fortran 通过语言标准并行性支持 GPU。
- en: Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 库如 PyCUDA、CuPy 和 Numba 提供了 GPU 编程能力。
- en: Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU
    programming.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia 有 CUDA.jl、AMDGPU.jl、oneAPI.jl 和 Metal.jl 等用于 GPU 编程的包。
- en: These approaches provide high-level abstraction and interfaces for GPU programming
    in the respective languages.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法为各自语言中的 GPU 编程提供了高级抽象和接口。
- en: Python
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python
- en: Python offers support for GPU programming through multiple abstraction levels.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Python 通过多个抽象级别提供对 GPU 编程的支持。
- en: '**CUDA Python, HIP Python and PyCUDA**'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA Python，HIP Python 和 PyCUDA**'
- en: These projects are, respectively, [NVIDIA-](https://developer.nvidia.com/cuda-python),
    [AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/) and [community-supported](https://documen.tician.de/pycuda/)
    wrappers providing Python bindings to the low-level CUDA and HIP APIs. To use
    these approaches directly, in most cases knowledge of CUDA or HIP programming
    is needed.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目分别是 [NVIDIA-](https://developer.nvidia.com/cuda-python)、[AMD-](https://rocm.docs.amd.com/projects/hip-python/en/latest/)
    和 [社区支持](https://documen.tician.de/pycuda/) 的包装器，提供对低级 CUDA 和 HIP API 的 Python
    绑定。要直接使用这些方法，在大多数情况下需要了解 CUDA 或 HIP 编程。
- en: CUDA Python also aims to support higher-level toolkits and libraries, such as
    **CuPy** and **Numba**.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA Python 还旨在支持更高级的工具包和库，如 **CuPy** 和 **Numba**。
- en: '**CuPy**'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**CuPy**'
- en: '[CuPy](https://cupy.dev/) is a GPU-based data array library compatible with
    NumPy/SciPy. It offers a highly similar interface to NumPy and SciPy, making it
    easy for developers to transition to GPU computing. Code written with NumPy can
    often be adapted to use CuPy with minimal modifications; in most straightforward
    cases, one might simply replace ‘numpy’ and ‘scipy’ with ‘cupy’ and ‘cupyx.scipy’
    in their Python code.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[CuPy](https://cupy.dev/) 是一个基于 GPU 的数据数组库，与 NumPy/SciPy 兼容。它提供了与 NumPy 和 SciPy
    非常相似的接口，使得开发者可以轻松过渡到 GPU 计算。使用 NumPy 编写的代码通常可以经过最小修改后适应 CuPy；在大多数直接情况下，人们可能只需在他们的
    Python 代码中将‘numpy’和‘scipy’替换为‘cupy’和‘cupyx.scipy’。'
- en: '**Numba**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**Numba**'
- en: '[Numba](https://numba.pydata.org/) is an open-source JIT compiler that translates
    a subset of Python and NumPy code into optimized machine code. Numba supports
    CUDA-capable GPUs and is able to generate code for them using several different
    syntax variants. In 2021, upstream support for [AMD (ROCm) support](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)
    was discontinued. However, as of 2025, AMD has added downstream support for the
    Numba API through the [Numba HIP package](https://github.com/ROCm/numba-hip).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[Numba](https://numba.pydata.org/) 是一个开源的 JIT 编译器，可以将 Python 和 NumPy 代码的子集转换为优化的机器代码。Numba
    支持 CUDA 兼容的 GPU，并能够使用几种不同的语法变体为它们生成代码。到 2021 年，对 [AMD (ROCm) 支持](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-54-0-19-august-2021)的上游支持已被终止。然而，截至
    2025 年，AMD 通过 [Numba HIP 包](https://github.com/ROCm/numba-hip)为 Numba API 添加了下游支持。'
- en: Julia
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Julia
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 通过以下针对所有三个主要供应商的 GPU 的包，提供了对 GPU 编程的一流支持：
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA.jl](https://cuda.juliagpu.org/stable/) 用于 NVIDIA GPU'
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) 用于 AMD GPU'
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) 用于 Intel GPU'
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) 用于 Apple M 系列GPU'
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance. Their respective
    APIs are however completely analogous and translation between libraries is straightforward.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA.jl` 是最成熟的，`AMDGPU.jl` 稍微落后但仍然适用于通用用途，而 `oneAPI.jl` 和 `Metal.jl` 虽然功能齐全但可能存在错误，缺少一些功能，并提供次优性能。然而，它们的相应
    API 完全类似，库之间的转换简单直接。'
- en: All packages offer both high-level abstractions that require very little programming
    effort and a lower level approach for writing kernels for fine-grained control.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 所有包都提供了高级抽象，需要极少的编程努力，以及一个较低级别的编写内核的方法，以实现细粒度控制。
- en: In short
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: '**Directive-based programming:**'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于指令的编程：**'
- en: Existing serial code is annotated with directives to indicate which parts should
    be executed on the GPU.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的串行代码通过指令进行注释，以指示应在GPU上执行哪些部分。
- en: OpenACC and OpenMP are common directive-based programming models.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenACC和OpenMP是常见的基于指令的编程模型。
- en: Productivity and easy usage are prioritized over performance.
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑生产力和易用性，而不是性能。
- en: Minimum programming effort is required to add parallelism to existing code.
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将并行性添加到现有代码中所需的编程工作量最小。
- en: '**Non-portable kernel-based models:**'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可移植的基于内核的模型：**'
- en: Low-level code is written to directly communicate with the GPU.
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低级代码是直接与GPU通信编写的。
- en: CUDA is NVIDIA’s parallel computing platform and API for GPU programming.
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA是NVIDIA的并行计算平台和GPU编程API。
- en: HIP is an API developed by AMD that provides a similar programming interface
    to CUDA for both NVIDIA and AMD GPUs.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HIP是AMD开发的一个API，为NVIDIA和AMD GPU提供了与CUDA类似的编程接口。
- en: Deeper understanding of GPU architecture and programming methods is needed.
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要更深入理解GPU架构和编程方法。
- en: '**Portable kernel-based models:**'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植的基于内核的模型：**'
- en: Higher-level abstractions for GPU programming that provide portability.
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供可移植性的GPU编程的高级抽象。
- en: Examples include OpenCL, Kokkos, alpaka, RAJA, and SYCL.
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，OpenCL、Kokkos、alpaka、RAJA和SYCL。
- en: Aim to achieve performance portability with a single-source application.
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 力求通过单源应用程序实现性能可移植性。
- en: Can run on various GPUs and platforms, reducing the effort required to maintain
    and deploy GPU-accelerated applications.
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在多种GPU和平台上运行，从而减少了维护和部署GPU加速应用程序所需的工作量。
- en: '**High-level language support:**'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级语言支持：**'
- en: C++ and Fortran feature initiatives to support GPUs through language-standard
    parallelism.
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++和Fortran都有支持GPU的语言标准并行的计划。
- en: Python libraries like PyCUDA, CuPy, and Numba offer GPU programming capabilities.
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库如PyCUDA、CuPy和Numba提供了GPU编程功能。
- en: Julia has packages such as CUDA.jl, AMDGPU.jl, oneAPI.jl, and Metal.jl for GPU
    programming.
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia有CUDA.jl、AMDGPU.jl、oneAPI.jl和Metal.jl等用于GPU编程的包。
- en: These approaches provide high-level abstraction and interfaces for GPU programming
    in the respective languages.
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法为各自语言中的GPU编程提供了高级抽象和接口。
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'Each of these GPU programming environments has its own strengths and weaknesses,
    and the best choice for a given project will depend on a range of factors, including:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这些GPU编程环境各有其优势和劣势，对于特定项目最佳选择将取决于一系列因素，包括：
- en: the hardware platforms being targeted,
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标硬件平台，
- en: the type of computation being performed, and
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在执行的计算类型，以及
- en: the developer’s experience and preferences.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者的经验和偏好。
- en: '**High-level and productivity-focused APIs** provide a simplified programming
    model and maximize code portability, while **low-level and performance-focused
    APIs** provide a high level of control over the GPU’s hardware but also require
    more coding effort and expertise.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**高级且以生产力为导向的API**提供了一个简化的编程模型，并最大化代码的可移植性，而**低级且以性能为导向的API**提供了对GPU硬件的高级别控制，但也需要更多的编码努力和专业知识。'
- en: Exercises
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Discussion
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论
- en: Which GPU programming frameworks have you already used previously, if any?
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你之前是否已经使用过任何GPU编程框架？
- en: What did you find most challenging? What was most useful?
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你认为最具有挑战性的是什么？最有用的是什么？
- en: Let us know in the main room or via comments in HackMD document.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 请在主房间或通过HackMD文档中的评论告诉我们。
- en: Keypoints
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: GPU programming approaches can be split into 1) directive-based, 2) non-portable
    kernel-based, 3) portable kernel-based, and 4) high-level language support.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU编程方法可以分为1)基于指令的，2)不可移植的基于内核的，3)可移植的基于内核的，和4)高级语言支持。
- en: There are multiple frameworks/languages available for each approach, each with
    pros and cons.*
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种方法都有多个框架/语言可供选择，各有优缺点。
