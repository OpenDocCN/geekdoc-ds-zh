- en: 3  Asymptotics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 渐近性
- en: 原文：[https://mattblackwell.github.io/gov2002-book/asymptotics.html](https://mattblackwell.github.io/gov2002-book/asymptotics.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mattblackwell.github.io/gov2002-book/asymptotics.html](https://mattblackwell.github.io/gov2002-book/asymptotics.html)
- en: '[Statistical Inference](./design.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[统计推断](./design.html)'
- en: '[3  Asymptotics](./asymptotics.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 渐近性](./asymptotics.html)'
- en: 3.1 Introduction
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 简介
- en: Suppose we are still interested in estimating the proportion of citizens who
    prefer increasing legal immigration. Based on the last chapter, a good strategy
    would be to use the sample proportion of immigration supporters in a random sample
    of citizens. You would have good reason to be confident with this estimator, with
    its finite-sample properties like unbiasedness and a sampling variance. We call
    these “finite-sample” properties since they hold at any sample size—they are as
    true for random samples of size for \(n = 10\) as they are for random samples
    of size \(n = 1,000,000\).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们仍然对估计公民中支持增加法律移民比例的比例感兴趣。根据上一章的内容，一个不错的策略是使用随机抽取公民样本中移民支持者的样本比例。你会有充分的理由对这个估计量感到自信，因为它具有无偏性和有限的样本方差等特性。我们称之为“有限样本”特性，因为它们在任何样本量下都成立——对于
    \(n = 10\) 的随机样本和 \(n = 1,000,000\) 的随机样本都是同样适用的。
- en: 'Finite-sample results, though, are of limited value because they only tell
    us about the center and spread of the sampling distribution of \(\Xbar_n\). Suppose
    we found that \(\Xbar_n = 0.47\) or 47% of respondents in a single survey supported
    increasing immigration. We might want to know how plausible it would be for the
    true population proportion – which is distinct from the sample proportion – to
    be 50% or greater. Questions like this are critical for a decision maker and,
    to answer this, we need to know the (approximate) distribution of \(\Xbar_n\)
    in addition to its mean and variance. We can often derive the exact distribution
    of an estimator if we are willing to make certain, sometimes strong assumptions
    about the underlying data (for example, if the population is normal, then the
    sample means will also be normal). Still, this approach is brittle: if our parametric
    assumption is false, we are back to square one.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有限样本结果的价值有限，因为它们只能告诉我们 \(\Xbar_n\) 的抽样分布的中心和分布。假设我们发现 \(\Xbar_n = 0.47\)
    或 47% 的受访者在一个单独的调查中支持增加移民。我们可能想知道，真正的总体比例——与样本比例不同——达到 50% 或更高的可能性有多大。这类问题对于决策者至关重要，为了回答这个问题，我们需要知道
    \(\Xbar_n\) 的（近似）分布，除了它的均值和方差。如果我们愿意对基础数据做出某些，有时是强烈的假设（例如，如果总体是正态分布的，那么样本均值也将是正态分布的），我们通常可以推导出估计量的确切分布。尽管如此，这种方法是脆弱的：如果我们的参数假设是错误的，我们就会回到起点。
- en: In this chapter, we take a different approach by asking what happens to the
    sampling distribution of estimators as the sample size gets very large, which
    we refer to as **asymptotic theory**. While asymptotics will often simplify derivations,
    an essential point is that everything we do with asymptotics will be an approximation.
    No one ever has infinite data, but we hope that the approximations will be closer
    to the truth as our samples get larger.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们采取了一种不同的方法，即探讨当样本量变得非常大时，估计量的抽样分布会发生什么变化，我们称之为**渐近理论**。虽然渐近性通常会简化推导，但一个基本观点是，我们使用渐近性所做的一切都将是一个近似。没有人拥有无限的数据，但我们希望随着样本量的增大，这些近似将更接近于真相。
- en: Asymptotic results are key to modern statistical methods because many methods
    of quantifying uncertainty about estimates rely on asymptotic approximations.
    We will rely on the asymptotic results we derive in this chapter to estimate standard
    errors, construct confidence intervals, and perform hypothesis tests, all without
    assuming a fully parametric model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 渐近结果对于现代统计方法至关重要，因为许多量化估计不确定性的方法都依赖于渐近近似。我们将依赖本章推导出的渐近结果来估计标准误差、构建置信区间以及进行假设检验，而无需假设一个完全参数化模型。
- en: 3.2 Convergence of deterministic sequences
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 确定性序列的收敛
- en: 'A helpful place to begin is by reviewing the basic idea of convergence in deterministic
    sequences from calculus:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用的起点是回顾微积分中确定性序列收敛的基本概念：
- en: '**Definition 3.1** A sequence \(\{a_n: n = 1, 2, \ldots\}\) has the **limit**
    \(a\) written \(a_n \rightarrow a\) as \(n\rightarrow \infty\) or \(\lim_{n\rightarrow
    \infty} a_n = a\) if for all \(\epsilon > 0\) there is some \(n_{\epsilon} < \infty\)
    such that for all \(n \geq n_{\epsilon}\), \(|a_n - a| \leq \epsilon\).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义 3.1** 一个序列 \(\{a_n: n = 1, 2, \ldots\}\) 有极限 \(a\)，写作 \(a_n \rightarrow
    a\) 当 \(n\rightarrow \infty\) 或 \(\lim_{n\rightarrow \infty} a_n = a\) 如果对于所有
    \(\epsilon > 0\)，存在某个 \(n_{\epsilon} < \infty\)，使得对于所有 \(n \geq n_{\epsilon}\)，\(
    |a_n - a| \leq \epsilon \)。'
- en: We say that \(a_n\) **converges** to \(a\) if \(\lim_{n\rightarrow\infty} a_n
    = a\). Basically, a sequence converges to a number if the sequence gets closer
    and closer to that number as the sequence goes on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说 \(a_n\) **收敛**到 \(a\) 如果 \(\lim_{n\rightarrow\infty} a_n = a\)。基本上，一个序列收敛到一个数，如果序列越来越接近那个数，随着序列的进行。
- en: '**Example 3.1** One important sequence that arises often in statistics is \(1/n\)
    as \(n\to\infty\). It may seem clear that this sequence converges to 0, but showing
    this using the formal definition of convergence is helpful.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 3.1** 在统计学中，经常出现的一个重要序列是 \(1/n\) 当 \(n\to\infty\)。这个序列收敛到 0 可能看起来很清楚，但使用收敛的正式定义来证明这一点是有帮助的。'
- en: Let us pick a specific value of \(\epsilon = 0.3\). Now we need to find an integer
    \(n_{\epsilon}\) so that \(|1/n - 0| = 1/n \leq \epsilon\) for all \(n \geq n_{\epsilon}\).
    Clearly, if \(\epsilon = 0.3\), then \(n_{\epsilon} = 4\) would satisfy this condition
    since \(1/4 \leq 0.3\).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取一个特定的值 \(\epsilon = 0.3\)。现在我们需要找到一个整数 \(n_{\epsilon}\)，使得对于所有 \(n \geq
    n_{\epsilon}\)，\( |1/n - 0| = 1/n \leq \epsilon \)。显然，如果 \(\epsilon = 0.3\)，那么
    \(n_{\epsilon} = 4\) 就会满足这个条件，因为 \(1/4 \leq 0.3\)。
- en: '![](../Images/a714d4c650bc40216559b85729c14913.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a714d4c650bc40216559b85729c14913.png)'
- en: More generally, for any \(\epsilon\), \(n \geq 1/\epsilon\) implies \(1/n \leq
    \epsilon\). Thus, setting \(n_{\epsilon} = 1/\epsilon\) ensures that the definition
    holds for all values of \(\epsilon\) and that \(\lim_{n\to\infty} 1/n = 0\).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，对于任何 \(\epsilon\)，如果 \(n \geq 1/\epsilon\)，则 \(1/n \leq \epsilon\)。因此，设置
    \(n_{\epsilon} = 1/\epsilon\) 确保了定义对所有 \(\epsilon\) 的值都成立，并且 \(\lim_{n\to\infty}
    1/n = 0\)。
- en: 'We will mostly not use such formal definitions to establish a limit but, rather,
    rely on the properties of limits. For example, convergence and limits follow basic
    arithmetic operations. Suppose that we have two sequences with limits \(\lim_{n\to\infty}
    a_n = a\) and \(\lim_{n\to\infty} b_n = b\). Then the properties of limits imply:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要不使用这样的正式定义来建立极限，而是依赖于极限的性质。例如，收敛和极限遵循基本的算术运算。假设我们有两个序列，它们的极限分别是 \(\lim_{n\to\infty}
    a_n = a\) 和 \(\lim_{n\to\infty} b_n = b\)。那么极限的性质意味着：
- en: \(\lim_{n\to\infty} (a_n + b_n) = a + b\)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\lim_{n\to\infty} (a_n + b_n) = a + b\)
- en: \(\lim_{n\to\infty} a_nb_n = ab\)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\lim_{n\to\infty} a_nb_n = ab\)
- en: \(\lim_{n\to\infty} ca_n = c\cdot a\)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\lim_{n\to\infty} ca_n = c\cdot a\)
- en: \(\lim_{n\to\infty} (a_n/b_n) = a/b\) if \(b \neq 0\)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\lim_{n\to\infty} (a_n/b_n) = a/b\) 如果 \(b \neq 0\)
- en: \(\lim_{n\to\infty} a_n^{k} = a^{k}\)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\lim_{n\to\infty} a_n^{k} = a^{k}\)
- en: These rules plus the result in [Example 3.1](#exm-limit) allow us to prove other
    useful facts such as \[ \lim_{n\to\infty} \frac{2}{n} = 2 \cdot 0 = 0 \qquad \lim_{n\to\infty}
    \frac{1}{n^{2}} = 0. \]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则加上 [示例 3.1](#exm-limit) 中的结果，使我们能够证明其他有用的结论，例如 \[ \lim_{n\to\infty} \frac{2}{n}
    = 2 \cdot 0 = 0 \quad \lim_{n\to\infty} \frac{1}{n^{2}} = 0. \]
- en: Can we apply a similar definition of convergence to sequences of random variables
    (like estimators)? Possibly. Some examples clarify why this might be difficult.[¹](#fn1)
    Suppose we have a sequence of \(a_n = a\) for all \(n\) (that is, a constant sequence).
    Then obviously \(\lim_{n\rightarrow\infty} a_n = a\). Now let’s say we have a
    sequence of random variables, \(X_1, X_2, \ldots\), that are all independent with
    a standard normal distribution, \(N(0,1)\). From the analogy to the deterministic
    case, saying that \(X_n\) converges to \(X \sim N(0, 1)\) would be tempting, but
    note that because they are all different random variables, \(\P(X_n = X) = 0\).
    Thus, we must be careful about saying how one variable converges to another variable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否将类似的收敛定义应用于随机变量序列（如估计量）？可能可以。一些例子说明了为什么这可能会很困难。[¹](#fn1) 假设我们有一个序列 \(a_n
    = a\) 对于所有 \(n\) 都成立（即一个常数序列）。那么显然 \(\lim_{n\rightarrow\infty} a_n = a\)。现在假设我们有一个随机变量序列
    \(X_1, X_2, \ldots\)，它们都是具有标准正态分布 \(N(0,1)\) 的独立随机变量。从确定性情况的类比来看，说 \(X_n\) 收敛到
    \(X \sim N(0, 1)\) 可能很有吸引力，但请注意，因为它们都是不同的随机变量，\(\P(X_n = X) = 0\)。因此，我们必须小心地说一个变量如何收敛到另一个变量。
- en: Another example highlights subtle problems with a sequence of random variables
    converging to a single value. Suppose we have a sequence of random variables \(X_1,
    X_2, \ldots\) where \(X_n \sim N(0, 1/n)\). Clearly, the distribution of \(X_n\)
    will concentrate around 0 for large values of \(n\), so saying that \(X_n\) converges
    to 0 is tempting. But notice that \(\P(X_n = 0) = 0\) because of the nature of
    continuous random variables.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子突出了随机变量序列收敛到单个值时存在的微妙问题。假设我们有一个随机变量序列 \(X_1, X_2, \ldots\)，其中 \(X_n \sim
    N(0, 1/n)\)。显然，当 \(n\) 的值很大时，\(X_n\) 的分布将集中在 0 附近，所以认为 \(X_n\) 收敛到 0 是诱人的。但请注意，由于连续随机变量的性质，\(\P(X_n
    = 0) = 0\)。
- en: 3.3 Convergence in probability and consistency
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 概率收敛和一致性
- en: A sequence of random variables can converge in several different ways. The first
    type of convergence deals with sequences converging to a single value.[²](#fn2)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列随机变量可以以几种不同的方式收敛。第一种收敛类型涉及收敛到单个值的序列。[²](#fn2)
- en: '**Definition 3.2** A sequence of random variables, \(X_1, X_2, \ldots\), is
    said to **converge in probability** to a value \(b\) if for every \(\varepsilon
    > 0\), \[ \P(|X_n - b| > \varepsilon) \rightarrow 0, \] as \(n\rightarrow \infty\).
    We write this \(X_n \inprob b\).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义 3.2** 如果对于每个 \(\varepsilon > 0\)，随机变量序列 \(X_1, X_2, \ldots\) 被称为**以概率收敛**到值
    \(b\)，则 \[ \P(|X_n - b| > \varepsilon) \rightarrow 0, \] 随着 \(n\rightarrow \infty\)。我们写成
    \(X_n \inprob b\)。'
- en: What’s happening in this definition? The even \(|X_n - b| > \varepsilon\) says
    that a draw of \(X_n\) is more than \(\varepsilon\) away from \(b\) (above or
    below). So convergence in probability says that the probability of being some
    distance away from the limit value goes to zero as the \(n\) goes to \(\infty\).
    With deterministic sequences, we said that \(a_n\) converges to \(a\) as it gets
    closer and closer to \(a\) as \(n\) gets bigger. For convergence in probability,
    the sequence of random variables converges to \(b\) if the probability that random
    variables are far away from \(b\) gets smaller and smaller as \(n\) gets big.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个定义中发生了什么？偶数 \(|X_n - b| > \varepsilon\) 表示 \(X_n\) 的一个取值与 \(b\) 的距离超过 \(\varepsilon\)（在上方或下方）。因此，概率收敛表示当
    \(n\) 趋向于 \(\infty\) 时，距离极限值的概率趋向于零。对于确定性序列，我们说 \(a_n\) 随着 \(n\) 的增大而越来越接近 \(a\)。对于概率收敛，随机变量序列收敛到
    \(b\) 当且仅当随机变量远离 \(b\) 的概率随着 \(n\) 的增大而越来越小。
- en: '**Example 3.2** Let’s illustrate the definition of convergence in probability
    by constructing a sequence of random variables, \[ X_n \sim N(0, 1/n). \]'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 3.2** 让我们通过构造一个随机变量序列来阐述概率收敛的定义，\[ X_n \sim N(0, 1/n). \]'
- en: We can see intuitively that this sequence will be centered at zero with a shrinking
    variance. Below, we will see that this is enough to establish convergence in probability
    of \(X_n\) to 0, but we can also show this in terms of its definition. To do so,
    we need to show that \[ \P(|X_n| > \varepsilon) \to 0. \]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地看出，这个序列将以零为中心，方差逐渐减小。下面，我们将看到这足以建立 \(X_n\) 以概率收敛到 0，但我们也可以用其定义来展示这一点。要做到这一点，我们需要证明
    \[ \P(|X_n| > \varepsilon) \to 0. \]
- en: Let \(\Phi(\cdot)\) be the cdf for the standard normal. For any \(n\), the cdf
    for \(X_n\) is \(\P(X_{n} < x) = \Phi(\sqrt{n}x)\). Thus, \[ \begin{aligned} \P(|X_n|
    > \varepsilon) &= \P(X_n < -\varepsilon) + \P(X_n > \varepsilon) \\ &= \Phi(-\sqrt{n}\varepsilon)
    + (1 - \Phi(\sqrt{n}\varepsilon)) \to 0. \end{aligned} \] The last limit is due
    to \(\sqrt{n}\varepsilon \to \infty\) and thus, by the properties of the cdf,
    \(\Phi(-\sqrt{n}\varepsilon) \to 0\) and \(\Phi(\sqrt{n}\varepsilon) \to 1\).
    Clearly this holds for any \(\varepsilon\), so \(X_n \inprob 0\).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 设 \(\Phi(\cdot)\) 为标准正态分布的累积分布函数。对于任何 \(n\)，\(X_n\) 的累积分布函数为 \(\P(X_{n} < x)
    = \Phi(\sqrt{n}x)\)。因此，\[ \begin{aligned} \P(|X_n| > \varepsilon) &= \P(X_n <
    -\varepsilon) + \P(X_n > \varepsilon) \\ &= \Phi(-\sqrt{n}\varepsilon) + (1 -
    \Phi(\sqrt{n}\varepsilon)) \to 0. \end{aligned} \] 最后的极限是由于 \(\sqrt{n}\varepsilon
    \to \infty\)，因此，根据累积分布函数的性质，\(\Phi(-\sqrt{n}\varepsilon) \to 0\) 和 \(\Phi(\sqrt{n}\varepsilon)
    \to 1\)。显然，这对于任何 \(\varepsilon\) 都成立，所以 \(X_n \inprob 0\)。
- en: '*Notation alert* *Sometimes convergence in probability is written as \(\text{plim}(Z_n)
    = b\) when \(Z_n \inprob b\), \(\text{plim}\) stands for “probability limit.”*  *Convergence
    in probability is crucial for evaluating estimators. While we said that unbiasedness
    was not the be-all and end-all of properties of estimators, the following property
    is an essential and fundamental property of good estimators.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意符号* *有时当 \(Z_n \inprob b\) 时，概率收敛可以写成 \(\text{plim}(Z_n) = b\)，其中 \(\text{plim}\)
    表示“概率极限”。* 概率收敛对于评估估计量至关重要。虽然我们说过无偏性不是估计量性质的终极目标，但以下性质是良好估计量的基本和本质属性。'
- en: '**Definition 3.3** An estimator is **consistent** if \(\widehat{\theta}_n \inprob
    \theta\).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义 3.3** 如果 \(\widehat{\theta}_n \inprob \theta\)，则估计量是**一致的**。'
- en: Consistency of an estimator implies that the sampling distribution of the estimator
    “collapses” on the true value as the sample size gets large. An estimator is **inconsistent**
    if it converges in probability to any other value. As the sample size gets large,
    the probability that an inconsistent estimator will be close to the truth will
    approach 0\. Generally speaking, consistency is a very desirable property of an
    estimator.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 估计量的一致性意味着随着样本量的增大，估计量的抽样分布“收敛”到真实值。如果一个估计量以概率收敛到任何其他值，则该估计量是不一致的。随着样本量的增大，不一致估计量接近真实值的概率将趋近于0。一般来说，一致性是估计量一个非常理想化的属性。
- en: '*Note* *Estimators can be inconsistent yet still converge in probability to
    an understandable quantity. For example, we will discuss in later chapters that
    regression coefficients estimated by ordinary least squares (OLS) are consistent
    for the conditional expectation if the conditional expectation is linear. If that
    function is non-linear, however, then OLS will be consistent for the best linear
    approximation to that function. While not ideal, it does mean that this estimator
    is at least consistent for an interpretable quantity.*  *We can also define convergence
    in probability for a sequence of random vectors, \(\X_1, \X_2, \ldots\), where
    \(\X_i = (X_{i1}, \ldots, X_{ik})\) is a random vector of length \(k\). This sequence
    converges in probability to a vector \(\mb{b} = (b_1, \ldots, b_k)\) if and only
    if each random variable in the vector converges to the corresponding element in
    \(\mb{b}\), or that \(X_{nj} \inprob b_j\) for all \(j = 1, \ldots, k\).**  **##
    3.4 Useful inequalities'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *估计量可能不一致，但仍然以概率收敛到一个可理解的量。例如，我们将在后面的章节中讨论，如果条件期望是线性的，则由普通最小二乘法（OLS）估计的回归系数对于条件期望是一致的。然而，如果该函数是非线性的，那么OLS将对该函数的最佳线性近似一致。虽然这不是理想的，但这确实意味着这个估计量至少对于可解释的量是一致的。*
    *我们还可以定义随机向量序列 \(\X_1, \X_2, \ldots\) 的概率收敛，其中 \(\X_i = (X_{i1}, \ldots, X_{ik})\)
    是长度为 \(k\) 的随机向量。如果向量中的每个随机变量都收敛到 \(\mb{b} = (b_1, \ldots, b_k)\) 对应的元素，或者 \(X_{nj}
    \inprob b_j\) 对所有 \(j = 1, \ldots, k\) 成立，则该序列以概率收敛到向量 \(\mb{b}\)。**  **## 3.4
    有用的不等式'
- en: At first glance, establishing an estimator’s consistency will be difficult.
    How can we know if a distribution will collapse to a specific value without knowing
    the shape or family of the distribution? It turns out that there are certain relationships
    between the mean and variance of a random variable and certain probability statements
    that hold for all distributions (that have finite variance, at least). These relationships
    are key to establishing results that do not depend on a specific distribution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，建立估计量的一致性将很困难。在不了解分布的形状或家族的情况下，我们如何知道分布是否会收敛到特定的值？结果，随机变量的均值和方差之间存在某些关系，以及对于所有具有有限方差的分布（至少）都成立的某些概率陈述。这些关系是建立不依赖于特定分布的结果的关键。
- en: '**Theorem 3.1 (Markov Inequality)** For any r.v. \(X\) and any \(\delta >0\),
    \[ \P(|X| \geq \delta) \leq \frac{\E[|X|]}{\delta}. \]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 3.1（马尔可夫不等式）** 对于任何随机变量 \(X\) 和任何 \(\delta >0\)，\[ \P(|X| \geq \delta)
    \leq \frac{\E[|X|]}{\delta}. \]'
- en: '*Proof*. Note that we can let \(Y = |X|/\delta\) and rewrite the statement
    as \(\P(Y \geq 1) \leq \E[Y]\) (since \(\E[|X|]/\delta = \E[|X|/\delta]\) by the
    properties of expectation), which is what we will show. But also note that \[
    \mathbb{I}(Y \geq 1) \leq Y. \] Why does this hold? The two possible values of
    the indicator function show why. If \(Y\) is less than 1, then the indicator function
    will be 0, but recall that \(Y\) is nonnegative, so we know that it must be at
    least as big as 0 so that inequality holds. If \(Y \geq 1\), then the indicator
    function will take the value one, but we just said that \(Y \geq 1\), so the inequality
    holds. If we take the expectation of both sides of this inequality, we obtain
    the result (remember, the expectation of an indicator function is the probability
    of the event).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*。注意，我们可以让 \(Y = |X|/\delta\) 并将陈述重写为 \(\P(Y \geq 1) \leq \E[Y]\)（由于期望的性质，\(\E[|X|]/\delta
    = \E[|X|/\delta]\)），这正是我们将要证明的。但也要注意 \[ \mathbb{I}(Y \geq 1) \leq Y. \] 为什么这成立？指示函数的两个可能值说明了原因。如果
    \(Y\) 小于 1，则指示函数将为 0，但回想一下 \(Y\) 是非负的，所以我们知道它必须至少与 0 一样大，这样不等式才成立。如果 \(Y \geq
    1\)，则指示函数将取值为 1，但我们刚刚说过 \(Y \geq 1\)，所以不等式成立。如果我们对不等式的两边取期望，我们就会得到结果（记住，指示函数的期望是事件的概率）。'
- en: In words, Markov’s inequality says that the probability of a random variable
    being large in magnitude cannot be high if the average is not large in magnitude.
    Blitzstein and Hwang (2019) provide an excellent intuition behind this result
    using income as an example. Let \(X\) be the income of a randomly selected individual
    in a population and set \(\delta = 2\E[X]\) so that the inequality becomes \(\P(X
    > 2\E[X]) < 1/2\) (assuming that all income is nonnegative). Here, the inequality
    says that the share of the population with an income twice the average must be
    less than 0.5 since if more than half the population were making twice the average
    income, then the average would have to be higher.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用文字来说，马尔可夫不等式表明，如果随机变量的平均值在幅度上不大，那么该随机变量幅度大的概率就不会高。Blitzstein和Hwang（2019）使用收入作为例子，提供了一个关于这个结果的优秀直觉。设\(X\)为从总体中随机选择的一个个体的收入，并设\(\delta
    = 2\text{E}[X]\)，使得不等式变为\(\text{P}(X > 2\text{E}[X]) < 1/2\)（假设所有收入都是非负的）。在这里，不等式表明，收入是平均收入两倍的人口比例必须小于0.5，因为如果超过一半的人口收入是平均收入的两倍，那么平均收入就必须更高。
- en: It’s pretty astounding how general this result is since it holds for all random
    variables. Of course, its generality comes at the expense of not being very informative.
    If \(\E[|X|] = 5\), for instance, the inequality tells us that \(\P(|X| \geq 1)
    \leq 5\), which is not very helpful since we already know that probabilities are
    less than 1! We can get tighter bounds if we are willing to make some assumptions
    about \(X\).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果如此普遍确实令人惊讶，因为它适用于所有随机变量。当然，它的普遍性是以信息量不大为代价的。例如，如果\(\text{E}[|X|] = 5\)，不等式告诉我们\(\text{P}(|X|
    \geq 1) \leq 5\)，这并不很有帮助，因为我们已经知道概率小于1！如果我们愿意对\(X\)做出一些假设，我们可以得到更紧的界限。
- en: '**Theorem 3.2 (Chebyshev Inequality)** Suppose that \(X\) is r.v. for which
    \(\V[X] < \infty\). Then, for every real number \(\delta > 0\), \[ \P(|X-\E[X]|
    \geq \delta) \leq \frac{\V[X]}{\delta^2}. \]'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理3.2（切比雪夫不等式）** 假设\(X\)是方差\(\text{V}[X] < \infty\)的随机变量。那么，对于每一个正实数\(\delta
    > 0\)，有\[\text{P}(|X-\text{E}[X]| \geq \delta) \leq \frac{\text{V}[X]}{\delta^2}.\]'
- en: '*Proof*. To prove this, we only need to square both sides of the inequality
    inside the probability statement and apply Markov’s inequality: \[ \P\left( |X
    - \E[X]| \geq \delta \right) = \P((X-\E[X])^2 \geq \delta^2) \leq \frac{\E[(X
    - \E[X])^2]}{\delta^2} = \frac{\V[X]}{\delta^2}, \] with the last equality holding
    by the definition of variance.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*。为了证明这一点，我们只需要对概率陈述中的不等式两边进行平方，并应用马尔可夫不等式：\[\text{P}\left( |X - \text{E}[X]|
    \geq \delta \right) = \text{P}((X-\text{E}[X])^2 \geq \delta^2) \leq \frac{\text{E}[(X
    - \text{E}[X])^2]}{\delta^2} = \frac{\text{V}[X]}{\delta^2},\]最后这个等式是根据方差的定义得出的。'
- en: 'Chebyshev’s inequality is a straightforward extension of the Markov result:
    the probability of a random variable being far from its mean (that is, \(|X-\E[X]|\)
    being large) is limited by the variance of the random variable. If we let \(\delta
    = c\sigma\), where \(\sigma\) is the standard deviation of \(X\), we can use this
    result to bound the normalized deviation from the mean: \[ \P\left(\frac{|X -
    \E[X]|}{\sigma} > c \right) \leq \frac{1}{c^2}. \] This statement says the probability
    of being two standard deviations away from the mean must be less than 1/4 = 0.25\.
    Notice that this bound can be fairly wide. If \(X\) has a normal distribution,
    we know that about 5% of draws will be greater than 2 SDs away from the mean,
    much lower than the 25% bound implied by Chebyshev’s inequality.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 切比雪夫不等式是马尔可夫结果的直接扩展：随机变量远离其均值（即\(|X-\text{E}[X]|\)很大）的概率受随机变量方差限制。如果我们让\(\delta
    = c\sigma\)，其中\(\sigma\)是\(X\)的标准差，我们可以使用这个结果来界定标准化偏差：\[\text{P}\left(\frac{|X
    - \text{E}[X]|}{\sigma} > c \right) \leq \frac{1}{c^2}.\]这个陈述说，距离均值两个标准差的概率必须小于1/4
    = 0.25。注意，这个界限可能相当宽。如果\(X\)服从正态分布，我们知道大约5%的抽样将大于2个标准差远离均值，这比切比雪夫不等式暗示的25%界限要低得多。
- en: 3.5 The law of large numbers
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 大数定律
- en: We can now use these inequalities to show how estimators can be consistent for
    their target quantities of interest without making parametric assumptions. Why
    are these inequalities helpful? Remember that convergence in probability was about
    the probability of an estimator being far away from a value going to zero. Chebyshev’s
    inequality shows that we can bound these exact probabilities.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这些不等式来展示估计量如何在其感兴趣的量上保持一致性，而不需要做出参数假设。为什么这些不等式有帮助？记住，概率收敛是关于估计量远离某个值的概率趋于零。切比雪夫不等式表明我们可以对这些确切的概率进行界定。
- en: The most famous consistency result has a special name.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的收敛结果有一个特殊的名称。
- en: '**Theorem 3.3 (Weak Law of Large Numbers)** Let \(X_1, \ldots, X_n\) be i.i.d.
    draws from a distribution with mean \(\mu = \E[X_i]\) and variance \(\sigma^2
    = \V[X_i] < \infty\). Let \(\Xbar_n = \frac{1}{n} \sum_{i =1}^n X_i\). Then, \(\Xbar_n
    \inprob \mu\).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 3.3（弱大数定律）** 设 \(X_1, \ldots, X_n\) 是从具有均值 \(\mu = \E[X_i]\) 和方差 \(\sigma^2
    = \V[X_i] < \infty\) 的分布中独立同分布抽取的样本。设 \(\Xbar_n = \frac{1}{n} \sum_{i =1}^n X_i\)。那么，\(\Xbar_n
    \inprob \mu\)。'
- en: '*Proof*. Recall that the sample mean is unbiased, so \(\E[\Xbar_n] = \mu\)
    with sampling variance \(\sigma^2/n\). We can then apply Chebyshev to the sample
    mean to get \[ \P(|\Xbar_n - \mu| \geq \delta) \leq \frac{\V[\Xbar_n]}{\delta}
    = \frac{\sigma^2}{n\delta^2} \] An \(n\rightarrow\infty\), the right-hand side
    goes to 0, which means that the left-hand side also must go to 0, which is the
    definition of \(\Xbar_n\) converging in probability to \(\mu\).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*。回想样本均值是无偏的，所以 \(\E[\Xbar_n] = \mu\)，抽样方差为 \(\sigma^2/n\)。然后我们可以对样本均值应用切比雪夫不等式得到
    \[ \P(|\Xbar_n - \mu| \geq \delta) \leq \frac{\V[\Xbar_n]}{\delta} = \frac{\sigma^2}{n\delta^2}
    \] 当 \(n\rightarrow\infty\) 时，右边趋于 0，这意味着左边也必须趋于 0，这是 \(\Xbar_n\) 以概率收敛到 \(\mu\)
    的定义。'
- en: The weak law of large numbers (WLLN) shows that, under general conditions, the
    sample mean gets closer to the population mean as \(n\rightarrow\infty\). This
    result holds even when the variance of the data is infinite, though researchers
    will rarely face such a situation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 弱大数定律（WLLN）表明，在一般条件下，随着 \(n\rightarrow\infty\)，样本均值会越来越接近总体均值。即使数据方差是无限的，这个结果也成立，尽管研究人员很少会遇到这种情况。
- en: '*Note* *The naming of the “weak” law of large numbers seems to imply the existence
    of a “strong” law of large numbers (SLLN), which is true. The SLLN states that
    the sample mean converges to the population mean with probability 1\. This type
    of convergence, called **almost sure convergence**, is stronger than convergence
    in probability, which only says that the probability of the sample mean being
    close to the population mean converges to 1\. While it is nice to know that this
    stronger form of convergence holds for the sample mean under the same assumptions,
    it is rare for researchers outside of theoretical probability and statistics to
    rely on almost sure convergence.*  ***Example 3.3** Seeing how the distribution
    of the sample mean changes as a function of the sample size allows us to appreciate
    the WLLN. We can see this by taking repeated iid samples of different sizes from
    an exponential random variable with rate parameter 0.5 so that \(\E[X_i] = 2\).
    In [Figure 3.1](#fig-lln-sim), we show the distribution of the sample mean (across
    repeated samples) when the sample size is 15 (black), 30 (violet), 100 (blue),
    and 1000 (green). The sample mean distribution “collapses” on the true population
    mean, 2\. The probability of being far away from 2 becomes progressively smaller
    as the sample size increases.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *“大数定律”中的“弱”似乎暗示了存在一个“强”的大数定律（SLLN），这是正确的。SLLN 表明样本均值以概率 1 收敛到总体均值。这种收敛，称为**几乎处处收敛**，比概率收敛更强，概率收敛仅说明样本均值接近总体均值的概率收敛到
    1。虽然知道这种更强的收敛形式在相同的假设下适用于样本均值是令人欣慰的，但除了理论概率和统计学的研究者外，很少有研究人员依赖几乎处处收敛。***示例 3.3***
    通过观察样本均值随样本大小的变化，我们可以欣赏到弱大数定律。我们可以通过从具有速率参数 0.5 的指数随机变量中重复抽取不同大小的独立同分布样本，使得 \(\E[X_i]
    = 2\) 来做到这一点。在 [图 3.1](#fig-lln-sim) 中，我们展示了当样本大小为 15（黑色）、30（紫色）、100（蓝色）和 1000（绿色）时样本均值的分布。样本均值分布“坍缩”到真实的总体均值，2。远离
    2 的概率随着样本大小的增加而逐渐减小。'
- en: '![](../Images/537980c6302816356306bba02791dffe.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/537980c6302816356306bba02791dffe.png)'
- en: 'Figure 3.1: Sampling distribution of the sample mean as a function of sample
    size.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：样本均值的抽样分布随样本大小的变化。
- en: 'The WLLN also holds for random vectors in addition to random variables. Let
    \((\X_1, \ldots, \X_n)\) be an iid sample of random vectors of length \(k\), \(\mb{X}_i
    = (X_{i1}, \ldots, X_{ik})\). We can define the vector sample mean as just the
    vector of sample means for each of the entries:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 弱大数定律也适用于随机向量，而不仅仅是随机变量。设 \((\X_1, \ldots, \X_n)\) 是长度为 \(k\) 的随机向量的独立同分布样本，\(\mb{X}_i
    = (X_{i1}, \ldots, X_{ik})\)。我们可以定义向量样本均值就是每个条目的样本均值向量：
- en: \[ \overline{\mb{X}}_n = \frac{1}{n} \sum_{i=1}^n \mb{X}_i = \begin{pmatrix}
    \Xbar_{n,1} \\ \Xbar_{n,2} \\ \vdots \\ \Xbar_{n, k} \end{pmatrix} \] Since this
    is just a vector of sample means, each random variable in the random vector will
    converge in probability to the mean of that random variable. Fortunately, this
    is the exact definition of convergence in probability for random vectors. We formally
    write this in the following theorem.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \overline{\mb{X}}_n = \frac{1}{n} \sum_{i=1}^n \mb{X}_i = \begin{pmatrix}
    \Xbar_{n,1} \\ \Xbar_{n,2} \\ \vdots \\ \Xbar_{n, k} \end{pmatrix} \] 由于这只是一个样本均值的向量，随机向量中的每个随机变量在概率上都会收敛到该随机变量的均值。幸运的是，这正是随机向量概率收敛的精确定义。我们正式地用以下定理来表示这一点。
- en: '**Theorem 3.4** If \(\X_i \in \mathbb{R}^k\) are iid draws from a distribution
    with \(\E[X_{ij}] < \infty\) for all \(j=1,\ldots,k\) then as \(n\rightarrow\infty\)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理3.4** 如果 \(\X_i \in \mathbb{R}^k\) 是从具有 \(\E[X_{ij}] < \infty\) 的分布中独立同分布抽取的，对于所有
    \(j=1,\ldots,k\)，那么当 \(n\rightarrow\infty\) 时'
- en: \[ \overline{\mb{X}}_n \inprob \E[\X] = \begin{pmatrix} \E[X_{i1}] \\ \E[X_{i2}]
    \\ \vdots \\ \E[X_{ik}] \end{pmatrix}. \]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \overline{\mb{X}}_n \inprob \E[\X] = \begin{pmatrix} \E[X_{i1}] \\ \E[X_{i2}]
    \\ \vdots \\ \E[X_{ik}] \end{pmatrix}. \]
- en: '*Notation alert* *Note that many of the formal results presented so far have
    “moment conditions” that certain moments are finite. For the vector WLLN, we saw
    that applied to the mean of each variable in the vector. Some books use a shorthand
    for this: \(\E\Vert \X_i\Vert < \infty\), where \[ \Vert\X_i\Vert = \left(X_{i1}^2
    + X_{i2}^2 + \ldots + X_{ik}^2\right)^{1/2}. \] This expression has slightly more
    compact notation, but why does it work? One can show that this function, called
    the **Euclidean norm** or \(L_2\)-norm, is a **convex** function, so we can apply
    Jensen’s inequality to show that: \[ \E\Vert \X_i\Vert \geq \Vert \E[\X_i] \Vert
    = (\E[X_{i1}]^2 + \ldots + \E[X_{ik}]^2)^{1/2}. \] So if \(\E\Vert \X_i\Vert\)
    is finite, all the component means are finite. Otherwise, the right-hand side
    of the previous equation would be infinite.**  **## 3.6 Consistency of estimators'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意符号* *请注意，到目前为止所提出的许多正式结果都有“矩条件”，即某些矩是有限的。对于向量WLLN，我们看到了它应用于向量中每个变量的均值。一些书籍使用这种简写：\(\E\Vert
    \X_i\Vert < \infty\)，其中 \[ \Vert\X_i\Vert = \left(X_{i1}^2 + X_{i2}^2 + \ldots
    + X_{ik}^2\right)^{1/2}. \] 这个表达式有稍微紧凑的符号，但为什么它有效？可以证明这个函数，称为**欧几里得范数**或\(L_2\)-范数，是一个**凸**函数，因此我们可以应用Jensen不等式来证明：\[
    \E\Vert \X_i\Vert \geq \Vert \E[\X_i] \Vert = (\E[X_{i1}]^2 + \ldots + \E[X_{ik}]^2)^{1/2}.
    \] 因此，如果 \(\E\Vert \X_i\Vert\) 是有限的，所有分量均值都是有限的。否则，上一方程的右侧将是无限的。**  **## 3.6 估计量的一致性'
- en: 'The WLLN shows that the sample mean of iid draws is consistent for the population
    mean, which is a massive result given that so many estimators are sample means
    of potentially complicated functions of the data. What about other estimators?
    The proof of the WLLN points to one way to determine that an estimator is consistent:
    if it is unbiased and the sampling variance shrinks as the sample size grows.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: WLLN表明，对于独立同分布抽取的样本均值，对于总体均值是一致的，这是一个巨大的结果，因为许多估计量都是数据可能复杂函数的样本均值。那么其他估计量呢？WLLN的证明指出了确定一个估计量是否一致的一种方法：如果它是无偏的，并且随着样本量的增加，抽样方差减小。
- en: '**Theorem 3.5** For any estimator \(\widehat{\theta}_n\), if \(\text{bias}[\widehat{\theta}_n]
    = 0\) and \(\V[\widehat{\theta}_n] \rightarrow 0\) as \(n\rightarrow \infty\),
    then \(\widehat{\theta}_n\) is consistent.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理3.5** 对于任何估计量 \(\widehat{\theta}_n\)，如果 \(\text{bias}[\widehat{\theta}_n]
    = 0\) 并且 \(\V[\widehat{\theta}_n] \rightarrow 0\) 当 \(n\rightarrow \infty\)，那么
    \(\widehat{\theta}_n\) 是一致的。'
- en: Thus, for unbiased estimators, if we can characterize its sampling variance,
    we should be able to tell if it is consistent. This result is handy since working
    with the probability statements used for the WLLN can sometimes be confusing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于无偏估计量，如果我们能描述其抽样方差，我们就应该能够判断它是否一致。这个结果很有用，因为处理用于WLLN的概率陈述有时可能会令人困惑。
- en: What about biased estimators? Consider a situation where we calculate average
    household income, \(\Xbar_n\), from a random sample with mean \(\mu\), but our
    actual interest is in the log of average income, \(\alpha = \log(\mu)\). We can
    obviously use the standard plug-in estimator \(\widehat{\alpha} = \log(\Xbar_n)\),
    but, for nonlinear functions like logarithms we have \(\log\left(\E[Z]\right)
    \neq \E[\log(Z)]\), so \(\E[\widehat{\alpha}] \neq \log(\E[\Xbar_n])\) and the
    plug-in estimator will be biased for \(\log(\mu)\). Obtaining an expression for
    the bias in terms of \(n\) is also difficult. Is the quest doomed? Must we give
    up on consistency? No, and, in fact, a few key properties of consistency make
    working with it much easier compared to unbiasedness.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于有偏估计量呢？考虑一种情况，我们从一个具有均值 \(\mu\) 的随机样本中计算平均家庭收入，\(\Xbar_n\)，但我们的实际兴趣在于平均收入的对数，\(\alpha
    = \log(\mu)\)。我们可以显然使用标准的插值估计量 \(\widehat{\alpha} = \log(\Xbar_n)\)，但对于对数这样的非线性函数，我们有
    \(\log\left(\E[Z]\right) \neq \E[\log(Z)]\)，所以 \(\E[\widehat{\alpha}] \neq \log(\E[\Xbar_n])\)，因此插值估计量对于
    \(\log(\mu)\) 将是有偏的。用 \(n\) 来表示偏差的表达式也很困难。这个任务注定要失败吗？我们必须放弃一致性吗？不，实际上，一致性的一些关键性质使得与它相比，与无偏性相比，工作起来要容易得多。
- en: '**Theorem 3.6 (Properties of convergence in probability)** Let \(X_n\) and
    \(Z_n\) be two sequences of random variables such that \(X_n \inprob a\) and \(Z_n
    \inprob b\), and let \(g(\cdot)\) be a continuous function. Then,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 3.6（概率收敛的性质）** 设 \(X_n\) 和 \(Z_n\) 是两个随机变量的序列，使得 \(X_n \inprob a\) 和 \(Z_n
    \inprob b\)，设 \(g(\cdot)\) 是一个连续函数。那么，'
- en: \(g(X_n) \inprob g(a)\) (continuous mapping theorem)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(g(X_n) \inprob g(a)\) （连续映射定理）
- en: \(X_n + Z_n \inprob a + b\)
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(X_n + Z_n \inprob a + b\)
- en: \(X_nZ_n \inprob ab\)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(X_nZ_n \inprob ab\)
- en: \(X_n/Z_n \inprob a/b\) if \(b > 0\).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(X_n/Z_n \inprob a/b\) 如果 \(b > 0\).
- en: We can now see that many of the nasty problems with expectations and nonlinear
    functions are made considerably easier with convergence in probability in the
    asymptotic setting. So while we know that \(\log(\Xbar_n)\) is biased for \(\log(\mu)\),
    we know that it is consistent since \(\log(\Xbar_n) \inprob \log(\mu)\) because
    \(\log\) is a continuous function.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到，在渐近设置中的概率收敛使得许多与期望和非线性函数相关的问题变得容易得多。因此，虽然我们知道 \(\log(\Xbar_n)\) 对于
    \(\log(\mu)\) 是有偏的，但我们知道它是一致的，因为 \(\log(\Xbar_n) \inprob \log(\mu)\)，因为 \(\log\)
    是一个连续函数。
- en: '**Example 3.4** Suppose we implemented a survey by randomly selecting a sample
    from the population of size \(n\), but not everyone responds to the survey. Let
    the data consist of pairs of random variables, \((Y_1, R_1), \ldots, (Y_n, R_n)\),
    where \(Y_i\) is the question of interest and \(R_i\) is a binary indicator for
    if the respondent answered the question (\(R_i = 1\)) or not (\(R_i = 0\)). Our
    goal is to estimate the mean of the question for responders: \(\E[Y_i \mid R_i
    = 1]\). We can use the law of iterated expectation to obtain \[ \begin{aligned}
    \E[Y_iR_i] &= \E[Y_i \mid R_i = 1]\P(R_i = 1) + \E[ 0 \mid R_i = 0]\P(R_i = 0)
    \\ \implies \E[Y_i \mid R_i = 1] &= \frac{\E[Y_iR_i]}{\P(R_i = 1)} \end{aligned}
    \]'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**例 3.4** 假设我们通过从大小为 \(n\) 的总体中随机选择样本来实施一项调查，但并非每个人都回答调查。数据由随机变量的对组成，\((Y_1,
    R_1), \ldots, (Y_n, R_n)\)，其中 \(Y_i\) 是感兴趣的问题，\(R_i\) 是一个二元指标，表示受访者是否回答了问题（\(R_i
    = 1\)）或没有回答（\(R_i = 0\)）。我们的目标是估计回答者问题的均值：\(\E[Y_i \mid R_i = 1]\)。我们可以使用迭代期望定律来得到
    \[ \begin{aligned} \E[Y_iR_i] &= \E[Y_i \mid R_i = 1]\P(R_i = 1) + \E[ 0 \mid
    R_i = 0]\P(R_i = 0) \\ \implies \E[Y_i \mid R_i = 1] &= \frac{\E[Y_iR_i]}{\P(R_i
    = 1)} \end{aligned} \]'
- en: 'The relevant estimator for this quantity is the mean of the outcome among those
    who responded, which is slightly more complicated than a typical sample mean because
    the denominator is a random variable: \[ \widehat{\theta}_n = \frac{\sum_{i=1}^n
    Y_iR_i}{\sum_{i=1}^n R_i}. \] Notice that this estimator is the ratio of two random
    variables. The numerator has mean \(n\E[Y_iR_i]\) and the denominator has mean
    \(n\P(R_i = 1)\). It is then tempting to say that we can take the ratio of these
    means as the mean of \(\widehat{\theta}_n\), but expectations are not preserved
    in nonlinear functions like this.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量的相关估计量是那些回答者的结果的平均值，这比典型的样本均值稍微复杂一些，因为分母是一个随机变量：\[ \widehat{\theta}_n = \frac{\sum_{i=1}^n
    Y_iR_i}{\sum_{i=1}^n R_i}. \] 注意，这个估计量是两个随机变量的比值。分子具有均值 \(n\E[Y_iR_i]\)，分母具有均值
    \(n\P(R_i = 1)\)。因此，我们可能会说我们可以取这些均值的比作为 \(\widehat{\theta}_n\) 的均值，但是期望在非线性函数如这种情况下是不保留的。
- en: We can establish consistency of our estimator, though, by noting that we can
    rewrite the estimator as a ratio of sample means \[ \widehat{\theta}_n = \frac{(1/n)\sum_{i=1}^n
    Y_iR_i}{(1/n)\sum_{i=1}^n R_i}, \] where by the WLLN the numerator \((1/n)\sum_{i=1}^n
    Y_iR_i \inprob \E[Y_iR_i]\) and the denominator \((1/n)\sum_{i=1}^n R_i \inprob
    \P(R_i = 1)\). Thus, by [Theorem 3.6](#thm-inprob-properties), we have \[ \widehat{\theta}_n
    = \frac{(1/n)\sum_{i=1}^n Y_iR_i}{(1/n)\sum_{i=1}^n R_i} \inprob \frac{\E[Y_iR_i]}{\P[R_i
    = 1]} = \E[Y_i \mid R_i = 1], \] so long as the probability of responding is greater
    than zero. This establishes that our sample mean among responders, while biased
    for the conditional expectation among responders, is consistent for that quantity.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以通过注意到我们可以将估计量重写为样本均值之比来建立估计量的一致性 \[ \widehat{\theta}_n = \frac{(1/n)\sum_{i=1}^n
    Y_iR_i}{(1/n)\sum_{i=1}^n R_i}, \] 其中，根据大数定律，分子 \((1/n)\sum_{i=1}^n Y_iR_i \inprob
    \E[Y_iR_i]\) 和分母 \((1/n)\sum_{i=1}^n R_i \inprob \P(R_i = 1)\)。因此，根据[定理 3.6](#thm-inprob-properties)，我们有
    \[ \widehat{\theta}_n = \frac{(1/n)\sum_{i=1}^n Y_iR_i}{(1/n)\sum_{i=1}^n R_i}
    \inprob \frac{\E[Y_iR_i]}{\P[R_i = 1]} = \E[Y_i \mid R_i = 1], \] 只要响应的概率大于零。这表明，我们的响应者样本均值，尽管对于响应者的条件期望是有偏的，但对于该数量是一致的。
- en: Keeping the difference between unbiased and consistent clear in your mind is
    essential. You can easily create ridiculous unbiased estimators that are inconsistent.
    Let’s return to our iid sample, \(X_1, \ldots, X_n\), from a population with \(E[X_i]
    = \mu\). There is nothing in the rule book against defining an estimator \(\widehat{\theta}_{first}
    = X_1\) that uses the first observation as the estimate. This estimator is silly,
    but it is unbiased since \(\E[\widehat{\theta}_{first}] = \E[X_1] = \mu\). It
    is inconsistent since the sampling variance of this estimator is just the variance
    of the population distribution, \(\V[\widehat{\theta}_{first}] = \V[X_i] = \sigma^2\),
    which does not change as a function of the sample size. Generally speaking, we
    can regard “unbiased but inconsistent” estimators as silly and not worth our time
    (along with biased and inconsistent estimators).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在心中保持无偏和一致性的区别是至关重要的。你可以轻易地创建出荒谬的无偏估计量，它们是不一致的。让我们回到我们的独立同分布样本 \(X_1, \ldots,
    X_n\)，它来自一个期望值为 \(\mu\) 的总体。规则书中没有任何反对定义一个估计量 \(\widehat{\theta}_{first} = X_1\)，它使用第一个观测值作为估计。这个估计量很愚蠢，但它是无偏的，因为
    \(\E[\widehat{\theta}_{first}] = \E[X_1] = \mu\)。它是不一致的，因为该估计量的抽样方差就是总体分布的方差，\(\V[\widehat{\theta}_{first}]
    = \V[X_i] = \sigma^2\)，它不会随着样本大小的变化而变化。一般来说，我们可以将“无偏但不一致”的估计量视为愚蠢的，不值得我们花费时间（与有偏且不一致的估计量一样）。
- en: Some estimators are biased but consistent that are often much more interesting.
    We already saw one such estimator in [Example 3.4](#exm-nonresponse), but there
    are many more. Maximum likelihood estimators, for example, are (under some regularity
    conditions) consistent for the parameters of a parametric model but are often
    biased.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一些估计量是有偏但一致的，通常更有趣。我们已经在[示例 3.4](#exm-nonresponse)中看到了这样一个估计量，但还有很多。例如，最大似然估计量（在满足某些正则性条件的情况下）对于参数模型是一致的，但通常是有偏的。
- en: To study these estimator, we can broaden [Theorem 3.5](#thm-consis) to the class
    of **asymptotically unbiased** estimators that have bias that vanishes as the
    sample size grows.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究这些估计量，我们可以将[定理 3.5](#thm-consis)扩展到具有偏差随着样本大小增长而消失的**渐近无偏**估计量的类别。
- en: '**Theorem 3.7** For any estimator \(\widehat{\theta}_n\), if \(\text{bias}[\widehat{\theta}_n]
    \to 0\) and \(\V[\widehat{\theta}_n] \rightarrow 0\) as \(n\rightarrow \infty\),
    then \(\widehat{\theta}_n\) is consistent.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 3.7** 对于任何估计量 \(\widehat{\theta}_n\)，如果 \(\text{bias}[\widehat{\theta}_n]
    \to 0\) 和 \(\V[\widehat{\theta}_n] \rightarrow 0\) 当 \(n\rightarrow \infty\) 时，那么
    \(\widehat{\theta}_n\) 是一致的。'
- en: '*Proof*. Using Markov’s inequality, we have \[ \P\left( |\widehat{\theta}_n
    - \theta| \geq \delta \right) = \P((\widehat{\theta}_n-\theta)^2 \geq \delta^2)
    \leq \frac{\E[(\widehat{\theta}_n - \theta)^2]}{\delta^2} = \frac{\text{bias}[\widehat{\theta}_n]^2
    + \V[\widehat{\theta}]}{\delta^2} \to 0. \] The last inequality follows from the
    bias-variance decomposition of the mean squared error in [Equation 2.1](estimation.html#eq-mse-decomposition).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*。使用马尔可夫不等式，我们有 \[ \P\left( |\widehat{\theta}_n - \theta| \geq \delta \right)
    = \P((\widehat{\theta}_n-\theta)^2 \geq \delta^2) \leq \frac{\E[(\widehat{\theta}_n
    - \theta)^2]}{\delta^2} = \frac{\text{bias}[\widehat{\theta}_n]^2 + \V[\widehat{\theta}]}{\delta^2}
    \to 0. \] 最后的不等式来自于[方程 2.1](estimation.html#eq-mse-decomposition)中均方误差的偏差-方差分解。'
- en: We can use this result to show consistency for a large range of estimators.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个结果来证明大量估计量的一致性。
- en: '**Example 3.5 (Plug-in variance estimator)** In the last chapter, we introduced
    the plug-in estimator for the population variance, \[ \widehat{\sigma}^2 = \frac{1}{n}
    \sum_{i=1}^n (X_i - \Xbar_n)^2, \] which we will now show is biased but consistent.
    To see the bias note that we can rewrite the sum of square deviations \[\sum_{i=1}^n
    (X_i - \Xbar_n)^2 = \sum_{i=1}^n X_i^2 - n\Xbar_n^2\. \] Then, the expectation
    of the plug-in estimator is \[ \begin{aligned} \E[\widehat{\sigma}^2] & = \E\left[\frac{1}{n}\sum_{i=1}^n
    X_i^2\right] - \E[\Xbar_n^2] \\ &= \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n
    \E[X_iX_j] \\ &= \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n
    \sum_{j\neq i} \underbrace{\E[X_i]\E[X_j]}_{\text{independence}} \\ &= \E[X_i^2]
    - \frac{1}{n}\E[X_i^2] - \frac{1}{n^2} n(n-1)\mu^2 \\ &= \frac{n-1}{n} \left(\E[X_i^2]
    - \mu^2\right) \\ &= \frac{n-1}{n} \sigma^2 = \sigma^2 - \frac{1}{n}\sigma^2 \end{aligned}.
    \] Thus, we can see that the bias of the plug-in estimator is \(-(1/n)\sigma^2\),
    so it slightly underestimates the variance. Nicely, though, the bias shrinks as
    a function of the sample size, so according to [Theorem 3.7](#thm-consis-2), it
    will be consistent so long as the sampling variance of \(\widehat{\sigma}^2\)
    shrinks as a function of the sample size, which it does (though omit that proof
    here). Of course, simply multiplying this estimator by \(n/(n-1)\) will give an
    unbiased and consistent estimator that is also the typical sample variance estimator.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**例 3.5（插值方差估计量）** 在上一章中，我们介绍了用于总体方差的插值估计量 \[ \widehat{\sigma}^2 = \frac{1}{n}
    \sum_{i=1}^n (X_i - \Xbar_n)^2, \] 我们现在将证明它是有偏但一致的。为了看到偏差，请注意我们可以重写平方偏差之和 \[\sum_{i=1}^n
    (X_i - \Xbar_n)^2 = \sum_{i=1}^n X_i^2 - n\Xbar_n^2\. \] 然后，插值估计量的期望值是 \[ \begin{aligned}
    \E[\widehat{\sigma}^2] & = \E\left[\frac{1}{n}\sum_{i=1}^n X_i^2\right] - \E[\Xbar_n^2]
    \\ &= \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \E[X_iX_j] \\ &= \E[X_i^2]
    - \frac{1}{n^2}\sum_{i=1}^n \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \sum_{j\neq
    i} \underbrace{\E[X_i]\E[X_j]}_{\text{独立性}} \\ &= \E[X_i^2] - \frac{1}{n}\E[X_i^2]
    - \frac{1}{n^2} n(n-1)\mu^2 \\ &= \frac{n-1}{n} \left(\E[X_i^2] - \mu^2\right)
    \\ &= \frac{n-1}{n} \sigma^2 = \sigma^2 - \frac{1}{n}\sigma^2 \end{aligned}. \]
    因此，我们可以看到插值估计量的偏差是 \(-(1/n)\sigma^2\)，所以它略微低估了方差。然而，偏差会随着样本量的增加而缩小，所以根据 [定理 3.7](#thm-consis-2)，只要
    \(\widehat{\sigma}^2\) 的抽样方差随着样本量的增加而缩小（这确实如此，但在此省略证明），它将是一致的。当然，简单地乘以这个估计量 \(n/(n-1)\)
    将给出一个无偏且一致估计量，它也是典型的样本方差估计量。'
- en: 3.7 Convergence in distribution and the central limit theorem
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 分布收敛与中心极限定理
- en: Convergence in probability and the law of large numbers are beneficial for understanding
    how our estimators will (or will not) collapse to their estimand as the sample
    size increases. But what about the shape of the sampling distribution of our estimators?
    For statistical inference, we would like to be able to make probability statements
    such as \(\P(a \leq \widehat{\theta}_n \leq b)\). These statements will be the
    basis of hypothesis testing and confidence intervals. But to make those types
    of statements, we need to know the entire distribution of \(\widehat{\theta}_n\),
    not just the mean and variance. Luckily, established results will allow us to
    approximate the sampling distribution of a vast swath of estimators when our sample
    sizes are large.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 概率收敛和大数定律有助于我们理解当样本量增加时，我们的估计量将如何（或不会）收敛到其估计值。但我们的估计量的抽样分布的形状又如何呢？对于统计推断，我们希望能够做出概率陈述，例如
    \(\P(a \leq \widehat{\theta}_n \leq b)\)。这些陈述将是假设检验和置信区间的依据。但为了做出这些类型的陈述，我们需要知道
    \(\widehat{\theta}_n\) 的整个分布，而不仅仅是均值和方差。幸运的是，已建立的结果将允许我们在样本量较大时近似大量估计量的抽样分布。
- en: We need first to describe a weaker form of convergence to see how we will develop
    these approximations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要描述一种较弱形式的收敛，以了解我们将如何发展这些近似。
- en: '**Definition 3.4** Let \(X_1,X_2,\ldots\), be a sequence of r.v.s, and for
    \(n = 1,2, \ldots\) let \(F_n(x)\) be the c.d.f. of \(X_n\). Then it is said that
    \(X_1, X_2, \ldots\) **converges in distribution** to r.v. \(X\) with c.d.f. \(F(x)\)
    if \[ \lim_{n\rightarrow \infty} F_n(x) = F(x), \] for all values of \(x\) for
    which \(F(x)\) is continuous. We write this as \(X_n \indist X\) or sometimes
    \(X_n ⇝ X\).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义 3.4** 设 \(X_1,X_2,\ldots\) 为一系列随机变量，对于 \(n = 1,2, \ldots\)，令 \(F_n(x)\)
    为 \(X_n\) 的累积分布函数。如果 \[ \lim_{n\rightarrow \infty} F_n(x) = F(x), \] 对于 \(F(x)\)
    连续的所有 \(x\) 值，则说 \(X_1, X_2, \ldots\) **在分布上收敛**到随机变量 \(X\)，其累积分布函数为 \(F(x)\)。我们将其写作
    \(X_n \indist X\) 或有时写作 \(X_n ⇝ X\)。'
- en: Essentially, convergence in distribution means that as \(n\) gets large, the
    distribution of \(X_n\) becomes more and more similar to the distribution of \(X\),
    which we often call the **asymptotic distribution** of \(X_n\) (other names include
    the **large-sample distribution**). If we know that \(X_n \indist X\), then we
    can use the distribution of \(X\) as an approximation to the distribution of \(X_n\),
    and that distribution can be reasonably accurate.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，分布收敛意味着当 \(n\) 趋于无穷大时，\(X_n\) 的分布会越来越接近 \(X\) 的分布，我们通常称之为 \(X_n\) 的**渐近分布**（其他名称包括**大样本分布**）。如果我们知道
    \(X_n \indist X\)，那么我们可以使用 \(X\) 的分布来近似 \(X_n\) 的分布，而这个分布可以相当准确。
- en: '**Example 3.6** A simple example of convergence in distribution would be the
    sequence \[ X_n \sim N\left(\frac{1}{n}, 1 + \frac{1}{n}\right), \] which, of
    course, has the cdf, \[ \Phi\left(\frac{x - 1/n}{1+1/n}\right). \] By inspection,
    this converges to \(\Phi(x)\), which is the cdf for the standard normal. This
    implies \(X_n \indist N(0, 1)\).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**例 3.6** 分布收敛的一个简单例子是序列 \[ X_n \sim N\left(\frac{1}{n}, 1 + \frac{1}{n}\right),
    \] 当然，它有累积分布函数（cdf），\[ \Phi\left(\frac{x - 1/n}{1+1/n}\right). \] 通过观察，这收敛到 \(\Phi(x)\)，即标准正态分布的
    cdf。这意味着 \(X_n \indist N(0, 1)\)。'
- en: '![](../Images/b53a8e50461e0c0c5e321087f6a8a2e8.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/b53a8e50461e0c0c5e321087f6a8a2e8.png)'
- en: 'One of the most remarkable results in probability and statistics is that a
    large class of estimators will converge in distribution to one particular family
    of distributions: the normal. This result is one reason we study the normal so
    much and why investing in building intuition about it will pay off across many
    domains of applied work. We call this broad class of results the “central limit
    theorem” (CLT), but it would probably be more accurate to refer to them as “central
    limit theorems” since much of statistics is devoted to showing the result in different
    settings. We now present the simplest CLT for the sample mean.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 概率论和统计学中最令人瞩目的结果之一是，大量估计量将在分布上收敛到某一特定分布族：正态分布。这个结果是我们研究正态分布的原因之一，也是为什么在许多应用领域投资于建立对它的直觉会得到回报。我们称这个广泛的结果类为“中心极限定理”（CLT），但可能更准确地说它们是“中心极限定理”，因为统计学的大部分工作都致力于在不同的设置中证明这个结果。我们现在介绍样本均值的简单CLT。
- en: '**Theorem 3.8 (Central Limit Theorem)** Let \(X_1, \ldots, X_n\) be i.i.d.
    r.v.s from a distribution with mean \(\mu = \E[X_i]\) and variance \(\sigma^2
    = \V[X_i]\). Then if \(\E[X_i^2] < \infty\), we have \[ \frac{\Xbar_n - \mu}{\sqrt{\V[\Xbar_n]}}
    = \frac{\sqrt{n}\left(\Xbar_n - \mu\right)}{\sigma} \indist \N(0, 1). \]'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 3.8（中心极限定理）** 设 \(X_1, \ldots, X_n\) 是从均值为 \(\mu = \E[X_i]\) 和方差 \(\sigma^2
    = \V[X_i]\) 的分布中独立同分布的随机变量。如果 \(\E[X_i^2] < \infty\)，则有 \[ \frac{\Xbar_n - \mu}{\sqrt{\V[\Xbar_n]}}
    = \frac{\sqrt{n}\left(\Xbar_n - \mu\right)}{\sigma} \indist \N(0, 1). \]'
- en: 'In words: the sample mean of a random sample from a population with finite
    mean and variance will be approximately normally distributed in large samples.
    Notice how we have not made any assumptions about the distribution of the underlying
    random variables, \(X_i\). They could be binary, event count, continuous, or anything.
    The CLT is incredibly broadly applicable.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 用文字来说：从具有有限均值和方差的总体中抽取的随机样本的样本均值在大样本中将大致呈正态分布。注意我们并没有对基础随机变量 \(X_i\) 的分布做出任何假设。它们可以是二元的、事件计数、连续的，或者任何其他类型。中心极限定理（CLT）具有极其广泛的应用性。
- en: '*Notation alert* *Why do we state the CLT in terms of the sample mean after
    centering and scaling by its standard error? Suppose we don’t normalize the sample
    mean in this way. In that case, it isn’t easy to talk about convergence in distribution
    because we know from the WLLN that \(\Xbar_n \inprob \mu\), so in the limit, the
    distribution of \(\Xbar_n\) is concentrated at point mass around that value. Normalizing
    by centering and rescaling ensures that the variance of the resulting quantity
    will not depend on \(n\), so it makes sense to talk about its distribution converging.
    Sometimes you will see the equivalent result as \[ \sqrt{n}\left(\Xbar_n - \mu\right)
    \indist \N(0, \sigma^2). \]*  *We can use this result to state approximations
    that we can use when discussing estimators such as \[ \Xbar_n \overset{a}{\sim}
    N(\mu, \sigma^2/n), \] where we use \(\overset{a}{\sim}\) to be “approximately
    distributed as in large samples.” This approximation allows us to say things like:
    “in large samples, we should expect the sample mean to between within \(2\sigma/\sqrt{n}\)
    of the true mean in 95% of repeated samples.” These statements will be essential
    for hypothesis tests and confidence intervals! Estimators so often follow the
    CLT that we have an expression for this property.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意符号* *为什么我们在对样本均值进行中心化和按其标准误差缩放后，用样本均值来陈述中心极限定理（CLT）？假设我们不以这种方式对样本均值进行标准化。在这种情况下，我们很难讨论分布收敛，因为我们知道从大数定律（WLLN）中我们知道
    \(\Xbar_n \inprob \mu\)，所以在极限情况下，\(\Xbar_n\) 的分布将集中在那个值周围的质量点。通过中心化和重新缩放来标准化确保了结果的方差不会依赖于
    \(n\)，因此讨论其分布收敛是有意义的。有时你会看到等效的结果为 \[ \sqrt{n}\left(\Xbar_n - \mu\right) \indist
    \N(0, \sigma^2). \]* *我们可以使用这个结果来陈述在讨论估计量时可以使用的近似，例如 \[ \Xbar_n \overset{a}{\sim}
    N(\mu, \sigma^2/n), \] 其中我们使用 \(\overset{a}{\sim}\) 表示“在大样本中近似分布”。这种近似使我们能够说：“在大样本中，我们应该期望样本均值在95%的重复样本中在真实均值周围
    \(2\sigma/\sqrt{n}\) 的范围内。” 这些陈述对于假设检验和置信区间至关重要！估计量通常遵循CLT，因此我们有一个表示这个性质的公式。'
- en: '**Definition 3.5** An estimator \(\widehat{\theta}_n\) is **asymptotically
    normal** if for some \(\theta\) \[ \sqrt{n}\left( \widehat{\theta}_n - \theta
    \right) \indist N\left(0,\V_{\theta}\right). \]'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义3.5** 一个估计量 \(\widehat{\theta}_n\) 如果对于某个 \(\theta\) \[ \sqrt{n}\left(
    \widehat{\theta}_n - \theta \right) \indist N\left(0,\V_{\theta}\right). \] 则是**渐近正态**的。'
- en: '**Example 3.7** To illustrate how the CLT works, we can simulate the sampling
    distribution of the (normalized) sample mean at different sample sizes. Let \(X_1,
    \ldots, X_n\) be iid samples from a Bernoulli with probability of success 0.25\.
    We then draw repeated samples of size \(n=30\) and \(n=100\) and calculate \(\sqrt{n}(\Xbar_n
    - 0.25)/\sigma\) for each random sample. [Figure 3.2](#fig-clt) plots the density
    of these two sampling distributions along with a standard normal reference. We
    can see that even at \(n=30\), the rough shape of the density looks normal, with
    spikes and valleys due to the discrete nature of the data (the sample mean can
    only take on 31 possible values in this case). By \(n=100\), the sampling distribution
    is very close to the true standard normal.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例3.7** 为了说明CLT是如何工作的，我们可以模拟不同样本大小下（标准化）样本均值的抽样分布。设 \(X_1, \ldots, X_n\)
    是从成功概率为0.25的伯努利分布中独立同分布抽取的样本。然后我们重复抽取大小为 \(n=30\) 和 \(n=100\) 的样本，并计算每个随机样本的 \(\sqrt{n}(\Xbar_n
    - 0.25)/\sigma\)。[图3.2](#fig-clt) 绘制了这两个抽样分布的密度，以及一个标准正态参考。我们可以看到，即使在 \(n=30\)
    时，密度的粗略形状看起来是正态的，由于数据的离散性（在这种情况下，样本均值只能取31个可能值），存在峰值和谷值。到 \(n=100\) 时，抽样分布非常接近真实的标准正态分布。'
- en: '![](../Images/80299577639d66ee8f1bf7258901ee63.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80299577639d66ee8f1bf7258901ee63.png)'
- en: 'Figure 3.2: Sampling distributions of the normalized sample mean at n=30 and
    n=100.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：n=30和n=100时标准化样本均值的抽样分布。
- en: There are several properties of convergence in distribution that are helpful
    to us.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 分布收敛的几个性质对我们很有帮助。
- en: '**Theorem 3.9 (Properties of convergence in distribution)** Let \(X_n\) be
    a sequence of random variables \(X_1, X_2,\ldots\) that converges in distribution
    to some rv \(X\) and let \(Y_n\) be a sequence of random variables \(Y_1, Y_2,\ldots\)
    that converges in probability to some number, \(c\). Then,'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理3.9（分布收敛的性质）** 设 \(X_n\) 是一个随机变量序列 \(X_1, X_2,\ldots\)，它在分布上收敛到某个随机变量 \(X\)，并且设
    \(Y_n\) 是一个随机变量序列 \(Y_1, Y_2,\ldots\)，它在概率上收敛到某个数 \(c\)。那么，'
- en: \(g(X_n) \indist g(X)\) for all continuous functions \(g\).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有连续函数 \(g\)，有 \(g(X_n) \indist g(X)\)。
- en: \(X_nY_n\) converges in distribution to \(cX\)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(X_nY_n\) 在分布上收敛到 \(cX\)
- en: \(X_n + Y_n\) converges in distribution to \(X + c\)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(X_n + Y_n\) 在分布上收敛到 \(X + c\)
- en: \(X_n / Y_n\) converges in distribution to \(X / c\) if \(c \neq 0\)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 \(c \neq 0\)，则 \(X_n / Y_n\) 在分布上收敛到 \(X / c\)。
- en: We refer to the last three results as **Slutsky’s theorem**. These results are
    often crucial for determining an estimator’s asymptotic distribution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最后三个结果称为**Slutsky定理**。这些结果通常对于确定估计量渐近分布至关重要。
- en: A critical application of Slutsky’s theorem is when we replace the (unknown)
    population variance in the CLT with an estimate. Recall the definition of the
    **sample variance** as \[ S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \Xbar_n)^2,
    \] with the **sample standard deviation** defined as \(S_{n} = \sqrt{S_{n}^2}\).
    It’s easy to show that these are consistent estimators for their respective population
    parameters \[ S_{n}^2 \inprob \sigma^2 = \V[X_i], \qquad S_{n} \inprob \sigma,
    \] which, by Slutsky’s theorem, implies that \[ \frac{\sqrt{n}\left(\Xbar_n -
    \mu\right)}{S_n} \indist \N(0, 1) \] Comparing this result to the statement of
    CLT, we see that replacing the population variance with a consistent estimate
    of the variance (or standard deviation) does not affect the asymptotic distribution.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Slutsky定理的一个关键应用是在我们将CLT中的（未知的）总体方差替换为一个估计值时。回忆一下**样本方差**的定义为 \[ S_n^2 = \frac{1}{n-1}
    \sum_{i=1}^n (X_i - \Xbar_n)^2, \] 其中**样本标准差**定义为 \(S_{n} = \sqrt{S_{n}^2}\)。很容易证明这些是它们各自总体参数的一致估计量
    \[ S_{n}^2 \inprob \sigma^2 = \V[X_i], \qquad S_{n} \inprob \sigma, \] 根据Slutsky定理，这意味着
    \[ \frac{\sqrt{n}\left(\Xbar_n - \mu\right)}{S_n} \indist \N(0, 1) \] 将这个结果与CLT的陈述进行比较，我们看到用方差（或标准差）的一致估计值替换总体方差不会影响渐近分布。
- en: Like with the WLLN, the CLT holds for random vectors of sample means, where
    their centered and scaled versions converge to a multivariate normal distribution
    with a covariance matrix equal to the covariance matrix of the underlying random
    vectors of data, \(\X_i\).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与WLLN一样，CLT适用于样本均值的随机向量，其中它们的中心化和缩放版本收敛到一个协方差矩阵等于数据的基本随机向量的协方差矩阵的多变量正态分布，即 \(\X_i\)。
- en: '**Theorem 3.10** If \(\mb{X}_i \in \mathbb{R}^k\) are i.i.d. and \(\E\Vert
    \mb{X}_i \Vert^2 < \infty\), then as \(n \to \infty\), \[ \sqrt{n}\left( \overline{\mb{X}}_n
    - \mb{\mu}\right) \indist \N(0, \mb{\Sigma}), \] where \(\mb{\mu} = \E[\mb{X}_i]\)
    and \(\mb{\Sigma} = \V[\mb{X}_i] = \E\left[(\mb{X}_i-\mb{\mu})(\mb{X}_i - \mb{\mu})''\right]\).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理3.10** 如果 \(\mb{X}_i \in \mathbb{R}^k\) 是相互独立的，并且 \(\E\Vert \mb{X}_i \Vert^2
    < \infty\)，那么当 \(n \to \infty\) 时，\[ \sqrt{n}\left( \overline{\mb{X}}_n - \mb{\mu}\right)
    \indist \N(0, \mb{\Sigma}), \] 其中 \(\mb{\mu} = \E[\mb{X}_i]\) 和 \(\mb{\Sigma}
    = \V[\mb{X}_i] = \E\left[(\mb{X}_i-\mb{\mu})(\mb{X}_i - \mb{\mu})''\right]\)。'
- en: Notice that \(\mb{\mu}\) is the vector of population means for all the random
    variables in \(\X_i\) and \(\mb{\Sigma}\) is the variance-covariance matrix for
    that vector.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mb{\mu}\) 是 \(\X_i\) 中所有随机变量的总体均值向量，而 \(\mb{\Sigma}\) 是该向量的方差-协方差矩阵。
- en: '*Note* *As with the notation alert with the WLLN, we are using shorthand here,
    \(\E\Vert \mb{X}_i \Vert^2 < \infty\), which implies that \(\E[X_{ij}^2] < \infty\)
    for all \(j = 1,\ldots, k\), or equivalently, that the variances of each variable
    in the sample means has finite variance.**  **## 3.8 Confidence intervals'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *与WLLN中的符号警告一样，我们在这里使用缩写，\(\E\Vert \mb{X}_i \Vert^2 < \infty\)，这表示对于所有
    \(j = 1,\ldots, k\)，\(\E[X_{ij}^2] < \infty\)，或者等价地，样本均值中每个变量的方差是有限的。**  **##
    3.8 置信区间'
- en: 'We now turn to an essential application of the central limit theorem: confidence
    intervals.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向中心极限定理的一个基本应用：置信区间。
- en: 'Suppose we have run an experiment with a treatment and control group and have
    presented readers with our single best guess about the treatment effect using
    the difference in sample means. We have also presented the estimated standard
    error of this estimate to give readers a sense of how variable it is. But none
    of these approaches answer a fairly compelling question: what range of values
    of the treatment effect is **plausible** given the data we observe?'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经进行了一个包含处理组和对照组的实验，并使用样本均值的差异给出了我们对处理效应的最佳猜测。我们还给出了这个估计值的估计标准误差，以给读者一个关于其变异性的感觉。但是，这些方法都没有回答一个相当有说服力的问题：根据我们观察到的数据，处理效应的可能值范围是多少？
- en: A point estimate of the difference in sample means typically has 0 probability
    of being the exact true value, but intuitively we hope that the true treatment
    effect is close to our estimate. **Confidence intervals** make this kind of intuition
    more formal by instead estimating ranges of values with a fixed percentage of
    these ranges containing the actual unknown parameter value.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 样本均值差异的点估计通常有0的概率是确切的真值，但直观上我们希望真正的治疗效果接近我们的估计。**置信区间**通过估计包含实际未知参数值的固定百分比的值范围，使这种直觉更加正式。
- en: We begin with the basic definition of a confidence interval.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从置信区间的基本定义开始。
- en: '**Definition 3.6** A \(1-\alpha\) **confidence interval** for a real-valued
    parameter \(\theta\) is a pair of statistics \(L= L(X_1, \ldots, X_n)\) and \(U
    = U(X_1, \ldots, X_n)\) such that \(L < U\) for all values of the sample and such
    that \[ \P(L \leq \theta \leq U \mid \theta) \geq 1-\alpha, \quad \forall \theta
    \in \Theta. \]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义3.6** 对于实值参数 \(\theta\) 的 \(1-\alpha\) **置信区间**是一对统计量 \(L= L(X_1, \ldots,
    X_n)\) 和 \(U = U(X_1, \ldots, X_n)\)，使得对于所有样本值 \(L < U\)，并且满足 \[ \P(L \leq \theta
    \leq U \mid \theta) \geq 1-\alpha, \quad \forall \theta \in \Theta. \]'
- en: We say that a \(1-\alpha\) confidence interval covers (or contains, captures,
    traps, etc.) the true value at least \(100(1-\alpha)\%\) of the time, and we refer
    to \(1-\alpha\) as the **coverage probability** or simply **coverage**. Typical
    confidence intervals include 95% percent (\(\alpha = 0.05\)), 90% (\(\alpha =
    0.1\)), and 99% (\(\alpha = 0.01\)). All else equal, larger coverage will imply
    larger intervals.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说一个 \(1-\alpha\) 置信区间至少有 \(100(1-\alpha)\%\) 的时间覆盖（或包含、捕捉、陷阱等）真实值，我们将 \(1-\alpha\)
    称为**覆盖概率**或简单地称为**覆盖**。典型的置信区间包括95%（\(\alpha = 0.05\)）、90%（\(\alpha = 0.1\)）和99%（\(\alpha
    = 0.01\)）。在其他条件相同的情况下，更大的覆盖范围将意味着更大的区间。
- en: So a confidence interval is a random interval with a particular guarantee about
    how often it will contain the true value of the unknown population parameter (in
    our example, the true treatment effect). Remember what is random and what is fixed
    in this setup. The interval varies from sample to sample, but the true value of
    the parameter stays fixed even if it is unknown, and the coverage is how often
    we should expect the interval to contain that true value. The “repeating my sample
    over and over again” analogy can break down very quickly, so it is sometimes helpful
    to interpret it as giving guarantees across confidence intervals across different
    experiments. In particular, suppose that a journal publishes 100 quantitative
    articles annually, each producing a single 95% confidence interval for their quantity
    of interest. Then, if the confidence intervals are valid and each is constructed
    in the exact same way, we should expect 95 of those confidence intervals to contain
    the true value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，置信区间是一个具有特定保证的随机区间，关于它将包含未知总体参数（在我们的例子中，真正的治疗效果）的真实值的频率。记住在这个设置中什么是随机的，什么是固定的。区间会随着样本的不同而变化，但参数的真实值即使未知也保持固定，覆盖频率是指我们期望区间包含该真实值的频率。将“反复进行我的样本”的类比可能会很快失效，因此有时将其解释为在不同实验的置信区间之间提供保证是有帮助的。特别是，假设一家期刊每年发表100篇定量文章，每篇文章都为其感兴趣的数量提供一个单一的95%置信区间。那么，如果置信区间有效，并且每个置信区间都是按照完全相同的方式构建的，我们应该期望其中95个置信区间包含真实值。
- en: '*Warning* *Suppose we have a 95% confidence interval, \([0.1, 0.4]\). It would
    be tempting to make a probability statement like \(\P(0.1 \leq \theta \leq 0.4
    \mid \theta) = 0.95\) or that there’s a 95% chance that the parameter is in \([0.1,
    0.4]\). But looking at the probability statement, everything on the left-hand
    side of the conditioning bar is fixed, so the probability either has to be 0 (\(\theta\)
    is outside the interval) or 1 (\(\theta\) is in the interval); the unknown parameter
    is a fixed value, so it is either in the interval or it is not. Another way to
    think about this is that the coverage probability of a confidence interval refers
    to its status as a pair of random variables, \((L, U)\), not any particular realization
    of those variables like \((0.1, 0.4)\). As an analogy, consider if we calculated
    the sample mean as \(0.25\) and then tried to say that \(0.25\) is unbiased for
    the population mean. This statement doesn’t make sense because unbiasedness refers
    not to a fixed value but how the sample mean varies from sample to sample.*  *In
    most cases, we will not be able to derive exact confidence intervals but rather
    confidence intervals that are **asymptotically valid**, which means that if we
    write the interval as a function of the sample size, \((L_n, U_n)\), they would
    have **asymptotic coverage** \[ \lim_{n\to\infty} \P(L_n \leq \theta \leq U_n)
    \geq 1-\alpha \quad\forall\theta\in\Theta. \]'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*警告* *假设我们有一个95%的置信区间，\([0.1, 0.4]\)。我们可能会倾向于做出一个概率陈述，比如 \(\P(0.1 \leq \theta
    \leq 0.4 \mid \theta) = 0.95\) 或者说参数在 \([0.1, 0.4]\) 内的概率是95%。但是，观察这个概率陈述，条件符号左侧的所有内容都是固定的，所以概率要么是0（\(\theta\)
    在区间外）要么是1（\(\theta\) 在区间内）；未知参数是一个固定值，所以它要么在区间内要么不在。另一种思考方式是，置信区间的覆盖概率是指它作为一对随机变量
    \((L, U)\) 的状态，而不是这些变量的任何特定实现，比如 \((0.1, 0.4)\)。作为一个类比，考虑如果我们计算出的样本均值是 \(0.25\)，然后试图说
    \(0.25\) 是总体均值的无偏估计。这个陈述是没有意义的，因为无偏性不是指一个固定值，而是样本均值如何从样本到样本变化。* *在大多数情况下，我们无法推导出精确的置信区间，而是推导出**渐近有效**的置信区间，这意味着如果我们把区间写成样本大小的函数，\((L_n,
    U_n)\)，它们将具有**渐近覆盖** \[ \lim_{n\to\infty} \P(L_n \leq \theta \leq U_n) \geq 1-\alpha
    \quad\forall\theta\in\Theta. \]'
- en: We can show asymptotic coverage for most confidence intervals since we usually
    rely on large-sample approximations based on the central limit theorem.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示大多数置信区间的渐近覆盖，因为我们通常依赖于基于中心极限定理的大样本近似。
- en: 3.8.1 Deriving confidence intervals
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.1 推导置信区间
- en: To derive confidence intervals, consider the standard formula for the 95% confidence
    interval of the sample mean, \[ \left[\Xbar_n - 1.96\frac{s}{\sqrt{n}},\; \Xbar_n
    + 1.96\frac{s}{\sqrt{n}}\right], \] where \(s\) is the sample standard deviation
    and \(s/\sqrt{n}\) is the estimate of the standard error of the sample mean. If
    this is a 95% confidence interval, then the probability that it contains the true
    population mean \(\mu\) should be 0.95, but how can we derive this? We can justify
    this logic using the central limit theorem, and the argument will hold for any
    asymptotically normal estimator.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要推导置信区间，考虑样本均值的95%置信区间的标准公式，\[ \left[\Xbar_n - 1.96\frac{s}{\sqrt{n}},\; \Xbar_n
    + 1.96\frac{s}{\sqrt{n}}\right], \] 其中 \(s\) 是样本标准差，\(s/\sqrt{n}\) 是样本均值标准误差的估计。如果这是一个95%的置信区间，那么它包含真实总体均值
    \(\mu\) 的概率应该是0.95，但我们如何推导出这个结论呢？我们可以使用中心极限定理来证明这个逻辑，并且这个论点适用于任何渐近正态估计量。
- en: Suppose we have an estimator, \(\widehat{\theta}_n\) for the parameter \(\theta\)
    with estimated standard error \(\widehat{\se}[\widehat{\theta}_n]\). If the estimator
    is asymptotically normal, then in large samples, we know that \[ \frac{\widehat{\theta}_n
    - \theta}{\widehat{\se}[\widehat{\theta}_n]} \sim \N(0, 1). \] Then we use our
    knowledge of the standard normal to find \[ \P\left( -1.96 \leq \frac{\widehat{\theta}_n
    - \theta}{\widehat{\se}[\widehat{\theta}_n]} \leq 1.96\right) = 0.95. \] Multiplying
    each part of the inequality by \(\widehat{\se}[\widehat{\theta}_n]\) gives us
    \[ \P\left( -1.96\,\widehat{\se}[\widehat{\theta}_n] \leq \widehat{\theta}_n -
    \theta \leq 1.96\,\widehat{\se}[\widehat{\theta}_n]\right) = 0.95, \] We then
    subtract all parts by the estimator to get \[ \P\left(-\widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n]
    \leq - \theta \leq -\widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n]\right)
    = 0.95, \] and finally we multiply all parts by \(-1\) (and flipping the inequalities)
    to arrive at \[ \P\left(\widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n]
    \leq \theta \leq \widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n]\right)
    = 0.95. \] To connect back to the definition of the confidence interval, we have
    now shown that the random interval \([L, U]\) where \[ \begin{aligned} L = L(X_1,
    \ldots, X_n) &= \widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n] \\
    U = U(X_1, \ldots, X_n) &= \widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n],
    \end{aligned} \] is an asymptotically valid estimator.[³](#fn3) Replacing \(\Xbar_n\)
    for \(\widehat{\theta}_n\) and \(s/\sqrt{n}\) for \(\widehat{\se}[\widehat{\theta}_n]\)
    establishes how the standard 95% confidence interval for the sample mean above
    is asymptotically valid.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个参数 \(\theta\) 的估计量 \(\widehat{\theta}_n\)，其估计标准误差为 \(\widehat{\se}[\widehat{\theta}_n]\)。如果估计量是渐近正态的，那么在大样本中，我们知道
    \[ \frac{\widehat{\theta}_n - \theta}{\widehat{\se}[\widehat{\theta}_n]} \sim
    \N(0, 1). \] 然后，我们利用我们对标准正态分布的了解来找到 \[ \P\left( -1.96 \leq \frac{\widehat{\theta}_n
    - \theta}{\widehat{\se}[\widehat{\theta}_n]} \leq 1.96\right) = 0.95. \] 将不等式的每一部分乘以
    \(\widehat{\se}[\widehat{\theta}_n]\) 得到 \[ \P\left( -1.96\,\widehat{\se}[\widehat{\theta}_n]
    \leq \widehat{\theta}_n - \theta \leq 1.96\,\widehat{\se}[\widehat{\theta}_n]\right)
    = 0.95, \] 然后，我们将所有部分减去估计量，得到 \[ \P\left(-\widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n]
    \leq - \theta \leq -\widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n]\right)
    = 0.95, \] 最后，我们将所有部分乘以 \(-1\)（并翻转不等式）得到 \[ \P\left(\widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n]
    \leq \theta \leq \widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n]\right)
    = 0.95. \] 要将这个结果与置信区间的定义联系起来，我们现在已经证明了随机区间 \([L, U]\)，其中 \[ \begin{aligned} L
    = L(X_1, \ldots, X_n) &= \widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n]
    \\ U = U(X_1, \ldots, X_n) &= \widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n],
    \end{aligned} \] 是一个渐近有效的估计量。[³](#fn3) 用 \(\Xbar_n\) 替换 \(\widehat{\theta}_n\)，用
    \(s/\sqrt{n}\) 替换 \(\widehat{\se}[\widehat{\theta}_n]\) 建立了上述样本均值的95%标准置信区间是如何渐近有效的。
- en: '![](../Images/179977a3a41e7d7f561dc240bb8797a4.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/179977a3a41e7d7f561dc240bb8797a4.png)'
- en: 'Figure 3.3: Critical values for the standard normal.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：标准正态分布的临界值。
- en: 'How can we generalize this to \(1-\alpha\) confidence intervals? For a random
    variable that is distributed following a standard normal, \(Z\), we know that
    \[ \P(-z_{\alpha/2} \leq Z \leq z_{\alpha/2}) = 1-\alpha \] which implies that
    we can obtain a \(1-\alpha\) asymptotic confidence intervals by using the interval
    \([L, U]\), where \[ L = \widehat{\theta}_{n} - z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}],
    \quad U = \widehat{\theta}_{n} + z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}].
    \] This is sometimes shortened to \(\widehat{\theta}_n \pm z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}]\).
    Remember that we can obtain the values of \(z_{\alpha/2}\) easily from R:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这个方法推广到 \(1-\alpha\) 置信区间？对于一个服从标准正态分布的随机变量 \(Z\)，我们知道 \[ \P(-z_{\alpha/2}
    \leq Z \leq z_{\alpha/2}) = 1-\alpha \] 这意味着我们可以通过使用区间 \([L, U]\)，其中 \[ L = \widehat{\theta}_{n}
    - z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}], \quad U = \widehat{\theta}_{n}
    + z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}], \] 来获得 \(1-\alpha\) 的渐近置信区间。这有时可以简写为
    \(\widehat{\theta}_n \pm z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}]\)。记住，我们可以很容易地从R中获得
    \(z_{\alpha/2}\) 的值：
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*[PRE1]*  *As a concrete example, then, we could derive a 90% asymptotic confidence
    interval for the sample mean as \[ \left[\Xbar_{n} - 1.64 \frac{\widehat{\sigma}}{\sqrt{n}},
    \Xbar_{n} + 1.64 \frac{\widehat{\sigma}}{\sqrt{n}}\right] \]*  *### 3.8.2 Interpreting
    confidence intervals'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE1]*  *作为一个具体的例子，我们可以推导出样本均值的90%渐近置信区间为 \[ \left[\Xbar_{n} - 1.64 \frac{\widehat{\sigma}}{\sqrt{n}},
    \Xbar_{n} + 1.64 \frac{\widehat{\sigma}}{\sqrt{n}}\right] \]*  *### 3.8.2 置信区间的解释'
- en: A very important point is that the interpretation of confidence is how the random
    interval performs over repeated samples. A valid 95% confidence interval is a
    random interval that contains the true population value in 95% of samples. Simulating
    repeated samples helps clarify this.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常重要的观点是，置信度的解释是如何在重复样本中对随机区间进行表现。一个有效的 95% 置信区间是一个随机区间，在 95% 的样本中包含真实总体值。模拟重复样本有助于阐明这一点。
- en: '**Example 3.8** Suppose we are taking samples of size \(n=500\) of random variables
    where \(X_i \sim \N(1, 10)\), and we want to estimate the population mean \(\E[X]
    = 1\). To do so, we repeat the following steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 3.8** 假设我们正在对随机变量进行大小为 \(n=500\) 的抽样，其中 \(X_i \sim \N(1, 10)\)，并且我们想要估计总体均值
    \(\E[X] = 1\)。为此，我们重复以下步骤：'
- en: Draw a sample of \(n=500\) from \(\N(1, 10)\).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 \(\N(1, 10)\) 中抽取一个大小为 \(n=500\) 的样本。
- en: Calculate the 95% confidence interval sample mean \(\Xbar_n \pm 1.96\widehat{\sigma}/\sqrt{n}\).
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算样本均值的 95% 置信区间 \(\Xbar_n \pm 1.96\widehat{\sigma}/\sqrt{n}\)。
- en: Plot the intervals along the x-axis and color them blue if they contain the
    truth (1) and red if not.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 沿着 x 轴绘制区间，如果它们包含真实值（1）则用蓝色着色，如果不包含则用红色着色。
- en: '[Figure 3.4](#fig-ci-sim) shows 100 iterations of these steps. We see that,
    as expected, most calculated CIs do contain the true value. Five random samples
    produce intervals that fail to include 1, an exact coverage rate of 95%. Of course,
    this is just one simulation, and a different set of 100 random samples might have
    produced a slightly different coverage rate. The guarantee of the 95% confidence
    intervals is that if we were to continue to take these repeated samples, the long-run
    frequency of intervals covering the truth would approach 0.95.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.4](#fig-ci-sim) 展示了这些步骤的 100 次迭代。我们看到，正如预期的那样，大多数计算出的置信区间确实包含了真实值。五个随机样本产生的区间未能包含
    1，精确覆盖率为 95%。当然，这只是单一模拟，另一组 100 个随机样本可能会产生略有不同的覆盖率。95% 置信区间的保证是，如果我们继续进行这些重复抽样，覆盖真实值的长期频率将接近
    0.95。'
- en: '![](../Images/720f64f1280b95f6d278106fded36a95.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/720f64f1280b95f6d278106fded36a95.png)'
- en: 'Figure 3.4: 95% confidence intervals from 100 random samples. Intervals are
    blue if they contain the truth and red if they do not.**  **## 3.9 Delta method'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：100 个随机样本的 95% 置信区间。区间为蓝色表示包含真实值，红色表示不包含。**  **## 3.9 Delta 方法
- en: Suppose that we know that an estimator follows the CLT, and so we have \[ \sqrt{n}\left(\widehat{\theta}_n
    - \theta \right) \indist \N(0, V), \] but we actually want to estimate \(h(\theta)\)
    so we use the plug-in estimator, \(h(\widehat{\theta}_n)\). It seems like we should
    be able to apply part 1 of [Theorem 3.9](#thm-indist-properties) to obtain the
    asymptotic distribution of \(h(\widehat{\theta}_n)\). Still, the CLT established
    the large-sample distribution of the centered and scaled random sequence, \(\sqrt{n}(\widehat{\theta}_n
    - \theta)\), not to the original estimator itself, and we would need the latter
    to investigate the asymptotic distribution of \(h(\widehat{\theta}_n)\). We can
    use a little bit of calculus to get an approximation of the distribution we need.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们知道一个估计量遵循中心极限定理（CLT），因此我们有 \[ \sqrt{n}\left(\widehat{\theta}_n - \theta
    \right) \indist \N(0, V), \] 但我们实际上想要估计 \(h(\theta)\)，所以我们使用插值估计量，\(h(\widehat{\theta}_n)\)。看起来我们应该能够应用
    [定理 3.9](#thm-indist-properties) 的第一部分来获得 \(h(\widehat{\theta}_n)\) 的渐近分布。然而，CLT
    建立了中心化和缩放后的随机序列 \(\sqrt{n}(\widehat{\theta}_n - \theta)\) 的大样本分布，而不是原始估计量本身，我们需要后者来研究
    \(h(\widehat{\theta}_n)\) 的渐近分布。我们可以使用一点微积分来得到所需分布的近似。
- en: '**Theorem 3.11** If \(\sqrt{n}\left(\widehat{\theta}_n - \theta\right) \indist
    \N(0, V)\) and \(h(u)\) is continuously differentiable in a neighborhood around
    \(\theta\), then as \(n\to\infty\), \[ \sqrt{n}\left(h(\widehat{\theta}_n) - h(\theta)
    \right) \indist \N(0, (h''(\theta))^2 V). \]'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 3.11** 如果 \(\sqrt{n}\left(\widehat{\theta}_n - \theta\right) \indist \N(0,
    V)\) 并且 \(h(u)\) 在 \(\theta\) 附近的邻域内是连续可微的，那么当 \(n\to\infty\) 时，\[ \sqrt{n}\left(h(\widehat{\theta}_n)
    - h(\theta) \right) \indist \N(0, (h''(\theta))^2 V). \]'
- en: 'Understanding what is happening here provides intuition as to when this might
    go wrong. Why do we focus on continuously differentiable functions, \(h()\)? These
    functions can be well-approximated with a line in a neighborhood around a given
    point like \(\theta\). In [Figure 3.5](#fig-delta), we show this at the point
    where the tangent line at \(\theta_0\), which has slope \(h''(\theta_0)\), is
    very similar to \(h(\theta)\) for values close to \(\theta_0\). Because of this,
    we can approximate the difference between \(h(\widehat{\theta}_n)\) and \(h(\theta_0)\)
    with the what this tangent line would give us: \[ \underbrace{\left(h(\widehat{\theta_n})
    - h(\theta_0)\right)}_{\text{change in } y} \approx \underbrace{h''(\theta_0)}_{\text{slope}}
    \underbrace{\left(\widehat{\theta}_n - \theta_0\right)}_{\text{change in } x},
    \] and then multiplying both sides by the \(\sqrt{n}\) gives \[ \sqrt{n}\left(h(\widehat{\theta_n})
    - h(\theta_0)\right) \approx h''(\theta_0)\sqrt{n}\left(\widehat{\theta}_n - \theta_0\right).
    \] The right-hand side of this approximation converges to \(h''(\theta_0)Z\),
    where \(Z\) is a random variable with \(\N(0, V)\). The variance of this quantity
    will be \[ \V[h''(\theta_0)Z] = (h''(\theta_0))^2\V[Z] = (h''(\theta_0))^2V, \]
    by the properties of variances.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这里发生的事情，可以提供关于何时可能会出错的一些直觉。为什么我们关注连续可微的函数 \(h()\)？这些函数可以在以 \(\theta\) 为中心的某个邻域内用一条直线很好地近似。在[图3.5](#fig-delta)中，我们展示了在切线斜率为
    \(h'(\theta_0)\) 的 \(\theta_0\) 点，这条切线对于接近 \(\theta_0\) 的值与 \(h(\theta)\) 非常相似。正因为如此，我们可以用这条切线给出的结果来近似
    \(h(\widehat{\theta}_n)\) 和 \(h(\theta_0)\) 之间的差异：\[ \underbrace{\left(h(\widehat{\theta_n})
    - h(\theta_0)\right)}_{\text{y的变化}} \approx \underbrace{h'(\theta_0)}_{\text{斜率}}
    \underbrace{\left(\widehat{\theta}_n - \theta_0\right)}_{\text{x的变化}}， \] 然后将两边都乘以
    \(\sqrt{n}\)，得到 \[ \sqrt{n}\left(h(\widehat{\theta_n}) - h(\theta_0)\right) \approx
    h'(\theta_0)\sqrt{n}\left(\widehat{\theta}_n - \theta_0\right). \] 这个近似的右侧收敛到
    \(h'(\theta_0)Z\)，其中 \(Z\) 是一个具有 \(\N(0, V)\) 的随机变量。这个量的方差将是 \[ \V[h'(\theta_0)Z]
    = (h'(\theta_0))^2\V[Z] = (h'(\theta_0))^2V， \] 根据方差的性质。
- en: '![](../Images/5fdafd71b66211a18c3fa50efa84174e.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5fdafd71b66211a18c3fa50efa84174e.png)'
- en: 'Figure 3.5: Linear approximation to nonlinear functions.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：非线性函数的线性近似。
- en: '**Example 3.9** Let’s return to the iid sample \(X_1, \ldots, X_n\) with mean
    \(\mu = \E[X_i]\) and variance \(\sigma^2 = \V[X_i]\). From the CLT, we know that
    \(\sqrt{n}(\Xbar_n - \mu) \indist \N(0, \sigma^2)\). Suppose that we want to estimate
    \(\log(\mu)\), so we use the plug-in estimator \(\log(\Xbar_n)\) (assuming that
    \(X_i > 0\) for all \(i\) so that we can take the log). What is the asymptotic
    distribution of this estimator? This is a situation where \(\widehat{\theta}_n
    = \Xbar_n\) and \(h(\mu) = \log(\mu)\). From basic calculus, we know that \[ h''(\mu)
    = \frac{\partial \log(\mu)}{\partial \mu} = \frac{1}{\mu}, \] so applying the
    delta method, we can determine that \[ \sqrt{n}\left(\log(\Xbar_n) - \log(\mu)\right)
    \indist \N\left(0,\frac{\sigma^2}{\mu^2} \right). \]'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**例3.9** 让我们回到具有均值 \(\mu = \E[X_i]\) 和方差 \(\sigma^2 = \V[X_i]\) 的独立同分布样本 \(X_1,
    \ldots, X_n\)。从中心极限定理（CLT）我们知道 \(\sqrt{n}(\Xbar_n - \mu) \indist \N(0, \sigma^2)\)。假设我们想要估计
    \(\log(\mu)\)，因此我们使用插值估计量 \(\log(\Xbar_n)\)（假设对于所有 \(i\)，\(X_i > 0\)，这样我们可以取对数）。这个估计量的渐近分布是什么？这是一个
    \(\widehat{\theta}_n = \Xbar_n\) 和 \(h(\mu) = \log(\mu)\) 的情况。从基本的微积分知识，我们知道 \[
    h''(\mu) = \frac{\partial \log(\mu)}{\partial \mu} = \frac{1}{\mu}, \] 因此应用delta方法，我们可以确定
    \[ \sqrt{n}\left(\log(\Xbar_n) - \log(\mu)\right) \indist \N\left(0,\frac{\sigma^2}{\mu^2}
    \right). \]'
- en: '**Example 3.10** What about estimating the \(\exp(\mu)\) with \(\exp(\Xbar_n)\)?
    Recall that \[ h''(\mu) = \frac{\partial \exp(\mu)}{\partial \mu} = \exp(\mu)
    \] so applying the delta method, we have \[ \sqrt{n}\left(\exp(\Xbar_n) - \exp(\mu)\right)
    \indist \N(0, \exp(2\mu)\sigma^2), \] since \(\exp(\mu)^2 = \exp(2\mu)\).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**例3.10** 关于用 \(\exp(\Xbar_n)\) 来估计 \(\exp(\mu)\) 呢？回忆一下，\[ h''(\mu) = \frac{\partial
    \exp(\mu)}{\partial \mu} = \exp(\mu) \] 因此应用delta方法，我们得到 \[ \sqrt{n}\left(\exp(\Xbar_n)
    - \exp(\mu)\right) \indist \N(0, \exp(2\mu)\sigma^2), \] 因为 \(\exp(\mu)^2 = \exp(2\mu)\).'
- en: Like all of the results in this chapter, there is a multivariate version of
    the delta method that is incredibly useful in practical applications. For example,
    suppose we want to combine two different estimators (or two different estimated
    parameters) to estimate another quantity. We now let \(\mb{h}(\mb{\theta}) = (h_1(\mb{\theta}),
    \ldots, h_m(\mb{\theta}))\) map from \(\mathbb{R}^k \to \mathbb{R}^m\) and be
    continuously differentiable (we make the function bold since it returns an \(m\)-dimensional
    vector). It will help us to use more compact matrix notation if we introduce a
    \(m \times k\) Jacobian matrix of all partial derivatives \[ \mb{H}(\mb{\theta})
    = \mb{\nabla}_{\mb{\theta}}\mb{h}(\mb{\theta}) = \begin{pmatrix} \frac{\partial
    h_1(\mb{\theta})}{\partial \theta_1} & \frac{\partial h_1(\mb{\theta})}{\partial
    \theta_2} & \cdots & \frac{\partial h_1(\mb{\theta})}{\partial \theta_k} \\ \frac{\partial
    h_2(\mb{\theta})}{\partial \theta_1} & \frac{\partial h_2(\mb{\theta})}{\partial
    \theta_2} & \cdots & \frac{\partial h_2(\mb{\theta})}{\partial \theta_k} \\ \vdots
    & \vdots & \ddots & \vdots \\ \frac{\partial h_m(\mb{\theta})}{\partial \theta_1}
    & \frac{\partial h_m(\mb{\theta})}{\partial \theta_2} & \cdots & \frac{\partial
    h_m(\mb{\theta})}{\partial \theta_k} \end{pmatrix}, \] which we can use to generate
    the equivalent multivariate linear approximation \[ \left(\mb{h}(\widehat{\mb{\theta}}_n)
    - \mb{h}(\mb{\theta}_0)\right) \approx \mb{H}(\mb{\theta}_0)'\left(\widehat{\mb{\theta}}_n
    - \mb{\theta}_0\right). \] We can use this fact to derive the multivariate delta
    method.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章中的所有结果一样，Delta方法有一个多变量版本，在实用应用中非常有用。例如，假设我们想要结合两个不同的估计量（或两个不同的估计参数）来估计另一个量。现在我们让\(\mb{h}(\mb{\theta})
    = (h_1(\mb{\theta}), \ldots, h_m(\mb{\theta}))\)从\(\mathbb{R}^k \to \mathbb{R}^m\)映射，并且是连续可微的（我们使函数加粗，因为它返回一个\(m\)-维向量）。如果我们引入所有偏导数的\(m
    \times k\)雅可比矩阵，这将有助于我们使用更紧凑的矩阵符号：\[ \mb{H}(\mb{\theta}) = \mb{\nabla}_{\mb{\theta}}\mb{h}(\mb{\theta})
    = \begin{pmatrix} \frac{\partial h_1(\mb{\theta})}{\partial \theta_1} & \frac{\partial
    h_1(\mb{\theta})}{\partial \theta_2} & \cdots & \frac{\partial h_1(\mb{\theta})}{\partial
    \theta_k} \\ \frac{\partial h_2(\mb{\theta})}{\partial \theta_1} & \frac{\partial
    h_2(\mb{\theta})}{\partial \theta_2} & \cdots & \frac{\partial h_2(\mb{\theta})}{\partial
    \theta_k} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial h_m(\mb{\theta})}{\partial
    \theta_1} & \frac{\partial h_m(\mb{\theta})}{\partial \theta_2} & \cdots & \frac{\partial
    h_m(\mb{\theta})}{\partial \theta_k} \end{pmatrix}, \] 我们可以使用它来生成等效的多变量线性近似\[
    \left(\mb{h}(\widehat{\mb{\theta}}_n) - \mb{h}(\mb{\theta}_0)\right) \approx \mb{H}(\mb{\theta}_0)'\left(\widehat{\mb{\theta}}_n
    - \mb{\theta}_0\right). \] 我们可以使用这个事实来推导多变量Delta方法。
- en: '**Theorem 3.12** Suppose that \(\sqrt{n}\left(\widehat{\mb{\theta}}_n - \mb{\theta}_0
    \right) \indist \N(0, \mb{\Sigma})\), then for any function \(\mb{h}\) that is
    continuously differentiable in a neighborhood of \(\mb{\theta}_0\), we have \[
    \sqrt{n}\left(\mb{h}(\widehat{\mb{\theta}}_n) - \mb{h}(\mb{\theta}_0) \right)
    \indist \N(0, \mb{H}\mb{\Sigma}\mb{H}''), \] where \(\mb{H} = \mb{H}(\mb{\theta}_0)\).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理3.12** 假设\(\sqrt{n}\left(\widehat{\mb{\theta}}_n - \mb{\theta}_0 \right)
    \indist \N(0, \mb{\Sigma})\)，那么对于在\(\mb{\theta}_0\)邻域内连续可微的任何函数\(\mb{h}\)，我们有\[
    \sqrt{n}\left(\mb{h}(\widehat{\mb{\theta}}_n) - \mb{h}(\mb{\theta}_0) \right)
    \indist \N(0, \mb{H}\mb{\Sigma}\mb{H}''), \] 其中\(\mb{H} = \mb{H}(\mb{\theta}_0)\)。'
- en: This result follows from the approximation above plus rules about variances
    of random vectors. Recall that for any compatible matrix of constants, \(\mb{A}\),
    we have \(\V[\mb{A}'\mb{Z}] = \mb{A}\V[\mb{Z}]\mb{A}'\). The matrix of constants
    appears twice here, like the matrix version of the “squaring the constant” rule
    for variance.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此结果源于上述近似以及关于随机向量方差的规则。回想一下，对于任何兼容的常数矩阵\(\mb{A}\)，我们有\(\V[\mb{A}'\mb{Z}] = \mb{A}\V[\mb{Z}]\mb{A}'\)。常数矩阵在这里出现了两次，就像方差中“平方常数”规则的矩阵版本。
- en: The delta method is handy for generating closed-form approximations for asymptotic
    standard errors, but the math is often quite complex for even simple estimators.
    It is usually more straightforward for applied researchers to use computational
    tools such as the bootstrap to approximate the needed standard errors. The bootstrap
    has the trade-off of taking more computational time to implement compared to the
    delta method, but it is more easily adaptable across different estimators and
    domains.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Delta方法对于生成渐近标准误差的封闭形式近似很有用，但对于即使是简单估计量，数学通常也非常复杂。对于应用研究者来说，通常更直接的方法是使用计算工具，如自助法来近似所需的标准误差。与Delta方法相比，自助法在实现时需要更多的计算时间，但它更容易适应不同的估计量和领域。
- en: 3.10 Summary
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.10 摘要
- en: In this chapter, we covered asymptotic analysis, which considers how estimators
    behave as we feed them larger and larger samples. While we never actually have
    infinite data, asymptotic results provide approximations that work quite well
    in practice. A **consistent** estimator converges in probability to a desired
    quantity of interest. We saw several ways of establishing consistency, including
    the **Law of Large Numbers** for the sample mean, which converges in probability
    to the population mean. The **Central Limit Theorem** tells us that the sample
    mean will be approximately normally distributed when we have large, iid samples.
    We also saw how the **continuous mapping theorem** and **Slutsky’s theorem** allow
    us to determine asymptotic results for a broad class of estimators. Knowing the
    asymptotic normality of an estimator allows us to derive **confidence intervals**
    that are valid in large samples. Finally, the **delta method** is a general tool
    for finding the asymptotic distribution of an estimator that is a function of
    another estimator with a known asymptotic distribution.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了渐近分析，它考虑了当我们向估计量提供越来越大样本时，估计量的行为。虽然我们实际上从未拥有无限的数据，但渐近结果提供了在实践中相当有效的近似。一个**一致**的估计量将以概率收敛到我们感兴趣的某个量。我们看到了几种建立一致性的方法，包括样本均值的**大数定律**，它以概率收敛到总体均值。**中心极限定理**告诉我们，当我们有大量独立同分布的样本时，样本均值将近似正态分布。我们还看到了**连续映射定理**和**斯卢茨基定理**如何使我们能够确定一类广泛的估计量的渐近结果。了解估计量的渐近正态性使我们能够推导出在大样本中有效的**置信区间**。最后，**delta方法**是寻找作为具有已知渐近分布的另一个估计量函数的估计量的渐近分布的一般工具。
- en: 'In the next chapter, we will leverage these asymptotic results to introduce
    another important tool for statistical inference: the hypothesis test.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将利用这些渐近结果来介绍统计推断的另一个重要工具：假设检验。
- en: '* * *'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Due to Wasserman (2004), Chapter 5.[↩︎](#fnref1)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于Wasserman（2004），第五章。[↩︎](#fnref1)
- en: Technically, a sequence can also converge in probability to another random variable,
    but the use case of converging to a single number is much more common in evaluating
    estimators.[↩︎](#fnref2)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 技术上，一个序列也可以以概率收敛到另一个随机变量，但将收敛到单个数的用例在评估估计量时更为常见。
- en: Implicit in this analysis is that the standard error estimate is consistent.[↩︎](#fnref3)********
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个分析中隐含的是标准误差估计是一致的。[↩︎](#fnref3)********
