- en: '6.2\. Background: introduction to parametric families and maximum likelihood
    estimation#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.2\. 背景：参数族和最大似然估计介绍#
- en: 原文：[https://mmids-textbook.github.io/chap06_prob/02_parametric/roch-mmids-prob-parametric.html](https://mmids-textbook.github.io/chap06_prob/02_parametric/roch-mmids-prob-parametric.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap06_prob/02_parametric/roch-mmids-prob-parametric.html](https://mmids-textbook.github.io/chap06_prob/02_parametric/roch-mmids-prob-parametric.html)
- en: In this section, we introduce some fundamental concepts used to construct probabilistic
    models for statistical purposes. We also define a common family of distributions,
    the exponential family.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些用于构建统计模型的基本概念。我们还定义了一个常见的分布族，即指数族。
- en: Throughout this topic, all formal proofs are done under the assumption of a
    discrete distribution with finite support to avoid unnecessary technicalities
    and focus on the concepts. But everything we discuss can be adapted to continuous
    distributions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本主题的整个讨论中，所有形式证明都是在有限支撑的离散分布的假设下进行的，以避免不必要的复杂性并专注于概念。但我们讨论的每件事都可以适应连续分布。
- en: Parametric families\(\idx{parametric family}\xdi\) of probability distributions
    serve as basic building blocks for more complex models. By a parametric family,
    we mean a collection \(\{p_{\btheta}:\btheta \in \Theta\}\), where \(p_{\btheta}\)
    is a probability distribution over a set \(\S_{\btheta}\) and \(\btheta\) is a
    vector-valued parameter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布的参数族\(\idx{parametric family}\xdi\) 是更复杂模型的基本构建块。参数族是指一个集合 \(\{p_{\btheta}:\btheta
    \in \Theta\}\)，其中 \(p_{\btheta}\) 是在集合 \(\S_{\btheta}\) 上的概率分布，\(\btheta\) 是一个向量值参数。
- en: '**EXAMPLE:** **(Bernoulli)** The random variable \(X\) is Bernoulli\(\idx{Bernoulli}\xdi\)
    with parameter \(q \in [0,1]\), denoted \(X \sim \mathrm{Ber}(q)\), if it takes
    values in \(\S_X = \{0,1\}\) and \(\P[X=1] = q\). Varying \(q\) produces the family
    of Bernoulli distributions. \(\lhd\)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(伯努利)** 随机变量 \(X\) 是参数 \(q \in [0,1]\) 的伯努利分布 \(\idx{Bernoulli}\xdi\)，记作
    \(X \sim \mathrm{Ber}(q)\)，如果它在 \(\S_X = \{0,1\}\) 中取值，且 \(\P[X=1] = q\)。改变 \(q\)
    会产生伯努利分布族。 \(\lhd\)'
- en: Here we focus on exponential families, which include many common distributions
    (including the Bernoulli distribution).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们专注于指数族，它包括许多常见的分布（包括伯努利分布）。
- en: 6.2.1\. Exponential family[#](#exponential-family "Link to this heading")
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.1\. 指数族[#](#exponential-family "链接到这个标题")
- en: One particularly useful class of probability distributions in data science is
    the [exponential family](https://en.wikipedia.org/wiki/Exponential_family#Vector_parameter),
    which includes many well-known cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，一个特别有用的概率分布类是 [指数族](https://en.wikipedia.org/wiki/Exponential_family#Vector_parameter)，它包括许多已知的案例。
- en: '**DEFINITION** **(Exponential Family - Discrete Case)** \(\idx{exponential
    family}\xdi\) A parametric collection of probability distributions \(\{p_{\btheta}:\btheta
    \in \Theta\}\) over a discrete space \(\S\) is an exponential family if it can
    be written in the form'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**DEFINITION** **(指数族 - 离散情况)** \(\idx{exponential family}\xdi\) 一个参数化的概率分布集合
    \(\{p_{\btheta}:\btheta \in \Theta\}\) 在离散空间 \(\S\) 上是指数族，如果它可以写成以下形式'
- en: \[ p_{\btheta}(\mathbf{x}) = \frac{1}{Z(\btheta)} h(\mathbf{x}) \exp\left(\btheta^T
    \bphi(\mathbf{x})\right) \]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = \frac{1}{Z(\btheta)} h(\mathbf{x}) \exp\left(\btheta^T
    \bphi(\mathbf{x})\right) \]
- en: 'where \(\btheta \in \mathbb{R}^m\) are the canonical parameters, \(\bphi :
    \S \to \mathbb{R}^m\) are the sufficient statistics and \(Z(\btheta)\) is the
    partition function\(\idx{partition function}\xdi\). It is often convenient to
    introduce the log-partition function\(\idx{log-partition function}\xdi\) \(A(\btheta)
    = \log Z(\btheta)\) and re-write'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 \(\btheta \in \mathbb{R}^m\) 是典型参数，\(\bphi : \S \to \mathbb{R}^m\) 是充分统计量，\(Z(\btheta)\)
    是配分函数\(\idx{partition function}\xdi\)。引入对数配分函数\(\idx{log-partition function}\xdi\)
    \(A(\btheta) = \log Z(\btheta)\) 并重新写成'
- en: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right). \]
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right). \]
- en: \(\natural\)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: '**EXAMPLE:** **(Bernoulli, continued)** For \(x \in \{0,1\}\), the \(\mathrm{Ber}(q)\)
    distribution for \(0 < q < 1\) can be written as'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(伯努利，继续)** 对于 \(x \in \{0,1\}\)，当 \(0 < q < 1\) 时，\(\mathrm{Ber}(q)\)
    分布可以写成'
- en: \[\begin{align*} q^{x} (1-q)^{1-x} &= (1-q) \left(\frac{q}{1-q}\right)^x\\ &=
    (1-q) \exp\left[x \log \left(\frac{q}{1-q}\right)\right]\\ &= \frac{1}{Z(\theta)}
    h(x) \exp(\theta \,\phi(x)) \end{align*}\]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} q^{x} (1-q)^{1-x} &= (1-q) \left(\frac{q}{1-q}\right)^x\\ &=
    (1-q) \exp\left[x \log \left(\frac{q}{1-q}\right)\right]\\ &= \frac{1}{Z(\theta)}
    h(x) \exp(\theta \,\phi(x)) \end{align*}\]
- en: where we define \(h(x) \equiv 1\), \(\phi(x) = x\), \(\theta = \log \left(\frac{q}{1-q}\right)\)
    and, since \(Z(\theta)\) serves as the normalization constant in \(p_\theta\),
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(h(x) \equiv 1\)，\(\phi(x) = x\)，\(\theta = \log \left(\frac{q}{1-q}\right)\)，并且由于
    \(Z(\theta)\) 在 \(p_\theta\) 中作为归一化常数，
- en: \[ Z(\theta) = \sum_{x \in \{0,1\}} h(x) \exp(\theta \,\phi(x)) = 1 + e^\theta.
    \]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z(\theta) = \sum_{x \in \{0,1\}} h(x) \exp(\theta \,\phi(x)) = 1 + e^\theta.
    \]
- en: \(\lhd\)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: The following is an important generalization. Recall that i.i.d. is the abbreviation
    for independent and identically distributed. We use the convention \(0! = 1\).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个重要的推广。回忆一下，i.i.d. 是独立同分布的缩写。我们使用约定 \(0! = 1\)。
- en: '**EXAMPLE:** **(Categorical and Multinomial)** A categorical variable\(\idx{categorical}\xdi\)
    \(\mathbf{Y}\) takes \(K \geq 2\) possible values. A standard choice is to use
    one-hot encoding\(\idx{one-hot encoding}\xdi\) \(\S = \{\mathbf{e}_i : i=1,\ldots,K\}\)
    where \(\mathbf{e}_i\) is the \(i\)-th canonical basis in \(\mathbb{R}^K\). The
    distribution is specified by setting the probabilities \(\bpi = (\pi_1,\ldots,\pi_K)\)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(分类和多项式)** 一个分类变量\(\idx{categorical}\xdi\) \(\mathbf{Y}\) 取
    \(K \geq 2\) 个可能值。一个标准的选择是使用独热编码\(\idx{one-hot encoding}\xdi\) \(\S = \{\mathbf{e}_i
    : i=1,\ldots,K\}\)，其中 \(\mathbf{e}_i\) 是 \(\mathbb{R}^K\) 中的第 \(i\) 个标准基。分布由设置概率
    \(\bpi = (\pi_1,\ldots,\pi_K)\) 确定'
- en: \[ \pi_i = \P[\mathbf{Y} = \mathbf{e}_i]. \]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \pi_i = \P[\mathbf{Y} = \mathbf{e}_i]. \]
- en: We denote this \(\mathbf{Y} \sim \mathrm{Cat}(\bpi)\) and we assume \(\pi_i
    > 0\) for all \(i\).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此记为 \(\mathbf{Y} \sim \mathrm{Cat}(\bpi)\)，并假设对于所有 \(i\)，\(\pi_i > 0\)。
- en: To see that this is an exponential family, write the probability mass function
    at \(\mathbf{x} = (x_1,\ldots,x_K)\) as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这是一个指数族，我们将概率质量函数在 \(\mathbf{x} = (x_1,\ldots,x_K)\) 处写为
- en: \[ \prod_{i=1}^K \pi_i^{x_i} = \exp\left(\sum_{i=1}^K x_i \log \pi_i \right).
    \]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \prod_{i=1}^K \pi_i^{x_i} = \exp\left(\sum_{i=1}^K x_i \log \pi_i \right).
    \]
- en: So we can take \(h(\mathbf{x}) \equiv 1\), \(\btheta = (\log \pi_i)_{i=1}^K\),
    \(\bphi(\mathbf{x}) = (x_i)_{i=1}^K\) and \(Z(\btheta) \equiv 1\).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以取 \(h(\mathbf{x}) \equiv 1\)，\(\btheta = (\log \pi_i)_{i=1}^K\)，\(\bphi(\mathbf{x})
    = (x_i)_{i=1}^K\)，且 \(Z(\btheta) \equiv 1\)。如果我们认为 \(n\) 是固定的，那么这是一个指数族。
- en: The [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution)\(\idx{multinomial}\xdi\)
    arises as a sum of independent categorical variables. Let \(n \geq 1\) be the
    number of trials (or samples) and let \(\mathbf{Y}_1,\ldots,\mathbf{Y}_n\) be
    i.i.d. \(\mathrm{Cat}(\bpi)\). Define \(\mathbf{X} = \sum_{i=1}^n \mathbf{Y}_i\).
    The probability mass function of \(\mathbf{X}\) at
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[多项式分布](https://en.wikipedia.org/wiki/Multinomial_distribution)\(\idx{multinomial}\xdi\)
    作为独立分类变量的和出现。设 \(n \geq 1\) 为试验次数（或样本数），设 \(\mathbf{Y}_1,\ldots,\mathbf{Y}_n\)
    是独立同分布的 \(\mathrm{Cat}(\bpi)\)。定义 \(\mathbf{X} = \sum_{i=1}^n \mathbf{Y}_i\)。在
    \(\mathbf{X}\) 处的概率质量函数为'
- en: '\[ \mathbf{x} = (x_1,\ldots,x_K) \in \left\{ \mathbf{x} \in \{0,1,\ldots,n\}^K
    : \sum_{i=1}^K x_i = n \right\}=: \S \]'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \mathbf{x} = (x_1,\ldots,x_K) \in \left\{ \mathbf{x} \in \{0,1,\ldots,n\}^K
    : \sum_{i=1}^K x_i = n \right\}=: \S \]'
- en: is
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: is
- en: \[ \frac{n!}{x_1!\cdots x_K!} \prod_{i=1}^K \pi_i^{x_i} = \frac{n!}{x_1!\cdots
    x_K!} \exp\left(\sum_{i=1}^K x_i \log \pi_i\right) \]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{n!}{x_1!\cdots x_K!} \prod_{i=1}^K \pi_i^{x_i} = \frac{n!}{x_1!\cdots
    x_K!} \exp\left(\sum_{i=1}^K x_i \log \pi_i\right) \]
- en: and we can take \(h(\mathbf{x}) = \frac{n!}{x_1!\cdots x_K!}\), \(\btheta =
    (\log \pi_i)_{i=1}^K\), \(\bphi(\mathbf{x}) = (x_i)_{i=1}^K\) and \(Z(\btheta)
    \equiv 1\). This is an exponential family if we think of \(n\) as fixed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以取 \(h(\mathbf{x}) = \frac{n!}{x_1!\cdots x_K!}\)，\(\btheta = (\log \pi_i)_{i=1}^K\)，\(\bphi(\mathbf{x})
    = (x_i)_{i=1}^K\)，且 \(Z(\btheta) \equiv 1\)。
- en: We use the notation \(\mathbf{X} \sim \mathrm{Mult}(n, \bpi)\). \(\lhd\)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用符号 \(\mathbf{X} \sim \mathrm{Mult}(n, \bpi)\)。\(\lhd\)
- en: While we have focused so far on discrete distributions, one can adapt the definitions
    above by replacing mass functions with density functions. We give two important
    examples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们迄今为止一直关注离散分布，但可以通过用密度函数替换质量函数来调整上述定义。我们给出两个重要的例子。
- en: We need some definitions for our first example.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为我们的第一个例子定义一些概念。
- en: 'The [trace](https://en.wikipedia.org/wiki/Trace_%28linear_algebra%29)\(\idx{trace}\xdi\)
    of a square matrix \(A \in \mathbb{R}^{d \times d}\), denoted \(\mathrm{tr}(A)\),
    is the sum of its diagonal entries. We will need the following trace identity
    whose proof we leave as an exercise: \(\mathrm{tr}(ABC) = \mathrm{tr}(CAB) = \mathrm{tr}(BCA)\)
    for any matrices \(A, B, C\) for which \(AB\), \(BC\) and \(CA\) are well-defined.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(A \in \mathbb{R}^{d \times d}\) 的迹 \(\idx{trace}\xdi\)，记作 \(\mathrm{tr}(A)\)，是其对角线元素的和。我们将需要以下迹恒等式，其证明留作练习：\(\mathrm{tr}(ABC)
    = \mathrm{tr}(CAB) = \mathrm{tr}(BCA)\) 对于任何 \(A, B, C\) 矩阵，其中 \(AB\)，\(BC\) 和
    \(CA\) 都是定义良好的。
- en: The [determinant](https://en.wikipedia.org/wiki/Determinant)\(\idx{determinant}\xdi\)
    of a square matrix \(A\) is denoted by \(|A|\). For our purposes, it will be enough
    to consider symmetric, positive semidefinite matrices for which the determinant
    is the product of the eigenvalues (with repeats). Recall that we proved that the
    sequence of eigenvalues (with repeats) of a symmetric matrix is unique (in the
    sense that any two spectral decomposition have the same sequence of eigenvalues).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 方阵 \(A\) 的 [行列式](https://en.wikipedia.org/wiki/Determinant)\(\idx{行列式}\xdi\)
    表示为 \(|A|\)。对于我们的目的，考虑对称、正半定矩阵就足够了，其行列式是特征值（包括重复）的乘积。回忆一下，我们证明了对称矩阵的特征值（包括重复）序列是唯一的（在意义上，任何两个谱分解都有相同的特征值序列）。
- en: A symmetric, positive definite matrix \(A \in \mathbb{R}^{d \times d}\) is necessarily
    invertible. Indeed, it has a spectral decomposition
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对称、正定矩阵 \(A \in \mathbb{R}^{d \times d}\) 必然是可逆的。实际上，它有一个谱分解
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
- en: where \(\lambda_1 \geq \cdots \geq \lambda_d > 0\) and \(\mathbf{q}_1, \ldots,
    \mathbf{q}_d\) are orthonormal. Then
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\lambda_1 \geq \cdots \geq \lambda_d > 0\) 且 \(\mathbf{q}_1, \ldots, \mathbf{q}_d\)
    是正交归一。然后
- en: \[ A^{-1} = Q \Lambda^{-1} Q^T. \]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = Q \Lambda^{-1} Q^T. \]
- en: To see this, note that
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这一点，请注意
- en: \[ A A^{-1} = Q \Lambda Q^T Q \Lambda^{-1} Q^T = Q Q^T = I_{d \times d}. \]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A A^{-1} = Q \Lambda Q^T Q \Lambda^{-1} Q^T = Q Q^T = I_{d \times d}. \]
- en: The last equality follows from the fact that \(Q Q^T\) is the orthogonal projection
    on the orthonormal basis \(\mathbf{q}_1,\ldots,\mathbf{q}_d\). Similarly, \(A^{-1}
    A = I_{d \times d}\).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的等式来自于 \(Q Q^T\) 是在正交归一基 \(\mathbf{q}_1,\ldots,\mathbf{q}_d\) 上的正交投影的事实。同样，\(A^{-1}
    A = I_{d \times d}\).
- en: '**EXAMPLE:** **(Multivariate Gaussian)** \(\idx{multivariate normal}\xdi\)
    A multivariate Gaussian\(\idx{multivariate Gaussian}\xdi\) vector \(\mathbf{X}
    = (X_1,\ldots,X_d)\) on \(\mathbb{R}^d\) with mean \(\bmu \in \mathbb{R}^d\) and
    positive definite covariance matrix \(\bSigma \in \mathbb{R}^{d \times d}\) has
    probability density function'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(多元高斯)** \(\idx{多元正态}\xdi\) 在 \(\mathbb{R}^d\) 上的多元高斯\(\idx{多元高斯}\xdi\)向量
    \(\mathbf{X} = (X_1,\ldots,X_d)\) 具有均值 \(\bmu \in \mathbb{R}^d\) 和正定协方差矩阵 \(\bSigma
    \in \mathbb{R}^{d \times d}\)，其概率密度函数'
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right).
    \]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right).
    \]
- en: We use the notation \(\mathbf{X} \sim N_d(\bmu, \bSigma)\).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用符号 \(\mathbf{X} \sim N_d(\bmu, \bSigma)\).
- en: It can be shown that indeed the mean is
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，均值确实是
- en: \[ \E[\mathbf{X}] = \bmu \]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\mathbf{X}] = \bmu \]
- en: and the covariance matrix is
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 并且协方差矩阵是
- en: \[ \E[(\mathbf{X} - \bmu)(\mathbf{X} - \bmu)^T] = \E[\mathbf{X} \mathbf{X}^T]
    - \bmu \bmu^T = \bSigma. \]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[(\mathbf{X} - \bmu)(\mathbf{X} - \bmu)^T] = \E[\mathbf{X} \mathbf{X}^T]
    - \bmu \bmu^T = \bSigma. \]
- en: In the bivariate\(\idx{bivariate Gaussian}\xdi\) case (i.e., when \(d = 2\))\(\idx{bivariate
    normal}\xdi\), the covariance matrix reduces to
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在双变量\(\idx{双变量高斯}\xdi\)情况（即当 \(d = 2\)）\(\idx{双变量正态}\xdi\)中，协方差矩阵简化为
- en: \[\begin{split} \bSigma = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2
    \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bSigma = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2
    \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
- en: where \(\sigma_1^2\) and \(\sigma_2^2\) are the respective variances of \(X_1\)
    and \(X_2\), and
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_1^2\) 和 \(\sigma_2^2\) 分别是 \(X_1\) 和 \(X_2\) 的方差，并且
- en: \[ \rho = \frac{\mathrm{Cov}[X_1,X_2]}{\sigma_1 \sigma_2} \]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \rho = \frac{\mathrm{Cov}[X_1,X_2]}{\sigma_1 \sigma_2} \]
- en: is the correlation coefficient. Recall that, by the *Cauchy-Schwarz inequality*,
    it lies in \([-1,1]\).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 是相关系数。回忆一下，根据 *柯西-施瓦茨不等式*，它位于 \([-1,1]\) 之间。
- en: Rewriting the density as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将密度重新写为
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{e^{-(1/2) \bmu^T \bSigma^{-1} \bmu}}{(2\pi)^{d/2}
    \,|\bSigma|^{1/2}} \exp\left(- \mathbf{x}^T \bSigma^{-1}\bmu - \frac{1}{2} \mathrm{tr}\left(\mathbf{x}
    \mathbf{x}^T \bSigma^{-1}\right)\right) \]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{e^{-(1/2) \bmu^T \bSigma^{-1} \bmu}}{(2\pi)^{d/2}
    \,|\bSigma|^{1/2}} \exp\left(- \mathbf{x}^T \bSigma^{-1}\bmu - \frac{1}{2} \mathrm{tr}\left(\mathbf{x}
    \mathbf{x}^T \bSigma^{-1}\right)\right) \]
- en: where we used the symmetric nature of \(\bSigma^{-1}\) in the first term of
    the exponential and the previous trace identity in the second term. The expression
    in parentheses is linear in the entries of \(\mathbf{x}\) and \(\mathbf{x} \mathbf{x}^T\),
    which can then be taken as sufficient statistics (formally, using [vectorization](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29)).
    Indeed note that
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们在指数的第一项中使用了 \(\bSigma^{-1}\) 的对称性，在第二项中使用了之前的迹恒等式。括号中的表达式在 \(\mathbf{x}\)
    和 \(\mathbf{x} \mathbf{x}^T\) 的条目上是线性的，然后可以作为充分统计量（形式上，使用[向量化](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29)）。确实请注意
- en: \[ \mathbf{x}^T \bSigma^{-1}\bmu = \sum_{i=1}^d x_i (\bSigma^{-1}\bmu)_i \]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^T \bSigma^{-1}\bmu = \sum_{i=1}^d x_i (\bSigma^{-1}\bmu)_i \]
- en: and
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathrm{tr}\left(\mathbf{x} \mathbf{x}^T \bSigma^{-1}\right) = \sum_{i =
    1}^d \left(\sum_{j=1}^d (\mathbf{x} \mathbf{x}^T)_{i,j} (\bSigma^{-1})_{j,i}\right)
    = \sum_{i = 1}^d \sum_{j=1}^d x_i x_j (\bSigma^{-1})_{j,i}. \]
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{tr}\left(\mathbf{x} \mathbf{x}^T \bSigma^{-1}\right) = \sum_{i =
    1}^d \left(\sum_{j=1}^d (\mathbf{x} \mathbf{x}^T)_{i,j} (\bSigma^{-1})_{j,i}\right)
    = \sum_{i = 1}^d \sum_{j=1}^d x_i x_j (\bSigma^{-1})_{j,i}. \]
- en: So we can take
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以取
- en: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]\[\begin{align*} \btheta &= \bigg(-(\bSigma^{-1}\bmu)_1,\ldots,-(\bSigma^{-1}\bmu)_d,\\
    &\qquad - \frac{1}{2}(\bSigma^{-1})_{1,1}, \ldots, - \frac{1}{2}(\bSigma^{-1})_{1,d},\\
    &\qquad - \frac{1}{2}(\bSigma^{-1})_{2,1}, \ldots, - \frac{1}{2}(\bSigma^{-1})_{2,d},\\
    &\qquad \ldots, - \frac{1}{2}(\bSigma^{-1})_{d,1}, \ldots,- \frac{1}{2}(\bSigma^{-1})_{d,d}\bigg)
    \end{align*}\]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]
- en: and \(h (\mathbf{x}) \equiv 1\). Expressing \(Z(\btheta)\) explicitly is not
    straightforward. But note that \(\btheta\) includes all entries of \(\bSigma^{-1}\),
    from which \(\bSigma\) can be computed (e.g., from [Cramer’s rule](https://en.wikipedia.org/wiki/Cramer%27s_rule#Finding_inverse_matrix)),
    and in turn from which \(\bmu\) can be extracted out of the entries of \(\bSigma^{-1}\bmu\)
    in \(\btheta\). So the normalizing factor \(\frac{(2\pi)^{d/2} \,|\bSigma|^{1/2}}{e^{-(1/2)
    \bmu^T \bSigma^{-1} \bmu}}\) can in principle be expressed in terms of \(\btheta\).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 和 \(h (\mathbf{x}) \equiv 1\). 显式地表达 \(Z(\btheta)\) 并不简单。但请注意，\(\btheta\) 包含
    \(\bSigma^{-1}\) 的所有条目，从中可以计算 \(\bSigma\)（例如，从[Cramer的规则](https://en.wikipedia.org/wiki/Cramer%27s_rule#Finding_inverse_matrix)），进而从
    \(\bSigma^{-1}\bmu\) 的条目中提取出 \(\bmu\)。因此，归一化因子 \(\frac{(2\pi)^{d/2} \,|\bSigma|^{1/2}}{e^{-(1/2)
    \bmu^T \bSigma^{-1} \bmu}}\) 在原则上可以用 \(\btheta\) 来表达。
- en: This shows that the multivariate normal is an exponential family.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明多元正态分布是一个指数族。
- en: The matrix \(\bLambda = \bSigma^{-1}\) is also known as the precision matrix.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(\bLambda = \bSigma^{-1}\) 也被称为精度矩阵。
- en: Alternatively, let \(\mathbf{Z}\) be a standard normal \(d\)-vector, let \(\bmu
    \in \mathbb{R}^d\) and let \(\bSigma \in \mathbb{R}^{d \times d}\) be positive
    definite. Then the transformed random variable \(\mathbf{X} = \bmu + \bSigma \mathbf{Z}\)
    is a multivariate Gaussian with mean \(\bmu\) and covariance matrix \(\bSigma\).
    This can be proved using the [change of variables formula](https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function)
    (try it!). \(\lhd\)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，设 \(\mathbf{Z}\) 为标准正态 \(d\)-向量，设 \(\bmu \in \mathbb{R}^d\)，设 \(\bSigma \in
    \mathbb{R}^{d \times d}\) 为正定矩阵。那么变换后的随机变量 \(\mathbf{X} = \bmu + \bSigma \mathbf{Z}\)
    是具有均值 \(\bmu\) 和协方差矩阵 \(\bSigma\) 的多元高斯分布。这可以使用[变量变换公式](https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function)来证明（试试看！）。\(\lhd\)
- en: '**NUMERICAL CORNER:** The following code, which plots the density in the bivariate
    case, was adapted from [gauss_plot_2d.ipynb](https://github.com/probml/pyprobml/blob/master/notebooks/book1/03/gauss_plot_2d.ipynb)
    by ChatGPT.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 下面的代码，用于绘制二元情况下的密度图，由ChatGPT从[gauss_plot_2d.ipynb](https://github.com/probml/pyprobml/blob/master/notebooks/book1/03/gauss_plot_2d.ipynb)改编而来。'
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain the code! Try different
    covariance matrices. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 请你的喜欢的AI聊天机器人解释这段代码！尝试不同的协方差矩阵。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We plot the density for mean \((0,0)\) with two different covariance matrices:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了均值为 \((0,0)\) 的密度图，使用了两个不同的协方差矩阵：
- en: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{and} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1
    \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{and} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1
    \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
- en: where \(\sigma_1 = 1.5\), \(\sigma_2 = 0.5\) and \(\rho = -0.75\).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_1 = 1.5\), \(\sigma_2 = 0.5\) 和 \(\rho = -0.75\)。
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE1]</details> ![../../_images/aa8a076cd17782940f0aa5ed62929716a9ad484853e0e195abb9808dc5430649.png](../Images/bd4d757f39ffc955469d96c6ccc28859.png)<details
    class="hide above-input"><summary aria-label="Toggle hidden content">Show code
    cell source Hide code cell source</summary>'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</details> ![../../_images/aa8a076cd17782940f0aa5ed62929716a9ad484853e0e195abb9808dc5430649.png](../Images/bd4d757f39ffc955469d96c6ccc28859.png)<details
    class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>'
- en: '[PRE2]</details> ![../../_images/4a907efb3af995445fe3feb6f564880bfd1d84078a5876676a3758260bc699c8.png](../Images/38909f679dda3fe6c8703a40390160ef.png)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</details> ![../../_images/4a907efb3af995445fe3feb6f564880bfd1d84078a5876676a3758260bc699c8.png](../Images/38909f679dda3fe6c8703a40390160ef.png)'
- en: \(\unlhd\)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: The [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution),
    which we describe next, is a natural probability distribution over probability
    distributions. In particular, it is used in [Bayesian data analysis](https://en.wikipedia.org/wiki/Bayesian_statistics)
    as a [prior](https://en.wikipedia.org/wiki/Prior_probability) on the parameters
    of categorical and multinomial distribution, largely because of a property known
    as [conjuguacy](https://en.wikipedia.org/wiki/Conjugate_prior). We will not describe
    Bayesian approaches here.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要描述的[Dirichlet分布](https://en.wikipedia.org/wiki/Dirichlet_distribution)，是一种自然概率分布，它覆盖了概率分布。特别是，它在[贝叶斯数据分析](https://en.wikipedia.org/wiki/Bayesian_statistics)中作为分类和多项式分布参数的[先验](https://en.wikipedia.org/wiki/Prior_probability)，很大程度上是因为一个称为[共轭先验](https://en.wikipedia.org/wiki/Conjugate_prior)的性质。我们在这里不会描述贝叶斯方法。
- en: '**EXAMPLE:** **(Dirichlet)** \(\idx{Dirichlet}\xdi\) The Dirichlet distribution
    is a distribution over the \((K-1)\)-simplex'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(Dirichlet)** \(\idx{Dirichlet}\xdi\) Dirichlet分布是 \((K-1)\)-simplex
    上的分布'
- en: '\[ \S = \Delta_{K} := \left\{ \mathbf{x} = (x_1, \ldots, x_K) : \mathbf{x}
    \geq \mathbf{0},\ \sum_{i=1}^K x_i = 1 \right\}. \]'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \S = \Delta_{K} := \left\{ \mathbf{x} = (x_1, \ldots, x_K) : \mathbf{x}
    \geq \mathbf{0},\ \sum_{i=1}^K x_i = 1 \right\}. \]'
- en: Its parameters are \(\balpha = (\alpha_1, \ldots, \alpha_K) \in \mathbb{R}\)
    and the density is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 它的参数是 \(\balpha = (\alpha_1, \ldots, \alpha_K) \in \mathbb{R}\) 并且密度是
- en: \[ f_{\balpha}(\mathbf{x}) = \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1},
    \quad \mathbf{x} \in \Delta_{K} \]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{\balpha}(\mathbf{x}) = \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1},
    \quad \mathbf{x} \in \Delta_{K} \]
- en: where the normalizing constant \(B(\balpha)\) is the [multivariate Beta function](https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中归一化常数 \(B(\balpha)\) 是[多元Beta函数](https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function)。
- en: Rewriting the density as
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将密度重新写为
- en: \[ \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1} = \frac{1}{B(\balpha)}
    \frac{1}{\prod_{i=1}^K x_i} \exp\left(\sum_{i=1}^K \alpha_i \log x_i\right) \]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1} = \frac{1}{B(\balpha)}
    \frac{1}{\prod_{i=1}^K x_i} \exp\left(\sum_{i=1}^K \alpha_i \log x_i\right) \]
- en: shows that this is an exponential family with the canonical parameters \(\balpha\)
    and sufficient statistics \((\log x_i)_{i=1}^K\). \(\lhd\)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表明这是一个具有规范参数 \(\balpha\) 和充分统计量 \((\log x_i)_{i=1}^K\) 的指数家族。 \(\lhd\)
- en: See [here](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions)
    for many more examples. Observe, in particular, that the same distribution can
    have several representations as an exponential family.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例请见[这里](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions)。特别是，观察到一个分布可以作为指数家族有几种不同的表示形式。
- en: '**NUMERICAL CORNER:** In NumPy, as we have seen before, the module [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html)
    provides a way to sample from a variety of standard distributions. We first initialize
    the [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator)\(\idx{pseudorandom
    number generator}\xdi\) with a [random seed](https://en.wikipedia.org/wiki/Random_seed).
    Recall that it allows the results to be reproducible: using the same seed produces
    the same results again.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 在NumPy中，正如我们之前所看到的，模块 `numpy.random` 提供了一种从各种标准分布中进行采样的方法。我们首先使用一个
    [随机种子](https://en.wikipedia.org/wiki/Random_seed) 初始化 [伪随机数生成器](https://en.wikipedia.org/wiki/Pseudorandom_number_generator)\(\idx{伪随机数生成器}\xdi\)。回想一下，它允许结果可重现：使用相同的种子再次产生相同的结果。'
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here’s are lists of available [probability distributions](https://numpy.org/doc/stable/reference/random/generator.html#distributions).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些可用的 [概率分布](https://numpy.org/doc/stable/reference/random/generator.html#distributions)
    列表。
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here are a few other examples.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些其他示例。
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: \(\unlhd\)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**KNOWLEDGE CHECK:** The Weibull distribution with known shape parameter \(k
    > 0\) takes the following form'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 已知形状参数 \(k > 0\) 的韦伯尔分布具有以下形式'
- en: \[ f(x; \lambda) = \frac{k}{\lambda} \left(\frac{x}{\lambda}\right)^{k-1} e^{-(x/\lambda)^k},
    \]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x; \lambda) = \frac{k}{\lambda} \left(\frac{x}{\lambda}\right)^{k-1} e^{-(x/\lambda)^k},
    \]
- en: for \(x \geq 0\), where \(\lambda > 0\).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(x \geq 0\)，其中 \(\lambda > 0\)。
- en: What is the sufficient statistic of its exponential family form?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 它的指数族形式的充分统计量是什么？
- en: a) \(x\)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(x\)
- en: b) \(\log x\)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\log x\)
- en: c) \(x^{k-1}\)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(x^{k-1}\)
- en: d) \(x^k\)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(x^k\)
- en: e) \((\log x, x^k)\)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: e) \((\log x, x^k)\)
- en: \(\checkmark\)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: 6.2.2\. Parameter estimation[#](#parameter-estimation "Link to this heading")
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.2\. 参数估计[#](#parameter-estimation "链接到这个标题")
- en: When modeling data via a parametric family of distributions, the parameters
    must be determined from the data itself. In a typical setting, we assume that
    the data comprises \(n\) independent samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    from a parametric family \(p_{\btheta}\) with unknown \(\btheta \in \Theta\).
    Many methods exist for estimating \(\btheta\), depending on the context. Here
    we focus on [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).
    It has many [good theoretical properties](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties)
    which we will not describe here, as well as [drawbacks](https://stats.stackexchange.com/questions/261056/why-does-maximum-likelihood-estimation-have-issues-with-over-fitting).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过分布的参数族建模数据时，参数必须从数据本身确定。在典型情况下，我们假设数据由 \(n\) 个独立样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    组成，这些样本来自参数族 \(p_{\btheta}\)，其中 \(\btheta \in \Theta\) 是未知的。存在许多用于估计 \(\btheta\)
    的方法，具体取决于上下文。在这里，我们关注 [最大似然估计](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)。它具有许多
    [良好的理论特性](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties)，我们在此不进行描述，以及
    [缺点](https://stats.stackexchange.com/questions/261056/why-does-maximum-likelihood-estimation-have-issues-with-over-fitting)。
- en: 'The idea behind maximum likelihood estimation is simple and intuitive: we choose
    the parameter that maximizes the probability of observing the data.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计背后的思想简单直观：我们选择最大化观察数据概率的参数。
- en: '**DEFINITION** **(Maximum likelihood estimator)** \(\idx{maximum likelihood}\xdi\)
    Assume that \(\mathbf{X}_1,\ldots,\mathbf{X}_n\) are \(n\) independent samples
    from a parametric family \(p_{\btheta^*}\) with unknown \(\btheta^* \in \Theta\).
    The maximum likelihood estimator of \(\btheta\) is defined as'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** **(最大似然估计器)** \(\idx{最大似然}\xdi\) 假设 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    是从参数族 \(p_{\btheta^*}\) 中抽取的 \(n\) 个独立样本，其中 \(\btheta^* \in \Theta\) 是未知的。\(\btheta\)
    的最大似然估计定义为'
- en: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\max\left\{ \prod_{i=1}^n p_{\btheta}(\mathbf{X}_i)
    \,:\, \btheta \in \Theta \right\}. \]
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\max\left\{ \prod_{i=1}^n p_{\btheta}(\mathbf{X}_i)
    \,:\, \btheta \in \Theta \right\}. \]
- en: It is often useful to work with the negative log-likelihood (NLL)\(\idx{negative
    log-likelihood}\xdi\)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有时使用负对数似然 (NLL)\(\idx{负对数似然}\xdi\) 是很有用的
- en: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i),
    \]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i),
    \]
- en: in which case we are minimizing. \(\natural\)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们是在最小化。 \(\natural\)
- en: '**EXAMPLE:** **(Biased coin)** Suppose we observe \(n\) coin flips \(X_1,\ldots,
    X_n \in \{0,1\}\) from a biased coin with an unknown probability \(\theta^*\)
    of producing \(1\). We assume the flips are independent. We compute the MLE of
    the parameter \(\theta\).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(Biased coin)** 假设我们观察到一个未知概率 \(\theta^*\) 产生 \(1\) 的偏硬币的 \(n\)
    次抛掷 \(X_1,\ldots, X_n \in \{0,1\}\)。我们假设这些抛掷是独立的。我们计算参数 \(\theta\) 的最大似然估计值。'
- en: The definition is
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 定义是
- en: \[ \hat\theta_{\mathrm{MLE}} \in \arg\min\left\{ L_n(\theta; \{X_i\}_{i=1}^n)
    \,:\, \theta \in \Theta = [0,1] \right\} \]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} \in \arg\min\left\{ L_n(\theta; \{X_i\}_{i=1}^n)
    \,:\, \theta \in \Theta = [0,1] \right\} \]
- en: where, using our previous Bernoulli example, the NLL is
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，使用我们之前的伯努利例子，NLL 是
- en: \[\begin{align*} L_n(\theta; \{X_i\}_{i=1}^n) &= - \sum_{i=1}^n \log p_{\theta}(X_i)\\
    &= - \sum_{i=1}^n \log \left[\theta^{X_i} (1- \theta)^{1 -X_i}\right]\\ &= - \sum_{i=1}^n
    \left[ X_i \log \theta + (1 -X_i) \log (1- \theta)\right]. \end{align*}\]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\theta; \{X_i\}_{i=1}^n) &= - \sum_{i=1}^n \log p_{\theta}(X_i)\\
    &= - \sum_{i=1}^n \log \left[\theta^{X_i} (1- \theta)^{1 -X_i}\right]\\ &= - \sum_{i=1}^n
    \left[ X_i \log \theta + (1 -X_i) \log (1- \theta)\right]. \end{align*}\]
- en: 'We compute the first and second derivatives of \(L_n(\theta; \{X_i\}_{i=1}^n)\)
    as a function of \(\theta\):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 \(L_n(\theta; \{X_i\}_{i=1}^n)\) 关于 \(\theta\) 的第一和二阶导数：
- en: \[\begin{align*} \frac{\mathrm{d}}{\mathrm{d} \theta}L_n(\theta; \{X_i\}_{i=1}^n)
    &= - \sum_{i=1}^n \left[ \frac{X_i}{\theta} - \frac{1 -X_i}{1- \theta}\right]\\
    &= - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n X_i}{1- \theta}
    \end{align*}\]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\mathrm{d}}{\mathrm{d} \theta}L_n(\theta; \{X_i\}_{i=1}^n)
    &= - \sum_{i=1}^n \left[ \frac{X_i}{\theta} - \frac{1 -X_i}{1- \theta}\right]\\
    &= - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n X_i}{1- \theta}
    \end{align*}\]
- en: and
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} \frac{\mathrm{d}^2}{\mathrm{d} \theta^2}L_n(\theta; \{X_i\}_{i=1}^n)
    &= \frac{\sum_{i=1}^n X_i}{\theta^2} + \frac{n - \sum_{i=1}^n X_i}{(1- \theta)^2}.
    \end{align*}\]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\mathrm{d}^2}{\mathrm{d} \theta^2}L_n(\theta; \{X_i\}_{i=1}^n)
    &= \frac{\sum_{i=1}^n X_i}{\theta^2} + \frac{n - \sum_{i=1}^n X_i}{(1- \theta)^2}.
    \end{align*}\]
- en: The second derivative is non-negative and therefore the NLL is convex. To find
    a global minimizer, it suffices to find a stationary point.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数非负，因此NLL是凸的。为了找到全局最小值，只需找到驻点。
- en: We make the derivative of the NLL equal to \(0\)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使NLL的导数等于 \(0\)
- en: \[\begin{align*} &0 = - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff \frac{\sum_{i=1}^n X_i}{\theta} = \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff (1- \theta)\sum_{i=1}^n X_i = \theta \left(n - \sum_{i=1}^n
    X_i \right)\\ & \iff \sum_{i=1}^n X_i = \theta n. \end{align*}\]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &0 = - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff \frac{\sum_{i=1}^n X_i}{\theta} = \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff (1- \theta)\sum_{i=1}^n X_i = \theta \left(n - \sum_{i=1}^n
    X_i \right)\\ & \iff \sum_{i=1}^n X_i = \theta n. \end{align*}\]
- en: So
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[ \hat\theta_{\mathrm{MLE}} = \frac{\sum_{i=1}^n X_i}{n}. \]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} = \frac{\sum_{i=1}^n X_i}{n}. \]
- en: 'This is in fact a natural estimator: the empirical frequency of \(1\)s. \(\lhd\)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个自然的估计器：\(1\) 的经验频率。 \(\lhd\)
- en: We give an alternative perspective on the maximum likelihood estimator. Assume
    that \(p_{\btheta}\) is supported on a fixed finite set \(\X\) for all \(\btheta
    \in \Theta\). Given samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\), for each \(\mathbf{x}
    \in \X\), let
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对最大似然估计器给出另一种观点。假设对于所有 \(\btheta \in \Theta\)，\(p_{\btheta}\) 都在固定有限集 \(\X\)
    上。给定样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)，对于每个 \(\mathbf{x} \in \X\)，令
- en: \[ N_\mathbf{x} = \sum_{i=1}^n \mathbf{1}_{\{\mathbf{X}_i = \mathbf{x}\}} \]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \[ N_\mathbf{x} = \sum_{i=1}^n \mathbf{1}_{\{\mathbf{X}_i = \mathbf{x}\}} \]
- en: count the number of times \(\mathbf{x}\) is observed in the data and let
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据中 \(\mathbf{x}\) 出现的次数，并令
- en: \[ \hat\mu_n(\mathbf{x}) = \frac{N_\mathbf{x}}{n} \]
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\mu_n(\mathbf{x}) = \frac{N_\mathbf{x}}{n} \]
- en: be the empirical frequency of \(\mathbf{x}\) in the sample. Observe that \(\hat\mu_n\)
    is a probability distribution over \(\X\).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为样本中 \(\mathbf{x}\) 的经验频率。观察 \(\hat\mu_n\) 是 \(\X\) 上的一个概率分布。
- en: The following theorem characterizes the maximum likelihood estimator in terms
    of the [Kullback-Liebler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\(\idx{Kullback-Liebler
    divergence}\xdi\), which was introduced in a previous section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下定理用 [Kullback-Liebler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    来描述最大似然估计器，这在前面一节中已介绍。
- en: For two probability distributions
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个概率分布
- en: \[ \mathbf{p}, \mathbf{q} \in \Delta_K := \left\{ (p_1,\ldots,p_K) \in [0,1]^K
    \,:\, \sum_{k=1}^K p_k = 1 \right\}, \]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}, \mathbf{q} \in \Delta_K := \left\{ (p_1,\ldots,p_K) \in [0,1]^K
    \,:\, \sum_{k=1}^K p_k = 1 \right\}, \]
- en: it is defined as
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它被定义为
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
- en: where it will suffice to restrict ourselves to the case \(\mathbf{q} > \mathbf{0}\)
    and where we use the convention \(0 \log 0 = 0\) (so that terms with \(p_i = 0\)
    contribute \(0\) to the sum).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，我们只需将自身限制在 \(\mathbf{q} > \mathbf{0}\) 的情况下，并且使用约定 \(0 \log 0 = 0\)（这样 \(p_i
    = 0\) 的项对总和的贡献为 \(0\)）。
- en: Notice that \(\mathbf{p} = \mathbf{q}\) implies \(\mathrm{KL}(\mathbf{p} \|
    \mathbf{q}) = 0\). We show that \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\),
    a result known as *Gibbs’ inequality*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mathbf{p} = \mathbf{q}\) 意味着 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q})
    = 0\)。我们证明 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)，这是一个被称为 *吉布斯不等式* 的结果。
- en: '**THEOREM** **(Gibbs)** \(\idx{Gibbs'' inequality}\xdi\) For any \(\mathbf{p},
    \mathbf{q} \in \Delta_K\) with \(\mathbf{q} > \mathbf{0}\),'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(吉布斯)** \(\idx{Gibbs'' inequality}\xdi\) 对于任何 \(\mathbf{p}, \mathbf{q}
    \in \Delta_K\) 且 \(\mathbf{q} > \mathbf{0}\)，'
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0. \]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0. \]
- en: \(\sharp\)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: '*Proof:* Let \(I\) be the set of indices \(i\) such that \(p_i > 0\). Hence'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 设 \(I\) 为满足 \(p_i > 0\) 的索引 \(i\) 的集合。因此'
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i \in I} p_i \log \frac{p_i}{q_i}.
    \]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i \in I} p_i \log \frac{p_i}{q_i}.
    \]
- en: It can be proved that \(\log x \leq x - 1\) for all \(x > 0\) (Try it!). So
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明对于所有 \(x > 0\)，\(\log x \leq x - 1\)（试试看！）。所以
- en: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= - \sum_{i \in I} p_i
    \log \frac{q_i}{p_i}\\ &\geq - \sum_{i \in I} p_i \left(\frac{q_i}{p_i} - 1\right)\\
    &= - \sum_{i \in I} q_i + \sum_{i \in I} p_i\\ &= - \sum_{i \in I} q_i + 1\\ &\geq
    0 \end{align*}\]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= - \sum_{i \in I} p_i
    \log \frac{q_i}{p_i}\\ &\geq - \sum_{i \in I} p_i \left(\frac{q_i}{p_i} - 1\right)\\
    &= - \sum_{i \in I} q_i + \sum_{i \in I} p_i\\ &= - \sum_{i \in I} q_i + 1\\ &\geq
    0 \end{align*}\]
- en: where we used that \(\log z^{-1} = - \log z\) on the first line and the fact
    that \(p_i = 0\) for all \(i \notin I\) on the fourth line. \(\square\)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，我们在第一行使用了 \(\log z^{-1} = - \log z\) 的性质，并在第四行使用了 \(p_i = 0\) 对于所有 \(i \notin
    I\) 的事实。 \(\square\)
- en: '**THEOREM** **(MLE via KL)** \(\idx{MLE via KL theorem}\xdi\) Assume that,
    for all \(\btheta \in \Theta\), \(p_{\btheta}\) is supported on a fixed finite
    set \(\X\) and that \(p_{\btheta}(\mathbf{x}) > 0\) for all \(\mathbf{x} \in \X\).
    Given samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\) from \(p_{\btheta^*}\), let
    \(\{\hat\mu_n(\mathbf{x})\}_{\mathbf{x} \in \X}\) be the corresponding empirical
    frequencies. Then the maximum likelihood estimator \(\hat\btheta_{\mathrm{MLE}}\)
    of \(\btheta\) is also a solution to'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(通过 KL 的 MLE)** \(\idx{MLE via KL theorem}\xdi\) 假设对于所有 \(\btheta
    \in \Theta\)，\(p_{\btheta}\) 在一个固定的有限集合 \(\X\) 上有支撑，并且对于所有 \(\mathbf{x} \in \X\)，\(p_{\btheta}(\mathbf{x})
    > 0\)。给定从 \(p_{\btheta^*}\) 中抽取的样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)，令 \(\{\hat\mu_n(\mathbf{x})\}_{\mathbf{x}
    \in \X}\) 为相应的经验频率。那么 \(\btheta\) 的最大似然估计量 \(\hat\btheta_{\mathrm{MLE}}\) 也是以下方程的解'
- en: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\min\left\{ \mathrm{KL}(\hat{\mu}_n \|
    p_{\btheta}) \,:\, \btheta \in \Theta \right\}. \]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\min\left\{ \mathrm{KL}(\hat{\mu}_n \|
    p_{\btheta}) \,:\, \btheta \in \Theta \right\}. \]
- en: \(\sharp\)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: '*Proof idea:* Manipulate the negative log-likelihood to bring out its relationship
    to the Kullback-Liebler divergence.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路*: 通过操作负对数似然来揭示其与 Kullback-Liebler 散度的关系。'
- en: '*Proof:* We can re-write the negative log-likelihood as'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 我们可以将负对数似然重写为'
- en: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i)
    = - \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x}). \]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i)
    = - \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x}). \]
- en: To bring out the Kullback-Liebler divergence, we further transform the previous
    equation into
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了揭示 Kullback-Liebler 散度，我们进一步将前面的方程转换为
- en: \[\begin{align*} \frac{1}{n} L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= - \frac{1}{n}
    \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x})\\ &= \sum_{\mathbf{x}
    \in \X} (N_{\mathbf{x}}/n) \log \frac{N_{\mathbf{x}}/n}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} (N_{\mathbf{x}}/n) \log (N_{\mathbf{x}}/n)\\ &= \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x}) \log \hat\mu_n(\mathbf{x})\\
    &= \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) + \mathrm{H}(\hat\mu_n), \end{align*}\]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{1}{n} L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= - \frac{1}{n}
    \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x})\\ &= \sum_{\mathbf{x}
    \in \X} (N_{\mathbf{x}}/n) \log \frac{N_{\mathbf{x}}/n}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} (N_{\mathbf{x}}/n) \log (N_{\mathbf{x}}/n)\\ &= \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x}) \log \hat\mu_n(\mathbf{x})\\
    &= \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) + \mathrm{H}(\hat\mu_n), \end{align*}\]
- en: where the second term is referred to as the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))\(\idx{entropy}\xdi\)
    of \(\hat\mu_n\).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第二项被称为 \(\hat\mu_n\) 的 [熵](https://en.wikipedia.org/wiki/Entropy_(information_theory))\(\idx{熵}\xdi\)。
- en: Because \(\mathrm{H}(\hat\mu_n)\) does not depend on \(\btheta\), minimizing
    \(L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\) is equivalent to minimizing \(\mathrm{KL}(\hat{\mu}_n
    \| p_{\btheta})\) as claimed. \(\square\)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathrm{H}(\hat\mu_n)\) 不依赖于 \(\btheta\)，最小化 \(L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\)
    等价于最小化 \(\mathrm{KL}(\hat{\mu}_n \| p_{\btheta})\)，正如所声称的那样。 \(\square\)
- en: 'In words, the maximum likelihood estimator chooses the parametric distribution
    that is closest to \(\hat\mu_n\) in Kullback-Liebler divergence. One can think
    of this as “projecting” \(\hat\mu_n\) onto the space \(\{p_{\btheta} : \btheta
    \in \Theta\}\) under the Kullback-Liebler notion of distance.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '也就是说，最大似然估计器选择与 \(\hat\mu_n\) 在 Kullback-Liebler 散度上最接近的参数分布。可以将其视为在 Kullback-Liebler
    距离概念下将 \(\hat\mu_n\) “投影”到 \(\{p_{\btheta} : \btheta \in \Theta\}\) 空间中。'
- en: '**EXAMPLE:** **(Special case)** One special case is where \(\X\) is finite,
    \(\btheta = (\theta_\mathbf{x})_{\mathbf{x} \in \X}\) is a probability distribution
    over \(\X\), and \(p_{\btheta} = \btheta\). That is, we consider the class of
    all probability distributions over \(\X\). Given samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    from \(p_{\btheta^*}\), in this case we have'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(特殊情况)** 一个特殊情况是当 \(\X\) 是有限的，\(\btheta = (\theta_\mathbf{x})_{\mathbf{x}
    \in \X}\) 是 \(\X\) 上的概率分布，且 \(p_{\btheta} = \btheta\)。也就是说，我们考虑 \(\X\) 上所有概率分布的类别。给定从
    \(p_{\btheta^*}\) 中抽取的样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)，在这种情况下我们有'
- en: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x})
    \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})} = \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}},
    \]
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x})
    \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})} = \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}},
    \]
- en: where recall that, by convention, if \(\hat\mu_n(\mathbf{x}) = 0\) then \(\hat\mu_n(\mathbf{x})
    \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}} = 0\) for any \(\theta_\mathbf{x}\).
    So, letting \(\mathbb{X}_n = \{\mathbf{X}_1,\ldots,\mathbf{X}_n\}\) be the set
    of distinct values encountered in the sample (ignoring repetitions), we have
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，根据惯例，如果 \(\hat\mu_n(\mathbf{x}) = 0\)，则 \(\hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}}
    = 0\) 对于任何 \(\theta_\mathbf{x}\) 都成立。因此，让 \(\mathbb{X}_n = \{\mathbf{X}_1,\ldots,\mathbf{X}_n\}\)
    成为样本中遇到的唯一值集合（忽略重复），我们有
- en: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}}. \]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}}. \]
- en: Note that \(\sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1\).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1\).
- en: 'We have previously established *Gibbs’ inequality* which says that: for any
    \(\mathbf{p}, \mathbf{q} \in \Delta_K\) with \(\mathbf{q} > \mathbf{0}\), it holds
    that \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经建立了 *吉布斯不等式*，它说：对于任何 \(\mathbf{p}, \mathbf{q} \in \Delta_K\) 且 \(\mathbf{q}
    > \mathbf{0}\)，都有 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)。
- en: The minimum \(\mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = 0\) can be achieved
    by setting \(\btheta_{\mathbf{x}} = \hat\mu_n(\mathbf{x})\) for all \(\mathbf{x}
    \in \mathbb{X}_n\) and \(\btheta_{\mathbf{x}} = 0\) for all \(\mathbf{x} \notin
    \mathbb{X}_n\). The condition
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化 \(\mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = 0\) 可以通过将所有 \(\mathbf{x} \in
    \mathbb{X}_n\) 的 \(\btheta_{\mathbf{x}}\) 设置为 \(\hat\mu_n(\mathbf{x})\)，以及所有 \(\mathbf{x}
    \notin \mathbb{X}_n\) 的 \(\btheta_{\mathbf{x}}\) 设置为 0 来实现。条件
- en: \[ \sum_{\mathbf{x} \in \X} \btheta_{\mathbf{x}} = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \btheta_{\mathbf{x}} + \sum_{\mathbf{x} \notin \mathbb{X}_n} \btheta_{\mathbf{x}}
    = \sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1, \]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{\mathbf{x} \in \X} \btheta_{\mathbf{x}} = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \btheta_{\mathbf{x}} + \sum_{\mathbf{x} \notin \mathbb{X}_n} \btheta_{\mathbf{x}}
    = \sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1, \]
- en: is then satisfied.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 则满足。
- en: So in this case \(\hat\mu_n\) is a maximum likelihood estimator.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因此在这种情况下，\(\hat\mu_n\) 是一个最大似然估计量。
- en: A special case of this is the *biased coin* example. \(\lhd\)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个特殊情况是 *有偏硬币* 例子。\(\lhd\)
- en: '**CHAT & LEARN** Explore the concept of Bayesian parameter estimation. Ask
    your favorite AI chatbot how Bayesian parameter estimation differs from maximum
    likelihood estimation and discuss their relative strengths and weaknesses. Here
    are some possible follow-ups. (1) Get an example implementation using a simple
    dataset. (2) The categorical and multinomial distributions are related to the
    Dirichlet distribution. Ask about relationship and how the Dirichlet distribution
    is used in Bayesian inference for these distributions. \(\ddagger\)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊与学** 探索贝叶斯参数估计的概念。询问您最喜欢的 AI 聊天机器人贝叶斯参数估计与最大似然估计有何不同，并讨论它们的相对优缺点。以下是一些可能的后续问题。(1)
    获取使用简单数据集的示例实现。(2) 分类别和多项分布与 Dirichlet 分布相关。询问它们之间的关系以及 Dirichlet 分布如何用于这些分布的贝叶斯推断。\(\ddagger\)'
- en: 6.2.3\. Parameter estimation for exponential families[#](#parameter-estimation-for-exponential-families
    "Link to this heading")
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.3\. 参数估计对于指数族[#](#parameter-estimation-for-exponential-families "链接到这个标题")
- en: For exponential families, maximum likelihood estimation takes a particularly
    natural form. We provide details in the discrete case.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指数族，最大似然估计具有特别自然的形式。我们将在离散情况下提供详细说明。
- en: '**THEOREM** **(Maximum Likelihood Estimator for Exponential Families)** \(\idx{maximum
    likelihood estimator for exponential families}\xdi\) Assume that \(p_{\btheta}\)
    takes the exponential family form'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(指数族的最大似然估计量)** \(\idx{指数族的最大似然估计量}\xdi\) 假设 \(p_{\btheta}\) 采用指数族形式'
- en: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
- en: that the support \(\S\) is finite, and that \(A\) is twice continuously differentiable
    over the open set \(\Theta\). Let \(\mathbf{X}_1,\ldots,\mathbf{X}_n\) be \(n\)
    independent samples from a parametric family \(p_{\btheta^*}\) with unknown \(\btheta^*
    \in \Theta\). Then \(L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\), as a function of
    \(\btheta\), is convex and the maximum likelihood estimator of \(\btheta\) – if
    it exists – solves the system of moment-matching equations
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 支持集 \(\S\) 是有限的，且 \(A\) 在开集 \(\Theta\) 上是两次连续可微的。设 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    是从参数族 \(p_{\btheta^*}\) 中抽取的 \(n\) 个独立样本，其中 \(\btheta^* \in \Theta\) 是未知的。那么 \(L_n(\btheta;
    \{\mathbf{X}_i\}_{i=1}^n)\)，作为 \(\btheta\) 的函数，是凸的，并且 \(\btheta\) 的最大似然估计量（如果存在）解决了以下矩匹配方程组
- en: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i), \]
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i), \]
- en: where \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\). \(\sharp\)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\)。\(\sharp\)
- en: Recall that the covariance matrix of a random vector \(\mathbf{Z}\) taking values
    in \(\mathbb{R}^m\) whose components have finite variances is defined as \(\mathrm{K}_{\mathbf{Z},
    \mathbf{Z}} = \E[(\mathbf{Z} - \E[\mathbf{Z}])(\mathbf{Z} - \E[\mathbf{Z}])^T]\)
    and is a positive semidefinite matrix. It is also sometimes denoted as \(\bSigma_\mathbf{Z}\).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，随机向量 \(\mathbf{Z}\) 的协方差矩阵，其值在 \(\mathbb{R}^m\) 中，且各分量具有有限的方差，定义为 \(\mathrm{K}_{\mathbf{Z},
    \mathbf{Z}} = \E[(\mathbf{Z} - \E[\mathbf{Z}])(\mathbf{Z} - \E[\mathbf{Z}])^T]\)，它是一个正半定矩阵。它有时也记为
    \(\bSigma_\mathbf{Z}\)。
- en: The function \(A\) has properties worth highlighting that will be used in the
    proof.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 \(A\) 具有一些值得强调的性质，这些性质将在证明中使用。
- en: '**LEMMA** **(Derivatives of \(A\))** Assume that \(p_{\btheta}\) takes the
    exponential family form'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(A 的导数)** 假设 \(p_{\btheta}\) 采用指数族形式'
- en: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
- en: that the support \(\S\) is finite, and that \(A\) is twice continuously differentiable
    over the open set \(\Theta\). Then
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 支持集 \(\S\) 是有限的，且 \(A\) 在开集 \(\Theta\) 上是两次连续可微的。那么
- en: \[ \nabla A(\btheta) = \E[\bphi(\mathbf{X})] \qquad \text{and} \qquad \mathbf{H}_A
    (\btheta) = \mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}, \]
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla A(\theta) = \E[\phi(\mathbf{X})] \qquad \text{and} \qquad \mathbf{H}_A
    (\theta) = \mathrm{K}_{\phi(\mathbf{X}), \phi(\mathbf{X})}, \]
- en: where \(\mathbf{X} \sim p_{\btheta}\). \(\flat\)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\theta}\). \(\flat\)
- en: '*Proof idea:* Follows from a direct calculation.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 直接计算得出。'
- en: '*Proof:* We observe first that'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们首先观察到'
- en: \[ A(\btheta) = \log Z(\btheta) = \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}))\right), \]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A(\theta) = \log Z(\theta) = \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x})
    \exp(\theta^T \phi(\mathbf{x}))\right), \]
- en: where we used the fact that, by definition, \(Z(\btheta)\) is the normalization
    constant of \(p_{\btheta}\). In particular, as the logarithm of a finite, weighted
    sum of exponentials, the function \(A(\btheta)\) is continuously differentiable.
    Hence so is \(p_{\btheta}(\mathbf{x})\) as a function of \(\btheta\).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了这样一个事实，根据定义，\(Z(\theta)\) 是 \(p_{\theta}\) 的归一化常数。特别是，由于 \(A(\theta)\)
    是有限加权指数和的对数，该函数 \(A(\theta)\) 是连续可微的。因此，作为 \(\theta\) 的函数，\(p_{\theta}(\mathbf{x})\)
    也是连续可微的。
- en: From the formula above and the basic rules of calculus,
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的公式和微积分的基本规则，
- en: \[\begin{align*} \frac{\partial}{\partial \theta_j} A(\btheta) &= \frac{\partial}{\partial
    \theta_j} \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}))\right)\\
    &= \frac{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \,\phi_j(\mathbf{x}) \exp(\btheta^T
    \bphi(\mathbf{x}))}{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}))}\\
    &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x}) \frac{1}{Z(\btheta)} h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}))\\ &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\\ &= \E[\phi_j(\mathbf{X})],
    \end{align*}\]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial \theta_j} A(\theta) &= \frac{\partial}{\partial
    \theta_j} \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\theta^T \phi(\mathbf{x}))\right)\\
    &= \frac{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \,\phi_j(\mathbf{x}) \exp(\theta^T
    \phi(\mathbf{x}))}{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\theta^T \phi(\mathbf{x}))}\\
    &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x}) \frac{1}{Z(\theta)} h(\mathbf{x})
    \exp(\theta^T \phi(\mathbf{x}))\\ &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\theta^T \phi(\mathbf{x}) - A(\theta))\\ &= \E[\phi_j(\mathbf{X})],
    \end{align*}\]
- en: where \(\mathbf{X} \sim p_{\btheta}\).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\theta}\).
- en: Differentiating again, this time with respect to \(\theta_i\), we obtain
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 再次求导，这次是对 \(\theta_i\) 求导，我们得到
- en: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\btheta)
    &= \frac{\partial}{\partial \theta_i} \left\{\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\right\}\\ &= \sum_{\mathbf{x}
    \in \S} \phi_j(\mathbf{x}) h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))
    \left\{\phi_i(\mathbf{x}) - \frac{\partial}{\partial \theta_i} A(\btheta) \right\}\\
    &= \sum_{\mathbf{x} \in \S} \phi_i(\mathbf{x}) \phi_j(\mathbf{x}) h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\\ & \qquad - \left(\sum_{\mathbf{x}
    \in \S} \phi_i(\mathbf{x}) h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))
    \right)\\ & \qquad\qquad \times \left(\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta)) \right)\\ &= \E[\phi_i(\mathbf{X})\phi_j(\mathbf{X})]
    - \E[\phi_i(\mathbf{X})]\E[\phi_j(\mathbf{X})], \end{align*}\]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\theta)
    &= \frac{\partial}{\partial \theta_i} \left\{\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\theta^T \phi(\mathbf{x}) - A(\theta))\right\}\\ &= \sum_{\mathbf{x}
    \in \S} \phi_j(\mathbf{x}) h(\mathbf{x}) \exp(\theta^T \phi(\mathbf{x}) - A(\theta))
    \left\{\phi_i(\mathbf{x}) - \frac{\partial}{\partial \theta_i} A(\theta) \right\}\\
    &= \sum_{\mathbf{x} \in \S} \phi_i(\mathbf{x}) \phi_j(\mathbf{x}) h(\mathbf{x})
    \exp(\theta^T \phi(\mathbf{x}) - A(\theta))\\ & \qquad - \left(\sum_{\mathbf{x}
    \in \S} \phi_i(\mathbf{x}) h(\mathbf{x}) \exp(\theta^T \phi(\mathbf{x}) - A(\theta))
    \right)\\ & \qquad\qquad \times \left(\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\theta^T \phi(\mathbf{x}) - A(\theta)) \right)\\ &= \E[\phi_i(\mathbf{X})\phi_j(\mathbf{X})]
    - \E[\phi_i(\mathbf{X})]\E[\phi_j(\mathbf{X})], \end{align*}\]
- en: where again \(\mathbf{X} \sim p_{\btheta}\). That concludes the proof. \(\square\)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中再次 \(\mathbf{X} \sim p_{\theta}\)。这就完成了证明。 \(\square\)
- en: We are now ready to the prove the main theorem.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备证明主要定理。
- en: '*Proof:* *(Maximum Likelihood Estimator for Exponential Families)* We begin
    by computing the stationary points of the negative log-likelihood, for which we
    need the gradient with respect to \(\btheta \in \mathbb{R}^m\). We will also need
    the second-order derivatives to establish convexity. We have'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* *(指数族的最大似然估计)* 我们首先计算负对数似然函数的驻点，为此我们需要关于 \(\theta \in \mathbb{R}^m\)
    的梯度。我们还需要二阶导数来建立凸性。我们有'
- en: \[\begin{align*} \frac{\partial}{\partial \theta_j} \{- \log p_{\btheta}(\mathbf{x})\}
    &= \frac{\partial}{\partial \theta_j} \left\{- \log h(\mathbf{x}) - \btheta^T
    \bphi(\mathbf{x}) + A(\btheta)\right\}\\ &= - \phi_j(\mathbf{x}) + \frac{\partial}{\partial
    \theta_j} A(\btheta). \end{align*}\]
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial \theta_j} \{- \log p_{\btheta}(\mathbf{x})\}
    &= \frac{\partial}{\partial \theta_j} \left\{- \log h(\mathbf{x}) - \btheta^T
    \bphi(\mathbf{x}) + A(\btheta)\right\}\\ &= - \phi_j(\mathbf{x}) + \frac{\partial}{\partial
    \theta_j} A(\btheta). \end{align*}\]
- en: and
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} \{-
    \log p_{\btheta}(\mathbf{x})\} &= \frac{\partial}{\partial \theta_i} \left\{-
    \phi_j(\mathbf{x}) + \frac{\partial}{\partial \theta_j} A(\btheta)\right\}\\ &=
    \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\btheta). \end{align*}\]
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} \{-
    \log p_{\btheta}(\mathbf{x})\} &= \frac{\partial}{\partial \theta_i} \left\{-
    \phi_j(\mathbf{x}) + \frac{\partial}{\partial \theta_j} A(\btheta)\right\}\\ &=
    \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\btheta). \end{align*}\]
- en: We use the expressions for the derivatives of \(A\) obtained above.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上面得到的 \(A\) 的导数表达式。
- en: Plugging into the formula for the minus log-likelihood (as a function of \(\btheta\)),
    we get for the gradient with respect to \(\btheta\)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 将其代入负对数似然函数的公式（作为 \(\btheta\) 的函数），我们得到相对于 \(\btheta\) 的梯度
- en: \[\begin{align*} \nabla_\btheta L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= -
    \sum_{i=1}^n \nabla_\btheta \log p_{\btheta}(\mathbf{X}_i)\\ &= \sum_{i=1}^n \{-
    \bphi(\mathbf{X}_i) + \nabla_\btheta A(\btheta)\}\\ &= \sum_{i=1}^n \{- \bphi(\mathbf{X}_i)
    + \E[\bphi(\mathbf{X})]\}. \end{align*}\]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla_\btheta L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= -
    \sum_{i=1}^n \nabla_\btheta \log p_{\btheta}(\mathbf{X}_i)\\ &= \sum_{i=1}^n \{-
    \bphi(\mathbf{X}_i) + \nabla_\btheta A(\btheta)\}\\ &= \sum_{i=1}^n \{- \bphi(\mathbf{X}_i)
    + \E[\bphi(\mathbf{X})]\}. \end{align*}\]
- en: This is also known in statistics as the [score](https://en.wikipedia.org/wiki/Score_(statistics)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，这也被称为 [得分](https://en.wikipedia.org/wiki/Score_(statistics))。
- en: For the Hessian with respect to \(\btheta\), we get
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相对于 \(\btheta\) 的 Hessian 矩阵，我们得到
- en: \[\begin{align*} \mathbf{H}_{L_n}(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \mathbf{H}_A (\btheta) = n \,\mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}.
    \end{align*}\]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{H}_{L_n}(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \mathbf{H}_A (\btheta) = n \,\mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}.
    \end{align*}\]
- en: This is also known in statistics as the [observed information](https://en.wikipedia.org/wiki/Observed_information).
    (In fact, in this case, it reduces to the [Fisher information](https://en.wikipedia.org/wiki/Fisher_information).)
    Since \(\mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}\) is positive semidefinite,
    so is \(\mathbf{H}_{L_n}(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这在统计学中也被称为 [观测信息](https://en.wikipedia.org/wiki/Observed_information)。（实际上，在这种情况下，它简化为
    [费舍尔信息](https://en.wikipedia.org/wiki/Fisher_information)。）由于 \(\mathrm{K}_{\bphi(\mathbf{X}),
    \bphi(\mathbf{X})}\) 是正半定的，所以 \(\mathbf{H}_{L_n}(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\)
    也是正半定的。
- en: Hence, a stationary point \(\hat\btheta_{\mathrm{MLE}}\) must satisfy
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，驻点 \(\hat\btheta_{\mathrm{MLE}}\) 必须满足
- en: \[ \mathbf{0} = \nabla L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \{- \bphi(\mathbf{X}_i) + \E[\bphi(\mathbf{X})]\}, \]
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{0} = \nabla L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \{- \bphi(\mathbf{X}_i) + \E[\bphi(\mathbf{X})]\}, \]
- en: where \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\) or, after re-arranging,
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\) 或，经过重新排列，
- en: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i). \]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i). \]
- en: Because \(L_n\) is convex, a stationary point – if it exists – is necessarily
    a global minimum (and vice versa). \(\square\)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(L_n\) 是凸函数，所以一个驻点（如果存在）必然是全局最小值（反之亦然）。\(\square\)
- en: '**EXAMPLE:** **(Bernoulli/biased coin, continued)** For \(x \in \{0,1\}\),
    recall that the \(\mathrm{Ber}(q)\) distribution can be written as'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(伯努利/偏硬币，继续)** 对于 \(x \in \{0,1\}\)，回忆一下，\(\mathrm{Ber}(q)\) 分布可以写成'
- en: \[\begin{align*} p_{\theta}(x) &= \frac{1}{Z(\theta)} h(x) \exp(\theta \,\phi(x))
    \end{align*}\]
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_{\theta}(x) &= \frac{1}{Z(\theta)} h(x) \exp(\theta \,\phi(x))
    \end{align*}\]
- en: where we define \(h(x) \equiv 1\), \(\phi(x) = x\), \(\theta = \log \left(\frac{q}{1-q}\right)\)
    and \(Z(\theta) = 1 + e^\theta\). Let \(X_1,\ldots,X_n\) be independent samples
    from \(p_{\theta^*}\).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(h(x) \equiv 1\)，\(\phi(x) = x\)，\(\theta = \log \left(\frac{q}{1-q}\right)\)
    和 \(Z(\theta) = 1 + e^\theta\)。设 \(X_1,\ldots,X_n\) 是从 \(p_{\theta^*}\) 中独立抽取的样本。
- en: For \(X \sim p_{\hat\theta_{\mathrm{MLE}}}\), the moment-matching equations
    reduce to
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(X \sim p_{\hat\theta_{\mathrm{MLE}}}\)，矩匹配方程简化为
- en: \[ \hat{q}_{\mathrm{MLE}} := \E[X] = \E[\phi(X)] = \frac{1}{n} \sum_{i=1}^n
    \phi(X_i) = \frac{1}{n} \sum_{i=1}^n X_i. \]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{q}_{\mathrm{MLE}} := \E[X] = \E[\phi(X)] = \frac{1}{n} \sum_{i=1}^n
    \phi(X_i) = \frac{1}{n} \sum_{i=1}^n X_i. \]
- en: To compute the left-hand side in terms of \(\hat\theta_{\mathrm{MLE}}\) we use
    the relationship \(\theta = \log \left(\frac{q}{1-q}\right)\), that is,
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用 \(\hat\theta_{\mathrm{MLE}}\) 来计算左侧，我们使用关系 \(\theta = \log \left(\frac{q}{1-q}\right)\)，即，
- en: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right). \]
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right). \]
- en: Hence, \(\hat\theta_{\mathrm{MLE}}\) is well-defined when \(\frac{1}{n} \sum_{i=1}^n
    X_i \neq 0, 1\).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 \(\frac{1}{n} \sum_{i=1}^n X_i \neq 0, 1\) 时，\(\hat\theta_{\mathrm{MLE}}\)
    是有定义的。
- en: Define \(q^*\) as the solution to
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 \(q^*\) 为
- en: \[ \theta^* = \log \left(\frac{q^*}{1-q^*}\right) \]
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \theta^* = \log \left(\frac{q^*}{1-q^*}\right) \]
- en: that is,
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 即，
- en: \[ q^* = \frac{e^{\theta^*}}{1+e^{\theta^*}} = \frac{1}{1 + e^{-\theta*}} =
    \sigma(\theta^*), \]
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: \[ q^* = \frac{e^{\theta^*}}{1+e^{\theta^*}} = \frac{1}{1 + e^{-\theta*}} =
    \sigma(\theta^*), \]
- en: where \(\sigma\) is the sigmoid function.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma\) 是sigmoid函数。
- en: By the law of large numbers, as \(n \to +\infty\), we get the convergence
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 根据大数定律，当 \(n \to +\infty\) 时，我们得到收敛
- en: \[ \frac{1}{n} \sum_{i=1}^n X_i \to q^*, \]
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n} \sum_{i=1}^n X_i \to q^*, \]
- en: with probability one.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎肯定。
- en: Because the function \(\log \left(\frac{q}{1-q}\right)\) is continuous for \(q
    \in (0,1)\), we have furthermore
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因为函数 \(\log \left(\frac{q}{1-q}\right)\) 在 \(q \in (0,1)\) 上是连续的，所以我们还有
- en: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right) \to \log \left(\frac{q^*}{1-q^*}\right) = \theta^*. \]
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right) \to \log \left(\frac{q^*}{1-q^*}\right) = \theta^*. \]
- en: In words, the maximum likelihood estimator \(\hat\theta_{\mathrm{MLE}}\) is
    guaranteed to converge to the true parameter \(\theta^*\) when the number of samples
    grows. This fundamental property is known as [statistical consistency](https://en.wikipedia.org/wiki/Consistent_estimator)\(\idx{statistical
    consistency}\xdi\). \(\lhd\)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，当样本数量增加时，最大似然估计量 \(\hat\theta_{\mathrm{MLE}}\) 有保证收敛到真实参数 \(\theta^*\)。这个基本性质被称为
    [统计一致性](https://en.wikipedia.org/wiki/Consistent_estimator)\(\idx{statistical
    consistency}\xdi\)。 \(\lhd\)
- en: Statistical consistency holds more generally for the maximum likelihood estimator
    under exponential families, provided certain technical conditions hold. We will
    not provide further details here.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在指数族中，最大似然估计量在更一般的情况下保持统计一致性，前提是满足某些技术条件。这里我们不再提供更多细节。
- en: Unlike the previous example, one does not always have an explicit formula for
    the maximum likelihood estimator under exponential families. Instead, optimization
    methods, for instance [Newton’s method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization),
    are used in such cases.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个例子不同，在指数族中，并不总是有一个显式的最大似然估计量公式。相反，在这种情况下，使用优化方法，例如 [牛顿法](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)。
- en: '**EXAMPLE:** **(Multivariate Gaussian)** We established the theorem for finite
    \(\mathcal{S}\), but it holds more generally. Consider the multivariate Gaussian
    case. Here the sufficient statistics are'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(Multivariate Gaussian)** 我们为有限的 \(\mathcal{S}\) 建立了定理，但它更普遍地成立。考虑多元高斯情况。在这里，充分统计量是'
- en: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]
- en: which is simply the vector \(\mathbf{x}\) itself stacked with the vectorized
    form of the matrix \(\mathbf{x} \mathbf{x}^T\). So the moment-matching equations
    boil down to
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是将向量 \(\mathbf{x}\) 本身与其矩阵 \(\mathbf{x} \mathbf{x}^T\) 的向量形式堆叠起来。因此，矩匹配方程简化为
- en: \[ \E[\mathbf{X}] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\mathbf{X}] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \]
- en: and
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \E[\mathbf{X} \mathbf{X}^T ] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T.
    \]
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\mathbf{X} \mathbf{X}^T ] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T.
    \]
- en: The first equation says to choose \(\bmu = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i\).
    The second one says to take
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程说明选择 \(\bmu = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i\)。第二个方程说明取
- en: \[ \bSigma = \E[\mathbf{X} \mathbf{X}^T] - \E[\mathbf{X}]\,\E[\mathbf{X}]^T
    = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T - \left(\frac{1}{n} \sum_{i=1}^n
    \mathbf{X}_i\right) \left(\frac{1}{n} \sum_{i=1}^n \mathbf{X}_i^T \right). \]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bSigma = \E[\mathbf{X} \mathbf{X}^T] - \E[\mathbf{X}]\,\E[\mathbf{X}]^T
    = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T - \left(\frac{1}{n} \sum_{i=1}^n
    \mathbf{X}_i\right) \left(\frac{1}{n} \sum_{i=1}^n \mathbf{X}_i^T \right). \]
- en: \(\lhd\)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**KNOWLEDGE CHECK:** Consider again the Weibull distribution with known shape
    parameter \(k > 0\).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**KNOWLEDGE CHECK:** 再次考虑具有已知形状参数 \(k > 0\) 的威布尔分布。'
- en: a) Compute \(\E[X^k]\). [*Hint:* Perform a change of variables.]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: a) 计算 \(\E[X^k]\)。[*提示:* 进行变量替换。]
- en: b) What is the MLE of \(\lambda\)?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\lambda\) 的最大似然估计（MLE）是什么？
- en: \(\checkmark\)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: 6.2.4\. Generalized linear models[#](#generalized-linear-models "Link to this
    heading")
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.4\. 广义线性模型[#](#generalized-linear-models "链接到本标题")
- en: 'Generalized linear models\(\idx{generalized linear model}\xdi\) (GLM) provide
    a broad generalization of linear regression using exponential families. Quoting
    from [Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model), the
    context in which they arise is the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 广义线性模型（广义线性模型\(\idx{generalized linear model}\xdi\)，GLM）通过指数族对线性回归进行了广泛的推广。引用自[Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model)，它们出现的背景如下：
- en: Ordinary linear regression predicts the expected value of a given unknown quantity
    (the response variable, a random variable) as a linear combination of a set of
    observed values (predictors). This implies that a constant change in a predictor
    leads to a constant change in the response variable (i.e. a linear-response model).
    This is appropriate when the response variable can vary, to a good approximation,
    indefinitely in either direction, or more generally for any quantity that only
    varies by a relatively small amount compared to the variation in the predictive
    variables, e.g. human heights. However, these assumptions are inappropriate for
    some types of response variables. For example, in cases where the response variable
    is expected to be always positive and varying over a wide range, constant input
    changes lead to geometrically (i.e. exponentially) varying, rather than constantly
    varying, output changes. […] Similarly, a model that predicts a probability of
    making a yes/no choice (a Bernoulli variable) is even less suitable as a linear-response
    model, since probabilities are bounded on both ends (they must be between 0 and
    1). […] Generalized linear models cover all these situations by allowing for response
    variables that have arbitrary distributions (rather than simply normal distributions),
    and for an arbitrary function of the response variable (the link function) to
    vary linearly with the predicted values (rather than assuming that the response
    itself must vary linearly).
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 普通线性回归预测给定未知量（响应变量，一个随机变量）的期望值作为一组观察值（预测器）的线性组合。这意味着预测器的常数变化会导致响应变量的常数变化（即线性响应模型）。当响应变量可以无限变化，或者更一般地，对于与预测变量变化相比变化相对较小的任何数量，例如人类身高时，这是合适的。然而，对于某些类型的响应变量，这些假设是不合适的。例如，在响应变量预期始终为正且变化范围很广的情况下，常数输入变化会导致几何变化（即指数变化），而不是常数变化。
    [...] 同样，预测是/否选择概率（伯努利变量）的模型甚至更不适合作为线性响应模型，因为概率在两端都有界（它们必须在0和1之间）。 [...] 广义线性模型通过允许响应变量具有任意分布（而不是仅仅正态分布），以及响应变量的任意函数（连接函数）与预测值线性变化（而不是假设响应本身必须线性变化）来涵盖所有这些情况。
- en: In its simplest form, a generalized linear model assumes that an outcome variable
    \(y \in \mathbb{R}\) is generated from an exponential family \(p_\theta\), where
    \(\theta \in \mathbb{R}\) is a linear combination of the predictor variables \(\mathbf{x}
    \in \mathbb{R}^d\). That is, we assume that \(\theta = \mathbf{w}^T \mathbf{x}\)
    for unknown \(\mathbf{w} \in \mathbb{R}^d\) and the probability distribution of
    \(y\) is of the form
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，广义线性模型假设结果变量 \(y \in \mathbb{R}\) 是由指数族 \(p_\theta\) 生成的，其中 \(\theta
    \in \mathbb{R}\) 是预测变量 \(\mathbf{x} \in \mathbb{R}^d\) 的线性组合。也就是说，我们假设 \(\theta
    = \mathbf{w}^T \mathbf{x}\) 对于未知的 \(\mathbf{w} \in \mathbb{R}^d\)，并且 \(y\) 的概率分布形式为
- en: \[ p_{\mathbf{w}^T \mathbf{x}}(y) = h(y) \exp\left((\mathbf{w}^T\mathbf{x})
    \,\phi(y) - A(\mathbf{w}^T \mathbf{x})\right) \]
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\mathbf{w}^T \mathbf{x}}(y) = h(y) \exp\left((\mathbf{w}^T\mathbf{x})
    \,\phi(y) - A(\mathbf{w}^T \mathbf{x})\right) \]
- en: for some sufficient statistic \(\phi(y)\). We further assume that \(A\) is twice
    continuously differentiable over \(\mathbb{R}\).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个充分统计量 \(\phi(y)\)。我们进一步假设 \(A\) 在 \(\mathbb{R}\) 上是两次连续可微的。
- en: Given data points \((\mathbf{x}_i,y_i)_{i=1}^n\), the model is fitted using
    maximum likelihood as follows. Under independence of the samples, the likelihood
    of the data is \(\prod_{i=1}^n p_{\mathbf{w}^T \mathbf{x}_i}(y_i)\), which we
    seek to maximize over \(\mathbf{w}\) (which is different from maximizing over
    \(\theta\)!). As before, we work with the negative log-likelihood, which we denote
    as (with a slight abuse of notation)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据点\((\mathbf{x}_i,y_i)_{i=1}^n\)，模型使用最大似然法进行拟合，如下。在样本独立的情况下，数据的似然是\(\prod_{i=1}^n
    p_{\mathbf{w}^T \mathbf{x}_i}(y_i)\)，我们希望最大化\(\mathbf{w}\)（这与最大化\(\theta\)不同！）。像以前一样，我们处理负对数似然，我们将其表示为（这里有一点符号滥用）
- en: \[ L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n \log p_{\mathbf{w}^T
    \mathbf{x}_i}(y_i). \]
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n \log p_{\mathbf{w}^T
    \mathbf{x}_i}(y_i). \]
- en: The gradient with respect to \(\mathbf{w}\) is given by
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 关于\(\mathbf{w}\)的梯度由以下给出
- en: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \nabla_\mathbf{w} \log\left[ h(y_i) \exp\left(\mathbf{w}^T \mathbf{x}_i
    \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right)\right]\\ &= - \sum_{i=1}^n \nabla_\mathbf{w}
    \left[\log h(y_i) + \mathbf{w}^T \mathbf{x}_i \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right]\\
    &= - \sum_{i=1}^n \left[ \mathbf{x}_i \phi(y_i) - \nabla_\mathbf{w} A(\mathbf{w}^T
    \mathbf{x}_i)\right]. \end{align*}\]
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \nabla_\mathbf{w} \log\left[ h(y_i) \exp\left(\mathbf{w}^T \mathbf{x}_i
    \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right)\right]\\ &= - \sum_{i=1}^n \nabla_\mathbf{w}
    \left[\log h(y_i) + \mathbf{w}^T \mathbf{x}_i \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right]\\
    &= - \sum_{i=1}^n \left[ \mathbf{x}_i \phi(y_i) - \nabla_\mathbf{w} A(\mathbf{w}^T
    \mathbf{x}_i)\right]. \end{align*}\]
- en: By the *Chain Rule* and our previous formulas,
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 通过链式法则和我们的前公式，
- en: \[ \nabla_\mathbf{w} A(\mathbf{w}^T \mathbf{x}_i) = A'(\mathbf{w}^T \mathbf{x}_i)
    \,\mathbf{x}_i = \mu(\mathbf{w}; \mathbf{x}_i) \,\mathbf{x}_i \]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_\mathbf{w} A(\mathbf{w}^T \mathbf{x}_i) = A'(\mathbf{w}^T \mathbf{x}_i)
    \,\mathbf{x}_i = \mu(\mathbf{w}; \mathbf{x}_i) \,\mathbf{x}_i \]
- en: where \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{w}^T
    \mathbf{x}_i}\). That is,
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\)，\(Y_i \sim p_{\mathbf{w}^T
    \mathbf{x}_i}\)。也就是说，
- en: \[ \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n
    \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i)). \]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n
    \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i)). \]
- en: The Hessian of \(A(\mathbf{w}^T \mathbf{x}_i)\), again by the *Chain Rule* and
    our previous formulas, is
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过链式法则和我们的前公式，\(A(\mathbf{w}^T \mathbf{x}_i)\)的Hessian是
- en: \[ A''(\mathbf{w}^T \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T = \sigma^2 (\mathbf{w};
    \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T \]
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A''(\mathbf{w}^T \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T = \sigma^2 (\mathbf{w};
    \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T \]
- en: where \(\sigma^2(\mathbf{w}; \mathbf{x}_i) = \mathrm{K}_{\phi(Y_i), \phi(Y_i)}
    = \var[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{w}^T \mathbf{x}_i}\). So the Hessian
    of the negative log-likelihood is
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(\sigma^2(\mathbf{w}; \mathbf{x}_i) = \mathrm{K}_{\phi(Y_i), \phi(Y_i)} =
    \var[\phi(Y_i)]\)，\(Y_i \sim p_{\mathbf{w}^T \mathbf{x}_i}\)。因此，负对数似然的Hessian是
- en: \[ \mathbf{H}_{L_n}(\mathbf{w}) = \sum_{i=1}^n \sigma^2(\mathbf{w}; \mathbf{x}_i)
    \,\mathbf{x}_i \mathbf{x}_i^T \]
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{H}_{L_n}(\mathbf{w}) = \sum_{i=1}^n \sigma^2(\mathbf{w}; \mathbf{x}_i)
    \,\mathbf{x}_i \mathbf{x}_i^T \]
- en: which is positive semidefinite (prove it!).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有半正定性的（证明它！）。
- en: As a result, the negative log-likelihood is convex and the maximum likelihood
    estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) solves the equation \(\nabla_\mathbf{w}
    L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = \mathbf{0}\), that is,
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，负对数似然是凸的，最大似然估计器\(\hat{\mathbf{w}}_{\mathrm{MLE}}\)解方程\(\nabla_\mathbf{w}
    L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = \mathbf{0}\)，即，
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i). \]
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i). \]
- en: We revisit linear and logistic regression next.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来回顾线性回归和逻辑回归。
- en: '**EXAMPLE:** **(Linear regression)** \(\idx{linear regression}\xdi\) Consider
    the case where \(p_\theta\) is a univariate Gaussian with mean \(\theta\) and
    fixed variance \(1\). That is,'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(线性回归)** \(\idx{线性回归}\xdi\) 考虑\(p_\theta\)是一个均值为\(\theta\)且方差固定的单变量高斯分布的情况。也就是说，'
- en: \[\begin{align*} p_{\theta}(y) &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{(y
    - \theta)^2}{2}\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{1}{2}[y^2
    - 2 y \theta + \theta^2]\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{y^2}{2}\right)
    \exp\left(\theta y - \frac{\theta^2}{2}\right)\\ &= h(y) \exp\left(\theta \phi(y)
    - A(\theta)\right), \end{align*}\]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_{\theta}(y) &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{(y
    - \theta)^2}{2}\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{1}{2}[y^2
    - 2 y \theta + \theta^2]\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{y^2}{2}\right)
    \exp\left(\theta y - \frac{\theta^2}{2}\right)\\ &= h(y) \exp\left(\theta \phi(y)
    - A(\theta)\right), \end{align*}\]
- en: where \(\phi(y) = y\) and \(A(\theta) = \theta^2/2\). We now assume that \(\theta
    = \mathbf{x}^T \mathbf{w}\) to obtain the corresponding generalized linear model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，\(\phi(y) = y\) 且 \(A(\theta) = \theta^2/2\)。我们现在假设 \(\theta = \mathbf{x}^T
    \mathbf{w}\) 以获得相应的广义线性模型。
- en: Given data points \((\mathbf{x}_i,y_i)_{i=1}^n\), recall that the maximum likelihood
    estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) solves the equation
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据点 \((\mathbf{x}_i,y_i)_{i=1}^n\)，回忆一下最大似然估计量 \(\hat{\mathbf{w}}_{\mathrm{MLE}}\)
    解决了以下方程
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
- en: where \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\). Here \(\E[\phi(Y_i)] = \E[Y_i] = \mathbf{x}_i^T \mathbf{w}\). So
    the equation reduces to
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\)，且 \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\)。这里 \(\E[\phi(Y_i)] = \E[Y_i] = \mathbf{x}_i^T \mathbf{w}\)。因此，方程简化为
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T \mathbf{w} = \sum_{i=1}^n \mathbf{x}_i
    y_i. \]
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T \mathbf{w} = \sum_{i=1}^n \mathbf{x}_i
    y_i. \]
- en: You may not recognize this equation, but we have encountered it before in a
    different form. Let \(A\) be the matrix with row \(i\) equal to \(\mathbf{x}_i\)
    and let \(\mathbf{y}\) be the vector with \(i\)-th entry equal to \(y_i\). Then
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不认识这个方程，但我们之前以不同的形式遇到过它。设 \(A\) 为行 \(i\) 等于 \(\mathbf{x}_i\) 的矩阵，设 \(\mathbf{y}\)
    为第 \(i\) 个元素等于 \(y_i\) 的向量。那么
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T = A^T A \qquad \text{and} \qquad
    \sum_{i=1}^n \mathbf{x}_i y_i = A^T \mathbf{y} \]
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T = A^T A \qquad \text{和} \qquad \sum_{i=1}^n
    \mathbf{x}_i y_i = A^T \mathbf{y} \]
- en: as can be checked entry by entry or by using our previous characterizations
    of matrix-matrix products (in outer-product form) and matrix-vector products (as
    linear combinations of columns). Therefore, the equation above is equivalent to
    \(A^T A \mathbf{w} = A^T \mathbf{y}\) - the normal equations of linear regression.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 可以逐项检查或使用我们之前对矩阵-矩阵乘积（外积形式）和矩阵-向量乘积（作为列的线性组合）的描述来验证。因此，上述方程等价于 \(A^T A \mathbf{w}
    = A^T \mathbf{y}\) - 线性回归的正则方程。
- en: To make sense of this finding, we look back at the minus log-likelihood
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个发现，我们回顾一下负对数似然
- en: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\frac{1}{\sqrt{2
    \pi}} \exp\left(- \frac{(y_i - \mathbf{x}_i^T \mathbf{w})^2}{2}\right)\right)\\
    &= - \log (\sqrt{2 \pi}) + \frac{1}{2} \sum_{i=1}^n (y_i - \mathbf{x}_i^T \mathbf{w})^2.
    \end{align*}\]
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\frac{1}{\sqrt{2
    \pi}} \exp\left(- \frac{(y_i - \mathbf{x}_i^T \mathbf{w})^2}{2}\right)\right)\\
    &= - \log (\sqrt{2 \pi}) + \frac{1}{2} \sum_{i=1}^n (y_i - \mathbf{x}_i^T \mathbf{w})^2.
    \end{align*}\]
- en: Observe that minimizing this expression over \(\mathbf{w}\) is equivalent to
    solving the least-squares problem as the first term does not depend on \(\mathbf{w}\)
    and the factor of \(1/2\) does not affect the optimum.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在 \(\mathbf{w}\) 上最小化这个表达式等价于求解最小二乘问题，因为第一项不依赖于 \(\mathbf{w}\)，而 \(1/2\)
    的因子不影响最优解。
- en: While we have rederived the least squares problem from a probabilistic model,
    it should be noted that the Gaussian assumption is not in fact required for linear
    regression to be warranted. Rather, it gives a different perspective on the same
    problem. \(\lhd\)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经从概率模型重新推导出最小二乘问题，但应该注意的是，高斯假设实际上并不是线性回归所必需的。相反，它为相同的问题提供了不同的视角。 \(\lhd\)
- en: '**EXAMPLE:** **(Logistic regression)** \(\idx{logistic regression}\xdi\) Consider
    the case where \(p_{\theta}\) is a Bernoulli distribution. That is, for \(y \in
    \{0,1\}\),'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(逻辑回归)** \(\idx{logistic regression}\xdi\) 考虑 \(p_{\theta}\) 是伯努利分布的情况。也就是说，对于
    \(y \in \{0,1\}\)，'
- en: \[\begin{align*} p_{\theta}(y) &= h(y) \exp(\theta \,\phi(y) - A(\theta)), \end{align*}\]
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_{\theta}(y) &= h(y) \exp(\theta \,\phi(y) - A(\theta)), \end{align*}\]
- en: where \(h(y) \equiv 1\), \(\phi(y) = y\) and \(A(\theta) = \log(1 + e^\theta)\).
    We assume that \(\theta = \mathbf{x}^T \mathbf{w}\) to obtain the corresponding
    generalized linear model. Given data points \((\mathbf{x}_i,y_i)_{i=1}^n\), the
    maximum likelihood estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) solves the equation
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(h(y) \equiv 1\)，\(\phi(y) = y\) 和 \(A(\theta) = \log(1 + e^\theta)\)。我们假设
    \(\theta = \mathbf{x}^T \mathbf{w}\) 以获得相应的广义线性模型。给定数据点 \((\mathbf{x}_i,y_i)_{i=1}^n\)，最大似然估计量
    \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) 解方程
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
- en: where \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\). Here, by our formula for the gradient of \(A\),
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\)，且 \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\)。在这里，根据 \(A\) 的梯度公式，
- en: \[ \E[\phi(Y_i)] = \E[Y_i] = A'(\mathbf{x}_i^T \mathbf{w}) = \frac{e^{\mathbf{x}_i^T
    \mathbf{w}}}{1 + e^{\mathbf{x}_i^T \mathbf{w}}} = \sigma(\mathbf{x}_i^T \mathbf{w}),
    \]
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\phi(Y_i)] = \E[Y_i] = A'(\mathbf{x}_i^T \mathbf{w}) = \frac{e^{\mathbf{x}_i^T
    \mathbf{w}}}{1 + e^{\mathbf{x}_i^T \mathbf{w}}} = \sigma(\mathbf{x}_i^T \mathbf{w}),
    \]
- en: where \(\sigma\) is the sigmoid function. So the equation reduces to
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma\) 是 sigmoid 函数。因此，方程简化为
- en: \[ \sum_{i=1}^n \mathbf{x}_i \sigma(\mathbf{x}_i^T \mathbf{w}) = \sum_{i=1}^n
    \mathbf{x}_i y_i. \]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \sigma(\mathbf{x}_i^T \mathbf{w}) = \sum_{i=1}^n
    \mathbf{x}_i y_i. \]
- en: The equation in this case cannot be solved explicitly. Instead we can use gradient
    descent, or a variant, to minimize the negative log-likelihood directly. The lattter
    is
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，方程不能显式求解。相反，我们可以使用梯度下降或其变体来直接最小化负对数似然。后者是
- en: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\exp((\mathbf{x}_i^T
    \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}}))\right)\\ &= - \sum_{i=1}^n
    \left[(\mathbf{x}_i^T \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\
    &= - \sum_{i=1}^n \left[y_i \log(e^{\mathbf{x}_i^T \mathbf{w}}) - (y_i + (1-y_i))\log(1
    + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\ &= - \sum_{i=1}^n \left[y_i \log(\sigma(\mathbf{x}_i^T
    \mathbf{w})) + (1-y_i) \log(1 -\sigma(\mathbf{x}_i^T \mathbf{w}))\right]. \end{align*}\]
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\exp((\mathbf{x}_i^T
    \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}}))\right)\\ &= - \sum_{i=1}^n
    \left[(\mathbf{x}_i^T \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\
    &= - \sum_{i=1}^n \left[y_i \log(e^{\mathbf{x}_i^T \mathbf{w}}) - (y_i + (1-y_i))\log(1
    + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\ &= - \sum_{i=1}^n \left[y_i \log(\sigma(\mathbf{x}_i^T
    \mathbf{w})) + (1-y_i) \log(1 -\sigma(\mathbf{x}_i^T \mathbf{w}))\right]. \end{align*}\]
- en: Minimizing \(L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})\) is equivalent
    to logistic regression.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化 \(L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})\) 等价于逻辑回归。
- en: To use gradient descent, we compute
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用梯度下降，我们计算
- en: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i))\\ &=
    - \sum_{i=1}^n \mathbf{x}_i (y_i - \sigma(\mathbf{x}_i^T \mathbf{w})). \end{align*}\]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i))\\ &=
    - \sum_{i=1}^n \mathbf{x}_i (y_i - \sigma(\mathbf{x}_i^T \mathbf{w})). \end{align*}\]
- en: This expression is indeed consistent with what we previously derived for logistic
    regression. \(\lhd\)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式确实与我们之前推导出的逻辑回归结果一致。\(\lhd\)
- en: '**CHAT & LEARN** Generalized linear models can be extended to handle more complex
    data structures. Ask your favorite AI chatbot to explain what generalized additive
    models (GAMs) are and how they differ from generalized linear models. Also, ask
    about some common applications of GAMs. \(\ddagger\)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 广义线性模型可以扩展以处理更复杂的数据结构。请你的心仪 AI 聊天机器人解释广义加性模型（GAMs）是什么以及它们与广义线性模型有何不同。还要了解
    GAMs 的常见应用。\(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由 Claude、Gemini 和 ChatGPT 协助)*'
- en: '**1** Which of the following is NOT an example of an exponential family of
    distributions?'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 以下哪项不是指数族分布的例子？'
- en: a) Bernoulli
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: a) 伯努利
- en: b) Categorical
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: b) 类别型
- en: c) Uniform
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: c) 均匀分布
- en: d) Multivariate Gaussian
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: d) 多变量高斯分布
- en: '**2** In the exponential family form \(p_{\boldsymbol{\theta}}(\mathbf{x})
    = h(\mathbf{x}) \exp(\boldsymbol{\theta}^T \boldsymbol{\phi}(\mathbf{x}) - A(\boldsymbol{\theta}))\),
    what does \(A(\boldsymbol{\theta})\) represent?'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在指数族形式 \(p_{\boldsymbol{\theta}}(\mathbf{x}) = h(\mathbf{x}) \exp(\boldsymbol{\theta}^T
    \boldsymbol{\phi}(\mathbf{x}) - A(\boldsymbol{\theta}))\) 中，\(A(\boldsymbol{\theta})\)
    代表什么？'
- en: a) The sufficient statistic
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: a) 充分统计量
- en: b) The log-partition function
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: b) 对数配分函数
- en: c) The canonical parameter
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: c) 标准参数
- en: d) The base measure
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: d) 基础测度
- en: '**3** Given \(n\) independent samples \(X_1, \ldots, X_n\) from a parametric
    family \(p_{\boldsymbol{\theta}^*}\) with unknown \(\boldsymbol{\theta}^* \in
    \Theta\), the maximum likelihood estimator \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}\)
    is defined as:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 给定从参数族 \(p_{\boldsymbol{\theta}^*}\) 中抽取的 \(n\) 个独立样本 \(X_1, \ldots,
    X_n\)，其中 \(\boldsymbol{\theta}^* \in \Theta\) 是未知的，最大似然估计量 \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}\)
    定义为：'
- en: 'a) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: 'b) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: 'c) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: 'd) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: '**4** In a generalized linear model, the maximum likelihood estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\)
    solves the equation:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在广义线性模型中，最大似然估计量 \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) 解决以下方程：'
- en: a) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    \mathbf{x}_i \phi(y_i)\)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    \mathbf{x}_i \phi(y_i)\)
- en: b) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    y_i \phi(\mathbf{x}_i)\)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    y_i \phi(\mathbf{x}_i)\)
- en: c) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \phi(y_i)\)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \phi(y_i)\)
- en: d) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n y_i\)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n y_i\)
- en: '**5** In logistic regression, which distribution is used for the outcome variable?'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在逻辑回归中，用于结果变量的分布是什么？'
- en: a) Normal distribution
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: a) 正态分布
- en: b) Poisson distribution
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: b) 泊松分布
- en: c) Bernoulli distribution
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: c) 伯努利分布
- en: d) Exponential distribution
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: d) 指数分布
- en: 'Answer for 1: c. Justification: The text provides examples of Bernoulli, categorical,
    and multivariate Gaussian distributions as members of the exponential family.
    The uniform distribution, however, does not fit the exponential family form.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 1题的答案：c. 理由：文本提供了伯努利、分类和多元高斯分布作为指数族成员的例子。然而，均匀分布不符合指数族形式。
- en: 'Answer for 2: b. Justification: The text states that \(A(\boldsymbol{\theta})
    = \log Z(\boldsymbol{\theta})\), where \(Z(\boldsymbol{\theta})\) is referred
    to as the partition function.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 2题的答案：b. 理由：文本中提到 \(A(\boldsymbol{\theta}) = \log Z(\boldsymbol{\theta})\)，其中
    \(Z(\boldsymbol{\theta})\) 被称为配分函数。
- en: 'Answer for 3: a. Justification: The text provides the definition of the maximum
    likelihood estimator as \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max
    \left\{ \prod_{i=1}^n p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta
    \right\}\).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '3题的答案：a. 理由：文本提供了最大似然估计量的定义，即 \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in
    \arg\max \left\{ \prod_{i=1}^n p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta}
    \in \Theta \right\}\)。'
- en: 'Answer for 4: a. Justification: The text derives the equation \(\sum_{i=1}^n
    \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i \phi(y_i)\)
    as the one that the maximum likelihood estimator solves in a generalized linear
    model.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 4题的答案：a. 理由：文本推导出方程 \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i)
    = \sum_{i=1}^n \mathbf{x}_i \phi(y_i)\)，这是广义线性模型中最大似然估计量所解决的方程。
- en: 'Answer for 5: c. Justification: The text describes logistic regression as a
    GLM where the outcome variable is assumed to follow a Bernoulli distribution.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 5题的答案：c. 理由：文本描述逻辑回归为一种GLM，其中结果变量被假设遵循伯努利分布。
- en: 6.2.1\. Exponential family[#](#exponential-family "Link to this heading")
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.1\. 指数族[#](#exponential-family "链接到这个标题")
- en: One particularly useful class of probability distributions in data science is
    the [exponential family](https://en.wikipedia.org/wiki/Exponential_family#Vector_parameter),
    which includes many well-known cases.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中特别有用的一类概率分布是 [指数族](https://en.wikipedia.org/wiki/Exponential_family#Vector_parameter)，它包括许多已知的案例。
- en: '**DEFINITION** **(Exponential Family - Discrete Case)** \(\idx{exponential
    family}\xdi\) A parametric collection of probability distributions \(\{p_{\btheta}:\btheta
    \in \Theta\}\) over a discrete space \(\S\) is an exponential family if it can
    be written in the form'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义：** **（指数族 - 离散情况）** \(\idx{exponential family}\xdi\) 一个参数化的概率分布集合 \(\{p_{\btheta}:\btheta
    \in \Theta\}\) 在离散空间 \(\S\) 上是指数族，如果它可以写成以下形式'
- en: \[ p_{\btheta}(\mathbf{x}) = \frac{1}{Z(\btheta)} h(\mathbf{x}) \exp\left(\btheta^T
    \bphi(\mathbf{x})\right) \]
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = \frac{1}{Z(\btheta)} h(\mathbf{x}) \exp\left(\btheta^T
    \bphi(\mathbf{x})\right) \]
- en: 'where \(\btheta \in \mathbb{R}^m\) are the canonical parameters, \(\bphi :
    \S \to \mathbb{R}^m\) are the sufficient statistics and \(Z(\btheta)\) is the
    partition function\(\idx{partition function}\xdi\). It is often convenient to
    introduce the log-partition function\(\idx{log-partition function}\xdi\) \(A(\btheta)
    = \log Z(\btheta)\) and re-write'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 \(\btheta \in \mathbb{R}^m\) 是标准参数，\(\bphi : \S \to \mathbb{R}^m\) 是充分统计量，\(Z(\btheta)\)
    是配分函数 \(\idx{partition function}\xdi\)。通常方便引入对数配分函数 \(\idx{log-partition function}\xdi\)
    \(A(\btheta) = \log Z(\btheta)\) 并重新写成'
- en: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right). \]
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right). \]
- en: \(\natural\)
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: \(\natural\)
- en: '**EXAMPLE:** **(Bernoulli, continued)** For \(x \in \{0,1\}\), the \(\mathrm{Ber}(q)\)
    distribution for \(0 < q < 1\) can be written as'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（伯努利，继续）** 对于 \(x \in \{0,1\}\)，当 \(0 < q < 1\) 时，\(\mathrm{Ber}(q)\)
    分布可以写成'
- en: \[\begin{align*} q^{x} (1-q)^{1-x} &= (1-q) \left(\frac{q}{1-q}\right)^x\\ &=
    (1-q) \exp\left[x \log \left(\frac{q}{1-q}\right)\right]\\ &= \frac{1}{Z(\theta)}
    h(x) \exp(\theta \,\phi(x)) \end{align*}\]
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} q^{x} (1-q)^{1-x} &= (1-q) \left(\frac{q}{1-q}\right)^x\\ &=
    (1-q) \exp\left[x \log \left(\frac{q}{1-q}\right)\right]\\ &= \frac{1}{Z(\theta)}
    h(x) \exp(\theta \,\phi(x)) \end{align*}\]
- en: where we define \(h(x) \equiv 1\), \(\phi(x) = x\), \(\theta = \log \left(\frac{q}{1-q}\right)\)
    and, since \(Z(\theta)\) serves as the normalization constant in \(p_\theta\),
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(h(x) \equiv 1\)，\(\phi(x) = x\)，\(\theta = \log \left(\frac{q}{1-q}\right)\)，并且由于
    \(Z(\theta)\) 作为 \(p_\theta\) 中的规范化常数，
- en: \[ Z(\theta) = \sum_{x \in \{0,1\}} h(x) \exp(\theta \,\phi(x)) = 1 + e^\theta.
    \]
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z(\theta) = \sum_{x \in \{0,1\}} h(x) \exp(\theta \,\phi(x)) = 1 + e^\theta.
    \]
- en: \(\lhd\)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: The following is an important generalization. Recall that i.i.d. is the abbreviation
    for independent and identically distributed. We use the convention \(0! = 1\).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个重要的推广。回忆一下，i.i.d. 是独立同分布的缩写。我们使用约定 \(0! = 1\)。
- en: '**EXAMPLE:** **(Categorical and Multinomial)** A categorical variable\(\idx{categorical}\xdi\)
    \(\mathbf{Y}\) takes \(K \geq 2\) possible values. A standard choice is to use
    one-hot encoding\(\idx{one-hot encoding}\xdi\) \(\S = \{\mathbf{e}_i : i=1,\ldots,K\}\)
    where \(\mathbf{e}_i\) is the \(i\)-th canonical basis in \(\mathbb{R}^K\). The
    distribution is specified by setting the probabilities \(\bpi = (\pi_1,\ldots,\pi_K)\)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（分类和多项式）** 一个分类变量 \(\idx{categorical}\xdi\) \(\mathbf{Y}\) 取 \(K
    \geq 2\) 个可能的值。一个标准的选择是使用独热编码 \(\idx{one-hot encoding}\xdi\) \(\S = \{\mathbf{e}_i
    : i=1,\ldots,K\}\)，其中 \(\mathbf{e}_i\) 是 \(\mathbb{R}^K\) 中的第 \(i\) 个标准基。分布由设置概率
    \(\bpi = (\pi_1,\ldots,\pi_K)\) 来指定'
- en: \[ \pi_i = \P[\mathbf{Y} = \mathbf{e}_i]. \]
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \pi_i = \P[\mathbf{Y} = \mathbf{e}_i]. \]
- en: We denote this \(\mathbf{Y} \sim \mathrm{Cat}(\bpi)\) and we assume \(\pi_i
    > 0\) for all \(i\).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表示 \(\mathbf{Y} \sim \mathrm{Cat}(\bpi)\)，并假设对所有 \(i\)，\(\pi_i > 0\)。
- en: To see that this is an exponential family, write the probability mass function
    at \(\mathbf{x} = (x_1,\ldots,x_K)\) as
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看出这是一个指数族，将概率质量函数在 \(\mathbf{x} = (x_1,\ldots,x_K)\) 处写成
- en: \[ \prod_{i=1}^K \pi_i^{x_i} = \exp\left(\sum_{i=1}^K x_i \log \pi_i \right).
    \]
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \prod_{i=1}^K \pi_i^{x_i} = \exp\left(\sum_{i=1}^K x_i \log \pi_i \right).
    \]
- en: So we can take \(h(\mathbf{x}) \equiv 1\), \(\btheta = (\log \pi_i)_{i=1}^K\),
    \(\bphi(\mathbf{x}) = (x_i)_{i=1}^K\) and \(Z(\btheta) \equiv 1\).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以取 \(h(\mathbf{x}) \equiv 1\)，\(\btheta = (\log \pi_i)_{i=1}^K\)，\(\bphi(\mathbf{x})
    = (x_i)_{i=1}^K\) 和 \(Z(\btheta) \equiv 1\)。
- en: The [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution)\(\idx{multinomial}\xdi\)
    arises as a sum of independent categorical variables. Let \(n \geq 1\) be the
    number of trials (or samples) and let \(\mathbf{Y}_1,\ldots,\mathbf{Y}_n\) be
    i.i.d. \(\mathrm{Cat}(\bpi)\). Define \(\mathbf{X} = \sum_{i=1}^n \mathbf{Y}_i\).
    The probability mass function of \(\mathbf{X}\) at
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[多项式分布](https://en.wikipedia.org/wiki/Multinomial_distribution)\(\idx{multinomial}\xdi\)
    作为独立分类变量的和而出现。设 \(n \geq 1\) 为试验次数（或样本数），设 \(\mathbf{Y}_1,\ldots,\mathbf{Y}_n\)
    是 i.i.d. \(\mathrm{Cat}(\bpi)\)。定义 \(\mathbf{X} = \sum_{i=1}^n \mathbf{Y}_i\)。在
    \(\mathbf{X}\) 处的概率质量函数为'
- en: '\[ \mathbf{x} = (x_1,\ldots,x_K) \in \left\{ \mathbf{x} \in \{0,1,\ldots,n\}^K
    : \sum_{i=1}^K x_i = n \right\}=: \S \]'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \mathbf{x} = (x_1,\ldots,x_K) \in \left\{ \mathbf{x} \in \{0,1,\ldots,n\}^K
    : \sum_{i=1}^K x_i = n \right\}=: \S \]'
- en: is
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: \[ \frac{n!}{x_1!\cdots x_K!} \prod_{i=1}^K \pi_i^{x_i} = \frac{n!}{x_1!\cdots
    x_K!} \exp\left(\sum_{i=1}^K x_i \log \pi_i\right) \]
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{n!}{x_1!\cdots x_K!} \prod_{i=1}^K \pi_i^{x_i} = \frac{n!}{x_1!\cdots
    x_K!} \exp\left(\sum_{i=1}^K x_i \log \pi_i\right) \]
- en: and we can take \(h(\mathbf{x}) = \frac{n!}{x_1!\cdots x_K!}\), \(\btheta =
    (\log \pi_i)_{i=1}^K\), \(\bphi(\mathbf{x}) = (x_i)_{i=1}^K\) and \(Z(\btheta)
    \equiv 1\). This is an exponential family if we think of \(n\) as fixed.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以取 \(h(\mathbf{x}) = \frac{n!}{x_1!\cdots x_K!}\)，\(\btheta = (\log \pi_i)_{i=1}^K\)，\(\bphi(\mathbf{x})
    = (x_i)_{i=1}^K\) 和 \(Z(\btheta) \equiv 1\)。如果我们把 \(n\) 视为固定值，这是一个指数族。
- en: We use the notation \(\mathbf{X} \sim \mathrm{Mult}(n, \bpi)\). \(\lhd\)
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用符号 \(\mathbf{X} \sim \mathrm{Mult}(n, \bpi)\). \(\lhd\)
- en: While we have focused so far on discrete distributions, one can adapt the definitions
    above by replacing mass functions with density functions. We give two important
    examples.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们迄今为止一直关注离散分布，但可以通过用密度函数替换质量函数来调整上述定义。我们给出两个重要的例子。
- en: We need some definitions for our first example.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为我们的第一个例子定义一些内容。
- en: 'The [trace](https://en.wikipedia.org/wiki/Trace_%28linear_algebra%29)\(\idx{trace}\xdi\)
    of a square matrix \(A \in \mathbb{R}^{d \times d}\), denoted \(\mathrm{tr}(A)\),
    is the sum of its diagonal entries. We will need the following trace identity
    whose proof we leave as an exercise: \(\mathrm{tr}(ABC) = \mathrm{tr}(CAB) = \mathrm{tr}(BCA)\)
    for any matrices \(A, B, C\) for which \(AB\), \(BC\) and \(CA\) are well-defined.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(A \in \mathbb{R}^{d \times d}\) 的迹，记为 \(\mathrm{tr}(A)\)，是其对角线元素的和。我们需要以下迹恒等式，其证明留作练习：\(\mathrm{tr}(ABC)
    = \mathrm{tr}(CAB) = \mathrm{tr}(BCA)\) 对于任何矩阵 \(A, B, C\)，其中 \(AB\)，\(BC\) 和
    \(CA\) 是有定义的。
- en: The [determinant](https://en.wikipedia.org/wiki/Determinant)\(\idx{determinant}\xdi\)
    of a square matrix \(A\) is denoted by \(|A|\). For our purposes, it will be enough
    to consider symmetric, positive semidefinite matrices for which the determinant
    is the product of the eigenvalues (with repeats). Recall that we proved that the
    sequence of eigenvalues (with repeats) of a symmetric matrix is unique (in the
    sense that any two spectral decomposition have the same sequence of eigenvalues).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(A\) 的行列式记为 \(|A|\)。对于我们的目的，考虑对称的半正定矩阵就足够了，其行列式是特征值（包括重复）的乘积。回想一下，我们证明了对称矩阵的特征值序列（包括重复）是唯一的（在意义上，任何两个谱分解都有相同的特征值序列）。
- en: A symmetric, positive definite matrix \(A \in \mathbb{R}^{d \times d}\) is necessarily
    invertible. Indeed, it has a spectral decomposition
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对称的正定矩阵 \(A \in \mathbb{R}^{d \times d}\) 必然是可逆的。实际上，它有一个特征分解
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
- en: where \(\lambda_1 \geq \cdots \geq \lambda_d > 0\) and \(\mathbf{q}_1, \ldots,
    \mathbf{q}_d\) are orthonormal. Then
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\lambda_1 \geq \cdots \geq \lambda_d > 0\) 且 \(\mathbf{q}_1, \ldots, \mathbf{q}_d\)
    是正交归一的。然后
- en: \[ A^{-1} = Q \Lambda^{-1} Q^T. \]
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{-1} = Q \Lambda^{-1} Q^T. \]
- en: To see this, note that
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这一点，请注意
- en: \[ A A^{-1} = Q \Lambda Q^T Q \Lambda^{-1} Q^T = Q Q^T = I_{d \times d}. \]
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A A^{-1} = Q \Lambda Q^T Q \Lambda^{-1} Q^T = Q Q^T = I_{d \times d}. \]
- en: The last equality follows from the fact that \(Q Q^T\) is the orthogonal projection
    on the orthonormal basis \(\mathbf{q}_1,\ldots,\mathbf{q}_d\). Similarly, \(A^{-1}
    A = I_{d \times d}\).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的等式来自于 \(Q Q^T\) 是在正交归一基 \(\mathbf{q}_1,\ldots,\mathbf{q}_d\) 上的正交投影。同样，\(A^{-1}
    A = I_{d \times d}\)。
- en: '**EXAMPLE:** **(Multivariate Gaussian)** \(\idx{multivariate normal}\xdi\)
    A multivariate Gaussian\(\idx{multivariate Gaussian}\xdi\) vector \(\mathbf{X}
    = (X_1,\ldots,X_d)\) on \(\mathbb{R}^d\) with mean \(\bmu \in \mathbb{R}^d\) and
    positive definite covariance matrix \(\bSigma \in \mathbb{R}^{d \times d}\) has
    probability density function'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(Multivariate Gaussian)** \(\idx{multivariate normal}\xdi\)
    一个多元高斯\(\idx{multivariate Gaussian}\xdi\)向量 \(\mathbf{X} = (X_1,\ldots,X_d)\)
    在 \(\mathbb{R}^d\) 上，均值为 \(\bmu \in \mathbb{R}^d\)，协方差矩阵为正定的 \(\bSigma \in \mathbb{R}^{d
    \times d}\)，其概率密度函数为'
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right).
    \]
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right).
    \]
- en: We use the notation \(\mathbf{X} \sim N_d(\bmu, \bSigma)\).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用符号 \(\mathbf{X} \sim N_d(\bmu, \bSigma)\)。
- en: It can be shown that indeed the mean is
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，确实均值是
- en: \[ \E[\mathbf{X}] = \bmu \]
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\mathbf{X}] = \bmu \]
- en: and the covariance matrix is
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵是
- en: \[ \E[(\mathbf{X} - \bmu)(\mathbf{X} - \bmu)^T] = \E[\mathbf{X} \mathbf{X}^T]
    - \bmu \bmu^T = \bSigma. \]
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[(\mathbf{X} - \bmu)(\mathbf{X} - \bmu)^T] = \E[\mathbf{X} \mathbf{X}^T]
    - \bmu \bmu^T = \bSigma. \]
- en: In the bivariate\(\idx{bivariate Gaussian}\xdi\) case (i.e., when \(d = 2\))\(\idx{bivariate
    normal}\xdi\), the covariance matrix reduces to
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元\(\idx{bivariate Gaussian}\xdi\)情况（即当 \(d = 2\)）\(\idx{bivariate normal}\xdi\)，协方差矩阵简化为
- en: \[\begin{split} \bSigma = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2
    \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bSigma = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2
    \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
- en: where \(\sigma_1^2\) and \(\sigma_2^2\) are the respective variances of \(X_1\)
    and \(X_2\), and
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_1^2\) 和 \(\sigma_2^2\) 分别是 \(X_1\) 和 \(X_2\) 的方差，并且
- en: \[ \rho = \frac{\mathrm{Cov}[X_1,X_2]}{\sigma_1 \sigma_2} \]
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \rho = \frac{\mathrm{Cov}[X_1,X_2]}{\sigma_1 \sigma_2} \]
- en: is the correlation coefficient. Recall that, by the *Cauchy-Schwarz inequality*,
    it lies in \([-1,1]\).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 是相关系数。回想一下，根据 *柯西-施瓦茨不等式*，它位于 \([-1,1]\) 之间。
- en: Rewriting the density as
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 将密度重新写为
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{e^{-(1/2) \bmu^T \bSigma^{-1} \bmu}}{(2\pi)^{d/2}
    \,|\bSigma|^{1/2}} \exp\left(- \mathbf{x}^T \bSigma^{-1}\bmu - \frac{1}{2} \mathrm{tr}\left(\mathbf{x}
    \mathbf{x}^T \bSigma^{-1}\right)\right) \]
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{e^{-(1/2) \bmu^T \bSigma^{-1} \bmu}}{(2\pi)^{d/2}
    \,|\bSigma|^{1/2}} \exp\left(- \mathbf{x}^T \bSigma^{-1}\bmu - \frac{1}{2} \mathrm{tr}\left(\mathbf{x}
    \mathbf{x}^T \bSigma^{-1}\right)\right) \]
- en: where we used the symmetric nature of \(\bSigma^{-1}\) in the first term of
    the exponential and the previous trace identity in the second term. The expression
    in parentheses is linear in the entries of \(\mathbf{x}\) and \(\mathbf{x} \mathbf{x}^T\),
    which can then be taken as sufficient statistics (formally, using [vectorization](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29)).
    Indeed note that
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在指数的第一项中，我们使用了 \(\bSigma^{-1}\) 的对称性质，在第二项中使用了之前的迹恒等式。括号中的表达式在 \(\mathbf{x}\)
    和 \(\mathbf{x} \mathbf{x}^T\) 的条目上是线性的，然后可以作为充分统计量（形式上，使用[向量化](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29)）。确实，请注意
- en: \[ \mathbf{x}^T \bSigma^{-1}\bmu = \sum_{i=1}^d x_i (\bSigma^{-1}\bmu)_i \]
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^T \bSigma^{-1}\bmu = \sum_{i=1}^d x_i (\bSigma^{-1}\bmu)_i \]
- en: and
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathrm{tr}\left(\mathbf{x} \mathbf{x}^T \bSigma^{-1}\right) = \sum_{i =
    1}^d \left(\sum_{j=1}^d (\mathbf{x} \mathbf{x}^T)_{i,j} (\bSigma^{-1})_{j,i}\right)
    = \sum_{i = 1}^d \sum_{j=1}^d x_i x_j (\bSigma^{-1})_{j,i}. \]
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{tr}\left(\mathbf{x} \mathbf{x}^T \bSigma^{-1}\right) = \sum_{i =
    1}^d \left(\sum_{j=1}^d (\mathbf{x} \mathbf{x}^T)_{i,j} (\bSigma^{-1})_{j,i}\right)
    = \sum_{i = 1}^d \sum_{j=1}^d x_i x_j (\bSigma^{-1})_{j,i}. \]
- en: So we can take
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以取
- en: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]\[\begin{align*} \btheta &= \bigg(-(\bSigma^{-1}\bmu)_1,\ldots,-(\bSigma^{-1}\bmu)_d,\\
    &\qquad - \frac{1}{2}(\bSigma^{-1})_{1,1}, \ldots, - \frac{1}{2}(\bSigma^{-1})_{1,d},\\
    &\qquad - \frac{1}{2}(\bSigma^{-1})_{2,1}, \ldots, - \frac{1}{2}(\bSigma^{-1})_{2,d},\\
    &\qquad \ldots, - \frac{1}{2}(\bSigma^{-1})_{d,1}, \ldots,- \frac{1}{2}(\bSigma^{-1})_{d,d}\bigg)
    \end{align*}\]
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]\[\begin{align*} \btheta &= \bigg(-(\bSigma^{-1}\bmu)_1,\ldots,-(\bSigma^{-1}\bmu)_d,\\
    &\qquad - \frac{1}{2}(\bSigma^{-1})_{1,1}, \ldots, - \frac{1}{2}(\bSigma^{-1})_{1,d},\\
    &\qquad - \frac{1}{2}(\bSigma^{-1})_{2,1}, \ldots, - \frac{1}{2}(\bSigma^{-1})_{2,d},\\
    &\qquad \ldots, - \frac{1}{2}(\bSigma^{-1})_{d,1}, \ldots,- \frac{1}{2}(\bSigma^{-1})_{d,d}\bigg)
    \end{align*}\]
- en: and \(h (\mathbf{x}) \equiv 1\). Expressing \(Z(\btheta)\) explicitly is not
    straightforward. But note that \(\btheta\) includes all entries of \(\bSigma^{-1}\),
    from which \(\bSigma\) can be computed (e.g., from [Cramer’s rule](https://en.wikipedia.org/wiki/Cramer%27s_rule#Finding_inverse_matrix)),
    and in turn from which \(\bmu\) can be extracted out of the entries of \(\bSigma^{-1}\bmu\)
    in \(\btheta\). So the normalizing factor \(\frac{(2\pi)^{d/2} \,|\bSigma|^{1/2}}{e^{-(1/2)
    \bmu^T \bSigma^{-1} \bmu}}\) can in principle be expressed in terms of \(\btheta\).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 且 \(h (\mathbf{x}) \equiv 1\). 将 \(Z(\btheta)\) 明确表达出来并不直接。但请注意，\(\btheta\)
    包含了 \(\bSigma^{-1}\) 的所有条目，从中可以计算出 \(\bSigma\)（例如，从[Cramer的规则](https://en.wikipedia.org/wiki/Cramer%27s_rule#Finding_inverse_matrix)），进而从
    \(\bSigma^{-1}\bmu\) 的条目中提取出 \(\bmu\) 在 \(\btheta\) 中。因此，归一化因子 \(\frac{(2\pi)^{d/2}
    \,|\bSigma|^{1/2}}{e^{-(1/2) \bmu^T \bSigma^{-1} \bmu}}\) 在原则上可以用 \(\btheta\)
    来表达。
- en: This shows that the multivariate normal is an exponential family.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明多元正态分布是一个指数族。
- en: The matrix \(\bLambda = \bSigma^{-1}\) is also known as the precision matrix.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 \(\bLambda = \bSigma^{-1}\) 也被称为精度矩阵。
- en: Alternatively, let \(\mathbf{Z}\) be a standard normal \(d\)-vector, let \(\bmu
    \in \mathbb{R}^d\) and let \(\bSigma \in \mathbb{R}^{d \times d}\) be positive
    definite. Then the transformed random variable \(\mathbf{X} = \bmu + \bSigma \mathbf{Z}\)
    is a multivariate Gaussian with mean \(\bmu\) and covariance matrix \(\bSigma\).
    This can be proved using the [change of variables formula](https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function)
    (try it!). \(\lhd\)
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，设 \(\mathbf{Z}\) 为标准正态 \(d\)-向量，设 \(\bmu \in \mathbb{R}^d\)，设 \(\bSigma \in
    \mathbb{R}^{d \times d}\) 为正定矩阵。那么变换后的随机变量 \(\mathbf{X} = \bmu + \bSigma \mathbf{Z}\)
    是具有均值 \(\bmu\) 和协方差矩阵 \(\bSigma\) 的多元高斯分布。这可以通过[变量变换公式](https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function)（试一试！）来证明。
    \(\lhd\)
- en: '**NUMERICAL CORNER:** The following code, which plots the density in the bivariate
    case, was adapted from [gauss_plot_2d.ipynb](https://github.com/probml/pyprobml/blob/master/notebooks/book1/03/gauss_plot_2d.ipynb)
    by ChatGPT.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 下面的代码，用于绘制二元情况的密度图，是由 ChatGPT 从 [gauss_plot_2d.ipynb](https://github.com/probml/pyprobml/blob/master/notebooks/book1/03/gauss_plot_2d.ipynb)
    调整的。'
- en: '**CHAT & LEARN** Ask your favorite AI chatbot to explain the code! Try different
    covariance matrices. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习:** 请你的喜欢的 AI 聊天机器人解释这段代码！尝试不同的协方差矩阵。([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))
    \(\ddagger\)'
- en: '[PRE10]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We plot the density for mean \((0,0)\) with two different covariance matrices:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了均值 \((0,0)\) 的密度，使用了两个不同的协方差矩阵：
- en: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{and} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1
    \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \bSigma_1 = \begin{bmatrix} 1.0 & 0 \\ 0 & 1.0 \end{bmatrix}
    \quad \text{和} \quad \bSigma_2 = \begin{bmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2
    \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{bmatrix} \end{split}\]
- en: where \(\sigma_1 = 1.5\), \(\sigma_2 = 0.5\) and \(\rho = -0.75\).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma_1 = 1.5\)，\(\sigma_2 = 0.5\) 和 \(\rho = -0.75\)。
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE11]</details> ![../../_images/aa8a076cd17782940f0aa5ed62929716a9ad484853e0e195abb9808dc5430649.png](../Images/bd4d757f39ffc955469d96c6ccc28859.png)<details
    class="hide above-input"><summary aria-label="Toggle hidden content">Show code
    cell source Hide code cell source</summary>'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE11]</details> ![../../_images/aa8a076cd17782940f0aa5ed62929716a9ad484853e0e195abb9808dc5430649.png](../Images/bd4d757f39ffc955469d96c6ccc28859.png)<details
    class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>'
- en: '[PRE12]</details> ![../../_images/4a907efb3af995445fe3feb6f564880bfd1d84078a5876676a3758260bc699c8.png](../Images/38909f679dda3fe6c8703a40390160ef.png)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE12]</details> ![../../_images/4a907efb3af995445fe3feb6f564880bfd1d84078a5876676a3758260bc699c8.png](../Images/38909f679dda3fe6c8703a40390160ef.png)'
- en: \(\unlhd\)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: The [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution),
    which we describe next, is a natural probability distribution over probability
    distributions. In particular, it is used in [Bayesian data analysis](https://en.wikipedia.org/wiki/Bayesian_statistics)
    as a [prior](https://en.wikipedia.org/wiki/Prior_probability) on the parameters
    of categorical and multinomial distribution, largely because of a property known
    as [conjuguacy](https://en.wikipedia.org/wiki/Conjugate_prior). We will not describe
    Bayesian approaches here.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[狄利克雷分布](https://en.wikipedia.org/wiki/Dirichlet_distribution)是我们接下来要描述的，它是一种自然概率分布，覆盖概率分布。特别是，它在[贝叶斯数据分析](https://en.wikipedia.org/wiki/Bayesian_statistics)中作为分类和多项式分布参数的[先验](https://en.wikipedia.org/wiki/Prior_probability)，很大程度上是因为一个称为[共轭性](https://en.wikipedia.org/wiki/Conjugate_prior)的性质。我们在这里不会描述贝叶斯方法。'
- en: '**EXAMPLE:** **(Dirichlet)** \(\idx{Dirichlet}\xdi\) The Dirichlet distribution
    is a distribution over the \((K-1)\)-simplex'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** **(狄利克雷)** \(\idx{Dirichlet}\xdi\) 狄利克雷分布是 \((K-1)\)-单纯形的分布'
- en: '\[ \S = \Delta_{K} := \left\{ \mathbf{x} = (x_1, \ldots, x_K) : \mathbf{x}
    \geq \mathbf{0},\ \sum_{i=1}^K x_i = 1 \right\}. \]'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \S = \Delta_{K} := \left\{ \mathbf{x} = (x_1, \ldots, x_K) : \mathbf{x}
    \geq \mathbf{0},\ \sum_{i=1}^K x_i = 1 \right\}. \]'
- en: Its parameters are \(\balpha = (\alpha_1, \ldots, \alpha_K) \in \mathbb{R}\)
    and the density is
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 它的参数是 \(\balpha = (\alpha_1, \ldots, \alpha_K) \in \mathbb{R}\)，密度函数为
- en: \[ f_{\balpha}(\mathbf{x}) = \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1},
    \quad \mathbf{x} \in \Delta_{K} \]
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{\balpha}(\mathbf{x}) = \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1},
    \quad \mathbf{x} \in \Delta_{K} \]
- en: where the normalizing constant \(B(\balpha)\) is the [multivariate Beta function](https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 其中归一化常数 \(B(\balpha)\) 是 [多元Beta函数](https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function)。
- en: Rewriting the density as
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 将密度重新写为
- en: \[ \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1} = \frac{1}{B(\balpha)}
    \frac{1}{\prod_{i=1}^K x_i} \exp\left(\sum_{i=1}^K \alpha_i \log x_i\right) \]
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{B(\balpha)} \prod_{i=1}^K x_i^{\alpha_i-1} = \frac{1}{B(\balpha)}
    \frac{1}{\prod_{i=1}^K x_i} \exp\left(\sum_{i=1}^K \alpha_i \log x_i\right) \]
- en: shows that this is an exponential family with the canonical parameters \(\balpha\)
    and sufficient statistics \((\log x_i)_{i=1}^K\). \(\lhd\)
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 表明这是一个具有规范参数 \(\balpha\) 和充分统计量 \((\log x_i)_{i=1}^K\) 的指数族。 \(\lhd\)
- en: See [here](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions)
    for many more examples. Observe, in particular, that the same distribution can
    have several representations as an exponential family.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [这里](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions)
    可以找到更多示例。特别是，注意同一个分布可以作为指数族有几种表示形式。
- en: '**NUMERICAL CORNER:** In NumPy, as we have seen before, the module [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html)
    provides a way to sample from a variety of standard distributions. We first initialize
    the [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator)\(\idx{pseudorandom
    number generator}\xdi\) with a [random seed](https://en.wikipedia.org/wiki/Random_seed).
    Recall that it allows the results to be reproducible: using the same seed produces
    the same results again.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 在NumPy中，正如我们之前所看到的，模块 `numpy.random` 提供了一种从各种标准分布中进行采样的方法。我们首先使用 [随机种子](https://en.wikipedia.org/wiki/Random_seed)
    初始化 [伪随机数生成器](https://en.wikipedia.org/wiki/Pseudorandom_number_generator)\(\idx{伪随机数生成器}\xdi\)。回想一下，它允许结果可重现：使用相同的种子再次产生相同的结果。'
- en: '[PRE13]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here’s are lists of available [probability distributions](https://numpy.org/doc/stable/reference/random/generator.html#distributions).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了可用的 [概率分布](https://numpy.org/doc/stable/reference/random/generator.html#distributions)。
- en: '[PRE14]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here are a few other examples.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些其他的例子。
- en: '[PRE16]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: \(\unlhd\)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**KNOWLEDGE CHECK:** The Weibull distribution with known shape parameter \(k
    > 0\) takes the following form'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查:** 已知形状参数 \(k > 0\) 的Weibull分布具有以下形式'
- en: \[ f(x; \lambda) = \frac{k}{\lambda} \left(\frac{x}{\lambda}\right)^{k-1} e^{-(x/\lambda)^k},
    \]
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x; \lambda) = \frac{k}{\lambda} \left(\frac{x}{\lambda}\right)^{k-1} e^{-(x/\lambda)^k},
    \]
- en: for \(x \geq 0\), where \(\lambda > 0\).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(x \geq 0\)，其中 \(\lambda > 0\)。
- en: What is the sufficient statistic of its exponential family form?
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 它的指数族形式的充分统计量是什么？
- en: a) \(x\)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(x\)
- en: b) \(\log x\)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\log x\)
- en: c) \(x^{k-1}\)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(x^{k-1}\)
- en: d) \(x^k\)
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(x^k\)
- en: e) \((\log x, x^k)\)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: e) \((\log x, x^k)\)
- en: \(\checkmark\)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: 6.2.2\. Parameter estimation[#](#parameter-estimation "Link to this heading")
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.2\. 参数估计[#](#parameter-estimation "链接到这个标题")
- en: When modeling data via a parametric family of distributions, the parameters
    must be determined from the data itself. In a typical setting, we assume that
    the data comprises \(n\) independent samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    from a parametric family \(p_{\btheta}\) with unknown \(\btheta \in \Theta\).
    Many methods exist for estimating \(\btheta\), depending on the context. Here
    we focus on [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).
    It has many [good theoretical properties](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties)
    which we will not describe here, as well as [drawbacks](https://stats.stackexchange.com/questions/261056/why-does-maximum-likelihood-estimation-have-issues-with-over-fitting).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过分布族对数据进行建模时，参数必须从数据本身确定。在典型情况下，我们假设数据由来自参数族 \(p_{\btheta}\) 的未知 \(\btheta
    \in \Theta\) 的 \(n\) 个独立样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\) 组成。根据上下文，存在许多用于估计
    \(\btheta\) 的方法。在这里，我们关注 [最大似然估计](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)。它具有许多
    [良好的理论特性](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties)，我们在此不进行描述，以及
    [缺点](https://stats.stackexchange.com/questions/261056/why-does-maximum-likelihood-estimation-have-issues-with-over-fitting)。
- en: 'The idea behind maximum likelihood estimation is simple and intuitive: we choose
    the parameter that maximizes the probability of observing the data.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计背后的思想简单直观：我们选择最大化观察数据概率的参数。
- en: '**DEFINITION** **(Maximum likelihood estimator)** \(\idx{maximum likelihood}\xdi\)
    Assume that \(\mathbf{X}_1,\ldots,\mathbf{X}_n\) are \(n\) independent samples
    from a parametric family \(p_{\btheta^*}\) with unknown \(\btheta^* \in \Theta\).
    The maximum likelihood estimator of \(\btheta\) is defined as'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义：** **(最大似然估计器)** \(\idx{maximum likelihood}\xdi\) 假设 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    是来自参数族 \(p_{\btheta^*}\) 的 \(n\) 个独立样本，其中 \(\btheta^* \in \Theta\) 是未知的。\(\btheta\)
    的最大似然估计定义为'
- en: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\max\left\{ \prod_{i=1}^n p_{\btheta}(\mathbf{X}_i)
    \,:\, \btheta \in \Theta \right\}. \]
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\max\left\{ \prod_{i=1}^n p_{\btheta}(\mathbf{X}_i)
    \,:\, \btheta \in \Theta \right\}. \]
- en: It is often useful to work with the negative log-likelihood (NLL)\(\idx{negative
    log-likelihood}\xdi\)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 有时使用负对数似然（NLL）\(\idx{negative log-likelihood}\xdi\) 是很有用的
- en: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i),
    \]
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i),
    \]
- en: in which case we are minimizing. \(\natural\)
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们是在最小化。\(\natural\)
- en: '**EXAMPLE:** **(Biased coin)** Suppose we observe \(n\) coin flips \(X_1,\ldots,
    X_n \in \{0,1\}\) from a biased coin with an unknown probability \(\theta^*\)
    of producing \(1\). We assume the flips are independent. We compute the MLE of
    the parameter \(\theta\).'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(有偏硬币)** 假设我们观察到一个有偏硬币的 \(n\) 次抛掷 \(X_1,\ldots, X_n \in \{0,1\}\)，该硬币产生
    \(1\) 的未知概率为 \(\theta^*\)。我们假设这些抛掷是独立的。我们计算参数 \(\theta\) 的最大似然估计值。'
- en: The definition is
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 定义是
- en: \[ \hat\theta_{\mathrm{MLE}} \in \arg\min\left\{ L_n(\theta; \{X_i\}_{i=1}^n)
    \,:\, \theta \in \Theta = [0,1] \right\} \]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} \in \arg\min\left\{ L_n(\theta; \{X_i\}_{i=1}^n)
    \,:\, \theta \in \Theta = [0,1] \right\} \]
- en: where, using our previous Bernoulli example, the NLL is
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，使用我们之前的伯努利示例，负对数似然函数是
- en: \[\begin{align*} L_n(\theta; \{X_i\}_{i=1}^n) &= - \sum_{i=1}^n \log p_{\theta}(X_i)\\
    &= - \sum_{i=1}^n \log \left[\theta^{X_i} (1- \theta)^{1 -X_i}\right]\\ &= - \sum_{i=1}^n
    \left[ X_i \log \theta + (1 -X_i) \log (1- \theta)\right]. \end{align*}\]
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\theta; \{X_i\}_{i=1}^n) &= - \sum_{i=1}^n \log p_{\theta}(X_i)\\
    &= - \sum_{i=1}^n \log \left[\theta^{X_i} (1- \theta)^{1 -X_i}\right]\\ &= - \sum_{i=1}^n
    \left[ X_i \log \theta + (1 -X_i) \log (1- \theta)\right]. \end{align*}\]
- en: 'We compute the first and second derivatives of \(L_n(\theta; \{X_i\}_{i=1}^n)\)
    as a function of \(\theta\):'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 \(L_n(\theta; \{X_i\}_{i=1}^n)\) 关于 \(\theta\) 的一阶和二阶导数：
- en: \[\begin{align*} \frac{\mathrm{d}}{\mathrm{d} \theta}L_n(\theta; \{X_i\}_{i=1}^n)
    &= - \sum_{i=1}^n \left[ \frac{X_i}{\theta} - \frac{1 -X_i}{1- \theta}\right]\\
    &= - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n X_i}{1- \theta}
    \end{align*}\]
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\mathrm{d}}{\mathrm{d} \theta}L_n(\theta; \{X_i\}_{i=1}^n)
    &= - \sum_{i=1}^n \left[ \frac{X_i}{\theta} - \frac{1 -X_i}{1- \theta}\right]\\
    &= - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n X_i}{1- \theta}
    \end{align*}\]
- en: and
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} \frac{\mathrm{d}^2}{\mathrm{d} \theta^2}L_n(\theta; \{X_i\}_{i=1}^n)
    &= \frac{\sum_{i=1}^n X_i}{\theta^2} + \frac{n - \sum_{i=1}^n X_i}{(1- \theta)^2}.
    \end{align*}\]
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\mathrm{d}^2}{\mathrm{d} \theta^2}L_n(\theta; \{X_i\}_{i=1}^n)
    &= \frac{\sum_{i=1}^n X_i}{\theta^2} + \frac{n - \sum_{i=1}^n X_i}{(1- \theta)^2}.
    \end{align*}\]
- en: The second derivative is non-negative and therefore the NLL is convex. To find
    a global minimizer, it suffices to find a stationary point.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数非负，因此负对数似然函数是凸的。为了找到全局最小值，只需找到一个驻点。
- en: We make the derivative of the NLL equal to \(0\)
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 NLL 的导数设为 \(0\)
- en: \[\begin{align*} &0 = - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff \frac{\sum_{i=1}^n X_i}{\theta} = \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff (1- \theta)\sum_{i=1}^n X_i = \theta \left(n - \sum_{i=1}^n
    X_i \right)\\ & \iff \sum_{i=1}^n X_i = \theta n. \end{align*}\]
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &0 = - \frac{\sum_{i=1}^n X_i}{\theta} + \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff \frac{\sum_{i=1}^n X_i}{\theta} = \frac{n - \sum_{i=1}^n
    X_i}{1- \theta}\\ & \iff (1- \theta)\sum_{i=1}^n X_i = \theta \left(n - \sum_{i=1}^n
    X_i \right)\\ & \iff \sum_{i=1}^n X_i = \theta n. \end{align*}\]
- en: So
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 所以
- en: \[ \hat\theta_{\mathrm{MLE}} = \frac{\sum_{i=1}^n X_i}{n}. \]
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} = \frac{\sum_{i=1}^n X_i}{n}. \]
- en: 'This is in fact a natural estimator: the empirical frequency of \(1\)s. \(\lhd\)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个自然估计器：\(1\) 的经验频率。\(\lhd\)
- en: We give an alternative perspective on the maximum likelihood estimator. Assume
    that \(p_{\btheta}\) is supported on a fixed finite set \(\X\) for all \(\btheta
    \in \Theta\). Given samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\), for each \(\mathbf{x}
    \in \X\), let
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从另一个角度来探讨最大似然估计器。假设对于所有 \(\btheta \in \Theta\)，\(p_{\btheta}\) 都在固定有限集合 \(\X\)
    上有支撑。给定样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)，对于每个 \(\mathbf{x} \in \X\)，让
- en: \[ N_\mathbf{x} = \sum_{i=1}^n \mathbf{1}_{\{\mathbf{X}_i = \mathbf{x}\}} \]
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: \[ N_\mathbf{x} = \sum_{i=1}^n \mathbf{1}_{\{\mathbf{X}_i = \mathbf{x}\}} \]
- en: count the number of times \(\mathbf{x}\) is observed in the data and let
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据中 \(\mathbf{x}\) 出现的次数，并让
- en: \[ \hat\mu_n(\mathbf{x}) = \frac{N_\mathbf{x}}{n} \]
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\mu_n(\mathbf{x}) = \frac{N_\mathbf{x}}{n} \]
- en: be the empirical frequency of \(\mathbf{x}\) in the sample. Observe that \(\hat\mu_n\)
    is a probability distribution over \(\X\).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 是样本中 \(\mathbf{x}\) 的经验频率。注意到 \(\hat\mu_n\) 是 \(\X\) 上的一个概率分布。
- en: The following theorem characterizes the maximum likelihood estimator in terms
    of the [Kullback-Liebler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\(\idx{Kullback-Liebler
    divergence}\xdi\), which was introduced in a previous section.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 以下定理从 [Kullback-Liebler 散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\(\idx{Kullback-Liebler散度}\xdi\)
    的角度描述了最大似然估计器，该散度在前面章节中已介绍。
- en: For two probability distributions
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个概率分布
- en: \[ \mathbf{p}, \mathbf{q} \in \Delta_K := \left\{ (p_1,\ldots,p_K) \in [0,1]^K
    \,:\, \sum_{k=1}^K p_k = 1 \right\}, \]
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{p}, \mathbf{q} \in \Delta_K := \left\{ (p_1,\ldots,p_K) \in [0,1]^K
    \,:\, \sum_{k=1}^K p_k = 1 \right\}, \]
- en: it is defined as
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 它被定义为
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
    \]
- en: where it will suffice to restrict ourselves to the case \(\mathbf{q} > \mathbf{0}\)
    and where we use the convention \(0 \log 0 = 0\) (so that terms with \(p_i = 0\)
    contribute \(0\) to the sum).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们只需将 ourselves 限制在 \(\mathbf{q} > \mathbf{0}\) 的情况，并且我们使用约定 \(0 \log 0 =
    0\)（这样 \(p_i = 0\) 的项对总和的贡献为 \(0\)）。
- en: Notice that \(\mathbf{p} = \mathbf{q}\) implies \(\mathrm{KL}(\mathbf{p} \|
    \mathbf{q}) = 0\). We show that \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\),
    a result known as *Gibbs’ inequality*.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\mathbf{p} = \mathbf{q}\) 意味着 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q})
    = 0\)。我们证明 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)，这是一个被称为 *吉布斯不等式* 的结果。
- en: '**THEOREM** **(Gibbs)** \(\idx{Gibbs'' inequality}\xdi\) For any \(\mathbf{p},
    \mathbf{q} \in \Delta_K\) with \(\mathbf{q} > \mathbf{0}\),'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(吉布斯)** \(\idx{吉布斯不等式}\xdi\) 对于任何 \(\mathbf{p}, \mathbf{q} \in \Delta_K\)
    且 \(\mathbf{q} > \mathbf{0}\)，'
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0. \]
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0. \]
- en: \(\sharp\)
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: '*Proof:* Let \(I\) be the set of indices \(i\) such that \(p_i > 0\). Hence'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 令 \(I\) 为索引 \(i\) 的集合，使得 \(p_i > 0\)。因此'
- en: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i \in I} p_i \log \frac{p_i}{q_i}.
    \]
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\mathbf{p} \| \mathbf{q}) = \sum_{i \in I} p_i \log \frac{p_i}{q_i}.
    \]
- en: It can be proved that \(\log x \leq x - 1\) for all \(x > 0\) (Try it!). So
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明对于所有 \(x > 0\)，\(\log x \leq x - 1\)。所以
- en: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= - \sum_{i \in I} p_i
    \log \frac{q_i}{p_i}\\ &\geq - \sum_{i \in I} p_i \left(\frac{q_i}{p_i} - 1\right)\\
    &= - \sum_{i \in I} q_i + \sum_{i \in I} p_i\\ &= - \sum_{i \in I} q_i + 1\\ &\geq
    0 \end{align*}\]
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathrm{KL}(\mathbf{p} \| \mathbf{q}) &= - \sum_{i \in I} p_i
    \log \frac{q_i}{p_i}\\ &\geq - \sum_{i \in I} p_i \left(\frac{q_i}{p_i} - 1\right)\\
    &= - \sum_{i \in I} q_i + \sum_{i \in I} p_i\\ &= - \sum_{i \in I} q_i + 1\\ &\geq
    0 \end{align*}\]
- en: where we used that \(\log z^{-1} = - \log z\) on the first line and the fact
    that \(p_i = 0\) for all \(i \notin I\) on the fourth line. \(\square\)
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们在第一行使用了 \(\log z^{-1} = - \log z\) 的性质，并在第四行使用了 \(p_i = 0\) 对于所有 \(i \notin
    I\) 的事实。\(\square\)
- en: '**THEOREM** **(MLE via KL)** \(\idx{MLE via KL theorem}\xdi\) Assume that,
    for all \(\btheta \in \Theta\), \(p_{\btheta}\) is supported on a fixed finite
    set \(\X\) and that \(p_{\btheta}(\mathbf{x}) > 0\) for all \(\mathbf{x} \in \X\).
    Given samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\) from \(p_{\btheta^*}\), let
    \(\{\hat\mu_n(\mathbf{x})\}_{\mathbf{x} \in \X}\) be the corresponding empirical
    frequencies. Then the maximum likelihood estimator \(\hat\btheta_{\mathrm{MLE}}\)
    of \(\btheta\) is also a solution to'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(最大似然估计通过KL散度)** \(\idx{通过KL散度的最大似然估计定理}\xdi\) 假设对于所有 \(\btheta \in
    \Theta\)，\(p_{\btheta}\) 在一个固定的有限集合 \(\X\) 上有支撑，并且对于所有 \(\mathbf{x} \in \X\)，\(p_{\btheta}(\mathbf{x})
    > 0\)。给定从 \(p_{\btheta^*}\) 中抽取的样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)，令 \(\{\hat\mu_n(\mathbf{x})\}_{\mathbf{x}
    \in \X}\) 为相应的经验频率。那么 \(\btheta\) 的最大似然估计器 \(\hat\btheta_{\mathrm{MLE}}\) 也是以下方程的解：'
- en: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\min\left\{ \mathrm{KL}(\hat{\mu}_n \|
    p_{\btheta}) \,:\, \btheta \in \Theta \right\}. \]
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\btheta_{\mathrm{MLE}} \in \arg\min\left\{ \mathrm{KL}(\hat{\mu}_n \|
    p_{\btheta}) \,:\, \btheta \in \Theta \right\}. \]
- en: \(\sharp\)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: '*Proof idea:* Manipulate the negative log-likelihood to bring out its relationship
    to the Kullback-Liebler divergence.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 通过操作负对数似然来揭示其与Kullback-Liebler散度的关系。'
- en: '*Proof:* We can re-write the negative log-likelihood as'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明* 我们可以将负对数似然重写为'
- en: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i)
    = - \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x}). \]
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = - \sum_{i=1}^n \log p_{\btheta}(\mathbf{X}_i)
    = - \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x}). \]
- en: To bring out the Kullback-Liebler divergence, we further transform the previous
    equation into
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出 Kullback-Liebler 距离，我们进一步将前面的方程转换为
- en: \[\begin{align*} \frac{1}{n} L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= - \frac{1}{n}
    \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x})\\ &= \sum_{\mathbf{x}
    \in \X} (N_{\mathbf{x}}/n) \log \frac{N_{\mathbf{x}}/n}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} (N_{\mathbf{x}}/n) \log (N_{\mathbf{x}}/n)\\ &= \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x}) \log \hat\mu_n(\mathbf{x})\\
    &= \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) + \mathrm{H}(\hat\mu_n), \end{align*}\]
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{1}{n} L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= - \frac{1}{n}
    \sum_{\mathbf{x} \in \X} N_{\mathbf{x}} \log p_{\btheta}(\mathbf{x})\\ &= \sum_{\mathbf{x}
    \in \X} (N_{\mathbf{x}}/n) \log \frac{N_{\mathbf{x}}/n}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} (N_{\mathbf{x}}/n) \log (N_{\mathbf{x}}/n)\\ &= \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})}
    - \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x}) \log \hat\mu_n(\mathbf{x})\\
    &= \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) + \mathrm{H}(\hat\mu_n), \end{align*}\]
- en: where the second term is referred to as the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))\(\idx{entropy}\xdi\)
    of \(\hat\mu_n\).
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项被称为 \(\hat\mu_n\) 的 [熵](https://en.wikipedia.org/wiki/Entropy_(information_theory))\(\idx{熵}\xdi\)。
- en: Because \(\mathrm{H}(\hat\mu_n)\) does not depend on \(\btheta\), minimizing
    \(L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\) is equivalent to minimizing \(\mathrm{KL}(\hat{\mu}_n
    \| p_{\btheta})\) as claimed. \(\square\)
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\mathrm{H}(\hat\mu_n)\) 不依赖于 \(\btheta\)，所以最小化 \(L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\)
    等价于最小化 \(\mathrm{KL}(\hat{\mu}_n \| p_{\btheta})\)，正如所声称的那样。 \(\square\)
- en: 'In words, the maximum likelihood estimator chooses the parametric distribution
    that is closest to \(\hat\mu_n\) in Kullback-Liebler divergence. One can think
    of this as “projecting” \(\hat\mu_n\) onto the space \(\{p_{\btheta} : \btheta
    \in \Theta\}\) under the Kullback-Liebler notion of distance.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '也就是说，最大似然估计选择与 \(\hat\mu_n\) 在 Kullback-Liebler 距离上最接近的参数分布。可以这样理解，即在 Kullback-Liebler
    距离的概念下，“投影” \(\hat\mu_n\) 到 \(\{p_{\btheta} : \btheta \in \Theta\}\) 空间。'
- en: '**EXAMPLE:** **(Special case)** One special case is where \(\X\) is finite,
    \(\btheta = (\theta_\mathbf{x})_{\mathbf{x} \in \X}\) is a probability distribution
    over \(\X\), and \(p_{\btheta} = \btheta\). That is, we consider the class of
    all probability distributions over \(\X\). Given samples \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    from \(p_{\btheta^*}\), in this case we have'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **（特殊情况）** 一个特殊情况是当 \(\X\) 是有限的，\(\btheta = (\theta_\mathbf{x})_{\mathbf{x}
    \in \X}\) 是 \(\X\) 上的概率分布，且 \(p_{\btheta} = \btheta\)。也就是说，我们考虑 \(\X\) 上所有概率分布的类别。给定从
    \(p_{\btheta^*}\) 中抽取的样本 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)，在这种情况下，我们有'
- en: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x})
    \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})} = \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}},
    \]
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \X} \hat\mu_n(\mathbf{x})
    \log \frac{\hat\mu_n(\mathbf{x})}{p_{\btheta}(\mathbf{x})} = \sum_{\mathbf{x}
    \in \X} \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}},
    \]
- en: where recall that, by convention, if \(\hat\mu_n(\mathbf{x}) = 0\) then \(\hat\mu_n(\mathbf{x})
    \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}} = 0\) for any \(\theta_\mathbf{x}\).
    So, letting \(\mathbb{X}_n = \{\mathbf{X}_1,\ldots,\mathbf{X}_n\}\) be the set
    of distinct values encountered in the sample (ignoring repetitions), we have
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，请注意，按照惯例，如果 \(\hat\mu_n(\mathbf{x}) = 0\)，则对于任何 \(\theta_\mathbf{x}\)，\(\hat\mu_n(\mathbf{x})
    \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}} = 0\)。因此，设 \(\mathbb{X}_n
    = \{\mathbf{X}_1,\ldots,\mathbf{X}_n\}\) 为样本中遇到的不同的值集合（忽略重复），我们有
- en: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}}. \]
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \hat\mu_n(\mathbf{x}) \log \frac{\hat\mu_n(\mathbf{x})}{\theta_\mathbf{x}}. \]
- en: Note that \(\sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1\).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 \(\sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1\)。
- en: 'We have previously established *Gibbs’ inequality* which says that: for any
    \(\mathbf{p}, \mathbf{q} \in \Delta_K\) with \(\mathbf{q} > \mathbf{0}\), it holds
    that \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\).'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经建立了 *吉布斯不等式*，它说：对于任何 \(\mathbf{p}, \mathbf{q} \in \Delta_K\) 且 \(\mathbf{q}
    > \mathbf{0}\)，都有 \(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)。
- en: The minimum \(\mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = 0\) can be achieved
    by setting \(\btheta_{\mathbf{x}} = \hat\mu_n(\mathbf{x})\) for all \(\mathbf{x}
    \in \mathbb{X}_n\) and \(\btheta_{\mathbf{x}} = 0\) for all \(\mathbf{x} \notin
    \mathbb{X}_n\). The condition
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 最小的 \(\mathrm{KL}(\hat{\mu}_n \| p_{\btheta}) = 0\) 可以通过将所有 \(\mathbf{x} \in
    \mathbb{X}_n\) 的 \(\btheta_{\mathbf{x}}\) 设置为 \(\hat\mu_n(\mathbf{x})\)，以及所有 \(\mathbf{x}
    \notin \mathbb{X}_n\) 的 \(\btheta_{\mathbf{x}}\) 设置为 0 来实现。满足条件
- en: \[ \sum_{\mathbf{x} \in \X} \btheta_{\mathbf{x}} = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \btheta_{\mathbf{x}} + \sum_{\mathbf{x} \notin \mathbb{X}_n} \btheta_{\mathbf{x}}
    = \sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1, \]
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{\mathbf{x} \in \X} \btheta_{\mathbf{x}} = \sum_{\mathbf{x} \in \mathbb{X}_n}
    \btheta_{\mathbf{x}} + \sum_{\mathbf{x} \notin \mathbb{X}_n} \btheta_{\mathbf{x}}
    = \sum_{\mathbf{x} \in \mathbb{X}_n} \hat\mu_n(\mathbf{x}) = 1, \]
- en: is then satisfied.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 然后满足。
- en: So in this case \(\hat\mu_n\) is a maximum likelihood estimator.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下 \(\hat\mu_n\) 是一个最大似然估计量。
- en: A special case of this is the *biased coin* example. \(\lhd\)
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 这的一个特例是 *有偏硬币* 例子。\(\lhd\)
- en: '**CHAT & LEARN** Explore the concept of Bayesian parameter estimation. Ask
    your favorite AI chatbot how Bayesian parameter estimation differs from maximum
    likelihood estimation and discuss their relative strengths and weaknesses. Here
    are some possible follow-ups. (1) Get an example implementation using a simple
    dataset. (2) The categorical and multinomial distributions are related to the
    Dirichlet distribution. Ask about relationship and how the Dirichlet distribution
    is used in Bayesian inference for these distributions. \(\ddagger\)'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 探索贝叶斯参数估计的概念。询问您最喜欢的 AI 聊天机器人贝叶斯参数估计与最大似然估计有何不同，并讨论它们的相对优缺点。以下是一些可能的后续问题。(1)
    获取使用简单数据集的示例实现。(2) 分类别和多项分布与 Dirichlet 分布有关。询问它们之间的关系以及 Dirichlet 分布如何用于这些分布的贝叶斯推断。\(\ddagger\)'
- en: 6.2.3\. Parameter estimation for exponential families[#](#parameter-estimation-for-exponential-families
    "Link to this heading")
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.3\. 指数族参数估计[#](#parameter-estimation-for-exponential-families "链接到这个标题")
- en: For exponential families, maximum likelihood estimation takes a particularly
    natural form. We provide details in the discrete case.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指数族，最大似然估计具有特别自然的形式。我们将在离散情况下提供详细信息。
- en: '**THEOREM** **(Maximum Likelihood Estimator for Exponential Families)** \(\idx{maximum
    likelihood estimator for exponential families}\xdi\) Assume that \(p_{\btheta}\)
    takes the exponential family form'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(指数族的最大似然估计量)** \(\idx{指数族的最大似然估计量}\xdi\) 假设 \(p_{\btheta}\) 采用指数族形式'
- en: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
- en: that the support \(\S\) is finite, and that \(A\) is twice continuously differentiable
    over the open set \(\Theta\). Let \(\mathbf{X}_1,\ldots,\mathbf{X}_n\) be \(n\)
    independent samples from a parametric family \(p_{\btheta^*}\) with unknown \(\btheta^*
    \in \Theta\). Then \(L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\), as a function of
    \(\btheta\), is convex and the maximum likelihood estimator of \(\btheta\) – if
    it exists – solves the system of moment-matching equations
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 假设支持 \(\S\) 是有限的，并且 \(A\) 在开集 \(\Theta\) 上是两次连续可微的。设 \(\mathbf{X}_1,\ldots,\mathbf{X}_n\)
    是从参数族 \(p_{\btheta^*}\) 中抽取的 \(n\) 个独立样本，其中 \(\btheta^* \in \Theta\) 是未知的。那么 \(L_n(\btheta;
    \{\mathbf{X}_i\}_{i=1}^n)\)，作为一个关于 \(\btheta\) 的函数，是凸的，并且 \(\btheta\) 的最大似然估计量（如果存在）解决了以下矩匹配方程组
- en: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i), \]
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i), \]
- en: where \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\). \(\sharp\)
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\). \(\sharp\)
- en: Recall that the covariance matrix of a random vector \(\mathbf{Z}\) taking values
    in \(\mathbb{R}^m\) whose components have finite variances is defined as \(\mathrm{K}_{\mathbf{Z},
    \mathbf{Z}} = \E[(\mathbf{Z} - \E[\mathbf{Z}])(\mathbf{Z} - \E[\mathbf{Z}])^T]\)
    and is a positive semidefinite matrix. It is also sometimes denoted as \(\bSigma_\mathbf{Z}\).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，一个随机向量 \(\mathbf{Z}\) 的协方差矩阵，其值在 \(\mathbb{R}^m\) 中，且其分量具有有限的方差，定义为 \(\mathrm{K}_{\mathbf{Z},
    \mathbf{Z}} = \E[(\mathbf{Z} - \E[\mathbf{Z}])(\mathbf{Z} - \E[\mathbf{Z}])^T]\)，它是一个正半定矩阵。它有时也记为
    \(\bSigma_\mathbf{Z}\)。
- en: The function \(A\) has properties worth highlighting that will be used in the
    proof.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 \(A\) 具有一些值得强调的性质，这些性质将在证明中使用。
- en: '**LEMMA** **(Derivatives of \(A\))** Assume that \(p_{\btheta}\) takes the
    exponential family form'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(A 的导数)** 假设 \(p_{\btheta}\) 采用指数族形式'
- en: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\btheta}(\mathbf{x}) = h(\mathbf{x}) \exp\left(\btheta^T \bphi(\mathbf{x})
    - A(\btheta)\right), \]
- en: that the support \(\S\) is finite, and that \(A\) is twice continuously differentiable
    over the open set \(\Theta\). Then
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 假设支持集 \(\S\) 是有限的，并且 \(A\) 在开集 \(\Theta\) 上是二阶连续可微的。那么
- en: \[ \nabla A(\btheta) = \E[\bphi(\mathbf{X})] \qquad \text{and} \qquad \mathbf{H}_A
    (\btheta) = \mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}, \]
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla A(\btheta) = \E[\bphi(\mathbf{X})] \qquad \text{和} \qquad \mathbf{H}_A
    (\btheta) = \mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}, \]
- en: where \(\mathbf{X} \sim p_{\btheta}\). \(\flat\)
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\btheta}\). \(\flat\)
- en: '*Proof idea:* Follows from a direct calculation.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 直接计算得出。'
- en: '*Proof:* We observe first that'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们首先观察到'
- en: \[ A(\btheta) = \log Z(\btheta) = \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}))\right), \]
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A(\btheta) = \log Z(\btheta) = \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}))\right), \]
- en: where we used the fact that, by definition, \(Z(\btheta)\) is the normalization
    constant of \(p_{\btheta}\). In particular, as the logarithm of a finite, weighted
    sum of exponentials, the function \(A(\btheta)\) is continuously differentiable.
    Hence so is \(p_{\btheta}(\mathbf{x})\) as a function of \(\btheta\).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了这样一个事实：根据定义，\(Z(\btheta)\) 是 \(p_{\btheta}\) 的归一化常数。特别是，由于是指数函数的有限加权求和的对数，函数
    \(A(\btheta)\) 是连续可微的。因此，作为 \(\btheta\) 的函数，\(p_{\btheta}(\mathbf{x})\) 也是连续可微的。
- en: From the formula above and the basic rules of calculus,
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述公式和微积分的基本规则，
- en: \[\begin{align*} \frac{\partial}{\partial \theta_j} A(\btheta) &= \frac{\partial}{\partial
    \theta_j} \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}))\right)\\
    &= \frac{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \,\phi_j(\mathbf{x}) \exp(\btheta^T
    \bphi(\mathbf{x}))}{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}))}\\
    &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x}) \frac{1}{Z(\btheta)} h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}))\\ &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\\ &= \E[\phi_j(\mathbf{X})],
    \end{align*}\]
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial \theta_j} A(\btheta) &= \frac{\partial}{\partial
    \theta_j} \log\left(\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}))\right)\\
    &= \frac{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \,\phi_j(\mathbf{x}) \exp(\btheta^T
    \bphi(\mathbf{x}))}{\sum_{\mathbf{x} \in \S} h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}))}\\
    &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x}) \frac{1}{Z(\btheta)} h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}))\\ &= \sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\\ &= \E[\phi_j(\mathbf{X})],
    \end{align*}\]
- en: where \(\mathbf{X} \sim p_{\btheta}\).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\btheta}\)。
- en: Differentiating again, this time with respect to \(\theta_i\), we obtain
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 再次求导，这次是对 \(\theta_i\) 求导，我们得到
- en: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\btheta)
    &= \frac{\partial}{\partial \theta_i} \left\{\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\right\}\\ &= \sum_{\mathbf{x}
    \in \S} \phi_j(\mathbf{x}) h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))
    \left\{\phi_i(\mathbf{x}) - \frac{\partial}{\partial \theta_i} A(\btheta) \right\}\\
    &= \sum_{\mathbf{x} \in \S} \phi_i(\mathbf{x}) \phi_j(\mathbf{x}) h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\\ & \qquad - \left(\sum_{\mathbf{x}
    \in \S} \phi_i(\mathbf{x}) h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))
    \right)\\ & \qquad\qquad \times \left(\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta)) \right)\\ &= \E[\phi_i(\mathbf{X})\phi_j(\mathbf{X})]
    - \E[\phi_i(\mathbf{X})]\E[\phi_j(\mathbf{X})], \end{align*}\]
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\btheta)
    &= \frac{\partial}{\partial \theta_i} \left\{\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\right\}\\ &= \sum_{\mathbf{x}
    \in \S} \phi_j(\mathbf{x}) h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))
    \left\{\phi_i(\mathbf{x}) - \frac{\partial}{\partial \theta_i} A(\btheta) \right\}\\
    &= \sum_{\mathbf{x} \in \S} \phi_i(\mathbf{x}) \phi_j(\mathbf{x}) h(\mathbf{x})
    \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))\\ & \qquad - \left(\sum_{\mathbf{x}
    \in \S} \phi_i(\mathbf{x}) h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta))
    \right)\\ & \qquad\qquad \times \left(\sum_{\mathbf{x} \in \S} \phi_j(\mathbf{x})
    h(\mathbf{x}) \exp(\btheta^T \bphi(\mathbf{x}) - A(\btheta)) \right)\\ &= \E[\phi_i(\mathbf{X})\phi_j(\mathbf{X})]
    - \E[\phi_i(\mathbf{X})]\E[\phi_j(\mathbf{X})], \end{align*}\]
- en: where again \(\mathbf{X} \sim p_{\btheta}\). That concludes the proof. \(\square\)
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 其中再次 \(\mathbf{X} \sim p_{\btheta}\)。这就完成了证明。 \(\square\)
- en: We are now ready to the prove the main theorem.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始证明主要定理。
- en: '*Proof:* *(Maximum Likelihood Estimator for Exponential Families)* We begin
    by computing the stationary points of the negative log-likelihood, for which we
    need the gradient with respect to \(\btheta \in \mathbb{R}^m\). We will also need
    the second-order derivatives to establish convexity. We have'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：*(指数族的最大似然估计器)* 我们首先计算负对数似然函数的驻点，为此我们需要 \(\btheta \in \mathbb{R}^m\) 的梯度。我们还需要二阶导数来建立凸性。我们有'
- en: \[\begin{align*} \frac{\partial}{\partial \theta_j} \{- \log p_{\btheta}(\mathbf{x})\}
    &= \frac{\partial}{\partial \theta_j} \left\{- \log h(\mathbf{x}) - \btheta^T
    \bphi(\mathbf{x}) + A(\btheta)\right\}\\ &= - \phi_j(\mathbf{x}) + \frac{\partial}{\partial
    \theta_j} A(\btheta). \end{align*}\]
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial \theta_j} \{- \log p_{\btheta}(\mathbf{x})\}
    &= \frac{\partial}{\partial \theta_j} \left\{- \log h(\mathbf{x}) - \btheta^T
    \bphi(\mathbf{x}) + A(\btheta)\right\}\\ &= - \phi_j(\mathbf{x}) + \frac{\partial}{\partial
    \theta_j} A(\btheta). \end{align*}\]
- en: and
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} \{-
    \log p_{\btheta}(\mathbf{x})\} &= \frac{\partial}{\partial \theta_i} \left\{-
    \phi_j(\mathbf{x}) + \frac{\partial}{\partial \theta_j} A(\btheta)\right\}\\ &=
    \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\btheta). \end{align*}\]
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial^2}{\partial \theta_i \partial \theta_j} \{-
    \log p_{\btheta}(\mathbf{x})\} &= \frac{\partial}{\partial \theta_i} \left\{-
    \phi_j(\mathbf{x}) + \frac{\partial}{\partial \theta_j} A(\btheta)\right\}\\ &=
    \frac{\partial^2}{\partial \theta_i \partial \theta_j} A(\btheta). \end{align*}\]
- en: We use the expressions for the derivatives of \(A\) obtained above.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上面得到的 \(A\) 的导数表达式。
- en: Plugging into the formula for the minus log-likelihood (as a function of \(\btheta\)),
    we get for the gradient with respect to \(\btheta\)
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 将其代入负对数似然函数（作为 \(\btheta\) 的函数）的公式中，我们得到关于 \(\btheta\) 的梯度
- en: \[\begin{align*} \nabla_\btheta L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= -
    \sum_{i=1}^n \nabla_\btheta \log p_{\btheta}(\mathbf{X}_i)\\ &= \sum_{i=1}^n \{-
    \bphi(\mathbf{X}_i) + \nabla_\btheta A(\btheta)\}\\ &= \sum_{i=1}^n \{- \bphi(\mathbf{X}_i)
    + \E[\bphi(\mathbf{X})]\}. \end{align*}\]
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla_\btheta L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) &= -
    \sum_{i=1}^n \nabla_\btheta \log p_{\btheta}(\mathbf{X}_i)\\ &= \sum_{i=1}^n \{-
    \bphi(\mathbf{X}_i) + \nabla_\btheta A(\btheta)\}\\ &= \sum_{i=1}^n \{- \bphi(\mathbf{X}_i)
    + \E[\bphi(\mathbf{X})]\}. \end{align*}\]
- en: This is also known in statistics as the [score](https://en.wikipedia.org/wiki/Score_(statistics)).
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 这在统计学中也被称为[得分](https://en.wikipedia.org/wiki/Score_(statistics))。
- en: For the Hessian with respect to \(\btheta\), we get
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(\btheta\) 的Hessian矩阵，我们得到
- en: \[\begin{align*} \mathbf{H}_{L_n}(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \mathbf{H}_A (\btheta) = n \,\mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}.
    \end{align*}\]
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{H}_{L_n}(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \mathbf{H}_A (\btheta) = n \,\mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}.
    \end{align*}\]
- en: This is also known in statistics as the [observed information](https://en.wikipedia.org/wiki/Observed_information).
    (In fact, in this case, it reduces to the [Fisher information](https://en.wikipedia.org/wiki/Fisher_information).)
    Since \(\mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}\) is positive semidefinite,
    so is \(\mathbf{H}_{L_n}(\btheta; \{\mathbf{X}_i\}_{i=1}^n)\).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这在统计学中也被称为[观测信息](https://en.wikipedia.org/wiki/Observed_information)。 (实际上，在这种情况下，它简化为[Fisher信息](https://en.wikipedia.org/wiki/Fisher_information)。)
    由于 \(\mathrm{K}_{\bphi(\mathbf{X}), \bphi(\mathbf{X})}\) 是正半定的，因此 \(\mathbf{H}_{L_n}(\btheta;
    \{\mathbf{X}_i\}_{i=1}^n)\) 也是正半定的。
- en: Hence, a stationary point \(\hat\btheta_{\mathrm{MLE}}\) must satisfy
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，驻点 \(\hat\btheta_{\mathrm{MLE}}\) 必须满足
- en: \[ \mathbf{0} = \nabla L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \{- \bphi(\mathbf{X}_i) + \E[\bphi(\mathbf{X})]\}, \]
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{0} = \nabla L_n(\btheta; \{\mathbf{X}_i\}_{i=1}^n) = \sum_{i=1}^n
    \{- \bphi(\mathbf{X}_i) + \E[\bphi(\mathbf{X})]\}, \]
- en: where \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\) or, after re-arranging,
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{X} \sim p_{\hat\btheta_{\mathrm{MLE}}}\) 或，经过重新排列，
- en: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i). \]
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\bphi(\mathbf{X})] = \frac{1}{n} \sum_{i=1}^n \bphi(\mathbf{X}_i). \]
- en: Because \(L_n\) is convex, a stationary point – if it exists – is necessarily
    a global minimum (and vice versa). \(\square\)
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(L_n\) 是凸函数，所以一个驻点（如果存在）必然是全局最小值（反之亦然）。 \(\square\)
- en: '**EXAMPLE:** **(Bernoulli/biased coin, continued)** For \(x \in \{0,1\}\),
    recall that the \(\mathrm{Ber}(q)\) distribution can be written as'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(伯努利/偏硬币，继续)** 对于 \(x \in \{0,1\}\)，回忆一下 \(\mathrm{Ber}(q)\)
    分布可以写成'
- en: \[\begin{align*} p_{\theta}(x) &= \frac{1}{Z(\theta)} h(x) \exp(\theta \,\phi(x))
    \end{align*}\]
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_{\theta}(x) &= \frac{1}{Z(\theta)} h(x) \exp(\theta \,\phi(x))
    \end{align*}\]
- en: where we define \(h(x) \equiv 1\), \(\phi(x) = x\), \(\theta = \log \left(\frac{q}{1-q}\right)\)
    and \(Z(\theta) = 1 + e^\theta\). Let \(X_1,\ldots,X_n\) be independent samples
    from \(p_{\theta^*}\).
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(h(x) \equiv 1\)，\(\phi(x) = x\)，\(\theta = \log \left(\frac{q}{1-q}\right)\)
    和 \(Z(\theta) = 1 + e^\theta\)。设 \(X_1,\ldots,X_n\) 是从 \(p_{\theta^*}\) 独立抽取的样本。
- en: For \(X \sim p_{\hat\theta_{\mathrm{MLE}}}\), the moment-matching equations
    reduce to
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(X \sim p_{\hat\theta_{\mathrm{MLE}}}\)，矩匹配方程简化为
- en: \[ \hat{q}_{\mathrm{MLE}} := \E[X] = \E[\phi(X)] = \frac{1}{n} \sum_{i=1}^n
    \phi(X_i) = \frac{1}{n} \sum_{i=1}^n X_i. \]
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{q}_{\mathrm{MLE}} := \E[X] = \E[\phi(X)] = \frac{1}{n} \sum_{i=1}^n
    \phi(X_i) = \frac{1}{n} \sum_{i=1}^n X_i. \]
- en: To compute the left-hand side in terms of \(\hat\theta_{\mathrm{MLE}}\) we use
    the relationship \(\theta = \log \left(\frac{q}{1-q}\right)\), that is,
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用 \(\hat\theta_{\mathrm{MLE}}\) 来计算左侧，我们使用关系 \(\theta = \log \left(\frac{q}{1-q}\right)\)，即，
- en: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right). \]
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right). \]
- en: Hence, \(\hat\theta_{\mathrm{MLE}}\) is well-defined when \(\frac{1}{n} \sum_{i=1}^n
    X_i \neq 0, 1\).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 \(\frac{1}{n} \sum_{i=1}^n X_i \neq 0, 1\) 时，\(\hat\theta_{\mathrm{MLE}}\)
    是有定义的。
- en: Define \(q^*\) as the solution to
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 \(q^*\) 为
- en: \[ \theta^* = \log \left(\frac{q^*}{1-q^*}\right) \]
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \theta^* = \log \left(\frac{q^*}{1-q^*}\right) \]
- en: that is,
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 即，
- en: \[ q^* = \frac{e^{\theta^*}}{1+e^{\theta^*}} = \frac{1}{1 + e^{-\theta*}} =
    \sigma(\theta^*), \]
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: \[ q^* = \frac{e^{\theta^*}}{1+e^{\theta^*}} = \frac{1}{1 + e^{-\theta*}} =
    \sigma(\theta^*), \]
- en: where \(\sigma\) is the sigmoid function.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma\) 是Sigmoid函数。
- en: By the law of large numbers, as \(n \to +\infty\), we get the convergence
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 根据大数定律，当 \(n \to +\infty\) 时，我们得到收敛
- en: \[ \frac{1}{n} \sum_{i=1}^n X_i \to q^*, \]
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{1}{n} \sum_{i=1}^n X_i \to q^*, \]
- en: with probability one.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎必然。
- en: Because the function \(\log \left(\frac{q}{1-q}\right)\) is continuous for \(q
    \in (0,1)\), we have furthermore
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 因为函数 \(\log \left(\frac{q}{1-q}\right)\) 在 \(q \in (0,1)\) 上是连续的，所以我们还有
- en: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right) \to \log \left(\frac{q^*}{1-q^*}\right) = \theta^*. \]
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat\theta_{\mathrm{MLE}} = \log \left(\frac{\frac{1}{n} \sum_{i=1}^n X_i}{1-\frac{1}{n}
    \sum_{i=1}^n X_i}\right) \to \log \left(\frac{q^*}{1-q^*}\right) = \theta^*. \]
- en: In words, the maximum likelihood estimator \(\hat\theta_{\mathrm{MLE}}\) is
    guaranteed to converge to the true parameter \(\theta^*\) when the number of samples
    grows. This fundamental property is known as [statistical consistency](https://en.wikipedia.org/wiki/Consistent_estimator)\(\idx{statistical
    consistency}\xdi\). \(\lhd\)
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，最大似然估计量 \(\hat\theta_{\mathrm{MLE}}\) 在样本数量增加时保证收敛到真实参数 \(\theta^*\)。这个基本性质被称为[统计一致性](https://en.wikipedia.org/wiki/Consistent_estimator)\(\idx{statistical
    consistency}\xdi\). \(\lhd\)
- en: Statistical consistency holds more generally for the maximum likelihood estimator
    under exponential families, provided certain technical conditions hold. We will
    not provide further details here.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 在指数族下，当满足某些技术条件时，统计一致性对于最大似然估计量更普遍地成立。这里我们不再提供更多细节。
- en: Unlike the previous example, one does not always have an explicit formula for
    the maximum likelihood estimator under exponential families. Instead, optimization
    methods, for instance [Newton’s method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization),
    are used in such cases.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个例子不同，在指数族下，并不总是有最大似然估计量的显式公式。在这种情况下，可以使用优化方法，例如 [牛顿法](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)。
- en: '**EXAMPLE:** **(Multivariate Gaussian)** We established the theorem for finite
    \(\mathcal{S}\), but it holds more generally. Consider the multivariate Gaussian
    case. Here the sufficient statistics are'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**EXAMPLE:** **(多元高斯)** 我们为有限的 \(\mathcal{S}\) 建立了定理，但它具有更一般的适用性。考虑多元高斯情况。在这种情况下，充分统计量是'
- en: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bphi(\mathbf{x}) = (x_1,\ldots,x_d, x_1 x_1, \ldots, x_d x_1, x_1 x_2, \ldots,
    x_d x_2, \ldots, x_1 x_d, \ldots, x_d x_d) \]
- en: which is simply the vector \(\mathbf{x}\) itself stacked with the vectorized
    form of the matrix \(\mathbf{x} \mathbf{x}^T\). So the moment-matching equations
    boil down to
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是向量 \(\mathbf{x}\) 本身堆叠上矩阵 \(\mathbf{x} \mathbf{x}^T\) 的向量形式。因此，矩匹配方程归结为
- en: \[ \E[\mathbf{X}] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \]
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\mathbf{X}] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \]
- en: and
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[ \E[\mathbf{X} \mathbf{X}^T ] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T.
    \]
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\mathbf{X} \mathbf{X}^T ] = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T.
    \]
- en: The first equation says to choose \(\bmu = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i\).
    The second one says to take
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程表示选择 \(\bmu = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i\)。第二个方程表示取
- en: \[ \bSigma = \E[\mathbf{X} \mathbf{X}^T] - \E[\mathbf{X}]\,\E[\mathbf{X}]^T
    = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T - \left(\frac{1}{n} \sum_{i=1}^n
    \mathbf{X}_i\right) \left(\frac{1}{n} \sum_{i=1}^n \mathbf{X}_i^T \right). \]
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bSigma = \E[\mathbf{X} \mathbf{X}^T] - \E[\mathbf{X}]\,\E[\mathbf{X}]^T
    = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i \mathbf{X}_i^T - \left(\frac{1}{n} \sum_{i=1}^n
    \mathbf{X}_i\right) \left(\frac{1}{n} \sum_{i=1}^n \mathbf{X}_i^T \right). \]
- en: \(\lhd\)
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lhd\)
- en: '**KNOWLEDGE CHECK:** Consider again the Weibull distribution with known shape
    parameter \(k > 0\).'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识检查：** 再次考虑已知形状参数 \(k > 0\) 的韦伯尔分布。'
- en: a) Compute \(\E[X^k]\). [*Hint:* Perform a change of variables.]
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: a) 计算 \(\E[X^k]\)。[*提示：进行变量替换。*]
- en: b) What is the MLE of \(\lambda\)?
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\lambda\) 的最大似然估计（MLE）是什么？
- en: \(\checkmark\)
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: \(\checkmark\)
- en: 6.2.4\. Generalized linear models[#](#generalized-linear-models "Link to this
    heading")
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2.4\. 广义线性模型[#](#generalized-linear-models "链接到本标题")
- en: 'Generalized linear models\(\idx{generalized linear model}\xdi\) (GLM) provide
    a broad generalization of linear regression using exponential families. Quoting
    from [Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model), the
    context in which they arise is the following:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 广义线性模型\(\idx{广义线性模型}\xdi\) (GLM) 通过使用指数族对线性回归进行了广泛的推广。引用自 [维基百科](https://en.wikipedia.org/wiki/Generalized_linear_model)，它们出现的背景如下：
- en: Ordinary linear regression predicts the expected value of a given unknown quantity
    (the response variable, a random variable) as a linear combination of a set of
    observed values (predictors). This implies that a constant change in a predictor
    leads to a constant change in the response variable (i.e. a linear-response model).
    This is appropriate when the response variable can vary, to a good approximation,
    indefinitely in either direction, or more generally for any quantity that only
    varies by a relatively small amount compared to the variation in the predictive
    variables, e.g. human heights. However, these assumptions are inappropriate for
    some types of response variables. For example, in cases where the response variable
    is expected to be always positive and varying over a wide range, constant input
    changes lead to geometrically (i.e. exponentially) varying, rather than constantly
    varying, output changes. […] Similarly, a model that predicts a probability of
    making a yes/no choice (a Bernoulli variable) is even less suitable as a linear-response
    model, since probabilities are bounded on both ends (they must be between 0 and
    1). […] Generalized linear models cover all these situations by allowing for response
    variables that have arbitrary distributions (rather than simply normal distributions),
    and for an arbitrary function of the response variable (the link function) to
    vary linearly with the predicted values (rather than assuming that the response
    itself must vary linearly).
  id: totrans-575
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 普通线性回归预测给定未知量（响应变量，一个随机变量）的期望值作为一个观察值集合（预测变量）的线性组合。这意味着预测变量的一个常数变化会导致响应变量（即线性响应模型）的常数变化。当响应变量可以在任一方向上无限变化时，或者更一般地说，对于与预测变量变化相比变化相对较小的任何数量，例如人类身高，这是合适的。然而，对于某些类型的响应变量，这些假设是不合适的。例如，在响应变量预期始终为正且在广泛范围内变化的情况下，恒定的输入变化会导致几何变化（即指数变化），而不是恒定变化。
    [...] 类似地，预测是/否选择的概率（伯努利变量）的模型甚至更不适合作为线性响应模型，因为概率在两端都是有限的（它们必须在0和1之间）。 [...] 广义线性模型通过允许响应变量具有任意分布（而不是仅仅正态分布），以及响应变量的任意函数（连接函数）与预测值线性变化（而不是假设响应本身必须线性变化）来涵盖所有这些情况。
- en: In its simplest form, a generalized linear model assumes that an outcome variable
    \(y \in \mathbb{R}\) is generated from an exponential family \(p_\theta\), where
    \(\theta \in \mathbb{R}\) is a linear combination of the predictor variables \(\mathbf{x}
    \in \mathbb{R}^d\). That is, we assume that \(\theta = \mathbf{w}^T \mathbf{x}\)
    for unknown \(\mathbf{w} \in \mathbb{R}^d\) and the probability distribution of
    \(y\) is of the form
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单形式中，广义线性模型假设结果变量 \(y \in \mathbb{R}\) 是从指数族 \(p_\theta\) 生成的，其中 \(\theta
    \in \mathbb{R}\) 是预测变量 \(\mathbf{x} \in \mathbb{R}^d\) 的线性组合。也就是说，我们假设 \(\theta
    = \mathbf{w}^T \mathbf{x}\) 对于未知的 \(\mathbf{w} \in \mathbb{R}^d\)，并且 \(y\) 的概率分布形式如下
- en: \[ p_{\mathbf{w}^T \mathbf{x}}(y) = h(y) \exp\left((\mathbf{w}^T\mathbf{x})
    \,\phi(y) - A(\mathbf{w}^T \mathbf{x})\right) \]
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p_{\mathbf{w}^T \mathbf{x}}(y) = h(y) \exp\left((\mathbf{w}^T\mathbf{x})
    \,\phi(y) - A(\mathbf{w}^T \mathbf{x})\right) \]
- en: for some sufficient statistic \(\phi(y)\). We further assume that \(A\) is twice
    continuously differentiable over \(\mathbb{R}\).
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个充分统计量 \(\phi(y)\)，我们进一步假设 \(A\) 在 \(\mathbb{R}\) 上是二阶连续可微的。
- en: Given data points \((\mathbf{x}_i,y_i)_{i=1}^n\), the model is fitted using
    maximum likelihood as follows. Under independence of the samples, the likelihood
    of the data is \(\prod_{i=1}^n p_{\mathbf{w}^T \mathbf{x}_i}(y_i)\), which we
    seek to maximize over \(\mathbf{w}\) (which is different from maximizing over
    \(\theta\)!). As before, we work with the negative log-likelihood, which we denote
    as (with a slight abuse of notation)
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据点 \((\mathbf{x}_i,y_i)_{i=1}^n\)，模型使用最大似然法进行拟合。在样本独立的情况下，数据的似然函数是 \(\prod_{i=1}^n
    p_{\mathbf{w}^T \mathbf{x}_i}(y_i)\)，我们希望最大化 \(\mathbf{w}\)（这与最大化 \(\theta\) 不同！）。像以前一样，我们处理负对数似然函数，我们将其表示为（这里有一点符号上的滥用）
- en: \[ L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n \log p_{\mathbf{w}^T
    \mathbf{x}_i}(y_i). \]
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n \log p_{\mathbf{w}^T
    \mathbf{x}_i}(y_i). \]
- en: The gradient with respect to \(\mathbf{w}\) is given by
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 \(\mathbf{w}\) 的梯度由以下给出
- en: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \nabla_\mathbf{w} \log\left[ h(y_i) \exp\left(\mathbf{w}^T \mathbf{x}_i
    \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right)\right]\\ &= - \sum_{i=1}^n \nabla_\mathbf{w}
    \left[\log h(y_i) + \mathbf{w}^T \mathbf{x}_i \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right]\\
    &= - \sum_{i=1}^n \left[ \mathbf{x}_i \phi(y_i) - \nabla_\mathbf{w} A(\mathbf{w}^T
    \mathbf{x}_i)\right]. \end{align*}\]
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \nabla_\mathbf{w} \log\left[ h(y_i) \exp\left(\mathbf{w}^T \mathbf{x}_i
    \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right)\right]\\ &= - \sum_{i=1}^n \nabla_\mathbf{w}
    \left[\log h(y_i) + \mathbf{w}^T \mathbf{x}_i \phi(y_i) - A(\mathbf{w}^T \mathbf{x}_i)\right]\\
    &= - \sum_{i=1}^n \left[ \mathbf{x}_i \phi(y_i) - \nabla_\mathbf{w} A(\mathbf{w}^T
    \mathbf{x}_i)\right]. \end{align*}\]
- en: By the *Chain Rule* and our previous formulas,
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 通过链式法则和我们的前公式，
- en: \[ \nabla_\mathbf{w} A(\mathbf{w}^T \mathbf{x}_i) = A'(\mathbf{w}^T \mathbf{x}_i)
    \,\mathbf{x}_i = \mu(\mathbf{w}; \mathbf{x}_i) \,\mathbf{x}_i \]
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_\mathbf{w} A(\mathbf{w}^T \mathbf{x}_i) = A'(\mathbf{w}^T \mathbf{x}_i)
    \,\mathbf{x}_i = \mu(\mathbf{w}; \mathbf{x}_i) \,\mathbf{x}_i \]
- en: where \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{w}^T
    \mathbf{x}_i}\). That is,
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\)，且 \(Y_i \sim p_{\mathbf{w}^T
    \mathbf{x}_i}\)。也就是说，
- en: \[ \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n
    \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i)). \]
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = - \sum_{i=1}^n
    \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i)). \]
- en: The Hessian of \(A(\mathbf{w}^T \mathbf{x}_i)\), again by the *Chain Rule* and
    our previous formulas, is
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: \(A(\mathbf{w}^T \mathbf{x}_i)\) 的 Hessian 矩阵，再次通过链式法则和我们的前公式，是
- en: \[ A''(\mathbf{w}^T \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T = \sigma^2 (\mathbf{w};
    \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T \]
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A''(\mathbf{w}^T \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T = \sigma^2 (\mathbf{w};
    \mathbf{x}_i) \,\mathbf{x}_i \mathbf{x}_i^T \]
- en: where \(\sigma^2(\mathbf{w}; \mathbf{x}_i) = \mathrm{K}_{\phi(Y_i), \phi(Y_i)}
    = \var[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{w}^T \mathbf{x}_i}\). So the Hessian
    of the negative log-likelihood is
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma^2(\mathbf{w}; \mathbf{x}_i) = \mathrm{K}_{\phi(Y_i), \phi(Y_i)}
    = \var[\phi(Y_i)]\)，且 \(Y_i \sim p_{\mathbf{w}^T \mathbf{x}_i}\)。因此，负对数似然函数的 Hessian
    矩阵是
- en: \[ \mathbf{H}_{L_n}(\mathbf{w}) = \sum_{i=1}^n \sigma^2(\mathbf{w}; \mathbf{x}_i)
    \,\mathbf{x}_i \mathbf{x}_i^T \]
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{H}_{L_n}(\mathbf{w}) = \sum_{i=1}^n \sigma^2(\mathbf{w}; \mathbf{x}_i)
    \,\mathbf{x}_i \mathbf{x}_i^T \]
- en: which is positive semidefinite (prove it!).
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有界半正定的（证明它！）。
- en: As a result, the negative log-likelihood is convex and the maximum likelihood
    estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) solves the equation \(\nabla_\mathbf{w}
    L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = \mathbf{0}\), that is,
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，负对数似然函数是凸的，最大似然估计量 \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) 满足方程 \(\nabla_\mathbf{w}
    L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) = \mathbf{0}\)，即，
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i). \]
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i). \]
- en: We revisit linear and logistic regression next.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来回顾线性回归和逻辑回归。
- en: '**EXAMPLE:** **(Linear regression)** \(\idx{linear regression}\xdi\) Consider
    the case where \(p_\theta\) is a univariate Gaussian with mean \(\theta\) and
    fixed variance \(1\). That is,'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(线性回归)** \(\idx{linear regression}\xdi\) 考虑 \(p_\theta\) 是一个均值为 \(\theta\)
    且方差固定的单变量高斯分布的情况。也就是说，'
- en: \[\begin{align*} p_{\theta}(y) &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{(y
    - \theta)^2}{2}\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{1}{2}[y^2
    - 2 y \theta + \theta^2]\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{y^2}{2}\right)
    \exp\left(\theta y - \frac{\theta^2}{2}\right)\\ &= h(y) \exp\left(\theta \phi(y)
    - A(\theta)\right), \end{align*}\]
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_{\theta}(y) &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{(y
    - \theta)^2}{2}\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{1}{2}[y^2
    - 2 y \theta + \theta^2]\right)\\ &= \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{y^2}{2}\right)
    \exp\left(\theta y - \frac{\theta^2}{2}\right)\\ &= h(y) \exp\left(\theta \phi(y)
    - A(\theta)\right), \end{align*}\]
- en: where \(\phi(y) = y\) and \(A(\theta) = \theta^2/2\). We now assume that \(\theta
    = \mathbf{x}^T \mathbf{w}\) to obtain the corresponding generalized linear model.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，\(\phi(y) = y\) 且 \(A(\theta) = \theta^2/2\)。我们现在假设 \(\theta = \mathbf{x}^T
    \mathbf{w}\) 以获得相应的广义线性模型。
- en: Given data points \((\mathbf{x}_i,y_i)_{i=1}^n\), recall that the maximum likelihood
    estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) solves the equation
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据点 \((\mathbf{x}_i,y_i)_{i=1}^n\)，回忆一下最大似然估计量 \(\hat{\mathbf{w}}_{\mathrm{MLE}}\)
    解决了以下方程
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
- en: where \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\). Here \(\E[\phi(Y_i)] = \E[Y_i] = \mathbf{x}_i^T \mathbf{w}\). So
    the equation reduces to
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\)，且 \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\)。这里 \(\E[\phi(Y_i)] = \E[Y_i] = \mathbf{x}_i^T \mathbf{w}\)。因此，方程简化为
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T \mathbf{w} = \sum_{i=1}^n \mathbf{x}_i
    y_i. \]
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T \mathbf{w} = \sum_{i=1}^n \mathbf{x}_i
    y_i. \]
- en: You may not recognize this equation, but we have encountered it before in a
    different form. Let \(A\) be the matrix with row \(i\) equal to \(\mathbf{x}_i\)
    and let \(\mathbf{y}\) be the vector with \(i\)-th entry equal to \(y_i\). Then
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不认识这个方程，但我们之前以不同的形式遇到过它。设 \(A\) 为行 \(i\) 等于 \(\mathbf{x}_i\) 的矩阵，设 \(\mathbf{y}\)
    为第 \(i\) 个元素等于 \(y_i\) 的向量。那么
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T = A^T A \qquad \text{and} \qquad
    \sum_{i=1}^n \mathbf{x}_i y_i = A^T \mathbf{y} \]
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T = A^T A \qquad \text{和} \qquad \sum_{i=1}^n
    \mathbf{x}_i y_i = A^T \mathbf{y} \]
- en: as can be checked entry by entry or by using our previous characterizations
    of matrix-matrix products (in outer-product form) and matrix-vector products (as
    linear combinations of columns). Therefore, the equation above is equivalent to
    \(A^T A \mathbf{w} = A^T \mathbf{y}\) - the normal equations of linear regression.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 可以逐项检查或使用我们之前对矩阵-矩阵乘积（外积形式）和矩阵-向量乘积（作为列的线性组合）的描述来验证。因此，上述方程等价于 \(A^T A \mathbf{w}
    = A^T \mathbf{y}\) - 线性回归的正则方程。
- en: To make sense of this finding, we look back at the minus log-likelihood
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个发现，我们回顾一下负对数似然
- en: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\frac{1}{\sqrt{2
    \pi}} \exp\left(- \frac{(y_i - \mathbf{x}_i^T \mathbf{w})^2}{2}\right)\right)\\
    &= - \log (\sqrt{2 \pi}) + \frac{1}{2} \sum_{i=1}^n (y_i - \mathbf{x}_i^T \mathbf{w})^2.
    \end{align*}\]
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\frac{1}{\sqrt{2
    \pi}} \exp\left(- \frac{(y_i - \mathbf{x}_i^T \mathbf{w})^2}{2}\right)\right)\\
    &= - \log (\sqrt{2 \pi}) + \frac{1}{2} \sum_{i=1}^n (y_i - \mathbf{x}_i^T \mathbf{w})^2.
    \end{align*}\]
- en: Observe that minimizing this expression over \(\mathbf{w}\) is equivalent to
    solving the least-squares problem as the first term does not depend on \(\mathbf{w}\)
    and the factor of \(1/2\) does not affect the optimum.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到在 \(\mathbf{w}\) 上最小化这个表达式等价于求解最小二乘问题，因为第一项不依赖于 \(\mathbf{w}\)，而 \(1/2\)
    的系数不影响最优解。
- en: While we have rederived the least squares problem from a probabilistic model,
    it should be noted that the Gaussian assumption is not in fact required for linear
    regression to be warranted. Rather, it gives a different perspective on the same
    problem. \(\lhd\)
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们从概率模型重新推导了最小二乘问题，但应该注意的是，高斯假设实际上并不是线性回归所必需的。相反，它为相同的问题提供了不同的视角。 \(\lhd\)
- en: '**EXAMPLE:** **(Logistic regression)** \(\idx{logistic regression}\xdi\) Consider
    the case where \(p_{\theta}\) is a Bernoulli distribution. That is, for \(y \in
    \{0,1\}\),'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** **(逻辑回归)** \(\idx{logistic regression}\xdi\) 考虑 \(p_{\theta}\) 是伯努利分布的情况。也就是说，对于
    \(y \in \{0,1\}\),'
- en: \[\begin{align*} p_{\theta}(y) &= h(y) \exp(\theta \,\phi(y) - A(\theta)), \end{align*}\]
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} p_{\theta}(y) &= h(y) \exp(\theta \,\phi(y) - A(\theta)), \end{align*}\]
- en: where \(h(y) \equiv 1\), \(\phi(y) = y\) and \(A(\theta) = \log(1 + e^\theta)\).
    We assume that \(\theta = \mathbf{x}^T \mathbf{w}\) to obtain the corresponding
    generalized linear model. Given data points \((\mathbf{x}_i,y_i)_{i=1}^n\), the
    maximum likelihood estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) solves the equation
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(h(y) \equiv 1\)，\(\phi(y) = y\)，且 \(A(\theta) = \log(1 + e^\theta)\)。我们假设
    \(\theta = \mathbf{x}^T \mathbf{w}\) 以获得相应的广义线性模型。给定数据点 \((\mathbf{x}_i,y_i)_{i=1}^n\)，最大似然估计量
    \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) 解这个方程
- en: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i
    \phi(y_i) \]
- en: where \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\) with \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\). Here, by our formula for the gradient of \(A\),
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mu(\mathbf{w}; \mathbf{x}_i) = \E[\phi(Y_i)]\)，且 \(Y_i \sim p_{\mathbf{x}_i^T
    \mathbf{w}}\)。在这里，根据我们关于 \(A\) 的梯度的公式，
- en: \[ \E[\phi(Y_i)] = \E[Y_i] = A'(\mathbf{x}_i^T \mathbf{w}) = \frac{e^{\mathbf{x}_i^T
    \mathbf{w}}}{1 + e^{\mathbf{x}_i^T \mathbf{w}}} = \sigma(\mathbf{x}_i^T \mathbf{w}),
    \]
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \E[\phi(Y_i)] = \E[Y_i] = A'(\mathbf{x}_i^T \mathbf{w}) = \frac{e^{\mathbf{x}_i^T
    \mathbf{w}}}{1 + e^{\mathbf{x}_i^T \mathbf{w}}} = \sigma(\mathbf{x}_i^T \mathbf{w}),
    \]
- en: where \(\sigma\) is the sigmoid function. So the equation reduces to
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\sigma\) 是Sigmoid函数。因此，方程简化为
- en: \[ \sum_{i=1}^n \mathbf{x}_i \sigma(\mathbf{x}_i^T \mathbf{w}) = \sum_{i=1}^n
    \mathbf{x}_i y_i. \]
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \mathbf{x}_i \sigma(\mathbf{x}_i^T \mathbf{w}) = \sum_{i=1}^n
    \mathbf{x}_i y_i. \]
- en: The equation in this case cannot be solved explicitly. Instead we can use gradient
    descent, or a variant, to minimize the negative log-likelihood directly. The lattter
    is
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，方程不能显式求解。相反，我们可以使用梯度下降或其变体来直接最小化负对数似然。后者是
- en: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\exp((\mathbf{x}_i^T
    \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}}))\right)\\ &= - \sum_{i=1}^n
    \left[(\mathbf{x}_i^T \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\
    &= - \sum_{i=1}^n \left[y_i \log(e^{\mathbf{x}_i^T \mathbf{w}}) - (y_i + (1-y_i))\log(1
    + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\ &= - \sum_{i=1}^n \left[y_i \log(\sigma(\mathbf{x}_i^T
    \mathbf{w})) + (1-y_i) \log(1 -\sigma(\mathbf{x}_i^T \mathbf{w}))\right]. \end{align*}\]
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\}) &= - \sum_{i=1}^n
    \log p_{\mathbf{x}_i^T \mathbf{w}}(y_i)\\ &= - \sum_{i=1}^n \log \left(\exp((\mathbf{x}_i^T
    \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}}))\right)\\ &= - \sum_{i=1}^n
    \left[(\mathbf{x}_i^T \mathbf{w}) y_i - \log(1 + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\
    &= - \sum_{i=1}^n \left[y_i \log(e^{\mathbf{x}_i^T \mathbf{w}}) - (y_i + (1-y_i))\log(1
    + e^{\mathbf{x}_i^T \mathbf{w}})\right]\\ &= - \sum_{i=1}^n \left[y_i \log(\sigma(\mathbf{x}_i^T
    \mathbf{w})) + (1-y_i) \log(1 -\sigma(\mathbf{x}_i^T \mathbf{w}))\right]. \end{align*}\]
- en: Minimizing \(L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})\) is equivalent
    to logistic regression.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化 \(L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})\) 等同于逻辑回归。
- en: To use gradient descent, we compute
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用梯度下降，我们计算
- en: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i))\\ &=
    - \sum_{i=1}^n \mathbf{x}_i (y_i - \sigma(\mathbf{x}_i^T \mathbf{w})). \end{align*}\]
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla_\mathbf{w} L_n(\mathbf{w};\{(\mathbf{x}_i,y_i)_{i=1}^n\})
    &= - \sum_{i=1}^n \mathbf{x}_i (\phi(y_i) - \mu(\mathbf{w}; \mathbf{x}_i))\\ &=
    - \sum_{i=1}^n \mathbf{x}_i (y_i - \sigma(\mathbf{x}_i^T \mathbf{w})). \end{align*}\]
- en: This expression is indeed consistent with what we previously derived for logistic
    regression. \(\lhd\)
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式确实与我们之前推导出的逻辑回归结果一致。\(\lhd\)
- en: '**CHAT & LEARN** Generalized linear models can be extended to handle more complex
    data structures. Ask your favorite AI chatbot to explain what generalized additive
    models (GAMs) are and how they differ from generalized linear models. Also, ask
    about some common applications of GAMs. \(\ddagger\)'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 广义线性模型可以扩展以处理更复杂的数据结构。请你的最爱AI聊天机器人解释广义加性模型（GAMs）是什么以及它们与广义线性模型有何不同。还要了解GAMs的一些常见应用。\(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude、Gemini和ChatGPT协助)*'
- en: '**1** Which of the following is NOT an example of an exponential family of
    distributions?'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 以下哪项不是指数族分布的例子？'
- en: a) Bernoulli
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: a) 伯努利分布
- en: b) Categorical
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: b) 类别分布
- en: c) Uniform
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: c) 均匀分布
- en: d) Multivariate Gaussian
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: d) 多变量高斯分布
- en: '**2** In the exponential family form \(p_{\boldsymbol{\theta}}(\mathbf{x})
    = h(\mathbf{x}) \exp(\boldsymbol{\theta}^T \boldsymbol{\phi}(\mathbf{x}) - A(\boldsymbol{\theta}))\),
    what does \(A(\boldsymbol{\theta})\) represent?'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在指数族形式 \(p_{\boldsymbol{\theta}}(\mathbf{x}) = h(\mathbf{x}) \exp(\boldsymbol{\theta}^T
    \boldsymbol{\phi}(\mathbf{x}) - A(\boldsymbol{\theta}))\) 中，\(A(\boldsymbol{\theta})\)
    代表什么？'
- en: a) The sufficient statistic
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: a) 充分统计量
- en: b) The log-partition function
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: b) 对数配分函数
- en: c) The canonical parameter
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: c) 标准参数
- en: d) The base measure
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: d) 基础测度
- en: '**3** Given \(n\) independent samples \(X_1, \ldots, X_n\) from a parametric
    family \(p_{\boldsymbol{\theta}^*}\) with unknown \(\boldsymbol{\theta}^* \in
    \Theta\), the maximum likelihood estimator \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}\)
    is defined as:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 给定 \(n\) 个独立样本 \(X_1, \ldots, X_n\)，它们来自具有未知 \(\boldsymbol{\theta}^*
    \in \Theta\) 的参数族 \(p_{\boldsymbol{\theta}^*}\)，最大似然估计量 \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}\)
    定义为：'
- en: 'a) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: 'b) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 'b) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \prod_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: 'c) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 'c) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: 'd) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 'd) \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\min \left\{ \sum_{i=1}^n
    p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta \right\}\)'
- en: '**4** In a generalized linear model, the maximum likelihood estimator \(\hat{\mathbf{w}}_{\mathrm{MLE}}\)
    solves the equation:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在广义线性模型中，最大似然估计量 \(\hat{\mathbf{w}}_{\mathrm{MLE}}\) 解决以下方程：'
- en: a) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    \mathbf{x}_i \phi(y_i)\)
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    \mathbf{x}_i \phi(y_i)\)
- en: b) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    y_i \phi(\mathbf{x}_i)\)
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n
    y_i \phi(\mathbf{x}_i)\)
- en: c) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \phi(y_i)\)
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \phi(y_i)\)
- en: d) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n y_i\)
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\sum_{i=1}^n \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n y_i\)
- en: '**5** In logistic regression, which distribution is used for the outcome variable?'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在逻辑回归中，结果变量使用哪种分布？'
- en: a) Normal distribution
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: a) 正态分布
- en: b) Poisson distribution
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: b) 泊松分布
- en: c) Bernoulli distribution
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: c) 伯努利分布
- en: d) Exponential distribution
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: d) 指数分布
- en: 'Answer for 1: c. Justification: The text provides examples of Bernoulli, categorical,
    and multivariate Gaussian distributions as members of the exponential family.
    The uniform distribution, however, does not fit the exponential family form.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 答案1：c. 证明：文本提供了伯努利分布、分类分布和多变量高斯分布作为指数族成员的例子。然而，均匀分布不符合指数族的形式。
- en: 'Answer for 2: b. Justification: The text states that \(A(\boldsymbol{\theta})
    = \log Z(\boldsymbol{\theta})\), where \(Z(\boldsymbol{\theta})\) is referred
    to as the partition function.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 答案2：b. 证明：文本指出 \(A(\boldsymbol{\theta}) = \log Z(\boldsymbol{\theta})\)，其中 \(Z(\boldsymbol{\theta})\)
    被称为配分函数。
- en: 'Answer for 3: a. Justification: The text provides the definition of the maximum
    likelihood estimator as \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max
    \left\{ \prod_{i=1}^n p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta
    \right\}\).'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '答案3：a. 证明：文本提供了最大似然估计量的定义为 \(\hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \in \arg\max
    \left\{ \prod_{i=1}^n p_{\boldsymbol{\theta}}(X_i) : \boldsymbol{\theta} \in \Theta
    \right\}\)。'
- en: 'Answer for 4: a. Justification: The text derives the equation \(\sum_{i=1}^n
    \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i) = \sum_{i=1}^n \mathbf{x}_i \phi(y_i)\)
    as the one that the maximum likelihood estimator solves in a generalized linear
    model.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：a. 证明：文本推导出方程 \(\sum_{i=1}^n \mathbf{x}_i \mu(\mathbf{w}; \mathbf{x}_i)
    = \sum_{i=1}^n \mathbf{x}_i \phi(y_i)\) 是最大似然估计量在广义线性模型中解决的方程。
- en: 'Answer for 5: c. Justification: The text describes logistic regression as a
    GLM where the outcome variable is assumed to follow a Bernoulli distribution.'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：c. 证明：文本描述逻辑回归为一种广义线性模型，其中结果变量被假定为遵循伯努利分布。
