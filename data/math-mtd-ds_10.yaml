- en: '2.1\. Motivating example: predicting sales#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.1. 激励示例：预测销售#
- en: 原文：[https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html](https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html](https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html)
- en: '**Figure:** Helpful map of ML by scitkit-learn ([Source](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html))'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：scikit-learn提供的ML有帮助的地图（来源](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html))'
- en: '![ml-cheat-sheet](../Images/19ac9e49b2f297976e40fee63e1c4ba0.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![ml-cheat-sheet](../Images/19ac9e49b2f297976e40fee63e1c4ba0.png)'
- en: \(\bowtie\)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: 'The following dataset is from the excellent textbook [[ISLP]](https://www.statlearning.com/).
    Quoting [ISLP, Section 2.1]:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的数据集来自优秀的教科书 [[ISLP]](https://www.statlearning.com/)。引用 [ISLP，第2.1节](https://www.statlearning.com/)：
- en: 'Suppose that we are statistical consultants hired by a client to provide advice
    on how to improve sales of a particular product. The `advertising` data set consists
    of the `sales` of that product in 200 different markets, along with advertising
    budgets for the product in each of those markets for three different media: `TV`,
    `radio`, and `newspaper`. […] It is not possible for our client to directly increase
    sales of the product. On the other hand, they can control the advertising expenditure
    in each of the three media. Therefore, if we determine that there is an association
    between advertising and sales, then we can instruct our client to adjust advertising
    budgets, thereby indirectly increasing sales. In other words, our goal is to develop
    an accurate model that can be used to predict sales on the basis of the three
    media budgets.'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设我们是被客户雇佣的统计顾问，为客户提供如何提高特定产品销售的建议。`广告`数据集包含了该产品在200个不同市场的销售情况，以及每个市场针对三种不同媒体（`电视`、`广播`和`报纸`）的广告预算。
    [...] 我们的客户无法直接增加产品的销售。另一方面，他们可以控制三种媒体中的广告支出。因此，如果我们确定广告和销售之间存在关联，那么我们可以指导客户调整广告预算，从而间接增加销售。换句话说，我们的目标是开发一个准确的模型，该模型可以根据三种媒体的预算预测销售。
- en: This a [regression](https://en.wikipedia.org/wiki/Regression_analysis) problem.
    That is, we want to estimate the relationship between an outcome variable and
    one or more predictors (or features). We load the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 [回归](https://en.wikipedia.org/wiki/Regression_analysis)问题。也就是说，我们想要估计结果变量与一个或多个预测变量（或特征）之间的关系。我们加载数据。
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|  | TV | radio | newspaper | sales |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|  | TV | radio | newspaper | sales |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 230.1 | 37.8 | 69.2 | 22.1 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 230.1 | 37.8 | 69.2 | 22.1 |'
- en: '| 1 | 44.5 | 39.3 | 45.1 | 10.4 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 44.5 | 39.3 | 45.1 | 10.4 |'
- en: '| 2 | 17.2 | 45.9 | 69.3 | 9.3 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 17.2 | 45.9 | 69.3 | 9.3 |'
- en: '| 3 | 151.5 | 41.3 | 58.5 | 18.5 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 151.5 | 41.3 | 58.5 | 18.5 |'
- en: '| 4 | 180.8 | 10.8 | 58.4 | 12.9 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 180.8 | 10.8 | 58.4 | 12.9 |'
- en: We will focus for now on the TV budget.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将专注于电视预算。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We make a scatterplot showing the relation between those two quantities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们制作一个散点图，显示这两个数量之间的关系。
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![../../_images/513c36bbfdc95bb1193feb1a81faf6b12f49523a3e5972c61f8ebcbd7f20c997.png](../Images/b4db7dd46ce45bb3079256c749e70bf5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/513c36bbfdc95bb1193feb1a81faf6b12f49523a3e5972c61f8ebcbd7f20c997.png](../Images/b4db7dd46ce45bb3079256c749e70bf5.png)'
- en: There does seem to be a relationship between the two. Roughly a higher TV budget
    is linked to higher sales, although the correspondence is not perfect. To express
    the relationship more quantitatively, we seek a function \(f\) such that
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这两个变量之间确实存在某种关系。大致来说，较高的电视预算与较高的销售相关联，尽管这种对应关系并不完美。为了更定量地表达这种关系，我们寻求一个函数
    \(f\)，使得
- en: \[ y \approx f(x) \]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y \approx f(x) \]
- en: where \(x\) denotes a sample TV budget and \(y\) is the corresponding observed
    sales. We might posit for instance that there exists a true \(f\) and that each
    observation is disrupted by some noise \(\varepsilon\)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(x\) 表示样本电视预算，\(y\) 是相应的观察销售。例如，我们可能假设存在一个真实的 \(f\)，并且每个观察值都受到一些噪声 \(\varepsilon\)
    的干扰
- en: \[ y = f(x) + \varepsilon. \]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y = f(x) + \varepsilon. \]
- en: 'A natural way to estimate such an \(f\) from data is [\(k\)-nearest-neighbors
    (\(k\)-NN) regression](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression)\(\idx{k-NN
    regression}\xdi\). Let the data be of the form \(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)
    where \(\mathbf{x}_i \in \mathbb{R}^d\) and \(y_i \in \mathbb{R}\). In our case
    \(\mathbf{x}_i\) is the (real-valued) TV budget of the \(i\)-th sample (so \(d=1\))
    and \(y_i\) is the corresponding sales. For each \(\mathbf{x}\) (not necessarily
    in the data), we do the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中估计这样的 \(f\) 的一个自然方法是 [\(k\)-最近邻 (\(k\)-NN) 回归](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression)\(\idx{k-NN
    回归}\xdi\)。让数据的形式为 \(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)，其中 \(\mathbf{x}_i \in \mathbb{R}^d\)
    且 \(y_i \in \mathbb{R}\)。在我们的情况下，\(\mathbf{x}_i\) 是第 \(i\) 个样本的（实值）电视预算（因此 \(d=1\)），而
    \(y_i\) 是相应的销售额。对于每个 \(\mathbf{x}\)（不一定在数据中），我们执行以下操作：
- en: find the \(k\) nearest \(\mathbf{x}_i\)’s to \(\mathbf{x}\)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到与 \(\mathbf{x}\) 最接近的 \(k\) 个 \(\mathbf{x}_i\)。
- en: take an average of the corresponding \(y_i\)’s.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应的 \(y_i\) 取平均值。
- en: We implement this method in Python. We use the function [`numpy.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)
    to sort an array and the function [`numpy.absolute`](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html)
    to compute the absolute deviation. Our quick implementation here assumes that
    the \(\mathbf{x}_i\)’s are scalars.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中实现这种方法。我们使用函数 `numpy.argsort` 对数组进行排序，并使用函数 `numpy.absolute` 计算绝对偏差。我们这里的快速实现假设
    \(\mathbf{x}_i\) 是标量。
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For \(k=3\) and a grid of \(1000\) points, we get the following approximation
    \(\hat{f}\). Here the function [`numpy.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)
    creates an array of equally spaced points.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k=3\) 和 \(1000\) 个点的网格，我们得到以下近似 \(\hat{f}\)。在这里，函数 `numpy.linspace` 创建了一个等间距点的数组。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![../../_images/30b183f214399989ff56602bf85c4b553a4721e53dab5b33add8516d6bcb98ea.png](../Images/9d112f3fd54d45fb5d476e64c7a0950c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/30b183f214399989ff56602bf85c4b553a4721e53dab5b33add8516d6bcb98ea.png](../Images/9d112f3fd54d45fb5d476e64c7a0950c.png)'
- en: A higher \(k\) produces something less wiggly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 较高的 \(k\) 产生的东西不那么扭曲。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![../../_images/7182fdeb5a509496e08f6c00d370ca685553b74659c2124a83e0903a410a97f2.png](../Images/26d6f8c976ff1dbb81a5d207dab76ad6.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/7182fdeb5a509496e08f6c00d370ca685553b74659c2124a83e0903a410a97f2.png](../Images/26d6f8c976ff1dbb81a5d207dab76ad6.png)'
- en: 'One downside of \(k\)-NN regression is that it does not give an easily interpretable
    relationship: if I increase my TV budget by \(\Delta\) dollars, how is it expected
    to affect the sales? Another issue arises in high dimension where the counter-intuitive
    phenomena we discussed in a previous section can have a significant impact. Recall
    in particular the *High-dimensional Cube Theorem*. If we have \(d\) predictors
    – where \(d\) is large – and our data is distributed uniformly in a bounded region,
    then any given \(\mathbf{x}\) will be far from any of our data points. In that
    case, the \(y\)-values of the closest \(\mathbf{x}_i\)’s may not be predictive.
    This is referred to as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{curse
    of dimensionality}\xdi\).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \(k\)-NN 回归的一个缺点是它不提供易于解释的关系：如果我增加我的电视预算 \(\Delta\) 美元，它预计会如何影响销售额？在维度较高的情况下，我们之前章节中讨论的反直觉现象可能会产生重大影响。特别是回想一下*高维立方体定理*。如果我们有
    \(d\) 个预测因子——其中 \(d\) 很大——并且我们的数据在有限区域内均匀分布，那么任何给定的 \(\mathbf{x}\) 都会远离我们的数据点。在这种情况下，最近的
    \(\mathbf{x}_i\) 的 \(y\) 值可能不具有预测性。这被称为[维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{维度诅咒}\xdi\)。
- en: '**CHAT & LEARN** Ask your favorite AI chatbot for more details on the Curse
    of Dimensionality and how it arises in data science. \(\ddagger\)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 向您最喜欢的 AI 聊天机器人询问有关维度诅咒的更多详细信息及其在数据科学中如何出现的细节。\(\ddagger\)'
- en: One way out is to make stronger assumptions on the function \(f\). For instance,
    we can assume that the true relationship is (approximately) affine, that is, \(y
    \approx \beta_0 + \beta_1 x\), or if we have \(d\) predictors
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是对函数 \(f\) 做出更强的假设。例如，我们可以假设真实关系是（近似）线性的，即 \(y \approx \beta_0 + \beta_1
    x\)，或者如果我们有 \(d\) 个预测因子
- en: \[ y \approx \beta_0 + \sum_{j=1}^d \beta_j x_j. \]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y \approx \beta_0 + \sum_{j=1}^d \beta_j x_j. \]
- en: How do we estimate appropriate intercept and coefficients? The standard approach
    is to minimize the sum of the squared errors
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何估计适当的截距和系数？标准方法是使平方误差之和最小化
- en: \[ \sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2,
    \]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2,
    \]
- en: where \((\mathbf{x}_{i})_j\) is the \(j\)-th entry of input vector \(\mathbf{x}_i
    \in \mathbb{R}^d\) and \(y_i \in \mathbb{R}\) is the corresponding \(y\)-value.
    This is called [multiple linear regression](https://en.wikipedia.org/wiki/Linear_regression).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \((\mathbf{x}_{i})_j\) 是输入向量 \(\mathbf{x}_i \in \mathbb{R}^d\) 的第 \(j\) 个元素，\(y_i
    \in \mathbb{R}\) 是相应的 \(y\) 值。这被称为[多元线性回归](https://en.wikipedia.org/wiki/Linear_regression)。
- en: It is a [least-squares problem](https://en.wikipedia.org/wiki/Least_squares).
    We re-write it in a more convenient matrix form and combine \(\beta_0\) with the
    other \(\beta_i\)’s by adding a dummy predictor to each sample. Let
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个[最小二乘问题](https://en.wikipedia.org/wiki/Least_squares)。我们将它重写为更方便的矩阵形式，并通过向每个样本添加一个虚拟预测因子将
    \(\beta_0\) 与其他 \(\beta_i\) 结合起来。令
- en: \[\begin{split} \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix},
    \quad\quad A = \begin{pmatrix} 1 & \mathbf{x}_1^T \\ 1 & \mathbf{x}_2^T \\ \vdots
    & \vdots \\ 1 & \mathbf{x}_n^T \end{pmatrix} \quad\text{and}\quad \boldsymbol{\beta}
    = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_d \end{pmatrix}. \end{split}\]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix},
    \quad\quad A = \begin{pmatrix} 1 & \mathbf{x}_1^T \\ 1 & \mathbf{x}_2^T \\ \vdots
    & \vdots \\ 1 & \mathbf{x}_n^T \end{pmatrix} \quad\text{and}\quad \boldsymbol{\beta}
    = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_d \end{pmatrix}. \end{split}\]
- en: Then observe that
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，观察以下内容
- en: \[\begin{align*} \|\mathbf{y} - A \boldsymbol{\beta}\|^2 &= \sum_{i=1}^n \left(y_i
    - (A \boldsymbol{\beta})_i\right)^2\\ &= \sum_{i=1}^n \left(y_i - \left\{\beta_0
    + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2. \end{align*}\]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \|\mathbf{y} - A \boldsymbol{\beta}\|^2 &= \sum_{i=1}^n \left(y_i
    - (A \boldsymbol{\beta})_i\right)^2\\ &= \sum_{i=1}^n \left(y_i - \left\{\beta_0
    + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2. \end{align*}\]
- en: The linear least-squares problem is then formulated as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 线性最小二乘问题随后被表述为
- en: \[ \min_{\boldsymbol{\beta} \in \mathbb{R}^{d+1}} \|\mathbf{y} - A \boldsymbol{\beta}\|^2.
    \]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\boldsymbol{\beta} \in \mathbb{R}^{d+1}} \|\mathbf{y} - A \boldsymbol{\beta}\|^2.
    \]
- en: In words, we are looking for a linear combination of the columns of \(A\) that
    is closest to \(\mathbf{y}\) in Euclidean distance. Indeed, minimizing the squared
    Euclidean distance is equivalent to minimizing its square root, as the latter
    in an increasing function.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，我们正在寻找 \(A\) 的列的线性组合，使其在欧几里得距离上最接近 \(\mathbf{y}\)。实际上，最小化平方欧几里得距离等同于最小化其平方根，因为后者是一个增函数。
- en: One could solve this optimization problem through calculus (and we will come
    back to this approach later in the book), but understanding the geometric and
    algebraic structure of the problem turns out to provide powerful insights into
    its solution – and that of many of problems in data science. It will also be an
    opportunity to review some basic linear-algebraic concepts along the way.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过微积分（我们将在本书的后面部分回到这种方法）来解决这个优化问题，但理解问题的几何和代数结构最终证明为解决其解决方案——以及数据科学中许多问题的解决方案——提供了强大的洞察力。这也会是一个回顾一些基本线性代数概念的机会。
- en: We will come back to the `advertising` dataset later in the chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的后面部分回到 `advertising` 数据集。
