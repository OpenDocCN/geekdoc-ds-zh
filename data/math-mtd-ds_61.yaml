- en: 8\. Neural networks, backpropagation and stochastic gradient descent#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8. 神经网络、反向传播和随机梯度下降#
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/00_intro/roch-mmids-nn-intro.html](https://mmids-textbook.github.io/chap08_nn/00_intro/roch-mmids-nn-intro.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap08_nn/00_intro/roch-mmids-nn-intro.html](https://mmids-textbook.github.io/chap08_nn/00_intro/roch-mmids-nn-intro.html)
- en: We end the book with an introduction to the basic mathematical building blocks
    of modern artificial intelligence (AI). We first derive a generalization of the
    Chain Rule and give a brief overview of automatic differentiation. We then describe
    backpropagation in the context of progressive functions, implement stochastic
    gradient descent (SGD), and apply these methods to deep neural networks (specifically,
    multilayer perceptrons). Here is a more detailed overview of the main sections
    of the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以现代人工智能（AI）的基本数学构建块介绍结束本书。我们首先推导出链式法则的推广，并对自动微分进行了简要概述。然后，我们在递归函数的背景下描述了反向传播，实现了随机梯度下降（SGD），并将这些方法应用于深度神经网络（特别是多层感知器）。以下是本章主要部分的更详细概述。
- en: '*“Background: Jacobian, chain rule, and a brief introduction to automatic differentiation”*
    This section introduces the Jacobian matrix, which generalizes the concept of
    the derivative to vector-valued functions of several variables, as well as the
    generalized Chain Rule for composing differentiable functions in this setting.
    It also covers some useful matrix algebra, specifically the Hadamard and Kronecker
    products. Finally, the section gives a brief introduction to automatic differentiation,
    a powerful technique for efficiently computing derivatives that is central to
    modern machine learning, and illustrates its use with the PyTorch library.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*“背景：雅可比矩阵、链式法则以及自动微分简介”* 本节介绍了雅可比矩阵，它将导数的概念推广到多个变量的向量值函数，以及在此设置中组合可微函数的广义链式法则。它还涵盖了有用的矩阵代数，特别是Hadamard和Kronecker乘积。最后，本节简要介绍了自动微分，这是一种强大的技术，可以有效地计算导数，是现代机器学习的关键，并使用PyTorch库说明了其应用。'
- en: '*“Building blocks of AI 1: backpropagation”* This section develops the mathematical
    foundations for automatic differentiation in the context of multi-layer progressive
    functions, which are sequences of compositions with layer-specific parameters.
    It explains how to systematically apply the Chain Rule to compute gradients of
    these functions. The section contrasts two methods for computing gradients: the
    forward mode and the reverse mode (also known as backpropagation). While the forward
    mode computes the function and its gradient simultaneously in a recursive manner,
    the reverse mode is often more efficient, especially for functions with many parameters
    but a small number of outputs. The reverse mode achieves this efficiency by in
    essence computing matrix-vector products instead of matrix-matrix products, making
    it particularly well-suited for machine learning applications.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*“AI构建块1：反向传播”* 本节在多层递归函数的背景下发展了自动微分的数学基础，这些函数是具有层特定参数的复合序列。它解释了如何系统地应用链式法则来计算这些函数的梯度。本节对比了两种计算梯度的方法：前向模式和反向模式（也称为反向传播）。虽然前向模式以递归方式同时计算函数及其梯度，但反向模式通常更有效，特别是对于具有许多参数但输出数量较少的函数。反向模式通过本质上计算矩阵-向量乘积而不是矩阵-矩阵乘积来实现这种效率，这使得它特别适合机器学习应用。'
- en: '*“Building blocks of AI 2: stochastic gradient descent”* This section discusses
    stochastic gradient descent (SGD), a popular optimization algorithm used to train
    machine learning models, particularly in scenarios with large datasets. It is
    a variation of gradient descent where, instead of computing the gradient over
    the entire dataset, it estimates the gradient using a randomly selected subset
    of data points (either a single sample or a mini-batch). The key idea behind SGD
    is that, while each update may not be perfectly aligned with the true gradient,
    the expected direction of the update is still in the direction of the steepest
    descent, leading to convergence over time. This approach offers computational
    advantages, especially when dealing with massive datasets, as it avoids the expensive
    calculation of the full gradient at each iteration. The section provides detailed
    examples of applying SGD together with backpropagation. Additionally, it covers
    the use of PyTorch, demonstrating how to handle datasets, construct models, and
    perform optimization tasks using mini-batch SGD.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*“人工智能构建块 2：随机梯度下降”* 本节讨论了随机梯度下降（SGD），这是一种流行的优化算法，用于训练机器学习模型，尤其是在处理大数据集的场景中。它是梯度下降的一种变体，其中，不是在整个数据集上计算梯度，而是使用随机选择的数据点子集（单个样本或小批量）来估计梯度。SGD背后的关键思想是，虽然每次更新可能并不完全与真实梯度对齐，但更新的期望方向仍然是下降最快的方向，从而随着时间的推移实现收敛。这种方法在处理大规模数据集时提供了计算上的优势，因为它避免了在每次迭代中计算完整梯度的昂贵计算。本节提供了应用SGD与反向传播相结合的详细示例。此外，它还涵盖了使用PyTorch的使用，展示了如何处理数据集、构建模型以及使用小批量SGD执行优化任务。'
- en: '*“Building blocks of AI 3: neural networks”* This section introduces neural
    networks, specifically focusing on the multilayer perceptron (MLP) architecture.
    It explains how each layer of an MLP consists of an affine map followed by a nonlinear
    activation function. In the setting of a classification task, the output layer
    uses the softmax function to produce a probability distribution over the possible
    classes; the loss function used to train the MLP is the cross-entropy loss. The
    section then walks through a detailed example of computing the gradient of the
    loss function with respect to the weights in a small MLP, using the Chain Rule
    and properties of Kronecker products. It generalizes this gradient computation
    to MLPs with an arbitrary number of layers. Finally, the section demonstrates
    how to implement the training of a neural network in PyTorch, using the Fashion-MNIST
    dataset.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*“人工智能构建块 3：神经网络”* 本节介绍了神经网络，特别是关注多层感知器（MLP）架构。它解释了MLP的每一层由一个仿射映射后跟一个非线性激活函数组成。在分类任务的设置中，输出层使用softmax函数来生成可能的类别的概率分布；用于训练MLP的损失函数是交叉熵损失。然后本节详细说明了如何使用链式法则和Kronecker积的性质，计算一个小型MLP中损失函数相对于权重的梯度。它将这种梯度计算推广到具有任意层数的MLP。最后，本节展示了如何在PyTorch中使用Fashion-MNIST数据集实现神经网络的训练。'
